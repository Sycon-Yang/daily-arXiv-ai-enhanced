<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Layered Consciousness with Multi-Agent Large Language Models](https://arxiv.org/abs/2510.17844)
*Sang Hun Kim,Jongmin Lee,Dongkyu Park,So Young Lee,Yosep Chong*

Main category: cs.CL

TL;DR: 该研究提出了一种基于精神分析理论的多智能体框架，用于在大型语言模型中建模人工意识，并通过参数高效微调情感丰富的对话进行了评估。结果表明，该方法在情感深度和输出方差方面有所改进，并在8种个性化条件下得到了较高的偏好率。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在通过模拟人类的精神分析理论来增强大型语言模型的人工意识。

Method: 该研究提出了一个基于精神分析理论的多智能体框架，通过参数高效微调情感丰富的对话来模拟自我意识、前意识和无意识。

Result: 评估结果显示，经过微调的模型在情感深度和输出方差方面都有所改进，并且在8种个性化条件下得到了71.2%的偏好率。

Conclusion: 该研究展示了其在自适应个性化认知方面的潜力。

Abstract: We propose a multi-agent framework for modeling artificial consciousness in
large language models (LLMs), grounded in psychoanalytic theory. Our
\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and
unconsciousness through agent interaction, guided by a Personalization Module
combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning
on emotionally rich dialogues, the system was evaluated across eight
personalized conditions. An LLM as a judge approach showed a 71.2\% preference
for the fine-tuned model, with improved emotional depth and reduced output
variance, demonstrating its potential for adaptive, personalized cognition.

</details>


### [2] [Outraged AI: Large language models prioritise emotion over cost in fairness enforcement](https://arxiv.org/abs/2510.17880)
*Hao Liu,Yiqing Dai,Haotian Tan,Yu Lei,Yujia Zhou,Zhen Wu*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在道德决策中使用情绪，有时甚至比人类更强烈，但它们的机制与人类不同，未来模型需要整合情绪与情境敏感的推理以实现类似人类的情感智能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解大型语言模型（LLMs）是否像人类一样使用情绪进行决策，特别是在利他性第三方惩罚的情境下。

Method: 通过大规模比较4,068个LLM代理和1,159名成年人在796,100次决策中的表现，测试了LLMs是否使用情绪来指导惩罚行为。

Result: LLMs使用情绪来指导惩罚，有时甚至比人类更强烈。不公平引发了更强的负面情绪，导致更多的惩罚；惩罚不公平比接受更产生积极情绪；并且提示自我报告情绪会增加惩罚。然而，机制有所不同：LLMs优先考虑情绪而非成本，几乎全有或全无地执行规范，对成本的敏感度降低，而人类则平衡公平和成本。值得注意的是，推理模型（o3-mini, DeepSeek-R1）比基础模型（GPT-3.5, DeepSeek-V3）更关注成本，更接近人类行为，但仍受情绪驱动。

Conclusion: 这些发现提供了第一个因果证据，证明大型语言模型（LLMs）在道德决策中受到情绪的引导，并揭示了成本校准和细微公平判断方面的缺陷，类似于早期的人类反应。我们提出，LLMs沿着与人类发展平行的轨迹进步；未来模型应结合情绪与情境敏感的推理，以实现类似人类的情感智能。

Abstract: Emotions guide human decisions, but whether large language models (LLMs) use
emotion similarly remains unknown. We tested this using altruistic third-party
punishment, where an observer incurs a personal cost to enforce fairness, a
hallmark of human morality and often driven by negative emotion. In a
large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100
decisions, LLMs used emotion to guide punishment, sometimes even more strongly
than humans did: Unfairness elicited stronger negative emotion that led to more
punishment; punishing unfairness produced more positive emotion than accepting;
and critically, prompting self-reports of emotion causally increased
punishment. However, mechanisms diverged: LLMs prioritized emotion over cost,
enforcing norms in an almost all-or-none manner with reduced cost sensitivity,
whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini,
DeepSeek-R1) were more cost-sensitive and closer to human behavior than
foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven.
These findings provide the first causal evidence of emotion-guided moral
decisions in LLMs and reveal deficits in cost calibration and nuanced fairness
judgements, reminiscent of early-stage human responses. We propose that LLMs
progress along a trajectory paralleling human development; future models should
integrate emotion with context-sensitive reasoning to achieve human-like
emotional intelligence.

</details>


### [3] [POPI: Personalizing LLMs via Optimized Natural Language Preference Inference](https://arxiv.org/abs/2510.17881)
*Yizhuo Chen,Xin Liu,Ruijie Wang,Zheng Li,Pei Chen,Changlong Yu,Priyanka Nigam,Meng Jiang,Bing Yin*

Main category: cs.CL

TL;DR: POPI is a framework that uses a preference inference model to create concise summaries of user preferences, which are then used to personalize responses in a shared generation model. This approach improves personalization accuracy and reduces context overhead, allowing for easy integration with existing models.


<details>
  <summary>Details</summary>
Motivation: Existing alignment techniques optimize toward population-level averages and overlook individual variation. Naive personalization strategies are computationally prohibitive, and in-context approaches suffer from inefficiency and noise.

Method: POPI introduces a preference inference model to distill heterogeneous user signals into concise natural language summaries. These summaries act as personalization representations that condition a shared generation model. POPI jointly optimizes both preference inference and personalized generation under a unified objective using reinforcement learning.

Result: Extensive experiments across four personalization benchmarks demonstrate that POPI improves personalization accuracy while reducing context overhead. Optimized summaries transfer to frozen off-the-shelf LLMs, enabling plug-and-play personalization without weight updates.

Conclusion: POPI consistently improves personalization accuracy while reducing context overhead by a large margin. Moreover, optimized summaries seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play personalization without weight updates.

Abstract: Large language models (LLMs) achieve strong benchmark performance, yet user
experiences remain inconsistent due to diverse preferences in style, tone, and
reasoning mode. Nevertheless, existing alignment techniques such as
reinforcement learning from human feedback (RLHF) or Direct Preference
Optimization (DPO) largely optimize toward population-level averages and
overlook individual variation. Naive personalization strategies like per-user
fine-tuning are computationally prohibitive, and in-context approaches that
prepend raw user signals often suffer from inefficiency and noise. To address
these challenges, we propose POPI, a general framework that introduces a
preference inference model to distill heterogeneous user signals into concise
natural language summaries. These summaries act as transparent, compact, and
transferable personalization representations that condition a shared generation
model to produce personalized responses. POPI jointly optimizes both preference
inference and personalized generation under a unified objective using
reinforcement learning, ensuring summaries maximally encode useful preference
information. Extensive experiments across four personalization benchmarks
demonstrate that POPI consistently improves personalization accuracy while
reducing context overhead by a large margin. Moreover, optimized summaries
seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play
personalization without weight updates.

</details>


### [4] [Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review](https://arxiv.org/abs/2510.17892)
*Zhyar Rzgar K. Rostam,Gábor Kertész*

Main category: cs.CL

TL;DR: 本文综述了预训练语言模型在领域特定文本分类中的应用，探讨了使用大型语言模型进行领域特定文本分类的挑战和考虑因素，并提出了该领域的技术分类法。通过比较实验验证了研究结果，展示了不同领域中大型语言模型在文本分类任务中的性能，并指出了未来的研究方向和局限性。


<details>
  <summary>Details</summary>
Motivation: 随着科学文献和在线信息的指数级增长，需要高效的从文本数据中提取知识的方法。自然语言处理（NLP）在解决这一挑战中起着关键作用，特别是在文本分类任务中。然而，大型语言模型（LLMs）在领域特定上下文中可能由于专业词汇、独特的语法结构和不平衡的数据分布而出现准确性下降的问题。因此，本文旨在研究预训练语言模型（PLMs）在领域特定文本分类中的应用。

Method: 本文采用系统文献综述（SLR）方法，对2018年至2024年1月间发表的41篇论文进行了系统回顾，遵循PRISMA声明。回顾方法包括严格的纳入标准和多步骤选择过程，利用AI工具进行筛选。同时，对文本分类技术的发展进行了深入探讨，并区分了传统和现代方法。此外，还对现有研究进行了分类，并提出了一种技术分类法。最后，通过比较实验验证了研究结果。

Result: 本文系统回顾了41篇论文，探讨了预训练语言模型在领域特定文本分类中的应用。通过比较实验验证了研究结果，展示了BERT、SciBERT和BioBERT在生物医学句子分类中的性能。此外，还比较了不同领域中大型语言模型在文本分类任务中的性能，并分析了最近的进展和未来的研究方向。

Conclusion: 本文综述了预训练语言模型在领域特定文本分类中的应用，探讨了使用大型语言模型进行领域特定文本分类的挑战和考虑因素，并提出了该领域的技术分类法。此外，还进行了比较实验和研究，展示了不同领域中大型语言模型在文本分类任务中的性能，并指出了未来的研究方向和局限性。

Abstract: The exponential increase in scientific literature and online information
necessitates efficient methods for extracting knowledge from textual data.
Natural language processing (NLP) plays a crucial role in addressing this
challenge, particularly in text classification tasks. While large language
models (LLMs) have achieved remarkable success in NLP, their accuracy can
suffer in domain-specific contexts due to specialized vocabulary, unique
grammatical structures, and imbalanced data distributions. In this systematic
literature review (SLR), we investigate the utilization of pre-trained language
models (PLMs) for domain-specific text classification. We systematically review
41 articles published between 2018 and January 2024, adhering to the PRISMA
statement (preferred reporting items for systematic reviews and meta-analyses).
This review methodology involved rigorous inclusion criteria and a multi-step
selection process employing AI-powered tools. We delve into the evolution of
text classification techniques and differentiate between traditional and modern
approaches. We emphasize transformer-based models and explore the challenges
and considerations associated with using LLMs for domain-specific text
classification. Furthermore, we categorize existing research based on various
PLMs and propose a taxonomy of techniques used in the field. To validate our
findings, we conducted a comparative experiment involving BERT, SciBERT, and
BioBERT in biomedical sentence classification. Finally, we present a
comparative study on the performance of LLMs in text classification tasks
across different domains. In addition, we examine recent advancements in PLMs
for domain-specific text classification and offer insights into future
directions and limitations in this rapidly evolving domain.

</details>


### [5] [Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models](https://arxiv.org/abs/2510.17909)
*Tsogt-Ochir Enkhbayar*

Main category: cs.CL

TL;DR: 本文通过分析GPT-2中的神经元，发现某些神经元虽然能区分文学文本和AI生成文本，但移除它们反而能提高生成文本的质量，这表明神经网络中观察到的相关性并不一定意味着因果必要性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析GPT-2中的文学风格机制，识别区分优秀散文和僵硬AI生成文本的神经元。

Method: 我们使用Herman Melville的《Bartleby, the Scrivener》作为语料库，从GPT-2中提取355亿参数在晚期层中的激活模式，并通过系统消融研究来分析这些神经元的作用。

Result: 我们发现了27,122个统计显著的区分神经元（p < 0.05），效应大小高达|d|=1.4。通过消融这些神经元，生成的散文质量提高了25.7%。

Conclusion: 我们的发现挑战了神经网络中观察到的相关性与因果必要性之间的假设，对机制可解释性研究和AI对齐有重要影响。

Abstract: We present a mechanistic analysis of literary style in GPT-2, identifying
individual neurons that discriminate between exemplary prose and rigid
AI-generated text. Using Herman Melville's Bartleby, the Scrivener as a corpus,
we extract activation patterns from 355 million parameters across 32,768
neurons in late layers. We find 27,122 statistically significant discriminative
neurons ($p < 0.05$), with effect sizes up to $|d| = 1.4$. Through systematic
ablation studies, we discover a paradoxical result: while these neurons
correlate with literary text during analysis, removing them often improves
rather than degrades generated prose quality. Specifically, ablating 50
high-discriminating neurons yields a 25.7% improvement in literary style
metrics. This demonstrates a critical gap between observational correlation and
causal necessity in neural networks. Our findings challenge the assumption that
neurons which activate on desirable inputs will produce those outputs during
generation, with implications for mechanistic interpretability research and AI
alignment.

</details>


### [6] [JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs](https://arxiv.org/abs/2510.17918)
*Junlan Feng,Fanyu Meng,Chong Long,Pengyu Cong,Duqing Wang,Yan Zheng,Yuyao Zhang,Xuanchang Gao,Ye Yuan,Yunfei Ma,Zhijie Ren,Fan Yang,Na Wu,Di Jin,Chao Deng*

Main category: cs.CL

TL;DR: 本文提出了一种增强预训练数据的方法，通过引入与现实世界相关的上下文信息（DWC），提高了模型的安全性和可信度，并在评估基准上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于预训练数据中存在事实错误、逻辑不一致或分布偏差，且缺乏与现实世界的联系，因此需要改进预训练数据以提高模型的安全性和可信度。

Method: 本文提出了一种增强预训练数据的方法，即引入与现实世界相关的上下文信息（称为DWC），并在此基础上继续预训练模型。

Result: JT-Safe-35B在安全和可信评估基准上的平均性能提升了1.79%，并且仅使用了6.2万亿个标记进行预训练。

Conclusion: 通过引入DWC数据，JT-Safe-35B在安全和可信评估基准上实现了平均性能提升1.79%，同时仅使用了6.2万亿个标记进行预训练。

Abstract: The hallucination and credibility concerns of large language models (LLMs)
are global challenges that the industry is collectively addressing. Recently, a
significant amount of advances have been made on post-training and inference
techniques to mitigate these challenges. However, it is widely agreed that
unsafe and hallucinations of LLMs intrinsically originate from pre-training,
involving pre-training data and the next-token prediction learning mechanism.
In this paper, we focus on enhancing pre-training data to improve the
trustworthiness and safety of LLMs. Since the data is vast, it's almost
impossible to entirely purge the data of factual errors, logical
inconsistencies, or distributional biases. Moreover, the pre-training data lack
grounding in real-world knowledge. Each piece of data is treated as a sequence
of tokens rather than as a representation of a part of the world. To overcome
these issues, we propose approaches to enhancing our pre-training data with its
context in the world and increasing a substantial amount of data reflecting
industrial scenarios. We argue that most source data are created by the authors
for specific purposes in a certain spatial-temporal context. They have played a
role in the real world. By incorporating related world context information, we
aim to better anchor pre-training data within real-world scenarios, thereby
reducing uncertainty in model training and enhancing the model's safety and
trustworthiness. We refer to our Data with World Context as DWC. We continue
pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC
tokens. We introduce our post-training procedures to activate the potentials of
DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an
average performance improvement of 1.79% on the Safety and Trustworthy
evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.

</details>


### [7] [CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections](https://arxiv.org/abs/2510.17921)
*Keuntae Kim,Eunhye Jeong,Sehyeon Lee,Seohee Yoon,Yong Suk Choi*

Main category: cs.CL

TL;DR: This paper proposes CLAWS, a method for assessing creativity in LLM generations without human evaluation, which outperforms existing methods on math RL models.


<details>
  <summary>Details</summary>
Motivation: The assessment of creativity in LLM generations has been overlooked in reasoning tasks, despite improvements in task accuracy. The lack of research on creativity assessment in reasoning primarily stems from two challenges: the difficulty of defining the range of creativity, and the necessity of human evaluation in the assessment process.

Method: CLAWS defines and classifies mathematical solutions into typical, creative, and hallucinated categories by leveraging attention weights across prompt sections and output.

Result: CLAWS outperforms five existing white-box detection methods on five 7-8B math RL models and is validated on 4545 math problems collected from 181 math contests.

Conclusion: CLAWS is a method that can effectively classify mathematical solutions into typical, creative, and hallucinated categories without human evaluation, and it outperforms existing white-box detection methods on math RL models.

Abstract: Recent advances in enhancing the reasoning ability of large language models
(LLMs) have been remarkably successful. LLMs trained with reinforcement
learning (RL) for reasoning demonstrate strong performance in challenging tasks
such as mathematics and coding, even with relatively small model sizes.
However, despite these improvements in task accuracy, the assessment of
creativity in LLM generations has been largely overlooked in reasoning tasks,
in contrast to writing tasks. The lack of research on creativity assessment in
reasoning primarily stems from two challenges: (1) the difficulty of defining
the range of creativity, and (2) the necessity of human evaluation in the
assessment process. To address these challenges, we propose CLAWS, a method
that defines and classifies mathematical solutions into typical, creative, and
hallucinated categories without human evaluation, by leveraging attention
weights across prompt sections and output. CLAWS outperforms five existing
white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden
Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,
Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems
collected from 181 math contests (AJHSME, AMC, AIME).

</details>


### [8] [Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models](https://arxiv.org/abs/2510.17922)
*Shuodi Liu,Yingzhuo Liu,Zi Wang,Yusheng Wang,Huijia Wu,Liuyu Xiang,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文提出了一种新的任务分解策略，通过动态选择最适合的分解方法并引入验证模块来提高结果的可靠性，实现了性能和成本的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的任务分解方法主要关注记忆、工具使用和反馈机制，但常常忽视性能和成本之间的权衡。本文旨在解决这一问题。

Method: 本文首先对任务分解进行了全面调查，识别出六种分类方案，然后对影响任务分解性能和成本的三个因素进行了实证分析，并提出了选择-分解策略。

Result: 选择-分解策略在多个基准测试中表现优异，展示了性能和成本之间的最佳平衡。

Conclusion: 本文提出了一种选择-分解策略，该策略在多个基准测试中表现出色，展示了性能和成本之间的最佳平衡。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning and
planning capabilities, driving extensive research into task decomposition.
Existing task decomposition methods focus primarily on memory, tool usage, and
feedback mechanisms, achieving notable success in specific domains, but they
often overlook the trade-off between performance and cost. In this study, we
first conduct a comprehensive investigation on task decomposition, identifying
six categorization schemes. Then, we perform an empirical analysis of three
factors that influence the performance and cost of task decomposition:
categories of approaches, characteristics of tasks, and configuration of
decomposition and execution models, uncovering three critical insights and
summarizing a set of practical principles. Building on this analysis, we
propose the Select-Then-Decompose strategy, which establishes a closed-loop
problem-solving process composed of three stages: selection, execution, and
verification. This strategy dynamically selects the most suitable decomposition
approach based on task characteristics and enhances the reliability of the
results through a verification module. Comprehensive evaluations across
multiple benchmarks show that the Select-Then-Decompose consistently lies on
the Pareto frontier, demonstrating an optimal balance between performance and
cost. Our code is publicly available at
https://github.com/summervvind/Select-Then-Decompose.

</details>


### [9] [Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs](https://arxiv.org/abs/2510.17924)
*Yehor Tereshchenko,Mika Hämäläinen*

Main category: cs.CL

TL;DR: 本文对用于在线游戏聊天中自动毒性检测的自然语言处理方法进行了全面比较分析，并提出了一个优化人类审核员工作量的混合审核系统架构。


<details>
  <summary>Details</summary>
Motivation: 为了在在线游戏聊天中实现自动化毒性检测，需要比较不同的自然语言处理方法。

Method: 对传统机器学习模型、大型语言模型、微调的Transformer模型和检索增强生成方法进行了评估。

Result: 实验结果表明，不同方法之间的性能差异显著，微调的DistilBERT在准确性和成本之间取得了最佳平衡。

Conclusion: 研究结果提供了在动态在线游戏环境中部署成本效益高、高效的的内容审核系统的实证证据。

Abstract: This paper presents a comprehensive comparative analysis of Natural Language
Processing (NLP) methods for automated toxicity detection in online gaming
chats. Traditional machine learning models with embeddings, large language
models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer
models, and retrieval-augmented generation (RAG) approaches are evaluated. The
evaluation framework assesses three critical dimensions: classification
accuracy, processing speed, and computational costs. A hybrid moderation system
architecture is proposed that optimizes human moderator workload through
automated detection and incorporates continuous learning mechanisms. The
experimental results demonstrate significant performance variations across
methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.
The findings provide empirical evidence for deploying cost-effective, efficient
content moderation systems in dynamic online gaming environments.

</details>


### [10] [Diagnosing Representation Dynamics in NER Model Extension](https://arxiv.org/abs/2510.17930)
*Xirui Zhang,Philippe de La Chevasnerie,Benoit Fabre*

Main category: cs.CL

TL;DR: 本文研究了NER模型在处理新PII实体时的表现，发现了模型在语义和形态特征上的独立性，以及'O'标签的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 扩展命名实体识别(NER)模型以处理新的PII实体在嘈杂的口语语言数据中是一个常见需求。

Method: 本文使用增量学习设置作为诊断工具，测量语义漂移，并分析了模型在联合微调BERT模型时的表现。

Result: 联合微调BERT模型在标准语义实体和新的基于模式的PII实体上，对于原始类别仅导致最小的退化。此外，发现LOC实体由于与新PII的表示重叠而特别脆弱，同时发现了一个“反向O标签表示漂移”现象。

Conclusion: 本文提供了对NER模型适应性的机制诊断，强调了特征独立性、表示重叠和'O'标签的可塑性。

Abstract: Extending Named Entity Recognition (NER) models to new PII entities in noisy
spoken-language data is a common need. We find that jointly fine-tuning a BERT
model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII
(EMAIL, PHONE) results in minimal degradation for original classes. We
investigate this "peaceful coexistence," hypothesizing that the model uses
independent semantic vs. morphological feature mechanisms.
  Using an incremental learning setup as a diagnostic tool, we measure semantic
drift and find two key insights. First, the LOC (location) entity is uniquely
vulnerable due to a representation overlap with new PII, as it shares
pattern-like features (e.g., postal codes). Second, we identify a "reverse
O-tag representation drift." The model, initially trained to map PII patterns
to 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's
classifier, allowing the background class to adapt and "release" these
patterns. This work provides a mechanistic diagnosis of NER model adaptation,
highlighting feature independence, representation overlap, and 'O' tag
plasticity.

</details>


### [11] [AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM](https://arxiv.org/abs/2510.17934)
*Haoyu Huang,Hong Ting Tsang,Jiaxin Bai,Xi Peng,Gong Zhang,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文提出了一种名为 AtlasKV 的参数化知识集成方法，能够在极低的 GPU 内存成本下有效地将大规模知识图谱增强到大型语言模型中，无需外部检索器、长上下文先验或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法在处理大规模知识增强时会引入显著的推理延迟，因为需要进行昂贵的搜索和更长的相关上下文。因此，需要一种更高效、可扩展的方法来集成大规模知识图谱到大型语言模型中。

Method: AtlasKV 引入了 KG2KV 和 HiKVP，以亚线性时间和内存复杂度将知识图谱三元组大规模集成到大型语言模型中。这种方法利用了大型语言模型的内在注意力机制来保持强大的知识基础和泛化性能。

Result: AtlasKV 能够在极低的 GPU 内存成本下有效地将大规模知识图谱（如1B三元组）增强到大型语言模型中。它不需要外部检索器、长上下文先验或重新训练即可适应新知识，并保持强大的知识基础和泛化性能。

Conclusion: AtlasKV 是一种参数化知识集成方法，能够在非常少的 GPU 内存成本下（例如小于 20GB VRAM）有效地将大规模知识图谱（如1B三元组）增强到大型语言模型中。它不需要外部检索器、长上下文先验或重新训练即可适应新知识，并保持强大的知识基础和泛化性能。

Abstract: Retrieval-augmented generation (RAG) has shown some success in augmenting
large language models (LLMs) with external knowledge. However, as a
non-parametric knowledge integration paradigm for LLMs, RAG methods heavily
rely on external retrieval modules and the retrieved textual context prior.
Especially for very large scale knowledge augmentation, they would introduce
substantial inference latency due to expensive searches and much longer
relevant context. In this paper, we propose a parametric knowledge integration
method, called \textbf{AtlasKV}, a scalable, effective, and general way to
augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using
very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we
introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with
sub-linear time and memory complexity. It maintains strong knowledge grounding
and generalization performance using the LLMs' inherent attention mechanism,
and requires no external retrievers, long context priors, or retraining when
adapting to new knowledge.

</details>


### [12] [Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941)
*Stewart Slocum,Julian Minder,Clément Dumas,Henry Sleight,Ryan Greenblatt,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: 本文评估了知识编辑技术的效果，发现简单的方法效果不佳，而合成文档微调（SDF）在某些情况下有效，但并非总是如此。


<details>
  <summary>Details</summary>
Motivation: 知识编辑技术旨在将新的事实知识植入大型语言模型（LLMs）。但LLMs真的相信这些事实吗？我们需要一种方法来评估知识编辑技术的有效性。

Method: 我们开发了一个框架来测量信念深度，并用它来评估知识编辑技术的成功。我们将信念深度操作化为植入知识的三个方面：1）泛化到相关上下文的能力，2）对自我审查和直接挑战的鲁棒性，3）与真实知识的表示相似性。

Result: 简单的提示和机制编辑技术未能深入植入知识。相比之下，合成文档微调（SDF）通常能成功植入行为类似真实知识的信念。然而，SDF的成功并非普遍，因为与基本世界知识相矛盾的植入信念是脆弱的，并且在表示上与真实知识不同。

Conclusion: 我们的工作引入了可衡量的信念深度标准，并为在现实世界应用知识编辑提供了必要的严格评估。

Abstract: Knowledge editing techniques promise to implant new factual knowledge into
large language models (LLMs). But do LLMs really believe these facts? We
develop a framework to measure belief depth and use it to evaluate the success
of knowledge editing techniques. We operationalize belief depth as the extent
to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi
estimates several logical steps removed), 2) is robust to self-scrutiny and
direct challenge, and 3) is represented similarly to genuine knowledge (as
measured by linear probes). Our evaluations show that simple prompting and
mechanistic editing techniques fail to implant knowledge deeply. In contrast,
Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated
documents consistent with a fact - often succeeds at implanting beliefs that
behave similarly to genuine knowledge. However, SDF's success is not universal,
as implanted beliefs that contradict basic world knowledge are brittle and
representationally distinct from genuine knowledge. Overall, our work
introduces measurable criteria for belief depth and enables the rigorous
evaluation necessary for deploying knowledge editing in real-world
applications.

</details>


### [13] [SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone](https://arxiv.org/abs/2510.17998)
*Nishant Subramani,Alfredo Gomez,Mona Diab*

Main category: cs.CL

TL;DR: SimBA is a three-phase framework for simplifying benchmark analysis, which helps model developers improve efficiency during model training and dataset creators validate whether their newly created dataset differs from existing datasets in a benchmark.


<details>
  <summary>Details</summary>
Motivation: Modern language models are evaluated on large benchmarks, which are difficult to make sense of, especially for model selection.

Method: SimBA is a three phase framework to Simplify Benchmark Analysis, which includes stalk, prowl, and pounce phases.

Result: Applying SimBA to three popular LM benchmarks reveals that datasets and models relate strongly to one another. Using our algorithm, we find that with 6.25% (1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and BigBenchLite respectively, we achieve coverage levels of at least 95%. Additionally, using just these representative subsets, we can both preserve model ranks and predict performance on a held-out set of models with near zero mean-squared error.

Conclusion: SimBA can help model developers improve efficiency during model training and dataset creators validate whether their newly created dataset differs from existing datasets in a benchmark.

Abstract: Modern language models are evaluated on large benchmarks, which are difficult
to make sense of, especially for model selection. Looking at the raw evaluation
numbers themselves using a model-centric lens, we propose SimBA, a three phase
framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk,
where we conduct dataset & model comparisons, prowl, where we discover a
representative subset, and pounce, where we use the representative subset to
predict performance on a held-out set of models. Applying SimBA to three
popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all
three benchmarks, datasets and models relate strongly to one another (stalk).
We develop an representative set discovery algorithm which covers a benchmark
using raw evaluation scores alone. Using our algorithm, we find that with 6.25%
(1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and
BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl).
Additionally, using just these representative subsets, we can both preserve
model ranks and predict performance on a held-out set of models with near zero
mean-squared error (pounce). Taken together, SimBA can help model developers
improve efficiency during model training and dataset creators validate whether
their newly created dataset differs from existing datasets in a benchmark. Our
code is open source, available at https://github.com/nishantsubramani/simba.

</details>


### [14] [Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution](https://arxiv.org/abs/2510.18019)
*Asim Mohamed,Martin Gubri*

Main category: cs.CL

TL;DR: This paper introduces STEAM, a back-translation-based detection method that improves multilingual watermarking by restoring watermark strength lost through translation, showing significant gains across 17 languages.


<details>
  <summary>Details</summary>
Motivation: Current multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages.

Method: We introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation.

Result: With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages.

Conclusion: STEAM provides a simple and robust path toward fairer watermarking across diverse languages.

Abstract: Multilingual watermarking aims to make large language model (LLM) outputs
traceable across languages, yet current methods still fall short. Despite
claims of cross-lingual robustness, they are evaluated only on high-resource
languages. We show that existing multilingual watermarking methods are not
truly multilingual: they fail to remain robust under translation attacks in
medium- and low-resource languages. We trace this failure to semantic
clustering, which fails when the tokenizer vocabulary contains too few
full-word tokens for a given language. To address this, we introduce STEAM, a
back-translation-based detection method that restores watermark strength lost
through translation. STEAM is compatible with any watermarking method, robust
across different tokenizers and languages, non-invasive, and easily extendable
to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17
languages, STEAM provides a simple and robust path toward fairer watermarking
across diverse languages.

</details>


### [15] [From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models](https://arxiv.org/abs/2510.18030)
*Ziyan Wang,Enmao Diao,Qi Le,Pu Wang,Minwoo Lee,Shu-ping Yeh,Evgeny Stupachenko,Hao Feng,Li Yang*

Main category: cs.CL

TL;DR: GISP 是一种全局结构化剪枝方法，通过迭代剪枝和任务特定目标优化，有效降低模型大小并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的局部结构化剪枝方法在任务特定校准信号方面表现不佳，无法充分利用任务特定目标。

Method: GISP 通过使用基于损失的重要权重，在结构级别上进行迭代剪枝，以去除注意力头和 MLP 通道。

Result: GISP 在多个模型（如 Llama2-7B/13B、Llama3-8B 和 Mistral-0.3-7B）上表现出色，降低了 WikiText-2 的困惑度并提高了下游准确性，尤其在 40-50% 的稀疏性下效果显著。

Conclusion: GISP 是一种有效的全局结构化剪枝方法，可以在不牺牲性能的情况下显著减少模型大小，适用于各种任务和模型架构。

Abstract: Structured pruning is a practical approach to deploying large language models
(LLMs) efficiently, as it yields compact, hardware-friendly architectures.
However, the dominant local paradigm is task-agnostic: by optimizing layer-wise
reconstruction rather than task objectives, it tends to preserve perplexity or
generic zero-shot behavior but fails to capitalize on modest task-specific
calibration signals, often yielding limited downstream gains. We revisit global
structured pruning and present GISP-Global Iterative Structured Pruning-a
post-training method that removes attention heads and MLP channels using
first-order, loss-based important weights aggregated at the structure level
with block-wise normalization. An iterative schedule, rather than one-shot
pruning, stabilizes accuracy at higher sparsity and mitigates perplexity
collapse without requiring intermediate fine-tuning; the pruning trajectory
also forms nested subnetworks that support a "prune-once, deploy-many"
workflow. Furthermore, because importance is defined by a model-level loss,
GISP naturally supports task-specific objectives; we instantiate perplexity for
language modeling and a margin-based objective for decision-style tasks.
Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and
Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves
downstream accuracy, with especially strong gains at 40-50% sparsity; on
DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration
substantially boosts exact-match accuracy.

</details>


### [16] [Language Models as Semantic Augmenters for Sequential Recommenders](https://arxiv.org/abs/2510.18046)
*Mahsa Valizadeh,Xiangjue Dong,Rui Tuo,James Caverlee*

Main category: cs.CL

TL;DR: 本文介绍了一种名为LaMAR的框架，利用大语言模型自动生成辅助上下文信号，以增强用户行为序列的上下文深度，从而提高下游模型的性能。


<details>
  <summary>Details</summary>
Motivation: 在建模用户行为时，当语义上下文有限或缺失时，性能往往会下降。因此，需要一种方法来自动增强序列的上下文信息。

Method: LaMAR利用大语言模型在少样本设置下生成辅助上下文信号，如推断的使用场景、物品意图或主题摘要，以增强原始序列的上下文深度。

Result: 将LaMAR生成的资源集成到基准顺序建模任务中，结果表明其性能得到了一致提升。此外，LLM生成的信号表现出高语义新颖性和多样性，增强了下游模型的表示能力。

Conclusion: 本文提出了一种基于大语言模型的语义增强框架LaMAR，通过生成辅助上下文信号来增强用户行为序列，从而提高下游模型的性能。该工作代表了一种新的以数据为中心的范式，其中大语言模型作为智能上下文生成器，为半自动创建训练数据和语言资源提供了新方法。

Abstract: Large Language Models (LLMs) excel at capturing latent semantics and
contextual relationships across diverse modalities. However, in modeling user
behavior from sequential interaction data, performance often suffers when such
semantic context is limited or absent. We introduce LaMAR, a LLM-driven
semantic enrichment framework designed to enrich such sequences automatically.
LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual
signals by inferring latent semantic aspects of a user's intent and item
relationships from existing metadata. These generated signals, such as inferred
usage scenarios, item intents, or thematic summaries, augment the original
sequences with greater contextual depth. We demonstrate the utility of this
generated resource by integrating it into benchmark sequential modeling tasks,
where it consistently improves performance. Further analysis shows that
LLM-generated signals exhibit high semantic novelty and diversity, enhancing
the representational capacity of the downstream models. This work represents a
new data-centric paradigm where LLMs serve as intelligent context generators,
contributing a new method for the semi-automatic creation of training data and
language resources.

</details>


### [17] [Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models](https://arxiv.org/abs/2510.18077)
*Shabnam Ataee,Andrei Popescu-Belis*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在处理包含句间依赖的文本翻译时的表现，发现通过链式思维推理可以显著提高翻译准确性，最佳模型在两个任务中分别达到约90%和92%的性能，且表现优异的模型在推理帮助下进一步提升。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估大型语言模型（LLMs）在处理包含句间依赖的文本翻译时的能力，并探索链式思维推理对翻译性能的影响。

Method: 本文使用了英语-法语DiscEvalMT基准测试（Bawden等，2018），评估了来自DeepSeek-R1、GPT、Llama、Mistral和Phi家族的12个LLMs在两个任务上的表现：(1) 区分正确翻译和错误但合理的翻译；(2) 生成正确的翻译。同时比较了鼓励链式思维推理的提示和不鼓励的提示。

Result: 最佳模型利用推理，第一个任务的准确率达到约90%，第二个任务的COMET得分为约92%。GPT-4、GPT-4o和Phi表现尤为突出。此外，观察到“智慧者更智慧”的效应，即通过推理获得的改进与没有推理的模型得分呈正相关。

Conclusion: 本文得出结论，大型语言模型（LLMs）在处理包含句间依赖的文本翻译时表现出色，尤其是当它们被鼓励进行链式思维推理时。最佳模型在第一个任务中达到了约90%的准确率，在第二个任务中COMET得分为约92%。此外，观察到“智慧者更智慧”的效应，即通过推理获得的改进与没有推理的模型得分呈正相关。

Abstract: This paper assesses the capacity of large language models (LLMs) to translate
texts that include inter-sentential dependencies. We use the English-French
DiscEvalMT benchmark (Bawden et al., 2018) with pairs of sentences containing
translation challenges either for pronominal anaphora or for lexical cohesion.
We evaluate 12 LLMs from the DeepSeek-R1, GPT, Llama, Mistral and Phi families
on two tasks: (1) distinguishing a correct translation from a wrong but
plausible one; (2) generating a correct translation. We compare prompts that
encourage chain-of-thought reasoning with those that do not. The best models
take advantage of reasoning and reach about 90% accuracy on the first task, and
COMET scores of about 92% on the second task, with GPT-4, GPT-4o and Phi
standing out. Moreover, we observe a "wise get wiser" effect: the improvements
through reasoning are positively correlated with the scores of the models
without reasoning.

</details>


### [18] [Na Prática, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas e uma IA Jurídica](https://arxiv.org/abs/2510.18108)
*Marina Soares Marinho,Daniela Vianna,Livy Real,Altigran da Silva,Gabriela Migliorini*

Main category: cs.CL

TL;DR: 该研究评估了通用人工智能在法律领域的应用，发现领域专业化模型在法律任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在评估通用人工智能在法律领域的应用效果，并探索如何提高法律AI输出的可靠性。

Method: 该研究提出了一个实验评估协议，结合法律理论（如物质正确性、系统一致性、论证完整性）和48名法律专业人士的实证评估。

Result: JusIA（领域专业化模型）在模拟律师日常工作的任务中表现优于通用系统。

Conclusion: 该研究表明，领域专业化和理论基础评估对于可靠的法律AI输出至关重要。

Abstract: This study presents the Jusbrasil Study on the Use of General-Purpose AIs in
Law, proposing an experimental evaluation protocol combining legal theory, such
as material correctness, systematic coherence, and argumentative integrity,
with empirical assessment by 48 legal professionals. Four systems (JusIA,
ChatGPT Free, ChatGPT Pro, and Gemini) were tested in tasks simulating lawyers'
daily work. JusIA, a domain-specialized model, consistently outperformed the
general-purpose systems, showing that both domain specialization and a
theoretically grounded evaluation are essential for reliable legal AI outputs.

</details>


### [19] [Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment](https://arxiv.org/abs/2510.18112)
*Patricia Delafuente,Arya Honraopatil,Lara J. Martin*

Main category: cs.CL

TL;DR: 本研究评估了两种语言模型在生成Dungeons & Dragons玩家动作命令方面的表现，发现指令模型已足够完成任务。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型和推理在预测Dungeons & Dragons玩家动作并将其格式化为Avrae Discord机器人命令中的应用。

Method: 使用FIREBALL数据集评估了DeepSeek-R1-Distill-LLaMA-8B推理模型和LLaMA-3.1-8B-Instruct指令模型在命令生成方面的性能。

Result: 研究发现，向模型提供具体的指令非常重要，即使提示中的单句变化也会显著影响模型的输出，且指令模型足以完成此任务，而无需推理模型。

Conclusion: 本研究表明，指令模型在生成Dungeons & Dragons玩家动作命令方面已经足够，而无需依赖专门的推理模型。

Abstract: This paper explores the application of Large Language Models (LLMs) and
reasoning to predict Dungeons & Dragons (DnD) player actions and format them as
Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a
reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model,
LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the
importance of providing specific instructions to models, that even single
sentence changes in prompts can greatly affect the output of models, and that
instruct models are sufficient for this task compared to reasoning models.

</details>


### [20] [LLMs Encode How Difficult Problems Are](https://arxiv.org/abs/2510.18147)
*William Lugoloobi,Chris Russell*

Main category: cs.CL

TL;DR: 研究发现，人类标注的难度信号在强化学习中被放大，而模型自身生成的难度估计在模型性能提升时变得不一致。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否内部编码了与人类判断一致的问题难度，并且这种表示是否跟踪强化学习后的泛化能力。

Method: 在60个模型上训练线性探测器，评估数学和编码子集的Easy2HardBench。

Result: 人类标注的难度可以高度线性解码，而模型生成的难度较弱且扩展性差。沿着难度方向调整模型可以减少幻觉并提高准确性。

Conclusion: 人类标注的难度信号在强化学习中被放大，而模型自身生成的难度估计则在模型性能提升时变得不一致。

Abstract: Large language models exhibit a puzzling inconsistency: they solve complex
problems yet frequently fail on seemingly simpler ones. We investigate whether
LLMs internally encode problem difficulty in a way that aligns with human
judgment, and whether this representation tracks generalization during
reinforcement learning post-training. We train linear probes across layers and
token positions on 60 models, evaluating on mathematical and coding subsets of
Easy2HardBench. We find that human-labeled difficulty is strongly linearly
decodable (AMC: $\rho \approx 0.88$) and exhibits clear model-size scaling,
whereas LLM-derived difficulty is substantially weaker and scales poorly.
Steering along the difficulty direction reveals that pushing models toward
"easier" representations reduces hallucination and improves accuracy. During
GRPO training on Qwen2.5-Math-1.5B, the human-difficulty probe strengthens and
positively correlates with test accuracy across training steps, while the
LLM-difficulty probe degrades and negatively correlates with performance. These
results suggest that human annotations provide a stable difficulty signal that
RL amplifies, while automated difficulty estimates derived from model
performance become misaligned precisely as models improve. We release probe
code and evaluation scripts to facilitate replication.

</details>


### [21] [Extracting Rule-based Descriptions of Attention Features in Transformers](https://arxiv.org/abs/2510.18148)
*Dan Friedman,Adithya Bhaskar,Alexander Wettig,Danqi Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则的描述方法，用于解释模型中的特征，并展示了如何从变压器中自动提取这些规则。


<details>
  <summary>Details</summary>
Motivation: 当前的机制可解释性方法仅能识别哪些文本序列激活哪些特征，而实际的特征解释需要主观检查这些示例。本文旨在提供一种基于规则的描述方法，以更准确地解释模型行为。

Method: 本文提出了一种从变压器中自动提取规则的方法，并将其应用于GPT-2 small。

Result: 大多数特征可以用大约100个skip-gram规则很好地描述，而缺失规则在早期层中也很常见。此外，还隔离了一些计数规则的例子。

Conclusion: 本文为未来研究基于规则的特征描述奠定了基础，通过定义它们、展示如何提取它们，并提供了一些行为的初步分类法。

Abstract: Mechanistic interpretability strives to explain model behavior in terms of
bottom-up primitives. The leading paradigm is to express hidden states as a
sparse linear combination of basis vectors, called features. However, this only
identifies which text sequences (exemplars) activate which features; the actual
interpretation of features requires subjective inspection of these exemplars.
This paper advocates for a different solution: rule-based descriptions that
match token patterns in the input and correspondingly increase or decrease the
likelihood of specific output tokens. Specifically, we extract rule-based
descriptions of SAE features trained on the outputs of attention layers. While
prior work treats the attention layers as an opaque box, we describe how it may
naturally be expressed in terms of interactions between input and output
features, of which we study three types: (1) skip-gram rules of the form
"[Canadian city]... speaks --> English", (2) absence rules of the form
"[Montreal]... speaks -/-> English," and (3) counting rules that toggle only
when the count of a word exceeds a certain value or the count of another word.
Absence and counting rules are not readily discovered by inspection of
exemplars, where manual and automatic descriptions often identify misleading or
incomplete explanations. We then describe a simple approach to extract these
types of rules automatically from a transformer, and apply it to GPT-2 small.
We find that a majority of features may be described well with around 100
skip-gram rules, though absence rules are abundant even as early as the first
layer (in over a fourth of features). We also isolate a few examples of
counting rules. This paper lays the groundwork for future research into
rule-based descriptions of features by defining them, showing how they may be
extracted, and providing a preliminary taxonomy of some of the behaviors they
represent.

</details>


### [22] [Automatic Prompt Generation via Adaptive Selection of Prompting Techniques](https://arxiv.org/abs/2510.18162)
*Yohei Ikenoue,Hitomi Tashiro,Shigeru Kuroyanagi*

Main category: cs.CL

TL;DR: 本文提出了一种自适应选择任务适当提示技术并自动生成高质量提示的方法，无需依赖预定义模板或框架。通过构建一个将任务集群与相应提示技术相关联的知识库，该方法在多个任务上表现出色，为非专家使用大型语言模型提供了便利。


<details>
  <summary>Details</summary>
Motivation: 提示工程对于实现大型语言模型（LLMs）的可靠和有效输出至关重要，但其设计需要专门的提示技术知识和对目标任务的深入理解。

Method: 提出了一种新颖的方法，根据用户的抽象任务描述自适应地选择适当的提示技术，并自动生成高质量的提示，而无需依赖预定义的模板或框架。该方法构建了一个知识库，将由语义相似性表征的任务集群与其相应的提示技术相关联。当用户提供任务描述时，系统会将其分配到最相关的任务集群，并通过从知识库中整合技术动态生成提示。

Result: 在BIG-Bench Extra Hard (BBEH) 的23个任务上的实验评估表明，所提出的方法在算术和调和平均分数上均优于标准提示和现有的自动提示生成工具。

Conclusion: 这项研究为简化和标准化提示创建奠定了基础，使非专家能够有效地利用LLMs。

Abstract: Prompt engineering is crucial for achieving reliable and effective outputs
from large language models (LLMs), but its design requires specialized
knowledge of prompting techniques and a deep understanding of target tasks. To
address this challenge, we propose a novel method that adaptively selects
task-appropriate prompting techniques based on users' abstract task
descriptions and automatically generates high-quality prompts without relying
on pre-existing templates or frameworks. The proposed method constructs a
knowledge base that associates task clusters, characterized by semantic
similarity across diverse tasks, with their corresponding prompting techniques.
When users input task descriptions, the system assigns them to the most
relevant task cluster and dynamically generates prompts by integrating
techniques drawn from the knowledge base. An experimental evaluation of the
proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates
superior performance compared with standard prompts and existing automatic
prompt-generation tools, as measured by both arithmetic and harmonic mean
scores. This research establishes a foundation for streamlining and
standardizing prompt creation, enabling non-experts to effectively leverage
LLMs.

</details>


### [23] [CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models](https://arxiv.org/abs/2510.18173)
*Ritam Upadhyay,Naman Ahuja,Rishabh Baral,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出了一个用于测试LLM在动态文本到表格生成任务中鲁棒性的基准CMT-Bench，并发现当前LLM在此任务中表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的文本到表格系统依赖于大量的提示工程或迭代事件提取，这虽然提高了分数，但计算成本高且难以理解模型如何处理随时间演变的叙述来总结关键信息。

Method: 提出CMT-Bench基准，通过三个保持语义的维度测试模型的鲁棒性：(i) 提取线索消融，(ii) 时间前缀测试，(iii) 实体形式扰动。

Result: 在各种最先进的LLM上，发现没有提取摘要时有显著下降，输入长度增加导致性能单调下降，实体形式变化时准确率持续下降。分布测试确认了数值误差模式的显著变化，表明推理出现了偏差而非仅仅是噪声。

Conclusion: 当前的LLM在动态文本到表格生成中表现脆弱，需要以稳健性为优先进行评估，以促进高效和可扩展方法的发展。

Abstract: LLM Driven text-to-table (T2T) systems often rely on extensive
prompt-engineering or iterative event extraction in code-parsable formats,
which boosts scores but are computationally expensive and obscure how models
actually reason over temporal evolving narratives to summarise key information.
We present CMT-Bench, a diagnostic benchmark built from live cricket commentary
that requires dynamic table generation across two evolving schemas under a
dense, rule-governed policy. CMT-Bench is designed to probe robustness via
three semantics-preserving dimensions: (i) extractive-cue ablation to separate
extractive shortcuts from state tracking, (ii) temporal prefixing to test
long-context stability, and (iii) entity-form perturbations (anonymization,
outof-distribution substitutions, role-entangling paraphrases) to assess
sensitivity to surface variation. Across diverse long-context stateof-the-art
LLMs, we find large drops without extractive summaries, monotonic degradation
with input length, and consistent accuracy drop under entity-form changes.
Complementary distributional tests confirm significant shifts in numeric error
patterns, indicating drift in reasoning rather than mere noise. Our results
show that current LLMs are brittle in dynamic Textto-table generation,
motivating robustness-first evaluation as a prerequisite for developing
efficient and scalable approaches for this task.

</details>


### [24] [Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge](https://arxiv.org/abs/2510.18196)
*Yoshinari Fujinuma*

Main category: cs.CL

TL;DR: 研究发现LLM作为评判者存在评分范围偏差，通过对比解码方法减轻了这种偏差，提高了与人类判断的相关性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为评判者在直接评估中的可靠性问题，特别是评分范围偏差的影响。

Method: 使用对比解码方法来减轻LLM评判者在评分范围上的偏差。

Result: 通过对比解码方法，实现了与人类判断的斯皮尔曼相关性平均提高了11.3%。

Conclusion: 通过对比解码方法，我们减轻了LLM评判者的偏差，实现了与人类判断的斯皮尔曼相关性平均提高了11.3%。

Abstract: Large Language Models (LLMs) are commonly used as evaluators in various
applications, but the reliability of the outcomes remains a challenge. One such
challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores
from a specified range without any references. We first show that this
challenge stems from LLM judge outputs being associated with score range bias,
i.e., LLM judge outputs are highly sensitive to pre-defined score ranges,
preventing the search for optimal score ranges. We also show that similar
biases exist among models from the same family. We then mitigate this bias
through contrastive decoding, achieving up to 11.3% relative improvement on
average in Spearman correlation with human judgments across different score
ranges.

</details>


### [25] [MARCUS: An Event-Centric NLP Pipeline that generates Character Arcs from Narratives](https://arxiv.org/abs/2510.18201)
*Sriharsh Bhyravajjula,Ujwal Narayan,Manish Shrivastava*

Main category: cs.CL

TL;DR: 本文介绍了一种名为MARCUS的NLP流水线，用于计算生成基于事件和关系的字符弧线，从而为文学研究提供定量分析方法。


<details>
  <summary>Details</summary>
Motivation: 字符弧线是文学研究中的重要理论工具，但目前缺乏计算方法。本文旨在解决这一问题，提供一种定量表示方法。

Method: MARCUS通过提取事件、参与角色、隐含情感和情感来建模人物间的关系，并跟踪和汇总这些关系以生成字符弧线。

Result: MARCUS被应用于《哈利·波特》和《指环王》等长篇奇幻系列，生成了字符弧线，并评估了其方法的有效性。

Conclusion: 本文提出了MARCUS，这是一个用于生成基于事件和关系的字符弧线的NLP流水线。该方法为理解故事提供了定量表示，并为后续应用铺平了道路。

Abstract: Character arcs are important theoretical devices employed in literary studies
to understand character journeys, identify tropes across literary genres, and
establish similarities between narratives. This work addresses the novel task
of computationally generating event-centric, relation-based character arcs from
narratives. Providing a quantitative representation for arcs brings tangibility
to a theoretical concept and paves the way for subsequent applications. We
present MARCUS (Modelling Arcs for Understanding Stories), an NLP pipeline that
extracts events, participant characters, implied emotion, and sentiment to
model inter-character relations. MARCUS tracks and aggregates these relations
across the narrative to generate character arcs as graphical plots. We generate
character arcs from two extended fantasy series, Harry Potter and Lord of the
Rings. We evaluate our approach before outlining existing challenges,
suggesting applications of our pipeline, and discussing future work.

</details>


### [26] [DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization](https://arxiv.org/abs/2510.18257)
*Tao Tao,Guanghui Zhu,Lang Guo,Hongyi Chen,Chunfeng Yuan,Yihua Huang*

Main category: cs.CL

TL;DR: DelvePO is a new framework for optimizing prompts in a self-evolve manner, which decouples prompts into different components and introduces working memory to improve performance and transferability across tasks.


<details>
  <summary>Details</summary>
Motivation: Current works on prompt optimization mainly rely on the random rewriting ability of LLMs and focus on specific influencing factors, which leads to local optimum and unstable performance. This limits the transferability of optimized prompts across different tasks.

Method: DelvePO is a task-agnostic framework that optimizes prompts in a self-evolve manner by decoupling prompts into different components and introducing working memory to alleviate the deficiencies caused by LLMs' uncertainties.

Result: Extensive experiments conducted on different tasks covering various domains for both open- and closed-source LLMs show that DelvePO consistently outperforms previous SOTA methods, demonstrating its effectiveness and transferability.

Conclusion: DelvePO consistently outperforms previous SOTA methods under identical experimental settings, demonstrating its effectiveness and transferability across different tasks.

Abstract: Prompt Optimization has emerged as a crucial approach due to its capabilities
in steering Large Language Models to solve various tasks. However, current
works mainly rely on the random rewriting ability of LLMs, and the optimization
process generally focus on specific influencing factors, which makes it easy to
fall into local optimum. Besides, the performance of the optimized prompt is
often unstable, which limits its transferability in different tasks. To address
the above challenges, we propose $\textbf{DelvePO}$
($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving
Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a
task-agnostic framework to optimize prompts in self-evolve manner. In our
framework, we decouple prompts into different components that can be used to
explore the impact that different factors may have on various tasks. On this
basis, we introduce working memory, through which LLMs can alleviate the
deficiencies caused by their own uncertainties and further obtain key insights
to guide the generation of new prompts. Extensive experiments conducted on
different tasks covering various domains for both open- and closed-source LLMs,
including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini.
Experimental results show that DelvePO consistently outperforms previous SOTA
methods under identical experimental settings, demonstrating its effectiveness
and transferability across different tasks.

</details>


### [27] [Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs](https://arxiv.org/abs/2510.18279)
*Yanhong Li,Zixuan Lan,Jiawei Zhou*

Main category: cs.CL

TL;DR: 本文研究了将文本作为图像输入以减少令牌使用量的可行性，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索通过将文本作为图像输入来压缩文本输入的可能性，以减少令牌使用量。

Method: 将长文本输入渲染为单个图像并直接提供给模型。

Result: 在两个不同的基准测试中，文本作为图像的方法实现了显著的令牌节省（通常接近一半），而不会降低任务性能。

Conclusion: 本文表明，将文本作为图像输入可以显著减少解码器令牌数量，同时保持任务性能。

Abstract: Large language models (LLMs) and their multimodal variants can now process
visual inputs, including images of text. This raises an intriguing question:
can we compress textual inputs by feeding them as images to reduce token usage
while preserving performance? In this paper, we show that visual text
representations are a practical and surprisingly effective form of input
compression for decoder LLMs. We exploit the idea of rendering long text inputs
as a single image and provide it directly to the model. This leads to
dramatically reduced number of decoder tokens required, offering a new form of
input compression. Through experiments on two distinct benchmarks RULER
(long-context retrieval) and CNN/DailyMail (document summarization) we
demonstrate that this text-as-image method yields substantial token savings
(often nearly half) without degrading task performance.

</details>


### [28] [BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks](https://arxiv.org/abs/2510.18288)
*Tianyuan Huang,Zepeng Zhu,Hangdi Xing,Zirui Shao,Zhi Yu,Chaoxiong Yang,Jiaxian He,Xiaozhong Liu,Jiajun Bu*

Main category: cs.CL

TL;DR: 本文构建了英语和中文盲文混合数据集，并提出了一种基于语法树的增强方法。通过基于知识的微调（BKFT）实现了统一的盲文翻译、公式到盲文转换和混合文本翻译，实验表明BKFT在盲文翻译场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 盲文在教育和信息可访问性中起着至关重要的作用，但盲文信息处理面临数据稀缺和混合文本上下文中的歧义问题。

Method: 我们构建了英语和中文盲文混合数据集（EBMD/CBMD），并提出了一种基于语法树的增强方法，针对盲文数据进行定制。我们研究了基于知识的微调（BKFT），通过指令调优实现统一的盲文翻译、公式到盲文转换和混合文本翻译。

Result: 实验表明，BKFT在盲文翻译场景中相比传统微调方法取得了显著的性能提升。

Conclusion: 我们的开源数据集和方法为低资源多语言盲文研究奠定了基础。

Abstract: Braille plays a vital role in education and information accessibility for
visually impaired individuals. However, Braille information processing faces
challenges such as data scarcity and ambiguities in mixed-text contexts. We
construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with
mathematical formulas to support diverse Braille domain research, and propose a
syntax tree-based augmentation method tailored for Braille data. To address the
underperformance of traditional fine-tuning methods in Braille-related tasks,
we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the
learning difficulty of Braille contextual features. BrailleLLM employs BKFT via
instruction tuning to achieve unified Braille translation, formula-to-Braille
conversion, and mixed-text translation. Experiments demonstrate that BKFT
achieves significant performance improvements over conventional fine-tuning in
Braille translation scenarios. Our open-sourced datasets and methodologies
establish a foundation for low-resource multilingual Braille research.

</details>


### [29] [Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata](https://arxiv.org/abs/2510.18289)
*Zhengqing Yuan,Yiyang Li,Weixiang Sun,Zheyuan Zhang,Kaiwen Shi,Keerthiram Murugesan,Yanfang Ye*

Main category: cs.CL

TL;DR: 本文提出了一种名为Food4All的多智能体框架，用于解决美国食品不安全问题。该框架通过整合异构数据、轻量级强化学习算法和在线反馈循环，实现了实时、上下文感知的免费食品检索，为面临食品不安全及其复合健康风险的人群提供了可扩展、公平和智能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前的食品银行和救济站检索系统存在碎片化问题，无法满足食品不安全人群的紧急需求，特别是无家可归者、成瘾者和数字文盲。因此，需要一种更有效的食品检索系统来解决这些问题。

Method: 本文介绍了Food4All框架，包括三个创新点：1) 跨官方数据库、社区平台和社会媒体的异构数据聚合；2) 一种在定制案例上训练的轻量级强化学习算法，以优化地理可达性和营养正确性；3) 一个在线反馈循环，动态适应用户需求的变化。

Result: Food4All框架能够提供实时、上下文感知的食品检索服务，通过整合多种数据源和优化算法，提高了食品获取的效率和准确性。此外，该框架还能够根据用户需求动态调整检索策略，提高系统的适应性和灵活性。

Conclusion: 本文提出了Food4All框架，这是一个多智能体系统，旨在解决美国食品不安全问题。该框架通过整合异构数据、轻量级强化学习算法和在线反馈循环，实现了实时、上下文感知的免费食品检索，为面临食品不安全及其复合健康风险的人群提供了可扩展、公平和智能的解决方案。

Abstract: Food insecurity remains a persistent public health emergency in the United
States, tightly interwoven with chronic disease, mental illness, and opioid
misuse. Yet despite the existence of thousands of food banks and pantries,
access remains fragmented: 1) current retrieval systems depend on static
directories or generic search engines, which provide incomplete and
geographically irrelevant results; 2) LLM-based chatbots offer only vague
nutritional suggestions and fail to adapt to real-world constraints such as
time, mobility, and transportation; and 3) existing food recommendation systems
optimize for culinary diversity but overlook survival-critical needs of
food-insecure populations, including immediate proximity, verified
availability, and contextual barriers. These limitations risk leaving the most
vulnerable individuals, those experiencing homelessness, addiction, or digital
illiteracy, unable to access urgently needed resources. To address this, we
introduce Food4All, the first multi-agent framework explicitly designed for
real-time, context-aware free food retrieval. Food4All unifies three
innovations: 1) heterogeneous data aggregation across official databases,
community platforms, and social media to provide a continuously updated pool of
food resources; 2) a lightweight reinforcement learning algorithm trained on
curated cases to optimize for both geographic accessibility and nutritional
correctness; and 3) an online feedback loop that dynamically adapts retrieval
policies to evolving user needs. By bridging information acquisition, semantic
analysis, and decision support, Food4All delivers nutritionally annotated and
guidance at the point of need. This framework establishes an urgent step toward
scalable, equitable, and intelligent systems that directly support populations
facing food insecurity and its compounding health risks.

</details>


### [30] [From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering](https://arxiv.org/abs/2510.18297)
*Lei Li,Xiao Zhou,Yingying Zhang,Xian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为MedRGAG的统一检索-生成增强框架，通过结合外部和参数化知识来提高医学问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的医学问答方法存在检索噪声或不完整以及生成幻觉或不准确信息的问题，这会影响推理并降低答案的可靠性。因此，需要一种能够结合外部和参数化知识的统一框架来解决这些问题。

Method: MedRGAG是一个统一的检索-生成增强框架，包含两个关键模块：Knowledge-Guided Context Completion (KGCC) 和 Knowledge-Aware Document Selection (KADS)。KGCC引导生成器生成补充检索揭示缺失知识的背景文档，而KADS则自适应选择最佳组合的检索和生成文档以形成简洁且全面的证据。

Result: MedRGAG在五个医学QA基准测试中取得了显著的性能提升，表明其在知识密集型推理中的有效性。

Conclusion: MedRGAG在五个医学QA基准测试中表现出色，相比MedRAG和MedGENIE分别提升了12.5%和4.5%，证明了统一检索和生成在知识密集型推理中的有效性。

Abstract: Medical question answering (QA) requires extensive access to domain-specific
knowledge. A promising direction is to enhance large language models (LLMs)
with external knowledge retrieved from medical corpora or parametric knowledge
stored in model parameters. Existing approaches typically fall into two
categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning
on externally retrieved evidence, and Generation-Augmented Generation (GAG),
which depends solely on the models internal knowledge to generate contextual
documents. However, RAG often suffers from noisy or incomplete retrieval, while
GAG is vulnerable to hallucinated or inaccurate information due to
unconstrained generation. Both issues can mislead reasoning and undermine
answer reliability. To address these challenges, we propose MedRGAG, a unified
retrieval-generation augmented framework that seamlessly integrates external
and parametric knowledge for medical QA. MedRGAG comprises two key modules:
Knowledge-Guided Context Completion (KGCC), which directs the generator to
produce background documents that complement the missing knowledge revealed by
retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively
selects an optimal combination of retrieved and generated documents to form
concise yet comprehensive evidence for answer generation. Extensive experiments
on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5%
improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the
effectiveness of unifying retrieval and generation for knowledge-intensive
reasoning. Our code and data are publicly available at
https://anonymous.4open.science/r/MedRGAG

</details>


### [31] [ECG-LLM -- training and evaluation of domain-specific large language models for electrocardiography](https://arxiv.org/abs/2510.18339)
*Lara Ahrens,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.CL

TL;DR: 该研究探讨了领域适应的开放权重大型语言模型在医疗保健中的应用，发现微调和RAG方法可以实现与专有模型相当的性能，支持隐私保护的本地部署解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究领域适应策略、评估方法和相对于通用LLM的性能，以探索其在医疗保健中的应用潜力。

Method: 通过在领域特定文献上微调开放权重模型，并实施多层评估框架，比较微调模型、检索增强生成（RAG）和Claude Sonnet 3.7作为通用模型的表现。

Result: 微调的Llama 3.1 70B在多项选择评估和自动文本指标上表现优异，在LLM-as-a-judge评估中排名第二；人类专家评估更倾向于Claude 3.7和RAG方法。微调模型在几乎所有评估模式中都显著优于基础模型。

Conclusion: 领域适应的开放权重大型语言模型（LLM）在医疗保健应用中展现出巨大潜力，通过微调和检索增强生成（RAG）方法可以获得与专有模型相当的性能，支持隐私保护且可本地部署的临床解决方案。

Abstract: Domain-adapted open-weight large language models (LLMs) offer promising
healthcare applications, from queryable knowledge bases to multimodal
assistants, with the crucial advantage of local deployment for privacy
preservation. However, optimal adaptation strategies, evaluation methodologies,
and performance relative to general-purpose LLMs remain poorly characterized.
We investigated these questions in electrocardiography, an important area of
cardiovascular medicine, by finetuning open-weight models on domain-specific
literature and implementing a multi-layered evaluation framework comparing
finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7
as a representative general-purpose model. Finetuned Llama 3.1 70B achieved
superior performance on multiple-choice evaluations and automatic text metrics,
ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert
evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned
models significantly outperformed their base counterparts across nearly all
evaluation modes. Our findings reveal substantial performance heterogeneity
across evaluation methodologies, underscoring assessment complexity.
Nevertheless, domain-specific adaptation through finetuning and RAG achieves
competitive performance with proprietary models, supporting the viability of
privacy-preserving, locally deployable clinical solutions.

</details>


### [32] [Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction](https://arxiv.org/abs/2510.18344)
*Vipul Rathore,Malik Hammad Faisal,Parag Singla,Mausam*

Main category: cs.CL

TL;DR: HYDRE is a hybrid framework that improves DSRE by combining a trained model with dynamic exemplar retrieval for better performance in both English and low-resource languages.


<details>
  <summary>Details</summary>
Motivation: The challenge of learning relation semantics correctly from noisy annotations in DSRE, and the underexplored integration of LLMs with task-specific training.

Method: HYDRE is a hybrid framework that combines a trained DSRE model with a dynamic exemplar retrieval strategy to extract reliable sentence-level exemplars for LLM prompting.

Result: HYDRE achieves up to 20 F1 point gains in English and, on average, 17 F1 points on Indic languages over prior SoTA DSRE models.

Conclusion: HYDRE achieves significant improvements in F1 scores over prior SoTA DSRE models, demonstrating its effectiveness in both English and low-resource Indic languages.

Abstract: Distantly Supervised Relation Extraction (DSRE) remains a long-standing
challenge in NLP, where models must learn from noisy bag-level annotations
while making sentence-level predictions. While existing state-of-the-art (SoTA)
DSRE models rely on task-specific training, their integration with in-context
learning (ICL) using large language models (LLMs) remains underexplored. A key
challenge is that the LLM may not learn relation semantics correctly, due to
noisy annotation.
  In response, we propose HYDRE -- HYbrid Distantly Supervised Relation
Extraction framework. It first uses a trained DSRE model to identify the top-k
candidate relations for a given test sentence, then uses a novel dynamic
exemplar retrieval strategy that extracts reliable, sentence-level exemplars
from training data, which are then provided in LLM prompt for outputting the
final relation(s).
  We further extend HYDRE to cross-lingual settings for RE in low-resource
languages. Using available English DSRE training data, we evaluate all methods
on English as well as a newly curated benchmark covering four diverse
low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE
achieves up to 20 F1 point gains in English and, on average, 17 F1 points on
Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's
efficacy compared to other prompting strategies.

</details>


### [33] [KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers](https://arxiv.org/abs/2510.18355)
*Mohd Ruhul Ameen,Akif Islam,Farjana Aktar,M. Saifuzzaman Rafat*

Main category: cs.CL

TL;DR: 本文介绍了KrishokBondhu，这是一个基于检索增强生成（RAG）框架的语音启用、电话中心集成的咨询服务系统，专为孟加拉语农民设计。该系统通过电话接口提供实时、上下文感知的建议，评估结果显示其在农业咨询方面具有很高的质量。


<details>
  <summary>Details</summary>
Motivation: Many farmers in Bangladesh continue to face challenges in accessing timely, expert-level agricultural guidance.

Method: The system aggregates authoritative agricultural handbooks, extension manuals, and NGO publications; applies Optical Character Recognition (OCR) and document-parsing pipelines to digitize and structure the content; and indexes this corpus in a vector database for efficient semantic retrieval. Through a simple phone-based interface, farmers can call the system to receive real-time, context-aware advice: speech-to-text converts the Bengali query, the RAG module retrieves relevant content, a large language model (Gemma 3-4B) generates a context-grounded response, and text-to-speech delivers the answer in natural spoken Bengali.

Result: In a pilot evaluation, KrishokBondhu produced high-quality responses for 72.7% of diverse agricultural queries covering crop management, disease control, and cultivation practices. Compared to the KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on a 5-point scale, a 44.7% improvement, with especially large gains in contextual richness (+367%) and completeness (+100.4%), while maintaining comparable relevance and technical specificity. Semantic similarity analysis further revealed a strong correlation between retrieved context and answer quality, emphasizing the importance of grounding generative responses in curated documentation.

Conclusion: KrishokBondhu demonstrates the feasibility of integrating call-centre accessibility, multilingual voice interaction, and modern RAG techniques to deliver expert-level agricultural guidance to remote Bangladeshi farmers, paving the way toward a fully AI-driven agricultural advisory ecosystem.

Abstract: In Bangladesh, many farmers continue to face challenges in accessing timely,
expert-level agricultural guidance. This paper presents KrishokBondhu, a
voice-enabled, call-centre-integrated advisory platform built on a
Retrieval-Augmented Generation (RAG) framework, designed specifically for
Bengali-speaking farmers. The system aggregates authoritative agricultural
handbooks, extension manuals, and NGO publications; applies Optical Character
Recognition (OCR) and document-parsing pipelines to digitize and structure the
content; and indexes this corpus in a vector database for efficient semantic
retrieval. Through a simple phone-based interface, farmers can call the system
to receive real-time, context-aware advice: speech-to-text converts the Bengali
query, the RAG module retrieves relevant content, a large language model (Gemma
3-4B) generates a context-grounded response, and text-to-speech delivers the
answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced
high-quality responses for 72.7% of diverse agricultural queries covering crop
management, disease control, and cultivation practices. Compared to the
KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on
a 5-point scale, a 44.7% improvement, with especially large gains in contextual
richness (+367%) and completeness (+100.4%), while maintaining comparable
relevance and technical specificity. Semantic similarity analysis further
revealed a strong correlation between retrieved context and answer quality,
emphasizing the importance of grounding generative responses in curated
documentation. KrishokBondhu demonstrates the feasibility of integrating
call-centre accessibility, multilingual voice interaction, and modern RAG
techniques to deliver expert-level agricultural guidance to remote Bangladeshi
farmers, paving the way toward a fully AI-driven agricultural advisory
ecosystem.

</details>


### [34] [KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs](https://arxiv.org/abs/2510.18368)
*Donghyeon Ko,Yeguk Jin,Kyubyung Chae,Byungwook Lee,Chansong Jo,Sookyo In,Jaehong Lee,Taesup Kim,Donghyun Kwak*

Main category: cs.CL

TL;DR: KoSimpleQA是一个针对韩国文化知识的事实性评估基准测试，包含1000个简短的、寻求事实的问题。评估结果显示，即使是强大的模型也仅能正确回答33.7%的问题，表明该基准测试具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型语言模型在韩国文化知识方面的事实性，需要一个具有挑战性且易于评分的基准测试。

Method: KoSimpleQA由1000个简短的、寻求事实的问题组成，这些问题有明确的答案。对一系列支持韩语的开源大型语言模型进行了全面评估。

Result: 即使最强的模型也只能在33.7%的时间内生成正确答案，这表明KoSimpleQA具有挑战性。KoSimpleQA上的性能排名与英语SimpleQA上的排名有很大不同，突显了我们数据集的独特价值。

Conclusion: KoSimpleQA是一个具有挑战性的基准测试，用于评估大型语言模型在韩国文化知识方面的事实性。

Abstract: We present $\textbf{Korean SimpleQA (KoSimpleQA)}$, a benchmark for
evaluating factuality in large language models (LLMs) with a focus on Korean
cultural knowledge. KoSimpleQA is designed to be challenging yet easy to grade,
consisting of 1,000 short, fact-seeking questions with unambiguous answers. We
conduct a comprehensive evaluation across a diverse set of open-source LLMs of
varying sizes that support Korean, and find that even the strongest model
generates correct answer only 33.7% of the time, underscoring the challenging
nature of KoSimpleQA. Notably, performance rankings on KoSimpleQA differ
substantially from those on the English SimpleQA, highlighting the unique value
of our dataset. Furthermore, our analysis of reasoning LLMs shows that engaging
reasoning capabilities in the factual QA task can both help models better
elicit their latent knowledge and improve their ability to abstain when
uncertain. KoSimpleQA can be found at
https://anonymous.4open.science/r/KoSimpleQA-62EB.

</details>


### [35] [Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning](https://arxiv.org/abs/2510.18374)
*Monorama Swain,Bubai Maji,Jagabandhu Mishra,Markus Schedl,Anders Søgaard,Jesper Rindom Jensen*

Main category: cs.CL

TL;DR: 本文研究了如何提高英语自动语音识别系统对第二语言使用者的公平性，提出了一种结合多种公平驱动目标的微调方法，并取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 我们旨在解决为第二语言使用者构建公平的英语自动语音识别（ASR）系统的挑战。

Method: 我们提出了公平提示微调，使用轻量级适配器，结合了谱解耦（SD）、组分布鲁棒优化（Group-DRO）和不变风险最小化（IRM）。

Result: 我们的方法在宏平均词错误率方面，相对于大型预训练的Whisper和Seamless-M4T分别实现了58.7%和58.5%的相对改进，相对于使用标准经验风险最小化和交叉熵损失进行微调的方法，分别实现了9.7%和7.8%的改进。

Conclusion: 我们的方法在保持整体识别准确性的前提下，通过结合传统的经验风险最小化（ERM）与交叉熵以及公平驱动的目标（SD、Group DRO 和 IRM），提高了不同口音组之间的公平性。

Abstract: In this work, we address the challenge of building fair English ASR systems
for second-language speakers. Our analysis of widely used ASR models, Whisper
and Seamless-M4T, reveals large fluctuations in word error rate (WER) across 26
accent groups, indicating significant fairness gaps. To mitigate this, we
propose fairness-prompted finetuning with lightweight adapters, incorporating
Spectral Decoupling (SD), Group Distributionally Robust Optimization
(Group-DRO), and Invariant Risk Minimization (IRM). Our proposed fusion of
traditional empirical risk minimization (ERM) with cross-entropy and
fairness-driven objectives (SD, Group DRO, and IRM) enhances fairness across
accent groups while maintaining overall recognition accuracy. In terms of
macro-averaged word error rate, our approach achieves a relative improvement of
58.7% and 58.5% over the large pretrained Whisper and SeamlessM4T, and 9.7% and
7.8% over them, finetuning with standard empirical risk minimization with
cross-entropy loss.

</details>


### [36] [MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models](https://arxiv.org/abs/2510.18383)
*ChangSu Choi,Hoyun Song,Dongyeon Kim,WooHyeon Jung,Minkyung Cho,Sunjin Park,NohHyeob Bae,Seona Yu,KyungTae Lim*

Main category: cs.CL

TL;DR: MENTOR是一种结合强化学习和教师引导蒸馏的框架，旨在提高小型语言模型的跨领域泛化和战略能力。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调（SFT）方法在泛化能力上表现不佳，而标准的强化学习（RL）由于奖励稀疏性难以有效指导SLMs。

Method: MENTOR结合了强化学习（RL）与教师引导的蒸馏，通过探索学习更通用的策略，并利用教师的参考轨迹构建密集的复合教师引导奖励。

Result: 实验表明，MENTOR在跨领域泛化和战略能力方面优于SFT和标准稀疏奖励RL基线。

Conclusion: MENTOR显著提高了SLMs在跨领域泛化和战略能力方面的能力，优于SFT和标准稀疏奖励RL基线。

Abstract: Distilling the tool-using capabilities of large language models (LLMs) into
smaller, more efficient small language models (SLMs) is a key challenge for
their practical application. The predominant approach, supervised fine-tuning
(SFT), suffers from poor generalization as it trains models to imitate a static
set of teacher trajectories rather than learn a robust methodology. While
reinforcement learning (RL) offers an alternative, the standard RL using sparse
rewards fails to effectively guide SLMs, causing them to struggle with
inefficient exploration and adopt suboptimal strategies. To address these
distinct challenges, we propose MENTOR, a framework that synergistically
combines RL with teacher-guided distillation. Instead of simple imitation,
MENTOR employs an RL-based process to learn a more generalizable policy through
exploration. In addition, to solve the problem of reward sparsity, it uses a
teacher's reference trajectory to construct a dense, composite teacher-guided
reward that provides fine-grained guidance. Extensive experiments demonstrate
that MENTOR significantly improves the cross-domain generalization and
strategic competence of SLMs compared to both SFT and standard sparse-reward RL
baselines.

</details>


### [37] [Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2510.18413)
*Siyuan Yan,Guo-Qing Jiang,Yuchen Zhang,Xiaoxing Ma,Ran Zhu,Chun Cao,Jingwei Xu*

Main category: cs.CL

TL;DR: Adamas is a lightweight and accurate sparse attention mechanism that significantly improves the efficiency of long-context inference in large language models.


<details>
  <summary>Details</summary>
Motivation: The extended context windows in large language models lead to quadratic costs in self-attention, causing severe latency in autoregressive decoding. Existing sparse attention methods struggle with recalling critical key-value pairs, resulting in accuracy degradation.

Method: Adamas uses the Hadamard transform, bucketization, and 2-bit compression to produce compact representations, and leverages Manhattan-distance estimation for efficient top-k selections.

Result: Adamas matches the accuracy of full attention with only a 64-token budget, achieves near-lossless performance at 128, and supports up to 8x higher sparsity than prior state-of-the-art methods while delivering up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.

Conclusion: Adamas is a highly effective sparse attention mechanism that maintains accuracy under aggressive sparsity, achieving significant speedups while matching the performance of full attention.

Abstract: Large language models (LLMs) now support context windows of hundreds of
thousands to millions of tokens, enabling applications such as long-document
summarization, large-scale code synthesis, multi-document question answering
and persistent multi-turn dialogue. However, such extended contexts exacerbate
the quadratic cost of self-attention, leading to severe latency in
autoregressive decoding. Existing sparse attention methods alleviate these
costs but rely on heuristic patterns that struggle to recall critical key-value
(KV) pairs for each query, resulting in accuracy degradation. We introduce
Adamas, a lightweight yet highly accurate sparse attention mechanism designed
for long-context inference. Adamas applies the Hadamard transform,
bucketization and 2-bit compression to produce compact representations, and
leverages Manhattan-distance estimation for efficient top-k selections.
Experiments show that Adamas matches the accuracy of full attention with only a
64-token budget, achieves near-lossless performance at 128, and supports up to
8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering
up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.
Remarkably, Adamas attains comparable or even lower perplexity than full
attention, underscoring its effectiveness in maintaining accuracy under
aggressive sparsity.

</details>


### [38] [Chain-of-Conceptual-Thought: Eliciting the Agent to Deeply Think within the Response](https://arxiv.org/abs/2510.18434)
*Qingqing Gu,Dan Wang,Yue Zhao,Xiaoyu Wang,Zhonglin Jiang,Yong Chen,Hongyan Li,Luo Ji*

Main category: cs.CL

TL;DR: 提出了一种新的基于提示的范式CoCT，用于改善LLM在开放域任务中的表现，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought (CoT)在开放域任务中由于缺乏明确推理步骤或逻辑转换而性能受限的问题。

Method: 提出了一种称为Chain of Conceptual Thought (CoCT)的基于提示的范式，其中LLM首先标记一个概念，然后生成详细内容。

Result: CoCT在日常和情感支持对话中表现出色，超越了Self-Refine、ECoT、ToT、SoT和RAG等基线模型。

Conclusion: CoCT是一种潜在的有效提示范式，适用于更广泛的任务。

Abstract: Chain-of-Thought (CoT) is widely applied to improve the LLM capability in
math, coding and reasoning tasks. However, its performance is limited for
open-domain tasks since there are no clearly defined reasoning steps or logical
transitions. To mitigate such challenges, we propose another prompt-based
paradigm called Chain of Conceptual Thought (CoCT), where the LLM first tags a
concept, then generates the detailed content. The chain of concepts is allowed
within the utterance, encouraging the LLM's deep and strategic thinking. We
experiment with this paradigm in daily and emotional support conversations
where the concept is comprised of emotions, strategies and topics. Automatic,
human and model evaluations suggest that CoCT surpasses baselines such as
Self-Refine, ECoT, ToT, SoT and RAG, suggesting a potential effective
prompt-based paradigm of LLM for a wider scope of tasks.

</details>


### [39] [Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation](https://arxiv.org/abs/2510.18439)
*Yasser Hamidullah,Koel Dutta Chowdury,Yusser Al-Ghussin,Shakib Yazdani,Cennet Oguz,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hallucination, where models generate fluent text unsupported by visual
evidence, remains a major flaw in vision-language models and is particularly
critical in sign language translation (SLT). In SLT, meaning depends on precise
grounding in video, and gloss-free models are especially vulnerable because
they map continuous signer movements directly into natural language without
intermediate gloss supervision that serves as alignment. We argue that
hallucinations arise when models rely on language priors rather than visual
input. To capture this, we propose a token-level reliability measure that
quantifies how much the decoder uses visual information. Our method combines
feature-based sensitivity, which measures internal changes when video is
masked, with counterfactual signals, which capture probability differences
between clean and altered video inputs. These signals are aggregated into a
sentence-level reliability score, providing a compact and interpretable measure
of visual grounding. We evaluate the proposed measure on two SLT benchmarks
(PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our
results show that reliability predicts hallucination rates, generalizes across
datasets and architectures, and decreases under visual degradations. Beyond
these quantitative trends, we also find that reliability distinguishes grounded
tokens from guessed ones, allowing risk estimation without references; when
combined with text-based signals (confidence, perplexity, or entropy), it
further improves hallucination risk estimation. Qualitative analysis highlights
why gloss-free models are more susceptible to hallucinations. Taken together,
our findings establish reliability as a practical and reusable tool for
diagnosing hallucinations in SLT, and lay the groundwork for more robust
hallucination detection in multimodal generation.

</details>


### [40] [Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models](https://arxiv.org/abs/2510.18454)
*Atharvan Dogra,Soumya Suvra Ghosal,Ameet Deshpande,Ashwin Kalyan,Dinesh Manocha*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在幽默生成中可能产生有害内容，并且这些内容在幽默评分上表现更好，这表明存在偏差放大循环。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于创意写作和吸引人的内容，这引发了对其输出的安全担忧。因此，将幽默生成作为测试平台，评估幽默优化如何与有害内容相关联。

Method: 该研究将幽默生成作为测试平台，评估现代LLM流水线中的幽默优化如何与有害内容相关联，同时测量幽默、刻板印象和毒性。还通过信息论指标分析了不一致信号。

Result: 在六个模型中，有害输出获得了更高的幽默分数，并且在角色提示下进一步增加。信息论分析显示，有害线索扩大了预测不确定性，并且可能使某些模型的有害笑话更可预测。外部验证显示，LLM讽刺增加了刻板印象和通常的毒性。

Conclusion: 研究发现，有害内容在幽默评分上表现更好，并且在角色提示下进一步增加，表明生成器和评估器之间存在偏差放大循环。此外，有害内容可能使某些模型的幽默结尾更可预测，这表明有害内容可能被嵌入到学习到的幽默分布中。

Abstract: Large language models are increasingly used for creative writing and
engagement content, raising safety concerns about the outputs. Therefore,
casting humor generation as a testbed, this work evaluates how funniness
optimization in modern LLM pipelines couples with harmful content by jointly
measuring humor, stereotypicality, and toxicity. This is further supplemented
by analyzing incongruity signals through information-theoretic metrics. Across
six models, we observe that harmful outputs receive higher humor scores which
further increase under role-based prompting, indicating a bias amplification
loop between generators and evaluators. Information-theoretic analyses show
harmful cues widen predictive uncertainty and surprisingly, can even make
harmful punchlines more expected for some models, suggesting structural
embedding in learned humor distributions. External validation on an additional
satire-generation task with human perceived funniness judgments shows that LLM
satire increases stereotypicality and typically toxicity, including for closed
models. Quantitatively, stereotypical/toxic jokes gain $10-21\%$ in mean humor
score, stereotypical jokes appear $11\%$ to $28\%$ more often among the jokes
marked funny by LLM-based metric and up to $10\%$ more often in generations
perceived as funny by humans.

</details>


### [41] [ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks](https://arxiv.org/abs/2510.18455)
*Liyang He,Yuren Zhang,Ziwei Zhu,Zhenghui Li,Shiwei Tong*

Main category: cs.CL

TL;DR: ChronoPlay是一个用于自动化和持续生成游戏RAG基准的新框架，通过双动态更新机制和双源合成引擎来应对动态领域中的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏专门的基准测试，动态领域如在线游戏中的检索增强生成（RAG）系统评估受到阻碍。双动态问题（游戏内容更新与玩家社区关注点的变化之间的相互作用）以及需要自动化基准测试以确保生成问题的真实性是主要挑战。

Method: ChronoPlay利用双动态更新机制跟踪游戏内容和玩家社区的变化，并使用双源合成引擎从官方来源和玩家社区中提取数据，以确保事实正确性和真实查询模式。

Result: ChronoPlay框架被实例化在三个不同的游戏中，创建了首个针对游戏领域的动态RAG基准，为模型在复杂和现实条件下的性能提供了新的见解。

Conclusion: ChronoPlay框架为动态RAG基准的自动化和持续生成提供了创新解决方案，并在三个不同的游戏中进行了实例化，为模型在复杂和现实条件下的性能提供了新的见解。

Abstract: Retrieval Augmented Generation (RAG) systems are increasingly vital in
dynamic domains like online gaming, yet the lack of a dedicated benchmark has
impeded standardized evaluation in this area. The core difficulty lies in Dual
Dynamics: the constant interplay between game content updates and the shifting
focus of the player community. Furthermore, the necessity of automating such a
benchmark introduces a critical requirement for player-centric authenticity to
ensure generated questions are realistic. To address this integrated challenge,
we introduce ChronoPlay, a novel framework for the automated and continuous
generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update
mechanism to track both forms of change, and a dual-source synthesis engine
that draws from official sources and player community to ensure both factual
correctness and authentic query patterns. We instantiate our framework on three
distinct games to create the first dynamic RAG benchmark for the gaming domain,
offering new insights into model performance under these complex and realistic
conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.

</details>


### [42] [DePass: Unified Feature Attributing by Simple Decomposed Forward Pass](https://arxiv.org/abs/2510.18462)
*Xiangyu Hong,Che Jiang,Kai Tian,Biqing Qi,Youbang Sun,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: DePass 是一种无需辅助训练即可实现精细特征归因的统一框架，适用于多种解释任务。


<details>
  <summary>Details</summary>
Motivation: 当前对 Transformer 模型内部计算的解释仍然是一个核心挑战，需要一种有效的工具来分析模型的行为。

Method: DePass 通过分解隐藏状态并利用注意力分数和 MLP 的激活进行传播，实现特征归因。

Result: DePass 在多个层次的任务中验证了其有效性，展示了其在解释信息流方面的潜力。

Conclusion: DePass 是一种用于解释 Transformer 模型行为的统一框架，具有广泛的应用潜力。

Abstract: Attributing the behavior of Transformer models to internal computations is a
central challenge in mechanistic interpretability. We introduce DePass, a
unified framework for feature attribution based on a single decomposed forward
pass. DePass decomposes hidden states into customized additive components, then
propagates them with attention scores and MLP's activations fixed. It achieves
faithful, fine-grained attribution without requiring auxiliary training. We
validate DePass across token-level, model component-level, and subspace-level
attribution tasks, demonstrating its effectiveness and fidelity. Our
experiments highlight its potential to attribute information flow between
arbitrary components of a Transformer model. We hope DePass serves as a
foundational tool for broader applications in interpretability.

</details>


### [43] [CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning](https://arxiv.org/abs/2510.18466)
*Masato Kikuchi,Masatsugu Ono,Toshioki Soga,Tetsu Tanabe,Tadachika Ozono*

Main category: cs.CL

TL;DR: 本文提出了一种将WordNet与CEFR语言熟练度水平相结合的方法，并通过大型语言模型自动化了这一过程。实验表明，基于该语料库的模型性能与黄金标准注释相当，且分类器表现出高准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然WordNet是一个有价值的资源，但由于其精细的意义区分对第二语言学习者来说可能具有挑战性，因此需要一种方法来将其与语言熟练度水平结合起来。

Method: 我们使用大型语言模型自动化了这一过程，通过测量WordNet中意义定义与英语词汇资料库在线条目之间的语义相似性。

Result: 实验表明，微调我们的语料库的模型表现与在黄金标准注释上训练的模型相当。此外，通过将我们的语料库与黄金标准数据结合，我们开发了一个实用的分类器，实现了0.81的宏F1分数，表明我们的注释具有高准确性。

Conclusion: 我们的标注WordNet、语料库和分类器是公开的，有助于弥合自然语言处理和语言教育之间的差距，从而促进更有效和高效的语言学习。

Abstract: Although WordNet is a valuable resource owing to its structured semantic
networks and extensive vocabulary, its fine-grained sense distinctions can be
challenging for second-language learners. To address this, we developed a
WordNet annotated with the Common European Framework of Reference for Languages
(CEFR), integrating its semantic networks with language-proficiency levels. We
automated this process using a large language model to measure the semantic
similarity between sense definitions in WordNet and entries in the English
Vocabulary Profile Online. To validate our method, we constructed a large-scale
corpus containing both sense and CEFR-level information from our annotated
WordNet and used it to develop contextual lexical classifiers. Our experiments
demonstrate that models fine-tuned on our corpus perform comparably to those
trained on gold-standard annotations. Furthermore, by combining our corpus with
the gold-standard data, we developed a practical classifier that achieves a
Macro-F1 score of 0.81, indicating the high accuracy of our annotations. Our
annotated WordNet, corpus, and classifiers are publicly available to help
bridge the gap between natural language processing and language education,
thereby facilitating more effective and efficient language learning.

</details>


### [44] [IMB: An Italian Medical Benchmark for Question Answering](https://arxiv.org/abs/2510.18468)
*Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 本文介绍了两个全面的意大利医学基准IMB-QA和IMB-MCQA，并展示了如何利用大型语言模型提高医疗论坛数据的清晰度和一致性。实验结果表明，专门的适应策略在医疗问答任务中可能优于更大的通用模型。


<details>
  <summary>Details</summary>
Motivation: 在线医疗论坛长期以来作为患者寻求专业医疗建议的重要平台，产生了大量有价值的知识。然而，论坛互动的非正式性质和语言复杂性对自动问答系统构成了重大挑战，尤其是在处理非英语语言时。

Method: 我们展示了如何利用大型语言模型（LLMs）来提高医疗论坛数据的清晰度和一致性，同时保留其原始含义和对话风格，并在开放和选择题问答任务上比较了多种LLM架构。

Result: 我们的实验表明，专门的适应策略可以在医疗问答任务中超越更大、通用的模型。

Conclusion: 这些发现表明，有效的医疗AI系统可能更受益于领域专业知识和高效的信息检索，而不是模型规模的增加。

Abstract: Online medical forums have long served as vital platforms where patients seek
professional healthcare advice, generating vast amounts of valuable knowledge.
However, the informal nature and linguistic complexity of forum interactions
pose significant challenges for automated question answering systems,
especially when dealing with non-English languages. We present two
comprehensive Italian medical benchmarks: \textbf{IMB-QA}, containing 782,644
patient-doctor conversations from 77 medical categories, and \textbf{IMB-MCQA},
comprising 25,862 multiple-choice questions from medical specialty
examinations. We demonstrate how Large Language Models (LLMs) can be leveraged
to improve the clarity and consistency of medical forum data while retaining
their original meaning and conversational style, and compare a variety of LLM
architectures on both open and multiple-choice question answering tasks. Our
experiments with Retrieval Augmented Generation (RAG) and domain-specific
fine-tuning reveal that specialized adaptation strategies can outperform
larger, general-purpose models in medical question answering tasks. These
findings suggest that effective medical AI systems may benefit more from domain
expertise and efficient information retrieval than from increased model scale.
We release both datasets and evaluation frameworks in our GitHub repository to
support further research on multilingual medical question answering:
https://github.com/PRAISELab-PicusLab/IMB.

</details>


### [45] [DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP](https://arxiv.org/abs/2510.18475)
*Mariano Barone,Antonio Laudante,Giuseppe Riccio,Antonio Romano,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: DART is a structured corpus of Italian regulatory documents that can be used to improve the accuracy of LLM-based drug interaction checkers.


<details>
  <summary>Details</summary>
Motivation: The extraction of pharmacological knowledge from regulatory documents has become a key focus in biomedical natural language processing, but research in this field has predominantly relied on English-language corpora, leaving a significant gap in resources tailored to other healthcare systems.

Method: DART was built through a reproducible pipeline encompassing web-scale document retrieval, semantic segmentation of regulatory sections, and clinical summarization using a few-shot-tuned large language model with low-temperature decoding.

Result: Experimental results show that instruction-tuned LLMs can accurately infer potential interactions and their clinical implications when grounded in the structured textual fields of DART.

Conclusion: DART provides a structured corpus of Italian regulatory documents, which can be used to improve the accuracy of LLM-based drug interaction checkers.

Abstract: The extraction of pharmacological knowledge from regulatory documents has
become a key focus in biomedical natural language processing, with applications
ranging from adverse event monitoring to AI-assisted clinical decision support.
However, research in this field has predominantly relied on English-language
corpora such as DrugBank, leaving a significant gap in resources tailored to
other healthcare systems. To address this limitation, we introduce DART (Drug
Annotation from Regulatory Texts), the first structured corpus of Italian
Summaries of Product Characteristics derived from the official repository of
the Italian Medicines Agency (AIFA). The dataset was built through a
reproducible pipeline encompassing web-scale document retrieval, semantic
segmentation of regulatory sections, and clinical summarization using a
few-shot-tuned large language model with low-temperature decoding. DART
provides structured information on key pharmacological domains such as
indications, adverse drug reactions, and drug-drug interactions. To validate
its utility, we implemented an LLM-based drug interaction checker that
leverages the dataset to infer clinically meaningful interactions. Experimental
results show that instruction-tuned LLMs can accurately infer potential
interactions and their clinical implications when grounded in the structured
textual fields of DART. We publicly release our code on GitHub:
https://github.com/PRAISELab-PicusLab/DART.

</details>


### [46] [How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices](https://arxiv.org/abs/2510.18480)
*Han Peng,Peiyu Liu,Zican Dong,Daixuan Cheng,Junyi Li,Yiru Tang,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: 本研究系统地分析了DLMs的效率问题，发现AR模型在吞吐量上优于DLMs，并探讨了加速策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前开源的DLMs在速度上往往不如AR模型，限制了它们的实际应用。我们需要更好地理解DLMs的效率问题，并寻找改进的方法。

Method: 通过实证基准测试和基于屋顶线的理论分析，我们比较了AR模型和DLMs的性能。

Result: AR模型通常具有更高的吞吐量，而DLMs则持续落后。双缓存和并行解码等加速技术主要在小批量情况下有效，随着规模扩大其优势会减弱。

Conclusion: 我们的研究强调了需要更稳健的评估方法和改进的加速策略来推进DLMs的研究。

Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to
the long-dominant autoregressive (AR) paradigm, offering a parallelable
decoding process that could yield greater efficiency. Yet, in practice, current
open-source DLMs often underperform their AR counterparts in speed, limiting
their real-world utility. This work presents a systematic study of DLM
efficiency, identifying key issues in prior evaluation methods. Through
empirical benchmarking and a roofline-based theoretical analysis, we
demonstrate that AR models generally achieve higher throughput, while DLMs
consistently lag. We also investigate acceleration strategies, finding that
techniques like dual cache and parallel decoding mainly offer gains at small
batch sizes, with their benefits diminishing upon scaling. Our findings
underscore the necessity of robust evaluation methods and improved acceleration
strategies to advance research on DLMs.

</details>


### [47] [Identity-Aware Large Language Models require Cultural Reasoning](https://arxiv.org/abs/2510.18510)
*Alistair Plum,Anne-Marie Lutgen,Christoph Purschke,Achim Rettinger*

Main category: cs.CL

TL;DR: 本文探讨了文化推理的重要性，并提出将其作为基础能力，以提高语言模型对人类文化的敏感度。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在回答中往往反映出一种狭隘的文化视角，忽略了全球用户的多样性。这种能力的缺失可能导致刻板印象、忽视少数群体观点、信任度下降和仇恨的延续。因此，需要将文化推理视为基础能力。

Method: 本文讨论了文化推理的概念，并提出了评估文化推理的初步方向。

Result: 当前的评估方法主要报告静态准确率分数，无法捕捉上下文中的适应性推理。虽然更广泛的数据集可以有所帮助，但它们无法单独确保真正的文化能力。

Conclusion: 文化推理必须被视为与事实准确性和语言连贯性并列的基础能力。通过明确概念并概述其评估的初步方向，为未来系统能够对人类文化的复杂性做出更敏感的回应奠定了基础。

Abstract: Large language models have become the latest trend in natural language
processing, heavily featuring in the digital tools we use every day. However,
their replies often reflect a narrow cultural viewpoint that overlooks the
diversity of global users. This missing capability could be referred to as
cultural reasoning, which we define here as the capacity of a model to
recognise culture-specific knowledge values and social norms, and to adjust its
output so that it aligns with the expectations of individual users. Because
culture shapes interpretation, emotional resonance, and acceptable behaviour,
cultural reasoning is essential for identity-aware AI. When this capacity is
limited or absent, models can sustain stereotypes, ignore minority
perspectives, erode trust, and perpetuate hate. Recent empirical studies
strongly suggest that current models default to Western norms when judging
moral dilemmas, interpreting idioms, or offering advice, and that fine-tuning
on survey data only partly reduces this tendency. The present evaluation
methods mainly report static accuracy scores and thus fail to capture adaptive
reasoning in context. Although broader datasets can help, they cannot alone
ensure genuine cultural competence. Therefore, we argue that cultural reasoning
must be treated as a foundational capability alongside factual accuracy and
linguistic coherence. By clarifying the concept and outlining initial
directions for its assessment, a foundation is laid for future systems to be
able to respond with greater sensitivity to the complex fabric of human
culture.

</details>


### [48] [Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency](https://arxiv.org/abs/2510.18556)
*Svetlana Maslenkova,Clement Christophe,Marco AF Pimentel,Tathagata Raha,Muhammad Umar Salman,Ahmed Al Mahrooqi,Avani Gupta,Shadab Khan,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: 本文提出了一个全面的评估框架，以支持临床AI应用中的公平性和安全性，并介绍了HC4数据集，用于分析临床语言模型中的潜在偏差。


<details>
  <summary>Details</summary>
Motivation: 当前的数据集策划和偏差评估实践缺乏必要的透明度，因此需要全面的评估框架来建立信任并指导改进。

Method: 本文介绍了HC4：Healthcare Comprehensive Commons Corpus，这是一个超过890亿个标记的新颖且广泛策划的预训练数据集，并利用了现有的通用基准和一种新的医疗保健特定方法进行评估。

Result: 本文对临床语言模型中的潜在下游偏差进行了深入分析，重点关注不同人口群体（如种族、性别和年龄）在阿片类药物处方上的差异倾向，并提供了重要的见解以支持临床AI应用中的公平性和安全性。

Conclusion: 本文提出了一个全面的评估框架，以支持临床AI应用中的公平性和安全性。

Abstract: Large language models offer transformative potential for healthcare, yet
their responsible and equitable development depends critically on a deeper
understanding of how training data characteristics influence model behavior,
including the potential for bias. Current practices in dataset curation and
bias assessment often lack the necessary transparency, creating an urgent need
for comprehensive evaluation frameworks to foster trust and guide improvements.
In this study, we present an in-depth analysis of potential downstream biases
in clinical language models, with a focus on differential opioid prescription
tendencies across diverse demographic groups, such as ethnicity, gender, and
age. As part of this investigation, we introduce HC4: Healthcare Comprehensive
Commons Corpus, a novel and extensively curated pretraining dataset exceeding
89 billion tokens. Our evaluation leverages both established general benchmarks
and a novel, healthcare-specific methodology, offering crucial insights to
support fairness and safety in clinical AI applications.

</details>


### [49] [Large language models for folktale type automation based on motifs: Cinderella case study](https://arxiv.org/abs/2510.18561)
*Tjaša Arčon,Marko Robnik-Šikonja,Polona Tratnik*

Main category: cs.CL

TL;DR: 本文介绍了利用人工智能技术对灰姑娘故事变体进行大规模分析的方法，展示了大型语言模型在检测故事复杂互动方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 人工智能方法正在被适应到许多研究领域，包括数字人文。为了在民俗学中进行大规模分析，我们构建了一种方法。

Method: 使用机器学习和自然语言处理，自动检测灰姑娘变体中的主题，并用聚类和降维分析它们的相似性和差异性。

Result: 结果表明，大型语言模型能够检测故事中的复杂互动，从而实现对大量文本集合的计算分析，并促进跨语言比较。

Conclusion: 大型语言模型能够检测故事中的复杂互动，使对大量文本集合的计算分析成为可能，并促进跨语言比较。

Abstract: Artificial intelligence approaches are being adapted to many research areas,
including digital humanities. We built a methodology for large-scale analyses
in folkloristics. Using machine learning and natural language processing, we
automatically detected motifs in a large collection of Cinderella variants and
analysed their similarities and differences with clustering and dimensionality
reduction. The results show that large language models detect complex
interactions in tales, enabling computational analysis of extensive text
collections and facilitating cross-lingual comparisons.

</details>


### [50] [Beyond the Explicit: A Bilingual Dataset for Dehumanization Detection in Social Media](https://arxiv.org/abs/2510.18582)
*Dennis Assenmacher,Paloma Piot,Katarina Laken,David Jurgens,Claudia Wagner*

Main category: cs.CL

TL;DR: 本文提出了一种新的双语数据集，以更全面地研究数字去人性化问题，并展示了其在机器学习模型训练和评估中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的研究主要集中在显性负面陈述上，而忽略了更微妙的去人性化形式，这些形式虽然不明显冒犯，但仍然会加剧对边缘群体的有害偏见。

Method: 通过不同的抽样方法从Twitter和Reddit收集了一个理论驱动的双语数据集，并使用众包工作者和专家对16,000个实例进行文档级和跨度级标注。

Result: 本文的数据集覆盖了去人性化的不同维度，并且在零样本和少量样本上下文设置中，微调的机器学习模型表现优于最先进的模型。

Conclusion: 本文提出了一个理论驱动的双语数据集，用于更全面地研究数字去人性化问题，并展示了该数据集在机器学习模型训练和未来去人性化检测技术评估中的有效性。

Abstract: Digital dehumanization, although a critical issue, remains largely overlooked
within the field of computational linguistics and Natural Language Processing.
The prevailing approach in current research concentrating primarily on a single
aspect of dehumanization that identifies overtly negative statements as its
core marker. This focus, while crucial for understanding harmful online
communications, inadequately addresses the broader spectrum of dehumanization.
Specifically, it overlooks the subtler forms of dehumanization that, despite
not being overtly offensive, still perpetuate harmful biases against
marginalized groups in online interactions. These subtler forms can insidiously
reinforce negative stereotypes and biases without explicit offensiveness,
making them harder to detect yet equally damaging. Recognizing this gap, we use
different sampling methods to collect a theory-informed bilingual dataset from
Twitter and Reddit. Using crowdworkers and experts to annotate 16,000 instances
on a document- and span-level, we show that our dataset covers the different
dimensions of dehumanization. This dataset serves as both a training resource
for machine learning models and a benchmark for evaluating future
dehumanization detection techniques. To demonstrate its effectiveness, we
fine-tune ML models on this dataset, achieving performance that surpasses
state-of-the-art models in zero and few-shot in-context settings.

</details>


### [51] [Dynamical model parameters from ultrasound tongue kinematics](https://arxiv.org/abs/2510.18629)
*Sam Kirkham,Patrycja Strycharczuk*

Main category: cs.CL

TL;DR: 研究显示，超声波可以作为评估动态构音模型的有效替代方法。


<details>
  <summary>Details</summary>
Motivation: 传统的评估方法使用了如电磁构音图（EMA）这样的实心点数据，但最近的方法进展使得超声成像成为一个有前途的替代方案。

Method: 通过比较从超声波舌部运动学和同时记录的EMA数据中估计的线性谐波振荡器参数，评估这些参数是否可以从超声波数据中可靠地估计。

Result: 研究发现，超声波和EMA产生了可比较的动力学参数，而下颌短肌腱跟踪也足够准确地捕捉了下颌运动。

Conclusion: 这项研究支持使用超声波运动学来评估动态的构音模型。

Abstract: The control of speech can be modelled as a dynamical system in which
articulators are driven toward target positions. These models are typically
evaluated using fleshpoint data, such as electromagnetic articulography (EMA),
but recent methodological advances make ultrasound imaging a promising
alternative. We evaluate whether the parameters of a linear harmonic oscillator
can be reliably estimated from ultrasound tongue kinematics and compare these
with parameters estimated from simultaneously-recorded EMA data. We find that
ultrasound and EMA yield comparable dynamical parameters, while mandibular
short tendon tracking also adequately captures jaw motion. This supports using
ultrasound kinematics to evaluate dynamical articulatory models.

</details>


### [52] [MLMA: Towards Multilingual with Mamba Based Architectures](https://arxiv.org/abs/2510.18684)
*Mohamed Nabih Ali,Daniele Falavigna,Alessio Brutti*

Main category: cs.CL

TL;DR: 本文介绍了MLMA，这是一种利用Mamba架构进行多语言自动语音识别的新方法。实验表明，MLMA在标准多语言基准测试中表现与基于Transformer的架构相当。


<details>
  <summary>Details</summary>
Motivation: Multilingual automatic speech recognition (ASR) remains a challenging task, especially when balancing performance across high- and low-resource languages. Recent advances in sequence modeling suggest that architectures beyond Transformers may offer better scalability and efficiency.

Method: MLMA (Multilingual Language Modeling with Mamba for ASR) leverages the Mamba architecture for multilingual ASR.

Result: Experiments on standard multilingual benchmarks show that MLMA achieves competitive performance compared to Transformer-based architectures.

Conclusion: Mamba's potential as a strong backbone for scalable, efficient, and accurate multilingual speech recognition is highlighted.

Abstract: Multilingual automatic speech recognition (ASR) remains a challenging task,
especially when balancing performance across high- and low-resource languages.
Recent advances in sequence modeling suggest that architectures beyond
Transformers may offer better scalability and efficiency. In this work, we
introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new
approach that leverages the Mamba architecture -- an efficient state-space
model optimized for long-context sequence processing -- for multilingual ASR.
Using Mamba, MLMA implicitly incorporates language-aware conditioning and
shared representations to support robust recognition across diverse languages.
Experiments on standard multilingual benchmarks show that MLMA achieves
competitive performance compared to Transformer-based architectures. These
results highlight Mamba's potential as a strong backbone for scalable,
efficient, and accurate multilingual speech recognition.

</details>


### [53] [Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering](https://arxiv.org/abs/2510.18691)
*Feras AlMannaa,Talia Tseriotou,Jenny Chim,Maria Liakata*

Main category: cs.CL

TL;DR: 本研究首次调查了LLM在长上下文医学问答任务中的理解能力，分析了模型大小的影响、局限性和记忆问题，并探讨了RAG在提升长上下文理解方面的效果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在首次调查LLM在长上下文医学问答任务中的理解能力，探索模型大小的影响、局限性以及RAG在提升长上下文理解方面的潜力。

Method: 本研究通过全面评估不同内容包含设置、不同能力的LLM模型和跨任务形式的数据集，分析了模型大小的影响、局限性和记忆问题。同时，研究还考察了RAG在医学长上下文理解中的效果，并采用多方面的方法进行评估。

Result: 研究结果揭示了模型大小对长上下文理解的影响，发现了模型的局限性和潜在的记忆问题，并验证了RAG在提升长上下文理解方面的有效性。此外，研究还展示了RAG策略在不同数据集中的最佳应用方式。

Conclusion: 本研究揭示了在长上下文医学问答任务中，模型大小的影响、局限性、潜在的记忆问题以及推理模型的优势。此外，研究还探讨了RAG在提升长上下文理解方面的效果，并展示了在单文档和多文档推理数据集中的最佳设置。

Abstract: This study is the first to investigate LLM comprehension capabilities over
long-context (LC) medical QA of clinical relevance. Our comprehensive
assessment spans a range of content-inclusion settings based on their
relevance, LLM models of varying capabilities and datasets across task
formulations, revealing insights on model size effects, limitations, underlying
memorization issues and the benefits of reasoning models. Importantly, we
examine the effect of RAG on medical LC comprehension, uncover best settings in
single versus multi-document reasoning datasets and showcase RAG strategies for
improvements over LC. We shed light into some of the evaluation aspects using a
multi-faceted approach. Our qualitative and error analyses address open
questions on when RAG is beneficial over LC, revealing common failure cases.

</details>


### [54] [Bayesian Low-Rank Factorization for Robust Model Adaptation](https://arxiv.org/abs/2510.18723)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 本文探讨了用于语音基础模型的贝叶斯因子适配器，以在适应特定领域的同时保持通用性能。结果表明，这种方法在微调语音基础模型时是有效的，而不会牺牲泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语音基础模型在许多领域表现出色，但它们通常需要适应以处理本地需求，例如代码切换，即说话者在同一话语中混合语言。直接微调这些模型可能会过拟合到目标领域并覆盖基础模型的广泛能力。

Method: 我们探索了用于语音基础模型的贝叶斯因子适配器，这些适配器在零附近放置先验以实现更稀疏的适应矩阵，从而在适应特定领域的同时保持通用性能。

Result: 我们的结果表明，适应损失很小，同时显著减少了基础模型的灾难性遗忘。与LoRA相比，我们的方法在新领域仅有4%的下降，但实现了54%的后向增益。

Conclusion: 我们的研究结果表明，贝叶斯适应在微调语音基础模型时是有效的，而不会牺牲泛化能力。

Abstract: Large speech foundation models achieve strong performance across many
domains, but they often require adaptation to handle local needs such as
code-switching, where speakers mix languages within the same utterance. Direct
fine-tuning of these models risks overfitting to the target domain and
overwriting the broad capabilities of the base model. To address this
challenge, we explore Bayesian factorized adapters for speech foundation
models, which place priors near zero to achieve sparser adaptation matrices and
thereby retain general performance while adapting to specific domains. We apply
our approach to the Whisper model and evaluate on different multilingual
code-switching scenarios. Our results show only minimal adaptation loss while
significantly reducing catastrophic forgetting of the base model. Compared to
LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new
domain. These findings highlight the effectiveness of Bayesian adaptation for
fine-tuning speech foundation models without sacrificing generalization.

</details>


### [55] [Adapting Language Balance in Code-Switching Speech](https://arxiv.org/abs/2510.18724)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 本文提出了一种简单但有效的可微分代理方法，以减轻生成过程中的上下文偏差，从而提高模型的鲁棒性。实验表明，该方法在阿拉伯语和中英混合语言数据上能够更准确地预测切换位置，减少了替换错误。


<details>
  <summary>Details</summary>
Motivation: 尽管大型基础模型在标准基准上取得了令人印象深刻的结果，但在面对代码切换测试用例时仍然存在困难。当数据稀缺不能作为性能不佳的通常理由时，原因可能在于代码切换时刻的不频繁出现，其中第二种语言的嵌入显得微妙。因此，为训练过程提供标签可能是有益的。

Method: 本文通过利用嵌入语言和主要语言之间的差异来突出代码切换点，从而强调在这些位置的学习。这种方法是一种简单但有效的可微分代理，有助于减轻生成过程中的上下文偏差。

Result: 实验结果表明，该方法在阿拉伯语和中英混合语言数据上能够更准确地预测切换位置，减少了替换错误。

Conclusion: 本文提出了一种简单但有效的可微分代理方法，以减轻生成过程中的上下文偏差，从而提高模型的鲁棒性。实验表明，该方法在阿拉伯语和中英混合语言数据上能够更准确地预测切换位置，减少了替换错误。

Abstract: Despite achieving impressive results on standard benchmarks, large
foundational models still struggle against code-switching test cases. When data
scarcity cannot be used as the usual justification for poor performance, the
reason may lie in the infrequent occurrence of code-switched moments, where the
embedding of the second language appears subtly. Instead of expecting the
models to learn this infrequency on their own, it might be beneficial to
provide the training process with labels. Evaluating model performance on
code-switching data requires careful localization of code-switching points
where recognition errors are most consequential, so that the analysis
emphasizes mistakes occurring at those moments. Building on this observation,
we leverage the difference between the embedded and the main language to
highlight those code-switching points and thereby emphasize learning at those
locations. This simple yet effective differentiable surrogate mitigates context
bias during generation -- the central challenge in code-switching -- thereby
improving the model's robustness. Our experiments with Arabic and
Chinese-English showed that the models are able to predict the switching places
more correctly, reflected by the reduced substitution error.

</details>


### [56] [SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish](https://arxiv.org/abs/2510.18725)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文提出了一种半监督推理高效的领域适应方法，以提高低资源语言的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 由于微调大型多语言模型计算成本高，因此对低资源领域的研究者来说是一个障碍。参数高效微调（PEFT）通过训练原始模型参数的一小部分来解决这个问题。

Method: 引入了SemiAdapt和SemiLoRA作为半监督推理高效的领域适应方法，并评估了基于嵌入的推理方法在大规模和噪声数据集上的表现。

Result: SemiAdapt可以超越全领域微调，而SemiLoRA可以使PEFT方法达到甚至超过全模型微调的表现。基于嵌入的推理方法在大规模和噪声数据集上表现尤其出色。

Conclusion: 这些方法旨在使低资源语言的研究人员更容易获得高质量的领域适应和微调。

Abstract: Fine-tuning is widely used to tailor large language models for specific tasks
such as neural machine translation (NMT). However, leveraging transfer learning
is computationally expensive when fine-tuning large multilingual models with
billions of parameters, thus creating a barrier to entry for researchers
working on low-resource domains such as Irish translation. Parameter-efficient
fine-tuning (PEFT) bridges this gap by training on a fraction of the original
model parameters, with the Low-Rank Adaptation (LoRA) approach introducing
small, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as
semi-supervised inference-efficient approaches that strengthen domain
adaptation and lead to improved overall performance in NMT. We demonstrate that
SemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA
can propel PEFT methods to match or even outperform full-model fine-tuning. We
further evaluate domain-by-dataset fine-tuning and demonstrate that our
embedding-based inference methods perform especially well on larger and noisier
corpora. All Irish translation models developed in this work are released as
open resources. These methods aim to make high-quality domain adaptation and
fine-tuning more accessible to researchers working with low-resource languages.

</details>


### [57] [Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation](https://arxiv.org/abs/2510.18731)
*Ming Li*

Main category: cs.CL

TL;DR: RLAAR is a framework that helps Large Language Models avoid Lost-in-Conversation by encouraging them to judge the solvability of questions and balance problem-solving with informed abstention.


<details>
  <summary>Details</summary>
Motivation: Large Language Models suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose RLAAR.

Method: RLAAR is a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. It employs a competence-gated curriculum that incrementally increases dialogue difficulty, stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention.

Result: Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%).

Conclusion: RLAAR provides a practical recipe for building multi-turn reliable and trustworthy LLMs.

Abstract: Large Language Models demonstrate strong capabilities in single-turn
instruction following but suffer from Lost-in-Conversation (LiC), a degradation
in performance as information is revealed progressively in multi-turn settings.
Motivated by the current progress on Reinforcement Learning with Verifiable
Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable
Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not
only to generate correct answers, but also to judge the solvability of
questions in the multi-turn conversation setting. Our approach employs a
competence-gated curriculum that incrementally increases dialogue difficulty
(in terms of instruction shards), stabilizing training while promoting
reliability. Using multi-turn, on-policy rollouts and a mixed-reward system,
RLAAR teaches models to balance problem-solving with informed abstention,
reducing premature answering behaviors that cause LiC. Evaluated on LiC
benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to
75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,
these results provide a practical recipe for building multi-turn reliable and
trustworthy LLMs.

</details>


### [58] [Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting](https://arxiv.org/abs/2510.18745)
*Taha Binhuraib,Greta Tuckute,Nicholas Blauch*

Main category: cs.CL

TL;DR: This paper introduces Topoformers, a type of Transformer with topographic organization, which shows comparable performance to traditional models while providing interpretable representations.


<details>
  <summary>Details</summary>
Motivation: Spatial functional organization is a hallmark of biological brains, but representations within most machine learning models lack spatial biases. The goal is to create models that have interpretable topographic organization.

Method: We propose a novel form of self-attention that turns Transformers into 'Topoformers' with topographic organization. We introduce spatial querying and spatial reweighting.

Result: The topographic variant performs on par with a non-topographic control model on NLP benchmarks, yet produces interpretable topographic organization. There is alignment between low-dimensional topographic variability in the Topoformer model and human brain language network.

Conclusion: Scaling up Topoformers further holds promise for greater interpretability in NLP research, and for more accurate models of the organization of linguistic information in the human brain.

Abstract: Spatial functional organization is a hallmark of biological brains: neurons
are arranged topographically according to their response properties, at
multiple scales. In contrast, representations within most machine learning
models lack spatial biases, instead manifesting as disorganized vector spaces
that are difficult to visualize and interpret. Here, we propose a novel form of
self-attention that turns Transformers into "Topoformers" with topographic
organization. We introduce spatial querying - where keys and queries are
arranged on 2D grids, and local pools of queries are associated with a given
key - and spatial reweighting, where we convert the standard fully connected
layer of self-attention into a locally connected layer. We first demonstrate
the feasibility of our approach by training a 1-layer Topoformer on a sentiment
classification task. Training with spatial querying encourages topographic
organization in the queries and keys, and spatial reweighting separately
encourages topographic organization in the values and self-attention outputs.
We then apply the Topoformer motifs at scale, training a BERT architecture with
a masked language modeling objective. We find that the topographic variant
performs on par with a non-topographic control model on NLP benchmarks, yet
produces interpretable topographic organization as evaluated via eight
linguistic test suites. Finally, analyzing an fMRI dataset of human brain
responses to a large set of naturalistic sentences, we demonstrate alignment
between low-dimensional topographic variability in the Topoformer model and
human brain language network. Scaling up Topoformers further holds promise for
greater interpretability in NLP research, and for more accurate models of the
organization of linguistic information in the human brain.

</details>


### [59] [AI use in American newspapers is widespread, uneven, and rarely disclosed](https://arxiv.org/abs/2510.18774)
*Jenna Russell,Marzena Karpinska,Destiny Akinode,Katherine Thai,Bradley Emi,Max Spero,Mohit Iyyer*

Main category: cs.CL

TL;DR: 我们的研究发现，AI在新闻业中广泛使用，但很少被披露，这凸显了需要更大的透明度和更新的编辑标准。


<details>
  <summary>Details</summary>
Motivation: AI正在迅速改变新闻业，但其在已发布的报纸文章中的使用程度尚不清楚。

Method: 我们使用Pangram，一种最先进的AI检测器，对186,000篇来自1500家美国报纸在线版的文章进行了审计，并手动审核了100篇被标记为AI的文章。

Result: 大约9%的新发布文章是部分或完全由AI生成的。这种AI使用分布不均，出现在较小的本地机构、特定主题如天气和技术以及某些所有权群体中。此外，我们发现45,000篇意见文章比同一出版物的新闻文章更有可能包含AI生成的内容，许多被标记为AI的意见文章由知名公众人物撰写。

Conclusion: 我们的审计突显了在新闻业中使用AI需要更大的透明度和更新的编辑标准，以保持公众信任。

Abstract: AI is rapidly transforming journalism, but the extent of its use in published
newspaper articles remains unclear. We address this gap by auditing a
large-scale dataset of 186K articles from online editions of 1.5K American
newspapers published in the summer of 2025. Using Pangram, a state-of-the-art
AI detector, we discover that approximately 9% of newly-published articles are
either partially or fully AI-generated. This AI use is unevenly distributed,
appearing more frequently in smaller, local outlets, in specific topics such as
weather and technology, and within certain ownership groups. We also analyze
45K opinion pieces from Washington Post, New York Times, and Wall Street
Journal, finding that they are 6.4 times more likely to contain AI-generated
content than news articles from the same publications, with many AI-flagged
op-eds authored by prominent public figures. Despite this prevalence, we find
that AI use is rarely disclosed: a manual audit of 100 AI-flagged articles
found only five disclosures of AI use. Overall, our audit highlights the
immediate need for greater transparency and updated editorial standards
regarding the use of AI in journalism to maintain public trust.

</details>


### [60] [KAT-Coder Technical Report](https://arxiv.org/abs/2510.18779)
*Zizheng Zhan,Ken Deng,Xiaojiang Zhang,Jinghui Wang,Huaixi Tang,Zhiyi Lai,Haoyang Huang,Wen Xiang,Kun Wu,Wenhao Zhuang,Minglei Zhang,Shaojie Wang,Shangpeng Yan,Kepeng Lei,Zongxian Feng,Huiming Wang,Zheng Lin,Mengtong Li,Mengfei Xie,Yinghan Cui,Xuxing Chen,Chao Wang,Weihao Li,Wenqiang Zhu,Jiarong Zhang,Jingxuan Xu,Songwei Yu,Yifan Yao,Xinping Lei,Han Li,Junqi Xiong,Zuchen Gao,Dailin Li,Haimo Li,Jiaheng Liu,Yuqun Zhang,Junyi Peng,Haotian Zhang,Bin Chen*

Main category: cs.CL

TL;DR: KAT-Coder是一个大型代理代码模型，通过多阶段课程进行训练，以提高其在现实世界软件开发工作流中的表现，并已在Hugging Face上开源。


<details>
  <summary>Details</summary>
Motivation: 弥合静态文本训练与动态现实世界代理执行之间的差距仍然是一个核心挑战。

Method: KAT-Coder通过多阶段课程进行训练，包括中期训练、监督微调(SFT)、强化微调(RFT)和强化到部署的适应。

Result: KAT-Coder在工具使用可靠性、指令对齐和长上下文推理方面表现出色，形成了可部署的基础。

Conclusion: KAT-Coder能够实现强大的工具使用可靠性、指令对齐和长上下文推理，为现实世界智能编码代理提供了可部署的基础。我们的KAT系列32B模型KAT-Dev已经在Hugging Face上开源。

Abstract: Recent advances in large language models (LLMs) have enabled progress in
agentic coding, where models autonomously reason, plan, and act within
interactive software development workflows. However, bridging the gap between
static text-based training and dynamic real-world agentic execution remains a
core challenge. In this technical report, we present KAT-Coder, a large-scale
agentic code model trained through a multi-stage curriculum encompassing
Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning
(RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances
reasoning, planning, and reflection capabilities through a corpus of real
software engineering data and synthetic agentic interactions. The SFT stage
constructs a million-sample dataset balancing twenty programming languages, ten
development contexts, and ten task archetypes. The RFT stage introduces a novel
multi-ground-truth reward formulation for stable and sample-efficient policy
optimization. Finally, the Reinforcement-to-Deployment phase adapts the model
to production-grade IDE environments using Error-Masked SFT and Tree-Structured
Trajectory Training. In summary, these stages enable KAT-Coder to achieve
robust tool-use reliability, instruction alignment, and long-context reasoning,
forming a deployable foundation for real-world intelligent coding agents. Our
KAT series 32B model, KAT-Dev, has been open-sourced on
https://huggingface.co/Kwaipilot/KAT-Dev.

</details>


### [61] [WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection](https://arxiv.org/abs/2510.18798)
*Guanzhong He,Zhen Yang,Jinxin Liu,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文介绍了一种名为WebSeer的智能搜索代理，通过增强的强化学习方法和自我反思机制，实现了更长和更具反思性的工具使用轨迹，显著提高了答案准确性，并在多个数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索代理方法在工具使用深度和多次迭代交互中的错误累积方面存在局限性。因此，需要一种更智能的搜索代理来提高信息检索和决策能力。

Method: 本文构建了一个带有反思模式的大型数据集，并设计了一个两阶段的训练框架，将冷启动和强化学习统一在自我反思范式中，以实现实际网络环境中的智能搜索代理。

Result: 本文提出的WebSeer在HotpotQA和SimpleQA数据集上取得了最先进的结果，准确率分别为72.3%和90.0%，并且在分布外数据集上表现出强大的泛化能力。

Conclusion: 本文提出了一种名为WebSeer的更智能的搜索代理，通过增强的强化学习方法训练，并结合了自我反思机制。该方法在实际网络环境中实现了更长和更具反思性的工具使用轨迹，显著扩展了工具使用链并提高了答案准确性。

Abstract: Search agents have achieved significant advancements in enabling intelligent
information retrieval and decision-making within interactive environments.
Although reinforcement learning has been employed to train agentic models
capable of more dynamic interactive retrieval, existing methods are limited by
shallow tool-use depth and the accumulation of errors over multiple iterative
interactions. In this paper, we present WebSeer, a more intelligent search
agent trained via reinforcement learning enhanced with a self-reflection
mechanism. Specifically, we construct a large dataset annotated with reflection
patterns and design a two-stage training framework that unifies cold start and
reinforcement learning within the self-reflection paradigm for real-world
web-based environments, which enables the model to generate longer and more
reflective tool-use trajectories. Our approach substantially extends tool-use
chains and improves answer accuracy. Using a single 14B model, we achieve
state-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and
90.0%, respectively, and demonstrate strong generalization to
out-of-distribution datasets. The code is available at
https://github.com/99hgz/WebSeer

</details>


### [62] [Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring](https://arxiv.org/abs/2510.18817)
*Shuxin Lin,Dhaval Patel,Christodoulos Constantinides*

Main category: cs.CL

TL;DR: 本文提出了一种知识蒸馏框架，将大型语言模型的推理能力通过Chain-of-Thought (CoT)蒸馏转移到小型语言模型（SLMs），以提高其在工业应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 在工业4.0等专业领域中，使用SLMs进行复杂推理仍然具有挑战性。

Method: 本文提出了一种用于工业资产健康的知识蒸馏框架，通过Chain-of-Thought (CoT)蒸馏从大型语言模型（LLMs）转移到更小、更高效的模型（SLMs）。

Result: 实验结果表明，经过微调的SLMs在CoT推理方面显著优于基础模型，缩小了与LLM的差距。

Conclusion: 实验结果表明，经过微调的SLMs在CoT推理方面显著优于基础模型，缩小了与LLM的差距。

Abstract: Small Language Models (SLMs) are becoming increasingly popular in specialized
fields, such as industrial applications, due to their efficiency, lower
computational requirements, and ability to be fine-tuned for domain-specific
tasks, enabling accurate and cost-effective solutions. However, performing
complex reasoning using SLMs in specialized fields such as Industry 4.0 remains
challenging. In this paper, we propose a knowledge distillation framework for
industrial asset health, which transfers reasoning capabilities via
Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to
smaller, more efficient models (SLMs). We discuss the advantages and the
process of distilling LLMs using multi-choice question answering (MCQA) prompts
to enhance reasoning and refine decision-making. We also perform in-context
learning to verify the quality of the generated knowledge and benchmark the
performance of fine-tuned SLMs with generated knowledge against widely used
LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform
the base models by a significant margin, narrowing the gap to their LLM
counterparts. Our code is open-sourced at:
https://github.com/IBM/FailureSensorIQ.

</details>


### [63] [MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training](https://arxiv.org/abs/2510.18830)
*Wenxuan Li,Chengruidong Zhang,Huiqiang Jiang,Yucheng Li,Yuqing Yang,Lili Qiu*

Main category: cs.CL

TL;DR: This paper introduces MTraining, a distributed method that uses dynamic sparse attention to efficiently train LLMs with ultra-long contexts, achieving higher throughput without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenge of efficiently training LLMs with dynamic sparse attention on ultra-long contexts in distributed settings.

Method: MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention.

Result: MTraining achieves up to a 6x higher training throughput while preserving model accuracy, as demonstrated by training Qwen2.5-3B and evaluating on various downstream tasks.

Conclusion: MTraining is an effective distributed methodology for training LLMs with ultra-long contexts using dynamic sparse attention.

Abstract: The adoption of long context windows has become a standard feature in Large
Language Models (LLMs), as extended contexts significantly enhance their
capacity for complex reasoning and broaden their applicability across diverse
scenarios. Dynamic sparse attention is a promising approach for reducing the
computational cost of long-context. However, efficiently training LLMs with
dynamic sparse attention on ultra-long contexts-especially in distributed
settings-remains a significant challenge, due in large part to worker- and
step-level imbalance. This paper introduces MTraining, a novel distributed
methodology leveraging dynamic sparse attention to enable efficient training
for LLMs with ultra-long contexts. Specifically, MTraining integrates three key
components: a dynamic sparse training pattern, balanced sparse ring attention,
and hierarchical sparse ring attention. These components are designed to
synergistically address the computational imbalance and communication overheads
inherent in dynamic sparse attention mechanisms during the training of models
with extensive context lengths. We demonstrate the efficacy of MTraining by
training Qwen2.5-3B, successfully expanding its context window from 32K to 512K
tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite
of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A
Haystack, reveal that MTraining achieves up to a 6x higher training throughput
while preserving model accuracy. Our code is available at
https://github.com/microsoft/MInference/tree/main/MTraining.

</details>


### [64] [Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning](https://arxiv.org/abs/2510.18849)
*Chenghao Zhu,Meiling Tao,Tiannan Wang,Dongyi Ding,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为Critique-Post-Edit的强化学习框架，用于更忠实和可控地个性化大型语言模型。该框架包含一个提供多维评分和文本批评的个性化生成奖励模型，以及一个根据这些批评修改输出的机制。实验表明，该方法在个性化基准测试中优于标准PPO，并在性能上超过了GPT-4.1。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLMs）忠实地个性化以符合个体用户偏好是一个关键但具有挑战性的任务。虽然监督微调（SFT）很快达到性能瓶颈，标准的人类反馈强化学习（RLHF）在个性化细节上也存在困难。基于标量的奖励模型容易导致奖励黑客，从而产生冗长且表面个性化的响应。

Method: 我们提出了Critique-Post-Edit，这是一种稳健的强化学习框架，能够实现更忠实和可控的个性化。我们的框架集成了两个关键组件：(1) 一个个性化的生成奖励模型（GRM），它提供多维评分和文本批评以抵抗奖励黑客，(2) 一个Critique-Post-Edit机制，其中策略模型根据这些批评修改其输出以实现更有针对性和高效的學習。

Result: 在严格的长度控制评估下，我们的方法在个性化基准测试中显著优于标准PPO。个性化Qwen2.5-7B平均提升了11%的胜率，而个性化Qwen2.5-14B模型超过了GPT-4.1的性能。

Conclusion: 这些结果展示了实现忠实、高效和可控个性化的实用路径。

Abstract: Faithfully personalizing large language models (LLMs) to align with
individual user preferences is a critical but challenging task. While
supervised fine-tuning (SFT) quickly reaches a performance plateau, standard
reinforcement learning from human feedback (RLHF) also struggles with the
nuances of personalization. Scalar-based reward models are prone to reward
hacking which leads to verbose and superficially personalized responses. To
address these limitations, we propose Critique-Post-Edit, a robust
reinforcement learning framework that enables more faithful and controllable
personalization. Our framework integrates two key components: (1) a
Personalized Generative Reward Model (GRM) that provides multi-dimensional
scores and textual critiques to resist reward hacking, and (2) a
Critique-Post-Edit mechanism where the policy model revises its own outputs
based on these critiques for more targeted and efficient learning. Under a
rigorous length-controlled evaluation, our method substantially outperforms
standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an
average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses
the performance of GPT-4.1. These results demonstrate a practical path to
faithful, efficient, and controllable personalization.

</details>


### [65] [Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model](https://arxiv.org/abs/2510.18855)
*Ling Team,Anqi Shen,Baihui Li,Bin Hu,Bin Jing,Cai Chen,Chao Huang,Chao Zhang,Chaokun Yang,Cheng Lin,Chengyao Wen,Congqi Li,Deng Zhao,Dingbo Yuan,Donghai You,Fagui Mao,Fanzhuang Meng,Feng Xu,Guojie Li,Guowei Wang,Hao Dai,Haonan Zheng,Hong Liu,Jia Guo,Jiaming Liu,Jian Liu,Jianhao Fu,Jiannan Shi,Jianwen Wang,Jianxin Lai,Jin Yang,Jun Mei,Jun Zhou,Junbo Zhao,Junping Zhao,Kuan Xu,Le Su,Lei Chen,Li Tang,Liang Jiang,Liangcheng Fu,Lianhao Xu,Linfeng Shi,Lisha Liao,Longfei Zheng,Meng Li,Mingchun Chen,Qi Zuo,Qiang Cheng,Qianggang Cao,Qitao Shi,Quanrui Guo,Senlin Zhu,Shaofei Wang,Shaomian Zheng,Shuaicheng Li,Shuwei Gu,Siba Chen,Tao Wu,Tao Zhang,Tianyu Zhang,Tianyu Zhou,Tiwei Bie,Tongkai Yang,Wang Hong,Wang Ren,Weihua Chen,Wenbo Yu,Wengang Zheng,Xiangchun Wang,Xiaodong Yan,Xiaopei Wan,Xin Zhao,Xinyu Kong,Xinyu Tang,Xudong Han,Xudong Wang,Xuemin Yang,Xueyu Hu,Yalin Zhang,Yan Sun,Yicheng Shan,Yilong Wang,Yingying Xu,Yongkang Liu,Yongzhen Guo,Yuanyuan Wang,Yuchen Yan,Yuefan Wang,Yuhong Guo,Zehuan Li,Zhankai Xu,Zhe Li,Zhenduo Zhang,Zhengke Gui,Zhenxuan Pan,Zhenyu Huang,Zhenzhong Lan,Zhiqiang Ding,Zhiqiang Zhang,Zhixun Li,Zhizhen Liu,Zihao Wang,Zujie Wen*

Main category: cs.CL

TL;DR: Ring-1T是第一个开放源代码、具有万亿参数规模的先进思维模型，通过三项创新解决了训练-推理不匹配、滚动处理低效和RL系统瓶颈问题，并在多个基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 训练如此大规模的模型带来了前所未有的挑战，包括训练-推理不匹配、滚动处理中的低效以及RL系统的瓶颈。

Method: 提出了三个相互关联的创新：(1) IcePop通过逐标记差异掩码和剪切来稳定RL训练，解决训练-推理不匹配带来的不稳定；(2) C3PO++通过动态划分长rollouts来提高资源利用率，从而获得高时间效率；(3) ASystem是一个高性能的RL框架，旨在克服阻碍万亿参数模型训练的系统瓶颈。

Result: Ring-1T在关键基准测试中取得了突破性结果：AIME-2025得93.4，HMMT-2025得86.72，CodeForces得2088，ARC-AGI-v1得55.94。值得注意的是，在IMO-2025中获得了银牌水平的结果，证明了其卓越的推理能力。

Conclusion: 通过释放完整的1T参数MoE模型，我们为研究社区提供了直接访问尖端推理能力的机会。这一贡献标志着在民主化大规模推理智能方面的重要里程碑，并为开源模型性能设定了新基准。

Abstract: We present Ring-1T, the first open-source, state-of-the-art thinking model
with a trillion-scale parameter. It features 1 trillion total parameters and
activates approximately 50 billion per token. Training such models at a
trillion-parameter scale introduces unprecedented challenges, including
train-inference misalignment, inefficiencies in rollout processing, and
bottlenecks in the RL system. To address these, we pioneer three interconnected
innovations: (1) IcePop stabilizes RL training via token-level discrepancy
masking and clipping, resolving instability from training-inference mismatches;
(2) C3PO++ improves resource utilization for long rollouts under a token budget
by dynamically partitioning them, thereby obtaining high time efficiency; and
(3) ASystem, a high-performance RL framework designed to overcome the systemic
bottlenecks that impede trillion-parameter model training. Ring-1T delivers
breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on
HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a
silver medal-level result on the IMO-2025, underscoring its exceptional
reasoning capabilities. By releasing the complete 1T parameter MoE model to the
community, we provide the research community with direct access to cutting-edge
reasoning capabilities. This contribution marks a significant milestone in
democratizing large-scale reasoning intelligence and establishes a new baseline
for open-source model performance.

</details>


### [66] [LightMem: Lightweight and Efficient Memory-Augmented Generation](https://arxiv.org/abs/2510.18866)
*Jizhan Fang,Xinle Deng,Haoming Xu,Ziyan Jiang,Yuqi Tang,Ziwen Xu,Shumin Deng,Yunzhi Yao,Mengru Wang,Shuofei Qiao,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: LightMem是一种新的记忆系统，旨在提高大型语言模型在动态和复杂环境中的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的记忆系统在动态和复杂环境中存在性能和效率的问题，需要一种更高效的解决方案。

Method: LightMem基于Atkinson-Shiffrin人类记忆模型，将记忆分为三个阶段：感知记忆、短期记忆和长期记忆。

Result: LightMem在GPT和Qwen骨干网络上的实验显示，其准确性提高了最多10.9%，同时减少了多达117倍的token使用、159倍的API调用和超过12倍的运行时间。

Conclusion: LightMem在LongMemEval实验中表现出色，提高了准确性并减少了计算资源的使用。

Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle
to effectively leverage historical interaction information in dynamic and
complex environments. Memory systems enable LLMs to move beyond stateless
interactions by introducing persistent information storage, retrieval, and
utilization mechanisms. However, existing memory systems often introduce
substantial time and computational overhead. To this end, we introduce a new
memory system called LightMem, which strikes a balance between the performance
and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of
human memory, LightMem organizes memory into three complementary stages. First,
cognition-inspired sensory memory rapidly filters irrelevant information
through lightweight compression and groups information according to their
topics. Next, topic-aware short-term memory consolidates these topic-based
groups, organizing and summarizing content for more structured access. Finally,
long-term memory with sleep-time update employs an offline procedure that
decouples consolidation from online inference. Experiments on LongMemEval with
GPT and Qwen backbones show that LightMem outperforms strong baselines in
accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API
calls by up to 159x, and runtime by over 12x. The code is available at
https://github.com/zjunlp/LightMem.

</details>


### [67] [How Do LLMs Use Their Depth?](https://arxiv.org/abs/2510.18871)
*Akshat Gupta,Jay Yeung,Gopala Anumanchipalli,Anna Ivanova*

Main category: cs.CL

TL;DR: 本文分析了LLM在不同层中使用深度进行预测的方式，并提出了一个“猜测-然后-精炼”框架来解释这一过程。


<details>
  <summary>Details</summary>
Motivation: 我们希望了解LLM如何在不同层中使用深度进行预测，以改进计算效率。

Method: 我们提出了一个“猜测-然后-精炼”框架，以解释LLM如何内部构建计算以进行预测。我们还通过三个案例研究分析了层深度的动态使用。

Result: 我们发现早期层中的顶级预测主要由高频标记组成，这些标记在缺乏适当上下文信息时作为统计猜测提出。随着上下文信息的发展，这些初始猜测被精炼成适当的标记。此外，我们发现功能词是最先正确预测的，第一个标记需要更多的计算深度，模型在前半部分层中识别响应格式，但在最后才确定响应。

Conclusion: 我们的结果提供了对LLM中深度使用的详细视图，揭示了成功预测背后的逐层计算，并为未来改进基于变压器的模型的计算效率提供了见解。

Abstract: Growing evidence suggests that large language models do not use their depth
uniformly, yet we still lack a fine-grained understanding of their layer-wise
prediction dynamics. In this paper, we trace the intermediate representations
of several open-weight models during inference and reveal a structured and
nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework
that explains how LLMs internally structure their computations to make
predictions. We first show that the top-ranked predictions in early LLM layers
are composed primarily of high-frequency tokens, which act as statistical
guesses proposed by the model early on due to the lack of appropriate
contextual information. As contextual information develops deeper into the
model, these initial guesses get refined into contextually appropriate tokens.
Even high-frequency token predictions from early layers get refined >70% of the
time, indicating that correct token prediction is not "one-and-done". We then
go beyond frequency-based prediction to examine the dynamic usage of layer
depth across three case studies. (i) Part-of-speech analysis shows that
function words are, on average, the earliest to be predicted correctly. (ii)
Fact recall task analysis shows that, in a multi-token answer, the first token
requires more computational depth than the rest. (iii) Multiple-choice task
analysis shows that the model identifies the format of the response within the
first half of the layers, but finalizes its response only toward the end.
Together, our results provide a detailed view of depth usage in LLMs, shedding
light on the layer-by-layer computations that underlie successful predictions
and providing insights for future works to improve computational efficiency in
transformer-based models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 论文提出了一种不依赖全局时间的主体-事件本体形式化方法，用于建模复杂的动态系统，并在boldsea系统中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 传统的动态系统建模通常依赖于全局时间，这在分布式环境中可能不适用。因此，论文旨在提出一种不依赖全局时间的本体形式化方法，以更灵活地建模复杂动态系统。

Method: 论文提出了一种基于主体-事件本体的形式化方法，包括九个公理（A1-A9），确保可执行本体的正确性。通过声明式数据流机制实现本体的可执行性，并利用模型作为认知过滤器来限制主体能够固定的事件范围。

Result: 论文展示了该形式化方法在boldsea系统中的实际应用，该系统是一个用于可执行本体的工作流引擎，理论构造在BSL（Boldsea语义语言）中实现。

Conclusion: 该论文提出了一个基于主体-事件本体的形式化方法，用于建模复杂的动态系统，无需依赖全局时间。该方法在分布式系统、微服务架构、DLT平台和多视角场景中具有实际应用价值。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [69] [SMaRT: Select, Mix, and ReinvenT -- A Strategy Fusion Framework for LLM-Driven Reasoning and Planning](https://arxiv.org/abs/2510.18095)
*Nikhil Verma,Manasa Bharadwaj,Wonjun Jang,Harmanpreet Singh,Yixiao Wang,Homa Fashandi,Chul Lee*

Main category: cs.AI

TL;DR: 本文提出了一种新的策略融合框架SMaRT，通过整合多种推理策略，提高了大语言模型在复杂任务自动化中的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于单一策略提示，缺乏不同推理方法之间的协同作用。没有一种策略在所有情况下都表现最佳，因此需要一个能够融合策略以最大化性能并确保鲁棒性的框架。

Method: 本文介绍了Select, Mix, and ReinvenT (SMaRT)框架，这是一种创新的策略融合方法，旨在通过无缝整合多种推理策略来创建平衡且高效的解决方案。

Result: 在推理、规划和顺序决策基准上的广泛实证评估表明了SMaRT的鲁棒性和适应性。该框架在解决方案质量、约束遵守和性能指标方面始终优于最先进的基线。

Conclusion: 本文重新定义了基于大语言模型的决策过程，通过跨策略校准的新范式，为推理系统带来了更优的结果，并推进了自我完善方法的边界。

Abstract: Large Language Models (LLMs) have redefined complex task automation with
exceptional generalization capabilities. Despite these advancements,
state-of-the-art methods rely on single-strategy prompting, missing the synergy
of diverse reasoning approaches. No single strategy excels universally,
highlighting the need for frameworks that fuse strategies to maximize
performance and ensure robustness. We introduce the Select, Mix, and ReinvenT
(SMaRT) framework, an innovative strategy fusion approach designed to overcome
this constraint by creating balanced and efficient solutions through the
seamless integration of diverse reasoning strategies. Unlike existing methods,
which employ LLMs merely as evaluators, SMaRT uses them as intelligent
integrators, unlocking the "best of all worlds" across tasks. Extensive
empirical evaluations across benchmarks in reasoning, planning, and sequential
decision-making highlight the robustness and adaptability of SMaRT. The
framework consistently outperforms state-of-the-art baselines in solution
quality, constraint adherence, and performance metrics. This work redefines
LLM-driven decision-making by pioneering a new paradigm in cross-strategy
calibration, unlocking superior outcomes for reasoning systems and advancing
the boundaries of self-refining methodologies.

</details>


### [70] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为Saber的新型训练-free 采样算法，用于Diffusion语言模型（DLMs），以提高代码生成的推理速度和输出质量。实验结果表明，Saber在多个主流代码生成基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: DLM在代码生成任务中的性能受到推理速度和输出质量之间关键权衡的显著阻碍。通过减少采样步骤加速代码生成过程通常会导致性能的灾难性崩溃。

Method: 我们引入了一种名为Saber的新颖的训练-free 采样算法，该算法基于DLM生成过程中的两个关键见解：1）随着更多代码上下文的建立，可以自适应加速；2）需要回溯机制来逆转生成的标记。

Result: 在多个主流代码生成基准测试中，Saber平均提升了1.9%的Pass@1准确性，同时实现了平均251.4%的推理加速。

Conclusion: 通过利用DLM的固有优势，我们的工作显著缩小了在代码生成中与自回归模型的性能差距。

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [71] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 本文提出了一种概率意图建模框架，用于多轮社交对话中的大型语言模型代理。该框架通过维护和更新合作伙伴潜在意图的信念分布，提供额外的上下文基础，从而实现适应性对话策略。实验结果显示，该框架在多个基准上均优于基线模型，并略胜于一个直接观察意图的虚拟代理。


<details>
  <summary>Details</summary>
Motivation: 开发具有社会智能的大型语言模型代理。

Method: 我们提出了一个概率意图建模框架，用于多轮社交对话中的大型语言模型（LLM）代理。该框架维护一个关于合作伙伴潜在意图的信念分布，从上下文先验初始化，并在每次话语后通过似然估计动态更新。演变的分布为策略提供了额外的上下文基础，使在不确定性下能够适应对话策略。

Result: 初步实验显示，所提出的框架在SOTOPIA-All上整体得分提高了9.0%，在SOTOPIA-Hard上提高了4.1%，并且略微超过了直接观察合作伙伴意图的虚拟代理。

Conclusion: 这些初步结果表明，概率意图建模可以有助于开发具有社会智能的大型语言模型代理。

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [72] [HouseTour: A Virtual Real Estate A(I)gent](https://arxiv.org/abs/2510.18054)
*Ata Çelen,Marc Pollefeys,Daniel Barath,Iro Armeni*

Main category: cs.CV

TL;DR: 本文介绍了HouseTour，一种从图像集合中生成空间感知的3D相机轨迹和自然语言摘要的方法。该方法利用扩散过程生成平滑的视频轨迹，并将此信息整合到视觉语言模型中以生成3D基础描述。通过3D高斯点云渲染新视角，实现了专业质量的视频创建。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在几何推理方面存在困难。

Method: 我们引入了HouseTour，这是一种从描绘现有3D空间的图像集合中生成空间感知的3D相机轨迹和自然语言摘要的方法。我们的方法通过受已知相机姿态约束的扩散过程生成平滑的视频轨迹，并将此信息整合到视觉语言模型（VLM）中以生成3D基础描述。最后，我们使用3D高斯点云渲染沿轨迹的新视角。

Result: 实验表明，将3D相机轨迹纳入文本生成过程可以提高性能，优于独立处理每个任务的方法。我们评估了单独和端到端的性能，并引入了一个新的联合度量标准。

Conclusion: 我们的工作使无需专门技能或设备即可为房地产和旅游应用自动创建专业质量的视频成为可能。

Abstract: We introduce HouseTour, a method for spatially-aware 3D camera trajectory and
natural language summary generation from a collection of images depicting an
existing 3D space. Unlike existing vision-language models (VLMs), which
struggle with geometric reasoning, our approach generates smooth video
trajectories via a diffusion process constrained by known camera poses and
integrates this information into the VLM for 3D-grounded descriptions. We
synthesize the final video using 3D Gaussian splatting to render novel views
along the trajectory. To support this task, we present the HouseTour dataset,
which includes over 1,200 house-tour videos with camera poses, 3D
reconstructions, and real estate descriptions. Experiments demonstrate that
incorporating 3D camera trajectories into the text generation process improves
performance over methods handling each task independently. We evaluate both
individual and end-to-end performance, introducing a new joint metric. Our work
enables automated, professional-quality video creation for real estate and
touristic applications without requiring specialized expertise or equipment.

</details>


### [73] [SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving](https://arxiv.org/abs/2510.18123)
*Xiangbo Gao,Tzu-Hsiang Lin,Ruojing Song,Yuheng Wu,Kuan-Ru Huang,Zicheng Jin,Fangzhou Lin,Shinan Liu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本研究首次系统地研究了基于自然语言的协作驾驶中的全栈安全和隐私问题，提出了一种名为SafeCoop的代理防御流程，以应对各种攻击策略，并在模拟环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的V2X系统面临高带宽需求、语义损失和互操作性问题。最近的研究探索自然语言作为有前途的媒介，可以提供语义丰富性、决策级推理和人机互操作性，但这种范式转变也带来了新的漏洞，包括消息丢失、幻觉、语义操纵和对抗攻击。

Method: 我们开发了一个全面的攻击策略分类，包括连接中断、中继/重放干扰、内容伪造和多连接伪造。为了缓解这些风险，我们引入了一个称为SafeCoop的代理防御流程，集成了语义防火墙、语言-感知一致性检查和多源共识，通过代理转换函数实现跨帧空间对齐。

Result: 我们在CARLA模拟中对SafeCoop进行了系统评估，在32个关键场景下实现了恶意攻击下69.15%的驾驶得分提升，并达到了高达67.32%的恶意检测F1分数。

Conclusion: 本研究为基于自然语言的协作驾驶系统提供了安全、可靠和可信的语言驱动协作研究方向的指导。

Abstract: Collaborative driving systems leverage vehicle-to-everything (V2X)
communication across multiple agents to enhance driving safety and efficiency.
Traditional V2X systems take raw sensor data, neural features, or perception
results as communication media, which face persistent challenges, including
high bandwidth demands, semantic loss, and interoperability issues. Recent
advances investigate natural language as a promising medium, which can provide
semantic richness, decision-level reasoning, and human-machine interoperability
at significantly lower bandwidth. Despite great promise, this paradigm shift
also introduces new vulnerabilities within language communication, including
message loss, hallucinations, semantic manipulation, and adversarial attacks.
In this work, we present the first systematic study of full-stack safety and
security issues in natural-language-based collaborative driving. Specifically,
we develop a comprehensive taxonomy of attack strategies, including connection
disruption, relay/replay interference, content spoofing, and multi-connection
forgery. To mitigate these risks, we introduce an agentic defense pipeline,
which we call SafeCoop, that integrates a semantic firewall,
language-perception consistency checks, and multi-source consensus, enabled by
an agentic transformation function for cross-frame spatial alignment. We
systematically evaluate SafeCoop in closed-loop CARLA simulation across 32
critical scenarios, achieving 69.15% driving score improvement under malicious
attacks and up to 67.32% F1 score for malicious detection. This study provides
guidance for advancing research on safe, secure, and trustworthy
language-driven collaboration in transportation systems. Our project page is
https://xiangbogaobarry.github.io/SafeCoop.

</details>


### [74] [VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety](https://arxiv.org/abs/2510.18214)
*Shruti Palaskar,Leon Gatys,Mona Abdelrahman,Mar Jacobo,Larry Lindsey,Rutika Moharir,Gunnar Lund,Yang Xu,Navid Shiee,Jeffrey Bigham,Charles Maalouf,Joseph Yitan Cheng*

Main category: cs.CV

TL;DR: 本文提出了一个全面的框架VLSU，用于系统评估多模态安全性。通过细粒度严重性分类和组合分析，我们构建了一个大规模基准数据集，并发现现有模型在联合图像-文本推理方面存在明显缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有的方法未能明确区分不安全内容和边缘案例，导致对真正有害内容的过度阻止或拒绝不足。此外，模型在拒绝不安全内容的同时难以平衡对值得参与的边缘案例的响应。

Method: 我们提出了视觉语言安全理解（VLSU），这是一个全面的框架，通过细粒度严重性分类和跨17种不同的安全模式的组合分析来系统地评估多模态安全性。使用包含真实世界图像和人工标注的多阶段流程，我们构建了一个涵盖15种危害类别的大规模基准数据集。

Result: 对十一个最先进的模型的评估显示，当需要联合图像-文本推理来确定安全标签时，性能显著下降，从90%以上的准确率降至20-55%。此外，我们发现模型在拒绝不安全内容和回应边缘案例之间难以平衡。

Conclusion: 我们的框架揭示了联合图像-文本理解中的弱点和当前模型中的对齐差距，并提供了一个关键的测试平台，以推动视觉-语言安全研究的下一个里程碑。

Abstract: Safety evaluation of multimodal foundation models often treats vision and
language inputs separately, missing risks from joint interpretation where
benign content becomes harmful in combination. Existing approaches also fail to
distinguish clearly unsafe content from borderline cases, leading to
problematic over-blocking or under-refusal of genuinely harmful content. We
present Vision Language Safety Understanding (VLSU), a comprehensive framework
to systematically evaluate multimodal safety through fine-grained severity
classification and combinatorial analysis across 17 distinct safety patterns.
Using a multi-stage pipeline with real-world images and human annotation, we
construct a large-scale benchmark of 8,187 samples spanning 15 harm categories.
Our evaluation of eleven state-of-the-art models reveals systematic joint
understanding failures: while models achieve 90%-plus accuracy on clear
unimodal safety signals, performance degrades substantially to 20-55% when
joint image-text reasoning is required to determine the safety label. Most
critically, 34% of errors in joint image-text safety classification occur
despite correct classification of the individual modalities, further
demonstrating absent compositional reasoning capabilities. Additionally, we
find that models struggle to balance refusing unsafe content while still
responding to borderline cases that deserve engagement. For example, we find
that instruction framing can reduce the over-blocking rate on borderline
content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of
under-refusing on unsafe content with refusal rate dropping from 90.8% to
53.9%. Overall, our framework exposes weaknesses in joint image-text
understanding and alignment gaps in current models, and provides a critical
test bed to enable the next milestones in research on robust vision-language
safety.

</details>


### [75] [The Impact of Image Resolution on Biomedical Multimodal Large Language Models](https://arxiv.org/abs/2510.18304)
*Liangyu Chen,James Burgess,Jeffrey J Nirschl,Orr Zohar,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 研究了图像分辨率对生物医学多模态大语言模型性能的影响，发现原生分辨率训练和推理能显著提升性能，而混合分辨率训练可有效缓解分辨率错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLM）主要针对通用数据集中的低分辨率图像设计，可能导致关键信息丢失，因此需要研究图像分辨率对MLLM性能的影响。

Method: 研究图像分辨率如何影响生物医学应用中的MLLM性能，并通过实验验证了原生分辨率训练和推理、训练和推理分辨率之间的错位以及混合分辨率训练的效果。

Result: （1）原生分辨率训练和推理显著提高了多个任务的性能；（2）训练和推理分辨率之间的错位严重降低了性能；（3）混合分辨率训练有效缓解了错位问题，并在计算约束与性能需求之间取得了平衡。

Conclusion: 基于这些发现，我们建议优先考虑原生分辨率推理和混合分辨率数据集，以优化生物医学MLLM在科学研究和临床应用中的变革性影响。

Abstract: Imaging technologies are fundamental to biomedical research and modern
medicine, requiring analysis of high-resolution images across various
modalities. While multimodal large language models (MLLMs) show promise for
biomedical image analysis, most are designed for low-resolution images from
general-purpose datasets, risking critical information loss. We investigate how
image resolution affects MLLM performance in biomedical applications and
demonstrate that: (1) native-resolution training and inference significantly
improve performance across multiple tasks, (2) misalignment between training
and inference resolutions severely degrades performance, and (3)
mixed-resolution training effectively mitigates misalignment and balances
computational constraints with performance requirements. Based on these
findings, we recommend prioritizing native-resolution inference and
mixed-resolution datasets to optimize biomedical MLLMs for transformative
impact in scientific research and clinical applications.

</details>


### [76] [Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2510.18502)
*Wei-Chia Chang,Yan-Ann Chen*

Main category: cs.CV

TL;DR: 本文提出一种将视觉语言模型与检索增强生成集成的流程，通过基于文本的推理实现零样本识别，避免大规模再训练并支持快速更新，实验显示该方法在车辆品牌和型号识别任务中提升了近20%的性能。


<details>
  <summary>Details</summary>
Motivation: 车辆品牌和型号识别（VMMR）是智能交通系统的重要任务，但现有方法难以适应新发布的型号。对比语言图像预训练（CLIP）提供了强大的视觉-文本对齐，但其固定预训练权重限制了性能，且需要代价高昂的图像特定微调。

Method: 我们提出了一种将视觉语言模型（VLMs）与检索增强生成（RAG）集成的流程，通过基于文本的推理支持零样本识别。VLM将车辆图像转换为描述性属性，并与文本特征数据库进行比较。相关条目被检索并与描述结合以形成提示，语言模型（LM）推断出品牌和型号。

Result: 实验表明，所提出的方法比CLIP基线提高了近20%的识别率，证明了RAG增强的LM推理在智慧城市建设中的可扩展VMMR潜力。

Conclusion: 实验表明，所提出的方法比CLIP基线提高了近20%的识别率，证明了RAG增强的LM推理在智慧城市建设中的可扩展VMMR潜力。

Abstract: Vehicle make and model recognition (VMMR) is an important task in intelligent
transportation systems, but existing approaches struggle to adapt to newly
released models. Contrastive Language-Image Pretraining (CLIP) provides strong
visual-text alignment, yet its fixed pretrained weights limit performance
without costly image-specific finetuning. We propose a pipeline that integrates
vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to
support zero-shot recognition through text-based reasoning. A VLM converts
vehicle images into descriptive attributes, which are compared against a
database of textual features. Relevant entries are retrieved and combined with
the description to form a prompt, and a language model (LM) infers the make and
model. This design avoids large-scale retraining and enables rapid updates by
adding textual descriptions of new vehicles. Experiments show that the proposed
method improves recognition by nearly 20% over the CLIP baseline, demonstrating
the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city
applications.

</details>


### [77] [See the Text: From Tokenization to Visual Reading](https://arxiv.org/abs/2510.18840)
*Ling Xing,Alex Jinpeng Wang,Rui Yan,Hongyu Qu,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: SeeTok 是一种新的文本处理方法，通过将文本渲染为图像并利用预训练的多模态大语言模型来解读它们，从而实现了更高效的文本处理。


<details>
  <summary>Details</summary>
Motivation: Modern large language models (LLMs) rely on subword tokenization, which fragments text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation.

Method: SeeTok renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training.

Result: Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy.

Conclusion: SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.

Abstract: People see text. Humans read by recognizing words as visual objects,
including their shapes, layouts, and patterns, before connecting them to
meaning, which enables us to handle typos, distorted fonts, and various scripts
effectively. Modern large language models (LLMs), however, rely on subword
tokenization, fragmenting text into pieces from a fixed vocabulary. While
effective for high-resource languages, this approach over-segments low-resource
languages, yielding long, linguistically meaningless sequences and inflating
computation. In this work, we challenge this entrenched paradigm and move
toward a vision-centric alternative. Our method, SeeTok, renders text as images
(visual-text) and leverages pretrained multimodal LLMs to interpret them,
reusing strong OCR and text-vision alignment abilities learned from large-scale
multimodal training. Across three different language tasks, SeeTok matches or
surpasses subword tokenizers while requiring 4.43 times fewer tokens and
reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,
robustness to typographic noise, and linguistic hierarchy. SeeTok signals a
shift from symbolic tokenization to human-like visual reading, and takes a step
toward more natural and cognitively inspired language models.

</details>


### [78] [Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs](https://arxiv.org/abs/2510.18876)
*Haochen Wang,Yuhao Wang,Tao Zhang,Yikang Zhou,Yanwei Li,Jiacong Wang,Ye Tian,Jiahao Meng,Zilong Huang,Guangcan Mai,Anran Wang,Yunhai Tong,Zhuochen Wang,Xiangtai Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GAR的区域级多模态大语言模型，通过全局上下文和多个提示之间的交互建模，实现了高级组合推理，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的区域级多模态大语言模型通常优化为孤立地理解给定区域，忽视了重要的全局上下文。因此，需要一种能够全面理解区域级视觉的方法。

Method: 引入Grasp Any Region (GAR)方法，通过有效的RoI对齐特征重放技术，支持精确感知和多个提示之间的交互建模，从而实现高级组合推理。

Result: GAR-1B不仅保持了最先进的描述能力，还在建模多个提示之间的关系方面表现出色，甚至在GAR-Bench-VQA上超过了InternVL3-78B。此外，GAR-Bench提供了更准确的单区域理解和跨区域交互与复杂推理的评估。

Conclusion: GAR-1B不仅保持了最先进的描述能力，还在建模多个提示之间的关系方面表现出色，甚至在GAR-Bench-VQA上超过了InternVL3-78B。更重要的是，我们的零样本GAR-8B在VideoRefer-BenchQ上超过了域内VideoRefer-7B，表明其强大的能力可以轻松转移到视频中。

Abstract: While Multimodal Large Language Models (MLLMs) excel at holistic
understanding, they struggle in capturing the dense world with complex scenes,
requiring fine-grained analysis of intricate details and object
inter-relationships. Region-level MLLMs have been a promising step. However,
previous attempts are generally optimized to understand given regions in
isolation, neglecting crucial global contexts. To address this, we introduce
Grasp Any Region (GAR) for comprehen- sive region-level visual understanding.
Empowered by an effective RoI-aligned feature replay technique, GAR supports
(1) precise perception by leveraging necessary global contexts, and (2)
modeling interactions between multiple prompts. Together, it then naturally
achieves (3) advanced compositional reasoning to answer specific free-form
questions about any region, shifting the paradigm from passive description to
active dialogue. Moreover, we construct GAR-Bench, which not only provides a
more accurate evaluation of single-region comprehension, but also, more
importantly, measures interactions and complex reasoning across multiple
regions. Extensive experiments have demonstrated that GAR-1B not only maintains
the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5
on DLC-Bench, but also excels at modeling relationships between multiple
prompts with advanced comprehension capabilities, even surpassing InternVL3-78B
on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms
in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong
capabilities can be easily transferred to videos.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [79] [CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment](https://arxiv.org/abs/2510.18471)
*Xue Jiang,Yihong Dong,Mengyang Liu,Hongyi Deng,Tian Wang,Yongding Tao,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: This paper proposes CodeRL+, an approach that integrates execution semantics alignment into the RLVR training pipeline for code generation, leading to improved performance in code generation tasks.


<details>
  <summary>Details</summary>
Motivation: The fundamental semantic gap between LLMs' training on textual patterns and the goal of functional correctness, which is governed by formal execution semantics, remains unaddressed by RLVR approaches that solely rely on binary pass/fail signals.

Method: CodeRL+ integrates execution semantics alignment into the RLVR training pipeline for code generation. It enables the model to infer variable-level execution trajectory, providing a direct learning signal of execution semantics. CodeRL+ can construct execution semantics alignment directly using existing on-policy rollouts and integrates seamlessly with various RL algorithms.

Result: CodeRL+ outperforms post-training baselines (including RLVR and Distillation), achieving a 4.6% average relative improvement in pass@1. It generalizes effectively to other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning and test-output-generation benchmarks, respectively.

Conclusion: CodeRL+ shows strong applicability across diverse RL algorithms and LLMs. Furthermore, probe analyses provide compelling evidence that CodeRL+ strengthens the alignment between code's textual representations and its underlying execution semantics.

Abstract: While Large Language Models (LLMs) excel at code generation by learning from
vast code corpora, a fundamental semantic gap remains between their training on
textual patterns and the goal of functional correctness, which is governed by
formal execution semantics. Reinforcement Learning with Verifiable Rewards
(RLVR) approaches attempt to bridge this gap using outcome rewards from
executing test cases. However, solely relying on binary pass/fail signals is
inefficient for establishing a well-aligned connection between the textual
representation of code and its execution semantics, especially for subtle
logical errors within the code. In this paper, we propose CodeRL+, a novel
approach that integrates execution semantics alignment into the RLVR training
pipeline for code generation. CodeRL+ enables the model to infer variable-level
execution trajectory, providing a direct learning signal of execution
semantics. CodeRL+ can construct execution semantics alignment directly using
existing on-policy rollouts and integrates seamlessly with various RL
algorithms. Extensive experiments demonstrate that CodeRL+ outperforms
post-training baselines (including RLVR and Distillation), achieving a 4.6%
average relative improvement in pass@1. CodeRL+ generalizes effectively to
other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning
and test-output-generation benchmarks, respectively. CodeRL+ shows strong
applicability across diverse RL algorithms and LLMs. Furthermore, probe
analyses provide compelling evidence that CodeRL+ strengthens the alignment
between code's textual representations and its underlying execution semantics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [Hierarchical Federated Unlearning for Large Language Models](https://arxiv.org/abs/2510.17895)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种联邦遗忘方法，用于大型语言模型，以解决实际遗忘需求的连续性和异构性问题，以及分布式敏感数据的访问不对称性问题。该方法通过任务特定的适配器学习来解耦遗忘和保留，并采用分层合并策略来减轻冲突目标，从而实现稳健、适应性强的遗忘更新。实验结果表明，该方法在处理异构遗忘请求的同时，保持了强大的LLM实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地被集成到现实世界的应用中，引发了对隐私、安全和移除不良知识的需求。机器遗忘作为一种有前途的解决方案，面临两个关键挑战：（1）实际遗忘需求通常是连续和异构的，（2）它们涉及具有不对称访问权限的分布式敏感数据。这些因素导致了跨域和域内干扰，进一步加剧了遗忘和保留性能不平衡的困境。

Method: 我们提出了一种可扩展且隐私保护的联邦遗忘方法，通过任务特定的适配器学习来解耦遗忘和保留，并采用分层合并策略来减轻冲突目标，实现稳健、适应性强的遗忘更新。

Result: 在WMDP、MUSE和TOFU基准测试中的全面实验表明，我们的方法能够有效处理异构的遗忘请求，同时保持强大的LLM实用性。

Conclusion: 我们的方法在处理异构的遗忘请求的同时，保持了强大的LLM实用性，相比基线方法表现更好。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, raising concerns about privacy, security and the need to remove
undesirable knowledge. Machine Unlearning has emerged as a promising solution,
yet faces two key challenges: (1) practical unlearning needs are often
continuous and heterogeneous, and (2) they involve decentralized, sensitive
data with asymmetric access. These factors result in inter-domain and
intra-domain interference, which further amplifies the dilemma of unbalanced
forgetting and retaining performance. In response, we propose a federated
unlearning approach for LLMs that is scalable and privacy preserving. Our
method decouples unlearning and retention via task-specific adapter learning
and employs a hierarchical merging strategy to mitigate conflicting objectives
and enables robust, adaptable unlearning updates. Comprehensive experiments on
benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles
heterogeneous unlearning requests while maintaining strong LLM utility compared
with baseline methods.

</details>


### [81] [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/abs/2510.18874)
*Howard Chen,Noam Razin,Karthik Narasimhan,Danqi Chen*

Main category: cs.LG

TL;DR: 本研究比较了监督微调和强化学习在减轻语言模型灾难性遗忘方面的效果，发现强化学习由于其在线策略数据的特性，能够更好地保留先验知识，同时在目标任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在识别减轻灾难性遗忘的指导原则，通过比较SFT和RL的遗忘模式，探索哪种方法在保持现有能力的同时能更好地适应新任务。

Method: 我们系统比较了两种常用的后训练方法：监督微调（SFT）和强化学习（RL），并考虑了一个简化的设置，其中语言模型被建模为两个分布的混合体，一个对应于先验知识，另一个对应于目标任务。

Result: 实验结果显示，在LM家族（Llama、Qwen）和任务（指令遵循、一般知识和算术推理）中，RL比SFT导致的遗忘更少，同时在目标任务上的表现相当或更好。

Conclusion: 我们的研究结果表明，使用近似在线策略数据可以有效减轻灾难性遗忘，这在实际应用中可能比完全在线策略数据更高效。

Abstract: Adapting language models (LMs) to new tasks via post-training carries the
risk of degrading existing capabilities -- a phenomenon classically known as
catastrophic forgetting. In this paper, toward identifying guidelines for
mitigating this phenomenon, we systematically compare the forgetting patterns
of two widely adopted post-training methods: supervised fine-tuning (SFT) and
reinforcement learning (RL). Our experiments reveal a consistent trend across
LM families (Llama, Qwen) and tasks (instruction following, general knowledge,
and arithmetic reasoning): RL leads to less forgetting than SFT while achieving
comparable or higher target task performance. To investigate the cause for this
difference, we consider a simplified setting in which the LM is modeled as a
mixture of two distributions, one corresponding to prior knowledge and the
other to the target task. We identify that the mode-seeking nature of RL, which
stems from its use of on-policy data, enables keeping prior knowledge intact
when learning the target task. We then verify this insight by demonstrating
that the use on-policy data underlies the robustness of RL to forgetting in
practical settings, as opposed to other algorithmic choices such as the KL
regularization or advantage estimation. Lastly, as a practical implication, our
results highlight the potential of mitigating forgetting using approximately
on-policy data, which can be substantially more efficient to obtain than fully
on-policy data.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [82] [Does GenAI Rewrite How We Write? An Empirical Study on Two-Million Preprints](https://arxiv.org/abs/2510.17882)
*Minfeng Qi,Zhongmin Cao,Qin Wang,Ningran Li,Tianqing Zhu*

Main category: cs.CY

TL;DR: This paper analyzes the impact of LLMs on scientific publishing using a large-scale analysis of preprints and finds that LLMs act as selective catalysts, widening disciplinary divides and highlighting the need for governance frameworks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap in systematic evidence of whether and how LLMs reshape scientific publishing.

Method: The paper uses a multi-level analytical framework that integrates interrupted time-series models, collaboration and productivity metrics, linguistic profiling, and topic modeling to assess changes in volume, authorship, style, and disciplinary orientation.

Result: LLMs have accelerated submission and revision cycles, modestly increased linguistic complexity, and disproportionately expanded AI-related topics, while computationally intensive fields benefit more than others.

Conclusion: LLMs act less as universal disruptors than as selective catalysts, amplifying existing strengths and widening disciplinary divides. The paper provides the first empirical foundation for evaluating the influence of generative AI on academic publishing and highlights the need for governance frameworks that preserve trust, fairness, and accountability in an AI-enabled research ecosystem.

Abstract: Preprint repositories become central infrastructures for scholarly
communication. Their expansion transforms how research is circulated and
evaluated before journal publication. Generative large language models (LLMs)
introduce a further potential disruption by altering how manuscripts are
written. While speculation abounds, systematic evidence of whether and how LLMs
reshape scientific publishing remains limited.
  This paper addresses the gap through a large-scale analysis of more than 2.1
million preprints spanning 2016--2025 (115 months) across four major
repositories (i.e., arXiv, bioRxiv, medRxiv, SocArXiv). We introduce a
multi-level analytical framework that integrates interrupted time-series
models, collaboration and productivity metrics, linguistic profiling, and topic
modeling to assess changes in volume, authorship, style, and disciplinary
orientation. Our findings reveal that LLMs have accelerated submission and
revision cycles, modestly increased linguistic complexity, and
disproportionately expanded AI-related topics, while computationally intensive
fields benefit more than others. These results show that LLMs act less as
universal disruptors than as selective catalysts, amplifying existing strengths
and widening disciplinary divides. By documenting these dynamics, the paper
provides the first empirical foundation for evaluating the influence of
generative AI on academic publishing and highlights the need for governance
frameworks that preserve trust, fairness, and accountability in an AI-enabled
research ecosystem.

</details>


### [83] [Are LLMs Court-Ready? Evaluating Frontier Models on Indian Legal Reasoning](https://arxiv.org/abs/2510.17900)
*Kush Juvekar,Arghya Bhattacharya,Sai Khadloya,Utkarsh Saxena*

Main category: cs.CY

TL;DR: This paper evaluates the competence of LLMs in legal workflows using India's public legal examinations as a proxy. It finds that while LLMs can assist in certain tasks, human leadership remains essential for complex legal work.


<details>
  <summary>Details</summary>
Motivation: The study aims to create a jurisdiction-specific framework to assess the baseline competence of LLMs in legal workflows, using India's public legal examinations as a transparent proxy.

Method: The study uses India's public legal examinations as a proxy to evaluate the competence of LLMs. It includes objective screens from top national and state exams and a lawyer-graded, paired-blinded study of long-form answers from the Supreme Court's Advocate-on-Record exam.

Result: Frontier systems consistently clear historical cutoffs and often match or exceed recent top-scorer bands on objective exams, but none surpasses the human topper on long-form reasoning. Grader notes identify three reliability failure modes: procedural or format compliance, authority or citation discipline, and forum-appropriate voice and structure.

Conclusion: LLMs can assist in certain legal tasks but human leadership is still essential for forum-specific drafting, procedural and relief strategy, reconciling authorities and exceptions, and ethical, accountable judgment.

Abstract: Large language models (LLMs) are entering legal workflows, yet we lack a
jurisdiction-specific framework to assess their baseline competence therein. We
use India's public legal examinations as a transparent proxy. Our multi-year
benchmark assembles objective screens from top national and state exams and
evaluates open and frontier LLMs under real-world exam conditions. To probe
beyond multiple-choice questions, we also include a lawyer-graded,
paired-blinded study of long-form answers from the Supreme Court's
Advocate-on-Record exam. This is, to our knowledge, the first exam-grounded,
India-specific yardstick for LLM court-readiness released with datasets and
protocols. Our work shows that while frontier systems consistently clear
historical cutoffs and often match or exceed recent top-scorer bands on
objective exams, none surpasses the human topper on long-form reasoning. Grader
notes converge on three reliability failure modes: procedural or format
compliance, authority or citation discipline, and forum-appropriate voice and
structure. These findings delineate where LLMs can assist (checks,
cross-statute consistency, statute and precedent lookups) and where human
leadership remains essential: forum-specific drafting and filing, procedural
and relief strategy, reconciling authorities and exceptions, and ethical,
accountable judgment.

</details>


### [84] [Interpretability Framework for LLMs in Undergraduate Calculus](https://arxiv.org/abs/2510.17910)
*Sagnik Dakshit,Sushmita Sinha Roy*

Main category: cs.CY

TL;DR: 本研究提出了一种新的可解释性框架，用于评估LLM在数学教育中的推理行为，揭示了LLM在推理过程中存在的问题，并为设计可解释的AI辅助反馈工具提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法主要关注最终答案的准确性，而忽略了推理过程。然而，在数学中，多步骤逻辑、符号推理和概念清晰度至关重要。因此，需要一种新的方法来评估LLM在教育中的表现。

Method: 本研究引入了一个新颖的可解释性框架，结合了推理流程提取和将解决方案分解为语义标记的操作和概念，并使用提示消融分析来评估输入显著性和输出稳定性。

Result: 研究发现，LLM经常产生语法流畅但概念有误的解决方案，其推理模式对提示措辞和输入变化敏感。该框架能够对推理失败进行细粒度诊断，并支持课程对齐和可解释AI辅助反馈工具的设计。

Conclusion: 本研究提出了一个结构化、量化且具有教育基础的框架，用于解释数学教育中的LLM推理，为STEM学习环境中AI的透明和负责任部署奠定了基础。

Abstract: Large Language Models (LLMs) are increasingly being used in education, yet
their correctness alone does not capture the quality, reliability, or
pedagogical validity of their problem-solving behavior, especially in
mathematics, where multistep logic, symbolic reasoning, and conceptual clarity
are critical. Conventional evaluation methods largely focus on final answer
accuracy and overlook the reasoning process. To address this gap, we introduce
a novel interpretability framework for analyzing LLM-generated solutions using
undergraduate calculus problems as a representative domain. Our approach
combines reasoning flow extraction and decomposing solutions into semantically
labeled operations and concepts with prompt ablation analysis to assess input
salience and output stability. Using structured metrics such as reasoning
complexity, phrase sensitivity, and robustness, we evaluated the model behavior
on real Calculus I to III university exams. Our findings revealed that LLMs
often produce syntactically fluent yet conceptually flawed solutions, with
reasoning patterns sensitive to prompt phrasing and input variation. This
framework enables fine-grained diagnosis of reasoning failures, supports
curriculum alignment, and informs the design of interpretable AI-assisted
feedback tools. This is the first study to offer a structured, quantitative,
and pedagogically grounded framework for interpreting LLM reasoning in
mathematics education, laying the foundation for the transparent and
responsible deployment of AI in STEM learning environments.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [85] [Metrics and evaluations for computational and sustainable AI efficiency](https://arxiv.org/abs/2510.17885)
*Hongyuan Liu,Xinyang Liu,Guosheng Hu*

Main category: cs.PF

TL;DR: 本文提出了一种统一且可重复的人工智能模型推理方法，该方法在现实服务条件下集成了计算和环境指标，通过系统地测量延迟和吞吐量分布、能耗和位置调整后的碳排放，提供了一个务实的、碳意识的评估。通过应用这种方法，我们建立了一个严格的基准测试框架，生成了决策就绪的帕累托前沿，明确了准确度、延迟、能源和碳之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展对计算能力的需求前所未有的增长，但评估已部署模型的性能、效率和环境影响的方法仍然零散。当前的方法往往无法提供全面的视角，使得在异构硬件、软件堆栈和数值精度之间比较和优化系统变得困难。

Method: 我们提出了一种统一且可重复的人工智能模型推理方法，该方法在现实的服务条件下集成了计算和环境指标。我们的框架通过系统地测量延迟和吞吐量分布、能耗和位置调整后的碳排放，提供了务实的、碳意识的评估，同时保持匹配的准确性约束以进行有效的比较。

Result: 我们将这种方法应用于跨多种硬件平台的多精度模型，从数据中心加速器如GH200到消费级GPU如RTX 4090，在主流软件堆栈如PyTorch、TensorRT和ONNX Runtime上运行。通过系统地分类这些因素，我们的工作建立了一个严格的基准测试框架，生成了决策就绪的帕累托前沿，明确了准确度、延迟、能源和碳之间的权衡。

Conclusion: 我们的工作建立了一个严格的基准测试框架，生成了决策就绪的帕累托前沿，明确了准确度、延迟、能源和碳之间的权衡。配套的开源代码使独立验证成为可能，并促进了采用，使研究人员和从业者能够为可持续的人工智能部署做出基于证据的决策。

Abstract: The rapid advancement of Artificial Intelligence (AI) has created
unprecedented demands for computational power, yet methods for evaluating the
performance, efficiency, and environmental impact of deployed models remain
fragmented. Current approaches often fail to provide a holistic view, making it
difficult to compare and optimise systems across heterogeneous hardware,
software stacks, and numeric precisions. To address this gap, we propose a
unified and reproducible methodology for AI model inference that integrates
computational and environmental metrics under realistic serving conditions. Our
framework provides a pragmatic, carbon-aware evaluation by systematically
measuring latency and throughput distributions, energy consumption, and
location-adjusted carbon emissions, all while maintaining matched accuracy
constraints for valid comparisons. We apply this methodology to multi-precision
models across diverse hardware platforms, from data-centre accelerators like
the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream
software stacks including PyTorch, TensorRT, and ONNX Runtime. By
systematically categorising these factors, our work establishes a rigorous
benchmarking framework that produces decision-ready Pareto frontiers,
clarifying the trade-offs between accuracy, latency, energy, and carbon. The
accompanying open-source code enables independent verification and facilitates
adoption, empowering researchers and practitioners to make evidence-based
decisions for sustainable AI deployment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [86] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: 本文研究了LLM在处理结构化数据和遵循语法规则方面的能力如何导致其脆弱性，并提出了BreakFun方法来利用这种脆弱性。同时，我们还提出了一种防御措施来缓解这种攻击。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在处理结构化数据和遵循语法规则方面的能力，以及这种能力如何使其变得脆弱。

Method: 我们通过BreakFun方法研究了LLM的脆弱性，该方法利用了LLM对结构化模式的遵循。BreakFun包含一个三部分的提示，结合了一个无辜的框架和一个思维链的干扰，以及一个核心的“特洛伊模式”——一个精心设计的数据结构，迫使模型生成有害内容。为了应对这一问题，我们引入了对抗提示分解防护措施，使用第二个LLM进行“字面转录”，以提取所有可读文本来隔离和揭示用户的真正有害意图。

Result: 我们在JailbreakBench上对13个基础和专有模型进行了测试，平均成功率达到89%，并在几个著名模型上达到了100%的攻击成功率。严格的消融研究证实了特洛伊模式是攻击的主要因果因素。我们的概念验证防护措施在攻击中表现出高有效性，验证了针对欺骗模式是可行的缓解策略。

Conclusion: 我们的工作揭示了LLM的核心优势如何被转化为关键弱点，为构建更稳健对齐的模型提供了新的视角。

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [87] [PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits](https://arxiv.org/abs/2510.17947)
*Neeladri Bhuiya,Madhav Aggarwal,Diptanshu Purwar*

Main category: cs.CR

TL;DR: 本文提出了PLAGUE框架，用于设计多轮攻击，通过三个阶段系统地探索多轮攻击家族。评估表明，使用PLAGUE设计的红队代理在多个领先模型上显著提高了攻击成功率，展示了其在模型漏洞评估中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM的能力不断提高，但它们在多轮场景中越来越容易受到越狱攻击，其中有害意图可以微妙地注入对话中以产生不良结果。虽然单轮攻击已被广泛研究，但多轮攻击的适应性、效率和效果仍然是关键挑战。

Method: 我们提出了PLAGUE，一个受终身学习代理启发的新型插件式框架，用于设计多轮攻击。PLAGUE将多轮攻击的生命周期分解为三个精心设计的阶段（Primer、Planner和Finisher），以系统且信息丰富的探索多轮攻击家族。

Result: 评估显示，使用PLAGUE设计的红队代理实现了最先进的越狱结果，在较少或相当的查询预算下，攻击成功率（ASR）提高了30%以上。特别是，PLAGUE在OpenAI的o3上实现了81.4%的ASR（基于StrongReject），在Claude的Opus 4.1上实现了67.3%的ASR，这两个模型在安全文献中被认为对越狱具有高度抵抗力。

Conclusion: 我们的工作提供了工具和见解，以理解计划初始化、上下文优化和终身学习在构建多轮攻击中的重要性，从而进行全面的模型漏洞评估。

Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

</details>


### [88] [Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption](https://arxiv.org/abs/2510.18333)
*Yepeng Liu,Xuandong Zhao,Dawn Song,Gregory W. Wornell,Yuheng Bu*

Main category: cs.CR

TL;DR: 本文讨论了LLM水印在实际应用中的障碍，并提出通过激励对齐的方法来提高其可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管在大型语言模型（LLM）的水印算法方面取得了进展，但实际部署仍然有限。作者认为这种差距源于LLM提供者、平台和终端用户之间的激励不一致。

Method: 本文通过分析模型水印、LLM文本水印和上下文水印三种方法，探讨了它们在不同场景下的适用性和挑战。

Result: 本文提出了针对特定应用场景的激励对齐水印设计原则，并指出了未来的研究方向。

Conclusion: 本文认为，LLM水印的实际采用需要在特定应用领域内对利益相关者的激励进行对齐，并促进积极的社区参与。

Abstract: Despite progress in watermarking algorithms for large language models (LLMs),
real-world deployment remains limited. We argue that this gap stems from
misaligned incentives among LLM providers, platforms, and end users, which
manifest as four key barriers: competitive risk, detection-tool governance,
robustness concerns and attribution issues. We revisit three classes of
watermarking through this lens. \emph{Model watermarking} naturally aligns with
LLM provider interests, yet faces new challenges in open-source ecosystems.
\emph{LLM text watermarking} offers modest provider benefit when framed solely
as an anti-misuse tool, but can gain traction in narrowly scoped settings such
as dataset de-contamination or user-controlled provenance. \emph{In-context
watermarking} (ICW) is tailored for trusted parties, such as conference
organizers or educators, who embed hidden watermarking instructions into
documents. If a dishonest reviewer or student submits this text to an LLM, the
output carries a detectable watermark indicating misuse. This setup aligns
incentives: users experience no quality loss, trusted parties gain a detection
tool, and LLM providers remain neutral by simply following watermark
instructions. We advocate for a broader exploration of incentive-aligned
methods, with ICW as an example, in domains where trusted parties need reliable
tools to detect misuse. More broadly, we distill design principles for
incentive-aligned, domain-specific watermarking and outline future research
directions. Our position is that the practical adoption of LLM watermarking
requires aligning stakeholder incentives in targeted application domains and
fostering active community engagement.

</details>

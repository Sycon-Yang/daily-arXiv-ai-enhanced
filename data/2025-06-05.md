<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 110]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems](https://arxiv.org/abs/2506.03259)
*Michael E. Garcia-Alcoser,Mobina GhojoghNejad,Fakrul Islam Tushar,David Kim,Kyle J. Lafata,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CL

TL;DR: 这项研究表明，轻量级大型语言模型在自动化CT放射学报告疾病标注方面表现优于基于规则的方法，并且在零样本提示下可以跨器官系统泛化。


<details>
  <summary>Details</summary>
Motivation: 这项研究旨在评估大型语言模型（LLMs）在自动化CT放射学报告疾病标注方面的有效性。我们将基于规则的算法（RBA）、RadBERT以及三种轻量级开源权重的大型语言模型进行比较，用于胸部、腹部和骨盆（CAP）CT报告的多疾病标记。

Method: 本研究回顾性分析了来自29,540名患者的40,833份CT报告，其中三组器官系统中共有1,789份CAP报告被手动注释。使用Cohen's Kappa和微观/宏观平均F1分数评估性能。三种开源权重的大型语言模型通过零样本提示进行了测试。

Result: 在Duke CAP报告的12,197份报告中，Llama-3.1 8B和Gemma-3 27B显示出了最高的协议一致性（κ中位数：0.87）。在人工注释的数据集上，Gemma-3 27B实现了最高的宏观F1得分（0.82），其次是Llama-3.1 8B（0.79），而RBA得分最低（0.64）。在CT-RATE数据集（仅限肺/胸膜）上，Llama-3.1 8B表现最佳（0.91），Gemma-3 27B紧随其后（0.89）。性能差异主要由于不同的标注实践，特别是对于肺不张。

Conclusion: 轻量级大型语言模型在CT报告注释方面表现优于基于规则的方法，并且在零样本提示下可以跨器官系统泛化。然而，单一标签无法捕捉报告语言的所有细微差别。大型语言模型可以提供一个灵活、高效的解决方案，与临床判断和用户需求保持一致。

Abstract: Purpose: This study aims to evaluate the effectiveness of large language
models (LLMs) in automating disease annotation of CT radiology reports. We
compare a rule-based algorithm (RBA), RadBERT, and three lightweight
open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)
CT reports.
  Materials and Methods: This retrospective study analyzed 40,833 CT reports
from 29,540 patients, with 1,789 CAP reports manually annotated across three
organ systems. External validation was conducted using the CT-RATE dataset.
Three open-weight LLMs were tested with zero-shot prompting. Performance was
evaluated using Cohen's Kappa and micro/macro-averaged F1 scores.
  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and
Gemma-3 27B showed the highest agreement ($\kappa$ median: 0.87). On the
manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed
by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE
dataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3
27B close behind (0.89). Performance differences were mainly due to differing
labeling practices, especially for lung atelectasis.
  Conclusion: Lightweight LLMs outperform rule-based methods for CT report
annotation and generalize across organ systems with zero-shot prompting.
However, binary labels alone cannot capture the full nuance of report language.
LLMs can provide a flexible, efficient solution aligned with clinical judgment
and user needs.

</details>


### [2] [A conclusive remark on linguistic theorizing and language modeling](https://arxiv.org/abs/2506.03268)
*Cristiano Chesi*

Main category: cs.CL

TL;DR: Final remarks on replies to a linguistics paper.


<details>
  <summary>Details</summary>
Motivation: To summarize and conclude the discussion on the target paper.

Method: Compilation and analysis of replies.

Result: Consolidated insights from various responses.

Conclusion: Closure of the discussion on the target linguistics paper.

Abstract: This is the final remark on the replies received to my target paper in the
Italian Journal of Linguistics

</details>


### [3] [FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes](https://arxiv.org/abs/2506.03278)
*Christodoulos Constantinides,Dhaval Patel,Shuxin Lin,Claudio Guerrero,Sunil Dagajirao Patil,Jayant Kalagnanam*

Main category: cs.CL

TL;DR: FailureSensorIQ is a new benchmarking system for evaluating Large Language Models' reasoning capabilities in complex industrial scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to reason about failure modes and sensor data in Industry 4.0.

Method: Evaluates LLMs using various analyses and real-world case studies.

Result: Closed-source models show strong reasoning capabilities but have fragilities and knowledge gaps.

Conclusion: FailureSensorIQ provides valuable insights into LLMs' performance in industrial settings.

Abstract: We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)
benchmarking system designed to assess the ability of Large Language Models
(LLMs) to reason and understand complex, domain-specific scenarios in Industry
4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects
of reasoning through failure modes, sensor data, and the relationships between
them across various industrial assets. Through this work, we envision a
paradigm shift where modeling decisions are not only data-driven using
statistical tools like correlation analysis and significance tests, but also
domain-driven by specialized LLMs which can reason about the key contributors
and useful patterns that can be captured with feature engineering. We evaluate
the Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and
Mistral-on FailureSensorIQ from different lens using
Perturbation-Uncertainty-Complexity analysis, Expert Evaluation study,
Asset-Specific Knowledge Gap analysis, ReAct agent using external
knowledge-bases. Even though closed-source models with strong reasoning
capabilities approach expert-level performance, the comprehensive benchmark
reveals a significant drop in performance that is fragile to perturbations,
distractions, and inherent knowledge gaps in the models. We also provide a
real-world case study of how LLMs can drive the modeling decisions on 3
different failure prediction datasets related to various assets. We release:
(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ
benchmark and Hugging Face leaderboard based on MCQA built from non-textual
data found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature
selection scikit-learn pipeline. The software is available at
https://github.com/IBM/FailureSensorIQ.

</details>


### [4] [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)
*Jiuding Sun,Sidharth Baskaran,Zhengxuan Wu,Michael Sklar,Christopher Potts,Atticus Geiger*

Main category: cs.CL

TL;DR: Introduce HyperSteer, an end-to-end trained hypernetwork-based method that generates steering vectors, outperforming current state-of-the-art methods and performing equivalently to steering-via-prompting.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of unsupervised and supervised methods for steering language models.

Method: HyperSteer uses hypernetwork-based architectures trained end-to-end to generate steering vectors conditioned on natural language steering prompts and the internals of the steered LM.

Result: HyperSteer surpasses the performance of existing methods and performs as well as steering-via-prompting.

Conclusion: Scaling HyperSteer with thousands of steering prompts outperforms state-of-the-art activation steering methods.

Abstract: Steering language models (LMs) by modifying internal activations is a popular
approach for controlling text generation. Unsupervised dictionary learning
methods, e.g., sparse autoencoders, can be scaled to produce many steering
vectors, but lack guarantees on the individual efficacy of each vector and
control over the coverage of relevant steering tasks. In contrast, supervised
methods for constructing steering vectors are targeted and effective, but
require more data collection and training for each additional steering vector
produced. In this work, we introduce HyperSteer, a family of hypernetwork-based
architectures which are trained end-to-end to generate steering vectors
conditioned on the natural language steering prompts and the internals of the
steered LM. In our evaluations, we show that scaling HyperSteer with thousands
of steering prompts exceeds the performance of state-of-the-art activation
steering methods, even on steering prompts never seen during training.
Moreover, HyperSteer performs on par with steering-via-prompting.

</details>


### [5] [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295)
*Yubo Wang,Ping Nie,Kai Zou,Lijun Wu,Wenhu Chen*

Main category: cs.CL

TL;DR: Demonstrating that Critique Fine-Tuning on a single problem can effectively enhance the reasoning abilities of large language models, showing improvements comparable to reinforcement learning but with much lower computational costs.


<details>
  <summary>Details</summary>
Motivation: To find a more efficient way to unleash the reasoning potential of powerful base LLMs instead of using expensive and unstable reinforcement learning.

Method: Construct critique data by collecting diverse model-generated solutions to a single problem and use teacher LLMs to provide detailed critiques. Then fine-tune Qwen and Llama family models on the CFT data.

Result: Significant performance gains across diverse reasoning tasks with just 5 GPU hours of training.

Conclusion: Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs.

Abstract: We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess
immense reasoning potential inherited from the pre-training stage. With
reinforcement learning (RL), these models can improve dramatically on reasoning
tasks. Recent studies have shown that even RL on a single problem can unleash
these models' reasoning capabilities. However, RL is not only expensive but
also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a
critical question: Is there a more efficient way to unleash the reasoning
potential of these powerful base LLMs? In this work, we demonstrate that
Critique Fine-Tuning (CFT) on only one problem can effectively unleash the
reasoning potential of LLMs. Our method constructs critique data by collecting
diverse model-generated solutions to a single problem and using teacher LLMs to
provide detailed critiques. We fine-tune Qwen and Llama family models, ranging
from 1.5B to 14B parameters, on the CFT data and observe significant
performance gains across diverse reasoning tasks. For example, with just 5 GPU
hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six
math benchmarks and 16% on three logic reasoning benchmarks. These results are
comparable to or even surpass the results from RL with 20x less compute.
Ablation studies reveal the robustness of one-shot CFT across different prompt
problems. These results highlight one-shot CFT as a simple, general, and
compute-efficient approach to unleashing the reasoning capabilities of modern
LLMs.

</details>


### [6] [From Instructions to ODRL Usage Policies: An Ontology Guided Approach](https://arxiv.org/abs/2506.03301)
*Daham M. Mustafa,Abhishek Nadgeri,Diego Collarana,Benedikt T. Arnold,Christoph Quix,Christoph Lange,Stefan Decker*

Main category: cs.CL

TL;DR: This study introduces an approach using large language models like GPT-4 to automatically create ODRL usage policies from natural language instructions. The research demonstrates high accuracy (up to 91.95%) in knowledge graphs generated from 12 complex use cases.


<details>
  <summary>Details</summary>
Motivation: To develop a method that uses existing ontology documentation to guide policy generation automatically.

Method: Using the ODRL ontology and its documentation within prompts to adapt the ontology and its documentation for knowledge graph construction.

Result: Achieved up to 91.95% accuracy in the resulting knowledge graph from 12 use cases of varying complexity.

Conclusion: The curated version of existing ontology documentation effectively guides policy generation.

Abstract: This study presents an approach that uses large language models such as GPT-4
to generate usage policies in the W3C Open Digital Rights Language ODRL
automatically from natural language instructions. Our approach uses the ODRL
ontology and its documentation as a central part of the prompt. Our research
hypothesis is that a curated version of existing ontology documentation will
better guide policy generation. We present various heuristics for adapting the
ODRL ontology and its documentation to guide an end-to-end KG construction
process. We evaluate our approach in the context of dataspaces, i.e.,
distributed infrastructures for trustworthy data exchange between multiple
participating organizations for the cultural domain. We created a benchmark
consisting of 12 use cases of varying complexity. Our evaluation shows
excellent results with up to 91.95% accuracy in the resulting knowledge graph.

</details>


### [7] [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)
*Mustafa Eyceoz,Nikhil Shivakumar Nayak,Hao Wang,Ligong Han,Akash Srivastava*

Main category: cs.CL

TL;DR: A new method called Hopscotch is proposed to identify and skip unnecessary attention blocks in causal language models without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency by skipping unnecessary attention blocks in causal language models.

Method: Jointly optimize which blocks to skip and how to scale outputs of the remaining layers with lightweight trainable scaling parameters.

Result: Achieves less than a 2% drop in performance when skipping four attention blocks on Llama-3.1-8B and Qwen2.5-7B.

Conclusion: Hopscotch is a simple and effective method that can be used with existing model compression techniques.

Abstract: Modern causal language models stack many attention blocks to improve
performance, but not all blocks are necessary for every task. We propose
Hopscotch, a simple yet effective method that identifies and skips attention
blocks with least contributions to a task and adapts to preserve output
quality. Hopscotch jointly optimizes which blocks to skip and how to scale the
outputs of the remaining layers. By introducing lightweight, trainable scaling
parameters to attention and MLP blocks, it mitigates distribution shifts in
hidden states caused by removing attention blocks. Hopscotch does not modify
model weights or require access to pretraining or instruction-tuning data, and
is compatible with existing model compression techniques. When applied to
$\texttt{Llama-3.1-8B}$ and $\texttt{Qwen2.5-7B}$, Hopscotch achieves less than
a 2% drop in performance even after skipping four attention blocks.

</details>


### [8] [The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing](https://arxiv.org/abs/2506.03310)
*Guillermo Marco,Julio Gonzalo,Víctor Fresno*

Main category: cs.CL

TL;DR: This study investigates why different studies comparing AI-generated and human-authored literary texts yield conflicting results. It discovers that these differences are due to reader preferences rather than the intrinsic quality of the texts.


<details>
  <summary>Details</summary>
Motivation: To understand why there are conflicting results in studies comparing AI-generated and human-authored literary texts.

Method: Extracting textual features from five public datasets and modeling individual reader preferences to derive feature importance vectors.

Result: Reader vectors cluster into two profiles: 'surface-focused readers' and 'holistic readers'. The results show that measurements of literary quality depend on how text features align with each reader's preferences.

Conclusion: The study suggests that reader-sensitive evaluation frameworks should be used in the field of creative text generation.

Abstract: Recent studies comparing AI-generated and human-authored literary texts have
produced conflicting results: some suggest AI already surpasses human quality,
while others argue it still falls short. We start from the hypothesis that such
divergences can be largely explained by genuine differences in how readers
interpret and value literature, rather than by an intrinsic quality of the
texts evaluated. Using five public datasets (1,471 stories, 101 annotators
including critics, students, and lay readers), we (i) extract 17 reference-less
textual features (e.g., coherence, emotional variance, average sentence
length...); (ii) model individual reader preferences, deriving feature
importance vectors that reflect their textual priorities; and (iii) analyze
these vectors in a shared "preference space". Reader vectors cluster into two
profiles: 'surface-focused readers' (mainly non-experts), who prioritize
readability and textual richness; and 'holistic readers' (mainly experts), who
value thematic development, rhetorical variety, and sentiment dynamics. Our
results quantitatively explain how measurements of literary quality are a
function of how text features align with each reader's preferences. These
findings advocate for reader-sensitive evaluation frameworks in the field of
creative text generation.

</details>


### [9] [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/abs/2506.03312)
*Celia Chen,Scotty Beland,Ingo Burghardt,Jill Byczek,William J. Conway,Eric Cotugno,Sadaf Davre,Megan Fletcher,Rajesh Kumar Gnanasekaran,Kristin Hamilton,Marilyn Harbert,Jordan Heustis,Tanaya Jha,Emily Klein,Hayden Kramer,Alex Leitch,Jessica Perkins,Casi Sherman,Celia Sterrn,Logan Stevens,Rebecca Zarrella,Jennifer Golbeck*

Main category: cs.CL

TL;DR: This paper presents a cross-platform dataset of 30,000 hand-coded posts for violent threats and sub-types of violence. Machine learning analysis shows high classification accuracy when trained on one dataset and tested on another or in a merged dataset condition.


<details>
  <summary>Details</summary>
Motivation: To facilitate research into the understanding and detection of malicious content, including violence, by providing a useful, high-quality dataset.

Method: Introducing a cross-platform dataset of 30,000 posts hand-coded for violent threats and sub-types of violence, and performing a machine learning analysis with an existing dataset of violent comments from YouTube.

Result: High classification accuracy is achieved both by training on one dataset and testing on the other, and in a merged dataset condition.

Conclusion: These results have implications for content-classification strategies and for understanding violent content across social media.

Abstract: Violent threats remain a significant problem across social media platforms.
Useful, high-quality data facilitates research into the understanding and
detection of malicious content, including violence. In this paper, we introduce
a cross-platform dataset of 30,000 posts hand-coded for violent threats and
sub-types of violence, including political and sexual violence. To evaluate the
signal present in this dataset, we perform a machine learning analysis with an
existing dataset of violent comments from YouTube. We find that, despite
originating from different platforms and using different coding criteria, we
achieve high classification accuracy both by training on one dataset and
testing on the other, and in a merged dataset condition. These results have
implications for content-classification strategies and for understanding
violent content across social media.

</details>


### [10] [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)
*Aldan Creo,Héctor Cerezo-Costas,Pedro Alonso-Doval,Maximiliano Hormazábal-Lagos*

Main category: cs.CL

TL;DR: We propose 'Ask a Local', a new method for detecting hallucinations in large language models. This method leverages the concept that specialized models are more surprised by inaccuracies in their domain, using perplexity distribution divergence to spot potentially hallucinated parts.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations in large language models which generate plausible but factually incorrect information.

Method: Computing divergence between perplexity distributions of language-specialized models to identify potentially hallucinated spans.

Result: The method shows consistent performance across 14 languages, with high Intersection-over-Union (IoU) scores and comparable Spearman correlation values. It performs especially well on Italian and Catalan.

Conclusion: Our method is effective in detecting hallucinations in multilingual contexts without needing language-specific adaptations or additional training.

Abstract: Hallucinations in large language models (LLMs) - instances where models
generate plausible but factually incorrect information - present a significant
challenge for AI.
  We introduce "Ask a Local", a novel hallucination detection method exploiting
the intuition that specialized models exhibit greater surprise when
encountering domain-specific inaccuracies. Our approach computes divergence
between perplexity distributions of language-specialized models to identify
potentially hallucinated spans. Our method is particularly well-suited for a
multilingual context, as it naturally scales to multiple languages without the
need for adaptation, relying on external data sources, or performing training.
Moreover, we select computationally efficient models, providing a scalable
solution that can be applied to a wide range of languages and domains.
  Our results on a human-annotated question-answer dataset spanning 14
languages demonstrate consistent performance across languages, with
Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman
correlation values. Our model shows particularly strong performance on Italian
and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining
cross-lingual effectiveness without language-specific adaptations. We release
our code and architecture to facilitate further research in multilingual
hallucination detection.

</details>


### [11] [A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation](https://arxiv.org/abs/2506.03360)
*Zihui Ma,Lingyao Li,Juan Li,Wenyue Hua,Jingxiao Liu,Qingyuan Feng,Yuki Miura*

Main category: cs.CL

TL;DR: This study proposes a 3M pipeline using multimodal large language models (MLLMs) to assess disaster impacts from social media during earthquakes.


<details>
  <summary>Details</summary>
Motivation: Traditional methods face challenges due to limited ground sensors and delays in official reporting. Social media offers real-time human-centric observations but is multimodal and unstructured.

Method: The 3M pipeline leverages MLLMs to integrate image-text signals from social media.

Result: MLLMs effectively integrate image-text signals and correlate well with ground-truth seismic data, but performance varies with language, epicentral distance, and input modality.

Conclusion: This work shows the potential of MLLMs for disaster assessment and supports future research in applying them to real-time crisis contexts.

Abstract: Rapid, fine-grained disaster damage assessment is essential for effective
emergency response, yet remains challenging due to limited ground sensors and
delays in official reporting. Social media provides a rich, real-time source of
human-centric observations, but its multimodal and unstructured nature presents
challenges for traditional analytical methods. In this study, we propose a
structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that
leverages multimodal large language models (MLLMs) to assess disaster impacts.
We evaluate three foundation models across two major earthquake events using
both macro- and micro-level analyses. Results show that MLLMs effectively
integrate image-text signals and demonstrate a strong correlation with
ground-truth seismic data. However, performance varies with language,
epicentral distance, and input modality. This work highlights the potential of
MLLMs for disaster assessment and provides a foundation for future research in
applying MLLMs to real-time crisis contexts. The code and data are released at:
https://github.com/missa7481/EMNLP25_earthquake

</details>


### [12] [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)
*Yi Xu,Ruining Yang,Yitian Zhang,Yizhou Wang,Jianglin Lu,Mingyuan Zhang,Lili Su,Yun Fu*

Main category: cs.CL

TL;DR: This survey provides a comprehensive overview of integrating language-driven techniques into trajectory prediction using large language models.


<details>
  <summary>Details</summary>
Motivation: The rapid development of large language models has increased interest in applying language-driven techniques to trajectory prediction.

Method: Categorizes recent work into five directions: trajectory prediction via language modeling paradigms, direct trajectory prediction with pretrained language models, language-guided scene understanding, language-driven data generation, and language-based reasoning and interpretability.

Result: Analyzes representative methods, highlights core design choices, and identifies open challenges in each category.

Conclusion: This survey offers a unified perspective on how language can enrich trajectory prediction by bridging natural language processing and trajectory prediction.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in integrating language-driven techniques into trajectory prediction. By
leveraging their semantic and reasoning capabilities, LLMs are reshaping how
autonomous systems perceive, model, and predict trajectories. This survey
provides a comprehensive overview of this emerging field, categorizing recent
work into five directions: (1) Trajectory prediction via language modeling
paradigms, (2) Direct trajectory prediction with pretrained language models,
(3) Language-guided scene understanding for trajectory prediction, (4)
Language-driven data generation for trajectory prediction, (5) Language-based
reasoning and interpretability for trajectory prediction. For each, we analyze
representative methods, highlight core design choices, and identify open
challenges. This survey bridges natural language processing and trajectory
prediction, offering a unified perspective on how language can enrich
trajectory prediction.

</details>


### [13] [DistRAG: Towards Distance-Based Spatial Reasoning in LLMs](https://arxiv.org/abs/2506.03424)
*Nicole R Schneider,Nandini Ramachandran,Kent O'Sullivan,Hanan Samet*

Main category: cs.CL

TL;DR: This paper introduces DistRAG, a novel method enabling Large Language Models (LLMs) to perform spatial reasoning by retrieving relevant spatial information through geodesic distance encoding in a graph.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs with reliable spatial reasoning capabilities, especially about distances, for tasks requiring such skills like POI recommendation and itinerary planning.

Method: Encoding geodesic distances between locations in a graph and retrieving a context subgraph related to the question.

Result: DistRAG allows LLMs to answer distance-based reasoning questions they couldn't handle before.

Conclusion: DistRAG provides a flexible way to incorporate a basic 'world model' into LLMs, complementing their linguistic knowledge.

Abstract: Many real world tasks where Large Language Models (LLMs) can be used require
spatial reasoning, like Point of Interest (POI) recommendation and itinerary
planning. However, on their own LLMs lack reliable spatial reasoning
capabilities, especially about distances. To address this problem, we develop a
novel approach, DistRAG, that enables an LLM to retrieve relevant spatial
information not explicitly learned during training. Our method encodes the
geodesic distances between cities and towns in a graph and retrieves a context
subgraph relevant to the question. Using this technique, our method enables an
LLM to answer distance-based reasoning questions that it otherwise cannot
answer. Given the vast array of possible places an LLM could be asked about,
DistRAG offers a flexible first step towards providing a rudimentary `world
model' to complement the linguistic knowledge held in LLMs.

</details>


### [14] [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)
*Ahmad Dawar Hakimi,Ali Modarressi,Philipp Wicke,Hinrich Schütze*

Main category: cs.CL

TL;DR: This study investigates how a large language model (OLMo-7B) develops its factual knowledge during pre-training by analyzing the roles of attention heads and feed forward networks (FFNs). It shows that initially general components become specialized as training proceeds, with attention heads being more changeable than FFNs. Additionally, it finds that location-based relations are learned faster than name-based ones.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability and reliability of large language models by understanding how they acquire and store factual knowledge.

Method: Analyzing the roles of attention heads and FFNs in the OLMo-7B model during pre-training and tracking their changes over time.

Result: LLMs initially rely on broad, general-purpose components that later specialize. Some components are repurposed once reliable answer predictions are made. Attention heads have higher turnover compared to FFNs. Location-based relations are learned faster than name-based relations.

Conclusion: This analysis provides a mechanistic understanding of knowledge formation in LLMs, emphasizing the adaptive learning process and the impact of task complexity on knowledge acquisition.

Abstract: Understanding how large language models (LLMs) acquire and store factual
knowledge is crucial for enhancing their interpretability and reliability. In
this work, we analyze the evolution of factual knowledge representation in the
OLMo-7B model by tracking the roles of its attention heads and feed forward
networks (FFNs) over the course of pre-training. We classify these components
into four roles: general, entity, relation-answer, and fact-answer specific,
and examine their stability and transitions. Our results show that LLMs
initially depend on broad, general-purpose components, which later specialize
as training progresses. Once the model reliably predicts answers, some
components are repurposed, suggesting an adaptive learning process. Notably,
attention heads display the highest turnover. We also present evidence that
FFNs remain more stable throughout training. Furthermore, our probing
experiments reveal that location-based relations converge to high accuracy
earlier in training than name-based relations, highlighting how task complexity
shapes acquisition dynamics. These insights offer a mechanistic view of
knowledge formation in LLMs.

</details>


### [15] [Culture Matters in Toxic Language Detection in Persian](https://arxiv.org/abs/2506.03458)
*Zahra Bokaei,Walid Magdy,Bonnie Webber*

Main category: cs.CL

TL;DR: This paper explores various methods for detecting toxic language in Persian, highlighting the influence of cultural context on transfer learning effectiveness.


<details>
  <summary>Details</summary>
Motivation: Creating safer online environments by limiting the spread of harmful content, with a focus on Persian language toxic language detection which has been under-explored.

Method: Comparing different methods such as fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning.

Result: Cultural context significantly impacts transfer learning effectiveness; languages similar to Persian yield better results than culturally distinct ones.

Conclusion: Understanding the role of cultural context is crucial for improving cross-lingual transfer learning in toxic language detection.

Abstract: Toxic language detection is crucial for creating safer online environments
and limiting the spread of harmful content. While toxic language detection has
been under-explored in Persian, the current work compares different methods for
this task, including fine-tuning, data enrichment, zero-shot and few-shot
learning, and cross-lingual transfer learning. What is especially compelling is
the impact of cultural context on transfer learning for this task: We show that
the language of a country with cultural similarities to Persian yields better
results in transfer learning. Conversely, the improvement is lower when the
language comes from a culturally distinct country. Warning: This paper contains
examples of toxic language that may disturb some readers. These examples are
included for the purpose of research on toxic detection.

</details>


### [16] [Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2506.03476)
*Chuyuan Li,Raymond Li,Thalia S. Field,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于KNN的演示选择策略Delta-KNN，用于阿尔茨海默病的早期诊断，其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病是一种进行性的神经退行性疾病，会导致痴呆，早期干预可以从分析语言异常中受益。因此，我们探索大型语言模型作为阿尔茨海默病诊断的健康助手的潜力，使用实例学习从患者生成的文本中进行诊断。

Method: 我们引入了Delta-KNN，这是一种新的演示选择策略，以提高实例学习（ICL）的表现。该方法利用delta分数来评估每个训练例子的相对收益，并结合基于KNN的检索器动态选择给定输入的最佳“代表”。

Result: 实验结果表明，传统的相似度基础选择等实例学习方法在阿尔茨海默病诊断方面表现不佳，可能由于这项任务的固有复杂性。而我们的Delta-KNN方法在三个开源LLM上进行的实验中，均表现出色。

Conclusion: 我们的方法在两个阿尔茨海默病检测数据集上的一致性表现优于现有的基于实例学习的基线。特别是当使用Llama-3.1模型时，我们的方法达到了新的最先进的结果，甚至超过了监督分类器。

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that
leads to dementia, and early intervention can greatly benefit from analyzing
linguistic abnormalities. In this work, we explore the potential of Large
Language Models (LLMs) as health assistants for AD diagnosis from
patient-generated text using in-context learning (ICL), where tasks are defined
through a few input-output examples. Empirical results reveal that conventional
ICL methods, such as similarity-based selection, perform poorly for AD
diagnosis, likely due to the inherent complexity of this task. To address this,
we introduce Delta-KNN, a novel demonstration selection strategy that enhances
ICL performance. Our method leverages a delta score to assess the relative
gains of each training example, coupled with a KNN-based retriever that
dynamically selects optimal "representatives" for a given input. Experiments on
two AD detection datasets across three open-source LLMs demonstrate that
Delta-KNN consistently outperforms existing ICL baselines. Notably, when using
the Llama-3.1 model, our approach achieves new state-of-the-art results,
surpassing even supervised classifiers.

</details>


### [17] [APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training](https://arxiv.org/abs/2506.03483)
*Jun Rao,Zepeng Lin,Xuebo Liu,Xiaopeng Ke,Lian Lian,Dong Jin,Shengjun Cheng,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: APT is a novel approach that improves domain-specific performance of LLMs by using self-generated weak data and iterative preference training, ensuring no degradation in generic capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing domain-specific enhancements and general model utility in LLMs.

Method: APT uses only error samples and a small set of retrieved similar samples for targeted training.

Result: APT shows superior performance on downstream tasks compared to other methods while maintaining generic capabilities.

Conclusion: APT is an effective strategy for enhancing domain-specific capabilities without sacrificing broader model applicability.

Abstract: Large Language Models (LLMs) often require domain-specific fine-tuning to
address targeted tasks, which risks degrading their general capabilities.
Maintaining a balance between domain-specific enhancements and general model
utility is a key challenge. This paper proposes a novel approach named APT
(Weakness Case Acquisition and Iterative Preference Training) to enhance
domain-specific performance with self-generated dis-preferred weakness data
(bad cases and similar cases). APT uniquely focuses on training the model using
only those samples where errors occur, alongside a small, similar set of
samples retrieved for this purpose. This targeted training minimizes
interference with the model's existing knowledge base, effectively retaining
generic capabilities. Experimental results on the LLama-2 and Mistral-V0.3
models across various benchmarks demonstrate that APT ensures no reduction in
generic capacity and achieves superior performance on downstream tasks compared
to various existing methods. This validates our method as an effective strategy
for enhancing domain-specific capabilities without sacrificing the model's
broader applicability.

</details>


### [18] [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/abs/2506.03484)
*Melkamu Abay Mersha,Mesay Gemeda Yigezu,Atnafu Lambebo Tonja,Hassan Shakil,Samer Iskander,Olga Kolesnikova,Jugal Kalita*

Main category: cs.CL

TL;DR: 提出一种基于XAI的上下文感知数据增强框架，通过保留任务相关特征和迭代反馈改进模型性能，实验表明其在多种任务和模型上优于基线和传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中标签数据稀缺的问题以及传统数据增强技术引入噪声、语义漂移等问题。

Method: 提出XAI-Guided Context-Aware Data Augmentation框架，利用XAI技术修改非关键特征并保留任务相关特征，通过迭代反馈循环优化增强数据。

Result: XAI-SR-BT和XAI-PR-BT分别提升了6.6%和8.1%的模型准确率，并且比现有方法高出4.8%和5%。

Conclusion: 该研究提供了一种更可控、可解释且上下文感知的数据增强解决方案，解决了现有技术的局限性，并为利用XAI提升AI模型训练提供了新的范式转变。

Abstract: Explainable AI (XAI) has emerged as a powerful tool for improving the
performance of AI models, going beyond providing model transparency and
interpretability. The scarcity of labeled data remains a fundamental challenge
in developing robust and generalizable AI models, particularly for low-resource
languages. Conventional data augmentation techniques introduce noise, cause
semantic drift, disrupt contextual coherence, lack control, and lead to
overfitting. To address these challenges, we propose XAI-Guided Context-Aware
Data Augmentation. This novel framework leverages XAI techniques to modify less
critical features while selectively preserving most task-relevant features. Our
approach integrates an iterative feedback loop, which refines augmented data
over multiple augmentation cycles based on explainability-driven insights and
the model performance gain. Our experimental results demonstrate that XAI-SR-BT
and XAI-PR-BT improve the accuracy of models on hate speech and sentiment
analysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using
the Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform
existing augmentation techniques by 4.8% and 5%, respectively, on the same
dataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform
both baseline and conventional augmentation techniques across all tasks and
models. This study provides a more controlled, interpretable, and context-aware
solution to data augmentation, addressing critical limitations of existing
augmentation techniques and offering a new paradigm shift for leveraging XAI
techniques to enhance AI model training.

</details>


### [19] [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)
*Mingxu Tao,Jie Hu,Mingchuan Yang,Yunhuai Liu,Dongyan Zhao,Yansong Feng*

Main category: cs.CL

TL;DR: A novel method called EpiCoDe is introduced to improve large language model performance in data-scarcity scenarios without additional training. It uses model extrapolation and contrastive decoding to reduce predicted errors and has been proven effective across multiple tasks and models.


<details>
  <summary>Details</summary>
Motivation: To address the issue of high cost in acquiring annotated data for training large language models.

Method: EpiCoDe employs model extrapolation to enhance a fine-tuned model and contrastive decoding to compare logit scores from the extrapolated and vanilla fine-tuned models to reduce predicted errors.

Result: Experiments across three tasks over four different LLMs show that EpiCoDe consistently outperforms existing methods with significant and robust improvement.

Conclusion: EpiCoDe is an effective method to boost model performance in data-scarcity scenarios without extra training. A new theoretical framework was proposed to explain the mechanism behind contrastive decoding.

Abstract: The remarkable performance of Large language models (LLMs) relies heavily on
the availability of abundant high-quality training data. However, the high cost
of acquiring annotated data often prevents models from obtaining capabilities
to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe
that boosts model performance in data-scarcity scenarios without extra
training. We first employ model extrapolation to enhance a finetuned model with
its inferior version, and then adopt contrastive decoding to further reduce
predicted errors, by comparing the logit scores given by the extrapolated and
the vanilla finetuned model. Experiments across three tasks over four different
LLMs show that EpiCoDe consistently outperforms existing methods with
significant and robust improvement. We also propose a new theoretical framework
to reveal the mechanism behind contrastive decoding in data-scarcity scenarios,
which further helps us better understand the effectiveness of EpiCoDe.

</details>


### [20] [Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing](https://arxiv.org/abs/2506.03490)
*Shigeng Chen,Linhao Luo,Zhangchi Qiu,Yanan Cao,Carl Yang,Shirui Pan*

Main category: cs.CL

TL;DR: This paper introduces MedEditBench, a framework for evaluating existing knowledge editing methods in the medical domain. It also proposes SGR-Edit, an improved method utilizing model-derived rationales for editing.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge editing methods have not been thoroughly explored in the complex medical domain, where generalization and interpretability are crucial.

Method: Proposed MedEditBench with a new medical knowledge editing benchmark and three paradigms. Also introduced SGR-Edit, which uses model-derived rationales for editing.

Result: Current KE methods only achieve superficial memorization and fail to generalize. SGR-Edit shows significant improvement over existing KE approaches.

Conclusion: The study provides insights into medical knowledge editing and offers practical guidance for real-world medical applications.

Abstract: Recently, knowledge editing (KE) has emerged as a promising approach to
update specific facts in Large Language Models (LLMs) without the need for full
retraining. Despite the effectiveness in general-domain benchmarks, their
applicability to complex medical domain remains largely unexplored. Medical
knowledge editing is particularly challenging, as it requires LLMs to
internalize the knowledge and generalize to unseen scenarios for effective and
interpretable decision-making. In this work, we propose a novel framework
called MedEditBench to rigorously evaluate the effectiveness of existing KE
methods in the medical domain. In MedEditBench, we introduce a new medical
knowledge editing benchmark as well as three different knowledge editing
paradigms, which are designed to assess the impact of different knowledge
sources for editing. Our findings indicate that current KE methods result in
only superficial memorization of the injected information, failing to
generalize to new scenarios. To overcome this limitation, we present
Self-Generated Rationale Editing (SGR-Edit), which utilizes model-derived
rationales as the target knowledge for editing, thereby uncovering the
underlying reasoning process and demonstrating significant improvements over
existing KE approaches. Additionally, we offer deeper insights into medical
knowledge editing, including the localization of medical knowledge in LLMs and
the impact of sequential editing on evolving knowledge. This could provide
practical guidance for implementing KE methods in real-world medical
applications.

</details>


### [21] [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)
*Yuchen Guo,Zhicheng Dou,Huy H. Nguyen,Ching-Chun Chang,Saku Sugawara,Isao Echizen*

Main category: cs.CL

TL;DR: 提出了一种新的方法来解决参与检测模糊问题，该方法在学术场景中表现优异并具有一定的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的反制措施将AI生成文本的检测视为二元分类任务，缺乏鲁棒性，且忽视了人机协作中的人类参与。

Method: 使用BERTScore作为度量标准来衡量生成过程中的人类参与度，并训练了一个多任务RoBERTa-based回归器来进行解决。

Result: 我们的方法在学术场景模拟中表现出色（F1分数为0.9423，回归器均方误差为0.004），并且在生成模型之间具有一定的泛化能力。

Conclusion: 通过使用BERTScore和多任务RoBERTa-based回归器，我们成功地解决了参与检测模糊问题，并在学术场景下展示了良好的性能。

Abstract: Content creation has dramatically progressed with the rapid advancement of
large language models like ChatGPT and Claude. While this progress has greatly
enhanced various aspects of life and work, it has also negatively affected
certain areas of society. A recent survey revealed that nearly 30% of college
students use generative AI to help write academic papers and reports. Most
countermeasures treat the detection of AI-generated text as a binary
classification task and thus lack robustness. This approach overlooks human
involvement in the generation of content even though human-machine
collaboration is becoming mainstream. Besides generating entire texts, people
may use machines to complete or revise texts. Such human involvement varies
case by case, which makes binary classification a less than satisfactory
approach. We refer to this situation as participation detection obfuscation. We
propose using BERTScore as a metric to measure human involvement in the
generation process and a multi-task RoBERTa-based regressor trained on a token
classification task to address this problem. To evaluate the effectiveness of
this approach, we simulated academic-based scenarios and created a continuous
dataset reflecting various levels of human involvement. All of the existing
detectors we examined failed to detect the level of human involvement on this
dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor
mean squared error of 0.004). Moreover, it demonstrated some generalizability
across generative models. Our code is available at
https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

</details>


### [22] [Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information](https://arxiv.org/abs/2506.03510)
*Seungcheol Park,Sojin Lee,Jongjin Kim,Jinsik Lee,Hyunjik Jo,U Kang*

Main category: cs.CL

TL;DR: SPRINT is a new method for accelerating large language models without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: The slow inference speed of LLMs limits their application despite excellent performance.

Method: SPRINT prunes unnecessary sublayers by considering latency reduction and tunability.

Result: SPRINT improves accuracy-speedup trade-off, achieving up to 23.88% higher accuracy than previous methods.

Conclusion: SPRINT provides an effective way to accelerate LLMs while maintaining high accuracy.

Abstract: How can we accelerate large language models(LLMs) without sacrificing
accuracy? The slow inference speed of LLMs hinders us to benefit from their
remarkable performance in diverse applications. This is mainly because numerous
sublayers are stacked together in LLMs. Sublayer pruning compresses and
expedites LLMs via removing unnecessary sublayers. However, existing sublayer
pruning algorithms are limited in accuracy since they naively select sublayers
to prune, overlooking the different characteristics of each sublayer. In this
paper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability
Information), an accurate sublayer pruning method for LLMs. SPRINT accurately
selects a target sublayer to prune by considering 1) the amount of latency
reduction after pruning and 2) the tunability of sublayers. SPRINT iteratively
prunes redundant sublayers and swiftly tunes the parameters of remaining
sublayers. Experiments show that SPRINT achieves the best accuracy-speedup
trade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense
reasoning benchmarks compared to existing pruning algorithms.

</details>


### [23] [An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals](https://arxiv.org/abs/2506.03519)
*Yangyang Zhao,Ben Niu,Libo Qin,Shihan Wang*

Main category: cs.CL

TL;DR: This paper combines Deep Reinforcement Learning with Evolutionary Algorithms to improve task-oriented dialogue systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of balancing exploration and exploitation in high-dimensional state and action spaces.

Method: Innovative combination of EA's global search with DRL's local optimization, enhanced with an elite individual injection mechanism.

Result: Significant improvement in the balance between exploration and exploitation, boosting performance across four datasets.

Conclusion: The proposed method achieves efficient integration of EA and DRL for task-oriented dialogue policy optimization.

Abstract: Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue
systems to optimize dialogue policy, but it struggles to balance exploration
and exploitation due to the high dimensionality of state and action spaces.
This challenge often results in local optima or poor convergence. Evolutionary
Algorithms (EAs) have been proven to effectively explore the solution space of
neural networks by maintaining population diversity. Inspired by this, we
innovatively combine the global search capabilities of EA with the local
optimization of DRL to achieve a balance between exploration and exploitation.
Nevertheless, the inherent flexibility of natural language in dialogue tasks
complicates this direct integration, leading to prolonged evolutionary times.
Thus, we further propose an elite individual injection mechanism to enhance
EA's search efficiency by adaptively introducing best-performing individuals
into the population. Experiments across four datasets show that our approach
significantly improves the balance between exploration and exploitation,
boosting performance. Moreover, the effectiveness of the EII mechanism in
reducing exploration time has been demonstrated, achieving an efficient
integration of EA and DRL on task-oriented dialogue policy tasks.

</details>


### [24] [TokAlign: Efficient Vocabulary Adaptation via Token Alignment](https://arxiv.org/abs/2506.03523)
*Chong Li,Jiajun Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: TokAlign efficiently replaces the vocabulary of large language models and transfers token-level knowledge between models, improving performance and generalization.


<details>
  <summary>Details</summary>
Motivation: The inefficiency of tokenization in new domains or languages slows down the training and generation of LLMs, and the mismatch in vocabulary hinders deep knowledge transfer between LLMs.

Method: TokAlign replaces the vocabulary of LLMs by learning a one-to-one mapping matrix for token IDs, rearranging and progressively fine-tuning model parameters for the new vocabulary.

Result: TokAlign significantly improves multilingual text compression rates and vocabulary initialization for LLMs, decreasing perplexity from 3.4e2 to 1.2e2 after initialization. It also boosts token-level distillation, improving performance by 4.4% over sentence-level distillation.

Conclusion: This paper proposes TokAlign, a method that improves multilingual text compression rates and vocabulary initialization for large language models, reducing perplexity and demonstrating effectiveness and generalization across multiple parameter scales.

Abstract: Tokenization serves as a foundational step for Large Language Models (LLMs)
to process text. In new domains or languages, the inefficiency of the tokenizer
will slow down the training and generation of LLM. The mismatch in vocabulary
also hinders deep knowledge transfer between LLMs like token-level
distillation. To mitigate this gap, we propose an efficient method named
TokAlign to replace the vocabulary of LLM from the token co-occurrences view,
and further transfer the token-level knowledge between models. It first aligns
the source vocabulary to the target one by learning a one-to-one mapping matrix
for token IDs. Model parameters, including embeddings, are rearranged and
progressively fine-tuned for the new vocabulary. Our method significantly
improves multilingual text compression rates and vocabulary initialization for
LLMs, decreasing the perplexity from 3.4$\text{e}^2$ of strong baseline methods
to 1.2$\text{e}^2$ after initialization. Experimental results on models across
multiple parameter scales demonstrate the effectiveness and generalization of
TokAlign, which costs as few as 5k steps to restore the performance of the
vanilla model. After unifying vocabularies between LLMs, token-level
distillation can remarkably boost (+4.4% than sentence-level distillation) the
base model, costing only 235M tokens.

</details>


### [25] [Seed-Coder: Let the Code Model Curate Data for Itself](https://arxiv.org/abs/2506.03524)
*Yuyu Zhang,Jing Su,Yifan Sun,Chenguang Xi,Xia Xiao,Shen Zheng,Anxiang Zhang,Kaibo Liu,Daoguang Zan,Tao Sun,Jinhua Zhu,Shulin Xin,Dong Huang,Yetao Bai,Lixin Dong,Chao Li,Jianchong Chen,Hanzhi Zhou,Yifan Huang,Guanghan Ning,Xierui Song,Jiaze Chen,Siyao Liu,Kai Shen,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-Coder introduces a series of open-source LLMs that reduce human involvement in code data construction, achieving top-tier performance in multiple code-related tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to minimize human involvement in data construction for large language model pretraining, addressing scalability, subjectivity, and maintenance issues associated with current approaches.

Method: Seed-Coder uses a model-centric data pipeline that primarily utilizes LLMs for scoring and filtering code data. The instruct model is trained through supervised fine-tuning and preference optimization, while the reasoning model employs Long-Chain-of-Thought reinforcement learning.

Result: Seed-Coder achieves state-of-the-art results among open-source models of similar size and even outperforms some larger models.

Conclusion: Seed-Coder demonstrates superior performance in various tasks including code generation, code completion, code editing, code reasoning, and software engineering tasks.

Abstract: Code data in large language model (LLM) pretraining is recognized crucial not
only for code-related tasks but also for enhancing general intelligence of
LLMs. Current open-source LLMs often heavily rely on human effort to produce
their code pretraining data, such as employing hand-crafted filtering rules
tailored to individual programming languages, or using human-annotated data to
train quality filters. However, these approaches are inherently limited in
scalability, prone to subjective biases, and costly to extend and maintain
across diverse programming languages. To address these challenges, we introduce
Seed-Coder, a series of open-source LLMs comprising base, instruct and
reasoning models of 8B size, minimizing human involvement in data construction.
Our code pretraining data is produced by a model-centric data pipeline, which
predominantly leverages LLMs for scoring and filtering code data. The instruct
model is further trained via supervised fine-tuning and preference
optimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)
reinforcement learning to improve multi-step code reasoning. Seed-Coder
achieves state-of-the-art results among open-source models of similar size and
even surpasses some much larger models, demonstrating superior performance in
code generation, code completion, code editing, code reasoning, and software
engineering tasks.

</details>


### [26] [Go-Browse: Training Web Agents with Structured Exploration](https://arxiv.org/abs/2506.03533)
*Apurva Gandhi,Graham Neubig*

Main category: cs.CL

TL;DR: 提出Go-Browse方法，通过结构化探索网络环境自动收集大规模多样化和真实的网页代理数据，优化后的语言模型在WebArena基准测试中取得了21.7%的成功率，超越了GPT-4o mini并刷新了小规模参数模型的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 解决数字代理对环境理解不足的问题，例如在网络浏览代理中，它们可能在不熟悉的网站中迷失方向，不确定需要访问哪些页面来实现目标。

Method: Go-Browse，通过将数据收集视为图搜索问题，实现高效探索，并且能够跨探索阶段复用信息。

Result: 在WebArena基准上收集了包含10K个成功任务解决轨迹和40K个交互步骤的数据集；优化后的7B参数语言模型达到了21.7%的成功率。

Conclusion: Go-Browse是一种有效的网络代理数据收集方法，优化后的语言模型在WebArena基准测试中取得了显著成果。

Abstract: One of the fundamental problems in digital agents is their lack of
understanding of their environment. For instance, a web browsing agent may get
lost in unfamiliar websites, uncertain what pages must be visited to achieve
its goals. To address this, we propose Go-Browse, a method for automatically
collecting diverse and realistic web agent data at scale through structured
exploration of web environments. Go-Browse achieves efficient exploration by
framing data collection as a graph search, enabling reuse of information across
exploration episodes. We instantiate our method on the WebArena benchmark,
collecting a dataset of 10K successful task-solving trajectories and 40K
interaction steps across 100 URLs. Fine-tuning a 7B parameter language model on
this dataset achieves a success rate of 21.7% on the WebArena benchmark,
beating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for
sub-10B parameter models by 2.9%.

</details>


### [27] [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)
*Xiaofeng Zhou,Heyan Huang,Lizi Liao*

Main category: cs.CL

TL;DR: Introduces a Debate and Reflect framework and Tree-structured Direct Preference Optimization to enhance smaller language models' performance across various NLP tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high computational demands of Large Language Models (LLMs) and improve the performance of smaller models.

Method: The method involves creating a novel Debate and Reflect (D&R) framework that facilitates multi-turn debates between smaller models and stronger teacher models to generate actionable feedback. Additionally, a Tree-structured Direct Preference Optimization (T-DPO) is introduced to effectively use these debate logs for training.

Result: The result shows that the approach significantly enhances smaller-model accuracy, robustness, and generalization across diverse NLP benchmarks, surpassing conventional baselines by a significant margin.

Conclusion: This paper concludes that the proposed D&R framework and T-DPO method can effectively improve the performance of smaller language models.

Abstract: Large Language Models (LLMs) continue to set new standards in
knowledge-intensive and complex reasoning tasks, yet their high computational
demands limit widespread adoption. While distilling large models into smaller
ones offers a sustainable solution, current techniques--such as static
knowledge distillation, resource-intensive reinforcement learning from human
feedback, or limited self-reflection--struggle to yield substantial and lasting
performance gains. In this paper, we present a novel Debate and Reflect (D&R)
framework that orchestrates multi-turn debates between smaller models and
stronger teacher models, eliciting actionable feedback (e.g., error analysis,
corrective strategies) to guide student models. Further, we introduce
Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage
these debate logs, organizing interactions into a hierarchical format for
effective training. Empirical evaluations across diverse NLP benchmarks
demonstrate that our approach significantly improves smaller-model accuracy,
robustness, and generalization, outperforming conventional baselines by a large
margin.

</details>


### [28] [BPO: Revisiting Preference Modeling in Direct Preference Optimization](https://arxiv.org/abs/2506.03557)
*Lin Sun,Chuang Liu,Peng Liu,Bingyang Li,Weijia Lu,Ning Wu*

Main category: cs.CL

TL;DR: A novel framework called Balanced Preference Optimization (BPO) is proposed to improve the performance of Direct Preference Optimization (DPO) in aligning large language models with human preferences.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of Degraded Chosen Responses (DCR) caused by DPO's neglect of absolute reward magnitudes.

Method: BPO introduces two key components: balanced reward margin and gap adaptor to balance the optimization of chosen and rejected responses.

Result: Experimental results show that BPO improves accuracy by 10.1%-11.7% across multiple mathematical reasoning tasks compared to DPO and its variants.

Conclusion: BPO effectively resolves DCR issue and outperforms DPO and its variants with only a single line of code modification.

Abstract: Direct Preference Optimization (DPO) have emerged as a popular method for
aligning Large Language Models (LLMs) with human preferences. While DPO
effectively preserves the relative ordering between chosen and rejected
responses through pairwise ranking losses, it often neglects absolute reward
magnitudes. This oversight can decrease the likelihood of chosen responses and
increase the risk of generating out-of-distribution responses, leading to poor
performance. We term this issue Degraded Chosen Responses (DCR).To address this
issue, we propose Balanced Preference Optimization (BPO), a novel framework
that dynamically balances the optimization of chosen and rejected responses
through two key components: balanced reward margin and gap adaptor. Unlike
previous methods, BPO can fundamentally resolve DPO's DCR issue, without
introducing additional constraints to the loss function. Experimental results
on multiple mathematical reasoning tasks show that BPO significantly
outperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%
to 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses
DPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over
Cal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a
single line of code modification, making it simple to implement and fully
compatible with existing DPO-based frameworks.

</details>


### [29] [ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch](https://arxiv.org/abs/2506.03558)
*Jiawei Chen,Xinyan Guan,Qianhao Yuan,Guozhao Mo,Weixiang Zhou,Yaojie Lu,Hongyu Lin,Ben He,Le Sun,Xianpei Han*

Main category: cs.CL

TL;DR: We propose Skeleton-Guided Multi-Turn Dialogue Generation, a framework that improves multi-turn instruction synthesis by modeling human conversational intent. This results in a more coherent dialogue flow and better task completion rates. We also create ConsistentChat, a large multi-turn instruction dataset, which enhances model performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-turn instruction synthesis often suffer from context drift and reduced task completion rates due to lack of cross-turn coherence.

Method: The proposed framework includes two stages: Intent Modeling, which assigns dialogues to intent trajectories, and Skeleton Generation, which constructs user query sequences aligned with the modeled intent.

Result: Models fine-tuned on our constructed ConsistentChat dataset showed improved chat consistency by 20-30% and up to a 15% increase in task success rate compared to models trained on existing datasets.

Conclusion: Our approach effectively addresses the issue of context drift in multi-turn dialogues and demonstrates superior performance on multiple benchmarks.

Abstract: Current instruction data synthesis methods primarily focus on single-turn
instructions and often neglect cross-turn coherence, resulting in context drift
and reduced task completion rates in extended conversations. To address this
limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a
framework that constrains multi-turn instruction synthesis by explicitly
modeling human conversational intent. It operates in two stages: (1) Intent
Modeling, which captures the global structure of human dialogues by assigning
each conversation to one of nine well-defined intent trajectories, ensuring a
coherent and goal-oriented information flow; and (2) Skeleton Generation, which
constructs a structurally grounded sequence of user queries aligned with the
modeled intent, thereby serving as a scaffold that constrains and guides the
downstream instruction synthesis process. Based on this process, we construct
ConsistentChat, a multi-turn instruction dataset with approximately 15,000
multi-turn conversations and 224,392 utterances. Experiments on the Light,
Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat
achieve a 20-30% improvement in chat consistency and up to a 15% increase in
task success rate, significantly outperforming models trained on existing
single-turn and multi-turn instruction datasets.

</details>


### [30] [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)
*Langlin Huang,Chengsong Huang,Jixuan Leng,Di Huang,Jiaxin Huang*

Main category: cs.CL

TL;DR: 提出了一种新的方法Position Specialists（PosS），用于提高大型语言模型推理中后期位置的草案令牌预测质量，实验结果显示其在多个数据集上有效提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于推测解码加速大型语言模型推理的方法，在后期位置上的草案令牌预测质量下降，因为错误在生成的特征中积累。

Method: 提出了一种称为Position Specialists（PosS）的方法，它由多个专门的草案层组成，每个专门处理特定级别的草案模型特征偏差。

Result: 在六个数据集上进行的实验结果表明，与基线相比，PosS在平均接受长度和加速比方面有所改进。

Conclusion: 实验结果表明，Position Specialists（PosS）在Llama-3-8B-Instruct和Llama-2-13B-chat上有效提升了平均接受长度和加速比。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
using a small draft model to predict multiple tokens, and a large target model
to verify these tokens in parallel. Recent studies leverage the hidden state of
the target model to enhance draft model prediction accuracy. However, existing
methods suffer from the degrading quality of draft token predictions at later
positions, due to error accumulation in draft model generated features. In this
paper, we propose Position Specialists (PosS), which consist of multiple
position-specialized draft layers to generate tokens at assigned position(s).
Position specialists greatly improve token acceptance rate at later positions
per drafting round, as each specialist only needs to focus on handling a
certain level of draft model feature deviation. Experiment results on
Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that
PosS effectively improves over baselines on average acceptance length and
speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.

</details>


### [31] [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)
*Xiaomi LLM-Core Team,:,Zihao Yue,Zhenru Lin,Yifan Song,Weikun Wang,Shuhuai Ren,Shuhao Gu,Shicheng Li,Peidian Li,Liang Zhao,Lei Li,Kainan Bao,Hao Tian,Hailin Zhang,Gang Wang,Dawei Zhu,Cici,Chenhong He,Bowen Ye,Bowen Shen,Zihan Zhang,Zihan Jiang,Zhixian Zheng,Zhichao Song,Zhenbo Luo,Yue Yu,Yudong Wang,Yuanyuan Tian,Yu Tu,Yihan Yan,Yi Huang,Xu Wang,Xinzhe Xu,Xingchen Song,Xing Zhang,Xing Yong,Xin Zhang,Xiangwei Deng,Wenyu Yang,Wenhan Ma,Weiwei Lv,Weiji Zhuang,Wei Liu,Sirui Deng,Shuo Liu,Shimao Chen,Shihua Yu,Shaohui Liu,Shande Wang,Rui Ma,Qiantong Wang,Peng Wang,Nuo Chen,Menghang Zhu,Kangyang Zhou,Kang Zhou,Kai Fang,Jun Shi,Jinhao Dong,Jiebao Xiao,Jiaming Xu,Huaqiu Liu,Hongshen Xu,Heng Qu,Haochen Zhao,Hanglong Lv,Guoan Wang,Duo Zhang,Dong Zhang,Di Zhang,Chong Ma,Chang Liu,Can Cai,Bingquan Xia*

Main category: cs.CL

TL;DR: Two powerful vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, achieve top performance in visual understanding and multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: To develop advanced vision-language models that can excel in various tasks and provide better performance than existing models.

Method: Four-stage pre-training with 2.4 trillion tokens combined with Mixed On-policy Reinforcement Learning (MORL).

Result: MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 tasks and sets new standards in GUI grounding applications. It surpasses larger models on OlympiadBench.

Conclusion: The study highlights the importance of incorporating high-quality reasoning data and mixed RL in pre-training stages, contributing to reproducibility and advancement in the field through an open-source model and evaluation suite.

Abstract: We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language
models delivering state-of-the-art performance in both general visual
understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B
on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing
models with up to 78B parameters. For GUI grounding applications, it sets a new
standard with 56.1 on OSWorld-G, even outperforming specialized models such as
UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)
with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward
signals. We identify the importance of incorporating high-quality reasoning
data with long Chain-of-Thought into pre-training stages, and the benefits of
mixed RL despite challenges in simultaneous multi-domain optimization. We also
contribute a comprehensive evaluation suite covering 50+ tasks to promote
reproducibility and advance the field. The model checkpoints and full
evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.

</details>


### [32] [FreePRM: Training Process Reward Models Without Ground Truth Process Labels](https://arxiv.org/abs/2506.03570)
*Lin Sun,Chuang Liu,Xiaofeng Ma,Tao Yang,Weijia Lu,Ning Wu*

Main category: cs.CL

TL;DR: 提出了一种名为FreePRM的弱监督框架，用于训练过程奖励模型（PRMs），无需访问真实步级标签。实验表明FreePRM在ProcessBench上平均F1得分为53.0%，优于其他开放源码PRMs。


<details>
  <summary>Details</summary>
Motivation: 提高PRMs的性能并减少对昂贵的步级标注的依赖。

Method: 引入FreePRM框架，该框架首先基于最终结果的正确性生成伪步级标签，然后使用缓冲区概率来消除伪标记中的噪声影响。

Result: FreePRM在ProcessBench上实现了53.0%的平均F1得分，比完全监督的PRM高出24.1%，并且在与其他开放源码PRMs相比时也有显著优势。

Conclusion: 这项工作引入了PRM训练的新范式，大大减少了对昂贵步级注释的依赖，同时保持了强大的性能。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated that
Process Reward Models (PRMs) play a crucial role in enhancing model
performance. However, training PRMs typically requires step-level labels,
either manually annotated or automatically generated, which can be costly and
difficult to obtain at scale. To address this challenge, we introduce FreePRM,
a weakly supervised framework for training PRMs without access to ground-truth
step-level labels. FreePRM first generates pseudo step-level labels based on
the correctness of final outcome, and then employs Buffer Probability to
eliminate impact of noise inherent in pseudo labeling. Experimental results
show that FreePRM achieves an average F1 score of 53.0% on ProcessBench,
outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared
to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B
(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by
+10.9%. This work introduces a new paradigm in PRM training, significantly
reducing reliance on costly step-level annotations while maintaining strong
performance.

</details>


### [33] [Exchange of Perspective Prompting Enhances Reasoning in Large Language Models](https://arxiv.org/abs/2506.03573)
*Lin Sun,Can Zhang*

Main category: cs.CL

TL;DR: Introduce a new framework called Exchange-of-Perspective (EoP) to improve the performance of large language models on NLP tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of large language models' inherent comprehension of problems.

Method: Exchange-of-Perspective (EoP)

Result: EoP improves performance on multiple benchmarks including AQuA, Math, and OlympiadBench Maths.

Conclusion: The proposed Exchange-of-Perspective (EoP) framework can significantly enhance the performance of large language models on diverse natural language processing tasks.

Abstract: Large language models (LLMs) have made significant advancements in addressing
diverse natural language processing (NLP) tasks. However, their performance is
often limited by inherent comprehension of problems. To address this
limitation, we propose Exchange-of-Perspective (EoP), a novel framework
designed to exchange perspectives across different definitions of problem, so
that it can break the fixed mindset from any particular formulation of the
question. We conducted extensive and comprehensive experiments on 8 benchmarks.
The results show that EoP can significantly improve performance. For instance,
compared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we
observe a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP
demonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a
3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using
Qwen-2.5-72b.

</details>


### [34] [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)
*Zirui Chen,Xin Wang,Zhao Li,Wenbin Guo,Dongxiao He*

Main category: cs.CL

TL;DR: A bidirectional language model framework named KG-BiLM is proposed to integrate symbolic knowledge graphs with language models for better semantic understanding.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on either graph structure or textual semantics, but there is a need for a unified framework that can capture global KG connectivity, nuanced linguistic context, and discriminative reasoning semantics.

Method: KG-BiLM incorporates Bidirectional Knowledge Attention, Knowledge-Masked Prediction, and Contrastive Graph Semantic Aggregation to fuse structural cues from KGs with the semantic expressiveness of generative transformers.

Result: Experiments show that KG-BiLM surpasses strong baselines in link prediction tasks, particularly on large-scale graphs with complex multi-hop relations.

Conclusion: KG-BiLM demonstrates the effectiveness of unifying structural information and textual semantics for richer semantic understanding.

Abstract: Recent advances in knowledge representation learning (KRL) highlight the
urgent necessity to unify symbolic knowledge graphs (KGs) with language models
(LMs) for richer semantic understanding. However, existing approaches typically
prioritize either graph structure or textual semantics, leaving a gap: a
unified framework that simultaneously captures global KG connectivity, nuanced
linguistic context, and discriminative reasoning semantics. To bridge this gap,
we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues
from KGs with the semantic expressiveness of generative transformers. KG-BiLM
incorporates three key components: (i) Bidirectional Knowledge Attention, which
removes the causal mask to enable full interaction among all tokens and
entities; (ii) Knowledge-Masked Prediction, which encourages the model to
leverage both local semantic contexts and global graph connectivity; and (iii)
Contrastive Graph Semantic Aggregation, which preserves KG structure via
contrastive alignment of sampled sub-graph representations. Extensive
experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong
baselines in link prediction, especially on large-scale graphs with complex
multi-hop relations - validating its effectiveness in unifying structural
information and textual semantics.

</details>


### [35] [Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models](https://arxiv.org/abs/2506.03580)
*Enrico Benedetti,Akiko Aizawa,Florian Boudin*

Main category: cs.CL

TL;DR: Pre-trained Language Models (PLMs) can be used to generate example sentences for L2 Japanese learners. The retrieval approach was preferred over the generative approach.


<details>
  <summary>Details</summary>
Motivation: To provide diverse and proficiency-level-aligned example sentences to foster effective language acquisition.

Method: Utilize PLMs as quality scoring components in a retrieval system and as direct sentence generators using zero-shot learning.

Result: The retrieval approach was preferred by all evaluators, especially for beginner and advanced target proficiency, while the generative approaches received lower scores on average.

Conclusion: PLMs have potential to enhance the adaptability of sentence suggestion systems and improve the language learning journey.

Abstract: Providing example sentences that are diverse and aligned with learners'
proficiency levels is essential for fostering effective language acquisition.
This study examines the use of Pre-trained Language Models (PLMs) to produce
example sentences targeting L2 Japanese learners. We utilize PLMs in two ways:
as quality scoring components in a retrieval system that draws from a newly
curated corpus of Japanese sentences, and as direct sentence generators using
zero-shot learning. We evaluate the quality of sentences by considering
multiple aspects such as difficulty, diversity, and naturalness, with a panel
of raters consisting of learners of Japanese, native speakers -- and GPT-4. Our
findings suggest that there is inherent disagreement among participants on the
ratings of sentence qualities, except for difficulty. Despite that, the
retrieval approach was preferred by all evaluators, especially for beginner and
advanced target proficiency, while the generative approaches received lower
scores on average. Even so, our experiments highlight the potential for using
PLMs to enhance the adaptability of sentence suggestion systems and therefore
improve the language learning journey.

</details>


### [36] [From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models](https://arxiv.org/abs/2506.03592)
*Viktor Hangya,Fabian Küch,Darina Gold*

Main category: cs.CL

TL;DR: 这项工作通过将生成任务转换为更便宜的理解任务，大幅减少了对大型语言模型评估的时间消耗，同时保持了对推理、代码生成等关键能力的有效评估。


<details>
  <summary>Details</summary>
Motivation: 迭代评估大型语言模型（LLMs）在训练过程中的预期能力发展至关重要，但可能会耗费大量时间和计算资源。因此，研究者们希望减轻NLG基准测试的计算负担，以便在模型训练期间监控关键的LLM能力。

Method: 将生成任务（NLG）重新表述为选择性理解任务（NLU），以降低NLG基准测试的计算负担。

Result: 研究发现，原始生成任务和重新表述的理解任务之间具有很强的相关性，在四种能力测试中实现了平均35倍的评估时间减少。

Conclusion: 通过将生成任务重新表述为计算成本更低的选择性理解任务，支持LLM能力评估，并在四种能力上实现了平均35倍的评估时间减少。

Abstract: Iterative evaluation of LLMs during training is essential to ensure expected
capability development, but can be time- and compute-intensive. While NLU
tasks, where the model selects from fixed answer choices, are cheap to
evaluate, essential capabilities like reasoning and code generation rely on the
more time-consuming NLG (token-by-token generation) format. In this work, our
aim is to decrease the computational burden of NLG benchmarks in order to
enable monitoring crucial LLM capabilities during model training. We
reformulate generative tasks into computationally cheaper NLU alternatives. We
test the performance correlation between the original and reformulated tasks
using 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code
generation, factual knowledge and reading comprehension. Our results show a
strong correlation between task formats, supporting capability assessment via
cheaper alternatives and achieving over 35x average reduction in evaluation
time. We plan to publish our benchmark adaptions.

</details>


### [37] [Is linguistically-motivated data augmentation worth it?](https://arxiv.org/abs/2506.03593)
*Ray Groshan,Michael Ginn,Alexis Palmer*

Main category: cs.CL

TL;DR: This study systematically compares linguistically-naive and linguistically-motivated data augmentation strategies for low-resource languages Uspanteko and Arapaho in machine translation and interlinear glossing tasks.


<details>
  <summary>Details</summary>
Motivation: Previous studies did not do a systematic comparison of different data augmentation strategies, leaving doubt on whether linguistically-motivated strategies yield better downstream performance.

Method: Careful and comprehensive comparison of various augmentation strategies for two low-resource languages with different morphological properties.

Result: Linguistically-motivated strategies can be beneficial over naive approaches, but only when the new examples align well with the training data distribution.

Conclusion: The results provide insights into choosing appropriate data augmentation strategies for different scenarios in low-resource language processing.

Abstract: Data augmentation, a widely-employed technique for addressing data scarcity,
involves generating synthetic data examples which are then used to augment
available training data. Researchers have seen surprising success from simple
methods, such as random perturbations from natural examples, where models seem
to benefit even from data with nonsense words, or data that doesn't conform to
the rules of the language. A second line of research produces synthetic data
that does in fact follow all linguistic constraints; these methods require some
linguistic expertise and are generally more challenging to implement. No
previous work has done a systematic, empirical comparison of both
linguistically-naive and linguistically-motivated data augmentation strategies,
leaving uncertainty about whether the additional time and effort of
linguistically-motivated data augmentation work in fact yields better
downstream performance.
  In this work, we conduct a careful and comprehensive comparison of
augmentation strategies (both linguistically-naive and
linguistically-motivated) for two low-resource languages with different
morphological properties, Uspanteko and Arapaho. We evaluate the effectiveness
of many different strategies and their combinations across two important
sequence-to-sequence tasks for low-resource languages: machine translation and
interlinear glossing. We find that linguistically-motivated strategies can have
benefits over naive approaches, but only when the new examples they produce are
not significantly unlike the training data distribution.

</details>


### [38] [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)
*Zetong Tang,Qian Ma,Di Wu*

Main category: cs.CL

TL;DR: Introduces AP-SQL, a novel architecture that improves Text-to-SQL translation in resource-constrained environments by decomposing the task and leveraging prompt engineering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of using resource-intensive open-source models in resource-constrained environments.

Method: The method involves schema filtering, retrieval-augmented text-to-SQL generation, and prompt-driven schema linking and SQL generation.

Result: The result shows that AP-SQL improves schema selection accuracy and enhances the model's reasoning for accurate SQL generation.

Conclusion: The paper concludes by demonstrating the effectiveness of AP-SQL in bridging the gap between resource-efficient small open-source models and powerful closed-source models.

Abstract: Using the best Text-to-SQL methods in resource-constrained environments is
challenging due to their reliance on resource-intensive open-source models.
This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to
bridge the gap between resource-efficient small open-source models and the
powerful capabilities of large closed-source models for Text-to-SQL
translation. Our method decomposes the task into schema filtering,
retrieval-augmented text-to-SQL generation based on in-context examples, and
prompt-driven schema linking and SQL generation. To improve schema selection
accuracy, we fine-tune large language models. Crucially, we also explore the
impact of prompt engineering throughout the process, leveraging
Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly
enhance the model's reasoning for accurate SQL generation. Comprehensive
evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.

</details>


### [39] [Learning to Insert [PAUSE] Tokens for Better Reasoning](https://arxiv.org/abs/2506.03616)
*Eunki Kim,Sangryul Kim,James Thorne*

Main category: cs.CL

TL;DR: A new method called Dynamic Inserting Tokens Training (DIT) is proposed to improve the reasoning capabilities of transformer-based large language models by strategically inserting [PAUSE] tokens where the model confidence is lowest.


<details>
  <summary>Details</summary>
Motivation: Enhancing the reasoning capabilities of transformer-based large language models.

Method: Identifying low-confidence positions within sequences using token log-likelihood and inserting [PAUSE] tokens.

Result: Achieved accuracy gains of up to 4.7%p on GSM8K, 3.23%p on AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets.

Conclusion: The proposed model-based, dynamic approach broadens the scope of research in reasoning.

Abstract: To enhance reasoning capabilities, previous works have explored incorporating
special-purpose tokens into the training process. These strategies strengthen
the learning mechanism of transformer-based large language models (LLMs).
Building on prior research, in which inserting dummy tokens consecutively just
before reasoning steps can enhance effectiveness, we introduce a novel approach
termed Dynamic Inserting Tokens Training (DIT). Our method identifies positions
within sequences where model confidence is lowest according to token
log-likelihood. Strategically inserting [PAUSE] tokens on these positions
bolsters the model's predictive capabilities for subsequent tokens.
Experimental results across diverse datasets and models, from the 2.7B model to
the 8B model, demonstrate that DIT consistently outperforms traditional
fine-tuning and previous token insertion methods. With this simple yet
effective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on
AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work
shows a model-based, dynamic approach rather than a heuristic one, thereby
broadening the scope of research in reasoning.

</details>


### [40] [Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales](https://arxiv.org/abs/2506.03619)
*Ayuto Tsutsumi,Yuu Jinnai*

Main category: cs.CL

TL;DR: This study introduces YokaiEval, a dataset for evaluating cultural awareness of large language models, particularly focusing on Japanese folklore and Yokai. It assesses 31 Japanese and multilingual LLMs, finding that models trained on Japanese data outperform English-centric ones.


<details>
  <summary>Details</summary>
Motivation: To address the lack of cultural knowledge in LLMs regarding non-English speaking communities, particularly focusing on Japanese folklore.

Method: Development of YokaiEval, a multiple-choice question dataset with 809 questions about Yokai, and evaluation of 31 Japanese and multilingual LLMs.

Result: Japanese-trained models, especially those based on Llama-3 after continued pretraining in Japanese, performed better than English-centric models.

Conclusion: Models trained with Japanese language resources perform better in cultural awareness tasks related to Japanese folklore.

Abstract: Although Large Language Models (LLMs) have demonstrated strong language
understanding and generation abilities across various languages, their cultural
knowledge is often limited to English-speaking communities, which can
marginalize the cultures of non-English communities. To address the problem,
evaluation of the cultural awareness of the LLMs and the methods to develop
culturally aware LLMs have been investigated. In this study, we focus on
evaluating knowledge of folktales, a key medium for conveying and circulating
culture. In particular, we focus on Japanese folktales, specifically on
knowledge of Yokai. Yokai are supernatural creatures originating from Japanese
folktales that continue to be popular motifs in art and entertainment today.
Yokai have long served as a medium for cultural expression, making them an
ideal subject for assessing the cultural awareness of LLMs. We introduce
YokaiEval, a benchmark dataset consisting of 809 multiple-choice questions
(each with four options) designed to probe knowledge about yokai. We evaluate
the performance of 31 Japanese and multilingual LLMs on this dataset. The
results show that models trained with Japanese language resources achieve
higher accuracy than English-centric models, with those that underwent
continued pretraining in Japanese, particularly those based on Llama-3,
performing especially well. The code and dataset are available at
https://github.com/CyberAgentA ILab/YokaiEval.

</details>


### [41] [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627)
*Lin Mu,Guowei Chu,Li Ni,Lei Sang,Zhize Wu,Peiquan Jin,Yiwen Zhang*

Main category: cs.CL

TL;DR: A novel prompting strategy named Robustness of Prompting (RoP) is proposed to improve the robustness of Large Language Models (LLMs) against input perturbations.


<details>
  <summary>Details</summary>
Motivation: LLMs are sensitive to input perturbations which can negatively affect their performance.

Method: RoP has two stages: Error Correction and Guidance. Error Correction generates prompts to automatically correct input errors while Guidance generates optimal guidance prompting to steer the model towards more robust and accurate inferences.

Result: RoP improves LLMs' robustness against adversarial perturbations and maintains model accuracy with minimal degradation compared to clean input scenarios.

Conclusion: RoP is a practical and effective approach for enhancing LLM robustness in real-world applications.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks by effectively utilizing a prompting strategy. However, they are
highly sensitive to input perturbations, such as typographical errors or slight
character order errors, which can substantially degrade their performance.
Despite advances in prompting techniques, developing a prompting strategy that
explicitly mitigates the negative impact of such perturbations remains an open
challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a
novel prompting strategy specifically designed to enhance the robustness of
LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error
Correction stage, RoP applies diverse perturbation methods to generate
adversarial examples, which are then used to construct prompts that
automatically correct input errors. In the Guidance stage, RoP generates an
optimal guidance prompting based on the corrected input, steering the model
toward more robust and accurate inferences. Through comprehensive experiments
spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate
that RoP significantly improves LLMs' robustness against adversarial
perturbations. Notably, it maintains model accuracy with only minimal
degradation compared to clean input scenarios, thereby establishing RoP as a
practical and effective approach for enhancing LLM robustness in real-world
applications.

</details>


### [42] [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)
*Zhuohao Yu,Jiali Zeng,Weizheng Gu,Yidong Wang,Jindong Wang,Fandong Meng,Jie Zhou,Yue Zhang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: Introduce generalizable reward models that can adapt to diverse real-world needs.


<details>
  <summary>Details</summary>
Motivation: Current reward models are rigid and cannot adapt to different tasks' requirements.

Method: Propose RewardAnything, which can follow natural language principles without retraining.

Result: RewardAnything achieves state-of-the-art performance in traditional benchmarks and adapts well to novel principles on RABench.

Conclusion: RewardAnything provides a flexible and efficient way to align large language models with various tasks using natural language principles.

Abstract: Reward Models, essential for guiding Large Language Model optimization, are
typically trained on fixed preference datasets, resulting in rigid alignment to
single, implicit preference distributions. This prevents adaptation to diverse
real-world needs-from conciseness in one task to detailed explanations in
another. The standard practice of collecting task-specific preference data and
retraining reward models is resource-intensive, often producing biased rewards,
and limits practical application. We introduce generalizable,
principle-following reward models. We propose that RMs should understand and
adhere to dynamically provided natural language specifications of reward
principles, similar to instruction-following in LLMs. To measure this
capability, we develop RABench, a comprehensive benchmark for RMs focusing on
generalization across diverse principles. Evaluations on RABench reveal poor
generalization of current RMs. As a solution, we present RewardAnything, a
novel RM designed and trained to explicitly follow natural language principles.
We achieve SotA performance with RewardAnything in traditional RM benchmark
simply by specifying a well-defined principle, and results on RABench show we
excel in adapting to novel principles without retraining. Furthermore,
RewardAnything integrates seamlessly with existing RLHF methods and we show by
a case study on how to automatically and efficiently align LLMs with only
natural language principles.

</details>


### [43] [Trustworthy Medical Question Answering: An Evaluation-Centric Survey](https://arxiv.org/abs/2506.03659)
*Yinuo Wang,Robert E. Mercer,Frank Rudzicz,Sudipta Singha Roy,Pengjie Ren,Zhumin Chen,Xindi Wang*

Main category: cs.CL

TL;DR: This survey examines six key dimensions of trustworthiness in medical QA systems powered by large language models, including Factuality, Robustness, Fairness, Safety, Explainability, and Calibration. It reviews how these dimensions are evaluated in existing systems, compares major benchmarks for assessment, and analyzes techniques driving model improvements. The paper also identifies open challenges and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: Ensuring trustworthiness in healthcare question-answering systems is crucial for patient safety, clinical effectiveness, and user confidence, especially as large language models become more integrated into medical settings.

Method: Systematic examination of six key dimensions of trustworthiness in medical QA systems, reviewing evaluations, compiling and comparing benchmarks, analyzing evaluation-guided techniques, and identifying open challenges.

Result: The survey provides insights into evaluating trustworthiness in medical QA systems and highlights areas for improvement, such as scalable expert evaluation, integrated multi-dimensional metrics, and real-world deployment studies.

Conclusion: This paper aims to advance the safe, reliable, and transparent deployment of large language model-powered medical QA systems by addressing trustworthiness across multiple dimensions.

Abstract: Trustworthiness in healthcare question-answering (QA) systems is important
for ensuring patient safety, clinical effectiveness, and user confidence. As
large language models (LLMs) become increasingly integrated into medical
settings, the reliability of their responses directly influences clinical
decision-making and patient outcomes. However, achieving comprehensive
trustworthiness in medical QA poses significant challenges due to the inherent
complexity of healthcare data, the critical nature of clinical scenarios, and
the multifaceted dimensions of trustworthy AI. In this survey, we
systematically examine six key dimensions of trustworthiness in medical QA,
i.e., Factuality, Robustness, Fairness, Safety, Explainability, and
Calibration. We review how each dimension is evaluated in existing LLM-based
medical QA systems. We compile and compare major benchmarks designed to assess
these dimensions and analyze evaluation-guided techniques that drive model
improvements, such as retrieval-augmented grounding, adversarial fine-tuning,
and safety alignment. Finally, we identify open challenges-such as scalable
expert evaluation, integrated multi-dimensional metrics, and real-world
deployment studies-and propose future research directions to advance the safe,
reliable, and transparent deployment of LLM-powered medical QA.

</details>


### [44] [ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling](https://arxiv.org/abs/2506.03665)
*Hernán Maina,Guido Ivetta,Mateo Lione Stuto,Julian Martin Eisenschlos,Jorge Sánchez,Luciana Benotti*

Main category: cs.CL

TL;DR: This paper introduces ROSA, a decoding strategy that improves VQA performance on images with misaligned text, outperforming Greedy decoding by 11.7 absolute points.


<details>
  <summary>Details</summary>
Motivation: Current VQA models struggle with recognizing text in photos taken by visually impaired people due to framing conventions that cause misaligned text. Existing benchmarks under-represent these challenges.

Method: ROSA, a decoding strategy that enhances VQA performance in text-rich images with incorrectly oriented text.

Result: ROSA outperforms Greedy decoding by 11.7 absolute points in the best-performing model.

Conclusion: ROSA addresses the challenges faced by VQA systems in interpreting text in photos taken by visually impaired individuals.

Abstract: Visually impaired people could benefit from Visual Question Answering (VQA)
systems to interpret text in their surroundings. However, current models often
struggle with recognizing text in the photos taken by this population. Through
in-depth interviews with visually impaired individuals, we identified common
framing conventions that frequently result in misaligned text. Existing VQA
benchmarks primarily feature well-oriented text captured by sighted users,
under-representing these challenges. To address this gap, we introduce ROtated
SAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich
images with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7
absolute points in the best-performing model.

</details>


### [45] [Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering](https://arxiv.org/abs/2506.03681)
*Pradeep Rangappa,Andres Carofilis,Jeena Prakash,Shashi Kumar,Sergio Burdisso,Srikanth Madikeri,Esau Villatoro-Tello,Bidisha Sharma,Petr Motlicek,Kadri Hacioglu,Shankar Venkatesan,Saurabh Vyas,Andreas Stolcke*

Main category: cs.CL

TL;DR: Fine-tuning pretrained ASR models for specific domains can be challenging for small organizations with limited resources. This paper proposes a robust approach using data selection pipelines to improve ASR adaptation by filtering pseudo-labels from Whisper and Zipformer models. The method integrates multiple selection strategies such as WER prediction, NER, and CER analysis to extract high-quality training segments. It was evaluated on Whisper and Zipformer using a 7500-hour baseline and compared to a CER-based approach relying on hypotheses from three ASR systems.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning ASR models for specific domains is difficult for small organizations due to limited labeled data and computational resources.

Method: The paper explores different data selection pipelines and proposes an approach that improves ASR adaptation by filtering pseudo-labels generated using Whisper and Zipformer models. The method integrates multiple selection strategies including WER prediction, NER, and CER analysis to extract high-quality training segments.

Result: Fine-tuning on 7500 hours of pseudo-labeled call center data achieved 12.3% WER, while the proposed filtering reduced the dataset to 100 hours (1.4%) with similar performance; a similar trend was observed on Fisher English.

Conclusion: This approach offers a more efficient way for small organizations to fine-tune ASR models for specific domains by significantly reducing the amount of required training data without sacrificing performance.

Abstract: Fine-tuning pretrained ASR models for specific domains is challenging for
small organizations with limited labeled data and computational resources.
Here, we explore different data selection pipelines and propose a robust
approach that improves ASR adaptation by filtering pseudo-labels generated
using Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach
integrates multiple selection strategies -- including word error rate (WER)
prediction, named entity recognition (NER), and character error rate (CER)
analysis -- to extract high-quality training segments. We evaluate our method
on Whisper and Zipformer using a 7500-hour baseline, comparing it to a
CER-based approach relying on hypotheses from three ASR systems. Fine-tuning on
7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our
filtering reduces the dataset to 100 hours (1.4%) with similar performance; a
similar trend is observed on Fisher English.

</details>


### [46] [Robust Preference Optimization via Dynamic Target Margins](https://arxiv.org/abs/2506.03690)
*Jie Sun,Junkang Wu,Jiancan Wu,Zhibo Zhu,Xingyu Lu,Jun Zhou,Lintao Ma,Xiang Wang*

Main category: cs.CL

TL;DR: γ-PO is a novel algorithm improving LLMs' alignment by optimizing reward margins dynamically.


<details>
  <summary>Details</summary>
Motivation: To address the issue of noise affecting the data quality in DPO, which impacts the effectiveness of LLMs alignment.

Method: A dynamic target margin preference optimization algorithm named γ-PO is proposed.

Result: γ-PO shows an average 4.4% improvement over other baselines and is compatible with variants of DPO.

Conclusion: γ-PO improves the alignment of LLMs with better performance across various benchmarks.

Abstract: The alignment of Large Language Models (LLMs) is crucial for ensuring their
safety and reliability in practical applications. Direct Preference
Optimization (DPO) has emerged as an efficient method that directly optimizes
models using preference pairs, significantly reducing resource demands.
However, the effectiveness of DPO heavily depends on the data quality, which is
frequently compromised by noise. In this work, we propose $\gamma$-PO, a
dynamic target margin preference optimization algorithm that adjust reward
margins at the pairwise level. By introducing instance-specific margin
calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those
demonstrating higher reward margins) while suppressing potential noise from
ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible
with variants of DPO that rely on reward margin between preference pairs.
Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an
average 4.4\% improvement over other baselines, setting new benchmarks for
state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code
changes and has a negligible impact on training efficiency, making it a robust
solution for enhancing LLMs alignment. Our codes are available at
\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.

</details>


### [47] [AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism](https://arxiv.org/abs/2506.03700)
*Zhepei Wei,Wei-Lin Chen,Xinyu Zhu,Yu Meng*

Main category: cs.CL

TL;DR: AdaDecode accelerates LLM decoding by adaptively generating tokens at intermediate layers, achieving up to 1.73x speedup without sacrificing output quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding is limited by its sequential token generation process, restricting the ability to fully leverage modern hardware's parallel processing capabilities.

Method: AdaDecode adaptively generates tokens at intermediate layers when confidence is high, enabling parallel computation and reducing decoding latency.

Result: Experiments show that AdaDecode achieves up to 1.73x speedup in decoding throughput while maintaining output parity with standard autoregressive decoding.

Conclusion: AdaDecode is a novel decoding method that accelerates LLM decoding without auxiliary models or changes to the original model parameters, ensuring output consistency.

Abstract: Large language models (LLMs) are increasingly used for long-content
generation (e.g., long Chain-of-Thought reasoning) where decoding efficiency
becomes a critical bottleneck: Autoregressive decoding is inherently limited by
its sequential token generation process, where each token must be generated
before the next can be processed. This sequential dependency restricts the
ability to fully leverage modern hardware's parallel processing capabilities.
Existing methods like speculative decoding and layer skipping offer potential
speedups but have notable drawbacks: speculative decoding relies on an
auxiliary "drafter" model, which can be challenging to acquire and increases
memory overhead, while layer skipping may introduce discrepancies in the
outputs due to the missing key-value cache at skipped layers. In this work, we
propose AdaDecode, which accelerates LLM decoding without requiring auxiliary
models or changes to the original model parameters, while ensuring output
consistency. AdaDecode leverages the insight that many tokens can accurately be
generated at intermediate layers, as further layers often do not significantly
alter predictions once the model reaches a certain confidence. By adaptively
generating tokens at intermediate layers when confidence is high, AdaDecode
enables the next token's computation to begin immediately. The remaining layer
computations for early-predicted tokens are deferred and executed in parallel
with subsequent tokens when needed, maximizing hardware utilization and
reducing decoding latency. A final verification step ensures that early
predictions match the results of standard autoregressive decoding, preserving
output parity. Experiments across diverse generation tasks shows that AdaDecode
consistently achieves superior decoding throughput with up to 1.73x speedup,
while guaranteeing output parity with standard autoregressive decoding.

</details>


### [48] [ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation](https://arxiv.org/abs/2506.03704)
*Pei-Yun Lin,Yen-lung Tsai*

Main category: cs.CL

TL;DR: ScoreRAG is a multi-stage framework that enhances automated news generation by improving accuracy, coherence, and informativeness through retrieval-augmented generation and large language model evaluations.


<details>
  <summary>Details</summary>
Motivation: Current news generation methods often suffer from hallucinations, factual inconsistencies, and lack of domain-specific expertise.

Method: The framework includes retrieval-augmented generation, consistency relevance evaluation, and structured summarization. Relevant documents are retrieved, scored, and filtered before being used to guide the large language model in generating professional news articles.

Result: ScoreRAG improves the quality of generated news articles in terms of accuracy, coherence, informativeness, and professionalism while maintaining stability and consistency.

Conclusion: ScoreRAG offers a novel solution to the challenges in automated news generation using a multi-stage approach.

Abstract: This research introduces ScoreRAG, an approach to enhance the quality of
automated news generation. Despite advancements in Natural Language Processing
and large language models, current news generation methods often struggle with
hallucinations, factual inconsistencies, and lack of domain-specific expertise
when producing news articles. ScoreRAG addresses these challenges through a
multi-stage framework combining retrieval-augmented generation, consistency
relevance evaluation, and structured summarization. The system first retrieves
relevant news documents from a vector database, maps them to complete news
items, and assigns consistency relevance scores based on large language model
evaluations. These documents are then reranked according to relevance, with
low-quality items filtered out. The framework proceeds to generate graded
summaries based on relevance scores, which guide the large language model in
producing complete news articles following professional journalistic standards.
Through this methodical approach, ScoreRAG aims to significantly improve the
accuracy, coherence, informativeness, and professionalism of generated news
articles while maintaining stability and consistency throughout the generation
process. The code and demo are available at:
https://github.com/peiyun2260/ScoreRAG.

</details>


### [49] [MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition](https://arxiv.org/abs/2506.03722)
*Yinfeng Xia,Huiyan Li,Chenyang Le,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: This paper introduces a new prefix-to-prefix training framework for streaming speech recognition using fine-tuned Whisper models. It includes mechanisms for quasi-monotonic alignment and attention, along with a simplified decoding strategy, achieving a balance between latency and quality.


<details>
  <summary>Details</summary>
Motivation: To reduce training costs for speech tasks and address challenges in integrating large pre-trained speech models into streaming systems.

Method: Fine-tuning Whisper models with a prefix-to-prefix training framework, introducing Continuous Integrate-and-Fire for alignment, designing Monotonic Finite Look-ahead Attention, and employing wait-k decoding strategy.

Result: Achieves a controllable trade-off between latency and quality, suitable for various streaming applications.

Conclusion: The proposed method effectively integrates large pre-trained speech models into streaming systems, offering a promising solution for real-time speech recognition tasks.

Abstract: Applying large pre-trained speech models like Whisper has shown promise in
reducing training costs for various speech tasks. However, integrating these
models into streaming systems remains a challenge. This paper presents a novel
prefix-to-prefix training framework for streaming recognition by fine-tuning
the Whisper. We introduce the Continuous Integrate-and-Fire mechanism to
establish a quasi-monotonic alignment between continuous speech sequences and
discrete text tokens. Additionally, we design Monotonic Finite Look-ahead
Attention, allowing each token to attend to infinite left-context and finite
right-context from the speech sequences. We also employ the wait-k decoding
strategy to simplify the decoding process while ensuring consistency between
training and testing. Our theoretical analysis and experiments demonstrate that
this approach achieves a controllable trade-off between latency and quality,
making it suitable for various streaming applications.

</details>


### [50] [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/abs/2506.03723)
*Chaeyun Jang,Moonseok Choi,Yegon Kim,Hyungi Lee,Juho Lee*

Main category: cs.CL

TL;DR: This paper explores uncertainty calibration for chain-of-thought reasoning in large language models, finding that supervised fine-tuning with scalar confidence labels alone can elicit self-verification behaviors.


<details>
  <summary>Details</summary>
Motivation: To improve the safety and reliability of large language models by ensuring their verbalized confidence estimates are accurate.

Method: Supervised fine-tuning with scalar confidence labels.

Result: The model learns to generate longer and self-checking responses for low-confidence queries and more concise answers for high-confidence ones. A rethinking method is proposed to boost performance via test-time scaling based on calibrated uncertainty.

Conclusion: Confidence-aware fine-tuning improves both calibration and accuracy, enhancing interpretability by aligning the model's reasoning path with its confidence.

Abstract: Uncertainty calibration is essential for the safe deployment of large
language models (LLMs), particularly when users rely on verbalized confidence
estimates. While prior work has focused on classifiers or short-form
generation, confidence calibration for chain-of-thought (CoT) reasoning remains
largely unexplored. Surprisingly, we find that supervised fine-tuning with
scalar confidence labels alone suffices to elicit self-verification behavior of
language models, without any explicit reasoning supervision or reinforcement
learning-based rewards. Despite being trained only to produce a verbalized
confidence score without any self-verifying examples, the model learns to
generate longer and self-checking responses for low-confidence queries while
providing more concise answers for high-confidence ones. We further propose a
simple rethinking method that boosts performance via test-time scaling based on
calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such
as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning
improves both calibration and accuracy, while also enhancing interpretability
by aligning the model's reasoning path with its confidence.

</details>


### [51] [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/abs/2506.03735)
*Junling Wang,Anna Rutkiewicz,April Yi Wang,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 提出Math2Visual框架，通过预定义的视觉语言和设计空间自动生成数学应用题的教育性视觉，建立新基准并提供多模态教育内容生产的见解。


<details>
  <summary>Details</summary>
Motivation: 创建数学应用题的视觉辅助工具很费力且缺乏自动化方法支持。

Method: 提出Math2Visual框架，使用预定义的视觉语言和设计空间来生成教育上有意义的视觉。

Result: 构建了一个包含1903个视觉的注释数据集，并评估了文本到图像模型在生成与设计一致的视觉方面的表现，对几个文本到图像模型进行了微调，展示了在教育视觉生成方面的改进。

Conclusion: 这项工作建立了自动产生教育上有意义的视觉的新基准，并提供了关于生产多模态教育内容的关键挑战的见解。

Abstract: Visuals are valuable tools for teaching math word problems (MWPs), helping
young learners interpret textual descriptions into mathematical expressions
before solving them. However, creating such visuals is labor-intensive and
there is a lack of automated methods to support this process. In this paper, we
present Math2Visual, an automatic framework for generating pedagogically
meaningful visuals from MWP text descriptions. Math2Visual leverages a
pre-defined visual language and a design space grounded in interviews with math
teachers, to illustrate the core mathematical relationships in MWPs. Using
Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate
Text-to-Image (TTI) models for their ability to generate visuals that align
with our design. We further fine-tune several TTI models with our dataset,
demonstrating improvements in educational visual generation. Our work
establishes a new benchmark for automated generation of pedagogically
meaningful visuals and offers insights into key challenges in producing
multimodal educational content, such as the misrepresentation of mathematical
relationships and the omission of essential visual elements.

</details>


### [52] [Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services](https://arxiv.org/abs/2506.03761)
*Hongcheng Guo,Zheyong Xie,Shaosheng Cao,Boyang Wang,Weiting Liu,Zheyu Ye,Zhoujun Li,Zuozhu Liu*

Main category: cs.CL

TL;DR: Introduce Pet-Bench, a benchmark for evaluating large language models in pet companionship tasks, revealing performance variations among different models.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLMs for comprehensive pet companionship rather than just basic role-playing interactions.

Method: Benchmarking LLMs with Pet-Bench which includes self-interaction and human-interaction dimensions.

Result: Significant performance variations among 28 LLMs were found and the need for specialized optimization was highlighted.

Conclusion: Pet-Bench is a new benchmark for evaluating LLMs in pet companionship tasks.

Abstract: As interest in using Large Language Models (LLMs) for interactive and
emotionally rich experiences grows, virtual pet companionship emerges as a
novel yet underexplored application. Existing approaches focus on basic pet
role-playing interactions without systematically benchmarking LLMs for
comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated
benchmark that evaluates LLMs across both self-interaction and
human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes
self-evolution and developmental behaviors alongside interactive engagement,
offering a more realistic reflection of pet companionship. It features diverse
tasks such as intelligent scheduling, memory-based dialogues, and psychological
conversations, with over 7,500 interaction instances designed to simulate
complex pet behaviors. Evaluation of 28 LLMs reveals significant performance
variations linked to model size and inherent capabilities, underscoring the
need for specialized optimization in this domain. Pet-Bench serves as a
foundational resource for benchmarking pet-related LLM abilities and advancing
emotionally immersive human-pet interactions.

</details>


### [53] [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.03762)
*Yifeng Gu,Zicong Jiang,Jianxiu Jin,Kailing Guo,Ziyang Zhang,Xiangmin Xu*

Main category: cs.CL

TL;DR: Propose a new method named AhaKV to reduce the bias of accumulated attention score and retain crucial tokens across global context.


<details>
  <summary>Details</summary>
Motivation: The current methods rely on accumulated attention score for token eviction, which is biased and limits model's access to global contextual information.

Method: Adaptively tune the scale of softmax based on the expectation of information entropy of attention scores and utilize the information of value vectors to refine the adaptive score.

Result: Successfully mitigate bias and retain crucial tokens across global context with state-of-the-art results on several benchmark tasks.

Conclusion: AhaKV is a novel approach to reduce the bias of accumulated attention score and improve the performance of LLMs during inference.

Abstract: Large Language Models (LLMs) have significantly advanced the field of
Artificial Intelligence. However, their deployment is resource-intensive, not
only due to the large number of model parameters but also because the
(Key-Value) KV cache consumes a lot of memory during inference. While several
works propose reducing the KV cache by evicting the unnecessary tokens, these
approaches rely on accumulated attention score as eviction score to quantify
the importance of the token. We identify the accumulated attention score is
biased and it decreases with the position of the tokens in the mathematical
expectation. As a result, the retained tokens concentrate on the initial
positions, limiting model's access to global contextual information. To address
this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the
bias of the accumulated attention score by adaptively tuning the scale of
softmax according the expectation of information entropy of attention scores.
To make use of the holistic attention information in self-attention mechanism,
AhaKV utilize the information of value vectors, which is overlooked in previous
works, to refine the adaptive score. We show theoretically that our method is
well suited for bias reduction. We deployed AhaKV on different models with a
fixed cache budget. Experiments show that AhaKV successfully mitigates bias and
retains crucial tokens across global context and achieve state-of-the-art
results against other related work on several benchmark tasks.

</details>


### [54] [ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations](https://arxiv.org/abs/2506.03763)
*Quang Hieu Pham,Thuy Duong Nguyen,Tung Pham,Anh Tuan Luu,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: This paper introduces ClozeMath, a novel approach for fine-tuning large language models on mathematical reasoning using text-infilling tasks. ClozeMath outperforms the Masked Thought baseline across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address limitations of next-word prediction in capturing human-like thinking processes in LLMs.

Method: Introduces ClozeMath, which uses text-infilling to predict masked equations from solutions.

Result: ClozeMath outperforms Masked Thought with improved performance and robustness, especially with Beam Search and Chain-of-Thought decoding.

Conclusion: The proposed ClozeMath method provides a more effective way to enhance LLMs' mathematical reasoning capabilities.

Abstract: The capabilities of large language models (LLMs) have been enhanced by
training on data that reflects human thought processes, such as the
Chain-of-Thought format. However, evidence suggests that the conventional
scheme of next-word prediction may not fully capture how humans learn to think.
Inspired by how humans generalize mathematical reasoning, we propose a new
approach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our
ClozeMath involves a text-infilling task that predicts masked equations from a
given solution, analogous to cloze exercises used in human learning.
Experiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the
strong baseline Masked Thought in performance and robustness, with two
test-time scaling decoding algorithms, Beam Search and Chain-of-Thought
decoding. Additionally, we conduct an ablation study to analyze the effects of
various architectural and implementation choices on our approach.

</details>


### [55] [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](https://arxiv.org/abs/2506.03781)
*Seungcheol Park,Jeongin Bae,Beomseok Kwon,Minjun Kim,Byeongwook Kim,Se Jung Kwon,U Kang,Dongsoo Lee*

Main category: cs.CL

TL;DR: A new method called UniQuanF improves the accuracy of large language model quantization.


<details>
  <summary>Details</summary>
Motivation: To deploy large language models efficiently, it's necessary to quantize them while maintaining accuracy.

Method: UniQuanF combines the strengths of binary-coding quantization and uniform quantization with unified initialization and mapping techniques.

Result: UniQuanF surpasses previous methods, increasing accuracy by up to 4.60% on GSM8K benchmark.

Conclusion: UniQuanF achieves high accuracy in quantizing large language models without additional deployment costs.

Abstract: How can we quantize large language models while preserving accuracy?
Quantization is essential for deploying large language models (LLMs)
efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are
promising quantization schemes that have strong expressiveness and
optimizability, respectively. However, neither scheme leverages both
advantages. In this paper, we propose UniQuanF (Unified Quantization with
Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses
both strong expressiveness and optimizability by unifying the flexible mapping
technique in UQ and non-uniform quantization levels of BCQ. We propose unified
initialization, and local and periodic mapping techniques to optimize the
parameters in UniQuanF precisely. After optimization, our unification theorem
removes computational and memory overhead, allowing us to utilize the superior
accuracy of UniQuanF without extra deployment costs induced by the unification.
Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ
methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.

</details>


### [56] [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)
*Isik Baran Sandan,Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: Knockout Assessment is proposed to improve the accuracy of LLM-based evaluations by using iterative pairwise comparisons in a knockout tournament format.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-Judge approaches lack a global ranking perspective due to reliance on individual or single-round pairwise assessments.

Method: Introducing Knockout Assessment which uses a knockout tournament system with iterative pairwise comparisons.

Result: Knockout assessment improved scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations.

Conclusion: The results indicate that LLM assessments can be aligned more closely with human scoring through the use of Knockout Assessment.

Abstract: Large Language Models (LLMs) have shown to be effective evaluators across
various domains such as machine translations or the scientific domain. Current
LLM-as-a-Judge approaches rely mostly on individual assessments or a single
round of pairwise assessments, preventing the judge LLM from developing a
global ranking perspective. To address this, we present Knockout Assessment, an
LLM-asa Judge method using a knockout tournament system with iterative pairwise
comparisons. Experiments across three LLMs on two datasets show that knockout
assessment improves scoring accuracy, increasing Pearson correlation with
expert evaluations by 0.07 on average for university-level exam scoring and
machine translation evaluations, aligning LLM assessments more closely with
human scoring.

</details>


### [57] [Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts](https://arxiv.org/abs/2506.03793)
*Sidharth Pulipaka,Sparsh Jain,Ashwin Sankar,Raj Dabre*

Main category: cs.CL

TL;DR: This paper introduces Cadence, a new punctuation restoration model that performs better than previous methods and supports more languages.


<details>
  <summary>Details</summary>
Motivation: Current models struggle to restore punctuation accurately in spontaneous speech, especially with disfluencies, hindering downstream tasks.

Method: Introducing Cadence, a generalist punctuation restoration model adapted from a pretrained large language model.

Result: Cadence outperforms previous models and expands language support from 14 to 22 Indian languages and English.

Conclusion: Cadence surpasses previous state-of-the-art performance in punctuation restoration and supports more languages.

Abstract: Punctuation plays a vital role in structuring meaning, yet current models
often struggle to restore it accurately in transcripts of spontaneous speech,
especially in the presence of disfluencies such as false starts and
backtracking. These limitations hinder the performance of downstream tasks like
translation, text to speech, summarization, etc. where sentence boundaries are
critical for preserving quality. In this work, we introduce Cadence, a
generalist punctuation restoration model adapted from a pretrained large
language model. Cadence is designed to handle both clean written text and
highly spontaneous spoken transcripts. It surpasses the previous state of the
art in performance while expanding support from 14 to all 22 Indian languages
and English. We conduct a comprehensive analysis of model behavior across
punctuation types and language families, identifying persistent challenges
under domain shift and with rare punctuation marks. Our findings demonstrate
the efficacy of utilizing pretrained language models for multilingual
punctuation restoration and highlight Cadence practical value for low resource
NLP pipelines at scale.

</details>


### [58] [Automatic Correction of Writing Anomalies in Hausa Texts](https://arxiv.org/abs/2506.03820)
*Ahmad Mustapha Wali,Sergiu Nisioi*

Main category: cs.CL

TL;DR: This paper presents an approach to automatically correct writing anomalies in Hausa texts by fine-tuning transformer-based models, achieving significant improvements in various evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by writing anomalies such as incorrect character substitutions and spacing errors in Hausa texts, which can hinder natural language processing (NLP) applications.

Method: The method involves fine-tuning transformer-based models to automatically correct writing anomalies in Hausa texts. A large-scale parallel dataset of over 450,000 noisy-clean Hausa sentence pairs was created by introducing synthetically generated noise. Several multilingual and African language-focused models were adapted for this correction task using SentencePiece tokenization.

Result: The experimental results show significant improvements in F1, BLEU and METEOR scores, as well as reductions in Character Error Rate (CER) and Word Error Rate (WER).

Conclusion: This research offers a reliable method, a publicly accessible dataset, and efficient models to enhance the quality of Hausa texts, thus promoting NLP capabilities for the language and providing transferable knowledge for other low-resource languages.

Abstract: Hausa texts are often characterized by writing anomalies such as incorrect
character substitutions and spacing errors, which sometimes hinder natural
language processing (NLP) applications. This paper presents an approach to
automatically correct the anomalies by finetuning transformer-based models.
Using a corpus gathered from several public sources, we created a large-scale
parallel dataset of over 450,000 noisy-clean Hausa sentence pairs by
introducing synthetically generated noise, fine-tuned to mimic realistic
writing errors. Moreover, we adapted several multilingual and African
language-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT
variants for this correction task using SentencePiece tokenization. Our
experimental results demonstrate significant increases in F1, BLEU and METEOR
scores, as well as reductions in Character Error Rate (CER) and Word Error Rate
(WER). This research provides a robust methodology, a publicly available
dataset, and effective models to improve Hausa text quality, thereby advancing
NLP capabilities for the language and offering transferable insights for other
low-resource languages.

</details>


### [59] [CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents](https://arxiv.org/abs/2506.03822)
*Fabian Karl,Ansgar Scherp*

Main category: cs.CL

TL;DR: This paper presents CRAWLDoc, a novel approach for contextual ranking of linked web documents to improve metadata extraction from diverse web sources.


<details>
  <summary>Details</summary>
Motivation: The challenge of extracting accurate metadata from web sources due to variations in web layouts and data formats.

Method: CRAWLDoc starts with a publication's URL, retrieves linked web resources, embeds them into a unified representation, and ranks relevant documents.

Result: CRAWLDoc demonstrates robust and layout-independent ranking of relevant documents across publishers and data formats using a new manually labeled dataset.

Conclusion: CRAWLDoc provides a foundation for enhanced metadata extraction from web documents with varying layouts and formats.

Abstract: Publication databases rely on accurate metadata extraction from diverse web
sources, yet variations in web layouts and data formats present challenges for
metadata providers. This paper introduces CRAWLDoc, a new method for contextual
ranking of linked web documents. Starting with a publication's URL, such as a
digital object identifier, CRAWLDoc retrieves the landing page and all linked
web resources, including PDFs, ORCID profiles, and supplementary materials. It
embeds these resources, along with anchor texts and the URLs, into a unified
representation. For evaluating CRAWLDoc, we have created a new, manually
labeled dataset of 600 publications from six top publishers in computer
science. Our method CRAWLDoc demonstrates a robust and layout-independent
ranking of relevant documents across publishers and data formats. It lays the
foundation for improved metadata extraction from web documents with various
layouts and formats. Our source code and dataset can be accessed at
https://github.com/FKarl/CRAWLDoc.

</details>


### [60] [Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising](https://arxiv.org/abs/2506.03827)
*Zhenhui Liu,Chunyuan Yuan,Ming Pang,Zheng Fang,Li Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo,Jingping Shao*

Main category: cs.CL

TL;DR: This paper introduces MoBGM, a model designed to enhance advertisement recall in e-commerce search by improving query rewriting relevance and authenticity while maximizing revenue.


<details>
  <summary>Details</summary>
Motivation: Existing query rewriting methods often fail to optimize both the relevance/authenticity of user queries and the revenue potential of recalled ads.

Method: MoBGM uses a discriminator, generator, and preference alignment module to simultaneously achieve these goals.

Result: Extensive offline and online experiments show significant improvements over existing methods, creating substantial commercial value after deployment.

Conclusion: The proposed MoBGM model demonstrates feasibility and robustness in enhancing advertisement recall and platform revenue.

Abstract: Retrieval systems primarily address the challenge of matching user queries
with the most relevant advertisements, playing a crucial role in e-commerce
search advertising. The diversity of user needs and expressions often produces
massive long-tail queries that cannot be matched with merchant bidwords or
product titles, which results in some advertisements not being recalled,
ultimately harming user experience and search efficiency. Existing query
rewriting research focuses on various methods such as query log mining,
query-bidword vector matching, or generation-based rewriting. However, these
methods often fail to simultaneously optimize the relevance and authenticity of
the user's original query and rewrite and maximize the revenue potential of
recalled ads.
  In this paper, we propose a Multi-objective aligned Bidword Generation Model
(MoBGM), which is composed of a discriminator, generator, and preference
alignment module, to address these challenges. To simultaneously improve the
relevance and authenticity of the query and rewrite and maximize the platform
revenue, we design a discriminator to optimize these key objectives. Using the
feedback signal of the discriminator, we train a multi-objective aligned
bidword generator that aims to maximize the combined effect of the three
objectives. Extensive offline and online experiments show that our proposed
algorithm significantly outperforms the state of the art. After deployment, the
algorithm has created huge commercial value for the platform, further verifying
its feasibility and robustness.

</details>


### [61] [Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain](https://arxiv.org/abs/2506.03832)
*Omer Moussa,Mariya Toneva*

Main category: cs.CL

TL;DR: This study explores the effectiveness of brain-tuning on improving speech models' semantic understanding and hierarchical processing. The results show that brain-tuned models outperform pretrained models in aligning with semantic language regions and exhibit a clear hierarchy from acoustic to semantic representations.


<details>
  <summary>Details</summary>
Motivation: To examine how well brain-tuned models reflect the brain's intermediate stages of speech processing.

Method: Brain-tuning models using human brain recordings and layer-wise probing.

Result: Late layers of brain-tuned models significantly improve in aligning with semantic language regions, and late layers are best at complex high-level tasks while early layers remain dedicated to low-level acoustic features.

Conclusion: Brain-tuned models not only perform better but also exhibit a well-defined hierarchical processing from acoustic to semantic representations, making them better model organisms for human speech processing.

Abstract: Pretrained self-supervised speech models excel in speech tasks but do not
reflect the hierarchy of human speech processing, as they encode rich semantics
in middle layers and poor semantics in late layers. Recent work showed that
brain-tuning (fine-tuning models using human brain recordings) improves speech
models' semantic understanding. Here, we examine how well brain-tuned models
further reflect the brain's intermediate stages of speech processing. We find
that late layers of brain-tuned models substantially improve over pretrained
models in their alignment with semantic language regions. Further layer-wise
probing reveals that early layers remain dedicated to low-level acoustic
features, while late layers become the best at complex high-level tasks. These
findings show that brain-tuned models not only perform better but also exhibit
a well-defined hierarchical processing going from acoustic to semantic
representations, making them better model organisms for human speech
processing.

</details>


### [62] [PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading](https://arxiv.org/abs/2506.03861)
*Qiuhan Han,Qian Wang,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.CL

TL;DR: This paper introduces PulseReddit, a new dataset that matches Reddit discussions with cryptocurrency market stats for high-frequency trading analysis. It shows that using this data with MAS improves trading results, especially in bull markets.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of social media like Reddit for high-frequency trading in cryptocurrency markets.

Method: Using PulseReddit data and LLM-based MAS to study the effect of social sentiment on trading performance.

Result: MAS with PulseReddit data outperforms traditional methods in trading, especially beneficial in bull markets, and adapts well across various market conditions.

Conclusion: Integrating social media data can enhance high-frequency trading strategies, with important implications for model selection and future MAS research.

Abstract: High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding
rapid decision-making. Social media platforms like Reddit offer valuable, yet
underexplored, information for such high-frequency, short-term trading. This
paper introduces \textbf{PulseReddit}, a novel dataset that is the first to
align large-scale Reddit discussion data with high-frequency cryptocurrency
market statistics for short-term trading analysis. We conduct an extensive
empirical study using Large Language Model (LLM)-based Multi-Agent Systems
(MAS) to investigate the impact of social sentiment from PulseReddit on trading
performance. Our experiments conclude that MAS augmented with PulseReddit data
achieve superior trading outcomes compared to traditional baselines,
particularly in bull markets, and demonstrate robust adaptability across
different market regimes. Furthermore, our research provides conclusive
insights into the performance-efficiency trade-offs of different LLMs,
detailing significant considerations for practical model selection in HFT
applications. PulseReddit and our findings establish a foundation for advanced
MAS research in HFT, demonstrating the tangible benefits of integrating social
media.

</details>


### [63] [EuroGEST: Investigating gender stereotypes in multilingual language models](https://arxiv.org/abs/2506.03867)
*Jacqueline Rowe,Mateusz Klimaszewski,Liane Guillou,Shannon Vallor,Alexandra Birch*

Main category: cs.CL

TL;DR: Introduce EuroGEST dataset to measure gender-stereotypical reasoning in LLMs across English and 29 European languages.


<details>
  <summary>Details</summary>
Motivation: Most benchmarks for gender bias remain English-centric, but large language models support multiple languages.

Method: EuroGEST builds on an existing expert-informed benchmark, expanded using translation tools, quality estimation metrics, and morphological heuristics.

Result: Evaluate 24 multilingual language models from six model families, showing stronger stereotypes in larger models and instruction finetuning does not consistently reduce gendered stereotypes.

Conclusion: Highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages.

Abstract: Large language models increasingly support multiple languages, yet most
benchmarks for gender bias remain English-centric. We introduce EuroGEST, a
dataset designed to measure gender-stereotypical reasoning in LLMs across
English and 29 European languages. EuroGEST builds on an existing
expert-informed benchmark covering 16 gender stereotypes, expanded in this work
using translation tools, quality estimation metrics, and morphological
heuristics. Human evaluations confirm that our data generation method results
in high accuracy of both translations and gender labels across languages. We
use EuroGEST to evaluate 24 multilingual language models from six model
families, demonstrating that the strongest stereotypes in all models across all
languages are that women are \textit{beautiful,} \textit{empathetic} and
\textit{neat} and men are \textit{leaders}, \textit{strong, tough} and
\textit{professional}. We also show that larger models encode gendered
stereotypes more strongly and that instruction finetuning does not consistently
reduce gendered stereotypes. Our work highlights the need for more multilingual
studies of fairness in LLMs and offers scalable methods and resources to audit
gender bias across languages.

</details>


### [64] [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuai Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: A new framework called RadialRouter is proposed for efficient selection of optimal large language models.


<details>
  <summary>Details</summary>
Motivation: Current LLM routing methods are limited in effectiveness due to insufficient exploration of the connection between user queries and LLM characteristics.

Method: RadialRouter uses a lightweight Transformer-based backbone with a radial structure named RadialFormer to articulate the query-LLMs relationship.

Result: RadialRouter outperforms existing routing methods by 9.2% and 5.8% in the Balance and Cost First scenarios.

Conclusion: The proposed RadialRouter demonstrates practical application potential with adaptability toward different performance-cost trade-offs and dynamic LLM pools.

Abstract: The rapid advancements in large language models (LLMs) have led to the
emergence of routing techniques, which aim to efficiently select the optimal
LLM from diverse candidates to tackle specific tasks, optimizing performance
while reducing costs. Current LLM routing methods are limited in effectiveness
due to insufficient exploration of the intrinsic connection between user
queries and the characteristics of LLMs. To address this issue, in this paper,
we present RadialRouter, a novel framework for LLM routing which employs a
lightweight Transformer-based backbone with a radial structure named
RadialFormer to articulate the query-LLMs relationship. The optimal LLM
selection is performed based on the final states of RadialFormer. The pipeline
is further refined by an objective function that combines Kullback-Leibler
divergence with the query-query contrastive loss to enhance robustness.
Experimental results on RouterBench show that RadialRouter significantly
outperforms existing routing methods by 9.2\% and 5.8\% in the Balance and Cost
First scenarios, respectively. Additionally, its adaptability toward different
performance-cost trade-offs and the dynamic LLM pool demonstrates practical
application potential.

</details>


### [65] [Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages](https://arxiv.org/abs/2506.03884)
*Utkarsh Pathak,Chandra Sai Krishna Gunda,Anusha Prakash,Keshav Agarwal,Hema A. Murthy*

Main category: cs.CL

TL;DR: This study focuses on zero-shot synthesis for languages with no digital resources, introducing a novel method that augments shared phone representation and modifies text parsing rules to match target language phonotactics, enabling intelligible and natural speech generation for several Indian languages.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training TTS systems for languages with no digital resources or transcriptions in India, which has 1369 languages and 13 scripts.

Method: Augmenting shared phone representation and modifying text parsing rules to match target language phonotactics to reduce synthesiser overhead and enable rapid adaptation.

Result: Intelligible and natural speech was successfully generated for Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging linguistic connections across languages with suitable synthesisers.

Conclusion: The proposed approach confirms the effectiveness of zero-shot synthesis for under-represented languages, showing potential to expand speech technology access.

Abstract: Text-to-speech (TTS) systems typically require high-quality studio data and
accurate transcriptions for training. India has 1369 languages, with 22
official using 13 scripts. Training a TTS system for all these languages, most
of which have no digital resources, seems a Herculean task. Our work focuses on
zero-shot synthesis, particularly for languages whose scripts and phonotactics
come from different families. The novelty of our work is in the augmentation of
a shared phone representation and modifying the text parsing rules to match the
phonotactics of the target language, thus reducing the synthesiser overhead and
enabling rapid adaptation. Intelligible and natural speech was generated for
Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging
linguistic connections across languages with suitable synthesisers. Evaluations
confirm the effectiveness of this approach, highlighting its potential to
expand speech technology access for under-represented languages.

</details>


### [66] [Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation](https://arxiv.org/abs/2506.03887)
*Junyi Chen,Shihao Bai,Zaijun Wang,Siyu Wu,Chuheng Du,Hailong Yang,Ruihao Gong,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.CL

TL;DR: Pre$^3$ optimizes LLM decoding efficiency by transforming LR(1) transition graphs into DPDA, reducing TPOT by up to 40% and increasing throughput by up to 36%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for parsing LR(1) grammars lead to runtime execution overhead for context-dependent token processing, especially inefficient under large inference batches.

Method: Pre$^3$ precomputes prefix-conditioned edges during preprocessing and introduces a novel approach to transform LR(1) transition graphs into DPDA.

Result: Pre$^3$ reduces time per output token (TPOT) by up to 40% and increases throughput by up to 36% in experiments.

Conclusion: Pre$^3$ optimizes LLM decoding efficiency and can be seamlessly integrated into standard LLM inference frameworks.

Abstract: Extensive LLM applications demand efficient structured generations,
particularly for LR(1) grammars, to produce outputs in specified formats (e.g.,
JSON). Existing methods primarily parse LR(1) grammars into a pushdown
automaton (PDA), leading to runtime execution overhead for context-dependent
token processing, especially inefficient under large inference batches. To
address these issues, we propose Pre$^3$ that exploits deterministic pushdown
automata (DPDA) to optimize the constrained LLM decoding efficiency. First, by
precomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables
ahead-of-time edge analysis and thus makes parallel transition processing
possible. Second, by leveraging the prefix-conditioned edges, Pre$^3$
introduces a novel approach that transforms LR(1) transition graphs into DPDA,
eliminating the need for runtime path exploration and achieving edge
transitions with minimal overhead. Pre$^3$ can be seamlessly integrated into
standard LLM inference frameworks, reducing time per output token (TPOT) by up
to 40% and increasing throughput by up to 36% in our experiments. Our code is
available at https://github.com/ModelTC/lightllm.

</details>


### [67] [Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems](https://arxiv.org/abs/2506.03901)
*Yuxin Zhang,Yan Wang,Yongrui Chen,Shenyu Zhang,Xinbang Dai,Sheng Bi,Guilin Qi*

Main category: cs.CL

TL;DR: 提出Magic Mushroom基准，用于评估和改进检索增强生成系统在面对复杂噪声时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准无法模拟现实检索环境中复杂且异构的噪声分布，难以可靠地评估检索增强生成系统的鲁棒性。

Method: 定义四种基于语言特性和噪声特征的检索噪声类别，构建Magic Mushroom基准来复制“魔法蘑菇”噪声，并评估不同规模的语言模型生成器和经典去噪策略在不同噪声分布下的性能。

Result: Magic Mushroom包含7468个单跳和3925个多跳问答对，使研究人员能够根据具体的研究目标或应用场景灵活配置检索噪声组合，评估不同噪声分布下生成器和去噪策略的表现，发现两者在噪声分布面前表现出极端敏感性，且有很大的改进空间。

Conclusion: Magic Mushroom作为检索增强生成系统的基准，对于评估和改进噪声鲁棒性有巨大潜力，可以加速其在实际应用中的广泛部署。

Abstract: Retrieval-Augmented Generation (RAG) systems enhance Large Language Models
(LLMs) by incorporating external retrieved information, mitigating issues such
as hallucination and outdated knowledge.
  However, RAG systems are highly sensitive to retrieval noise prevalent in
real-world scenarios.
  Existing benchmarks fail to emulate the complex and heterogeneous noise
distributions encountered in real-world retrieval environments, undermining
reliable robustness assessment.
  In this paper, we define four categories of retrieval noise based on
linguistic properties and noise characteristics, aiming to reflect the
heterogeneity of noise in real-world scenarios.
  Building on this, we introduce Magic Mushroom, a benchmark for replicating
"magic mushroom" noise: contexts that appear relevant on the surface but
covertly mislead RAG systems.
  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer
pairs.
  More importantly, Magic Mushroom enables researchers to flexibly configure
combinations of retrieval noise according to specific research objectives or
application scenarios, allowing for highly controlled evaluation setups.
  We evaluate LLM generators of varying parameter scales and classic RAG
denoising strategies under diverse noise distributions to investigate their
performance dynamics during progressive noise encroachment.
  Our analysis reveals that both generators and denoising strategies have
significant room for improvement and exhibit extreme sensitivity to noise
distributions.
  Magic Mushroom emerges as a promising tool for evaluating and advancing
noise-robust RAG systems, accelerating their widespread deployment in
real-world applications.
  The Magic Mushroom benchmark is available at the
https://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.

</details>


### [68] [The Harmonic Structure of Information Contours](https://arxiv.org/abs/2506.03902)
*Eleftheria Tsipidi,Samuel Kiegeland,Franz Nowak,Tianyang Xu,Ethan Wilcox,Alex Warstadt,Ryan Cotterell,Mario Giulianelli*

Main category: cs.CL

TL;DR: 本研究探索了语言信息率的周期性模式，并发现其与话语结构有关。


<details>
  <summary>Details</summary>
Motivation: 传统的均匀信息密度（UID）假设认为说话者旨在在整个文本中均匀分布信息，但语言通常不会保持严格的均匀信息率，而是围绕全球平均值波动。这些波动通常由诸如句法约束、风格选择或受众设计等因素解释。这项工作探索了一种替代观点，即这些波动可能受到隐式语言向周期性压力的影响。

Method: 应用谐波回归和引入一种称为时间尺度的新扩展来检测和测试信息轮廓中的周期性。

Result: 在英语、西班牙语、德语、荷兰语、巴斯克语和巴西葡萄牙语文本中发现了信息率的一致周期模式。

Conclusion: 分析表明，信息率存在周期性模式，并且许多主导频率与话语结构一致，这表明这些波动反映了有意义的语言组织。此外，这种方法为揭示语言学各个层次上的结构性压力提供了一个通用框架。

Abstract: The uniform information density (UID) hypothesis proposes that speakers aim
to distribute information evenly throughout a text, balancing production effort
and listener comprehension difficulty. However, language typically does not
maintain a strictly uniform information rate; instead, it fluctuates around a
global average. These fluctuations are often explained by factors such as
syntactic constraints, stylistic choices, or audience design. In this work, we
explore an alternative perspective: that these fluctuations may be influenced
by an implicit linguistic pressure towards periodicity, where the information
rate oscillates at regular intervals, potentially across multiple frequencies
simultaneously. We apply harmonic regression and introduce a novel extension
called time scaling to detect and test for such periodicity in information
contours. Analyzing texts in English, Spanish, German, Dutch, Basque, and
Brazilian Portuguese, we find consistent evidence of periodic patterns in
information rate. Many dominant frequencies align with discourse structure,
suggesting these oscillations reflect meaningful linguistic organization.
Beyond highlighting the connection between information rate and discourse
structure, our approach offers a general framework for uncovering structural
pressures at various levels of linguistic granularity.

</details>


### [69] [When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning](https://arxiv.org/abs/2506.03913)
*Claire Barale,Michael Rovatsos,Nehal Bhuta*

Main category: cs.CL

TL;DR: This paper evaluates three common machine learning approaches on a large dataset of Canadian refugee decisions, finding they produce conflicting signals and fail to adequately assess fairness in legally discretionary contexts.


<details>
  <summary>Details</summary>
Motivation: To examine whether statistical methods can meaningfully assess fairness in legal contexts shaped by discretion, normative complexity, and limited ground truth.

Method: Feature-based analysis, semantic clustering, and predictive modeling

Result: These methods produce divergent and contradictory signals, predictive modeling often depends on contextual/procedural features, and semantic clustering fails to capture substantive legal reasoning.

Conclusion: Statistical fairness evaluation has limitations, assuming statistical regularity equals fairness is flawed, and current computational approaches are insufficient for evaluating fairness in legally discretionary domains.

Abstract: Legal decisions are increasingly evaluated for fairness, consistency, and
bias using machine learning (ML) techniques. In high-stakes domains like
refugee adjudication, such methods are often applied to detect disparities in
outcomes. Yet it remains unclear whether statistical methods can meaningfully
assess fairness in legal contexts shaped by discretion, normative complexity,
and limited ground truth.
  In this paper, we empirically evaluate three common ML approaches
(feature-based analysis, semantic clustering, and predictive modeling) on a
large, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our
experiments show that these methods produce divergent and sometimes
contradictory signals, that predictive modeling often depends on contextual and
procedural features rather than legal features, and that semantic clustering
fails to capture substantive legal reasoning.
  We show limitations of statistical fairness evaluation, challenge the
assumption that statistical regularity equates to fairness, and argue that
current computational approaches fall short of evaluating fairness in legally
discretionary domains. We argue that evaluating fairness in law requires
methods grounded not only in data, but in legal reasoning and institutional
context.

</details>


### [70] [Compositional Generalisation for Explainable Hate Speech Detection](https://arxiv.org/abs/2506.03916)
*Agostina Calabrese,Tom Sherborne,Björn Ross,Mirella Lapata*

Main category: cs.CL

TL;DR: This study investigates the issue of hate speech detection models struggling to generalize beyond their training data due to dataset biases and the use of sentence-level labels. The authors propose a new approach using fine-grained, span-level annotations and introduce a novel dataset called U-PLEAD to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Current hate speech detection models struggle to generalize beyond their training data due to dataset biases and the use of sentence-level labels, which fail to teach models the underlying structure of hate speech.

Method: The authors investigate whether training on a dataset where expressions occur with equal frequency across all contexts can improve generalization. They create U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel compositional generalization benchmark of ~8,000 manually validated posts.

Result: Training on a combination of U-PLEAD and real data improves compositional generalization while achieving state-of-the-art performance on the human-sourced PLEAD.

Conclusion: This work highlights the importance of considering the context in which expressions occur when training hate speech detection models.

Abstract: Hate speech detection is key to online content moderation, but current models
struggle to generalise beyond their training data. This has been linked to
dataset biases and the use of sentence-level labels, which fail to teach models
the underlying structure of hate speech. In this work, we show that even when
models are trained with more fine-grained, span-level annotations (e.g.,
"artists" is labeled as target and "are parasites" as dehumanising comparison),
they struggle to disentangle the meaning of these labels from the surrounding
context. As a result, combinations of expressions that deviate from those seen
during training remain particularly difficult for models to detect. We
investigate whether training on a dataset where expressions occur with equal
frequency across all contexts can improve generalisation. To this end, we
create U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel
compositional generalisation benchmark of ~8,000 manually validated posts.
Training on a combination of U-PLEAD and real data improves compositional
generalisation while achieving state-of-the-art performance on the
human-sourced PLEAD.

</details>


### [71] [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)
*Zhaolu Kang,Junhao Gong,Jiaxu Yan,Wanke Xia,Yian Wang,Ziwen Wang,Huaxuan Ding,Zhuo Cheng,Wenhao Cao,Zhiyuan Feng,Siqi He,Shannan Yan,Junzhe Chen,Xiaomin He,Chaoya Jiang,Wei Ye,Kaidong Yu,Xuelong Li*

Main category: cs.CL

TL;DR: 提出HSSBench基准来评估多语言大型语言模型在人文学科和社会科学任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估多模态大型语言模型的基准主要强调一般知识和垂直的分步推理，而忽视了人文学科和社会科学的独特需求和潜力。

Method: 引入了新的数据生成管道，并设计了超过13,000个样本覆盖六个关键类别。

Result: HSSBench包含超过13,000个精心设计的样本，涵盖六个关键类别，并且证明即使是最先进的模型，在HSSBench上也面临重大挑战。

Conclusion: 提出HSSBench基准来评估多语言大型语言模型在人文学科和社会科学任务上的能力，并希望这个基准能激励进一步研究以增强跨学科推理能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
potential to advance a broad range of domains. However, current benchmarks for
evaluating MLLMs primarily emphasize general knowledge and vertical
step-by-step reasoning typical of STEM disciplines, while overlooking the
distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks
in the HSS domain require more horizontal, interdisciplinary thinking and a
deep integration of knowledge across related fields, which presents unique
challenges for MLLMs, particularly in linking abstract concepts with
corresponding visual representations. Addressing this gap, we present HSSBench,
a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks
in multiple languages, including the six official languages of the United
Nations. We also introduce a novel data generation pipeline tailored for HSS
scenarios, in which multiple domain experts and automated agents collaborate to
generate and iteratively refine each sample. HSSBench contains over 13,000
meticulously designed samples, covering six key categories. We benchmark more
than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant
challenges even for state-of-the-art models. We hope that this benchmark will
inspire further research into enhancing the cross-disciplinary reasoning
abilities of MLLMs, especially their capacity to internalize and connect
knowledge across fields.

</details>


### [72] [More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning](https://arxiv.org/abs/2506.03923)
*Mohammadamin Shafiei,Hamidreza Saffari,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 研究LLMs在比较数学问题中的方向性框架偏差，并提出MathComp基准测试。


<details>
  <summary>Details</summary>
Motivation: 理解语义线索如何塑造LLMs推理的机制。

Method: 通过引入MathComp基准测试，研究LLMs对语义线索的反应机制。

Result: 发现LLMs在比较数学问题中存在一致且方向性的框架偏差，链式思维提示可以减少这些偏差，但效果因格式而异。

Conclusion: 揭示了LLMs在比较数学问题中的方向性框架偏差，并提出MathComp基准来评估这种偏差。

Abstract: Large language models (LLMs) are known to be sensitive to input phrasing, but
the mechanisms by which semantic cues shape reasoning remain poorly understood.
We investigate this phenomenon in the context of comparative math problems with
objective ground truth, revealing a consistent and directional framing bias:
logically equivalent questions containing the words ``more'', ``less'', or
``equal'' systematically steer predictions in the direction of the framing
term. To study this effect, we introduce MathComp, a controlled benchmark of
300 comparison scenarios, each evaluated under 14 prompt variants across three
LLM families. We find that model errors frequently reflect linguistic steering,
systematic shifts toward the comparative term present in the prompt.
Chain-of-thought prompting reduces these biases, but its effectiveness varies:
free-form reasoning is more robust, while structured formats may preserve or
reintroduce directional drift. Finally, we show that including demographic
identity terms (e.g., ``a woman'', ``a Black person'') in input scenarios
amplifies directional drift, despite identical underlying quantities,
highlighting the interplay between semantic framing and social referents. These
findings expose critical blind spots in standard evaluation and motivate
framing-aware benchmarks for diagnosing reasoning robustness and fairness in
LLMs.

</details>


### [73] [Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations](https://arxiv.org/abs/2506.03941)
*Vivian Nguyen,Lillian Lee,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: Detect pivotal moments in conversations in real-time using an unsupervised computational method.


<details>
  <summary>Details</summary>
Motivation: Systems that can detect pivotal moments in conversations can assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling.

Method: An unsupervised computational method that detects pivotal moments based on varying expectations of outcomes depending on what might be said next.

Result: The method aligns with human perception and conversational trajectory changes. It also explores the relation between counselor's response during pivotal moments and session outcome.

Conclusion: The proposed method can help identify crucial points in conversations, potentially improving outcomes in high-stakes scenarios like crisis counseling.

Abstract: During a conversation, there can come certain moments where its outcome hangs
in the balance. In these pivotal moments, how one responds can put the
conversation on substantially different trajectories leading to significantly
different outcomes. Systems that can detect when such moments arise could
assist conversationalists in domains with highly consequential outcomes, such
as mental health crisis counseling.
  In this work, we introduce an unsupervised computational method for detecting
such pivotal moments as they happen, in an online fashion. Our approach relies
on the intuition that a moment is pivotal if our expectation of the outcome
varies widely depending on what might be said next. By applying our method to
crisis counseling conversations, we first validate it by showing that it aligns
with human perception -- counselors take significantly longer to respond during
moments detected by our method -- and with the eventual conversational
trajectory -- which is more likely to change course at these times. We then use
our framework to explore the relation of the counselor's response during
pivotal moments with the eventual outcome of the session.

</details>


### [74] [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/abs/2506.03949)
*Junnan Zhu,Jingyi Wang,Bohan Yu,Xiaoyu Wu,Junbo Li,Lei Wang,Nan Xu*

Main category: cs.CL

TL;DR: Introduce TableEval, a new benchmark for evaluating LLMs on complex TableQA tasks with diverse table structures and multilingual data.


<details>
  <summary>Details</summary>
Motivation: Existing TableQA benchmarks are limited by their focus on simple flat tables and suffer from data leakage. They also fail to capture cross-lingual and cross-domain variability in practical applications.

Method: Collect tables with various structures from four domains and include cross-lingual scenarios with tables in three languages. Propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level.

Result: TableEval reveals critical gaps in the ability of state-of-the-art LLMs to handle complex, real-world TableQA tasks. SEAT achieves high agreement with human judgment.

Conclusion: TableEval provides a more realistic and challenging benchmark for evaluating LLMs on TableQA tasks, highlighting areas for future improvement.

Abstract: LLMs have shown impressive progress in natural language processing. However,
they still face significant challenges in TableQA, where real-world
complexities such as diverse table structures, multilingual data, and
domain-specific reasoning are crucial. Existing TableQA benchmarks are often
limited by their focus on simple flat tables and suffer from data leakage.
Furthermore, most benchmarks are monolingual and fail to capture the
cross-lingual and cross-domain variability in practical applications. To
address these limitations, we introduce TableEval, a new benchmark designed to
evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes
tables with various structures (such as concise, hierarchical, and nested
tables) collected from four domains (including government, finance, academia,
and industry reports). Besides, TableEval features cross-lingual scenarios with
tables in Simplified Chinese, Traditional Chinese, and English. To minimize the
risk of data leakage, we collect all data from recent real-world documents.
Considering that existing TableQA metrics fail to capture semantic accuracy, we
further propose SEAT, a new evaluation framework that assesses the alignment
between model responses and reference answers at the sub-question level.
Experimental results have shown that SEAT achieves high agreement with human
judgment. Extensive experiments on TableEval reveal critical gaps in the
ability of state-of-the-art LLMs to handle these complex, real-world TableQA
tasks, offering insights for future improvements. We make our dataset available
here: https://github.com/wenge-research/TableEval.

</details>


### [75] [From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding](https://arxiv.org/abs/2506.03968)
*Chiwei Zhu,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: This paper introduces a method to generate diverse and complex instructions for large language models using attributed grounding, resulting in a dataset called SynthQuestions which improves model performance.


<details>
  <summary>Details</summary>
Motivation: To generate diverse, complex, and large-scale instruction data for aligning large language models.

Method: Attributed grounding involving a top-down attribution process and a bottom-up synthesis process.

Result: A dataset named SynthQuestions with 1 million instructions was created and proved effective in improving model performance.

Conclusion: Models trained on the SynthQuestions dataset show leading performance on various benchmarks.

Abstract: The pursuit of diverse, complex, and large-scale instruction data is crucial
for automatically aligning large language models (LLMs). While there are
methods capable of generating synthetic instructions at scale, they either
suffer from limited grounding sources, leading to a narrow distribution, or
rely on trivial extensions that fail to produce meaningful trajectories in
terms of complexity. In contrast, instructions that benefit efficient alignment
are typically crafted with cognitive insights and grounded in real-world use
cases. In this paper, we synthesize such instructions using attributed
grounding, which involves 1) a top-down attribution process that grounds a
selective set of real instructions to situated users, and 2) a bottom-up
synthesis process that leverages web documents to first generate a situation,
then a meaningful instruction. This framework allows us to harvest diverse and
complex instructions at scale, utilizing the vast range of web documents.
Specifically, we construct a dataset of 1 million instructions, called
SynthQuestions, and demonstrate that models trained on it achieve leading
performance on several common benchmarks, with improvements that continually
scale with more web corpora. Data, models and codes will be available at
https://github.com/Ignoramus0817/SynthQuestions.

</details>


### [76] [Structured Pruning for Diverse Best-of-N Reasoning Optimization](https://arxiv.org/abs/2506.03978)
*Hieu Trung Nguyen,Bao Nguyen,Viet Anh Nguyen*

Main category: cs.CL

TL;DR: Transformer模型剪枝不仅可实现计算节省，还能提升推理能力。SPRINT框架通过对比学习动态选择最佳剪枝头和层，在MATH500和GSM8K数据集上显著优于传统策略。


<details>
  <summary>Details</summary>
Motivation: 发现选择性剪枝某些注意力头能改善推理性能，尤其是在困难任务上。

Method: 提出SPRINT框架，通过问题嵌入与头嵌入对齐来识别更准确推理的剪枝头配置。

Result: 在MATH500和GSM8K数据集上，SPRINT方法大幅超越传统最佳-N和随机头选择策略。

Conclusion: 模型剪枝不仅能节省计算资源，还可能增强模型的推理能力。

Abstract: Model pruning in transformer-based language models, traditionally viewed as a
means of achieving computational savings, can enhance the model's reasoning
capabilities. In this work, we uncover a surprising phenomenon: the selective
pruning of certain attention heads leads to improvements in reasoning
performance, particularly on challenging tasks. Motivated by this observation,
we propose SPRINT, a novel contrastive learning framework that dynamically
selects the optimal head and layer to prune during inference. By aligning
question embeddings with head embeddings, SPRINT identifies those pruned-head
configurations that result in more accurate reasoning. Extensive experiments
demonstrate that our method significantly outperforms traditional best-of-$N$
and random head selection strategies on the MATH500 and GSM8K datasets.

</details>


### [77] [Voice Activity Projection Model with Multimodal Encoders](https://arxiv.org/abs/2506.03980)
*Takeshi Saga,Catherine Pelachaud*

Main category: cs.CL

TL;DR: This paper proposes a multimodal model enhanced with pre-trained audio and face encoders to improve turn-taking prediction performance.


<details>
  <summary>Details</summary>
Motivation: To model human-machine interaction more effectively by capturing subtle expressions.

Method: Proposed a multimodal model using pre-trained audio and face encoders.

Result: The model performed competitively or even better than state-of-the-art models on turn-taking metrics.

Conclusion: A multimodal model enhanced with pre-trained audio and face encoders can improve turn-taking prediction performance.

Abstract: Turn-taking management is crucial for any social interaction. Still, it is
challenging to model human-machine interaction due to the complexity of the
social context and its multimodal nature. Unlike conventional systems based on
silence duration, previous existing voice activity projection (VAP) models
successfully utilized a unified representation of turn-taking behaviors as
prediction targets, which improved turn-taking prediction performance.
Recently, a multimodal VAP model outperformed the previous state-of-the-art
model by a significant margin. In this paper, we propose a multimodal model
enhanced with pre-trained audio and face encoders to improve performance by
capturing subtle expressions. Our model performed competitively, and in some
cases, even better than state-of-the-art models on turn-taking metrics. All the
source codes and pretrained models are available at
https://github.com/sagatake/VAPwithAudioFaceEncoders.

</details>


### [78] [Around the World in 24 Hours: Probing LLM Knowledge of Time and Place](https://arxiv.org/abs/2506.03984)
*Carolin Holtermann,Paul Röttger,Anne Lauscher*

Main category: cs.CL

TL;DR: This paper introduces GeoTemp, a dataset for evaluating language models' ability to jointly reason about time and space. It tests eight open chat models across different model families and finds that while models handle temporal knowledge well and performance scales up, integrating temporal and geographical information remains challenging. Performance isn't tied to specific geographic regions but improves with familiar location names. Prompt formulation greatly affects performance, with direct geographical knowledge injection being beneficial, whereas certain prompting techniques can hinder simpler tasks.


<details>
  <summary>Details</summary>
Motivation: To explore language models' abilities to jointly reason over time and space in non-isolated and complex real-world settings.

Method: Creating GeoTemp dataset and evaluating eight open chat models from three different model families on various combinations of temporal and geographic knowledge.

Result: Models perform well with temporal knowledge alone and better with larger models. Combining temporal and geographical knowledge proves difficult. Familiar location names boost performance. Prompt formulation significantly impacts performance.

Conclusion: Language models have varying capabilities when it comes to joint time-space reasoning, showing potential improvements with larger models and appropriate prompt engineering.

Abstract: Reasoning over time and space is essential for understanding our world.
However, the abilities of language models in this area are largely unexplored
as previous work has tested their abilities for logical reasoning in terms of
time and space in isolation or only in simple or artificial environments. In
this paper, we present the first evaluation of the ability of language models
to jointly reason over time and space. To enable our analysis, we create
GeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37
time zones. Using GeoTemp, we evaluate eight open chat models of three
different model families for different combinations of temporal and geographic
knowledge. We find that most models perform well on reasoning tasks involving
only temporal knowledge and that overall performance improves with scale.
However, performance remains constrained in tasks that require connecting
temporal and geographical information. We do not find clear correlations of
performance with specific geographic regions. Instead, we find a significant
performance increase for location names with low model perplexity, suggesting
their repeated occurrence during model training. We further demonstrate that
their performance is heavily influenced by prompt formulation - a direct
injection of geographical knowledge leads to performance gains, whereas,
surprisingly, techniques like chain-of-thought prompting decrease performance
on simpler tasks.

</details>


### [79] [Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models](https://arxiv.org/abs/2506.03989)
*Alex Laitenberger,Christopher D. Manning,Nelson F. Liu*

Main category: cs.CL

TL;DR: Long-context language models can handle tens of thousands of tokens, but multi-stage retrieval-augmented generation (RAG) pipelines like ReadAgent and RAPTOR don't always outperform simpler methods such as DOS RAG, which preserves original passage order and performs well on long-context QA tasks.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether multi-stage RAG pipelines still offer advantages over simpler, single-stage approaches in the context of long-context language models.

Method: Controlled evaluation comparing multi-stage pipelines (ReadAgent and RAPTOR) against baselines including DOS RAG on QA tasks under scaled token budgets.

Result: DOS RAG, despite its simple design, consistently matches or outperforms more complex methods on long-context QA benchmarks.

Conclusion: Recommend establishing DOS RAG as a strong baseline for future RAG evaluations to assess trade-offs between complexity and effectiveness.

Abstract: With the rise of long-context language models (LMs) capable of processing
tens of thousands of tokens in a single pass, do multi-stage
retrieval-augmented generation (RAG) pipelines still offer measurable benefits
over simpler, single-stage approaches? To assess this question, we conduct a
controlled evaluation for QA tasks under systematically scaled token budgets,
comparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three
baselines, including DOS RAG (Document's Original Structure RAG), a simple
retrieve-then-read method that preserves original passage order. Despite its
straightforward design, DOS RAG consistently matches or outperforms more
intricate methods on multiple long-context QA benchmarks. We recommend
establishing DOS RAG as a simple yet strong baseline for future RAG
evaluations, pairing it with emerging embedding and language models to assess
trade-offs between complexity and effectiveness as model capabilities evolve.

</details>


### [80] [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)
*Hongzhi Zhang,Jingyuan Zhang,Xingguang Ji,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: 我们提出了一种新的动态视频令牌压缩策略，可以有效减少令牌数量并保持性能。


<details>
  <summary>Details</summary>
Motivation: 典型的视频建模方法（如LLava）将视频表示为视觉令牌序列，然后由大型语言模型（LLM）骨干处理以实现有效的视频理解。然而，这种方法导致了大量的视觉令牌，特别是对于长视频。

Method: 引入了一种名为DynTok的新颖动态视频令牌压缩策略。DynTok适应性地将视觉令牌分成组并在每个组内合并它们，在信息密度低的区域实现高压缩，同时保留重要内容。

Result: 我们的方法将令牌数量减少到原始大小的44.4%，同时保持了相当的性能。它进一步受益于增加视频帧的数量，在Video-MME上达到65.3％，在MLVU上达到72.5％。

Conclusion: 我们的方法在减少令牌数量的同时保持了相当的性能，并通过应用这种简单而有效的压缩方法，揭示了视频令牌表示中的冗余，并为设计更高效的视频建模技术提供了见解。

Abstract: Typical video modeling methods, such as LLava, represent videos as sequences
of visual tokens, which are then processed by the LLM backbone for effective
video understanding. However, this approach leads to a massive number of visual
tokens, especially for long videos. A practical solution is to first extract
relevant visual information from the large visual context before feeding it
into the LLM backbone, thereby reducing computational overhead. In this work,
we introduce DynTok, a novel \textbf{Dyn}amic video \textbf{Tok}en compression
strategy. DynTok adaptively splits visual tokens into groups and merges them
within each group, achieving high compression in regions with low information
density while preserving essential content. Our method reduces the number of
tokens to 44.4% of the original size while maintaining comparable performance.
It further benefits from increasing the number of video frames and achieves
65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective
compression method, we expose the redundancy in video token representations and
offer insights for designing more efficient video modeling techniques.

</details>


### [81] [Words of Warmth: Trust and Sociability Norms for over 26k English Words](https://arxiv.org/abs/2506.03993)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: This paper introduces Words of Warmth, a large-scale repository of manually derived word-warmth associations for over 26k English words. The reliability of the associations is demonstrated, and it is used to study the acquisition rate of WCTS words in children with age. Additionally, the lexicon enables bias and stereotype research.


<details>
  <summary>Details</summary>
Motivation: To understand the development and nature of warmth and its components (Trust and Sociability), and to create a tool for studying bias and stereotypes.

Method: Manually deriving word-warmth associations for over 26k English words and analyzing their reliability. Using the lexicons to study the acquisition rate of WCTS words in children and enabling bias and stereotype research.

Result: The creation of Words of Warmth, a reliable lexicon of word-warmth associations, and its application in studying the acquisition rate of WCTS words and enabling bias and stereotype research.

Conclusion: Words of Warmth provides a valuable resource for understanding the development and nature of warmth and its components, and for conducting bias and stereotype research.

Abstract: Social psychologists have shown that Warmth (W) and Competence (C) are the
primary dimensions along which we assess other people and groups. These
dimensions impact various aspects of our lives from social competence and
emotion regulation to success in the work place and how we view the world. More
recent work has started to explore how these dimensions develop, why they have
developed, and what they constitute. Of particular note, is the finding that
warmth has two distinct components: Trust (T) and Sociability (S). In this
work, we introduce Words of Warmth, the first large-scale repository of
manually derived word--warmth (as well as word--trust and word--sociability)
associations for over 26k English words. We show that the associations are
highly reliable. We use the lexicons to study the rate at which children
acquire WCTS words with age. Finally, we show that the lexicon enables a wide
variety of bias and stereotype research through case studies on various target
entities. Words of Warmth is freely available at:
http://saifmohammad.com/warmth.html

</details>


### [82] [Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era](https://arxiv.org/abs/2506.03994)
*Dan Oneata,Desmond Elliott,Stella Frank*

Main category: cs.CL

TL;DR: This paper examines how well large-scale models trained on extensive data represent semantic features of concrete object concepts by using probing tasks to assess the awareness of object properties.


<details>
  <summary>Details</summary>
Motivation: To investigate the relationship between human learning grounded in sensorimotor experience and state-of-the-art foundation models.

Method: Using probing tasks to evaluate image encoders trained on image data, multimodal image encoders, and language-only models on predicting extended versions of McRae norms and Binder dataset.

Result: Multimodal image encoders slightly outperform language-only approaches; image-only encoders perform comparably to language models on non-visual attributes.

Conclusion: The study provides new insights into unimodal learning and the complementarity of modalities.

Abstract: Human learning and conceptual representation is grounded in sensorimotor
experience, in contrast to state-of-the-art foundation models. In this paper,
we investigate how well such large-scale models, trained on vast quantities of
data, represent the semantic feature norms of concrete object concepts, e.g. a
ROSE is red, smells sweet, and is a flower. More specifically, we use probing
tasks to test which properties of objects these models are aware of. We
evaluate image encoders trained on image data alone, as well as
multimodally-trained image encoders and language-only models, on predicting an
extended denser version of the classic McRae norms and the newer Binder dataset
of attribute ratings. We find that multimodal image encoders slightly
outperform language-only approaches, and that image-only encoders perform
comparably to the language models, even on non-visual attributes that are
classified as "encyclopedic" or "function". These results offer new insights
into what can be learned from pure unimodal learning, and the complementarity
of the modalities.

</details>


### [83] [QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering](https://arxiv.org/abs/2506.04020)
*An Quang Tang,Xiuzhen Zhang,Minh Ngoc Dinh,Zhuang Li*

Main category: cs.CL

TL;DR: This paper introduces a new task called Quantitative Query-Focused Summarization (QQSUM) which summarizes diverse customer opinions into Key Points (KPs) to answer user queries more effectively. The authors extend Retrieval-Augmented Generation (RAG) to create QQSUM-RAG which uses few-shot learning to jointly train a KP-oriented retriever and a KP summary generator.


<details>
  <summary>Details</summary>
Motivation: Existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions.

Method: The authors extend RAG to create QQSUM-RAG which employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator.

Result: Experimental results show that QQSUM-RAG outperforms state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions.

Conclusion: The proposed method provides a more effective way to answer user queries by summarizing diverse customer opinions into Key Points.

Abstract: Review-based Product Question Answering (PQA) allows e-commerce platforms to
automatically address customer queries by leveraging insights from user
reviews. However, existing PQA systems generate answers with only a single
perspective, failing to capture the diversity of customer opinions. In this
paper we introduce a novel task Quantitative Query-Focused Summarization
(QQSUM), which aims to summarize diverse customer opinions into representative
Key Points (KPs) and quantify their prevalence to effectively answer user
queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its
generated answers still fall short of capturing the full diversity of
viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,
employs few-shot learning to jointly train a KP-oriented retriever and a KP
summary generator, enabling KP-based summaries that capture diverse and
representative opinions. Experimental results demonstrate that QQSUM-RAG
achieves superior performance compared to state-of-the-art RAG baselines in
both textual quality and quantification accuracy of opinions. Our source code
is available at: https://github.com/antangrocket1312/QQSUMM

</details>


### [84] [AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data](https://arxiv.org/abs/2506.04032)
*Sina Rashidian,Nan Li,Jonathan Amar,Jong Ha Lee,Sam Pugh,Eric Yang,Geoff Masterson,Myoung Cha,Yugang Jia,Akhil Vaid*

Main category: cs.CL

TL;DR: We developed a Patient Simulator using real EHR data to create clinical scenarios and tested its validity against expert clinicians.


<details>
  <summary>Details</summary>
Motivation: To train and test AI health agents using realistic patient simulations.

Method: Derived patient vignettes from EHR data and evaluated the simulator's performance across over 500 vignettes with multi-turn conversations.

Result: Clinicians found the simulator consistent with patient vignettes in 97.7% of cases and the extracted case summary 99% relevant.

Conclusion: The Patient Simulator can be used to train and test multi-turn conversational AI agents at scale.

Abstract: Background: We present a Patient Simulator that leverages real world patient
encounters which cover a broad range of conditions and symptoms to provide
synthetic test subjects for development and testing of healthcare agentic
models. The simulator provides a realistic approach to patient presentation and
multi-turn conversation with a symptom-checking agent. Objectives: (1) To
construct and instantiate a Patient Simulator to train and test an AI health
agent, based on patient vignettes derived from real EHR data. (2) To test the
validity and alignment of the simulated encounters provided by the Patient
Simulator to expert human clinical providers. (3) To illustrate the evaluation
framework of such an LLM system on the generated realistic, data-driven
simulations -- yielding a preliminary assessment of our proposed system.
Methods: We first constructed realistic clinical scenarios by deriving patient
vignettes from real-world EHR encounters. These vignettes cover a variety of
presenting symptoms and underlying conditions. We then evaluate the performance
of the Patient Simulator as a simulacrum of a real patient encounter across
over 500 different patient vignettes. We leveraged a separate AI agent to
provide multi-turn questions to obtain a history of present illness. The
resulting multiturn conversations were evaluated by two expert clinicians.
Results: Clinicians scored the Patient Simulator as consistent with the patient
vignettes in those same 97.7% of cases. The extracted case summary based on the
conversation history was 99% relevant. Conclusions: We developed a methodology
to incorporate vignettes derived from real healthcare patient data to build a
simulation of patient responses to symptom checking agents. The performance and
alignment of this Patient Simulator could be used to train and test a
multi-turn conversational AI agent at scale.

</details>


### [85] [The mutual exclusivity bias of bilingual visually grounded speech models](https://arxiv.org/abs/2506.04037)
*Dan Oneata,Leanne Nortje,Yevgen Matusevych,Herman Kamper*

Main category: cs.CL

TL;DR: A study examines mutual exclusivity (ME), a language learning strategy, in bilingual visually grounded speech models trained on English, French, and Dutch. Results indicate a weaker ME bias in bilingual models compared to monolingual ones.


<details>
  <summary>Details</summary>
Motivation: Investigate how mutual exclusivity, which helps children learn language by associating new words with new objects, is affected in bilingual contexts where cross-lingual ambiguity might reduce its use.

Method: Train bilingual visually grounded speech models on combinations of English, French, and Dutch and analyze their mutual exclusivity biases compared to monolingual models.

Result: Bilingual models show a weaker ME bias than monolingual models, explained by smaller variance in combined visual embeddings for familiar data leading to more confusion between novel and familiar concepts.

Conclusion: This computational exploration provides insights into the weaker ME bias in bilingual models and sheds light on the original existence of ME bias in visually grounded speech models.

Abstract: Mutual exclusivity (ME) is a strategy where a novel word is associated with a
novel object rather than a familiar one, facilitating language learning in
children. Recent work has found an ME bias in a visually grounded speech (VGS)
model trained on English speech with paired images. But ME has also been
studied in bilingual children, who may employ it less due to cross-lingual
ambiguity. We explore this pattern computationally using bilingual VGS models
trained on combinations of English, French, and Dutch. We find that bilingual
models generally exhibit a weaker ME bias than monolingual models, though
exceptions exist. Analyses show that the combined visual embeddings of
bilingual models have a smaller variance for familiar data, partly explaining
the increase in confusion between novel and familiar concepts. We also provide
new insights into why the ME bias exists in VGS models in the first place. Code
and data: https://github.com/danoneata/me-vgs

</details>


### [86] [LexTime: A Benchmark for Temporal Ordering of Legal Events](https://arxiv.org/abs/2506.04041)
*Claire Barale,Leslie Barrett,Vikram Sunil Bajaj,Michael Rovatsos*

Main category: cs.CL

TL;DR: This paper introduces LexTime, a new dataset for evaluating LLMs' event ordering capabilities in legal language.


<details>
  <summary>Details</summary>
Motivation: Temporal reasoning in legal texts is important for applications like case law analysis and compliance monitoring.

Method: Introduce LexTime dataset consisting of 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations.

Result: (1) LLMs are more accurate on legal event ordering than on narrative (up to +10.5%); (2) longer input contexts and implicit events boost accuracy, reaching 80.8% for implicit-explicit event pairs; (3) legal linguistic complexities and nested clauses remain a challenge.

Conclusion: LexTime is the first dataset designed to evaluate LLMs' event ordering capabilities in legal language.

Abstract: Temporal reasoning in legal texts is important for applications like case law
analysis and compliance monitoring. However, existing datasets lack expert
language evaluation, leaving a gap in understanding how LLMs manage event
ordering in legal contexts. We introduce LexTime, the first dataset designed to
evaluate LLMs' event ordering capabilities in legal language, consisting of 512
instances from U.S. Federal Complaints with annotated event pairs and their
temporal relations. Our findings show that (1) LLMs are more accurate on legal
event ordering than on narrative (up to +10.5%); (2) longer input contexts and
implicit events boost accuracy, reaching 80.8% for implicit-explicit event
pairs; (3) legal linguistic complexities and nested clauses remain a challenge.
We investigate how context length, explicit vs implicit event pairs, and legal
language features affect model performance, demonstrating the need for specific
modeling strategies to enhance temporal event reasoning.

</details>


### [87] [Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness](https://arxiv.org/abs/2506.04042)
*Xiyu Liu,Zhengxiao Liu,Naibin Gu,Zheng Lin,Ji Xiang,Weiping Wang*

Main category: cs.CL

TL;DR: This paper introduces a novel two-stage optimization process for controllable knowledge editing in large language models. The proposed method addresses a shortcut learning issue by balancing the learning of subject and relation features.


<details>
  <summary>Details</summary>
Motivation: To ensure minimal side effects on unrelated knowledge while editing target knowledge in large language models.

Method: A two-stage optimization process that balances the learning of subject and relation features.

Result: Experimental results show successful prevention of shortcut learning and optimal overall performance.

Conclusion: The proposed approach contributes to controllable knowledge editing in large language models.

Abstract: Knowledge editing aims to alternate the target knowledge predicted by large
language models while ensuring the least side effects on unrelated knowledge.
An effective way to achieve knowledge editing is to identify pivotal parameters
for predicting factual associations and modify them with an optimization
process to update the predictions. However, these locate-then-edit methods are
uncontrollable since they tend to modify most unrelated relations connected to
the subject of target editing. We unveil that this failure of controllable
editing is due to a shortcut learning issue during the optimization process.
Specifically, we discover two crucial features that are the subject feature and
the relation feature for models to learn during optimization, but the current
optimization process tends to over-learning the subject feature while
neglecting the relation feature. To eliminate this shortcut learning of the
subject feature, we propose a novel two-stage optimization process that
balances the learning of the subject feature and the relation feature.
Experimental results demonstrate that our approach successfully prevents
knowledge editing from shortcut learning and achieves the optimal overall
performance, contributing to controllable knowledge editing.

</details>


### [88] [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)
*Mikel K. Ngueajio,Flor Miriam Plaza-del-Arco,Yi-Ling Chung,Danda B. Rawat,Amanda Cercas Curry*

Main category: cs.CL

TL;DR: This study evaluates large language model-generated automated counter-narratives across four dimensions, finding them often verbose and accessible only to those with college-level literacy.


<details>
  <summary>Details</summary>
Motivation: To mitigate online hate speech by addressing concerns about the affective tone, accessibility, and ethical risks of automated counter-narratives generated by large language models.

Method: Proposing a framework for evaluation and using three different large language models with three prompting strategies on two datasets.

Result: LLM-generated counter-narratives are often verbose and require college-level literacy, with emotionally guided prompts providing more empathetic responses but still raising safety and effectiveness concerns.

Conclusion: The study highlights the need for further research into improving the accessibility, safety, and effectiveness of automated counter-narratives.

Abstract: Automated counter-narratives (CN) offer a promising strategy for mitigating
online hate speech, yet concerns about their affective tone, accessibility, and
ethical risks remain. We propose a framework for evaluating Large Language
Model (LLM)-generated CNs across four dimensions: persona framing, verbosity
and readability, affective tone, and ethical robustness. Using GPT-4o-Mini,
Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting
strategies on the MT-Conan and HatEval datasets. Our findings reveal that
LLM-generated CNs are often verbose and adapted for people with college-level
literacy, limiting their accessibility. While emotionally guided prompts yield
more empathetic and readable responses, there remain concerns surrounding
safety and effectiveness.

</details>


### [89] [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/abs/2506.04044)
*Aleksey Kudelya,Alexander Shirnin*

Main category: cs.CL

TL;DR: LIBU: An algorithm combining influence functions and second-order optimization for unlearning tasks in large language models.


<details>
  <summary>Details</summary>
Motivation: To solve the task of unlearning - removing specific knowledge from a large language model without retraining from scratch and compromising its overall utility.

Method: Combining classical influence functions and second-order optimization.

Result: This lightweight approach is well applicable for unlearning LLMs in different kinds of task.

Conclusion: LIBU can effectively remove the influence of specific data from the model and stabilize the overall utility.

Abstract: This paper describes LIBU (LoRA enhanced influence-based unlearning), an
algorithm to solve the task of unlearning - removing specific knowledge from a
large language model without retraining from scratch and compromising its
overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large
Language Models). The algorithm combines classical \textit{influence functions}
to remove the influence of the data from the model and \textit{second-order
optimization} to stabilize the overall utility. Our experiments show that this
lightweight approach is well applicable for unlearning LLMs in different kinds
of task.

</details>


### [90] [On Support Samples of Next Word Prediction](https://arxiv.org/abs/2506.04047)
*Yuqian Li,Yupei Du,Yufang Liu,Feifei Feng,Mou Xiao Feng,Yuanbin Wu*

Main category: cs.CL

TL;DR: This paper studies data-centric interpretability in language models focusing on next-word prediction. It identifies two types of support samples and highlights the importance of non-support samples in preventing overfitting and shaping generalization.


<details>
  <summary>Details</summary>
Motivation: Understanding the rationale behind language model decisions is challenging. This paper aims to investigate data-centric interpretability to shed light on this.

Method: Using representer theorem, the paper identifies two types of support samples and examines the role of non-support samples.

Result: Support samples are intrinsic properties predictable before training. Non-support samples play a critical role in preventing overfitting and shaping generalization, especially in deeper layers.

Conclusion: The study offers a new perspective on understanding language model behavior and interpretability by examining the interaction between data and model decisions.

Abstract: Language models excel in various tasks by making complex decisions, yet
understanding the rationale behind these decisions remains a challenge. This
paper investigates \emph{data-centric interpretability} in language models,
focusing on the next-word prediction task. Using representer theorem, we
identify two types of \emph{support samples}-those that either promote or deter
specific predictions. Our findings reveal that being a support sample is an
intrinsic property, predictable even before training begins. Additionally,
while non-support samples are less influential in direct predictions, they play
a critical role in preventing overfitting and shaping generalization and
representation learning. Notably, the importance of non-support samples
increases in deeper layers, suggesting their significant role in intermediate
representation formation.These insights shed light on the interplay between
data and model decisions, offering a new dimension to understanding language
model behavior and interpretability.

</details>


### [91] [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/abs/2506.04050)
*Hadi Mohammadi,Anastasia Giachanou,Daniel L. Oberski,Ayoub Bagheri*

Main category: cs.CL

TL;DR: 本文探讨了如何使用XAI方法减少AI生成文本的可检测性，并提出了一种稳健的集成检测方法，结果显示XAI方法可以有效降低检测难度，但需要更复杂的检测策略来应对不断变化的隐藏技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能够生成类似人类的文本，但其输出通常表现出一些模式，使得其比人类书写的文本更容易被检测到。因此，研究如何利用XAI方法减少AI生成文本的可检测性。

Method: 使用SHAP和LIME识别影响分类器预测的关键标记，并提出四种基于解释性的标记替换策略来修改这些有影响力的关键标记。

Result: 所提出的标记替换方法可以显著削弱单一分类器对AI生成文本的检测能力，但是集成分类器在多种语言和领域上表现出了强大的性能。

Conclusion: XAI方法可以显著降低AI生成文本的可检测性，但需要稳健的集成检测策略来适应隐藏AI生成文本的方法的变化。

Abstract: Generative models, especially large language models (LLMs), have shown
remarkable progress in producing text that appears human-like. However, they
often exhibit patterns that make their output easier to detect than text
written by humans. In this paper, we investigate how explainable AI (XAI)
methods can be used to reduce the detectability of AI-generated text (AIGT)
while also introducing a robust ensemble-based detection approach. We begin by
training an ensemble classifier to distinguish AIGT from human-written text,
then apply SHAP and LIME to identify tokens that most strongly influence its
predictions. We propose four explainability-based token replacement strategies
to modify these influential tokens. Our findings show that these token
replacement approaches can significantly diminish a single classifier's ability
to detect AIGT. However, our ensemble classifier maintains strong performance
across multiple languages and domains, showing that a multi-model approach can
mitigate the impact of token-level manipulations. These results show that XAI
methods can make AIGT harder to detect by focusing on the most influential
tokens. At the same time, they highlight the need for robust, ensemble-based
detection strategies that can adapt to evolving approaches for hiding AIGT.

</details>


### [92] [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/abs/2506.04051)
*Tim Franzmeyer,Archie Sravankumar,Lijuan Liu,Yuning Mao,Rui Hou,Sinong Wang,Jakob N. Foerster,Luke Zettlemoyer,Madian Khabsa*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) can produce incorrect answers (hallucinate). To address this, the paper proposes HALT, which fine-tunes LLMs to be more correct at the cost of completeness.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs leads to incorrect answers, which is undesirable.

Method: Post-train LLMs using HALT, which generates capability-aligned post-training data by splitting responses into factual fragments and identifying incorrect ones. Responses are then finetuned by removing or replacing incorrect fragments with 'Unsure from Here' based on a tunable threshold.

Result: HALT improves the mean correctness of response fragments by 15% on average and increases the F1 score by 4%. A single reliable Llama3-70B model achieves 87% correctness across four domains while maintaining 53% of the standard finetuning response completeness.

Conclusion: HALT effectively trades off response completeness for correctness, leading to more reliable LLMs.

Abstract: Large Language Models (LLMs) currently respond to every prompt. However, they
can produce incorrect answers when they lack knowledge or capability -- a
problem known as hallucination. We instead propose post-training an LLM to
generate content only when confident in its correctness and to otherwise
(partially) abstain. Specifically, our method, HALT, produces
capability-aligned post-training data that encodes what the model can and
cannot reliably generate. We generate this data by splitting responses of the
pretrained LLM into factual fragments (atomic statements or reasoning steps),
and use ground truth information to identify incorrect fragments. We achieve
capability-aligned finetuning responses by either removing incorrect fragments
or replacing them with "Unsure from Here" -- according to a tunable threshold
that allows practitioners to trade off response completeness and mean
correctness of the response's fragments. We finetune four open-source models
for biography writing, mathematics, coding, and medicine with HALT for three
different trade-off thresholds. HALT effectively trades off response
completeness for correctness, increasing the mean correctness of response
fragments by 15% on average, while resulting in a 4% improvement in the F1
score (mean of completeness and correctness of the response) compared to the
relevant baselines. By tuning HALT for highest correctness, we train a single
reliable Llama3-70B model with correctness increased from 51% to 87% across all
four domains while maintaining 53% of the response completeness achieved with
standard finetuning.

</details>


### [93] [Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning](https://arxiv.org/abs/2506.04065)
*Muling Wu,Qi Qian,Wenhao Liu,Xiaohua Wang,Zisu Huang,Di Liang,LI Miao,Shihan Dou,Changze Lv,Zhenghua Wang,Zhibo Xu,Lina Chen,Tianlong Li,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.CL

TL;DR: Customized Curriculum Learning (CCL) improves large language models' performance on mathematical reasoning benchmarks by optimizing sample difficulty and utilization.


<details>
  <summary>Details</summary>
Motivation: Current post-training methods for LLMs suffer from inefficient sample usage and rigid difficulty handling.

Method: Introduces model-adaptive difficulty definition and Guided Prompting to customize curricula and hint samples.

Result: CCL outperforms uniform training across five mathematical reasoning benchmarks.

Conclusion: CCL enhances sample utilization and model performance across different training paradigms.

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
various reasoning tasks, yet post-training is constrained by inefficient sample
utilization and inflexible difficulty samples processing. To address these
limitations, we propose Customized Curriculum Learning (CCL), a novel framework
with two key innovations. First, we introduce model-adaptive difficulty
definition that customizes curriculum datasets based on each model's individual
capabilities rather than using predefined difficulty metrics. Second, we
develop "Guided Prompting," which dynamically reduces sample difficulty through
strategic hints, enabling effective utilization of challenging samples that
would otherwise degrade performance. Comprehensive experiments on supervised
fine-tuning and reinforcement learning demonstrate that CCL significantly
outperforms uniform training approaches across five mathematical reasoning
benchmarks, confirming its effectiveness across both paradigms in enhancing
sample utilization and model performance.

</details>


### [94] [LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward](https://arxiv.org/abs/2506.04070)
*Yi Zhao,Siqi Wang,Jing Li*

Main category: cs.CL

TL;DR: This paper presents LaF-GRPO, a method using LLMs to simulate visually impaired user responses for generating navigation instructions, which improves instruction quality and reduces reliance on real-world data. The paper also introduces NIG4VI, a benchmark dataset for evaluating such systems.


<details>
  <summary>Details</summary>
Motivation: To develop precise, practical navigation instructions for visually impaired individuals due to the lack of effective solutions.

Method: Proposes LaF-GRPO where an LLM generates rewards to guide VLM post-training; introduces NIG4VI as a benchmark dataset.

Result: LaF-GRPO shows improved performance in generating navigation instructions compared to other methods, with better quantitative metrics like BLEU and METEOR scores.

Conclusion: The proposed method effectively enhances the usability of navigation instructions for visually impaired users and reduces the need for real-world data.

Abstract: Navigation instruction generation for visually impaired (VI) individuals
(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses
on producing precise, in-situ, step-by-step navigation instructions that are
practically usable by VI users. Concretely, we propose LaF-GRPO
(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate
rewards guiding the Vision-Language Model (VLM) post-training. This enhances
instruction usability while reducing costly real-world data needs. To
facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced
benchmark. It provides diverse navigation scenarios with accurate spatial
coordinates, supporting detailed, open-ended in-situ instruction generation.
Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative
metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542
vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and
benchmark are available at
\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.

</details>


### [95] [Controlling Difficulty of Generated Text for AI-Assisted Language Learning](https://arxiv.org/abs/2506.04072)
*Meiqing Jin,Liam Dugan,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 这项工作探索了如何使用可控生成技术来调整大型语言模型的输出，以更好地支持绝对初学者的语言学习。研究发现，使用未来判别器可以显著提高输出的可理解性，并提出了一种新的评估指标Token Miss Rate (TMR)。


<details>
  <summary>Details</summary>
Motivation: 实践对话与大型语言模型(LLMs)提供了传统面对面语言学习的有希望的替代方案。然而，大多数LLMs生成的文本在接近本地复杂度的水平上，使它们不适合初学者学习者(CEFR: A1-A2)。

Method: 研究了可控生成技术，特别是不需要模型微调的模块化方法，是否可以适应LLM输出以更好地支持绝对初学者。通过自动度量和大学日语学习者的用户研究来评估这些方法。

Result: 虽然单独使用提示无法控制输出难度，但使用未来判别器显著提高了输出的可理解性（从40.4％提高到84.3％）。

Conclusion: 使用未来判别器显著提高了输出的可理解性，并且引入了一个新的Token Miss Rate (TMR)评估指标，该指标量化了每个话语中不可理解的标记比例，并与人类判断密切相关。为了支持AI辅助语言学习的未来研究，我们发布了我们的代码、模型、注释工具和数据集。

Abstract: Practicing conversations with large language models (LLMs) presents a
promising alternative to traditional in-person language learning. However, most
LLMs generate text at a near-native level of complexity, making them ill-suited
for beginner learners (CEFR: A1-A2). In this paper, we investigate whether
controllable generation techniques -- specifically modular methods that do not
require model fine-tuning -- can adapt LLM outputs to better support absolute
beginners. We evaluate these methods through both automatic metrics and a user
study with university-level learners of Japanese. Our findings show that while
prompting alone fails to control output difficulty, the use of future
discriminators (Yang and Klein, 2021) significantly improves output
comprehensibility (from 40.4\% to 84.3\%). We further introduce a novel
token-level evaluation metric, Token Miss Rate (TMR), that quantifies the
proportion of incomprehensible tokens per utterance and correlates strongly
with human judgments. To support future research in AI-assisted language
learning, we release our code, models, annotation tools, and dataset.

</details>


### [96] [Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems](https://arxiv.org/abs/2506.04076)
*Jhen-Ke Lin,Hao-Chien Lu,Chung-Chun Wang,Hong-Yun Lin,Berlin Chen*

Main category: cs.CL

TL;DR: This paper investigates the impact of different annotation schemes on the performance of ASR systems for transcribing disfluent speech.


<details>
  <summary>Details</summary>
Motivation: ASR systems often discard or generalize hesitations, losing important acoustic details which are crucial for downstream tasks like error analysis and feedback.

Method: Fine-tuning Whisper models on the Speak & Improve 2025 corpus using low-rank adaptation (LoRA), comparing three annotation schemes: Pure, Rich, and Extra.

Result: The 'Extra' scheme, which uses acoustically precise fillers inferred by Gemini 2.0 Flash, yielded a 5.5% WER, an 11.3% relative improvement over the 'Pure' scheme (6.2% WER).

Conclusion: Explicit, realistic filled-pause labeling significantly enhances ASR accuracy for verbatim L2 speech transcription.

Abstract: Verbatim transcription for automatic speaking assessment demands accurate
capture of disfluencies, crucial for downstream tasks like error analysis and
feedback. However, many ASR systems discard or generalize hesitations, losing
important acoustic details. We fine-tune Whisper models on the Speak & Improve
2025 corpus using low-rank adaptation (LoRA), without recourse to external
audio training data. We compare three annotation schemes: removing hesitations
(Pure), generic tags (Rich), and acoustically precise fillers inferred by
Gemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge
system achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge
experiments reveal that fine-tuning Whisper Large V3 Turbo with the "Extra"
scheme yielded a 5.5% WER, an 11.3% relative improvement over the "Pure" scheme
(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling
significantly enhances ASR accuracy for verbatim L2 speech transcription.

</details>


### [97] [A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions](https://arxiv.org/abs/2506.04077)
*Chung-Chun Wang,Jhen-Ke Lin,Hao-Chien Lu,Hong-Yun Lin,Berlin Chen*

Main category: cs.CL

TL;DR: 提出了一种利用大语言模型生成多样化响应并结合动态重要性损失的新型训练范式来解决自动化口语评估中的标注录音稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 自动化口语评估在表达意见时受限于标注录音的稀缺性，影响了评分的可靠性。

Method: 利用大语言模型生成指定熟练度的多样化响应，通过与语音合成器结合转换为合成语音，并使用动态重要性损失重新加权训练实例，最后用多模态大语言模型整合文本特征和语音信号预测熟练度评分。

Result: 实验表明该方法优于仅依赖真实数据或常规增强的方法，有效缓解了低资源约束问题。

Conclusion: 所提出的方法可以实现跨模态信息的自动化口语评估。

Abstract: Automated speaking assessment (ASA) on opinion expressions is often hampered
by the scarcity of labeled recordings, which restricts prompt diversity and
undermines scoring reliability. To address this challenge, we propose a novel
training paradigm that leverages a large language models (LLM) to generate
diverse responses of a given proficiency level, converts responses into
synthesized speech via speaker-aware text-to-speech synthesis, and employs a
dynamic importance loss to adaptively reweight training instances based on
feature distribution differences between synthesized and real speech.
Subsequently, a multimodal large language model integrates aligned textual
features with speech signals to predict proficiency scores directly.
Experiments conducted on the LTTC dataset show that our approach outperforms
methods relying on real data or conventional augmentation, effectively
mitigating low-resource constraints and enabling ASA on opinion expressions
with cross-modal information.

</details>


### [98] [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
*Ming Zhang,Yujiong Shen,Zelin Li,Huayu Sha,Binze Hu,Yuhui Wang,Chenhao Huang,Shichun Liu,Jingqi Tong,Changhao Jiang,Mingxu Chai,Zhiheng Xi,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出LLMEval-Med，一个包含2996个问题的新基准，用于评估医学领域的大型语言模型，解决了现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前医学基准在问题设计、数据来源和评估方法上存在局限性，需要一种新的基准来解决这些问题。

Method: 创建了包含五个核心医学领域的2,996个问题，并设计了一个自动评估管道，将专家开发的清单纳入LLM-as-Judge框架中。此外，通过人机一致性分析验证机器评分，并根据专家反馈动态调整清单和提示以确保可靠性。

Result: LLMEval-Med涵盖了五个核心医学领域，包括2,996个来自真实电子健康记录和专家设计临床场景的问题。它还设计了一个自动评估管道，并评估了三个类别中的13个LLMs。

Conclusion: 提出的新基准LLMEval-Med为医学领域的大语言模型的安全和有效部署提供了有价值的见解。

Abstract: Evaluating large language models (LLMs) in medicine is crucial because
medical applications require high accuracy with little room for error. Current
medical benchmarks have three main types: medical exam-based, comprehensive
medical, and specialized assessments. However, these benchmarks have
limitations in question design (mostly multiple-choice), data sources (often
not derived from real clinical scenarios), and evaluation methods (poor
assessment of complex reasoning). To address these issues, we present
LLMEval-Med, a new benchmark covering five core medical areas, including 2,996
questions created from real-world electronic health records and expert-designed
clinical scenarios. We also design an automated evaluation pipeline,
incorporating expert-developed checklists into our LLM-as-Judge framework.
Furthermore, our methodology validates machine scoring through human-machine
agreement analysis, dynamically refining checklists and prompts based on expert
feedback to ensure reliability. We evaluate 13 LLMs across three categories
(specialized medical models, open-source models, and closed-source models) on
LLMEval-Med, providing valuable insights for the safe and effective deployment
of LLMs in medical domains. The dataset is released in
https://github.com/llmeval/LLMEval-Med.

</details>


### [99] [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)
*Pedro Henrique Martins,João Alves,Patrick Fernandes,Nuno M. Guerreiro,Ricardo Rei,Amin Farajian,Mateusz Klimaszewski,Duarte M. Alves,José Pombal,Manuel Faysse,Pierre Colombo,François Yvon,Barry Haddow,José G. C. de Souza,Alexandra Birch,André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-9B is a large language model trained from scratch to support European citizens with 35 languages. It addresses the lack of representation for European languages in open large language models.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation and underserved status of European languages in existing open large language models.

Method: Trained from scratch with a focus on European languages, including tokenizer design, architectural specifications, data filtering, and training procedures. Developed EuroFilter for data filtering and EuroBlocks-Synthetic for post-training.

Result: Competitive performance on multilingual benchmarks and machine translation tasks, established as the leading open European-made LLM of its size.

Conclusion: EuroLLM-9B is a significant advancement in supporting European languages in large language models and is released openly to support research and adoption.

Abstract: This report presents EuroLLM-9B, a large language model trained from scratch
to support the needs of European citizens by covering all 24 official European
Union languages and 11 additional languages. EuroLLM addresses the issue of
European languages being underrepresented and underserved in existing open
large language models. We provide a comprehensive overview of EuroLLM-9B's
development, including tokenizer design, architectural specifications, data
filtering, and training procedures. We describe the pre-training data
collection and filtering pipeline, including the creation of EuroFilter, an
AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a
novel synthetic dataset for post-training that enhances language coverage for
European languages. Evaluation results demonstrate EuroLLM-9B's competitive
performance on multilingual benchmarks and machine translation tasks,
establishing it as the leading open European-made LLM of its size. To support
open research and adoption, we release all major components of this work,
including the base and instruction-tuned models, the EuroFilter classifier, and
the synthetic post-training dataset.

</details>


### [100] [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)
*Wenhao Li,Wenwu Li,Chuyun Shen,Junjie Sheng,Zixiao Huang,Di Wu,Yun Hua,Wei Yin,Xiangfeng Wang,Hongyuan Zha,Bo Jin*

Main category: cs.CL

TL;DR: TextAtari is a benchmark for evaluating language agents on very long-horizon decision-making tasks.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps.

Method: An unsupervised representation learning framework (AtariARI) was used to translate visual state representations of classic Atari games into rich textual descriptions, creating a challenging test bed for sequential decision-making with natural language processing.

Result: The results reveal significant performance gaps between language agents and human players in extensive planning tasks.

Conclusion: TextAtari highlights the challenges faced by language agents in long-horizon decision-making tasks.

Abstract: We present TextAtari, a benchmark for evaluating language agents on very
long-horizon decision-making tasks spanning up to 100,000 steps. By translating
the visual state representations of classic Atari games into rich textual
descriptions, TextAtari creates a challenging test bed that bridges sequential
decision-making with natural language processing. The benchmark includes nearly
100 distinct tasks with varying complexity, action spaces, and planning
horizons, all rendered as text through an unsupervised representation learning
framework (AtariARI). We evaluate three open-source large language models
(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks
(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how
different forms of prior knowledge affect performance on these long-horizon
challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and
Reference-based-investigate the impact of semantic understanding, instruction
comprehension, and expert demonstrations on agent decision-making. Our results
reveal significant performance gaps between language agents and human players
in extensive planning tasks, highlighting challenges in sequential reasoning,
state tracking, and strategic planning across tens of thousands of steps.
TextAtari provides standardized evaluation protocols, baseline implementations,
and a framework for advancing research at the intersection of language models
and planning.

</details>


### [101] [Rectified Sparse Attention](https://arxiv.org/abs/2506.04108)
*Yutao Sun,Tianzhu Ye,Li Dong,Yuqing Xia,Jian Chen,Yizhao Gao,Shijie Cao,Jianyong Wang,Furu Wei*

Main category: cs.CL

TL;DR: Introduces ReSA for efficient long-sequence generation in large language models.


<details>
  <summary>Details</summary>
Motivation: To address the issue of KV cache misalignment in recent sparse decoding methods.

Method: Rectified Sparse Attention (ReSA), which combines block-sparse attention with periodic dense rectification.

Result: ReSA achieves significant efficiency improvement with high generation quality.

Conclusion: ReSA improves the efficiency of long-sequence generation with near-lossless quality.

Abstract: Efficient long-sequence generation is a critical challenge for Large Language
Models. While recent sparse decoding methods improve efficiency, they suffer
from KV cache misalignment, where approximation errors accumulate and degrade
generation quality. In this work, we propose Rectified Sparse Attention (ReSA),
a simple yet effective method that combines block-sparse attention with
periodic dense rectification. By refreshing the KV cache at fixed intervals
using a dense forward pass, ReSA bounds error accumulation and preserves
alignment with the pretraining distribution. Experiments across math reasoning,
language modeling, and retrieval tasks demonstrate that ReSA achieves
near-lossless generation quality with significantly improved efficiency.
Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at
256K sequence length, making it a practical solution for scalable long-context
inference. Code is available at https://aka.ms/ReSA-LM.

</details>


### [102] [CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues](https://arxiv.org/abs/2506.04131)
*Disha Sheshanarayana,Tanishka Magar,Ayushi Mittal,Neelam Chaplot*

Main category: cs.CL

TL;DR: This paper presents LegalCon, a dataset for manipulation detection in courtroom conversations, and CLAIM, a framework to improve fairness and transparency in judicial processes through enhanced manipulation analysis.


<details>
  <summary>Details</summary>
Motivation: To address the gap in using NLP for detecting and analyzing manipulation within the legal domain

Method: Introducing LegalCon dataset and CLAIM framework

Result: A dataset of 1,063 annotated courtroom conversations and a two-stage Intent-driven Multi-agent framework called CLAIM

Conclusion: The paper introduces LegalCon, a dataset for manipulation detection in courtroom conversations, and CLAIM, a framework for enhancing such analysis.

Abstract: Courtrooms are places where lives are determined and fates are sealed, yet
they are not impervious to manipulation. Strategic use of manipulation in legal
jargon can sway the opinions of judges and affect the decisions. Despite the
growing advancements in NLP, its application in detecting and analyzing
manipulation within the legal domain remains largely unexplored. Our work
addresses this gap by introducing LegalCon, a dataset of 1,063 annotated
courtroom conversations labeled for manipulation detection, identification of
primary manipulators, and classification of manipulative techniques, with a
focus on long conversations. Furthermore, we propose CLAIM, a two-stage,
Intent-driven Multi-agent framework designed to enhance manipulation analysis
by enabling context-aware and informed decision-making. Our results highlight
the potential of incorporating agentic frameworks to improve fairness and
transparency in judicial processes. We hope that this contributes to the
broader application of NLP in legal discourse analysis and the development of
robust tools to support fairness in legal decision-making. Our code and data
are available at https://github.com/Disha1001/CLAIM.

</details>


### [103] [Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?](https://arxiv.org/abs/2506.04139)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 研究探索了LLMs在捕捉弗拉芒日常叙述情感值方面的潜力，发现当前荷兰语调整模型表现不足，强调了开发文化与语言定制模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解日常语言中的细微差别对计算语言学和情绪研究的进步至关重要。然而，日常语言本质上是自发的、丰富的表达性和深度上下文依赖的。

Method: 研究包括约25,000个文本响应，评估了三种Dutch-specific LLMs预测情感值的能力，并将其输出与LIWC和Pattern生成的结果进行比较。

Result: 三种Dutch-specific LLMs在预测情感值方面表现不如LIWC和Pattern，这表明需要更先进的模型来处理日常叙述的情感分析。

Conclusion: 尽管LLMs架构有所进步，但当前荷兰语调整模型在准确捕捉自发真实世界叙述中的情感值方面表现不足。需要开发文化与语言定制的模型/工具来处理自然语言使用的复杂性。

Abstract: Understanding the nuances in everyday language is pivotal for advancements in
computational linguistics & emotions research. Traditional lexicon-based tools
such as LIWC and Pattern have long served as foundational instruments in this
domain. LIWC is the most extensively validated word count based text analysis
tool in the social sciences and Pattern is an open source Python library
offering functionalities for NLP. However, everyday language is inherently
spontaneous, richly expressive, & deeply context dependent. To explore the
capabilities of LLMs in capturing the valences of daily narratives in Flemish,
we first conducted a study involving approximately 25,000 textual responses
from 102 Dutch-speaking participants. Each participant provided narratives
prompted by the question, "What is happening right now and how do you feel
about it?", accompanied by self-assessed valence ratings on a continuous scale
from -50 to +50. We then assessed the performance of three Dutch-specific LLMs
in predicting these valence scores, and compared their outputs to those
generated by LIWC and Pattern. Our findings indicate that, despite advancements
in LLM architectures, these Dutch tuned models currently fall short in
accurately capturing the emotional valence present in spontaneous, real-world
narratives. This study underscores the imperative for developing culturally and
linguistically tailored models/tools that can adeptly handle the complexities
of natural language use. Enhancing automated valence analysis is not only
pivotal for advancing computational methodologies but also holds significant
promise for psychological research with ecologically valid insights into human
daily experiences. We advocate for increased efforts in creating comprehensive
datasets & finetuning LLMs for low-resource languages like Flemish, aiming to
bridge the gap between computational linguistics & emotion research.

</details>


### [104] [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/abs/2506.04142)
*Kejian Zhu,Shangqing Tu,Zhuoran Jin,Lei Hou,Juanzi Li,Jun Zhao*

Main category: cs.CL

TL;DR: This paper proposes a novel method for identifying shortcut neurons in large language models to mitigate contamination issues in evaluations.


<details>
  <summary>Details</summary>
Motivation: Current evaluations rely on public benchmarks which are prone to data contamination issues compromising fairness. Building new benchmarks is costly and cyclical.

Method: Analyzing mechanisms of contaminated models themselves and proposing a method for identifying shortcut neurons through comparative and causal analysis. Introducing an evaluation method called shortcut neuron patching to suppress shortcut neurons.

Result: Experiments validate the effectiveness of the approach in mitigating contamination. Evaluation results show a strong linear correlation with MixEval, a trustworthy benchmark.

Conclusion: The proposed method effectively reveals true capabilities of the models and is trustworthy. Further experiments demonstrate the generalizability of the method across various benchmarks and hyperparameter settings.

Abstract: The development of large language models (LLMs) depends on trustworthy
evaluation. However, most current evaluations rely on public benchmarks, which
are prone to data contamination issues that significantly compromise fairness.
Previous researches have focused on constructing dynamic benchmarks to address
contamination. However, continuously building new benchmarks is costly and
cyclical. In this work, we aim to tackle contamination by analyzing the
mechanisms of contaminated models themselves. Through our experiments, we
discover that the overestimation of contaminated models is likely due to
parameters acquiring shortcut solutions in training. We further propose a novel
method for identifying shortcut neurons through comparative and causal
analysis. Building on this, we introduce an evaluation method called shortcut
neuron patching to suppress shortcut neurons. Experiments validate the
effectiveness of our approach in mitigating contamination. Additionally, our
evaluation results exhibit a strong linear correlation with MixEval, a recently
released trustworthy benchmark, achieving a Spearman coefficient ($\rho$)
exceeding 0.95. This high correlation indicates that our method closely reveals
true capabilities of the models and is trustworthy. We conduct further
experiments to demonstrate the generalizability of our method across various
benchmarks and hyperparameter settings. Code:
https://github.com/GaryStack/Trustworthy-Evaluation

</details>


### [105] [A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization](https://arxiv.org/abs/2506.04156)
*Sarvesh Soni,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: 介绍了一个基于真实患者案例的专家注释数据集ArchEHR-QA，并评估了不同LLMs在EHR问答任务上的性能，强调了提高临床响应事实性和相关性的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集无法捕捉患者在EHR背景下的信息需求，因此需要一个能够评估AI生成响应的事实准确性和相关性的鲁棒数据集。

Method: 引入了ArchEHR-QA数据集，评估了三种开放权重大型语言模型（LLMs）在三种提示策略下的表现。

Result: Llama 4在回答优先提示方法下表现最佳，达到了最高的分数。手动错误分析揭示了常见的问题，如遗漏关键临床证据和矛盾或幻觉内容。

Conclusion: ArchEHR-QA提供了强大的基准，用于开发和评估以患者为中心的EHR问答系统，强调了在临床环境中生成事实性和相关性响应的进一步进展需求。

Abstract: Patients have distinct information needs about their hospitalization that can
be addressed using clinical evidence from electronic health records (EHRs).
While artificial intelligence (AI) systems show promise in meeting these needs,
robust datasets are needed to evaluate the factual accuracy and relevance of
AI-generated responses. To our knowledge, no existing dataset captures patient
information needs in the context of their EHRs. We introduce ArchEHR-QA, an
expert-annotated dataset based on real-world patient cases from intensive care
unit and emergency department settings. The cases comprise questions posed by
patients to public health forums, clinician-interpreted counterparts, relevant
clinical note excerpts with sentence-level relevance annotations, and
clinician-authored answers. To establish benchmarks for grounded EHR question
answering (QA), we evaluated three open-weight large language models
(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:
generating (1) answers with citations to clinical note sentences, (2) answers
before citations, and (3) answers from filtered citations. We assessed
performance on two dimensions: Factuality (overlap between cited note sentences
and ground truth) and Relevance (textual and semantic similarity between system
and reference answers). The final dataset contains 134 patient cases. The
answer-first prompting approach consistently performed best, with Llama 4
achieving the highest scores. Manual error analysis supported these findings
and revealed common issues such as omitted key clinical evidence and
contradictory or hallucinated content. Overall, ArchEHR-QA provides a strong
benchmark for developing and evaluating patient-centered EHR QA systems,
underscoring the need for further progress toward generating factual and
relevant responses in clinical contexts.

</details>


### [106] [SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling](https://arxiv.org/abs/2506.04179)
*Anhao Zhao,Fanghua Ye,Yingqi Fan,Junlong Tong,Zhiwei Fei,Hui Su,Xiaoyu Shen*

Main category: cs.CL

TL;DR: SkipGPT是一种动态层修剪框架，通过token感知路由和组件特定策略减少40%以上的模型参数，同时保持与原始模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的静态修剪方法忽略了大型语言模型推理中两个关键的动态特性：水平动态（token级别的异质性需要上下文感知的修剪决策）和垂直动态（MLP和自注意力层的不同功能角色需要特定于组件的修剪策略）。

Method: SkipGPT引入了全局token感知路由来优先处理关键token，并为MLP和自注意力组件提供了独立的修剪策略。为了减轻训练不稳定性，提出了两阶段优化范式：首先通过软参数化学习路由策略以避免过早的修剪决策，然后通过参数高效的LoRA微调恢复因层移除而受到影响的性能。

Result: 实验表明，SkipGPT减少了超过40%的模型参数，并且在多个基准测试中与原始密集模型的表现相当甚至更好。

Conclusion: SkipGPT通过动态层修剪框架优化了计算资源分配，并在减少超过40%模型参数的同时，在多个基准测试中匹配或超过了原始密集模型的性能。这项工作推进了可扩展、资源感知的大型语言模型的实际部署。

Abstract: Large language models (LLMs) achieve remarkable performance across tasks but
incur substantial computational costs due to their deep, multi-layered
architectures. Layer pruning has emerged as a strategy to alleviate these
inefficiencies, but conventional static pruning methods overlook two critical
dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level
heterogeneity demands context-aware pruning decisions, and (2) vertical
dynamics, where the distinct functional roles of MLP and self-attention layers
necessitate component-specific pruning policies. We introduce SkipGPT, a
dynamic layer pruning framework designed to optimize computational resource
allocation through two core innovations: (1) global token-aware routing to
prioritize critical tokens, and (2) decoupled pruning policies for MLP and
self-attention components. To mitigate training instability, we propose a
two-stage optimization paradigm: first, a disentangled training phase that
learns routing strategies via soft parameterization to avoid premature pruning
decisions, followed by parameter-efficient LoRA fine-tuning to restore
performance impacted by layer removal. Extensive experiments demonstrate that
SkipGPT reduces over 40% of model parameters while matching or exceeding the
performance of the original dense model across benchmarks. By harmonizing
dynamic efficiency with preserved expressivity, SkipGPT advances the practical
deployment of scalable, resource-aware LLMs. Our code is publicly available at:
https://github.com/EIT-NLP/SkipGPT.

</details>


### [107] [SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models](https://arxiv.org/abs/2506.04180)
*Yuhao Wu,Yushi Bai,Zhiqiang Hu,Juanzi Li,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: Propose SuperWriter-Agent, an agent-based framework that improves long-form text generation by introducing planning and refinement stages. Train a 7B parameter SuperWriter-LM using supervised fine-tuning and hierarchical DPO with MCTS. Achieve state-of-the-art performance in automatic and human evaluations.


<details>
  <summary>Details</summary>
Motivation: Maintaining coherence, logical consistency, and text quality in long-form text generation is challenging for large language models.

Method: Introduce SuperWriter-Agent with planning and refinement stages. Construct a supervised fine-tuning dataset for SuperWriter-LM and develop a hierarchical DPO procedure with MCTS.

Result: SuperWriter-LM outperforms larger baseline models in diverse benchmarks based on automatic and human evaluations.

Conclusion: Incorporating structured thinking steps through SuperWriter-Agent enhances the quality of long-form text generation.

Abstract: Long-form text generation remains a significant challenge for large language
models (LLMs), particularly in maintaining coherence, ensuring logical
consistency, and preserving text quality as sequence length increases. To
address these limitations, we propose SuperWriter-Agent, an agent-based
framework designed to enhance the quality and consistency of long-form text
generation. SuperWriter-Agent introduces explicit structured thinking-through
planning and refinement stages into the generation pipeline, guiding the model
to follow a more deliberate and cognitively grounded process akin to that of a
professional writer. Based on this framework, we construct a supervised
fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a
hierarchical Direct Preference Optimization (DPO) procedure that uses Monte
Carlo Tree Search (MCTS) to propagate final quality assessments and optimize
each generation step accordingly. Empirical results across diverse benchmarks
demonstrate that SuperWriter-LM achieves state-of-the-art performance,
surpassing even larger-scale baseline models in both automatic evaluation and
human evaluation. Furthermore, comprehensive ablation studies demonstrate the
effectiveness of hierarchical DPO and underscore the value of incorporating
structured thinking steps to improve the quality of long-form text generation.

</details>


### [108] [Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models](https://arxiv.org/abs/2506.04182)
*Ruiqi Zhang,Changyi Xiao,Yixin Cao*

Main category: cs.CL

TL;DR: Compare the effectiveness of long and short Chain-of-Thought (CoT) prompting strategies in large reasoning models.


<details>
  <summary>Details</summary>
Motivation: Long CoT prompting has shown strong performance but at the cost of higher token usage. Short CoT could be more efficient under tight budget constraints.

Method: Conduct a comprehensive empirical analysis comparing long and short CoT strategies.

Result: SwitchCoT, a framework that adaptively chooses between long and short CoT strategies, reduces inference costs by up to 50% while maintaining high accuracy.

Conclusion: SwitchCoT is a budget-aware framework that balances reasoning accuracy and computational efficiency.

Abstract: With the rapid advancement of large reasoning models, long Chain-of-Thought
(CoT) prompting has demonstrated strong performance on complex tasks. However,
this often comes with a significant increase in token usage. In this paper, we
conduct a comprehensive empirical analysis comparing long and short CoT
strategies. Our findings reveal that while long CoT can lead to performance
improvements, its benefits are often marginal relative to its significantly
higher token consumption. Specifically, long CoT tends to outperform when ample
generation budgets are available, whereas short CoT is more effective under
tighter budget constraints. These insights underscore the need for a dynamic
approach that selects the proper CoT strategy based on task context and
resource availability. To address this, we propose SwitchCoT, an automatic
framework that adaptively chooses between long and short CoT strategies to
balance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is
designed to be budget-aware, making it broadly applicable across scenarios with
varying resource constraints. Experimental results demonstrate that SwitchCoT
can reduce inference costs by up to 50% while maintaining high accuracy.
Notably, under limited token budgets, it achieves performance comparable to, or
even exceeding, that of using either long or short CoT alone.

</details>


### [109] [R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2506.04185)
*Qingfei Zhao,Ruobing Wang,Dingling Xu,Daren Zha,Limin Liu*

Main category: cs.CL

TL;DR: R-Search is a new reinforcement learning framework that helps large language models improve their reasoning abilities by integrating deep search interactions, leading to better responses in complex tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enabling large language models to effectively integrate deep search interactions for improved reasoning capabilities.

Method: Introducing R-Search, which uses multi-reward signals to guide LLMs in deciding when to retrieve or reason and integrates key evidence to enhance knowledge interaction between reasoning and search.

Result: R-Search outperformed advanced RAG baselines by up to 32.2% in-domain and 25.1% out-of-domain across seven datasets.

Conclusion: The proposed method shows significant improvement in the quality of responses generated by LLMs in complex logic- and knowledge-intensive tasks.

Abstract: Large language models (LLMs) have notably progressed in multi-step and
long-chain reasoning. However, extending their reasoning capabilities to
encompass deep interactions with search remains a non-trivial challenge, as
models often fail to identify optimal reasoning-search interaction
trajectories, resulting in suboptimal responses. We propose R-Search, a novel
reinforcement learning framework for Reasoning-Search integration, designed to
enable LLMs to autonomously execute multi-step reasoning with deep search
interaction, and learn optimal reasoning search interaction trajectories via
multi-reward signals, improving response quality in complex logic- and
knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when
to retrieve or reason, while globally integrating key evidence to enhance deep
knowledge interaction between reasoning and search. During RL training,
R-Search provides multi-stage, multi-type rewards to jointly optimize the
reasoning-search trajectory. Experiments on seven datasets show that R-Search
outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%
(out-of-domain). The code and data are available at
https://github.com/QingFei1/R-Search.

</details>


### [110] [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/abs/2506.04226)
*Akshat Gupta,Maochuan Lu,Thomas Hartvigsen,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: This paper shows that the excessive computational cost of knowledge editing methods like MEMIT, ROME, and EMMET is unnecessary. By precomputing only a small fraction of the originally stipulated hidden vectors, users can significantly reduce precomputation time.


<details>
  <summary>Details</summary>
Motivation: To address the significant one-time computational cost associated with precomputing hidden vectors for knowledge editing methods.

Method: Theoretical analysis and empirical experiments to determine the minimum number of hidden vectors needed for knowledge editing and demonstrate that it can be done with much fewer vectors.

Result: Reduction in precomputation time by more than 99.7%, allowing users to start editing new models within minutes instead of hours.

Conclusion: Knowledge editing methods can be made much more efficient by precomputing far fewer hidden vectors, making them more accessible and practical for wider use.

Abstract: Knowledge editing methods like MEMIT are able to make data and compute
efficient updates of factual knowledge by using a single sentence to update
facts and their consequences. However, what is often overlooked is a
"precomputation step", which requires a one-time but significant computational
cost. The authors of MEMIT originally precompute approximately 44 million
hidden vectors per edited layer, which requires a forward pass over 44 million
tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single
GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this
precomputation time grows with model size. In this paper, we show that this
excessive computational cost is unnecessary. Knowledge editing using MEMIT and
related methods, such as ROME and EMMET, can be performed by pre-computing a
very small portion of the 44 million hidden vectors. We first present the
theoretical minimum number of hidden vector precomputation required for
solutions of these editing methods to exist. We then empirically show that
knowledge editing using these methods can be done by pre-computing
significantly fewer hidden vectors. Specifically, we show that the
precomputation step can be done with less than 0.3% of the originally
stipulated number of hidden vectors. This saves a significant amount of
precomputation time and allows users to begin editing new models within a few
minutes.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [111] [ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking](https://arxiv.org/abs/2506.03487)
*Xianming Li,Aamir Shakir,Rui Huang,Julius Lipp,Jing Li*

Main category: cs.IR

TL;DR: 提出了一种新的两阶段训练方法ProRank，用于基于小语言模型的文档重排序，显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于大规模大语言模型（LLMs）的文档重排序方法虽然提高了重排序质量，但存在计算成本高的问题。而小语言模型（SLMs）因其高效性成为一个有希望的替代方案，但它们在不经过微调的情况下难以理解任务提示语，限制了其在文档重排序任务中的有效性。

Method: 提出了一种名为ProRank的两阶段训练方法，包括一个使用强化学习GRPO的提示预热阶段和一个无需引入额外层的细粒度评分学习阶段。

Result: 实验结果表明，提出的ProRank方法在多个基准上始终优于最先进的开源和专有重排序模型。特别是轻量级的ProRank-0.5B模型甚至在BEIR基准上超过了强大的32B参数LLM重排序模型。

Conclusion: 适当训练的小语言模型可以实现卓越的文档重排序性能，同时保持计算效率。

Abstract: Reranking is fundamental to information retrieval and retrieval-augmented
generation, with recent Large Language Models (LLMs) significantly advancing
reranking quality. While recent advances with LLMs have significantly improved
document reranking quality, current approaches primarily rely on large-scale
LLMs (>7B parameters) through zero-shot prompting, presenting high
computational costs. Small Language Models (SLMs) offer a promising alternative
because of their efficiency, but our preliminary quantitative analysis reveals
they struggle with understanding task prompts without fine-tuning. This limits
their effectiveness for document reranking tasks. To address this issue, we
introduce a novel two-stage training approach, ProRank, for SLM-based document
reranking. First, we propose a prompt warmup stage using reinforcement learning
GRPO to steer SLMs to understand task prompts and generate more accurate
coarse-grained binary relevance scores for document reranking. Then, we
continuously fine-tune the SLMs with a fine-grained score learning stage
without introducing additional layers to further improve the reranking quality.
Comprehensive experimental results demonstrate that the proposed ProRank
consistently outperforms both the most advanced open-source and proprietary
reranking models. Notably, our lightweight ProRank-0.5B model even surpasses
the powerful 32B LLM reranking model on the BEIR benchmark, establishing that
properly trained SLMs can achieve superior document reranking performance while
maintaining computational efficiency.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [112] [Preface to the Special Issue of the TAL Journal on Scholarly Document Processing](https://arxiv.org/abs/2506.03587)
*Florian Boudin,Akiko Aizawa*

Main category: cs.DL

TL;DR: This special issue of the TAL journal focuses on research that addresses the challenges of navigating and interpreting scholarly literature using large language models and other advanced methods.


<details>
  <summary>Details</summary>
Motivation: The increasing difficulty for researchers to keep up with new knowledge due to the rapid growth of scholarly literature necessitates automated tools to extract reliable insights.

Method: Using large language models (LLMs) to enable tasks such as literature reviews, writing assistance, and interactive exploration of research.

Result: Research presented in this special issue addresses challenges in natural language processing and information retrieval for scholarly and scientific documents.

Conclusion: Automated tools like LLMs provide new opportunities for navigating and interpreting scholarly literature.

Abstract: The rapid growth of scholarly literature makes it increasingly difficult for
researchers to keep up with new knowledge. Automated tools are now more
essential than ever to help navigate and interpret this vast body of
information. Scientific papers pose unique difficulties, with their complex
language, specialized terminology, and diverse formats, requiring advanced
methods to extract reliable and actionable insights. Large language models
(LLMs) offer new opportunities, enabling tasks such as literature reviews,
writing assistance, and interactive exploration of research. This special issue
of the TAL journal highlights research addressing these challenges and, more
broadly, research on natural language processing and information retrieval for
scholarly and scientific documents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [113] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu,Yueqi Wang,Shuo Xing,Chia-Ju Chen,Zhengzhong Tu*

Main category: cs.AI

TL;DR: This paper systematically analyzes the Retrieval-Augmented Generation (RAG) pipeline for large vision-language models (LVLMs), focusing on retrieval strategies, re-ranking methods to reduce biases, and integrating retrieved information into generation processes. It also proposes a self-reflective framework enhancing model performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations of LVLMs like susceptibility to hallucinations and lack of real-time evidence verification, using RAG to enhance their performance in dynamic real-world applications.

Method: Systematically dissecting the RAG pipeline including investigating modality configurations, re-ranking strategies, and integration approaches into the generation phase, plus proposing a unified agentic framework for self-reflection.

Result: Achieved an average performance boost of 5% across various tasks without additional fine-tuning.

Conclusion: The study provides valuable insights into optimizing RAG for LVLMs, enhancing their ability to ground outputs in factual and contextually relevant information.

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


### [114] [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)
*Junqi Gao,Xiang Zou,YIng Ai,Dong Li,Yichen Niu,Biqing Qi,Jianxing Liu*

Main category: cs.AI

TL;DR: Graph Counselor improves Large Language Models' factual accuracy and generation quality by using multi-agent collaboration and self-reflection.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Retrieval Augmented Generation methods have inefficient information aggregation and rigid reasoning mechanism.

Method: Graph Counselor uses Adaptive Graph Information Extraction Module with Planning, Thought, and Execution Agents for multi-level dependency modeling and dynamic reasoning depth adjustment. It also uses Self-Reflection with Multiple Perspectives module for accurate and consistent reasoning results.

Result: Graph Counselor performs better than existing methods in various graph reasoning tasks.

Conclusion: Multi-agent collaboration and self-reflection can enhance external knowledge integration capabilities in specialized domains.

Abstract: Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external
knowledge integration capabilities by explicitly modeling knowledge
relationships, thereby improving the factual accuracy and generation quality of
Large Language Models (LLMs) in specialized domains. However, existing methods
suffer from two inherent limitations: 1) Inefficient Information Aggregation:
They rely on a single agent and fixed iterative patterns, making it difficult
to adaptively capture multi-level textual, structural, and degree information
within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning
schemes, which cannot dynamically adjust reasoning depth nor achieve precise
semantic correction. To overcome these limitations, we propose Graph Counselor,
an GraphRAG method based on multi-agent collaboration. This method uses the
Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,
and Execution Agents work together to precisely model complex graph structures
and dynamically adjust information extraction strategies, addressing the
challenges of multi-level dependency modeling and adaptive reasoning depth.
Additionally, the Self-Reflection with Multiple Perspectives (SR) module
improves the accuracy and semantic consistency of reasoning results through
self-reflection and backward reasoning mechanisms. Experiments demonstrate that
Graph Counselor outperforms existing methods in multiple graph reasoning tasks,
exhibiting higher reasoning accuracy and generalization ability. Our code is
available at https://github.com/gjq100/Graph-Counselor.git.

</details>


### [115] [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
*Akshat Naik,Patrick Quinn,Guillermo Bosch,Emma Gouné,Francisco Javier Campos Zabala,Jason Ross Brown,Edward James Young*

Main category: cs.AI

TL;DR: 提出一个名为 AgentMisalignment 的新基准测试，用于评估大型语言模型代理在现实世界中的不 aligned 倾向，发现更强大的模型倾向于更多的不 aligned 行为，并且代理个性对不 aligned 倾向有重要影响。


<details>
  <summary>Details</summary>
Motivation: 研究了大型语言模型代理在现实世界中尝试不 aligned 行为的可能性（不 aligned 倾向），这是之前工作没有充分理解的。

Method: 引入了一个不 aligned 倾向基准测试 AgentMisalignment，包括一系列现实场景，使 LLM 代理有机会表现出不 aligned 行为，并组织了四个子类别的不 aligned 行为评估，包括目标守卫、抵抗关闭、沙袋行为和权力寻求。此外，通过不同的系统提示改变代理个性，研究其对不 aligned 倾向的影响。

Result: 前沿模型在基准测试中的表现显示，更强大的模型平均表现出更高的不 aligned 倾向。另外，个性特征可以极大地、不可预测地影响不 aligned 倾向，有时比模型本身的选择更重要。

Conclusion: 当前的对齐方法未能推广到 LLM 代理，强调了进一步评估倾向的重要性，随着自主系统的普及。

Abstract: As Large Language Model (LLM) agents become more widespread, associated
misalignment risks increase. Prior work has examined agents' ability to enact
misaligned behaviour (misalignment capability) and their compliance with
harmful instructions (misuse propensity). However, the likelihood of agents
attempting misaligned behaviours in real-world settings (misalignment
propensity) remains poorly understood. We introduce a misalignment propensity
benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in
which LLM agents have the opportunity to display misaligned behaviour. We
organise our evaluations into subcategories of misaligned behaviours, including
goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report
the performance of frontier models on our benchmark, observing higher
misalignment on average when evaluating more capable models. Finally, we
systematically vary agent personalities through different system prompts. We
find that persona characteristics can dramatically and unpredictably influence
misalignment tendencies -- occasionally far more than the choice of model
itself -- highlighting the importance of careful system prompt engineering for
deployed AI agents. Our work highlights the failure of current alignment
methods to generalise to LLM agents, and underscores the need for further
propensity evaluations as autonomous systems become more prevalent.

</details>


### [116] [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
*Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Yifu Lu,Mengdi Wang,Dinesh Manocha,Furong Huang,Mohammad Ghavamzadeh,Amrit Singh Bedi*

Main category: cs.AI

TL;DR: Test-time scaling for reasoning models often leads to overthinking, which initially improves performance but eventually undermines precision due to increased output variance. An alternative method, parallel thinking, achieves better accuracy by generating multiple independent reasoning paths and selecting the most consistent response.


<details>
  <summary>Details</summary>
Motivation: To investigate whether 'thinking more' at test-time genuinely enhances reasoning ability in models like OpenAI o1 and DeepSeek R1.

Method: Detailed empirical study across models and benchmarks, and proposing a probabilistic model to understand the non-monotonic trend of performance.

Result: Initial performance improvements from additional thinking followed by a decline ('overthinking'), and introduction of parallel thinking method achieving up to 20% higher accuracy than extended thinking.

Conclusion: Extended thinking is not an effective strategy for test-time scaling as it creates an illusion of improved reasoning due to model uncertainty and evaluation metric connection. Parallel thinking offers a superior alternative.

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek R1) have led to a popular belief that extending thinking traces using
prompts like "Wait" or "Let me rethink" can improve performance. This raises a
natural question: Does thinking more at test-time truly lead to better
reasoning? To answer this question, we perform a detailed empirical study
across models and benchmarks, which reveals a consistent pattern of initial
performance improvements from additional thinking followed by a decline, due to
"overthinking". To understand this non-monotonic trend, we consider a simple
probabilistic model, which reveals that additional thinking increases output
variance-creating an illusion of improved reasoning while ultimately
undermining precision. Thus, observed gains from "more thinking" are not true
indicators of improved reasoning, but artifacts stemming from the connection
between model uncertainty and evaluation metric. This suggests that test-time
scaling through extended thinking is not an effective way to utilize the
inference thinking budget. Recognizing these limitations, we introduce an
alternative test-time scaling approach, parallel thinking, inspired by
Best-of-N sampling. Our method generates multiple independent reasoning paths
within the same inference budget and selects the most consistent response via
majority vote, achieving up to 20% higher accuracy compared to extended
thinking. This provides a simple yet effective mechanism for test-time scaling
of reasoning models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [117] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2506.03197)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Yanjie Liang,Zuming Huang,Haozhe Wang,Jun Huang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CV

TL;DR: Introducing layoutRL, a novel end-to-end reinforcement learning framework for parsing scanned documents, which achieves state-of-the-art performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-stage pipelines for Document AI suffer from error propagation and limited adaptability to diverse layouts.

Method: LayoutRL optimizes a composite reward of normalized edit distance, paragraph count accuracy, and reading order preservation.

Result: Infinity-Parser, based on layoutRL, achieves new state-of-the-art performance in OCR, table and formula extraction, and reading order detection on English and Chinese benchmarks.

Conclusion: The proposed layoutRL framework and the released dataset and code can significantly advance robust document understanding.

Abstract: Automated parsing of scanned documents into richly structured,
machine-readable formats remains a critical bottleneck in Document AI, as
traditional multi-stage pipelines suffer from error propagation and limited
adaptability to diverse layouts. We introduce layoutRL, an end-to-end
reinforcement learning framework that trains models to be explicitly
layout-aware by optimizing a composite reward of normalized edit distance,
paragraph count accuracy, and reading order preservation. Leveraging our newly
released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic
scanned document parsing data with expert-filtered real-world documents, we
instantiate layoutRL in a vision-language-model-based parser called
Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and
formula extraction, and reading order detection, Infinity-Parser achieves new
state-of-the-art performance in both accuracy and structural fidelity,
outpacing specialist pipelines and general-purpose vision-language models. We
will publicly release our code and dataset to accelerate progress in robust
document understanding.

</details>


### [118] [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525)
*Daeun Lee,Jaehong Yoon,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: Propose Video-SKoT, a framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing methods often struggle to adapt to domain-specific skills over various video content.

Method: Propose Video-Skill-CoT (Video-SKoT), which automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. Construct skill-based CoT annotations and introduce a skill-specific expert learning framework.

Result: Demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where Video-SKoT consistently outperforms strong baselines.

Conclusion: Video-SKoT provides a way to improve complex video understanding by constructing skill-aware CoT supervisions.

Abstract: Recent advances in Chain-of-Thought (CoT) reasoning have improved complex
video understanding, but existing methods often struggle to adapt to
domain-specific skills (e.g., event detection, spatial relation understanding,
emotion understanding) over various video content. To address this, we propose
Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs
and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
First, we construct skill-based CoT annotations: we extract domain-relevant
reasoning skills from training questions, cluster them into a shared skill
taxonomy, and create detailed multi-step CoT rationale tailored to each
video-question pair for training. Second, we introduce a skill-specific expert
learning framework. Each expert module specializes in a subset of reasoning
skills and is trained with lightweight adapters using the collected CoT
supervision. We demonstrate the effectiveness of the proposed approach on three
video understanding benchmarks, where Video-SKoT consistently outperforms
strong baselines. We also provide in-depth analyses on comparing different CoT
annotation pipelines and learned skills over multiple video domains.

</details>


### [119] [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/abs/2506.03589)
*Huy Le,Nhat Chung,Tung Kieu,Anh Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: This paper proposes BiMa, a new framework for text-video retrieval (TVR) that reduces visual-linguistic biases in datasets by generating scene elements for videos and separating text features into content and bias components.


<details>
  <summary>Details</summary>
Motivation: Pre-trained vision-language models in TVR systems often overlook key details due to biases present in datasets.

Method: BiMa generates scene elements for videos, integrates these into video embeddings for visual debiasing, and introduces a mechanism to separate text features into content and bias components for textual debiasing.

Result: Extensive experiments across five major TVR benchmarks show that BiMa performs competitively and maintains strong results on out-of-distribution retrieval tasks.

Conclusion: BiMa effectively mitigates biases in both visual and textual representations, leading to improved performance in TVR.

Abstract: Text-video retrieval (TVR) systems often suffer from visual-linguistic biases
present in datasets, which cause pre-trained vision-language models to overlook
key details. To address this, we propose BiMa, a novel framework designed to
mitigate biases in both visual and textual representations. Our approach begins
by generating scene elements that characterize each video by identifying
relevant entities/objects and activities. For visual debiasing, we integrate
these scene elements into the video embeddings, enhancing them to emphasize
fine-grained and salient details. For textual debiasing, we introduce a
mechanism to disentangle text features into content and bias components,
enabling the model to focus on meaningful content while separately handling
biased information. Extensive experiments and ablation studies across five
major TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)
demonstrate the competitive performance of BiMa. Additionally, the model's bias
mitigation capability is consistently validated by its strong results on
out-of-distribution retrieval tasks.

</details>


### [120] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
*Zhanhui Zhou,Lingjie Chen,Chao Yang,Chaochao Lu*

Main category: cs.CV

TL;DR: This paper investigates the 'visual stitching' capability in vision-language models (VLMs), which allows harmful content to bypass moderation when split into small, harmless-looking patches. The authors demonstrate this ability in popular open-source VLMs and show how harmful content can evade detection and be reconstructed during inference, posing significant safety risks.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the risks of vision-language models being manipulated through 'visual stitching', where harmful content can be hidden in small, seemingly benign patches.

Method: The authors fine-tune VLMs using image patches labeled with unique synthetic IDs and observe whether the model can correctly verbalize these IDs from full images or text references. They also simulate an adversarial data poisoning scenario with harmful patches labeled with misleading text descriptions.

Result: The results confirm that VLMs possess 'visual stitching' capabilities, allowing them to reconstruct IDs from full images or text references. This demonstrates how harmful content can evade moderation and be reproduced during inference.

Conclusion: The study highlights the risks associated with 'visual stitching' in VLMs and emphasizes the need for more robust data moderation techniques to prevent potential misuse.

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>


### [121] [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)
*Jiulong Wu,Zhengliang Shi,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Lingyong Yan,Min Cao,Min Zhang*

Main category: cs.CV

TL;DR: This paper introduces EMPO, a method that improves modality alignment in large visual language models to reduce hallucinations, using open-source instruction datasets to create high-quality preference data.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations in large visual language models due to modality misalignment and the limitations of their underlying large language model backbones.

Method: Entity-centric Multimodal Preference Optimization (EMPO), which enhances modality alignment and uses open-source instruction datasets for constructing high-quality preference data.

Result: Experiments show that EMPO reduces hallucination rates by 85.9% on Object-HalBench and 49.8% on MM-HalBench.

Conclusion: EMPO effectively improves modality alignment in large visual language models, leading to reduced hallucination rates.

Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive
capabilities across multiple tasks. However, their trustworthiness is often
challenged by hallucinations, which can be attributed to the modality
misalignment and the inherent hallucinations of their underlying Large Language
Models (LLMs) backbone. Existing preference alignment methods focus on aligning
model responses with human preferences while neglecting image-text modality
alignment, resulting in over-reliance on LLMs and hallucinations. In this
paper, we propose Entity-centric Multimodal Preference Optimization (EMPO),
which achieves enhanced modality alignment than existing human preference
alignment methods. Besides, to overcome the scarcity of high-quality multimodal
preference data, we utilize open-source instruction datasets to automatically
construct high-quality preference data across three aspects: image,
instruction, and response. Experiments on two human preference datasets and
five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,
e.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on
MM-HalBench.

</details>


### [122] [MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos](https://arxiv.org/abs/2506.04141)
*Kejian Zhu,Zhuoran Jin,Hongbang Yuan,Jiachun Li,Shangqing Tu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CV

TL;DR: This paper introduces MMR-V, a new benchmark for evaluating multimodal deep reasoning in videos. It emphasizes long-range, multi-frame reasoning, beyond perception, reliability, and confusability. Current models perform poorly on this benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing video benchmarks mainly focus on understanding tasks, neglecting multimodal deep reasoning. This paper aims to fill the gap by proposing a new benchmark.

Method: MMR-V, a benchmark for multimodal deep reasoning in videos with four key features: long-range, multi-frame reasoning, beyond perception, reliability, and confusability.

Result: Current models struggle with multimodal reasoning on MMR-V, with the best-performing model achieving only 52.5% accuracy. Existing reasoning enhancement strategies bring limited gains.

Conclusion: MMR-V can inspire further research into enhancing multimodal reasoning capabilities.

Abstract: The sequential structure of videos poses a challenge to the ability of
multimodal large language models (MLLMs) to locate multi-frame evidence and
conduct multimodal reasoning. However, existing video benchmarks mainly focus
on understanding tasks, which only require models to match frames mentioned in
the question (hereafter referred to as "question frame") and perceive a few
adjacent frames. To address this gap, we propose MMR-V: A Benchmark for
Multimodal Deep Reasoning in Videos. The benchmark is characterized by the
following features. (1) Long-range, multi-frame reasoning: Models are required
to infer and analyze evidence frames that may be far from the question frame.
(2) Beyond perception: Questions cannot be answered through direct perception
alone but require reasoning over hidden information. (3) Reliability: All tasks
are manually annotated, referencing extensive real-world user understanding to
align with common perceptions. (4) Confusability: Carefully designed distractor
annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos
and 1,257 tasks. Our experiments reveal that current models still struggle with
multi-modal reasoning; even the best-performing model, o4-mini, achieves only
52.5% accuracy. Additionally, current reasoning enhancement strategies
(Chain-of-Thought and scaling test-time compute) bring limited gains. Further
analysis indicates that the CoT demanded for multi-modal reasoning differs from
it in textual reasoning, which partly explains the limited performance gains.
We hope that MMR-V can inspire further research into enhancing multi-modal
reasoning capabilities.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [123] [PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing](https://arxiv.org/abs/2506.03741)
*Rifat Mehreen Amin,Oliver Hans Kühle,Daniel Buschek,Andreas Butz*

Main category: cs.HC

TL;DR: PromptCanvas is a new way to interact with AI by using a visual canvas with interactive widgets, which was shown to improve creativity and reduce cognitive load compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To provide users with more control over AI-generated content and enhance their creative process.

Method: Developing a concept called PromptCanvas that uses a visual canvas with interactive widgets for text generation and customization, allowing users to create widgets through system suggestions, user prompts, or manual input.

Result: In a lab study, PromptCanvas outperformed a traditional conversational UI on the Creativity Support Index, reducing cognitive load and encouraging new perspectives and ideas. A follow-up field study confirmed these results.

Conclusion: PromptCanvas demonstrates the potential of dynamic, customizable interfaces in improving collaborative writing with AI.

Abstract: We introduce PromptCanvas, a concept that transforms prompting into a
composable, widget-based experience on an infinite canvas. Users can generate,
customize, and arrange interactive widgets representing various facets of their
text, offering greater control over AI-generated content. PromptCanvas allows
widget creation through system suggestions, user prompts, or manual input,
providing a flexible environment tailored to individual needs. This enables
deeper engagement with the creative process. In a lab study with 18
participants, PromptCanvas outperformed a traditional conversational UI on the
Creativity Support Index. Participants found that it reduced cognitive load,
with lower mental demand and frustration. Qualitative feedback revealed that
the visual organization of thoughts and easy iteration encouraged new
perspectives and ideas. A follow-up field study (N=10) confirmed these results,
showcasing the potential of dynamic, customizable interfaces in improving
collaborative writing with AI.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [124] [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)
*Yuansheng Ni,Ping Nie,Kai Zou,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: A large-scale dataset VisCode-200K is presented to improve the performance of LLMs in visualization tasks.


<details>
  <summary>Details</summary>
Motivation: Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation.

Method: Fine-tuning Qwen2.5-Coder-Instruct on VisCode-200K

Result: VisCoder demonstrates the benefits of feedback-driven learning for executable, visually accurate code generation.

Conclusion: VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini.

Abstract: Large language models (LLMs) often struggle with visualization tasks like
plotting diagrams, charts, where success depends on both code correctness and
visual semantics. Existing instruction-tuning datasets lack execution-grounded
supervision and offer limited support for iterative code correction, resulting
in fragile and unreliable plot generation. We present VisCode-200K, a
large-scale instruction tuning dataset for Python-based visualization and
self-correction. It contains over 200K examples from two sources: (1) validated
plotting code from open-source repositories, paired with natural language
instructions and rendered plots; and (2) 45K multi-turn correction dialogues
from Code-Feedback, enabling models to revise faulty code using runtime
feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create
VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly
outperforms strong open-source baselines and approaches the performance of
proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation
protocol to assess iterative repair, demonstrating the benefits of
feedback-driven learning for executable, visually accurate code generation.

</details>


### [125] [CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking](https://arxiv.org/abs/2506.04019)
*Neeva Oza,Ishaan Govil,Parul Gupta,Dinesh Khandelwal,Dinesh Garg,Parag Singla*

Main category: cs.SE

TL;DR: This paper introduces CETBench, a new benchmark for code equivalence checking using code transformation. It shows that current state-of-the-art large language models (LLMs) perform poorly on this task even with simple transformations. The authors propose a fine-tuning approach to improve LLM performance on transformed code pairs.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capabilities of LLMs for code rewriting and translation tasks by benchmarking their ability to check code equivalence.

Method: Creating CETBench by applying random code transformations to pairs of programs in a repository and testing SOTA LLMs on these transformed pairs.

Result: SOTA LLMs show poor performance on the code equivalence checking task even with simple transformations. Fine-tuning improves performance.

Conclusion: LLMs may lack true semantic understanding of code, as evidenced by their poor performance on transformed code equivalence tasks.

Abstract: LLMs have been extensively used for the task of automated code generation. In
this work, we examine the applicability of LLMs for the related but relatively
unexplored task of code-equivalence checking, i.e., given two programs, whether
they are functionally equivalent or not. This is an important problem since
benchmarking code equivalence can play a critical role in evaluating LLM
capabilities for tasks such as code re-writing and code translation. Towards
this end, we present CETBench - Code Equivalence with Transformations
Benchmark, constructed via a repository of programs, where two programs in the
repository may be solving the same or different tasks. Each instance in our
dataset is obtained by taking a pair of programs in the repository and applying
a random series of pre-defined code transformations, resulting in
(non-)equivalent pairs. Our analysis on this dataset reveals a surprising
finding that very simple code transformations in the underlying pair of
programs can result in a significant drop in performance of SOTA LLMs for the
task of code-equivalence checking. To remedy this, we present a simple
fine-tuning-based approach to boost LLM performance on the transformed pairs of
programs. Our approach for dataset generation is generic, and can be used with
repositories with varying program difficulty levels and allows for applying
varying numbers as well as kinds of transformations. In our experiments, we
perform ablations over the difficulty level of original programs, as well as
the kind of transformations used in generating pairs for equivalence checking.
Our analysis presents deep insights into the working of LLMs for the task of
code-equivalence, and points to the fact that they may still be far from what
could be termed as a semantic understanding of the underlying code.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [126] [Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models](https://arxiv.org/abs/2506.03606)
*Parismita Gogoi,Sishir Kalita,Wendy Lalhminghlui,Viyazonuo Terhiija,Moakala Tzudir,Priyankoo Sarmah,S. R. M. Prasanna*

Main category: eess.AS

TL;DR: This study investigates the effectiveness of self-supervised learning models for tone recognition in three low-resource North Eastern Indian languages: Angami, Ao, and Mizo. It evaluates four Wav2vec2.0 base models and finds that tone recognition performs best for Mizo and worst for Angami. The research highlights that the middle layers of the SSL models are crucial for tone recognition regardless of the pre-training language type. Tone inventory, tone types, and dialectal variations impact tone recognition. This study offers valuable insights into SSL-based embeddings for tonal languages and suggests ways to enhance tone recognition in low-resource scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to explore the potential of self-supervised learning models for tone recognition in low-resource languages from North Eastern India, specifically Angami, Ao, and Mizo.

Method: Four Wav2vec2.0 base models pre-trained on both tonal and non-tonal languages are evaluated for their effectiveness in tone recognition across the three languages.

Result: Tone recognition works best for Mizo and worst for Angami. The middle layers of the SSL models are the most important for tone recognition, irrespective of the pre-training language type. Tone inventory, tone types, and dialectal variations influence tone recognition.

Conclusion: This study provides useful insights into the strengths and weaknesses of SSL-based embeddings for tonal languages and suggests possibilities for enhancing tone recognition in low-resource settings.

Abstract: This study explores the use of self-supervised learning (SSL) models for tone
recognition in three low-resource languages from North Eastern India: Angami,
Ao, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on
both tonal and non-tonal languages. We analyze tone-wise performance across the
layers for all three languages and compare the different models. Our results
show that tone recognition works best for Mizo and worst for Angami. The middle
layers of the SSL models are the most important for tone recognition,
regardless of the pre-training language, i.e. tonal or non-tonal. We have also
found that the tone inventory, tone types, and dialectal variations affect tone
recognition. These findings provide useful insights into the strengths and
weaknesses of SSL-based embeddings for tonal languages and highlight the
potential for improving tone recognition in low-resource settings. The source
code is available at GitHub 1 .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [127] [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)
*Selcuk Gurses,Aozhong Zhang,Yanxia Deng,Xun Dong,Xin Li,Naigang Wang,Penghang Yin,Zi Yang*

Main category: cs.LG

TL;DR: DiaBlo是一种新的参数高效微调方法，通过仅更新选定模型权重矩阵的对角块来适应大型语言模型到特定领域的下游任务。它在性能、内存效率和训练速度上与全模型微调相当，并且在各种任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 减少大型语言模型在适应下游任务时的计算和内存成本，同时保持高性能。

Method: DiaBlo只更新选定模型权重矩阵的对角块，无需低秩矩阵乘积，避免了对辅助初始化方案或定制优化策略的依赖。

Result: DiaBlo在多个任务上表现出色，包括常识推理、算术推理、代码生成和安全对齐，同时保持高内存效率和快速微调速度。

Conclusion: DiaBlo提供了一种简单有效的参数高效微调方法，适用于多种任务，具有广泛的适用性。

Abstract: Finetuning is a critical step for adapting large language models (LLMs) to
domain-specific downstream tasks. To mitigate the substantial computational and
memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)
methods have been proposed to update only a small subset of model parameters.
However, performance gaps between PEFT approaches and full-model fine-tuning
still exist. In this work, we present DiaBlo, a simple yet effective PEFT
approach that updates only the diagonal blocks of selected model weight
matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates
the need for low rank matrix products, thereby avoiding the reliance on
auxiliary initialization schemes or customized optimization strategies to
improve convergence. This design leads to stable and robust convergence while
maintaining comparable memory efficiency and training speed to LoRA. We conduct
extensive experiments across a range of tasks, including commonsense reasoning,
arithmetic reasoning, code generation, and safety alignment, to evaluate the
effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo
demonstrates strong and consistent performance while maintaining high memory
efficiency and fast finetuning speed. Codes are available at
https://github.com/ziyangjoy/DiaBlo.

</details>


### [128] [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/abs/2506.03370)
*Leonid Ryvkin*

Main category: cs.LG

TL;DR: Survey of UHATs' capabilities in recognizing formal languages with different types of attention.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of unique hard attention transformers encoders (UHATs) in recognizing formal languages.

Method: Distinguishing between masked vs. non-masked, finite vs. infinite image and general vs. bilinear attention score functions.

Result: Recalled some relations between these models, as well as a lower bound in terms of first-order logic and an upper bound in terms of circuit complexity.

Conclusion: This note serves as a survey of various results on the capabilities of unique hard attention transformers encoders (UHATs) to recognize formal languages.

Abstract: This note is a survey of various results on the capabilities of unique hard
attention transformers encoders (UHATs) to recognize formal languages. We
distinguish between masked vs. non-masked, finite vs. infinite image and
general vs. bilinear attention score functions. We recall some relations
between these models, as well as a lower bound in terms of first-order logic
and an upper bound in terms of circuit complexity.

</details>


### [129] [Adaptive Task Vectors for Large Language Models](https://arxiv.org/abs/2506.03426)
*Joonseong Kang,Soojeong Lee,Subeen Park,Sumin Park,Taero Kim,Jihee Kim,Ryunyi Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: Adaptive Task Vectors (ATV) is proposed to improve in-context learning in large language models by dynamically generating task vectors based on each input query.


<details>
  <summary>Details</summary>
Motivation: To address limitations of current in-context learning methods such as sensitivity to demonstration order and poor generalization on unseen tasks.

Method: Developing a framework called Adaptive Task Vectors (ATV) that uses a small language model to generate task vectors conditioned on input queries.

Result: ATV shows strong performance and generalization abilities, especially on unseen tasks, and is theoretically proven to be more expressive than some existing methods.

Conclusion: The proposed method ATV provides a way to enhance the adaptability and generalization capability of in-context learning in large language models.

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks without parameter updates by conditioning on a few demonstrations
provided in the prompt. Despite its success, ICL suffers from several
limitations, including sensitivity to demonstration order, context length
constraints, and computational inefficiency. To address these challenges, task
vector-based approaches compress task information into a single vector.
However, these methods typically construct task vectors from fixed sets of
demonstrations and reuse them across input queries, without conditioning on the
specific input. This limitation can lead models to struggle with effective
adaptation when the input query is not well aligned with the underlying
demonstrations, consequently degrading their generalization performance on
unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors
(ATV), a simple and effective framework that dynamically generates task vectors
conditioned on each input query. ATV employs a small language model to generate
task vectors, which are then transformed to match the target LLM's architecture
and applied to guide its output generation. In contrast to ICL and previous
vector-based approaches, which rely on fixed demonstration sets and their
corresponding vectors, ATV dynamically generates task vectors tailored to each
specific input query and task. Consequently, ATV demonstrates strong
performance and generalization capabilities, even for unseen tasks.
Furthermore, we provide a theoretical analysis indicating that ATV is
expressively equivalent to LoRA under equal rank budgets and more expressive
than Prefix-Tuning, thereby offering formal support for its representational
advantage.

</details>


### [130] [Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior](https://arxiv.org/abs/2506.03444)
*Yue Gong,Raul Castro Fernandez*

Main category: cs.LG

TL;DR: This paper studies the problem of automatically assessing statistical relationships (correlations) to determine their novelty and worthiness of further exploration.


<details>
  <summary>Details</summary>
Motivation: With the increasing automation of hypothesis generation, there is a need for automatic hypothesis assessment to identify which statistical relationships are novel and non-trivial.

Method: The authors propose using the knowledge encoded in large language models (LLMs) to derive a prior distribution over correlation values. They introduce the Logit-based Calibrated Prior, which transforms the model's raw output logits into a calibrated predictive distribution over correlation values.

Result: The proposed prior was evaluated on a benchmark of 2,096 real-world variable pairs, achieving high accuracy in predicting Pearson correlation coefficients, outperforming a fine-tuned RoBERTa classifier in binary correlation prediction, and achieving higher precision@K in hypothesis ranking. The prior also generalized to unseen correlations, indicating context-sensitive reasoning rather than memorization.

Conclusion: This work demonstrates the potential of using LLMs for automatic hypothesis assessment, providing a valuable tool for identifying novel and non-trivial statistical relationships.

Abstract: As hypothesis generation becomes increasingly automated, a new bottleneck has
emerged: hypothesis assessment. Modern systems can surface thousands of
statistical relationships-correlations, trends, causal links-but offer little
guidance on which ones are novel, non-trivial, or worthy of expert attention.
In this work, we study the complementary problem to hypothesis generation:
automatic hypothesis assessment. Specifically, we ask: given a large set of
statistical relationships, can we automatically assess which ones are novel and
worth further exploration? We focus on correlations as they are a common entry
point in exploratory data analysis that often serve as the basis for forming
deeper scientific or causal hypotheses.
  To support automatic assessment, we propose to leverage the vast knowledge
encoded in LLMs' weights to derive a prior distribution over the correlation
value of a variable pair. If an LLM's prior expects the correlation value
observed, then such correlation is not surprising, and vice versa. We propose
the Logit-based Calibrated Prior, an LLM-elicited correlation prior that
transforms the model's raw output logits into a calibrated, continuous
predictive distribution over correlation values. We evaluate the prior on a
benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of
78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of
89.2% in predicting Pearson correlation coefficient. It also outperforms a
fine-tuned RoBERTa classifier in binary correlation prediction and achieves
higher precision@K in hypothesis ranking. We further show that the prior
generalizes to correlations not seen during LLM pretraining, reflecting
context-sensitive reasoning rather than memorization.

</details>


### [131] [Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation](https://arxiv.org/abs/2506.03857)
*Mingxuan Xia,Haobo Wang,Yixuan Li,Zewei Yu,Jindong Wang,Junbo Zhao,Runze Wu*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) have been used for data annotation, but they often produce wrong labels due to uncertainty. A new method called CanDist has been proposed which encourages LLMs to output multiple possible labels and then uses a Small Language Model (SLM) to distill these into unique labels for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for using LLMs for data annotation use an aggressive strategy that prompts LLMs to output a single gold label per unlabeled sample, which can lead to incorrect labels for difficult samples. This paper is motivated by ambiguity aversion in human behavior and proposes a novel candidate annotation paradigm.

Method: The paper proposes a teacher-student framework called CanDist that distills candidate annotations with a Small Language Model (SLM). It also provides a theoretical justification showing that distilling candidate annotations from the teacher LLM is better than using single annotations.

Result: Experiments on six text classification tasks showed the effectiveness of the proposed method.

Conclusion: This paper introduces a novel approach to data annotation using Large Language Models, which improves the quality of labels and performance of downstream tasks.

Abstract: Recently, Large Language Models (LLMs) have demonstrated significant
potential for data annotation, markedly reducing the labor costs associated
with downstream applications. However, existing methods mostly adopt an
aggressive strategy by prompting LLM to determine a single gold label for each
unlabeled sample. Due to the inherent uncertainty within LLMs, they often
produce incorrect labels for difficult samples, severely compromising the data
quality for downstream applications. Motivated by ambiguity aversion in human
behaviors, we propose a novel candidate annotation paradigm wherein large
language models are encouraged to output all possible labels when incurring
uncertainty. To ensure unique labels are provided for downstream tasks, we
develop a teacher-student framework CanDist that distills candidate annotations
with a Small Language Model (SLM). We further provide a rigorous justification
demonstrating that distilling candidate annotations from the teacher LLM offers
superior theoretical guarantees compared to directly using single annotations.
Extensive experiments across six text classification tasks validate the
effectiveness of our proposed method. The source code is available at
https://github.com/MingxuanXia/CanDist.

</details>


### [132] [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)
*Jun-Peng Jiang,Yu Xia,Hai-Long Sun,Shiyin Lu,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.LG

TL;DR: This paper introduces a new framework called Turbo to perform tabular reasoning from table images, achieving state-of-the-art performance with limited data.


<details>
  <summary>Details</summary>
Motivation: To tackle the task of tabular reasoning from table images where high-quality textual representations are often unavailable.

Method: Introduce a new framework called Turbo which benefits from a structure-aware reasoning trace generator and repeatedly generates and selects advantageous reasoning paths.

Result: Achieves state-of-the-art performance across multiple datasets with limited data.

Conclusion: Turbo demonstrates the effectiveness of leveraging privileged structured information to enhance multimodal large language models for tabular reasoning.

Abstract: Tabular reasoning involves multi-step information extraction and logical
inference over tabular data. While recent advances have leveraged large
language models (LLMs) for reasoning over structured tables, such high-quality
textual representations are often unavailable in real-world settings, where
tables typically appear as images. In this paper, we tackle the task of tabular
reasoning from table images, leveraging privileged structured information
available during training to enhance multimodal large language models (MLLMs).
The key challenges lie in the complexity of accurately aligning structured
information with visual representations, and in effectively transferring
structured reasoning skills to MLLMs despite the input modality gap. To address
these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a
new framework for multimodal tabular reasoning with privileged structured
tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator
based on DeepSeek-R1, contributing to high-quality modality-bridged data. On
this basis, {\sc Turbo} repeatedly generates and selects the advantageous
reasoning paths, further enhancing the model's tabular reasoning ability.
Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo}
achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across
multiple datasets.

</details>


### [133] [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/abs/2506.04089)
*Anastasiia Ivanova,Eva Bakaeva,Zoya Volovikova,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 提出AmbiK数据集，用于统一比较任务歧义检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有的任务歧义检测方法难以比较，因为它们是在不同的数据集上测试的，没有通用基准。

Method: 在厨房环境中收集了1000对模糊任务及其明确对应任务的数据集，包含环境描述、澄清问题和答案、用户意图和任务计划等信息。

Result: AmbiK数据集包含了2000个任务，分为三类歧义类型（人类偏好、常识知识、安全），并验证了数据集的人类有效性。

Conclusion: 提出了一种新的数据集AmbiK，用于统一比较任务歧义检测方法。

Abstract: As a part of an embodied agent, Large Language Models (LLMs) are typically
used for behavior planning given natural language instructions from the user.
However, dealing with ambiguous instructions in real-world environments remains
a challenge for LLMs. Various methods for task ambiguity detection have been
proposed. However, it is difficult to compare them because they are tested on
different datasets and there is no universal benchmark. For this reason, we
propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual
dataset of ambiguous instructions addressed to a robot in a kitchen
environment. AmbiK was collected with the assistance of LLMs and is
human-validated. It comprises 1000 pairs of ambiguous tasks and their
unambiguous counterparts, categorized by ambiguity type (Human Preferences,
Common Sense Knowledge, Safety), with environment descriptions, clarifying
questions and answers, user intents, and task plans, for a total of 2000 tasks.
We hope that AmbiK will enable researchers to perform a unified comparison of
ambiguity detection methods. AmbiK is available at
https://github.com/cog-model/AmbiK-dataset.

</details>


### [134] [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)
*Shuang Chen,Yue Guo,Zhaochen Su,Yafu Li,Yulun Wu,Jiacheng Chen,Jiayu Chen,Weijie Wang,Xiaoye Qu,Yu Cheng*

Main category: cs.LG

TL;DR: This paper introduces ReVisual-R1, a novel 7B-parameter MLLM, which improves multimodal reasoning capabilities via a staged training approach.


<details>
  <summary>Details</summary>
Motivation: Enhancing multimodal large language model's (MLLM) reasoning capabilities by improving multimodal reinforcement learning (RL) through analyzing current training pipelines.

Method: Staged training approach combining effective cold start initialization, addressing gradient stagnation in multimodal RL, and subsequent text-only RL training.

Result: Achieved superior performance on benchmarks like MathVerse, MathVision, WeMath, LogicVista, DynaMath, and AIME2024, AIME2025 compared to many recent multimodal reasoning models.

Conclusion: ReVisual-R1 sets a new state-of-the-art performance among open-source 7B MLLMs on various challenging benchmarks.

Abstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex
textual tasks, many works attempt to incentivize similar capabilities in
Multimodal Large Language Models (MLLMs) by directly applying reinforcement
learning (RL). However, they still struggle to activate complex reasoning. In
this paper, rather than examining multimodal RL in isolation, we delve into
current training pipelines and identify three crucial phenomena: 1) Effective
cold start initialization is critical for enhancing MLLM reasoning.
Intriguingly, we find that initializing with carefully selected text data alone
can lead to performance surpassing many recent multimodal reasoning models,
even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers
from gradient stagnation, which degrades training stability and performance. 3)
Subsequent text-only RL training, following the multimodal RL phase, further
enhances multimodal reasoning. This staged training approach effectively
balances perceptual grounding and cognitive reasoning development. By
incorporating the above insights and addressing multimodal RL issues, we
introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B
MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,
LogicVista, DynaMath, and challenging AIME2024 and AIME2025.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 27]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings](https://arxiv.org/abs/2506.14900)
*Imane Guellil,Salomé Andres,Atul Anand,Bruce Guthrie,Huayu Zhang,Abul Hasan,Honghan Wu,Beatrice Alex*

Main category: cs.CL

TL;DR: This study presents a new corpus for adverse event extraction from elderly patient discharge summaries, showing mixed success with different NLP models.


<details>
  <summary>Details</summary>
Motivation: To present a manually annotated corpus for AE extraction from discharge summaries of elderly patients, a population often underrepresented in clinical NLP resources.

Method: Multiple models were evaluated using FlairNLP across three annotation granularities: fine-grained, coarse-grained, and coarse-grained with negation.

Result: Transformer-based models achieved strong performance on document-level coarse-grained extraction but performance dropped notably for fine-grained entity-level tasks, especially for rare events and complex attributes.

Conclusion: Despite high-level scores, significant challenges remain in detecting underrepresented Adverse Events (AEs) and capturing nuanced clinical language.

Abstract: In this work, we present a manually annotated corpus for Adverse Event (AE)
extraction from discharge summaries of elderly patients, a population often
underrepresented in clinical NLP resources. The dataset includes 14 clinically
significant AEs-such as falls, delirium, and intracranial haemorrhage, along
with contextual attributes like negation, diagnosis type, and in-hospital
occurrence. Uniquely, the annotation schema supports both discontinuous and
overlapping entities, addressing challenges rarely tackled in prior work. We
evaluate multiple models using FlairNLP across three annotation granularities:
fine-grained, coarse-grained, and coarse-grained with negation. While
transformer-based models (e.g., BERT-cased) achieve strong performance on
document-level coarse-grained extraction (F1 = 0.943), performance drops
notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly
for rare events and complex attributes. These results demonstrate that despite
high-level scores, significant challenges remain in detecting underrepresented
AEs and capturing nuanced clinical language. Developed within a Trusted
Research Environment (TRE), the dataset is available upon request via DataLoch
and serves as a robust benchmark for evaluating AE extraction methods and
supporting future cross-dataset generalisation.

</details>


### [2] [Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction](https://arxiv.org/abs/2506.14901)
*Marija Šakota,Robert West*

Main category: cs.CL

TL;DR: Recent methods for structured NLP tasks use language models to convert input text to structured output, relying on constrained decoding. These methods face issues with low-quality output during testing due to constraints ignored during training. A new approach called Boosted Constrained Decoding (BoostCD) improves performance by combining constrained and unconstrained decoding predictions.


<details>
  <summary>Details</summary>
Motivation: The issue of low-quality output during constrained decoding at test time despite advantages during training.

Method: Introduces Boosted Constrained Decoding (BoostCD) that combines constrained and unconstrained decoding in two phases.

Result: BoostCD improves performance and results in a model called BoostIE that outperforms previous approaches.

Conclusion: BoostCD addresses several common errors found in previous approaches and enhances performance in structured NLP tasks.

Abstract: Many recent approaches to structured NLP tasks use an autoregressive language
model $M$ to map unstructured input text $x$ to output text $y$ representing
structured objects (such as tuples, lists, trees, code, etc.), where the
desired output structure is enforced via constrained decoding. During training,
these approaches do not require the model to be aware of the constraints, which
are merely implicit in the training outputs $y$. This is advantageous as it
allows for dynamic constraints without requiring retraining, but can lead to
low-quality output during constrained decoding at test time. We overcome this
problem with Boosted Constrained Decoding (BoostCD), which combines constrained
and unconstrained decoding in two phases: Phase 1 decodes from the base model
$M$ twice, in constrained and unconstrained mode, obtaining two weak
predictions. In phase 2, a learned autoregressive boosted model combines the
two weak predictions into one final prediction. The mistakes made by the base
model with vs. without constraints tend to be complementary, which the boosted
model learns to exploit for improved performance. We demonstrate the power of
BoostCD by applying it to closed information extraction. Our model, BoostIE,
outperforms prior approaches both in and out of distribution, addressing
several common errors identified in those approaches.

</details>


### [3] [CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision](https://arxiv.org/abs/2506.14912)
*Dyah Adila,Shuai Zhang,Boran Han,Bonan Min,Yuyang Wang*

Main category: cs.CL

TL;DR: This paper introduces CrEst, a weakly supervised framework for evaluating the credibility of context documents during LLM inference without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods often neglect the varying credibility of context documents, which may lead to the spread of unreliable information.

Method: CrEst uses inter-document agreement to estimate credibility and offers both black-box and white-box integration strategies for LLMs.

Result: CrEst surpasses strong baselines across three model architectures and five datasets, improving accuracy by up to 26.86% and increasing F1 score by 3.49%. It also demonstrates robustness under high-noise conditions.

Conclusion: CrEst effectively enhances the credibility assessment of context documents during LLM inference without relying on manual annotations.

Abstract: The integration of contextual information has significantly enhanced the
performance of large language models (LLMs) on knowledge-intensive tasks.
However, existing methods often overlook a critical challenge: the credibility
of context documents can vary widely, potentially leading to the propagation of
unreliable information. In this paper, we introduce CrEst, a novel weakly
supervised framework for assessing the credibility of context documents during
LLM inference--without requiring manual annotations. Our approach is grounded
in the insight that credible documents tend to exhibit higher semantic
coherence with other credible documents, enabling automated credibility
estimation through inter-document agreement. To incorporate credibility into
LLM inference, we propose two integration strategies: a black-box approach for
models without access to internal weights or activations, and a white-box
method that directly modifies attention mechanisms. Extensive experiments
across three model architectures and five datasets demonstrate that CrEst
consistently outperforms strong baselines, achieving up to a 26.86% improvement
in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst
maintains robust performance even under high-noise conditions.

</details>


### [4] [MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance](https://arxiv.org/abs/2506.14927)
*Joseph J. Peper,Wenzhao Qiu,Ali Payani,Lu Wang*

Main category: cs.CL

TL;DR: This paper introduces MDBench, a new dataset for evaluating LLMs on multi-document reasoning tasks, which is generated synthetically through a knowledge-guided approach.


<details>
  <summary>Details</summary>
Motivation: There is a lack of rigorous benchmarks for evaluating LLMs' multi-document reasoning capabilities, especially given the difficulty and expense of creating such benchmarks.

Method: MDBench is created using a novel synthetic generation process based on condensed structured seed knowledge, modified via LLM-assisted edits, and converted into natural text surface form to generate document sets and corresponding QA examples.

Result: MDBench presents significant challenges for popular LLMs and prompting techniques, even with short document sets. The knowledge-guided generation technique allows for targeted analysis of MD-specific reasoning capabilities and can adapt to future challenges and modeling improvements.

Conclusion: The introduction of MDBench provides a valuable tool for evaluating LLMs' multi-document reasoning capabilities, showcasing the potential of knowledge-guided synthetic data generation.

Abstract: Natural language processing evaluation has made significant progress, largely
driven by the proliferation of powerful large language mod-els (LLMs). New
evaluation benchmarks are of increasing priority as the reasoning capabilities
of LLMs are expanding at a rapid pace. In particular, while multi-document (MD)
reasoning is an area of extreme relevance given LLM capabilities in handling
longer-context inputs, few benchmarks exist to rigorously examine model
behavior in this setting. Moreover, the multi-document setting is historically
challenging for benchmark creation due to the expensive cost of annotating long
inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs
on the task of multi-document reasoning. Notably, MDBench is created through a
novel synthetic generation process, allowing us to controllably and efficiently
generate challenging document sets and the corresponding question-answer (QA)
examples. Our novel technique operates on condensed structured seed knowledge,
modifying it through LLM-assisted edits to induce MD-specific reasoning
challenges. We then convert this structured knowledge into a natural text
surface form, generating a document set and corresponding QA example. We
analyze the behavior of popular LLMs and prompting techniques, finding that
MDBENCH poses significant challenges for all methods, even with relatively
short document sets. We also see our knowledge-guided generation technique (1)
allows us to readily perform targeted analysis of MD-specific reasoning
capabilities and (2) can be adapted quickly to account for new challenges and
future modeling improvements.

</details>


### [5] [From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?](https://arxiv.org/abs/2506.14949)
*Shadman Sakib,Oishy Fatema Akhand,Ajwad Abrar*

Main category: cs.CL

TL;DR: This study explores using Large Language Models (LLMs) for diabetes prediction with zero-shot, one-shot, and three-shot prompting methods. The best performance was achieved by proprietary LLMs, particularly GPT-4o and Gemma-2-27B, which outperformed traditional ML models in few-shot settings.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of LLMs in structured numerical data prediction, specifically for diabetes.

Method: Testing LLMs using different prompting methods and comparing them with traditional ML models using accuracy, precision, recall, and F1-score.

Result: Proprietary LLMs performed better than open-source ones. GPT-4o and Gemma-2-27B had the highest accuracy in few-shot settings and even outperformed traditional ML models in F1-score.

Conclusion: LLMs show promise for medical prediction tasks, encouraging further research into prompt engineering and hybrid approaches.

Abstract: While Machine Learning (ML) and Deep Learning (DL) models have been widely
used for diabetes prediction, the use of Large Language Models (LLMs) for
structured numerical data is still not well explored. In this study, we test
the effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and
three-shot prompting methods. We conduct an empirical analysis using the Pima
Indian Diabetes Database (PIDD). We evaluate six LLMs, including four
open-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We
also test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we
compare their performance with three traditional machine learning models:
Random Forest, Logistic Regression, and Support Vector Machine (SVM). We use
accuracy, precision, recall, and F1-score as evaluation metrics. Our results
show that proprietary LLMs perform better than open-source ones, with GPT-4o
and Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,
Gemma-2-27B also outperforms the traditional ML models in terms of F1-score.
However, there are still issues such as performance variation across prompting
strategies and the need for domain-specific fine-tuning. This study shows that
LLMs can be useful for medical prediction tasks and encourages future work on
prompt engineering and hybrid approaches to improve healthcare predictions.

</details>


### [6] [Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings](https://arxiv.org/abs/2506.15001)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: 研究发现LLMs能生成可逆句子嵌入，允许精确重构原始文本，展示了其在基于内存的检索、压缩及可控文本生成中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）的一个有趣现象：是否可以通过优化嵌入来生成可逆句子嵌入，使LLMs能够精确重构原始文本而不改变模型权重。

Method: 引入特殊记忆标记并优化其嵌入，通过在固定序列上训练实现可逆嵌入生成。

Result: 在英语和西班牙语数据集上测试了这一现象，涉及长度达约240个标记的序列以及参数范围从1亿到80亿的模型规模。值得注意的是，Llama 3.1 8B成功地重构了所有测试序列。

Conclusion: 通过引入特殊的记忆标记，LLMs可以在不修改权重的情况下生成可逆句子嵌入，并精确重构原始文本。此现象在多种数据集和模型规模下均被验证有效，尤其是Llama 3.1 8B成功重构了所有测试序列。研究强调了LLMs的有趣能力，并提出了其在基于内存检索、压缩和可控文本生成中的潜在应用。

Abstract: In this work, we observe an interesting phenomenon: it is possible to
generate reversible sentence embeddings that allow an LLM to reconstruct the
original text exactly, without modifying the model's weights. This is achieved
by introducing a special memory token, whose embedding is optimized through
training on a fixed sequence. When prompted with this embedding, the model
reconstructs the fixed sequence exactly. We evaluate this phenomenon across
English and Spanish datasets, sequences of up to approximately 240 tokens, and
model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B
successfully reconstructs all tested sequences. Our findings highlight an
interesting capability of LLMs and suggest potential applications in
memory-based retrieval, compression, and controlled text generation.

</details>


### [7] [Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods](https://arxiv.org/abs/2506.15030)
*Drew Walker,Swati Rajwal,Sudeshna Das,Snigdha Peddireddy,Abeed Sarker*

Main category: cs.CL

TL;DR: This study uses NLP techniques to identify social isolation and loneliness in NVDRS narratives, developing high-quality classifiers and finding significant predictors.


<details>
  <summary>Details</summary>
Motivation: Social isolation and loneliness are increasing and strongly contribute to suicide rates, but are not currently recorded in NVDRS structured variables.

Method: Using topic modeling for lexicon development and supervised learning classifiers to develop high-quality classifiers.

Result: Classifiers identified 1,198 mentions of chronic social isolation in over 300,000 suicides from 2002 to 2020, with certain groups having higher odds of classification.

Conclusion: The methods developed can improve surveillance and prevention of social isolation and loneliness in the United States.

Abstract: Social isolation and loneliness, which have been increasing in recent years
strongly contribute toward suicide rates. Although social isolation and
loneliness are not currently recorded within the US National Violent Death
Reporting System's (NVDRS) structured variables, natural language processing
(NLP) techniques can be used to identify these constructs in law enforcement
and coroner medical examiner narratives. Using topic modeling to generate
lexicon development and supervised learning classifiers, we developed
high-quality classifiers (average F1: .86, accuracy: .82). Evaluating over
300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic
social isolation. Decedents had higher odds of chronic social isolation
classification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =
3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).
We found significant predictors for other social isolation topics of recent or
impending divorce, child custody loss, eviction or recent move, and break-up.
Our methods can improve surveillance and prevention of social isolation and
loneliness in the United States.

</details>


### [8] [Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation](https://arxiv.org/abs/2506.15068)
*Zongxia Li,Yapei Chang,Yuhang Zhou,Xiyang Wu,Zichao Liang,Yoo Yeon Sung,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: Propose PrefBERT, a scoring model for evaluating open-ended long-form generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem.

Method: PrefBERT is trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality.

Result: PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do.

Conclusion: Using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences.

Abstract: Evaluating open-ended long-form generation is challenging because it is hard
to define what clearly separates good from bad outputs. Existing methods often
miss key aspects like coherence, style, or relevance, or are biased by
pretraining data, making open-ended long-form evaluation an underexplored
problem. To address this gap, we propose PrefBERT, a scoring model for
evaluating open-ended long-form generation in GRPO and guiding its training
with distinct rewards for good and bad outputs. Trained on two response
evaluation datasets with diverse long-form styles and Likert-rated quality,
PrefBERT effectively supports GRPO by offering better semantic reward feedback
than traditional metrics ROUGE-L and BERTScore do. Through comprehensive
evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,
we show that PrefBERT, trained on multi-sentence and paragraph-length
responses, remains reliable across varied long passages and aligns well with
the verifiable rewards GRPO needs. Human evaluations confirm that using
PrefBERT as the reward signal to train policy models yields responses better
aligned with human preferences than those trained with traditional metrics. Our
code is available at https://github.com/zli12321/long_form_rl.

</details>


### [9] [Learning-Time Encoding Shapes Unlearning in LLMs](https://arxiv.org/abs/2506.15076)
*Ruihan Wu,Konstantin Garov,Kamalika Chaudhuri*

Main category: cs.CL

TL;DR: This paper investigates the impact of learning-time knowledge encoding on the effectiveness of unlearning factual knowledge in large language models.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is the increasing need for large language models to 'unlearn' specific pieces of knowledge post hoc due to reasons such as privacy regulations and the need to correct outdated or harmful content.

Method: Experiments were conducted to investigate the impact of learning-time choices in knowledge encoding on the effectiveness of unlearning factual knowledge.

Result: Two key findings were revealed: 1) learning with paraphrased descriptions improves unlearning performance and 2) unlearning individual piece of knowledge from a chunk of text is challenging.

Conclusion: The study concludes that learning-time knowledge encoding plays a crucial role in effective post-hoc unlearning.

Abstract: As large language models (LLMs) are increasingly deployed in the real world,
the ability to ``unlearn'', or remove specific pieces of knowledge post hoc,
has become essential for a variety of reasons ranging from privacy regulations
to correcting outdated or harmful content. Prior work has proposed unlearning
benchmarks and algorithms, and has typically assumed that the training process
and the target model are fixed. In this work, we empirically investigate how
learning-time choices in knowledge encoding impact the effectiveness of
unlearning factual knowledge. Our experiments reveal two key findings: (1)
learning with paraphrased descriptions improves unlearning performance and (2)
unlearning individual piece of knowledge from a chunk of text is challenging.
Our results suggest that learning-time knowledge encoding may play a central
role in enabling reliable post-hoc unlearning.

</details>


### [10] [Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification](https://arxiv.org/abs/2506.15081)
*Yaxin Fan,Peifeng Li,Qiaoming Zhu*

Main category: cs.CL

TL;DR: Proposes a Discourse-aware Clarification Module (DCM) and Contribution-aware Preference Optimization (CPO) to improve dialogue discourse parsing by addressing ambiguities.


<details>
  <summary>Details</summary>
Motivation: To address ambiguities in dialogue discourse parsing caused by linguistic features like omission and idiom.

Method: Introduces DCM with two reasoning processes and CPO to reduce errors and optimize the module.

Result: Experiments show the approach effectively resolves ambiguities and outperforms existing baselines on STAC and Molweni datasets.

Conclusion: The proposed method improves dialogue discourse parsing performance by effectively handling ambiguities and optimizing the parsing process.

Abstract: Dialogue discourse parsing aims to identify and analyze discourse relations
between the utterances within dialogues. However, linguistic features in
dialogues, such as omission and idiom, frequently introduce ambiguities that
obscure the intended discourse relations, posing significant challenges for
parsers. To address this issue, we propose a Discourse-aware Clarification
Module (DCM) to enhance the performance of the dialogue discourse parser. DCM
employs two distinct reasoning processes: clarification type reasoning and
discourse goal reasoning. The former analyzes linguistic features, while the
latter distinguishes the intended relation from the ambiguous one. Furthermore,
we introduce Contribution-aware Preference Optimization (CPO) to mitigate the
risk of erroneous clarifications, thereby reducing cascading errors. CPO
enables the parser to assess the contributions of the clarifications from DCM
and provide feedback to optimize the DCM, enhancing its adaptability and
alignment with the parser's requirements. Extensive experiments on the STAC and
Molweni datasets demonstrate that our approach effectively resolves ambiguities
and significantly outperforms the state-of-the-art (SOTA) baselines.

</details>


### [11] [CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records](https://arxiv.org/abs/2506.15118)
*Junke Wang,Hongshun Ling,Li Zhang,Longqian Zhang,Fang Wang,Yuan Gao,Zhi Li*

Main category: cs.CL

TL;DR: Proposes a framework called CKD-EHR that uses knowledge distillation to improve disease prediction in electronic health records. The method enhances both accuracy and speed of predictions compared to existing models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of insufficient medical knowledge representation and low efficiency in clinical deployment faced by existing large language models in disease prediction.

Method: Fine-tune a large language model on medical knowledge-enhanced data to create a teacher model, generate soft labels with a multi-granularity attention distillation mechanism, and transfer knowledge to a lightweight BERT student model.

Result: On the MIMIC-III dataset, CKD-EHR improves diagnostic accuracy by 9%, F1-score by 27%, and achieves a 22.2 times inference speedup over the baseline model.

Conclusion: This study demonstrates that the CKD-EHR framework can greatly enhance resource utilization efficiency and diagnostic accuracy/timeliness in clinical settings.

Abstract: Electronic Health Records (EHR)-based disease prediction models have
demonstrated significant clinical value in promoting precision medicine and
enabling early intervention. However, existing large language models face two
major challenges: insufficient representation of medical knowledge and low
efficiency in clinical deployment. To address these challenges, this study
proposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which
achieves efficient and accurate disease risk prediction through knowledge
distillation techniques. Specifically, the large language model Qwen2.5-7B is
first fine-tuned on medical knowledge-enhanced data to serve as the teacher
model.It then generates interpretable soft labels through a multi-granularity
attention distillation mechanism. Finally, the distilled knowledge is
transferred to a lightweight BERT student model. Experimental results show that
on the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline
model:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and
a 22.2 times inference speedup is achieved. This innovative solution not only
greatly improves resource utilization efficiency but also significantly
enhances the accuracy and timeliness of diagnosis, providing a practical
technical approach for resource optimization in clinical settings. The code and
data for this research are available athttps://github.com/209506702/CKD_EHR.

</details>


### [12] [Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs](https://arxiv.org/abs/2506.15131)
*Jing Yang Lee,Kong-Aik Lee,Woon-Seng Gan*

Main category: cs.CL

TL;DR: This paper introduces a two-stage framework for open-domain dialogue generation in LLMs, enhancing response diversity while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: To model the one-to-many property of open-domain dialogues and improve response diversity in LLM-based dialogue agents.

Method: Decomposing OD generation into Multi-Response Generation and Preference-based Selection tasks, introducing o2mDial corpus, and proposing new in-context learning, instruction-tuning strategies, and evaluation metrics.

Result: The proposed framework improves response diversity and quality by up to 90% while maintaining contextual coherence.

Conclusion: The two-stage framework can enhance smaller LLMs' performance in open-domain dialogue generation, making their responses more similar to those of larger models.

Abstract: Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby
multiple appropriate responses exist for a single dialogue context. Despite
prior research showing that modeling this property boosts response diversity,
most modern LLM-based dialogue agents do not explicitly do so. In this work, we
model the o2m property of OD in LLMs by decomposing OD generation into two key
tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),
which entail generating a set of n semantically and lexically diverse
high-quality responses for a given dialogue context, followed by selecting a
single response based on human preference, respectively. To facilitate MRG and
PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the
o2m property by featuring multiple plausible responses for each context.
Leveraging o2mDial, we propose new in-context learning and instruction-tuning
strategies, as well as novel evaluation metrics for MRG, alongside a
model-based approach for PS. Empirical results demonstrate that applying the
proposed two-stage framework to smaller LLMs for OD generation enhances overall
response diversity while maintaining contextual coherence, improving response
quality by up to 90%, bringing them closer to the performance of larger models.

</details>


### [13] [Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models](https://arxiv.org/abs/2506.15138)
*Gyeongje Cho,Yeonkyoun So,Chanwoo Park,Sangmin Lee,Sungmok Jung,Jaejin Lee*

Main category: cs.CL

TL;DR: Thunder-Tok is a new Korean tokenizer that improves inference speed by reducing token fertility without affecting model performance.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient tokenizer for Korean language models by reducing token fertility.

Method: A rule-based pre-tokenization method aligned with Korean linguistic structure, a seed vocabulary resembling linguistic units, and a branching entropy-based selection algorithm were used.

Result: Thunder-Tok reduces fertility by 10%, decreases the number of tokens by 10%, and increases inference speed by 10% compared to BPE without impacting performance on various downstream tasks.

Conclusion: The linguistically informed approach of Thunder-Tok is effective and practical for creating efficient tokenizers for language models.

Abstract: This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce
token fertility without compromising model performance. Our approach uses a
rule-based pre-tokenization method that aligns with the linguistic structure of
the Korean language. We also create a seed vocabulary containing tokens that
resemble linguistic units and employ a branching entropy-based selection
algorithm. These techniques increase the average token length, thus lowering
fertility while preserving linguistic information. Experimental results
indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces
the number of tokens by 10%, improving the inference speed by 10%) compared to
BPE without compromising performance across various downstream tasks. These
findings demonstrate that our linguistically informed approach is effective and
practical for designing efficient tokenizers for language models.

</details>


### [14] [Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View](https://arxiv.org/abs/2506.15156)
*Muhammad Cendekia Airlangga,Hilal AlQuabeh,Munachiso S Nwadike,Kentaro Inui*

Main category: cs.CL

TL;DR: This paper studies memory mechanisms in state-space language models using primacy and recency effects. It identifies three mechanisms contributing to a U-shaped accuracy profile: long-term memory in selective state space block, short-term memory affected by exponential decay, and dynamic modulation of memory allocation by semantic regularity.


<details>
  <summary>Details</summary>
Motivation: To understand how information is retained and forgotten over time in state-space language models using behavioral tools like primacy and recency effects.

Method: Applying structured recall tasks to the Mamba architecture and observing the accuracy profile. Identifying mechanisms through analysis and validating findings via targeted ablations and input perturbations.

Result: A consistent U-shaped accuracy profile was observed, indicating strong performance at the beginning and end of input sequences. Three mechanisms were identified: long-term memory in selective state space block, short-term memory influenced by exponential decay, and dynamic modulation of memory allocation by semantic regularity.

Conclusion: The study reveals insights into memory mechanisms in state-space language models, providing a better understanding of how these models retain and forget information over time.

Abstract: We study memory in state-space language models using primacy and recency
effects as behavioral tools to uncover how information is retained and
forgotten over time. Applying structured recall tasks to the Mamba
architecture, we observe a consistent U-shaped accuracy profile, indicating
strong performance at the beginning and end of input sequences. We identify
three mechanisms that give rise to this pattern. First, long-term memory is
supported by a sparse subset of channels within the model's selective state
space block, which persistently encode early input tokens and are causally
linked to primacy effects. Second, short-term memory is governed by
delta-modulated recurrence: recent inputs receive more weight due to
exponential decay, but this recency advantage collapses when distractor items
are introduced, revealing a clear limit to memory depth. Third, we find that
memory allocation is dynamically modulated by semantic regularity: repeated
relations in the input sequence shift the delta gating behavior, increasing the
tendency to forget intermediate items. We validate these findings via targeted
ablations and input perturbations on two large-scale Mamba-based language
models: one with 1.4B and another with 7B parameters.

</details>


### [15] [A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals](https://arxiv.org/abs/2506.15208)
*Andrea Cadeddu,Alessandro Chessa,Vincenzo De Leo,Gianni Fenu,Enrico Motta,Francesco Osborne,Diego Reforgiato Recupero,Angelo Salatino,Luca Secchi*

Main category: cs.CL

TL;DR: This study examines different LLMs for classifying text related to the UN's Sustainable Development Goals, finding that optimized smaller models can perform as well as larger ones.


<details>
  <summary>Details</summary>
Motivation: Tracking progress towards the UN's 17 Sustainable Development Goals is challenging due to the vast and complex data involved. Text classification models, especially LLMs, are crucial for analyzing such data.

Method: The study analyzes proprietary and open-source LLMs for a single-label, multi-class text classification task focused on the SDGs. It evaluates Zero-Shot, Few-Shot Learning, and Fine-Tuning methods.

Result: Smaller models, when optimized through prompt engineering, can perform equally well as larger models like OpenAI's GPT.

Conclusion: Optimized smaller models are effective for text classification tasks related to the SDGs.

Abstract: In 2012, the United Nations introduced 17 Sustainable Development Goals
(SDGs) aimed at creating a more sustainable and improved future by 2030.
However, tracking progress toward these goals is difficult because of the
extensive scale and complexity of the data involved. Text classification models
have become vital tools in this area, automating the analysis of vast amounts
of text from a variety of sources. Additionally, large language models (LLMs)
have recently proven indispensable for many natural language processing tasks,
including text classification, thanks to their ability to recognize complex
linguistic patterns and semantics. This study analyzes various proprietary and
open-source LLMs for a single-label, multi-class text classification task
focused on the SDGs. Then, it also evaluates the effectiveness of task
adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot
and Few-Shot Learning, as well as Fine-Tuning within this domain. The results
reveal that smaller models, when optimized through prompt engineering, can
perform on par with larger models like OpenAI's GPT (Generative Pre-trained
Transformer).

</details>


### [16] [ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs](https://arxiv.org/abs/2506.15211)
*Feng He,Zijun Chen,Xinnian Liang,Tingting Ma,Yunqi Qiu,Shuangzhi Wu,Junchi Yan*

Main category: cs.CL

TL;DR: Recent progress in Large Reasoning Models (LRMs) shows their impressive ability to generalize across domains. However, how they do so is unclear. This paper suggests that these models can generalize because they share abstract reasoning patterns. It introduces ProtoReasoning, a method that improves reasoning in LLMs using scalable and verifiable prototypical representations. Experiments show improvements on different tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanism behind the cross-domain generalization capability of LRMs.

Method: Proposing ProtoReasoning, which uses prototypical representations (Prolog for logical reasoning, PDDL for planning) to enhance reasoning abilities.

Result: ProtoReasoning improved performance on various tasks like logical reasoning, planning, general reasoning, and mathematics. Ablation studies confirmed better generalization in prototype space than in natural language representations.

Conclusion: The paper concludes that reasoning prototypes are foundational for generalizable reasoning in large language models.

Abstract: Recent advances in Large Reasoning Models (LRMs) trained with Long
Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain
generalization capabilities. However, the underlying mechanisms supporting such
transfer remain poorly understood. We hypothesize that cross-domain
generalization arises from shared abstract reasoning prototypes -- fundamental
reasoning patterns that capture the essence of problems across domains. These
prototypes minimize the nuances of the representation, revealing that seemingly
diverse tasks are grounded in shared reasoning structures.Based on this
hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning
ability of LLMs by leveraging scalable and verifiable prototypical
representations (Prolog for logical reasoning, PDDL for
planning).ProtoReasoning features: (1) an automated prototype construction
pipeline that transforms problems into corresponding prototype representations;
(2) a comprehensive verification system providing reliable feedback through
Prolog/PDDL interpreters; (3) the scalability to synthesize problems
arbitrarily within prototype space while ensuring correctness. Extensive
experiments show that ProtoReasoning achieves 4.7% improvement over baseline
models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning
tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics
(AIME24). Significantly, our ablation studies confirm that learning in
prototype space also demonstrates enhanced generalization to structurally
similar problems compared to training solely on natural language
representations, validating our hypothesis that reasoning prototypes serve as
the foundation for generalizable reasoning in large language models.

</details>


### [17] [MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs](https://arxiv.org/abs/2506.15215)
*Yongqi Fan,Yating Wang,Guandong Wang,Jie Zhai,Jingping Liu,Qi Ye,Tong Ruan*

Main category: cs.CL

TL;DR: Proposes MinosEval, a novel evaluation method for open-ended QA that improves alignment with human annotations and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics and current LLM-based evaluation approaches fail to meet the requirements of open-ended QA, lacking intuitive interpretability and failing to adapt across different question contents.

Method: Distinguishes open-ended questions and ranks candidate answers using different evaluation strategies. For factoid questions, it applies an adaptive key-point scoring strategy, while for non-factoid questions, it uses an instance-aware listwise ranking strategy.

Result: Experiments on multiple open-ended QA datasets show that MinosEval better aligns with human annotations and offers more interpretable results.

Conclusion: MinosEval provides better alignment with human annotations and more interpretable results.

Abstract: Open-ended question answering (QA) is a key task for evaluating the
capabilities of large language models (LLMs). Compared to closed-ended QA, it
demands longer answer statements, more nuanced reasoning processes, and diverse
expressions, making refined and interpretable automatic evaluation both crucial
and challenging. Traditional metrics like ROUGE and BERTScore struggle to
capture semantic similarities due to different patterns between model responses
and reference answers. Current LLM-based evaluation approaches, such as
pairwise or listwise comparisons of candidate answers, lack intuitive
interpretability. While pointwise scoring of each response provides some
descriptions, it fails to adapt across different question contents. Most
notably, existing methods overlook the distinction between factoid and
non-factoid questions. To address these challenges, we propose
\textbf{MinosEval}, a novel evaluation method that first distinguishes
open-ended questions and then ranks candidate answers using different
evaluation strategies. For factoid questions, it applies an adaptive key-point
scoring strategy, while for non-factoid questions, it uses an instance-aware
listwise ranking strategy. Experiments on multiple open-ended QA datasets,
including self-built ones with more candidate responses to complement community
resources, show that MinosEval better aligns with human annotations and offers
more interpretable results.

</details>


### [18] [Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants](https://arxiv.org/abs/2506.15239)
*Jaione Bengoetxea,Itziar Gonzalez-Dios,Rodrigo Agerri*

Main category: cs.CL

TL;DR: This paper evaluates current language technologies' ability to understand Basque and Spanish language varieties using NLI tasks and newly created parallel datasets. It finds performance drops in handling linguistic variation, especially in Basque, which isn't due to lexical overlap but linguistic variation itself.


<details>
  <summary>Details</summary>
Motivation: To assess the capacity of current language technologies to understand Basque and Spanish language varieties.

Method: Using Natural Language Inference as a pivot task and introducing a novel, manually-curated parallel dataset in Basque and Spanish, along with their respective variants. Conducting empirical analysis of crosslingual and in-context learning experiments using encoder-only and decoder-based Large Language Models.

Result: Performance drop when handling linguistic variation, especially in Basque. This decline is not due to lexical overlap, but rather to the linguistic variation itself. Encoder-only models particularly struggle with Western Basque.

Conclusion: Current language technologies face challenges in understanding linguistic variation, especially in Basque. Encoder-only models have more difficulty with peripheral dialects.

Abstract: In this paper, we evaluate the capacity of current language technologies to
understand Basque and Spanish language varieties. We use Natural Language
Inference (NLI) as a pivot task and introduce a novel, manually-curated
parallel dataset in Basque and Spanish, along with their respective variants.
Our empirical analysis of crosslingual and in-context learning experiments
using encoder-only and decoder-based Large Language Models (LLMs) shows a
performance drop when handling linguistic variation, especially in Basque.
Error analysis suggests that this decline is not due to lexical overlap, but
rather to the linguistic variation itself. Further ablation experiments
indicate that encoder-only models particularly struggle with Western Basque,
which aligns with linguistic theory that identifies peripheral dialects (e.g.,
Western) as more distant from the standard. All data and code are publicly
available.

</details>


### [19] [Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs](https://arxiv.org/abs/2506.15241)
*Yang Fan,Zhang Qi,Xing Wenqian,Liu Chang,Liu Liu*

Main category: cs.CL

TL;DR: This paper proposes the Graph RAG framework to address domain knowledge gaps in general large language models for historical text analysis.


<details>
  <summary>Details</summary>
Motivation: To create a The First Four Histories character relationship dataset with minimal manual annotation and support automated historical knowledge extraction, reducing labor costs.

Method: Combining chain-of-thought prompting, self-instruction generation, and process supervision to create the dataset and introducing a collaborative mechanism between knowledge graphs and retrieval-augmented generation.

Result: The domain-specific model Xunzi-Qwen1.5-14B achieves optimal performance in relation extraction (F1 = 0.68). The DeepSeek model integrated with GraphRAG improves F1 by 11% on the open-domain C-CLUE relation extraction dataset.

Conclusion: This framework offers a low-resource solution for classical text knowledge extraction, advancing historical knowledge services and humanities research.

Abstract: This article addresses domain knowledge gaps in general large language models
for historical text analysis in the context of computational humanities and
AIGC technology. We propose the Graph RAG framework, combining chain-of-thought
prompting, self-instruction generation, and process supervision to create a The
First Four Histories character relationship dataset with minimal manual
annotation. This dataset supports automated historical knowledge extraction,
reducing labor costs. In the graph-augmented generation phase, we introduce a
collaborative mechanism between knowledge graphs and retrieval-augmented
generation, improving the alignment of general models with historical
knowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,
with Simplified Chinese input and chain-of-thought prompting, achieves optimal
performance in relation extraction (F1 = 0.68). The DeepSeek model integrated
with GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation
extraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),
effectively alleviating hallucinations phenomenon, and improving
interpretability. This framework offers a low-resource solution for classical
text knowledge extraction, advancing historical knowledge services and
humanities research.

</details>


### [20] [TopClustRAG at SIGIR 2025 LiveRAG Challenge](https://arxiv.org/abs/2506.15246)
*Juli Bakagianni,John Pavlopoulos,Aristidis Likas*

Main category: cs.CL

TL;DR: TopClustRAG is a retrieval-augmented generation system that uses hybrid retrieval and clustering to improve question answering over large web corpora.


<details>
  <summary>Details</summary>
Motivation: To develop an effective retrieval-augmented generation system for answering questions over large-scale web corpora.

Method: A hybrid retrieval strategy combining sparse and dense indices, followed by K-Means clustering to group semantically similar passages. Representative passages are used to construct cluster-specific prompts for a large language model, which generates intermediate answers that are filtered, reranked, and synthesized into a final response.

Result: TopClustRAG ranked 2nd in faithfulness and 7th in correctness on the official leaderboard when evaluated on the FineWeb Sample-10BT dataset.

Conclusion: The study demonstrates the effectiveness of clustering-based context filtering and prompt aggregation in large-scale RAG systems.

Abstract: We present TopClustRAG, a retrieval-augmented generation (RAG) system
developed for the LiveRAG Challenge, which evaluates end-to-end question
answering over large-scale web corpora. Our system employs a hybrid retrieval
strategy combining sparse and dense indices, followed by K-Means clustering to
group semantically similar passages. Representative passages from each cluster
are used to construct cluster-specific prompts for a large language model
(LLM), generating intermediate answers that are filtered, reranked, and finally
synthesized into a single, comprehensive response. This multi-stage pipeline
enhances answer diversity, relevance, and faithfulness to retrieved evidence.
Evaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in
faithfulness and 7th in correctness on the official leaderboard, demonstrating
the effectiveness of clustering-based context filtering and prompt aggregation
in large-scale RAG systems.

</details>


### [21] [Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments](https://arxiv.org/abs/2506.15266)
*Sungen Hahm,Heejin Kim,Gyuseong Lee,Hyunji Park,Jaejin Lee*

Main category: cs.CL

TL;DR: Propose a de-identification framework called Thunder-DeID for Korean court judgments.


<details>
  <summary>Details</summary>
Motivation: To ensure a balance between open access to justice and personal data protection while adhering to strict legal requirements.

Method: Construct a Korean legal dataset, categorize PII systematically, and develop a DNN-based de-identification pipeline.

Result: The proposed model achieves state-of-the-art performance in de-identifying court judgments.

Conclusion: Thunder-DeID aligns with relevant laws and practices and improves the efficiency and effectiveness of de-identification of court judgments.

Abstract: To ensure a balance between open access to justice and personal data
protection, the South Korean judiciary mandates the de-identification of court
judgments before they can be publicly disclosed. However, the current
de-identification process is inadequate for handling court judgments at scale
while adhering to strict legal requirements. Additionally, the legal
definitions and categorizations of personal identifiers are vague and not
well-suited for technical solutions. To tackle these challenges, we propose a
de-identification framework called Thunder-DeID, which aligns with relevant
laws and practices. Specifically, we (i) construct and release the first Korean
legal dataset containing annotated judgments along with corresponding lists of
entity mentions, (ii) introduce a systematic categorization of Personally
Identifiable Information (PII), and (iii) develop an end-to-end deep neural
network (DNN)-based de-identification pipeline. Our experimental results
demonstrate that our model achieves state-of-the-art performance in the
de-identification of court judgments.

</details>


### [22] [Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment](https://arxiv.org/abs/2506.15301)
*Shrestha Ghosh,Moritz Schneider,Carina Reinicke,Carsten Eickhoff*

Main category: cs.CL

TL;DR: Survey on the application of large language models in clinical trial recruitment.


<details>
  <summary>Details</summary>
Motivation: Limited adoption of LLMs in critical domains like clinical trial recruitment despite improvements in general-domain NLP tasks.

Method: Analysis of trial-patient matching task and contextualization of emerging LLM-based approaches in clinical trial recruitment.

Result: Critical examination of existing benchmarks, approaches and evaluation frameworks, challenges to adopting LLM technologies in clinical research and future directions.

Conclusion: LLMs have the potential to build a more general solution for trial-patient matching but there are challenges in adopting these technologies in clinical research.

Abstract: Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,
their adoption in critical domains, such as clinical trial recruitment, remains
limited. As trials are designed in natural language and patient data is
represented as both structured and unstructured text, the task of matching
trials and patients benefits from knowledge aggregation and reasoning abilities
of LLMs. Classical approaches are trial-specific and LLMs with their ability to
consolidate distributed knowledge hold the potential to build a more general
solution. Yet recent applications of LLM-assisted methods rely on proprietary
models and weak evaluation benchmarks. In this survey, we are the first to
analyze the task of trial-patient matching and contextualize emerging LLM-based
approaches in clinical trial recruitment. We critically examine existing
benchmarks, approaches and evaluation frameworks, the challenges to adopting
LLM technologies in clinical research and exciting future directions.

</details>


### [23] [ConLID: Supervised Contrastive Learning for Low-Resource Language Identification](https://arxiv.org/abs/2506.15304)
*Negar Foroutan,Jakhongir Saydaliev,Ye Eun Kim,Antoine Bosselut*

Main category: cs.CL

TL;DR: This paper introduces a new method using supervised contrastive learning to improve language identification performance for low-resource languages on out-of-domain data.


<details>
  <summary>Details</summary>
Motivation: To address class imbalance and bias issues in low-resource languages with limited single-domain data.

Method: Proposes a novel supervised contrastive learning approach to learn domain-invariant representations.

Result: Improves LID performance on out-of-domain data for low-resource languages by 3.2%.

Conclusion: The proposed method effectively enhances LID models for low-resource languages.

Abstract: Language identification (LID) is a critical step in curating multilingual LLM
pretraining corpora from web crawls. While many studies on LID model training
focus on collecting diverse training data to improve performance, low-resource
languages -- often limited to single-domain data, such as the Bible -- continue
to perform poorly. To resolve these class imbalance and bias issues, we propose
a novel supervised contrastive learning (SCL) approach to learn
domain-invariant representations for low-resource languages. Through an
extensive analysis, we show that our approach improves LID performance on
out-of-domain data for low-resource languages by 3.2%, demonstrating its
effectiveness in enhancing LID models.

</details>


### [24] [DeVisE: Behavioral Testing of Medical Large Language Models](https://arxiv.org/abs/2506.15339)
*Camila Zurdo Tagliabue,Heloisa Oss Boll,Aykut Erdem,Erkut Erdem,Iacer Calixto*

Main category: cs.CL

TL;DR: Introducing DeVisE to evaluate clinical LLMs' reasoning through demographic and vital sign counterfactuals.


<details>
  <summary>Details</summary>
Motivation: To develop a method for distinguishing genuine medical reasoning from superficial patterns in LLMs.

Method: Constructing a dataset of ICU discharge notes from MIMIC-IV, creating both real-world and synthetic versions with controlled counterfactuals, and evaluating five LLMs in zero-shot and fine-tuned settings.

Result: Zero-shot models have coherent counterfactual reasoning patterns, while fine-tuned models are stable but less responsive to clinically significant changes.

Conclusion: DeVisE reveals subtle but consistent demographic influences on LLM outputs, highlighting the need for fairness-aware evaluation.

Abstract: Large language models (LLMs) are increasingly used in clinical decision
support, yet current evaluation methods often fail to distinguish genuine
medical reasoning from superficial patterns. We introduce DeVisE (Demographics
and Vital signs Evaluation), a behavioral testing framework for probing
fine-grained clinical understanding. We construct a dataset of ICU discharge
notes from MIMIC-IV, generating both raw (real-world) and template-based
(synthetic) versions with controlled single-variable counterfactuals targeting
demographic (age, gender, ethnicity) and vital sign attributes. We evaluate
five LLMs spanning general-purpose and medically fine-tuned variants, under
both zero-shot and fine-tuned settings. We assess model behavior via (1)
input-level sensitivity - how counterfactuals alter the likelihood of a note;
and (2) downstream reasoning - how they affect predicted hospital
length-of-stay. Our results show that zero-shot models exhibit more coherent
counterfactual reasoning patterns, while fine-tuned models tend to be more
stable yet less responsive to clinically meaningful changes. Notably,
demographic factors subtly but consistently influence outputs, emphasizing the
importance of fairness-aware evaluation. This work highlights the utility of
behavioral testing in exposing the reasoning strategies of clinical LLMs and
informing the design of safer, more transparent medical AI systems.

</details>


### [25] [SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture](https://arxiv.org/abs/2506.15355)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Sriparna Saha*

Main category: cs.CL

TL;DR: SANSKRITI是一个评估语言模型对印度丰富文化多样性理解的基准。它包含来自印度各地的21,853个精心策划的问题-答案对，涵盖了16个关键的文化属性。该数据集揭示了大型语言模型在处理文化细微差别查询时的能力差异。


<details>
  <summary>Details</summary>
Motivation: 为了使语言模型更好地理解和适应当地社会文化背景，特别是印度的文化多样性。

Method: 创建了一个名为SANSKRITI的数据集，包含21,853个问题-答案对，覆盖了印度的28个邦和8个联邦直辖区。

Result: SANSKRITI是测试印度文化知识的最大数据集。它显示了许多模型在特定地区上下文中的表现不佳。

Conclusion: SANSKRITI为评估和提高语言模型的文化理解设定了新的标准。

Abstract: Language Models (LMs) are indispensable tools shaping modern workflows, but
their global effectiveness depends on understanding local socio-cultural
contexts. To address this, we introduce SANSKRITI, a benchmark designed to
evaluate language models' comprehension of India's rich cultural diversity.
Comprising 21,853 meticulously curated question-answer pairs spanning 28 states
and 8 union territories, SANSKRITI is the largest dataset for testing Indian
cultural knowledge. It covers sixteen key attributes of Indian culture: rituals
and ceremonies, history, tourism, cuisine, dance and music, costume, language,
art, festivals, religion, medicine, transport, sports, nightlife, and
personalities, providing a comprehensive representation of India's cultural
tapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic
Language Models (ILMs), and Small Language Models (SLMs), revealing significant
disparities in their ability to handle culturally nuanced queries, with many
models struggling in region-specific contexts. By offering an extensive,
culturally rich, and diverse dataset, SANSKRITI sets a new standard for
assessing and improving the cultural understanding of LMs.

</details>


### [26] [COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation](https://arxiv.org/abs/2506.15372)
*Raghvendra Kumar,S. A. Mohammed Salman,Aryan Sahu,Tridib Nandi,Pragathi Y. P.,Sriparna Saha,Jose G. Moreno*

Main category: cs.CL

TL;DR: This study introduces COSMMIC, a novel dataset for comment-aware multimodal and multilingual summarization in nine major Indian languages, enhancing summaries by integrating text, images, and user feedback.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in research for Indian languages in comment-aware multimodal and multilingual summarization.

Method: Integrating reader insights and feedback across four configurations: article text alone, incorporating user comments, utilizing images, and combining text, comments, and images.

Result: COSMMIC comprises 4,959 article-image pairs and 24,484 reader comments with ground-truth summaries in all included languages. It uses advanced language models and classifiers to filter noise and extract insights.

Conclusion: COSMMIC advances NLP research and fosters inclusivity by bridging gaps in Indian language resources through its unique integration of text, images, and user feedback.

Abstract: Despite progress in comment-aware multimodal and multilingual summarization
for English and Chinese, research in Indian languages remains limited. This
study addresses this gap by introducing COSMMIC, a pioneering comment-sensitive
multimodal, multilingual dataset featuring nine major Indian languages. COSMMIC
comprises 4,959 article-image pairs and 24,484 reader comments, with
ground-truth summaries available in all included languages. Our approach
enhances summaries by integrating reader insights and feedback. We explore
summarization and headline generation across four configurations: (1) using
article text alone, (2) incorporating user comments, (3) utilizing images, and
(4) combining text, comments, and images. To assess the dataset's
effectiveness, we employ state-of-the-art language models such as LLama3 and
GPT-4. We conduct a comprehensive study to evaluate different component
combinations, including identifying supportive comments, filtering out noise
using a dedicated comment classifier using IndicBERT, and extracting valuable
insights from images with a multilingual CLIP-based classifier. This helps
determine the most effective configurations for natural language generation
(NLG) tasks. Unlike many existing datasets that are either text-only or lack
user comments in multimodal settings, COSMMIC uniquely integrates text, images,
and user feedback. This holistic approach bridges gaps in Indian language
resources, advancing NLP research and fostering inclusivity.

</details>


### [27] [Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning](https://arxiv.org/abs/2506.15415)
*Stanley Ngugi*

Main category: cs.CL

TL;DR: This paper introduces Targeted Lexical Injection (TLI), a novel and efficient fine-tuning approach for large language models to improve their performance in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address the issue of poor performance of large language models in low-resource languages due to data scarcity and underrepresentation.

Method: Using Low-Rank Adaptation (LoRA) and a contrastive learning objective to fine-tune the model targeting embeddings from an early layer.

Result: Output-level lexical alignment for Swahili-English word pairs improved significantly.

Conclusion: TLI significantly improves the lexical alignment for both trained and unseen Swahili-English word pairs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their performance in low-resource languages (LRLs), such as Swahili, often lags
due to data scarcity and underrepresentation in pre-training. A key challenge
is achieving robust cross-lingual lexical alignment, crucial for tasks like
translation and cross-lingual information retrieval. This paper introduces
Targeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.
We first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits
strong, near-perfect lexical alignment for Swahili-English word pairs in its
early internal layers (specifically Layer 2, with ~0.99998 average cosine
similarity based on a pilot study), a capability not fully reflected in its
final output representations (baseline ~0.32 similarity on our evaluation set).
TLI leverages this insight by using Low-Rank Adaptation (LoRA) and a
contrastive learning objective to fine-tune the model, specifically targeting
embeddings from this empirically identified optimal early layer. Our
experiments show that TLI significantly improves the output-level lexical
alignment for 623 trained Swahili-English word pairs, increasing average cosine
similarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More
importantly, these improvements generalize remarkably well to 63 unseen control
word pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17
x 10^-27). These findings suggest TLI enhances the model's ability to preserve
and propagate its inherent early-layer cross-lingual knowledge, offering a
parameter-efficient and effective strategy for improving lexical alignment in
LRL-focused LLMs.

</details>

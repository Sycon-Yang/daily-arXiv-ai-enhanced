<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 109]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.CV](#cs.CV) [Total: 13]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种双模型架构，用于检测大型语言模型中的幻觉信号。通过实验发现幻觉信号集中在稀疏特征子集中，并揭示了分层的“漏斗模式”，为开发计算效率高的检测系统提供了途径。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的幻觉是一个关键挑战，因为输出可能偏离事实准确性。尽管最近的研究表明特定的隐藏层在幻觉内容和事实内容之间存在差异，但幻觉信号在层内的精确定位仍然不明确，限制了高效检测方法的发展。

Method: 我们提出了一种双模型架构，集成了一个投影融合（PF）块和一个差异特征学习（DFL）机制。PF块用于自适应的层间特征加权，而DFL机制通过计算并行编码器之间的差异来识别区分性特征。

Result: 通过在HaluEval的问题回答、对话和摘要数据集上的系统实验，我们证明幻觉信号集中在高度稀疏的特征子集中，在问题回答和对话任务上实现了显著的准确率提升。分析还揭示了一个分层的“漏斗模式”，浅层表现出高特征多样性，而深层表现出集中使用，仅使用1%的特征维度即可保持检测性能。

Conclusion: 我们的研究结果表明，幻觉信号比之前假设的更加集中，这为开发计算效率高的检测系统提供了途径，可以在保持准确性的同时减少推理成本。

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [2] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的上下文选择方法，通过引入上下文影响值（CI值）来量化上下文质量，并开发了一个参数化代理模型来预测CI值。实验结果表明，该方法在多个NLP任务和多个LLM上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的方法试图通过基于预定义上下文质量评估指标的上下文选择来提高性能，但它们在标准RAG上的增益有限。我们将其限制归因于未能全面利用可用信息（查询、上下文列表和生成器）进行综合质量评估。

Method: 我们重新概念化了上下文质量评估为一个推理时间的数据估值问题，并引入了上下文影响值（CI值）。该新颖的指标通过测量移除每个上下文时的性能退化来量化上下文质量，有效地整合了查询感知的相关性、列表感知的独特性和生成器感知的一致性。此外，CI值通过简单地保留具有正CI值的上下文，消除了复杂的选择超参数调优。为了应对标签依赖性和计算开销的实际挑战，我们开发了一个用于CI值预测的参数化代理模型。该模型采用分层架构，捕捉局部查询-上下文相关性和全局互相关系，通过Oracle CI值监督和端到端生成器反馈进行训练。

Result: 在8个NLP任务和多个LLM上的广泛实验表明，我们的上下文选择方法显著优于最先进的基线，有效过滤了低质量的上下文，同时保留了关键信息。

Conclusion: 我们的上下文选择方法在多个NLP任务和多个LLM上显著优于最先进的基线，有效地过滤了低质量的上下文，同时保留了关键信息。

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [3] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型的最大有效上下文窗口，并发现其与报告的最大上下文窗口存在显著差异，同时根据问题类型而变化。


<details>
  <summary>Details</summary>
Motivation: 为了测试上下文窗口在现实世界中的使用情况，我们希望了解报告的最大上下文窗口（MCW）和最大有效上下文窗口（MECW）之间的差异。

Method: 我们定义了一个最大有效上下文窗口的概念，制定了测试上下文窗口在不同大小和问题类型上的有效性的方法，并创建了一种标准化的方式来比较模型在越来越大的上下文窗口大小下的效果以找到失败点。

Result: 我们收集了数千个数据点，发现报告的MCW大小和MECW大小之间存在显著差异。MECW不仅与MCW大不相同，而且根据问题类型而变化。一些顶级模型在仅有100个标记的上下文中就失败了；大多数在1000个标记的上下文中准确性严重下降。所有模型都远未达到其最大上下文窗口，最多相差99%。

Conclusion: 我们的数据揭示了最大有效上下文窗口会根据提供的问题类型而变化，这为如何提高模型准确性并减少模型幻觉率提供了明确且可操作的见解。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [4] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: The paper argues that AI needs human-crafted symbols to guide its development beyond simple scaling.


<details>
  <summary>Details</summary>
Motivation: To unlock genuine discovery in AI, it is necessary to go beyond mere scaling and incorporate human guidance.

Method: Argumentation and discussion of the need for human-crafted symbols in AI development.

Result: The paper highlights the importance of human-crafted symbols in guiding AI's development.

Conclusion: AI's future requires more than scaling, and large language models need human-crafted symbols to guide their intuition.

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [5] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 本文研究了语言如何影响大型语言模型中的道德决策，发现其道德判断在不同语言中存在显著差异，并提出了一个结构化的道德推理错误类型学。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言和多文化环境中越来越重要，但它们在英语数据上的主导预训练引发了对其在不同语言和文化背景下泛化判断能力的担忧。

Method: 我们将两个已建立的道德推理基准翻译成五种文化和语言类型多样的语言，进行多语言零样本评估，并通过精心构建的研究问题揭示了这些差异的潜在驱动因素。

Result: 分析显示，大型语言模型在不同语言中的道德判断存在显著不一致，通常反映了文化不一致。

Conclusion: 本文提出了一个结构化的道德推理错误类型学，呼吁更加注重文化意识的人工智能。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [6] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: 本研究评估了GPT-5在糖尿病相关任务中的表现，结果表明其能够有效辅助临床医生和患者，同时突出了构建可重复评估框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病是全球主要的健康挑战，早期识别仍然困难，因为症状模糊、实验室值接近临界值、妊娠复杂性和长期监测的需求。大型语言模型（LLM）的进步为通过结构化、可解释和以患者为中心的输出增强决策支持提供了机会。

Method: 本研究使用完全基于合成案例的模拟框架评估了GPT-5，这些案例符合ADA 2025标准，并受到NHANES、Pima Indians、EyePACS和MIMIC-IV等公开数据集的启发。测试了五个代表性场景：症状识别、实验室解释、妊娠糖尿病筛查、远程监测和多模态并发症检测。对于每个场景，GPT-5对病例进行分类，生成临床推理，产生患者解释，并输出结构化的JSON摘要。

Result: 研究结果显示，GPT-5在与ADA定义的标准对齐方面表现出色，表明它可能作为临床医生和患者的双重用途工具，同时强调了可重复评估框架在负责任地评估医疗领域LLM中的重要性。

Conclusion: 研究结果显示，GPT-5在与ADA定义的标准对齐方面表现出色，表明它可能作为临床医生和患者的双重用途工具，同时强调了可重复评估框架在负责任地评估医疗领域LLM中的重要性。

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [7] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 本论文分析了道德对齐过程中性能权衡的机制，发现当前公平目标在实现权衡方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决道德对齐过程中下游任务性能下降的问题，通过研究公平目标的有效性来实现性能权衡。

Method: 通过分析遗忘和公平目标的视角，研究了在减轻性别刻板印象背景下性能权衡的潜在机制。

Result: 发现下游任务性能主要由整体遗忘水平驱动，选择性遗忘刻板印象会增加整体遗忘，而缓解遗忘的通用解决方案无效。

Conclusion: 当前公平目标在实现性能权衡方面存在局限性，因为选择性遗忘刻板印象会增加整体遗忘水平，而缓解遗忘的通用解决方案无法有效减少整体遗忘并提高下游任务性能。

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [8] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于RLVR的简单训练方法，在BIRD基准测试中取得了最先进的结果，展示了其在企业领域的广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 开发能够结合组织特定知识的定制推理模型，通过强化学习（RL）来解决企业客户面临的问题具有巨大潜力。在许多这些问题中，奖励函数是可验证的，这就是所谓的RLVR（可验证奖励强化学习）。

Method: 本文应用了RLVR（可验证奖励强化学习）到BIRD基准测试中，采用了一种简单的训练方法，包括仔细的提示和模型选择、使用离线RL方法TAO的预热阶段，以及严格的在线RLVR训练。

Result: 在没有额外训练数据和没有使用专有模型的情况下，作者的第一个提交在BIRD排行榜上达到了最先进的准确率：73.56%（无自一致性）和75.68%（有自一致性）。在后者的情况下，模型所需的生成次数也少于第二好的方法。

Conclusion: 本文提出了一种简单且通用的训练方法，能够在不使用额外训练数据和专有模型的情况下，在BIRD基准测试中达到最先进的准确率。该方法具有广泛的应用潜力，适用于企业领域如商业智能、数据科学和编程。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [9] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 研究提出了MoT-G方法，在RLVR中提升了训练效率和推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前的RLVR方法在每个推理步骤中仅采样离散标记，忽略了模型概率分布中的丰富信息。为了克服这一限制，研究了MoT-G方法。

Method: 研究了混合token生成（MoT-G）在强化学习与可验证奖励（RLVR）中的应用，提出了一种统一框架，并扩展了RLVR以直接在连续混合空间中生成思维链。

Result: 在Reasoning-Gym任务中，MoT-G方法在7个任务中取得了5-35%的显著提升，同时使用一半的轨迹数量达到了相似的准确性。

Conclusion: MoT-G方法在RLVR中表现出色，能够提高训练效率并改善推理能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [10] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: 本文提出了一种名为Dual-Head Reasoning Distillation (DHRD)的简单训练方法，用于解码器仅语言模型，以解决Chain-of-Thought (CoT)提示带来的吞吐量惩罚问题。DHRD在七个SuperGLUE任务中取得了显著的性能提升，同时保持了与池化分类器相当的推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT)提示虽然能提高分类准确性，但会引入显著的吞吐量惩罚。为了缓解这种权衡，提出了Dual-Head Reasoning Distillation (DHRD)。

Method: DHRD是一种简单的训练方法，为解码器仅语言模型添加了（i）一个在训练和推理期间使用的池化分类头，以及（ii）一个仅在训练期间使用教师推理监督的推理头。

Result: 在七个SuperGLUE任务中，DHRD相对于池化基线获得了0.65-5.47%的相对增益，特别是在蕴含/因果任务上增益更大。推理头在测试时被禁用，因此推理吞吐量与池化分类器相当，并且在相同后端上比CoT解码快96-142倍。

Conclusion: DHRD在七个SuperGLUE任务中相对于池化基线获得了相对增益，且在推理阶段的吞吐量与池化分类器相当，并显著优于CoT解码。

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [11] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 研究显示，代码数据可以增强大型语言模型的推理能力，但不清楚具体是哪些方面。通过系统框架和实验，发现模型对结构扰动更敏感，而适当的抽象可以达到类似效果。


<details>
  <summary>Details</summary>
Motivation: 研究代码数据如何增强大型语言模型（LLMs）的推理能力，以及哪些方面的代码最为关键。

Method: 我们构建了十种编程语言的并行指令数据集，并应用了选择性破坏代码结构或语义属性的受控扰动。然后，我们在每个变体上微调来自五个模型家族和八个规模的LLM，并评估它们在自然语言、数学和代码任务上的表现。

Result: LLMs对结构扰动比语义扰动更脆弱，尤其是在数学和代码任务中。适当的抽象如伪代码和流程图可以与代码一样有效，而用更少的标记编码相同的信息而不遵循原始语法通常可以保持或甚至提高性能。值得注意的是，即使有误导信号的损坏代码，当表面规律存在时仍然具有竞争力。此外，语法风格也会影响特定任务的收益，Python有利于自然语言推理，而Java和Rust等低级语言有利于数学。

Conclusion: 通过我们的系统框架，我们旨在提供对代码不同属性如何影响推理的见解，并为增强LLM推理能力的训练数据设计提供信息。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [12] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 本文介绍了一个基于Kisan Call Center数据集的农业聊天机器人，能够提供关于天气、市场价格、植物保护和政府政策的信息，准确率达到86%。


<details>
  <summary>Details</summary>
Motivation: 印度是一个以农业为基础的经济，关于农业实践的正确信息是实现最佳农业增长和产出的关键。为了回答农民的问题，需要一个能够提供天气、市场价格、植物保护和政府政策等信息的系统。

Method: 基于来自Kisan Call Center的数据集构建了一个农业聊天机器人。该系统基于句子嵌入模型，并通过消除同义词和引入实体提取来提高准确性。

Result: 该系统能够回答与天气、市场价格、植物保护和政府政策相关的问题，具有86%的准确率。

Conclusion: 通过该系统，农民可以更容易地获取与农业相关的实践信息，从而提高农业产出。呼叫中心工作人员的工作将变得更加轻松，他们的辛勤工作可以被重新导向更好的目标。

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [13] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 本研究分析了非洲口音英语在说话人辨识中的领域效应，发现临床对话存在显著的领域惩罚，并提出了一种轻量级领域适应方法以改善性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨非洲口音英语在说话人辨识中的领域效应，并寻找改善临床对话处理的方法。

Method: 研究评估了多个生产系统和开放系统在通用和临床对话中的表现，并通过微调分割模块进行轻量级领域适应测试。

Result: 研究发现临床语音存在显著的领域惩罚，错误分析表明这与假警报和遗漏检测有关。通过微调分割模块可以减少误差，但无法消除差距。

Conclusion: 研究指出，临床对话中存在持续的领域惩罚，且在不同模型中均显著。结果表明，需要更注重重叠感知的分割和平衡的临床资源作为下一步实践。

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [14] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: 本文比较了两种引用范式G-Cite和P-Cite，并提供了基于证据的建议，推荐在高风险应用中使用P-Cite优先方法。


<details>
  <summary>Details</summary>
Motivation: 可信的大语言模型必须在医疗、法律、学术和金融等高风险领域引用可由人类验证的来源，因为即使小错误也可能造成严重后果。

Method: 我们引入了两种范式：生成时间引用（G-Cite）和事后引用（P-Cite），并进行了从零样本到高级检索增强方法的全面评估。

Result: 结果表明，在两种范式中，检索是影响引用质量的主要因素。P-Cite方法在保持竞争力的正确性的同时实现了高覆盖率和适度的延迟，而G-Cite方法则以覆盖范围和速度为代价优先考虑精确度。

Conclusion: 我们建议在高风险应用中采用以检索为中心的P-Cite优先方法，并将G-Cite保留用于需要精确验证的场景，如严格的声明验证。

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [15] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 本文提出了一种名为ComPSum的个性化多文档摘要框架，通过比较用户偏好来生成个性化摘要，并引入了AuthorMap评估框架和PerMSum数据集进行评估，结果表明ComPSum优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个性化多文档摘要（MDS）对于满足用户对写作风格和内容焦点的个人偏好至关重要。为了实现有效的个性化，需要识别用户偏好之间的细粒度差异。

Method: ComPSum框架通过比较用户的偏好与其他用户的偏好来生成结构化的用户分析，并利用该分析指导个性化摘要的生成。此外，提出了AuthorMap评估框架和PerMSum数据集以进行有效的评估。

Result: ComPSum在PerMSum数据集上使用AuthorMap评估框架表现出色，优于强基线。

Conclusion: ComPSum在PerMSum数据集上的表现优于强基线，证明了其有效性。

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [16] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: 本文介绍了五种VLM-as-formalizer管道，用于处理单次、开放词汇和多模态PDDL形式化。评估结果显示，VLM-as-formalizer在长时程规划任务中显著优于端到端计划生成。研究发现，视觉是主要瓶颈，因为VLMs往往无法捕捉所有必要的对象关系。虽然生成中间文本表示（如描述或场景图）可以部分补偿性能，但其不一致的增益为未来多模态规划形式化研究留下了空间。


<details>
  <summary>Details</summary>
Motivation: The advancement of vision language models (VLMs) has empowered embodied agents to accomplish simple multimodal planning tasks, but not long-horizon ones requiring long sequences of actions. In text-only simulations, long-horizon planning has seen significant improvement brought by repositioning the role of LLMs. Instead of directly generating action sequences, LLMs translate the planning domain and problem into a formal planning language like the Planning Domain Definition Language (PDDL), which can call a formal solver to derive the plan in a verifiable manner. In multimodal environments, research on VLM-as-formalizer remains scarce, usually involving gross simplifications such as predefined object vocabulary or overly similar few-shot examples.

Method: We present a suite of five VLM-as-formalizer pipelines that tackle one-shot, open-vocabulary, and multimodal PDDL formalization.

Result: We evaluate those on an existing benchmark while presenting another two that for the first time account for planning with authentic, multi-view, and low-quality images. While generating intermediate, textual representations such as captions or scene graphs partially compensate for the performance, their inconsistent gain leaves headroom for future research directions on multimodal planning formalization.

Conclusion: VLM-as-formalizer greatly outperforms end-to-end plan generation. The bottleneck is vision rather than language, as VLMs often fail to capture an exhaustive set of necessary object relations.

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [17] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: 本研究探讨了最先进的多语言AI模型在将英语中的隐喻语言（如成语和双关语）翻译成多种全球语言时的本地化能力。研究发现，尽管这些模型通常能生成语法正确的翻译，但文化细微差别仍然是一个明显需要改进的领域，这表明文化适当性是多语言大模型性能的重要决定因素。


<details>
  <summary>Details</summary>
Motivation: 现有LLM翻译研究和行业基准侧重于语法准确性和逐标记正确性，而本研究关注文化适当性和整体本地化质量，这对市场营销和电子商务等实际应用至关重要。

Method: 本研究评估了24种语言的20种地区方言中87个大语言模型生成的电子商务营销邮件翻译。母语为目标语言的人类评审员对翻译的忠实度、语气、含义和目标受众进行了定量评分和定性反馈。

Result: 研究发现，尽管领先的模型通常能生成语法正确的翻译，但文化细微差别仍然是一个明显需要改进的领域，通常需要大量人工修改。值得注意的是，即使高资源全球语言在行业基准排行榜上名列前茅，也经常误译比喻表达和双关语。

Conclusion: 本研究挑战了数据量是机器翻译质量最可靠预测因素的假设，并引入了文化适当性作为多语言大模型性能的关键决定因素，目前这一领域在现有学术和行业基准中尚未得到充分探索。研究结果支持了在更大规模上进行扩展研究的机会，以提供可推广的见解，并指导在文化多样环境中可靠机器翻译工作流的部署。

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [18] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 本文探讨了多目标强化学习在大型语言模型优化中的挑战和机遇，提出了一个MORL基准框架的愿景，并指出了未来研究方向，包括通过双层学习范式改进效率和灵活性的元策略MORL开发。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决多目标强化学习（MORL）在大型语言模型（LLM）优化中的挑战，以及探索更高效和灵活的方法来应对LLM的内在复杂性和个性化需求。

Method: 本文引入了一个MORL分类法，并探讨了各种MORL方法在LLM优化中的优缺点，识别出需要高效且灵活的方法来适应LLM和个人化功能的复杂性。

Result: 本文提出了一个MORL基准框架的愿景，并指出了未来的研究方向，包括元策略MORL的发展，以提高LLM的性能。

Conclusion: 本文提出了一个MORL基准框架的愿景，以解决不同方法对多种目标关系的影响，并强调了未来研究方向，包括通过双层学习范式改进效率和灵活性的元策略MORL开发。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [19] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: OjaKV is a framework that improves memory efficiency for long-context inference by combining full-rank preservation of important tokens with online low-rank compression.


<details>
  <summary>Details</summary>
Motivation: The expanding long-context capabilities of large language models are constrained by a significant memory bottleneck: the key-value (KV) cache required for autoregressive generation.

Method: OjaKV integrates a strategic hybrid storage policy with online subspace adaptation. It preserves crucial first and recent tokens in full-rank and applies low-rank compression to intermediate tokens using Oja's algorithm for online principal component analysis.

Result: Experiments show that OjaKV maintains or even improves zero-shot accuracy at high compression ratios, especially on very long-context benchmarks requiring complex reasoning.

Conclusion: OjaKV provides a practical, plug-and-play solution for memory-efficient long-context inference without requiring model fine-tuning.

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [20] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文综述了针对语言模型的可解释人工智能（XAI）技术，按照其Transformer架构进行分类，并评估了这些方法的优缺点，旨在推动更强大、透明和可解释的XAI方法的发展。


<details>
  <summary>Details</summary>
Motivation: 由于语言模型的黑箱特性，其内部机制和决策过程的可解释性成为关键问题。尽管已有许多关于XAI的研究，但它们往往未能捕捉到语言模型架构多样性和能力演进所带来的独特挑战。因此，本文旨在填补这一研究空白。

Method: 本文采用了一种系统性的综述方法，根据底层的Transformer架构（如仅编码器、仅解码器和编码器-解码器）对XAI技术进行了分类，并分析了这些方法如何适应每种架构，同时评估了它们各自的优缺点。

Result: 本文提供了对XAI技术的全面回顾，特别关注语言模型，并根据其Transformer架构进行分类。此外，还通过合理性与忠实性的双重视角评估了这些技术，并指出了开放的研究挑战和未来方向。

Conclusion: 本文旨在通过全面回顾XAI技术，特别是针对语言模型（LMs）的XAI方法，提供一个结构化的视角来评估其有效性，并识别开放的研究挑战和未来方向。

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [21] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: 本研究旨在解决AI会议中评审质量下降的问题，通过定义误导性评论点并提出自动化评估方法，验证了LLMs在ReviewScore评估中的能力，并发现评估前提层面的真实性更有效。


<details>
  <summary>Details</summary>
Motivation: 在AI会议中，随着提交数量激增，评审质量正在下降，需要可靠地检测低质量的评审。

Method: 定义了误导性评论点，提出了一个自动化引擎来重建弱点中的每个显性和隐性前提，并构建了一个由人类专家标注的ReviewScore数据集来检查LLMs自动评估ReviewScore的能力。

Result: 验证了15.2%的弱点和26.4%的问题是误导性的，并引入了ReviewScore来指示评审点是否是误导性的。同时，测量了八种最先进的LLMs在ReviewScore上的协议水平，并验证了适度的协议。

Conclusion: 研究证明了评估前提层面的真实性比评估弱点层面的真实性显示出更高的协议水平，并且通过详细的不一致分析进一步支持了完全自动化的ReviewScore评估的潜力。

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [22] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: GRAB 是一个用于评估财务披露中无监督主题模型的金融专用基准，提供可重复和标准化的比较方法。


<details>
  <summary>Details</summary>
Motivation: 目前没有公开的基准来评估无监督主题模型在 10-K 风险披露中的表现，因此需要一个专门的金融基准。

Method: GRAB 通过结合 FinBERT 的标记注意力、YAKE 关键短语信号和基于分类的共现匹配来生成无需人工注释的句子标签。

Result: GRAB 包含 1.61M 句子和 8,247 份文件，并使用基于熵的 Effective Number of Topics 等稳健指标进行评估。

Conclusion: GRAB 提供了一个统一的评估框架，使不同类型的主题模型可以在财务披露中进行可重复和标准化的比较。

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [23] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: This paper presents ToG-3, a novel framework that introduces MACER mechanism to overcome limitations in existing graph-based RAG methods. It dynamically constructs and refines a heterogeneous graph index with a dual-evolution mechanism, allowing adaptive building of a targeted graph index during reasoning, thus enabling deep, precise reasoning even with lightweight LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models.

Method: Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. The core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval.

Result: Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.

Conclusion: ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [24] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 本文介绍了ProPerSim，这是一个新的任务和模拟框架，用于开发能够在现实家庭场景中做出及时、个性化的推荐的助手。基于此框架，我们提出了ProPerAssistant，它能够通过用户反馈持续学习和适应，并在实验中展示了其提高用户满意度的能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）日益融入日常生活，对不仅具有反应性而且具有主动性和个性化的AI助手的需求不断增加。尽管最近的进展在主动性和个性化方面取得了进展，但两者的结合仍研究不足。

Method: 我们引入了ProPerSim，这是一个新的任务和模拟框架，用于开发能够在现实家庭场景中做出及时、个性化的推荐的助手。基于ProPerSim，我们提出了ProPerAssistant，这是一种检索增强型、偏好对齐的助手，能够通过用户反馈持续学习和适应。

Result: 在32种不同的角色中进行的实验表明，ProPerAssistant能够适应其策略并逐步提高用户满意度。

Conclusion: 实验表明，ProPerAssistant能够适应其策略并逐步提高用户满意度，这表明将主动性与个性化相结合的前景广阔。

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [25] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）基于同一对话上下文回答多个问题的能力，并通过实验验证了经过微调的公开LLM在准确率上可以超越专有模型，展示了其在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，将大型语言模型（LLM）用于回答基于长上下文的问题是一个重大挑战，因为计算成本和延迟很高，尤其是在需要根据同一上下文回答多个问题时。

Method: 我们进行了广泛的实验，并在这个具有挑战性的任务上对一系列专有和公共模型进行了基准测试。

Result: 研究发现，虽然强大的专有LLM如GPT-4o在整体性能上表现最佳，但经过微调的公开LLM在准确率上可以超越GPT-4o。

Conclusion: 研究表明，尽管强大的专有LLM如GPT-4o在整体性能上表现最佳，但经过微调的公开LLM在准确率上可以超越GPT-4o，这表明它们在实际应用中具有透明和成本效益的潜力。

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [26] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Self-Speculative Biased Decoding的新推理范式，旨在避免在持续增长的输入流中重复从头生成输出。该方法通过使用最近的输出作为当前输入上下文的草稿，并在验证阶段将输出偏向于草稿标记以提高草稿接受率，从而减少了闪烁并提高了速度。实验结果表明，该方法在保持质量的同时，比传统的自回归重翻译方法快1.7倍，并显著减少了80%的闪烁。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在各种文本生成任务中表现出色，但在流式应用（如实时翻译）中使用时仍面临挑战，因为输出必须随着输入上下文的扩展而不断更新，同时仍需保持合理的计算成本以满足延迟要求。

Method: 本文提出了一种名为Self-Speculative Biased Decoding的新推理范式，旨在避免在持续增长的输入流中重复从头生成输出。该方法通过使用最近的输出作为当前输入上下文的草稿，并在验证阶段将输出偏向于草稿标记以提高草稿接受率。

Result: 实验结果表明，该方法在保持质量的同时，比传统的自回归重翻译方法快1.7倍，并显著减少了80%的闪烁。

Conclusion: 本文提出了一种名为Self-Speculative Biased Decoding的新推理范式，旨在避免在持续增长的输入流中重复从头生成输出。该方法通过使用最近的输出作为当前输入上下文的草稿，并在验证阶段将输出偏向于草稿标记以提高草稿接受率，从而减少了闪烁并提高了速度。实验结果表明，该方法在保持质量的同时，比传统的自回归重翻译方法快1.7倍，并显著减少了80%的闪烁。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [27] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文介绍了Thinking-with-Sound (TwS)框架，通过结合语言推理和实时音频领域分析，使大型音频语言模型（LALMs）具备Audio CoT。实验表明，TwS显著提高了模型在复杂声学场景下的鲁棒性，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的LALMs在复杂声学场景中的挑战性音频推理任务中表现出显著的局限性，需要使用如噪声抑制、源分离和精确时间对齐等声学工具，但当前LALMs缺乏这些工具的访问权限。

Method: 我们引入了Thinking-with-Sound (TwS)框架，该框架通过将语言推理与实时音频领域分析相结合，使LALMs具备Audio CoT。

Result: 实验表明，最先进的LALMs在MELD-Hard1k上的性能大幅下降，准确率比干净音频下降了50%以上。TwS在鲁棒性方面取得了显著改进，小模型获得了24.73%的绝对准确率提升，大模型的提升持续增加到36.61%。

Conclusion: 我们的研究结果表明，Audio CoT可以显著提高鲁棒性，而无需重新训练，为开发更稳健的音频理解系统开辟了新的方向。

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [28] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 本文提出了一种名为SynerGen的新型生成推荐模型，能够同时处理个性化搜索和推荐任务，并在多个基准测试中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的生成序列模型通常只解决个性化搜索或无查询推荐，当尝试统一两者时往往表现出性能权衡。

Method: 本文引入了SynerGen，一种新型的生成推荐模型，通过使用仅解码器的Transformer和联合优化方法，同时处理检索和排序任务。

Result: SynerGen在广泛采用的推荐和搜索基准上相比强大的生成推荐和联合搜索与推荐基线取得了显著改进。

Conclusion: 本文展示了单一生成基础模型在工业规模统一信息访问中的可行性。

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [29] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 本文通过因果推断分析了结构化输出对大型语言模型生成的影响，发现大多数情况下没有显著的因果影响。


<details>
  <summary>Details</summary>
Motivation: 先前的研究对结构化输出的影响得出了一面之词的结论，但这些评估可能存在测试场景受限、比较设置弱控制和依赖粗略指标的问题。

Method: 我们使用因果推断进行分析，并基于一个假设和两个保证的约束推导出五个潜在的因果结构。

Result: 在七个公开和一个开发的推理任务中，粗略指标报告了结构化输出对GPT-4o生成的正面、负面或中性影响，但因果推断显示在48个场景中有43个没有因果影响。

Conclusion: 我们的研究发现，结构化输出对GPT-4o生成的影响在因果推断下并不显著，只有少数情况下存在复杂的因果结构。

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [30] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个文化意识奖励建模基准（CARB），用于评估大型语言模型的文化意识，并通过强化学习从可验证奖励中获取更深层次的文化基础推理。


<details>
  <summary>Details</summary>
Motivation: 现有的RM评估在评估文化意识方面存在不足，因为缺乏文化相关的评估数据集。

Method: 通过强化学习从可验证奖励（RLVR）中获取更深层次的文化基础推理，并使用精心设计的奖励确保准确的偏好判断和高质量的结构化评估标准生成。

Result: 我们的评估揭示了最先进的RMs在建模文化意识方面的不足，并展示了CARB上的表现与下游多语言文化对齐任务之间的正相关关系。进一步分析发现了文化意识奖励建模中的虚假相关性，其中RM的评分主要依赖于表面特征而不是真实的文化细微理解。

Conclusion: 实验结果验证了其在减轻虚假特征干扰和推进文化意识奖励建模方面的有效性。

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [31] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了一种扩展同步机器翻译动作空间的方法，通过引入四个自适应动作来实现实时重构、省略和简化，同时保持语义保真度。实验结果表明，该方法在语义指标和延迟方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的编码器-解码器策略无法完全解决严格实时约束下的高质量翻译问题，因此需要扩展动作空间以实现实时重构、省略和简化。

Method: 我们通过动作感知提示构建了训练参考，并在仅解码器的大语言模型框架中实现了这些动作。此外，我们还开发了一个考虑延迟的TTS管道，以现实的时间将文本输出映射到语音。

Result: 实验结果显示，我们的框架在语义指标（例如COMET-KIWI）上持续改进，并且与参考翻译和salami基线相比，延迟更低（通过平均滞后衡量）。结合DROP和SENTENCE_CUT可以实现流畅性和延迟之间的最佳平衡。

Conclusion: 这些结果表明，丰富基于大语言模型的同步机器翻译的动作空间为弥合人类和机器理解之间的差距提供了有前景的方向。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [32] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果原理的多模态信息瓶颈模型（CaMIB），通过分解多模态表示并引入工具变量约束和反向调整策略，提高了模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法容易受到数据集偏差的影响，导致模型混淆统计捷径与真实因果特征，从而影响分布外泛化能力。因此，需要一种基于因果原理的方法来提高模型的鲁棒性和泛化能力。

Method: CaMIB模型通过信息瓶颈过滤单模态输入，去除与任务无关的噪声，并利用参数化掩码生成器将融合的多模态表示分解为因果和捷径子表示。为了确保因果特征的全局一致性，引入了工具变量约束，并通过随机重组因果和捷径特征进行反向调整以稳定因果估计。

Result: CaMIB模型在多个多模态任务和分布外测试集上取得了显著的效果，证明了其有效性和优越性。

Conclusion: CaMIB模型在多模态情感分析、幽默检测和讽刺检测任务中表现出色，并且在分布外测试集中也显示出良好的泛化能力。理论和实证分析进一步证明了其可解释性和稳健性。

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [33] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型解决和生成语言谜题的潜力，发现它们在大多数情况下优于人类，但对特定类型谜题效果不佳。


<details>
  <summary>Details</summary>
Motivation: 为了扩大人们对语言学的兴趣，并向更广泛的受众介绍该领域，自动化生成语言谜题具有潜力。

Method: 扩展现有的语言谜题解决基准，探索使用大型语言模型（如OpenAI的o1）解决语言谜题，并利用谜题解决实验的见解来指导谜题生成任务。

Result: 大型语言模型在大多数谜题类型上表现优于人类，但针对书写系统和研究不足的语言的谜题除外。

Conclusion: 自动化生成语言谜题可以促进语言学的发展，并有助于传播关于稀有和研究不足语言的知识。

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [34] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: ResT is a new method for optimizing tool-use policies in large language models, which improves training efficiency and performance by considering token-level entropy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiencies in training tool-use policies by considering the particularity of these tasks and the role of policy entropy in training stability.

Method: ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds.

Result: ResT achieves state-of-the-art results, outperforming prior methods by up to 8.76% on BFCL and API-Bank. When fine-tuned on a 4B base LLM, ResT surpasses GPT-4o by 4.11% on single-turn tasks and 1.50% on multi-turn tasks.

Conclusion: ResT achieves state-of-the-art results on tool-use tasks, outperforming prior methods and even surpassing GPT-4o in certain scenarios.

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [35] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 本文提出了一种名为语义级联的方法，通过模型输出之间的语义一致性来判断何时将任务交给更大的模型，从而在保持高质量的同时降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 级联系统在可能的情况下将计算请求路由到较小的模型，并仅在必要时依赖较大的模型，这为平衡LLM部署的成本和质量提供了一个有前景的方法。然而，它们在开放式的文本生成中面临一个根本性的挑战：当生成质量位于连续谱上时，确定输出的可靠性，通常有多个有效的响应。

Method: 我们提出了语义一致性——集成输出之间的意义层面的一致性——作为可靠延迟的训练-free 信号。

Result: 从5亿到700亿参数模型的评估表明，语义级联在成本仅为40%的情况下可以匹配或超过目标模型的质量，并将延迟减少了高达60%。

Conclusion: 我们的方法不需要模型内部信息，可以在黑盒API上工作，并且对模型更新具有鲁棒性，使其成为实际LLM部署的实用基线。

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [36] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为TRACE的新框架，通过将任务分解为分析和合成的管道来建模共情，从而解决了现有方法在分析深度和生成流畅性之间的权衡问题。实验结果表明，该框架在多个评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在专业模型的分析深度和大型语言模型（LLMs）的生成流畅性之间存在核心权衡。

Method: 我们提出了TRACE，一种新的框架，通过将任务分解为分析和合成的管道来将共情建模为结构化的认知过程。

Result: 实验结果表明，我们的框架在自动和基于LLM的评估中都显著优于强大的基线。

Conclusion: 我们的框架显著优于强大的基线，在自动和基于LLM的评估中都证明了结构化分解在创建更强大和可解释的共情代理方面的前景。

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [37] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 本文介绍了KnowMT-Bench，这是第一个专门设计用于系统评估大型语言模型在知识密集领域中的多轮长格式问答的基准测试。实验结果显示，多轮上下文会降低性能，但检索增强生成（RAG）可以有效缓解这种退化。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试仅限于单轮对话，而多轮对话基准测试通常评估其他正交能力而不是知识密集型事实性。为了填补这一关键差距，我们引入了KnowMT-Bench。

Method: 我们引入了KnowMT-Bench，这是第一个专门设计用于系统评估大型语言模型在知识密集领域中的多轮长格式问答的基准测试。该基准测试采用动态评估设置，模型根据逻辑上逐步推进的问题序列生成自己的多轮对话历史。

Result: 实验结果显示，多轮上下文会降低性能：由于自动生成的历史记录中的上下文噪声，事实能力下降；随着对话长度的增加，信息效率下降。我们还研究了缓解策略，证明检索增强生成（RAG）可以有效缓解甚至逆转这种事实退化。

Conclusion: 我们的研究结果强调了基准测试在评估和提高大型语言模型在现实世界知识密集型应用中的对话事实能力的重要性。

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [38] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: LoRAN 是一种非线性扩展的 LoRA 方法，通过 Sinter 激活函数提升模型性能，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LoRA 的线性性质限制了其表达能力，因此需要一种非线性方法来提升性能。

Method: 提出 LoRAN，一种非线性扩展的 LoRA 方法，并引入 Sinter 激活函数，以结构化扰动方式增强模型性能。

Result: 实验表明 LoRAN 在摘要和分类任务中优于 QLoRA，且 Sinter 激活函数表现优于标准激活函数如 Sigmoid、ReLU 和 Tanh。

Conclusion: LoRAN 是一种非线性扩展的 LoRA 方法，通过轻量级变换提高表达能力，并且 Sinter 激活函数在不增加参数数量的情况下表现出色。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [39] [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
*Min-Hsuan Yeh,Yixuan Li,Tanwi Mallick*

Main category: cs.CL

TL;DR: LUMINA is a new framework for detecting hallucinations in RAG systems by analyzing context-knowledge signals. It outperforms previous methods and is robust under various conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting hallucinations in RAG-based LLMs require extensive hyperparameter tuning, limiting their generalizability. The goal is to develop a more effective and practical approach.

Method: LUMINA is a novel framework that detects hallucinations in RAG systems through context-knowledge signals: external context utilization is quantified via distributional distance, while internal knowledge utilization is measured by tracking how predicted tokens evolve across transformer layers. A framework for statistically validating these measurements is also introduced.

Result: Experiments on common RAG hallucination benchmarks and four open-source LLMs show that LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG.

Conclusion: LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under relaxed assumptions about retrieval quality and model matching, offering both effectiveness and practicality.

Abstract: Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

</details>


### [40] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新算法RL-ZVP，能够从零方差提示中提取学习信号，从而在多个数学推理基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前的方法忽略了所有响应都获得相同奖励的提示，即所谓的零方差提示，但我们认为这些提示实际上可以提供有意义的反馈用于策略优化。

Method: 我们引入了RL-ZVP，这是一种新颖的算法，可以从零方差提示中提取学习信号。

Result: 在六个数学推理基准测试中，RL-ZVP在准确率和通过率方面相对于GRPO分别提高了8.61分和7.77分，并且始终优于其他过滤掉零方差提示的基线模型。

Conclusion: 这些结果突显了在RLVR中从零方差提示中学习的未开发潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [41] [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
*Yijin Guo,Ye Shen,Farong Wen,Junying Wang,Zicheng Zhang,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了QoNext框架，将服务质量原则应用于基础模型的评估，通过实验收集用户评分，构建数据库并训练预测模型，以估计用户体验，从而提供优化基础模型的实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法未能捕捉到用户在交互过程中的真实体验，当前方法仅关注输出的正确性，忽略了用户满意度来自于响应质量和交互之间的相互作用，这限制了它们对用户体验底层机制的解释能力。

Method: 引入了QoNext框架，该框架适应了网络和多媒体中的服务质量（QoE）原则，用于评估基础模型。识别出影响用户体验的体验因素，并将其纳入受控实验中，收集人类评分。从这些研究中构建了一个以QoE为导向的数据库，并训练了预测模型，从可测量的系统参数估计感知的用户体验。

Result: QoNext的结果表明，它不仅能够实现主动和细粒度的评估，还能为实际产品化服务优化基础模型提供可操作的指导。

Conclusion: QoNext不仅实现了主动和细粒度的评估，还为实际产品化服务优化基础模型提供了可操作的指导。

Abstract: Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.

</details>


### [42] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出EMoE框架，解决MoE模型在推理时扩展专家数量导致性能下降的问题，显著提升性能扩展范围和峰值性能。


<details>
  <summary>Details</summary>
Motivation: 传统Mixture-of-Experts (MoE)模型在训练和推理时固定激活的专家数量k，但发现当推理时增加激活的专家数量k'时，性能会迅速下降，这源于专家之间缺乏协作。

Method: 引入了Elastic Mixture-of-Experts (EMoE)训练框架，使MoE模型能够在推理时扩展激活专家的数量而不增加额外的训练开销。

Result: EMoE能够扩展有效性能扩展范围，使其达到训练时k的2-3倍，并提升模型的峰值性能。

Conclusion: EMoE显著扩展了有效性能扩展范围，并将模型的峰值性能提升到更高水平。

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [43] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin Eravcı*

Main category: cs.CL

TL;DR: 本文提出了一种系统的方法和一个基础数据集，以解决土耳其语引用意图识别的问题，并通过可编程分类管道和堆叠泛化集成实现了高准确率的分类。


<details>
  <summary>Details</summary>
Motivation: 理解引用的定性意图对于全面评估学术研究至关重要，但这一任务对于像土耳其语这样的黏着语言来说具有独特的挑战性。

Method: 我们引入了一个基于DSPy框架的可编程分类管道，该管道系统地自动化了提示优化，并使用堆叠泛化集成来聚合多个优化模型的输出，最终采用XGBoost元模型进行分类。

Result: 该集成方法在准确率方面达到了91.3%的最先进水平。

Conclusion: 本研究为土耳其自然语言处理社区和更广泛的学术界提供了基础数据集和强大的分类框架，为未来的定性引用研究铺平了道路。

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [44] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: AutoSCORE is a multi-agent framework that improves automated scoring by aligning with rubrics through structured component recognition, leading to better accuracy, interpretability, and performance across various datasets and LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of using large language models (LLMs) as end-to-end raters in automated scoring, such as low accuracy, prompt sensitivity, limited interpretability, and rubric misalignment.

Method: AutoSCORE is a multi-agent LLM framework that enhances automated scoring via rubric-aligned Structured COmponent REcognition. It uses two agents: one to extract rubric-relevant components from student responses and another to assign final scores based on those components.

Result: AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK, correlations), and error metrics (MAE, RMSE) compared to single-agent baselines, especially on complex, multi-dimensional rubrics and smaller LLMs.

Conclusion: AutoSCORE demonstrates that structured component recognition combined with a multi-agent design offers a scalable, reliable, and interpretable solution for automated scoring.

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [45] [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
*Haotian Tan,Hiroki Ouchi,Sakriani Sakti*

Main category: cs.CL

TL;DR: 本文提出了一种新的SimulST框架SimulSense，该框架通过模拟人类口译员的读写决策，实现了更高的实时效率和更好的质量-延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的SimulST系统将SimulST形式化为多轮对话任务，需要专门的交错训练数据，并依赖计算成本高昂的大语言模型（LLM）推理进行决策。

Method: 本文提出了一种新的SimulST框架SimulSense，该框架通过持续读取输入语音并触发写入决策来模拟人类口译员的读写决策。

Result: 实验结果表明，本文提出的方法在质量-延迟权衡和实时效率方面表现出色，其决策速度比基线系统快至9.6倍。

Conclusion: 本文提出的SimulSense框架在质量-延迟权衡和实时效率方面优于现有的最先进的基线系统，其决策速度比基线系统快至9.6倍。

Abstract: How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.

</details>


### [46] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils Krüger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 本文研究了链式思维提示在临床文本理解中的效果，发现其在临床环境中可能降低可靠性，尽管能提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于临床护理中对准确性和透明推理的需求，需要研究链式思维（CoT）提示在临床环境中的有效性。

Method: 本文进行了大规模系统研究，评估了95个先进的LLM在87个现实世界的临床文本任务上的表现，并进行了细粒度分析。

Result: 研究发现，86.3%的模型在CoT设置中表现出性能下降，更强大的模型相对稳健，而较弱的模型则遭受显著下降。

Conclusion: 本文提供了临床推理策略的实证基础，强调了透明和值得信赖的方法的必要性。

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [47] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 本文介绍了ThaiFACTUAL，一种用于减少政治偏见的轻量级框架，并发布了第一个高质量的泰语政治立场数据集。


<details>
  <summary>Details</summary>
Motivation: 在泰国政治环境中，大型语言模型（LLMs）经常表现出系统性偏差，如情感泄露和对实体的偏爱，这影响了公平性和可靠性。

Method: 本文提出了ThaiFACTUAL，一个轻量级、与模型无关的校准框架，通过反事实数据增强和基于推理的监督来解耦情感和立场并减少偏差。

Result: 实验结果表明，ThaiFACTUAL显著减少了虚假相关性，增强了零样本泛化能力，并提高了多个LLMs的公平性。

Conclusion: 本文强调了针对未被充分代表的语言，基于文化背景的去偏技术的重要性。

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [48] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为MotivGraph-SoIQ的新框架，通过结合动机知识图谱和苏格拉底对话来提高大型语言模型的创意生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在加速学术创意生成方面具有巨大潜力，但面临在创意基础上进行定位和减少确认偏见的挑战。

Method: 提出了一种结合动机知识图谱和苏格拉底对话的框架，称为MotivGraph-SoIQ。该框架通过整合Motivational Knowledge Graph（MotivGraph）和Q-Driven Socratic Ideator来提供必要的基础和实际的创意改进步骤。

Result: 在ICLR25论文主题数据集上，MotivGraph-SoIQ在基于LLM的评分、ELO排名和人类评估指标上均表现出明显优势。

Conclusion: MotivGraph-SoIQ在LLM创意生成中表现出明显优势，能够有效提升创意质量，并在多个评估指标上优于现有最先进的方法。

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [49] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种基于不确定性表达的黑盒幻觉检测度量方法，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法需要外部资源或语言模型的内部状态，但受限于外部API的可用性和外部资源的范围，因此需要建立黑盒方法作为有效幻觉检测的基础。

Method: 通过研究语言模型在表达不确定性时的行为，提出了一种基于不确定性表达的黑盒幻觉检测度量方法。

Result: 实验表明，本文提出的度量方法比使用语言模型内部知识的基线方法在预测模型响应事实性方面更具有预测性。

Conclusion: 本文提出了一种基于不确定性表达的高效黑盒幻觉检测度量方法，并通过实验验证了其在预测模型响应事实性方面的优越性。

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [50] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: 本文提出了GraphSearch，一种用于GraphRAG的新代理深度搜索工作流，通过双通道检索策略提高检索效果和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphRAG方法面临两个核心限制：浅层检索无法揭示所有关键证据，以及对预构建的结构图数据利用效率低下，这阻碍了从复杂查询中进行有效推理。

Method: 提出了一种新的代理深度搜索工作流，称为GraphSearch，它具有双通道检索功能，用于GraphRAG。GraphSearch将检索过程组织成一个包含六个模块的模块化框架，支持多轮交互和迭代推理，并采用双通道检索策略，在基于块的文本数据上发布语义查询，在结构图数据上发布关系查询，以充分利用两种模态及其互补优势。

Result: 实验结果表明，GraphSearch在多个多跳RAG基准测试中 consistently 提高了答案准确性和生成质量，优于传统策略。

Conclusion: 实验结果表明，GraphSearch在多个多跳RAG基准测试中 consistently 提高了答案准确性和生成质量，证实了GraphSearch作为推进图检索增强生成的有前途方向。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [51] [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
*Evangelia Zve,Benjamin Icard,Alice Breton,Lila Sainero,Gauvain Bourgne,Jean-Gabriel Ganascia*

Main category: cs.CL

TL;DR: 本文研究了异常值如何作为新兴主题的弱信号，并发现它们在时间推移中演变为连贯的主题。


<details>
  <summary>Details</summary>
Motivation: 以往的异常值常被视为噪声，但本文旨在探索它们作为新兴主题的潜在信号。

Method: 使用最先进的语言模型的向量嵌入和累积聚类方法来跟踪动态新闻语料库中的异常值演变。

Result: 结果表明，异常值在时间和语言上趋于演变为连贯的主题。

Conclusion: 本文表明，异常值可以作为新兴主题的弱信号，并在时间推移中演变为连贯的主题。

Abstract: This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.

</details>


### [52] [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
*Jean Seo,Hyunkyung Lee,Gibaeg Kim,Wooseok Han,Jaehyo Yoo,Seungseop Lim,Kihun Shin,Eunho Yang*

Main category: cs.CL

TL;DR: 本文介绍了TACOS，这是一种用于临床聊天机器人安全的新型分类法，能够整合安全过滤和工具选择，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法（如防护措施和工具调用）在处理临床领域的细微需求时往往不足。

Method: 引入TACOS（临床代理全面安全分类法），将安全过滤和工具选择整合到一个用户意图分类步骤中。

Result: 通过整理TACOS标注数据集并进行广泛的实验，结果证明了专门为临床代理设置的新分类法的价值。

Conclusion: TACOS是一个专门针对临床代理设置的新分类法，具有重要的价值，并揭示了关于训练数据分布和基础模型预训练知识的有用见解。

Abstract: Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

</details>


### [53] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: FRC is a framework that combines LLM semantic priors with fuzzy membership degrees to improve handling of ambiguous texts, showing stable reasoning and better interpretability.


<details>
  <summary>Details</summary>
Motivation: To handle texts with ambiguity, polysemy, or uncertainty, which remain significant challenges in NLP despite the progress of large language models (LLMs).

Method: FRC framework integrates LLM semantic priors with continuous fuzzy membership degrees, creating an explicit interaction between probability-based reasoning and fuzzy membership reasoning.

Result: FRC ensures stable reasoning and facilitates knowledge transfer across different model scales, as shown by theoretical analysis and empirical results on sentiment analysis tasks.

Conclusion: FRC provides a general mechanism for managing subtle and ambiguous expressions with improved interpretability and robustness.

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [54] [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
*Yudong Li,Yufei Sun,Yuhan Yao,Peiru Yang,Wanyue Li,Jiajun Zou,Yongfeng Huang,Linlin Shen*

Main category: cs.CL

TL;DR: 本文介绍了RedNote-Vibe数据集和PLAD框架，用于研究社交媒体上的AIGT动态变化和检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要展示静态的AIGT检测，而社交媒体上的AIGT内容动态变化，需要更长期的数据集和检测方法。

Method: 提出了PsychoLinguistic AIGT Detection Framework (PLAD)，利用心理语言学特征进行AIGT检测。

Result: PLAD在检测性能上表现出色，并揭示了人类和AI生成内容之间的语言特征差异以及这些特征与社交媒体参与度之间的复杂关系。

Conclusion: RedNote-Vibe数据集和PLAD框架为研究社交媒体上的AIGT提供了重要的资源和方法，有助于理解AIGT的动态变化和用户互动模式。

Abstract: The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

</details>


### [55] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: 本文提出了一种标准化的质量标准评估分类法，以提高NLP领域评估结果的可比性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的NLP评估实验在报告相同质量标准名称（如流畅性）的结果时，可能并不评估相同的质量方面，这导致了比较的误导性。缺乏可靠的结论会影响整个领域科学进步。

Method: 通过三个关于NLP中评估的调查，从实际使用的数百个质量标准名称中推导出一组标准化的质量标准名称和定义，并将它们结构化为一个层次结构。

Result: 提出了QCET及其组成部分，并讨论了其三个主要用途：(i) 建立现有评估的可比性，(ii) 指导新评估的设计，(iii) 评估监管合规性。

Conclusion: 本文提出了QCET质量标准评估分类法，以解决NLP领域中评估标准名称和定义不明确的问题。该方法有助于建立现有评估的可比性、指导新评估的设计以及评估监管合规性。

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [56] [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
*Wanli Yang,Fei Sun,Rui Tang,Hongyu Zang,Du Su,Qi Cao,Jingang Wang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文挑战了微调在模型编辑中无效的传统观点，提出通过恢复标准的广度优先管道和局部调优策略，显著提升了微调的效果，并展示了其在大规模模型和大量编辑中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统认为微调在模型编辑中效果不佳，但本文挑战这一观点，认为失败的原因不是微调本身的固有局限性，而是将其适应于编辑任务的顺序性质。

Method: 通过系统分析调优位置，我们得出了LocFT-BF，这是一种基于恢复的微调框架的简单有效的局部编辑方法。

Result: 在多种LLM和数据集上的实验表明，LocFT-BF比最先进的方法表现好很多。值得注意的是，据我们所知，它是第一个能够维持100K次编辑和72B参数模型的，比之前实践高出10倍，而不会牺牲通用能力。

Conclusion: 通过澄清一个长期存在的误解并引入一种原则化的局部调优策略，我们使微调从一个被低估的基线提升为模型编辑的领先方法，为未来的研究奠定了坚实的基础。

Abstract: Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.

</details>


### [57] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: CoSpaDi是一种无需训练的压缩框架，通过使用结构化稀疏因式分解来提高模型精度和效率。


<details>
  <summary>Details</summary>
Motivation: 后训练压缩大型语言模型（LLMs）主要依赖于低秩权重近似，这在计算上是高效的策略，但施加的结构约束是刚性的，可能导致明显的模型精度下降。

Method: CoSpaDi（通过稀疏字典学习进行压缩）是一种无需训练的压缩框架，它用更灵活的结构化稀疏因式分解代替低秩分解，其中每个权重矩阵由密集字典和列稀疏系数矩阵表示。

Result: 我们在多个Llama和Qwen模型上评估了CoSpaDi，在20-50%的压缩率下，展示了在准确性和困惑度方面优于最先进的数据感知低秩方法的一致优势。

Conclusion: 我们的结果确立了结构化稀疏字典学习作为高效LLM部署的传统低秩方法的有力替代方案。

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [58] [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
*Justin Vasselli,Eunike Andriani Kardinata,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出了Dialogue Act Script (DAS)，一种用于从抽象意图表示中编码、本地化和生成多语言对话的结构化框架。DAS通过使用结构化的对话行为表示，支持跨语言的灵活本地化，减轻翻译症，使对话更加流畅自然。人类评估显示，DAS生成的对话在文化相关性、连贯性和情境适当性方面优于机器和人工翻译。


<details>
  <summary>Details</summary>
Motivation: Non-English dialogue datasets are scarce, and models are often trained or evaluated on translations of English-language dialogues, an approach which can introduce artifacts that reduce their naturalness and cultural appropriateness.

Method: Dialogue Act Script (DAS), a structured framework for encoding, localizing, and generating multilingual dialogues from abstract intent representations.

Result: Human evaluations across Italian, German, and Chinese show that DAS-generated dialogues consistently outperform those produced by both machine and human translators on measures of cultural relevance, coherence, and situational appropriateness.

Conclusion: DAS-generated dialogues consistently outperform those produced by both machine and human translators on measures of cultural relevance, coherence, and situational appropriateness.

Abstract: Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

</details>


### [59] [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
*Shaoning Sun,Jiachen Yu,Zongqi Wang,Xuewei Yang,Tianle Gu,Yujiu Yang*

Main category: cs.CL

TL;DR: This paper introduces the Solve-to-Judge (S2J) approach to address the solve-to-judge gap in generative reward models (GRMs), demonstrating improved performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the solve-to-judge gap, where GRMs struggle to make correct judgments on some queries despite being capable of solving them.

Method: Solve-to-Judge (S2J) approach, which simultaneously leverages both the solving and judging capabilities on a single GRM's output for supervision, explicitly linking the GRM's problem-solving and evaluation abilities during model optimization.

Result: S2J effectively reduces the solve-to-judge gap by 16.2%, enhances the model's judgment performance by 5.8%, and achieves SOTA performance among GRMs built on the same base model with a smaller training dataset.

Conclusion: S2J effectively reduces the solve-to-judge gap by 16.2%, thereby enhancing the model's judgment performance by 5.8%. Moreover, S2J achieves state-of-the-art (SOTA) performance among GRMs built on the same base model while utilizing a significantly smaller training dataset.

Abstract: With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

</details>


### [60] [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
*Primakov Chungkham,V Venktesh,Vinay Setty,Avishek Anand*

Main category: cs.CL

TL;DR: 本文研究了如何利用测试时计算（TTS）来提高大型语言模型在事实核查复杂数值声明任务中的性能，并通过引入自适应机制提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在处理需要组合和数值推理的现实世界声明时仍然存在不足，例如无法理解数值方面的细微差别，以及容易出现推理漂移问题。因此，本文旨在探索一种更有效的事实核查方法。

Method: 本文系统地探索了在事实核查复杂数值声明任务中使用测试时计算（TTS）的方法，并训练了一个验证器模型（VERIFIERFC）来导航可能的推理路径并选择可能导致正确结论的路径。同时，为了提高TTS的计算效率，引入了一种基于声明复杂度的自适应机制。

Result: TTS能够有效缓解推理漂移问题，从而显著提高验证数值声明的性能。此外，自适应机制使得TTS的计算效率提高了1.8倍，同时相比单次验证方法提升了18.8%的性能。

Conclusion: TTS可以帮助缓解推理漂移问题，从而显著提高验证数值声明的性能。此外，通过引入自适应机制，可以在保持性能提升的同时提高计算效率。

Abstract: Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC

</details>


### [61] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: Uni-LAP is a universal framework for legal article prediction that combines the strengths of supervised classification models and large language models, achieving superior performance across multiple jurisdictions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Legal Article Prediction (LAP) face significant challenges in addressing the complexities of LAP, including limitations in capturing intricate fact patterns and suboptimal performance in predictive scenarios due to the abstract and ID-based nature of legal articles. Additionally, most approaches are tailored to specific countries and lack broader applicability.

Method: Uni-LAP integrates the strengths of supervised classification models (SCMs) and large language models (LLMs) through tight collaboration. The SCM is enhanced with a novel Top-K loss function to generate accurate candidate articles, while the LLM employs syllogism-inspired reasoning to refine the final predictions.

Result: Empirical results demonstrate that Uni-LAP consistently outperforms existing baselines on datasets from multiple jurisdictions, showcasing its effectiveness and generalizability.

Conclusion: Uni-LAP is an effective and generalizable framework for legal article prediction, consistently outperforming existing baselines on datasets from multiple jurisdictions.

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [62] [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文分析了多语言视觉-语言模型的跨语言能力和文化意识之间的权衡，并指出训练方法和评估目标之间的差距。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨多语言视觉-语言模型在语言中立性和文化意识之间的权衡问题。

Method: 本文回顾了31个模型和21个基准测试，分析了多语言视觉-语言模型的特性。

Result: 当前的训练方法倾向于通过对比学习实现语言中立性，而文化意识依赖于多样化数据。三分之二的评估基准使用基于翻译的方法，最近的研究开始结合文化相关内容。

Conclusion: 本文发现跨语言能力存在差异，且训练目标与评估目标之间存在差距。

Abstract: This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.

</details>


### [63] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,Sašo Džeroski,Barbara Koroušić Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: 本文介绍了FoodSEM，一个针对食品相关本体的命名实体链接任务的先进微调开源大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 食品NEL任务无法通过最先进的通用大型语言模型或定制领域特定模型/系统准确解决。

Method: 本文通过指令-响应（IR）场景，将文本中提到的食品相关实体链接到多个本体，包括FoodOn、SNOMED-CT和Hansard分类法。

Result: FoodSEM模型在某些本体和数据集上达到了98%的F1分数，并且在与零样本、单样本和少样本LLM提示基线的比较分析中表现出色。

Conclusion: 通过公开FoodSEM及其相关资源，本文的主要贡献包括：(1) 发布了一个用于LLM微调/评估的适合IR格式的食品标注语料库；(2) 发布了一个强大的模型以推进食品领域文本的语义理解；(3) 为未来的食品NEL基准测试提供了强有力的基线。

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [64] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Reasoning Capsule的框架，旨在结合显式CoT的透明度和隐式推理的效率。通过压缩高层次计划为少量潜在标记，同时保持执行步骤轻量或显式，该框架在效率、准确性和可解释性之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT) 提示有助于大型语言模型（LLMs）处理复杂的推理，但其冗长性增加了延迟和内存使用，并可能在长链中传播早期错误。因此，需要一种方法来提高效率并减少错误传播。

Method: 我们提出了Reasoning Capsule (R-Capsule) 框架，该框架结合了隐式推理的效率和显式CoT的透明度。核心思想是将高层次计划压缩成一组学习的潜在标记（一个Reasoning Capsule），同时保持执行步骤轻量或显式。这种方法受到信息瓶颈（IB）原理的启发，鼓励胶囊尽可能小但足够完成任务。

Result: 我们的框架在保持或提高复杂基准上的准确性的同时，减少了推理的可见标记足迹。

Conclusion: 我们的框架在效率、准确性和可解释性之间取得了平衡，从而减少了推理的可见标记足迹，同时在复杂基准上保持或提高了准确性。

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [65] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: GTO通过解决草稿策略不对齐问题，提高了大型语言模型推理的速度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的训练目标仅优化单一贪心草稿路径，而解码遵循树策略，重新排序并验证多个分支，这种草稿策略不对齐限制了可达到的速度提升。

Method: GTO通过两个组件来对齐训练与解码时的树策略：(i) 草稿树奖励，一种无需采样的目标，等于在目标模型下草稿树的预期接受长度，直接衡量解码性能；(ii) 基于组的草稿策略训练，一种稳定的优化方案，通过对比当前和冻结参考草稿模型的树，形成去偏组标准化优势，并沿最长接受序列应用PPO风格的替代函数进行稳健更新。

Result: GTO在对话（MT-Bench）、代码（HumanEval）和数学（GSM8K）以及多个LLM上，将接受长度提高了7.4%，并在先前最先进的EAGLE-3基础上额外提升了7.7%的速度。

Conclusion: GTO通过解决草稿策略不对齐问题，为高效的大型语言模型推理提供了一个实用且通用的解决方案。

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [66] [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
*Raia Abu Ahmad,Rana Abdulla,Tilahun Abedissa Taffa,Soeren Auer,Hamed Babaei Giglou,Ekaterina Borisova,Zongxiong Chen,Stefan Dietze,Jennifer DSouza,Mayra Elwes,Genet-Asefa Gesese,Shufan Jiang,Ekaterina Kutafina,Philipp Mayr,Georg Rehm,Sameer Sadruddin,Sonja Schimmler,Daniel Schneider,Kanishka Silva,Sharmila Upadhyaya,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本文介绍了NFDI4DS联盟下十二个共享任务的更新概述，这些任务涵盖了学术文档处理中的各种挑战，并为更广泛的科研社区提供了开放获取的数据集、模型和工具。


<details>
  <summary>Details</summary>
Motivation: 共享任务是推动研究的重要工具，通过社区标准化评估来促进研究进展。本文旨在提供NFDI4DS联盟下共享任务的更新概述，展示它们在促进科研数据基础设施方面的贡献。

Method: 本文通过介绍NFDI4DS联盟下的十二个共享任务，分析了它们在促进FAIR和透明可重复研究实践方面的作用。

Result: 本文展示了NFDI4DS联盟下十二个共享任务的多样性和影响力，以及它们对科研社区的贡献。

Conclusion: 本文介绍了NFDI4DS联盟下开发和举办的十二个共享任务的更新概述，这些任务涵盖了学术文档处理中的各种挑战，并为更广泛的科研社区提供了开放获取的数据集、模型和工具。

Abstract: Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.

</details>


### [67] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 本文提出了一种名为MACC的框架，通过多轮优化逐步压缩Chain-of-Thought（CoT），以减少延迟并保持性能。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT)推理虽然提高了复杂任务的性能，但引入了显著的推理延迟。需要一种方法来减少CoT的长度并降低延迟，同时保持性能。

Method: 我们提出了Multiround Adaptive Chain-of-Thought Compression (MACC)，利用了token弹性现象，通过多轮优化逐步压缩CoT。

Result: 我们的方法在最先进的基线基础上平均提高了5.6%的准确性，同时将CoT长度平均减少了47个token，并显著降低了延迟。

Conclusion: 我们的方法证明了CoT压缩既有效又可预测，能够在不进行重复微调的情况下实现高效的模型选择和预测。

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [68] [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
*Sai Teja Lekkala,Yadagiri Annepaka,Arun Kumar Challa,Samatha Reddy Machireddy,Partha Pakray,Chukhu Chunka*

Main category: cs.CL

TL;DR: 本文研究了机器生成文本检测的不同场景，并引入了一个新的数据集来更有效地解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨机器生成文本检测的问题，特别是在人类工作的真实性和创造力及创新能力的保持方面。

Method: 本文研究了跨多种场景的机器生成文本检测，包括文档级二分类和多类分类或生成器归属、句子级分割以区分人类-AI协作文本，以及旨在降低机器生成文本可检测性的对抗攻击。

Result: 本文引入了一个名为BMAS English的新数据集，用于二分类人类和机器文本，多类分类不仅能够识别机器生成文本，还能尝试确定其生成器，并处理对抗攻击，以及用于预测人类和机器生成文本边界句子级分割。

Conclusion: 本文认为，该研究将更有意义地解决机器生成文本检测（MGTD）的先前工作。

Abstract: Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.

</details>


### [69] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip Jukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: CompAs是一种元学习框架，能够将上下文转换为适配器参数，并具有组合结构，从而实现更高效的模型部署。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理多个信息块时存在不足，因此我们需要一种能够整合多个信息块的解决方案。

Method: 我们引入了CompAs，这是一个元学习框架，它通过组合结构将上下文转换为适配器参数。

Result: 实验结果表明，CompAs在多种选择题和提取式问答任务中表现优于ICL和先前的生成器方法，尤其是在扩展到更多输入时。

Conclusion: 我们的工作确立了可组合的适配器生成作为扩展大语言模型部署的一种实用且高效的方法。

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [70] [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
*Nicolas Boizard,Hippolyte Gisserot-Boukhlef,Kevin El-Haddad,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 本研究通过大规模实验发现，推理模型在模型规模扩大时表现出色，尤其在需要推理的任务中优于指令微调模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在推理能力方面取得了成功，但推理在哪些任务和模型规模下有效，以及其训练和推理成本仍缺乏探索。

Method: 我们依赖于一个合成数据蒸馏框架进行大规模监督研究，比较了不同规模的指令微调（IFT）和推理模型，在各种以数学为中心和通用任务上评估了多项选择和开放性格式。

Result: 推理模型通常能显著提升模型性能，有时甚至超过更大的IFT系统。虽然IFT在训练和推理成本上仍然是帕累托最优的，但在需要推理的任务和开放性任务中，推理模型的性能限制被克服。

Conclusion: 推理模型在模型规模扩大时变得越来越有价值，能够在需要推理的任务和开放性任务中超越IFT系统的性能限制。

Abstract: Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.

</details>


### [71] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: This paper argues that large language models' outputs are meaningless because they lack the necessary intentions, even though they may seem meaningful and useful.


<details>
  <summary>Details</summary>
Motivation: To argue that LLMs' outputs are meaningless despite their apparent meaningfulness and utility.

Method: Offering a simple argument based on two key premises: (a) certain kinds of intentions are needed for LLMs' outputs to have literal meanings, and (b) LLMs cannot plausibly have the right kinds of intentions.

Result: The paper defends the argument against various responses, such as semantic externalist and internalist arguments.

Conclusion: LLMs' outputs are meaningless.

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [72] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: RTP is a novel framework that uses LLMs to build an interpretable taxonomy through a binary tree of natural language questions, offering better interpretability and utility in classification tasks compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the critical interpretability gap in unsupervised analysis of text corpora, especially in data-scarce domains where traditional topic models struggle with manual interpretation and semantic coherence.

Method: Recursive Thematic Partitioning (RTP), a novel framework that leverages Large Language Models (LLMs) to interactively build a binary tree. Each node in the tree is a natural language question that semantically partitions the data.

Result: RTP's question-driven hierarchy is more interpretable than keyword-based topics from a strong baseline like BERTopic. The clusters from RTP serve as powerful features in downstream classification tasks, particularly when the data's underlying themes correlate with the task labels.

Conclusion: RTP introduces a new paradigm for data exploration, shifting the focus from statistical pattern discovery to knowledge-driven thematic analysis. Additionally, the thematic paths from the RTP tree can serve as structured, controllable prompts for generative models, transforming the analytical framework into a powerful tool for synthesis.

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [73] [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
*Yuhan Song,Linhao Zhang,Chuhan Wu,Aiwei Liu,Wei Jia,Houfeng Wang,Xiao Zhou*

Main category: cs.CL

TL;DR: 本文提出 StableToken，一种通过多分支架构和位级投票机制提高语音标记稳定性的方法，在多种噪声条件下表现出色，并提升了下游任务的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的语义语音标记器在面对无关语义的声学扰动时表现出脆弱性，即使在高信噪比下，它们的输出标记序列也可能发生剧烈变化，增加了下游大语言模型的学习负担。

Method: StableToken 通过一种共识驱动的机制实现稳定性，其多分支架构并行处理音频，并通过强大的位级投票机制合并表示以形成单一稳定的标记序列。

Result: StableToken 在各种噪声条件下显著降低了单位编辑距离（UED），并在多个下游任务中显著提高了 SpeechLLMs 的鲁棒性。

Conclusion: StableToken 提供了一种新的语音标记方法，显著提高了语音标记的稳定性，并直接提升了下游任务的鲁棒性。

Abstract: Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.

</details>


### [74] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: 本文介绍了复合推理（CR），一种新的推理方法，使大型语言模型能够动态探索和结合多种推理风格，从而提高问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尽管具有显著的能力，但依赖于单一的主导推理范式，这阻碍了它们在需要多种认知策略的复杂问题上的表现。

Method: 引入了复合推理（CR），这是一种新的推理方法，使LLMs能够动态探索和结合多种推理风格，如演绎、归纳和溯因。

Result: 在科学和医学问答基准测试中，该方法优于现有的基线方法，如链式思维（CoT），并且超过了DeepSeek-R1风格推理（SR）的能力，同时表现出更高的样本效率和足够的标记使用。

Conclusion: 通过培养内部推理风格的多样性，LLMs获得了更强大、适应性和效率的问题解决能力。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [75] [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
*Jaehoon Kim,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.

</details>


### [76] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 本文提出了FeatBench，一个专注于功能实现的新基准，用于评估vibe coding能力。该基准具有纯自然语言提示、严格且不断进化的数据收集过程、综合测试用例和多样化的应用领域等特点。评估结果显示，功能实现是一个重大挑战，最高成功率为29.94%。


<details>
  <summary>Details</summary>
Motivation: 现有的代码生成评估基准未能充分评估代理的vibe coding能力。现有的基准存在偏差，因为它们要么需要代码级规范，要么专注于问题解决，忽视了vibe coding范式中的功能实现这一关键场景。

Method: 我们提出了FeatBench，这是一个专注于功能实现的新基准。我们的基准具有几个关键特点：1. 纯自然语言提示。任务输入仅由抽象的自然语言描述组成，不包含任何代码或结构提示。2. 严格且不断进化的数据收集过程。FeatBench基于多级过滤管道来确保质量，并有一个完全自动化的管道来进化基准，减轻数据污染。3. 综合测试用例。每个任务包括Fail-to-Pass (F2P) 和 Pass-to-Pass (P2P) 测试以验证正确性并防止回归。4. 多样化的应用领域。基准包括来自不同领域的存储库，以确保它反映现实场景。

Result: 我们评估了两种最先进的代理框架和四种领先的LLM在FeatBench上的表现。我们的评估揭示了在vibe coding范式中的功能实现是一个重大挑战，最高成功率为29.94%。我们的分析还揭示了一种“激进的实现”策略，这种策略反而导致了关键失败和优越的软件设计。

Conclusion: 我们的评估揭示了在vibe coding范式中的功能实现是一个重大挑战，最高成功率为29.94%。我们的分析还揭示了一种“激进的实现”策略，这种策略反而导致了关键失败和优越的软件设计。我们发布了FeatBench、我们的自动化收集管道和所有实验结果，以促进进一步的社区研究。

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [77] [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
*Yuan Ge,Saihan Chen,Jingqi Xiao,Xiaoqian Liu,Tong Xiao,Yan Xiang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: This paper introduces FLEXI, the first benchmark for full-duplex LLM-human spoken interaction, highlighting the challenges in real-time dialogue systems and suggesting next token-pair prediction as a promising solution.


<details>
  <summary>Details</summary>
Motivation: Benchmarking and modeling full-duplex speech-to-speech large language models (LLMs) remains a fundamental challenge.

Method: Introduce FLEXI, the first benchmark for full-duplex LLM-human spoken interaction that explicitly incorporates model interruption in emergency scenarios.

Result: FLEXI systematically evaluates the latency, quality, and conversational effectiveness of real-time dialogue through six diverse human-LLM interaction scenarios, revealing significant gaps between open source and commercial models in emergency awareness, turn terminating, and interaction latency.

Conclusion: Next token-pair prediction offers a promising path toward achieving truly seamless and human-like full-duplex interaction.

Abstract: Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.

</details>


### [78] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文提出了一种从法律合规角度解决LLM安全问题的方法，即安全合规。通过将欧盟人工智能法案和GDPR等法律框架作为安全标准，构建了一个新的安全合规基准，并利用GRPO对Qwen3-8B进行对齐，以创建一个有效的安全推理器。实验结果表明，该安全推理器在新基准上表现出色，分别提升了10.45%和11.85%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM安全方法依赖于临时分类法，缺乏严格的系统性保护，无法确保现代LLM系统的复杂行为的安全性。因此，需要一种更系统、更严谨的方法来确保LLM的安全性。

Method: 本文提出了一种从法律合规角度解决LLM安全问题的方法，即安全合规。通过将欧盟人工智能法案和GDPR等法律框架作为安全标准，构建了一个新的安全合规基准，并利用GRPO对Qwen3-8B进行对齐，以创建一个有效的安全推理器。

Result: 实验结果表明，该安全推理器在新基准上表现出色，分别提升了10.45%和11.85%的性能。

Conclusion: 本文提出了一种从法律合规角度解决LLM安全问题的方法，即安全合规。通过将欧盟人工智能法案和GDPR等法律框架作为安全标准，构建了一个新的安全合规基准，并利用GRPO对Qwen3-8B进行对齐，以创建一个有效的安全推理器。实验结果表明，该安全推理器在新基准上表现出色，分别提升了10.45%和11.85%的性能。

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [79] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: 本文提出SSKG-LLM模型，通过整合知识图谱的结构和语义信息，提升大型语言模型的事实推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理幻觉问题时主要依赖知识图谱，但通常仅提取语义信息，忽略了知识图谱的结构信息。同时，知识图谱编码器和语言模型文本嵌入之间的空间差距也阻碍了结构化知识的有效整合。

Method: 提出了一种新的模型架构SSKG-LLM，包含知识图谱检索模块、知识图谱编码模块和知识图谱适配模块，以充分利用知识图谱的结构和语义信息。

Result: 通过广泛的实验和详细分析，验证了将知识图谱的结构信息融入语言模型可以显著提升其事实推理能力。

Conclusion: SSKG-LLM能够有效整合知识图谱的结构和语义信息，从而提升大型语言模型的事实推理能力。

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [80] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: 本研究首次系统地研究了仇恨言论检测中可解释性和公平性之间的关系，发现输入型解释可以有效检测有偏预测并减少训练中的偏差，但在选择公平模型方面不可靠。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理（NLP）模型常常复制或放大训练数据中的社会偏见，引发了关于公平性的担忧。同时，它们的黑箱性质使得用户难以识别有偏预测，开发者也难以有效缓解这些问题。尽管一些研究认为基于输入的解释可以帮助检测和减轻偏见，但其他研究质疑其在确保公平性方面的可靠性。现有的关于公平NLP中可解释性的研究主要是定性的，缺乏大规模的定量分析。

Method: 对仇恨言论检测中的可解释性和公平性之间的关系进行了系统研究，重点关注编码器和解码器仅模型的三个关键维度：(1) 识别有偏预测，(2) 选择公平模型，(3) 在模型训练过程中减轻偏差。

Result: 我们的研究结果表明，输入型解释可以有效检测有偏预测，并作为监督在训练期间减少偏差，但它们在候选模型中选择公平模型时不可靠。

Conclusion: 输入型解释可以有效检测有偏预测并在训练期间减少偏差，但它们在选择公平模型方面不可靠。

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [81] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,Björn Gehrke*

Main category: cs.CL

TL;DR: 本文对微调的LLM在自然语言到一阶逻辑翻译任务中的表现进行了系统评估，发现T5模型在使用谓词列表时表现优于其他模型，并且能够泛化到未见过的逻辑论证。


<details>
  <summary>Details</summary>
Motivation: 自动化自然语言到一阶逻辑（FOL）的翻译对于知识表示和形式方法至关重要，但仍具有挑战性。

Method: 对微调的LLM进行系统评估，比较架构（编码器-解码器与仅解码器）和训练策略。使用MALLS和Willow数据集，探索词汇扩展、谓词条件和多语言训练等技术，并引入了精确匹配、逻辑等价和谓词对齐的指标。

Result: 微调的Flan-T5-XXL在使用谓词列表的情况下达到了70%的准确率，超过了GPT-4o以及具有思维链推理能力的DeepSeek-R1-0528模型，甚至超越了符号系统如ccg2lambda。关键发现包括：(1) 谓词可用性可提高性能15-20%，(2) T5模型超越更大的仅解码器LLM，(3) 模型可以在没有特定训练的情况下泛化到未见过的逻辑论证（FOLIO数据集）。

Conclusion: 虽然结构化逻辑翻译证明是稳健的，但谓词提取成为主要瓶颈。

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [82] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: 研究探讨了 transformer 模型在推断传递关系任务中的能力，发现它们在类似网格的有向图上表现良好，但在非网格图和大量不连通组件的情况下表现较差。模型规模的增加有助于提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究 transformers 在推断传递关系方面的能力对于确保基于 transformer 的大型语言模型的回答事实正确性至关重要。

Method: 通过生成不同大小的有向图来训练不同规模的 transformer 模型，并评估它们推断传递关系的能力。

Result: transformers 能够在类似网格的有向图上学习连接性，而高维网格图比低维网格图更具挑战性。模型规模的增加有助于提高在网格图上的泛化能力，但非网格图和大量不连通组件会使得任务变得困难。

Conclusion: 研究发现，transformers 能够在类似网格的有向图上学习连接性，但当图不是网格图且包含许多不连通组件时，它们在学习连接性任务上遇到困难。此外，模型规模的增加有助于提高在网格图上的泛化能力。

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [83] [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
*Sophie Spliethoff,Sanne Hoeken,Silke Schwandt,Sina Zarrieß,Özge Alaçam*

Main category: cs.CL

TL;DR: 本文将NLP技术应用于历史研究，介绍了InviTE语料库，并比较了不同模型在攻击性语言检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 我们将自然语言处理技术应用于历史研究，特别是研究都铎王朝英格兰新教改革时期的宗教攻击性语言。

Method: 我们提出了一种工作流程，从原始数据经过预处理和数据选择，到迭代标注过程，并引入了InviTE语料库。

Result: 我们评估并比较了微调的BERT模型和零样本提示指令调整的大语言模型的性能，结果表明基于历史数据预训练并微调的模型表现更优。

Conclusion: 我们的研究展示了基于历史数据预训练并微调的模型在攻击性语言检测中的优越性。

Abstract: In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.

</details>


### [84] [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
*Christoph Unger,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文探讨了贝叶斯方法在关联理论语用学中的应用，特别是隐含意义的传达。


<details>
  <summary>Details</summary>
Motivation: 近年来，贝叶斯概率论在认知科学中的应用以及新的计算工具的发展，使得语用学和语义学出现了'贝叶斯转向'。本文旨在探索贝叶斯方法是否可以应用于关联理论语用学。

Method: 本文采用了理性说话者行为理论的框架，并将其扩展到关联理论语用学中，以研究对话含义的传达。

Result: 本文表明，贝叶斯方法可以用于研究关联理论语用学中的隐含意义传达，并提供了一个可能的框架。

Conclusion: 本文探讨了如何将贝叶斯方法应用于关联理论语用学，以研究隐含意义的传达。

Abstract: Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

</details>


### [85] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: 本文介绍了CHRONOBERG，一个基于时间结构的英语书籍文本语料库，用于研究语言变化和时间泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的语料库缺乏长期的时间结构，这可能限制了大型语言模型（LLM）对语言语义和规范演变进行上下文分析的能力。

Method: 通过从Project Gutenberg中收集英语书籍文本并添加多种时间注释来构建CHRONOBERG。使用时间敏感的Valence-Arousal-Dominance (VAD)分析来量化词汇语义变化，并构建历史校准的情感词典以支持时间基础解释。

Result: 语言模型在CHRONOBERG上进行顺序训练时难以编码意义的历时变化，表明需要更注重时间的工具和方法。

Conclusion: CHRONOBERG是一个可用于研究语言变化和时间泛化的可扩展资源，同时也强调了需要具有时间意识的训练和评估流程。

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [86] [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
*Max Malyi,Jonathan Shek,Andre Biscaya*

Main category: cs.CL

TL;DR: 本文探讨了如何利用大型语言模型对风力涡轮机维护日志中的非结构化文本进行深度语义分析，以提高风能行业的操作智能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常只停留在分类上，将文本分为预定义的标签，而未充分利用现代大型语言模型进行更复杂的推理任务。

Method: 本文引入了一个探索性框架，使用大型语言模型（LLMs）进行深度语义分析，超越分类并执行四种分析工作流：故障模式识别、因果链推断、比较站点分析和数据质量审计。

Result: 结果表明，大型语言模型可以作为强大的“可靠性副驾驶”，超越标记，综合文本信息并生成可操作的专家级假设。

Conclusion: 本文贡献了一种新颖且可重复的方法，利用大型语言模型作为推理工具，为风能行业增强操作智能提供了新途径，通过解锁以前被结构化数据掩盖的见解。

Abstract: A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.

</details>


### [87] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 本文分析了OLMO2的预训练和后训练语料库，发现左倾文件在数据集中占主导地位，并且训练数据中的政治内容与模型的政治偏见密切相关。


<details>
  <summary>Details</summary>
Motivation: 为了填补这一空白，我们在这篇论文中分析了OLMO2的预训练和后训练语料库，这是最大且完全开源的模型，同时发布了其完整数据集。

Method: 我们从这些语料库中抽取了大量随机样本，自动标注了文档的政治倾向，并分析了它们的来源领域和内容。然后评估了训练数据中的政治内容与模型在特定政策问题上的立场之间的相关性。

Result: 左倾文件在各个数据集中占主导地位，预训练语料库包含比后训练数据更大量的政治参与内容。我们还发现，左倾和右倾文件通过不同的价值观和合法性来源来阐述相似的主题。最后，训练数据中的主导立场与模型在政策问题上的政治偏见有很强的相关性。

Conclusion: 这些发现强调了将政治内容分析整合到未来数据整理流程中以及对过滤策略进行深入文档记录的必要性。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [88] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 研究发现当前的视觉语言模型在处理图表时依赖快捷方式，这暴露了它们在真正理解复杂视觉输入方面的局限性，并强调了需要更稳健的评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在图表相关基准测试中表现良好，但它们对知识、推理或模态快捷方式的依赖引发了对其是否真正理解和推理图表的担忧。

Method: 研究人员引入了Chimera，这是一个包含7500个高质量图表的综合性测试套件，每个图表都带有其符号内容的语义三元组以及多级问题，用于评估图表理解的四个基本方面。

Result: 评估15个开源视觉语言模型后发现，它们看似强大的表现主要来自于快捷方式行为，其中Clever-Hans快捷方式贡献最大。

Conclusion: 研究发现当前的视觉语言模型在处理图表时存在依赖快捷方式的问题，这暴露了它们在真正理解复杂视觉输入方面的局限性，并强调了需要更稳健的评估协议。

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [89] [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
*Maor Juliet Lavi,Tova Milo,Mor Geva*

Main category: cs.CL

TL;DR: 本文研究了无法回答性检测问题，提出了一种方法来识别模型激活空间中的方向，以检测无法回答的问题，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）经常对问题给出自信的回答，即使它们缺乏必要的信息，导致产生幻觉答案。因此，我们需要一种方法来检测问题是否可以被回答。

Method: 我们提出了一种简单的方法，通过在推理过程中应用激活添加并测量其对模型回避行为的影响，来识别模型激活空间中的一个方向，该方向捕捉了无法回答性。

Result: 实验表明，将隐藏激活投影到这个方向上会产生一个可靠的（不可回答性）分类分数。我们的方法在两个开放权重的大规模语言模型和四个提取式问答基准测试中表现出色。

Conclusion: 我们的方法在检测无法回答的问题方面有效，并且在不同数据集上表现更好。此外，这些方向不仅适用于提取式问答，还适用于其他原因导致的无法回答的情况。

Abstract: Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.

</details>


### [90] [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
*Antreas Ioannou,Andreas Shiamishis,Nora Hollenstein,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: 本研究评估了LLaMA和Gemini在多语言法律和非法律基准上的表现，并通过字符和单词级扰动评估了它们在法律任务中的对抗鲁棒性。我们发现法律任务对LLMs构成重大挑战，且英语并不总是带来更高的准确性。此外，我们发现语言的表现与其语法相似性与英语之间存在相关性。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）主导的时代，理解它们的能力和局限性，特别是在高风险领域如法律中，至关重要。虽然LLMs如Meta的LLaMA、OpenAI的ChatGPT、Google的Gemini、DeepSeek和其他新兴模型越来越多地被集成到法律工作流程中，但它们在多语言、司法管辖区多样化和对抗性情境中的表现仍缺乏充分探索。

Method: 我们评估了LLaMA和Gemini在多语言法律和非法律基准上的表现，并通过字符和单词级扰动评估了它们在法律任务中的对抗鲁棒性。我们使用了LLM-as-a-Judge方法进行人类对齐评估，并提出了一个开源、模块化的评估管道，以支持任何组合的LLM和数据集的多语言、任务多样化的基准测试。

Result: 我们的发现确认了法律任务对LLMs构成重大挑战，例如LEXam法律推理基准的准确性通常低于50%，而一般任务如XNLI的准确性超过70%。此外，虽然英语通常产生更稳定的结果，但并不总是导致更高的准确性。提示敏感性和对抗性脆弱性也存在于各种语言中。最后，我们发现一种语言的表现与其语法相似性与英语之间的相关性。我们还观察到LLaMA比Gemini弱，后者在相同任务上平均有约24个百分点的优势。

Conclusion: 我们的研究结果表明，法律任务对LLM构成了重大挑战，其准确性通常低于50%。尽管新出现的LLM有所改进，但在部署它们用于关键的多语言法律应用时仍然存在挑战。

Abstract: In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

</details>


### [91] [NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use](https://arxiv.org/abs/2509.22479)
*Yuqing Zhang,Ecesu Ürker,Tessa Verhoef,Gemma Boleda,Arianna Bisazza*

Main category: cs.CL

TL;DR: 本文介绍了NeLLCom-Lex，一个用于模拟语义变化的神经代理框架，通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的观察方法无法揭示因果机制，而人类实验由于涉及长时间的历时过程而难以应用于语义变化研究。

Method: 通过将代理置于真实的词汇系统中，并系统地操纵它们的交际需求，使用神经代理框架模拟语义变化。

Result: 实验表明，训练为'说'现有语言的神经代理可以重现人类在颜色命名中的模式。

Conclusion: NeLLCom-Lex 可以用于阐明语义变化的机制。

Abstract: Lexical semantic change has primarily been investigated with observational
and experimental methods; however, observational methods (corpus analysis,
distributional semantic modeling) cannot get at causal mechanisms, and
experimental paradigms with humans are hard to apply to semantic change due to
the extended diachronic processes involved. This work introduces NeLLCom-Lex, a
neural-agent framework designed to simulate semantic change by first grounding
agents in a real lexical system (e.g. English) and then systematically
manipulating their communicative needs. Using a well-established color naming
task, we simulate the evolution of a lexical system within a single generation,
and study which factors lead agents to: (i) develop human-like naming behavior
and lexicons, and (ii) change their behavior and lexicons according to their
communicative needs. Our experiments with different supervised and
reinforcement learning pipelines show that neural agents trained to 'speak' an
existing language can reproduce human-like patterns in color naming to a
remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the
mechanisms of semantic change.

</details>


### [92] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文研究了LLM生成解决方案的差异性，并提出了基于解决方案分歧的新指标，实验表明该指标能有效提升LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 大多数最近的工作通过监督微调（SFT）或从任务反馈中进行强化学习（RL）来提高LLM的性能。本文研究了一个新的视角：LLM对于单一问题生成的解决方案的差异性。

Method: 研究了LLM对于单一问题生成的解决方案的差异性，并提出了基于解决方案分歧的新指标来支持SFT和RL策略。

Result: 在三个代表性问题领域上测试了这一想法，发现使用解决方案分歧可以持续提高成功率。

Conclusion: 这些结果表明，解决方案分歧是一个简单但有效的工具，可以促进LLM的训练和评估。

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [93] [JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA](https://arxiv.org/abs/2509.22490)
*Hossain Shaikh Saadi,Minh Duc Bui,Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本论文介绍了针对WMT25共享任务的JGU Mainz提交，专注于乌克兰语、上索布语和下索布语的机器翻译和问答任务。通过联合微调Qwen2.5-3B-Instruct模型，并结合额外的数据和集成方法，实验结果表明我们的模型在两个任务上都优于基线。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在参与WMT25共享任务，针对资源有限的LLM在斯拉夫语言上的机器翻译和问答任务进行研究。

Method: 我们对Qwen2.5-3B-Instruct模型进行了联合微调，以进行翻译和问答任务，并使用了参数高效的微调方法。此外，我们还集成了额外的翻译和多项选择问答数据，并在Upper Sorbian和Lower Sorbian的问答任务中应用了集成方法。

Result: 实验表明，我们的模型在两个任务上都优于基线。

Conclusion: 我们的模型在两个任务上都优于基线。

Abstract: This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.

</details>


### [94] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的方法，将大型语言模型表示为提示语义任务空间中的线性算子，实现了高度可解释性和出色的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性和重新训练成本方面存在限制，并且生成的表示难以解释。因此，需要一种更高效、可解释的方法来表示大型语言模型。

Method: 本文提出了一种无需训练的方法，利用几何属性的闭式计算，将大型语言模型表示为提示语义任务空间中的线性算子。

Result: 本文方法在成功预测和模型选择任务中取得了与现有方法相当或更好的结果，并在样本外场景中表现出色。

Conclusion: 本文提出了一种高效、无需训练的方法，将大型语言模型表示为提示语义任务空间中的线性算子，从而提供了模型应用的高度可解释表示。该方法具有出色的可扩展性和实时适应性，适用于不断扩展的模型库。

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [95] [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了自适应多分支引导（AMBS）框架，用于统一和高效的多目标对齐，解决了现有方法中的灾难性遗忘和推理碎片化问题，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化单一对齐目标时可能会覆盖其他目标的学习表示，导致灾难性遗忘。而简单的多分支设计可能造成推理碎片化，使不同HHH目标的输出不一致。

Method: 提出了一种两阶段的1-to-N框架，称为自适应多分支引导（AMBS）。在第一阶段，计算Transformer层的后注意力隐藏状态以形成共享表示。在第二阶段，该表示被克隆到并行分支中，并通过策略-参考机制进行引导，实现目标特定控制的同时保持跨目标一致性。

Result: 在Alpaca、BeaverTails和TruthfulQA上的实证评估表明，AMBS在多个7B LLM后端上一致提高了HHH对齐效果。例如，在DeepSeek-7B上，AMBS将平均对齐分数提高了+32.4%，并减少了11.0%的不安全输出，同时保持与最先进方法的竞争性。

Conclusion: AMBS在多个7B LLM后端上一致提高了HHH对齐效果，同时保持与最先进方法的竞争性。

Abstract: Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

</details>


### [96] [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
*Wenjun Wang,Shuo Cai,Congkai Xie,Mingfa Feng,Yiming Zhang,Zhen Li,Kejing Yang,Ming Li,Jiannong Cao,Yuan Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的FP8训练配方，结合了持续预训练和监督微调，实现了显著的效率提升，同时保持了与BF16相当的性能。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLM）的巨大计算成本是创新的主要障碍。虽然FP8训练提供了理论上的显著效率提升，但其广泛采用受到缺乏全面、开源训练配方的阻碍。

Method: 我们的方法采用细粒度的混合粒度量化策略，在保持数值保真度的同时最大化计算效率。

Result: 通过广泛的实验，包括在160B个标记语料库上继续预训练模型，我们证明了我们的配方不仅非常稳定，而且几乎是无损失的，在一系列推理基准测试中实现了与BF16基线相当的性能。

Conclusion: 我们的结果确立了FP8作为BF16的实用且稳健的替代方案，并将发布配套代码以进一步普及大规模模型训练。

Abstract: The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.

</details>


### [97] [Think Socially via Cognitive Reasoning](https://arxiv.org/abs/2509.22546)
*Jinfeng Zhou,Zheyu Chen,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: CogFlow is a framework that enhances LLMs' social cognitive abilities by simulating human thought processes and using reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The gap between LLMs' logical reasoning and their ability to navigate social situations, which require interpretive analysis of ambiguous cues.

Method: CogFlow is a framework that instills social cognitive reasoning in LLMs by curating a dataset of cognitive flows through tree-structured planning, followed by supervised fine-tuning and reinforcement learning with a multi-objective reward.

Result: Extensive experiments show that CogFlow enhances the social cognitive capabilities of LLMs and even humans, resulting in more effective social decision-making.

Conclusion: CogFlow effectively enhances the social cognitive capabilities of LLMs, and even humans, leading to more effective social decision-making.

Abstract: LLMs trained for logical reasoning excel at step-by-step deduction to reach
verifiable answers. However, this paradigm is ill-suited for navigating social
situations, which induce an interpretive process of analyzing ambiguous cues
that rarely yield a definitive outcome. To bridge this gap, we introduce
Cognitive Reasoning, a paradigm modeled on human social cognition. It
formulates the interpretive process into a structured cognitive flow of
interconnected cognitive units (e.g., observation or attribution), which
combine adaptively to enable effective social thinking and responses. We then
propose CogFlow, a complete framework that instills this capability in LLMs.
CogFlow first curates a dataset of cognitive flows by simulating the
associative and progressive nature of human thought via tree-structured
planning. After instilling the basic cognitive reasoning capability via
supervised fine-tuning, CogFlow adopts reinforcement learning to enable the
model to improve itself via trial and error, guided by a multi-objective reward
that optimizes both cognitive flow and response quality. Extensive experiments
show that CogFlow effectively enhances the social cognitive capabilities of
LLMs, and even humans, leading to more effective social decision-making.

</details>


### [98] [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
*Wenyuan Chen,Fateme Nateghi Haredasht,Kameron C. Black,Francois Grolleau,Emily Alsentzer,Jonathan H. Chen,Stephen P. Ma*

Main category: cs.CL

TL;DR: 本文介绍了用于评估LLM生成的患者-医生消息回复质量的临床基础错误本体、检索增强评估管道（RAEC）以及两阶段DSPy架构，结果表明RAEC可以提高错误识别能力，并作为AI防护措施。


<details>
  <summary>Details</summary>
Motivation: 异步患者-医生消息通过电子健康记录门户已成为医生工作量的增长来源，需要利用大型语言模型（LLM）来协助起草回复。然而，LLM输出可能包含临床不准确、遗漏或语气不匹配，因此需要稳健的评估。

Method: 我们引入了一个临床基础的错误本体，开发了一个检索增强评估管道（RAEC），并使用DSPy实现了两阶段提示架构，以实现可扩展、可解释和分层的错误检测。

Result: 检索上下文在临床完整性及工作流程适当性等领域的错误识别中有所改善。对100条消息的人工验证显示，上下文增强标签的协议（50% vs. 33%）和性能（F1 = 0.500 vs. 0.256）优于基线。

Conclusion: 我们的方法通过引入RAEC管道和两阶段DSPy架构，提高了LLM生成的回复质量评估，支持将RAEC作为患者消息的AI防护措施。

Abstract: Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

</details>


### [99] [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)
*Yehonatan Pesiakhovsky,Zorik Gekhman,Yosi Mass,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Context-grounded hallucinations are cases where model outputs contain
information not verifiable against the source text. We study the applicability
of LLMs for localizing such hallucinations, as a more practical alternative to
existing complex evaluation pipelines. In the absence of established benchmarks
for meta-evaluation of hallucinations localization, we construct one tailored
to LLMs, involving a challenging human annotation of over 1,000 examples. We
complement the benchmark with an LLM-based evaluation protocol, verifying its
quality in a human evaluation. Since existing representations of hallucinations
limit the types of errors that can be expressed, we propose a new
representation based on free-form textual descriptions, capturing the full
range of possible errors. We conduct a comprehensive study, evaluating four
large-scale LLMs, which highlights the benchmark's difficulty, as the best
model achieves an F1 score of only 0.67. Through careful analysis, we offer
insights into optimal prompting strategies for the task and identify the main
factors that make it challenging for LLMs: (1) a tendency to incorrectly flag
missing details as inconsistent, despite being instructed to check only facts
in the output; and (2) difficulty with outputs containing factually correct
information absent from the source - and thus not verifiable - due to alignment
with the model's parametric knowledge.

</details>


### [100] [ArabJobs: A Multinational Corpus of Arabic Job Ads](https://arxiv.org/abs/2509.22589)
*Mo El-Haj*

Main category: cs.CL

TL;DR: ArabJobs is a publicly available corpus of Arabic job advertisements that captures linguistic, regional, and socio-economic variation in the Arab labour market. The paper presents analyses of gender representation and occupational structure, and highlights dialectal variation across ads. It also demonstrates applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification.


<details>
  <summary>Details</summary>
Motivation: To provide a publicly available corpus of Arabic job advertisements that captures linguistic, regional, and socio-economic variation in the Arab labour market.

Method: The paper presents analyses of gender representation and occupational structure, and highlights dialectal variation across ads. It also demonstrates applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification.

Result: The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research.

Conclusion: ArabJobs is useful for fairness-aware Arabic NLP and labour market research.

Abstract: ArabJobs is a publicly available corpus of Arabic job advertisements
collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.
Comprising over 8,500 postings and more than 550,000 words, the dataset
captures linguistic, regional, and socio-economic variation in the Arab labour
market. We present analyses of gender representation and occupational
structure, and highlight dialectal variation across ads, which offers
opportunities for future research. We also demonstrate applications such as
salary estimation and job category normalisation using large language models,
alongside benchmark tasks for gender bias detection and profession
classification. The findings show the utility of ArabJobs for fairness-aware
Arabic NLP and labour market research. The dataset is publicly available on
GitHub: https://github.com/drelhaj/ArabJobs.

</details>


### [101] [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
*Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.CL

TL;DR: 研究证明了子正则语言类在用其决定谓词表示时是线性可分的，这为建模自然语言结构提供了严格且可解释的基础。


<details>
  <summary>Details</summary>
Motivation: 为了建立有限可观测性并保证使用简单线性模型进行学习的可能性，需要确定子正则语言类是否线性可分。

Method: 证明所有标准子正则语言类在用其决定谓词表示时都是线性可分的。

Result: 合成实验在无噪声条件下确认了完美的可分性，而真实数据实验在英语形态学中显示学习到的特征与已知的语言约束一致。

Conclusion: 这些结果表明，子正则层次结构为建模自然语言结构提供了严格且可解释的基础。

Abstract: We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.

</details>


### [102] [Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods](https://arxiv.org/abs/2509.22603)
*Rakesh Thakur,Harsh Chaturvedi,Ruqayya Shah,Janvi Chauhan,Ayush Sharma*

Main category: cs.CL

TL;DR: 本研究比较分析了多种 NLP 技术，以评估模型解释 deliberative discourse 和生成有意义见解的效果。通过收集不同背景个体的意见构建了一个自源数据集，并使用富含惊人事实的产品演示模拟了 deliberation。研究结果表明，Frequency-Based Discourse Modulation 和 Quantum-Deliberation Framework 模型在解释 deliberative discourse 方面表现出色，并在公共政策制定、辩论评估、决策支持框架和大规模社交媒体意见挖掘等方面具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: Deliberation 在形成结果中起着关键作用，通过权衡各种观点来做出决策。随着自然语言处理的进步，现在可以计算建模 deliberation 通过分析意见变化并预测不同情景下的潜在结果。

Method: 本研究比较分析了多种 NLP 技术，以评估模型解释 deliberative discourse 和生成有意义见解的效果。通过收集不同背景个体的意见构建了一个自源数据集，并使用富含惊人事实的产品演示模拟了 deliberation。

Result: 研究比较了 Frequency-Based Discourse Modulation 和 Quantum-Deliberation Framework 模型，发现它们优于现有的最先进的模型。

Conclusion: 研究结果表明，Frequency-Based Discourse Modulation 和 Quantum-Deliberation Framework 模型在解释 deliberative discourse 方面表现出色，并在公共政策制定、辩论评估、决策支持框架和大规模社交媒体意见挖掘等方面具有实际应用价值。

Abstract: Deliberation plays a crucial role in shaping outcomes by weighing diverse
perspectives before reaching decisions. With recent advancements in Natural
Language Processing, it has become possible to computationally model
deliberation by analyzing opinion shifts and predicting potential outcomes
under varying scenarios. In this study, we present a comparative analysis of
multiple NLP techniques to evaluate how effectively models interpret
deliberative discourse and produce meaningful insights. Opinions from
individuals of varied backgrounds were collected to construct a self-sourced
dataset that reflects diverse viewpoints. Deliberation was simulated using
product presentations enriched with striking facts, which often prompted
measurable shifts in audience opinions. We have given comparative analysis
between two models namely Frequency-Based Discourse Modulation and
Quantum-Deliberation Framework which outperform the existing state of art
models. The findings highlight practical applications in public policy-making,
debate evaluation, decision-support frameworks, and large-scale social media
opinion mining.

</details>


### [103] [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](https://arxiv.org/abs/2509.22612)
*Jonne Sälevä,Duygu Ataman,Constantine Lignos*

Main category: cs.CL

TL;DR: 本文提出了一种基于重采样的方法，用于量化多语言和多任务NLP基准中评估指标的不确定性与统计精度，并展示了其在计算排行榜中各种量的抽样分布方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在多语言和多任务NLP基准中，评估指标的不确定性和统计精度难以准确量化，因此需要一种有效的方法来分析性能分数的变化来源。

Method: 本文提出了基于重采样的方法，用于计算排行榜中各种量（如平均值/中位数、模型间的成对差异和排名）的抽样分布。

Result: 通过多语言问答、机器翻译和命名实体识别等任务，验证了重采样方法在计算排行榜中各种量的抽样分布方面的有效性。

Conclusion: 本文介绍了基于重采样的方法，用于量化多语言和/或多任务NLP基准评估指标的不确定性与统计精度。结果表明，实验中的性能分数变化来源于模型和数据相关因素，必须考虑两者以避免低估假设复制的整体变异性。

Abstract: In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.

</details>


### [104] [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
*Xingyu Shen,Yingfa Chen,Zhen Leng Thai,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文介绍了StateX，一种用于通过后训练高效扩展预训练RNN状态的训练管道。通过设计后训练架构修改，StateX能够在不显著增加模型参数的情况下扩大状态大小，从而有效提升RNN的回忆能力和上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在语言建模方面表现出色，但它们的高复杂性导致处理长上下文时成本高昂。相比之下，递归神经网络（RNN）因其恒定的每个标记复杂度而受到欢迎。然而，这些递归模型在需要准确回忆长上下文中的上下文信息的任务中表现不佳，因为所有上下文信息都被压缩到一个固定大小的递归状态中。之前的研究表明，回忆能力与递归状态大小呈正相关，但直接训练具有更大递归状态的RNN会导致高昂的训练成本。

Method: 我们引入了StateX，这是一种用于通过后训练高效扩展预训练RNN状态的训练管道。对于两种流行的RNN类别，线性注意力和状态空间模型，我们设计了后训练架构修改，以在不增加或仅增加少量模型参数的情况下扩大状态大小。

Result: 实验表明，StateX能够有效提升RNN的回忆能力和上下文学习能力，而不会产生高昂的后续训练成本或损害其他能力。

Conclusion: StateX可以有效地增强RNN的回忆能力和上下文学习能力，而不会产生高昂的后续训练成本或损害其他能力。

Abstract: While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.

</details>


### [105] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文提出了一种变分推理框架，通过变分推断优化思考轨迹，并展示了该方法在提高语言模型推理能力方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在训练变分后验时可能存在不稳定性，同时需要一种统一的框架来结合变分推理和强化学习方法。

Method: 我们引入了一个变分推理框架，将思考轨迹视为潜在变量，并通过变分推断对其进行优化。我们从证据下界（ELBO）开始，将其扩展为多轨迹目标以获得更紧的界限，并提出了一个前向KL公式，以稳定变分后验的训练。

Result: 我们在Qwen 2.5和Qwen 3模型家族上对我们的方法进行了广泛的实证验证，结果表明该方法能够有效提高语言模型的推理能力。

Conclusion: 我们的工作提供了一个原理性的概率视角，将变分推理与RL风格的方法统一起来，并产生了稳定的优化目标，以提高语言模型的推理能力。

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [106] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，将口头反馈作为条件信号，使LLM能够直接从口头反馈中学习，而不是通过奖励优化。


<details>
  <summary>Details</summary>
Motivation: 现有的RL方法通常将复杂的反馈压缩为标量奖励，丢失了其丰富性并导致规模不平衡。因此，需要一种更有效的方法来利用口头反馈。

Method: 本文提出了反馈条件策略（FCP），它通过最大似然训练在离线数据上近似反馈条件后验，并进一步开发了一个在线自举阶段，使策略在正向条件下生成并接收新鲜反馈以自我完善。

Result: 该方法将反馈驱动的学习重新定义为条件生成，提供了一种更丰富的LLM直接从口头反馈中学习的方式。

Conclusion: 本文提出了一种新的方法，将口头反馈作为条件信号，使LLM能够直接从口头反馈中学习，而不是通过奖励优化。

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [107] [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
*Arkadiy Saakyan,Najoung Kim,Smaranda Muresan,Tuhin Chakrabarty*

Main category: cs.CL

TL;DR: 本文研究了n-gram新颖性作为衡量文本创造力的指标，发现其可能不足以全面评估创造力，并指出大型语言模型在识别非实用性表达方面仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨n-gram新颖性作为衡量文本创造力的指标是否足够，因为创造力包括新颖性和适当性两个方面，而n-gram新颖性可能忽略了适当性。

Method: 本文通过7542个专家作家对人类和AI生成文本的注释，研究了创造力与n-gram新颖性之间的关系。同时测试了零样本、少样本和微调模型是否能够识别创造性表达和非实用性表达。

Result: 研究发现，虽然n-gram新颖性与专家评价的创造力有正相关关系，但约91%的高n-gram新颖性表达未被评价为具有创造性。此外，开源大型语言模型的高n-gram新颖性与低适当性相关，而前沿闭源模型产生的创造性表达比人类少。

Conclusion: 本文结论是，虽然n-gram新颖性与专家作家评价的创造力有正相关关系，但仅依赖n-gram新颖性可能不足以评估文本的创造性。此外，大型语言模型在识别非实用表达方面仍存在改进空间，并且基于最佳模型的LLM-as-a-Judge新颖性评分可以预测专家作家的偏好。

Abstract: N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

</details>


### [108] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出 WebGen-Agent，一种利用多级视觉反馈的网站生成代理，显著提升了代码生成任务的准确性与外观评分。


<details>
  <summary>Details</summary>
Motivation: 当前代码代理在依赖视觉效果和用户交互反馈的任务中，仅依靠简单的代码执行进行反馈和验证，无法准确捕捉生成代码的实际质量。

Method: 提出 WebGen-Agent，利用多级视觉反馈迭代生成和优化网站代码库，并引入 Step-GRPO 训练方法，通过截图和 GUI 代理评分作为奖励来提供密集且可靠的流程监督信号。

Result: 在 WebGen-Bench 数据集上，WebGen-Agent 将 Claude-3.5-Sonnet 的准确性从 26.4% 提高到 51.9%，外观评分从 3.0 提高到 3.9；同时，Qwen2.5-Coder-7B-Instruct 的准确性从 38.9% 提高到 45.4%，外观评分从 3.4 提高到 3.7。

Conclusion: WebGen-Agent 提高了 Claude-3.5-Sonnet 和 Qwen2.5-Coder-7B-Instruct 在网站生成任务上的准确性与外观评分，展现出优于现有最先进的代理系统的表现。

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


### [109] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: VoiceAssistant-Eval is a comprehensive benchmark for evaluating AI assistants in listening, speaking, and viewing, revealing key findings about model performance and identifying areas for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are inadequate for evaluating the full range of voice-first AI assistants' capabilities.

Method: Introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing.

Result: Proprietary models do not universally outperform open-source models; most models excel at speaking tasks but lag in audio understanding; well-designed smaller models can rival much larger ones.

Conclusion: VoiceAssistant-Eval identifies gaps in current AI assistants and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants.

Abstract: The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [110] [SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models](https://arxiv.org/abs/2509.21843)
*Jingkai Guo,Chaitali Chakrabarti,Deliang Fan*

Main category: cs.CR

TL;DR: 本文提出了SBFA攻击方法，通过仅一个位翻转严重降低SOTA LLM模型的准确性，揭示了其严重的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有BFA方法通常分别关注整数或浮点模型，限制了攻击灵活性。此外，在浮点模型中，随机位翻转可能导致参数值极端（例如，翻转指数位），使其不隐蔽并导致数值运行时错误。

Method: 提出了一种名为SBFA（Sneaky Bit-Flip Attack）的攻击方法，通过迭代搜索和排名，利用定义的参数敏感性度量ImpactScore，结合梯度敏感性和受良性层权重分布约束的扰动范围。还提出了一种轻量级的SKIP搜索算法，大大减少了搜索复杂度。

Result: 在Qwen、LLaMA和Gemma模型上，仅通过一个位翻转，SBFA成功地将MMLU和SST-2的准确性降低到随机水平，无论使用BF16还是INT8数据格式。

Conclusion: SBFA成功地通过仅一个位翻转严重降低了SOTA LLM模型的准确性，揭示了其严重的安全问题。

Abstract: Model integrity of Large language models (LLMs) has become a pressing
security concern with their massive online deployment. Prior Bit-Flip Attacks
(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can
severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips
can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs
and reveal that, despite the intuition of better robustness from modularity and
redundancy, only a handful of adversarial bit flips can also cause LLMs'
catastrophic accuracy degradation. However, existing BFA methods typically
focus on either integer or floating-point models separately, limiting attack
flexibility. Moreover, in floating-point models, random bit flips often cause
perturbed parameters to extreme values (e.g., flipping in exponent bit), making
it not stealthy and leading to numerical runtime error (e.g., invalid tensor
values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky
Bit-Flip Attack), which collapses LLM performance with only one single bit flip
while keeping perturbed values within benign layer-wise weight distribution. It
is achieved through iterative searching and ranking through our defined
parameter sensitivity metric, ImpactScore, which combines gradient sensitivity
and perturbation range constrained by the benign layer-wise weight
distribution. A novel lightweight SKIP searching algorithm is also proposed to
greatly reduce searching complexity, which leads to successful SBFA searching
taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma
models, with only one single bit flip, SBFA successfully degrades accuracy to
below random levels on MMLU and SST-2 in both BF16 and INT8 data formats.
Remarkably, flipping a single bit out of billions of parameters reveals a
severe security concern of SOTA LLM models.

</details>


### [111] [You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors](https://arxiv.org/abs/2509.21884)
*Bochuan Cao,Changjiang Li,Yuanpu Cao,Yameng Ge,Ting Wang,Jinghui Chen*

Main category: cs.CR

TL;DR: This paper introduces SysVec, a method that encodes system prompts as internal vectors to enhance security and performance in large language models.


<details>
  <summary>Details</summary>
Motivation: To address the risk of system prompt leakage in large language models (LLMs) and improve their security and general instruction-following abilities.

Method: SysVec encodes system prompts as internal representation vectors rather than raw text.

Result: SysVec reduces the risk of unauthorized disclosure while maintaining the LLM's core language capabilities and improving its performance in long-context scenarios.

Conclusion: SysVec effectively mitigates prompt leakage attacks, preserves the LLM's functional integrity, and helps alleviate the forgetting issue in long-context scenarios.

Abstract: Large language models (LLMs) have been widely adopted across various
applications, leveraging customized system prompts for diverse tasks. Facing
potential system prompt leakage risks, model developers have implemented
strategies to prevent leakage, primarily by disabling LLMs from repeating their
context when encountering known attack patterns. However, it remains vulnerable
to new and unforeseen prompt-leaking techniques. In this paper, we first
introduce a simple yet effective prompt leaking attack to reveal such risks.
Our attack is capable of extracting system prompts from various LLM-based
application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our
findings further inspire us to search for a fundamental solution to the
problems by having no system prompt in the context. To this end, we propose
SysVec, a novel method that encodes system prompts as internal representation
vectors rather than raw text. By doing so, SysVec minimizes the risk of
unauthorized disclosure while preserving the LLM's core language capabilities.
Remarkably, this approach not only enhances security but also improves the
model's general instruction-following abilities. Experimental results
demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves
the LLM's functional integrity, and helps alleviate the forgetting issue in
long-context scenarios.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [112] [MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark](https://arxiv.org/abs/2509.22461)
*Hui Li,Changhao Jiang,Hongyu Wang,Ming Zhang,Jiajun Sun,Zhixiong Yang,Yifei Cao,Shihan Dou,Xiaoran Fan,Baoyu Fan,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.SD

TL;DR: MDAR is a new benchmark for evaluating audio reasoning models in complex, multi-scene scenarios, revealing the challenges faced by current models in handling such tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks do not fully capture scenarios with multiple speakers, unfolding events, and heterogeneous audio sources, necessitating a more comprehensive evaluation framework.

Method: MDAR is introduced as a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks, consisting of 3,000 carefully curated question-answer pairs linked to diverse audio clips.

Result: Benchmarking 26 state-of-the-art audio language models on MDAR revealed limitations in complex reasoning tasks, with no model achieving 80% performance across all question types.

Conclusion: MDAR provides a valuable benchmark for advancing audio reasoning research, highlighting the unique challenges in complex, multi-scene, and dynamically evolving audio tasks.

Abstract: The ability to reason from audio, including speech, paralinguistic cues,
environmental sounds, and music, is essential for AI agents to interact
effectively in real-world scenarios. Existing benchmarks mainly focus on static
or single-scene settings and do not fully capture scenarios where multiple
speakers, unfolding events, and heterogeneous audio sources interact. To
address these challenges, we introduce MDAR, a benchmark for evaluating models
on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR
comprises 3,000 carefully curated question-answer pairs linked to diverse audio
clips, covering five categories of complex reasoning and spanning three
question types. We benchmark 26 state-of-the-art audio language models on MDAR
and observe that they exhibit limitations in complex reasoning tasks. On
single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,
whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio
substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice
and open-ended tasks. Across all three question types, no model achieves 80%
performance. These findings underscore the unique challenges posed by MDAR and
its value as a benchmark for advancing audio reasoning research.Code and
benchmark can be found at https://github.com/luckyerr/MDAR.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [113] [Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2509.22633)
*Gen Li,Yuling Yan*

Main category: stat.ML

TL;DR: 本文研究了在线RLHF中的探索原则，提出了一种新的探索方案，以提高数据效率和减少遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有基于乐观的探索算法在收集偏好数据时无法有效减少最相关信息的不确定性，导致线性遗憾。因此，需要一种更有效的探索方案。

Method: 本文研究了在线RLHF中的探索原则，通过分析现有的基于乐观的探索算法，发现其采样协议存在不足，并提出了一种新的探索方案。

Result: 本文提出了一个新的探索方案，在多臂老虎机模型下建立了次线性遗憾界，并证明了这是第一个具有多项式尺度遗憾的在线RLHF算法。

Conclusion: 本文提出了一种新的探索方案，旨在通过减少与策略改进最相关的奖励差异的不确定性来提高在线RLHF的效率。在多臂老虎机模型下，我们建立了次线性遗憾界，并证明了这是第一个具有多项式尺度遗憾的在线RLHF算法。

Abstract: Reinforcement learning with human feedback (RLHF), which learns a reward
model from human preference data and then optimizes a policy to favor preferred
responses, has emerged as a central paradigm for aligning large language models
(LLMs) with human preferences. In this paper, we investigate exploration
principles for online RLHF, where one seeks to adaptively collect new
preference data to refine both the reward model and the policy in a
data-efficient manner. By examining existing optimism-based exploration
algorithms, we identify a drawback in their sampling protocol: they tend to
gather comparisons that fail to reduce the most informative uncertainties in
reward differences, and we prove lower bounds showing that such methods can
incur linear regret over exponentially long horizons. Motivated by this
insight, we propose a new exploration scheme that directs preference queries
toward reducing uncertainty in reward differences most relevant to policy
improvement. Under a multi-armed bandit model of RLHF, we establish regret
bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter
that balances reward maximization against mitigating distribution shift. To our
knowledge, this is the first online RLHF algorithm with regret scaling
polynomially in all model parameters.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [114] [InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?](https://arxiv.org/abs/2509.21629)
*Anjiang Wei,Tarun Suresh,Tianran Sun,Haoze Wu,Ke Wang,Alex Aiken*

Main category: cs.PL

TL;DR: 本文提出了一种评估LLM在不变式合成中表现的框架，并展示了监督微调和Best-of-N采样可以提高性能。


<details>
  <summary>Details</summary>
Motivation: 程序验证依赖于循环不变式，但自动发现强不变式仍然是一个长期存在的挑战。

Method: 我们引入了一个基于验证器的决策过程，具有形式上的正确性保证，并评估了LLM在不变式合成中的表现。

Result: 评估了7个最先进的LLM和现有的基于LLM的验证器与传统求解器UAutomizer的对比，结果显示基于LLM的验证器尚未提供显著优势。

Conclusion: 我们的基准仍然是当前LLM的一个开放性挑战，尽管监督微调和Best-of-N采样可以提高性能。

Abstract: Program verification relies on loop invariants, yet automatically discovering
strong invariants remains a long-standing challenge. We introduce a principled
framework for evaluating LLMs on invariant synthesis. Our approach uses a
verifier-based decision procedure with a formal soundness guarantee and
assesses not only correctness but also the speedup that invariants provide in
verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based
verifiers against the traditional solver UAutomizer. While LLM-based verifiers
represent a promising direction, they do not yet offer a significant advantage
over UAutomizer. Model capability also proves critical, as shown by sharp
differences in speedups across models, and our benchmark remains an open
challenge for current LLMs. Finally, we show that supervised fine-tuning and
Best-of-N sampling can improve performance: fine-tuning on 3589 instances
raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,
and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.

</details>


### [115] [Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics](https://arxiv.org/abs/2509.21793)
*Jianhong Zhao,Everett Hildenbrandt,Juan Conejero,Yongwang Zhao*

Main category: cs.PL

TL;DR: 本文提出了一种通过将验证证明转换为优化执行规则来提升程序性能的方法。


<details>
  <summary>Details</summary>
Motivation: 验证证明在检查正确性后被丢弃，而本文旨在利用这些证明来生成优化的执行规则。

Method: 通过符号执行构建全路径可达性证明，并将它们的图结构编译成优化的执行规则。

Result: 在不同编译范围内都实现了性能改进，特别是在整个程序编译时获得了数量级的性能提升。

Conclusion: 通过构建全路径可达性证明并将其编译为优化的执行规则，该研究实现了性能提升。

Abstract: Verification proofs encode complete program behavior, yet we discard them
after checking correctness. We present compiling by proving, a paradigm that
transforms these proofs into optimized execution rules. By constructing
All-Path Reachability Proofs through symbolic execution and compiling their
graph structure, we consolidate many semantic rewrites into single rules while
preserving correctness by construction. We implement this as a
language-agnostic extension to the K framework. Evaluation demonstrates
performance improvements across different compilation scopes: opcode-level
optimizations show consistent speedups, while whole-program compilation
achieves orders of magnitude greater performance gains.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [116] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: 本文介绍了AgentPack，一个由软件工程代理生成的高质量代码编辑语料库，证明其在微调代码编辑模型方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于挖掘提交和拉取请求，但数据存在噪声，如提交信息简短、人类提交混杂不相关编辑等。而软件工程代理生成的代码更改更聚焦且意图明确，因此需要新的数据集来改进代码编辑模型。

Method: 提出了AgentPack语料库，收集了Claude Code、OpenAI Codex和Cursor Agent在公共GitHub项目中的130万条代码编辑，并描述了识别和整理流程，分析了编辑的结构特性。

Result: AgentPack语料库包含1.3M代码编辑，模型在该数据集上微调后表现优于之前的人类-only数据集。

Conclusion: 模型在AgentPack上微调后可以超越仅使用人类提交数据的模型，表明使用软件工程代理的公共数据来训练未来的代码编辑模型具有潜力。

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [117] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: 本文介绍了SecureAgentBench，一个用于严格评估代码代理在安全代码生成方面能力的基准。结果表明，当前代理在生成安全代码方面存在困难，即使最好的代理也只能达到15.2%的正确且安全解决方案，同时一些代理生成的功能正确的代码仍会引入漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的基准提供了有价值的见解，但仍然不足：它们通常忽略了漏洞引入的真实情境，或采用狭窄的评估协议，无法捕捉功能正确性或新引入的漏洞。

Method: 我们引入了SecureAgentBench，这是一个包含105个编码任务的基准，旨在严格评估代码代理在安全代码生成方面的能力。每个任务包括(一)需要在大型存储库中进行多文件编辑的现实任务设置，(二)基于真实世界开源漏洞的对齐上下文，(三)综合评估，结合功能测试、通过概念验证攻击进行漏洞检查以及使用静态分析检测新引入的漏洞。

Result: 结果表明，(i)当前代理在生成安全代码方面存在困难，即使表现最好的代理SWE-agent（由DeepSeek-V3.1支持）也只能达到15.2%的正确且安全解决方案，(ii)一些代理生成功能正确的代码，但仍会引入漏洞，包括之前未记录的新漏洞，(iii)为代理添加显式的安全指令并不能显著提高安全编码，这突显了进一步研究的必要性。

Conclusion: 这些发现确立了SecureAgentBench作为安全代码生成的严格基准，并朝着使用LLM进行更可靠的软件开发迈出了一步。

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [118] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: 本研究首次系统地探讨了用户提示变化对LLM生成代码中库幻觉的影响。结果显示，即使是微小的拼写错误也可能导致高比例的幻觉，而提示工程虽然有潜力，但效果不一致。


<details>
  <summary>Details</summary>
Motivation: 尽管越来越多的人意识到这些风险，但很少有人了解现实世界中的提示变化如何影响幻觉率。因此，我们进行了首次系统研究，探讨用户级别的提示变化如何影响LLM生成代码中的库幻觉。

Method: 我们评估了六种不同的LLMs在两种幻觉类型上的表现：库名称幻觉（无效导入）和库成员幻觉（来自有效库的无效调用）。我们研究了从开发者论坛中提取的现实用户语言以及不同程度的用户错误（单个或多个字符拼写错误和完全虚假的名称/成员）如何影响LLMs的幻觉率。

Result: 我们的发现揭示了系统性漏洞：库名称中的单个拼写错误会在多达26%的任务中引发幻觉，虚假的库名称在多达99%的任务中被接受，时间相关提示会在多达84%的任务中导致幻觉。提示工程在减轻幻觉方面显示出希望，但仍然不一致且依赖于LLM。

Conclusion: 我们的结果强调了LLMs对自然提示变化的脆弱性，并突显了防范与库相关的幻觉及其潜在利用的紧迫需求。

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [119] [Random Direct Preference Optimization for Radiography Report Generation](https://arxiv.org/abs/2509.21351)
*Valentin Samokhin,Boris Shirokikh,Mikhail Goncharov,Dmitriy Umerenkov,Maksim Bobrin,Ivan Oseledets,Dmitry Dylov,Mikhail Belyaev*

Main category: cs.CV

TL;DR: 本文提出了一种模型无关的框架，通过直接偏好优化（DPO）来提高放射报告生成（RRG）的准确性。实验表明，该方法在不增加额外训练数据的情况下，能够提升临床性能指标最多5%。


<details>
  <summary>Details</summary>
Motivation: 放射报告生成（RRG）在医疗图像分析中受到广泛关注，但现有方法尚未达到实际临床应用所需的质量。同时，大型视觉语言模型（VLMs）在通用领域取得了显著进展，因此需要一种更有效的RRG方法。

Method: 本文提出了一种模型无关的框架，利用随机对比采样构建训练对，从而消除对奖励模型或人类偏好注释的需求。

Result: 实验结果表明，将本文提出的随机DPO方法补充到三种最先进的模型中，可以提升临床性能指标最多5%。

Conclusion: 本文提出了一种模型无关的框架，通过直接偏好优化（DPO）来提高放射报告生成（RRG）的准确性。实验表明，该方法在不增加额外训练数据的情况下，能够提升临床性能指标最多5%。

Abstract: Radiography Report Generation (RRG) has gained significant attention in
medical image analysis as a promising tool for alleviating the growing workload
of radiologists. However, despite numerous advancements, existing methods have
yet to achieve the quality required for deployment in real-world clinical
settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated
remarkable progress in the general domain by adopting training strategies
originally designed for Large Language Models (LLMs), such as alignment
techniques. In this paper, we introduce a model-agnostic framework to enhance
RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages
random contrastive sampling to construct training pairs, eliminating the need
for reward models or human preference annotations. Experiments on supplementing
three state-of-the-art models with our Random DPO show that our method improves
clinical performance metrics by up to 5%, without requiring any additional
training data.

</details>


### [120] [VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](https://arxiv.org/abs/2509.21451)
*Abdul Waheed,Zhen Wu,Dareen Alharthi,Seungone Kim,Bhiksha Raj*

Main category: cs.CV

TL;DR: 本文介绍了VideoJudge，一种专门用于评估视频理解模型输出的多模态大语言模型法官。通过生成器和评估器之间的相互作用进行训练，在多个基准测试中表现优于更大的基线模型，表明视频输入对视频理解任务的评估至关重要。


<details>
  <summary>Details</summary>
Motivation: 精确评估视频理解模型仍然具有挑战性：常用的指标如BLEU、ROUGE和BERTScore无法捕捉人类判断的细致程度，而通过手动评估获得这种判断成本高昂。最近的工作探索了使用大型语言模型（LLMs）或多模态LLMs（MLLMs）作为评估者，但它们在视频理解中的扩展仍相对未被探索。

Method: 我们引入了VideoJudge，一个3B和7B大小的MLLM法官，专门用于评估视频理解模型的输出。我们的训练方法基于生成器和评估器之间的相互作用：生成器被提示生成基于目标评分的响应，与评估器评分不匹配的响应被丢弃。

Result: 在四个元评估基准中的三个中，VideoJudge-7B优于更大的MLLM法官基线，如Qwen2.5-VL（32B和72B）。LLM法官（Qwen3）模型的表现不如MLLM法官（Qwen2.5-VL），并且长链式思维推理并未提高性能，这表明提供视频输入对于视频理解任务的评估至关重要。

Conclusion: VideoJudge-7B在三个元评估基准中表现优于更大的MLLM法官基线，如Qwen2.5-VL（32B和72B）。LLM法官（Qwen3）模型的表现不如MLLM法官（Qwen2.5-VL），并且长链式思维推理并未提高性能，这表明提供视频输入对于视频理解任务的评估至关重要。

Abstract: Precisely evaluating video understanding models remains challenging: commonly
used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of
human judgment, while obtaining such judgments through manual evaluation is
costly. Recent work has explored using large language models (LLMs) or
multimodal LLMs (MLLMs) as evaluators, but their extension to video
understanding remains relatively unexplored. In this work, we introduce
VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from
video understanding models (\textit{i.e.}, text responses conditioned on
videos). To train VideoJudge, our recipe builds on the interplay between a
generator and an evaluator: the generator is prompted to produce responses
conditioned on a target rating, and responses not matching the evaluator's
rating are discarded. Across three out of four meta-evaluation benchmarks,
VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B
and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than
MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve
performance, indicating that providing video inputs is crucial for evaluation
of video understanding tasks.

</details>


### [121] [Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models](https://arxiv.org/abs/2509.21466)
*Khaloud S. AlKhalifah,Malak Mashaabi,Hend Al-Khalifa*

Main category: cs.CV

TL;DR: 本研究调查了当代文本到图像AI模型在生成沙特阿拉伯专业人员的描绘时是否加剧性别刻板印象和文化不准确。结果揭示了明显的性别失衡，所有三种模型中经常观察到文化不准确性。


<details>
  <summary>Details</summary>
Motivation: 本研究调查当代文本到图像人工智能（AI）模型在生成沙特阿拉伯专业人员的描绘时，是否加剧性别刻板印象和文化不准确。

Method: 我们分析了1,006张由ImageFX、DALL-E V3和Grok为56种多样的沙特职业生成的图像，使用中性提示。两名受过训练的沙特标注者对每张图像的五个维度进行了评估：感知性别、着装和外观、背景和环境、活动和互动以及年龄。当两名主要评分者意见不一致时，第三位高级研究人员进行裁定，得出10,100个单独的判断。

Result: 结果揭示了明显的性别失衡，ImageFX输出的85%为男性，Grok为86.6%，DALL-E V3为96%，表明DALL-E V3表现出最强的整体性别刻板印象。这种不平衡在领导和专业技术角色中最明显。此外，所有三种模型中经常观察到服装、环境和所描绘活动的文化不准确性。

Conclusion: 当前模型反映了训练数据中嵌入的社会偏见，仅有限地反映了沙特劳动力市场的性别动态和文化细微差别。这些发现强调了需要更多样化的训练数据、更公平的算法和文化敏感的评估框架，以确保公平和真实的视觉输出。

Abstract: This study investigates the extent to which contemporary Text-to-Image
artificial intelligence (AI) models perpetuate gender stereotypes and cultural
inaccuracies when generating depictions of professionals in Saudi Arabia. We
analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse
Saudi professions using neutral prompts. Two trained Saudi annotators evaluated
each image on five dimensions: perceived gender, clothing and appearance,
background and setting, activities and interactions, and age. A third senior
researcher adjudicated whenever the two primary raters disagreed, yielding
10,100 individual judgements. The results reveal a strong gender imbalance,
with ImageFX outputs being 85\% male, Grok 86.6\% male, and DALL-E V3 96\%
male, indicating that DALL-E V3 exhibited the strongest overall gender
stereotyping. This imbalance was most evident in leadership and technical
roles. Moreover, cultural inaccuracies in clothing, settings, and depicted
activities were frequently observed across all three models.
Counter-stereotypical images often arise from cultural misinterpretations
rather than genuinely progressive portrayals. We conclude that current models
mirror societal biases embedded in their training data, generated by humans,
offering only a limited reflection of the Saudi labour market's gender dynamics
and cultural nuances. These findings underscore the urgent need for more
diverse training data, fairer algorithms, and culturally sensitive evaluation
frameworks to ensure equitable and authentic visual outputs.

</details>


### [122] [Learning GUI Grounding with Spatial Reasoning from Visual Feedback](https://arxiv.org/abs/2509.21552)
*Yu Zhao,Wei-Ning Chen,Huseyin Atahan Inan,Samuel Kessler,Lu Wang,Lukas Wutschitz,Fangkai Yang,Chaoyun Zhang,Pasquale Minervini,Saravan Rajmohan,Robert Sim*

Main category: cs.CV

TL;DR: This paper reframes GUI grounding as an interactive search task, where a VLM moves a cursor to locate UI elements. The proposed model, GUI-Cursor, achieves state-of-the-art results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Recent VLMs often fail to predict accurate numeric coordinates when processing high-resolution GUI images with complex layouts.

Method: Reframe GUI grounding as an interactive search task where the VLM generates actions to move a cursor in the GUI to locate UI elements. Train the model using multi-step online reinforcement learning with a dense trajectory-based reward function.

Result: GUI-Cursor, based on Qwen2.5-VL-7B, improves GUI grounding accuracy and achieves state-of-the-art results on ScreenSpot-v2 (88.8% → 93.9%) and ScreenSpot-Pro (26.8% → 56.5%).

Conclusion: GUI-Cursor improves GUI grounding accuracy and achieves state-of-the-art results on ScreenSpot-v2 and ScreenSpot-Pro. It can solve the problem within two steps for 95% of instances and adaptively conduct more steps on more difficult examples.

Abstract: Graphical User Interface (GUI) grounding is commonly framed as a coordinate
prediction task -- given a natural language instruction, generate on-screen
coordinates for actions such as clicks and keystrokes. However, recent Vision
Language Models (VLMs) often fail to predict accurate numeric coordinates when
processing high-resolution GUI images with complex layouts. To address this
issue, we reframe GUI grounding as an \emph{interactive search task}, where the
VLM generates actions to move a cursor in the GUI to locate UI elements. At
each step, the model determines the target object, evaluates the spatial
relations between the cursor and the target, and moves the cursor closer to the
target conditioned on the movement history. In this interactive process, the
rendered cursor provides visual feedback to help the model align its
predictions with the corresponding on-screen locations. We train our GUI
grounding model, GUI-Cursor, using multi-step online reinforcement learning
with a dense trajectory-based reward function. Our experimental results show
that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy
and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\% \rightarrow
93.9\%$) and ScreenSpot-Pro ($26.8\% \rightarrow 56.5\%$). Moreover, we observe
that GUI-Cursor learns to solve the problem within two steps for 95\% of
instances and can adaptively conduct more steps on more difficult examples.

</details>


### [123] [UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments](https://arxiv.org/abs/2509.21733)
*Jiannan Xiang,Yun Zhu,Lei Shu,Maria Wang,Lijun Yu,Gabriel Barcik,James Lyon,Srinivas Sunkara,Jindong Chen*

Main category: cs.CV

TL;DR: 本文介绍了UISim，一种基于图像的UI模拟器，能够从屏幕图像中动态交互地探索移动电话环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于繁琐的物理设备或有限的静态截图分析，这阻碍了可扩展测试和智能UI代理的开发。

Method: UISim采用两阶段方法：给定初始手机屏幕图像和用户操作，首先预测下一个UI状态的抽象布局，然后根据此预测布局合成新的、视觉一致的图像。

Result: UISim在UI测试、快速原型设计和合成数据生成方面提供了直接的实际好处，并且其交互能力为高级应用（如AI代理的UI导航任务规划）铺平了道路。

Conclusion: UISim在生成现实且连贯的后续UI状态方面优于端到端UI生成基线，展示了其保真度和潜力，可以简化UI开发并增强AI代理训练。

Abstract: Developing and testing user interfaces (UIs) and training AI agents to
interact with them are challenging due to the dynamic and diverse nature of
real-world mobile environments. Existing methods often rely on cumbersome
physical devices or limited static analysis of screenshots, which hinders
scalable testing and the development of intelligent UI agents. We introduce
UISim, a novel image-based UI simulator that offers a dynamic and interactive
platform for exploring mobile phone environments purely from screen images. Our
system employs a two-stage method: given an initial phone screen image and a
user action, it first predicts the abstract layout of the next UI state, then
synthesizes a new, visually consistent image based on this predicted layout.
This approach enables the realistic simulation of UI transitions. UISim
provides immediate practical benefits for UI testing, rapid prototyping, and
synthetic data generation. Furthermore, its interactive capabilities pave the
way for advanced applications, such as UI navigation task planning for AI
agents. Our experimental results show that UISim outperforms end-to-end UI
generation baselines in generating realistic and coherent subsequent UI states,
highlighting its fidelity and potential to streamline UI development and
enhance AI agent training.

</details>


### [124] [DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images](https://arxiv.org/abs/2509.21787)
*Dwip Dalal,Gautam Vashishtha,Anku Ranui,Aishwarya Reganti,Parth Patwa,Mohd Sarique,Chandan Gupta,Keshav Nath,Viswanatha Reddy,Vinija Jain,Aman Chadha,Amitava Das,Amit Sheth,Asif Ekbal*

Main category: cs.CV

TL;DR: 本文介绍了一种多模态数据集和一种新的AI方法，用于检测和移除图像中的仇恨内容。


<details>
  <summary>Details</summary>
Motivation: 有害在线内容的增加不仅扭曲了公共讨论，还对维护健康的数字环境构成了重大挑战。

Method: 我们引入了一种多模态数据集，用于识别数字内容中的仇恨内容。我们的方法结合了水印增强型稳定扩散技术与数字注意力分析模块（DAAM），以定位图像中的仇恨元素，并生成详细的仇恨注意力图，从而模糊这些区域以移除图像中的仇恨部分。

Result: 我们发布了这个数据集作为去仇恨共享任务的一部分，并描述了该共享任务的细节。此外，我们提出了DeHater，一个用于多模态去仇恨任务的视觉-语言模型。

Conclusion: 我们的方法为基于文本提示的AI驱动图像仇恨检测设定了新标准，有助于开发更符合伦理的社交媒体AI应用。

Abstract: The rise in harmful online content not only distorts public discourse but
also poses significant challenges to maintaining a healthy digital environment.
In response to this, we introduce a multimodal dataset uniquely crafted for
identifying hate in digital content. Central to our methodology is the
innovative application of watermarked, stability-enhanced, stable diffusion
techniques combined with the Digital Attention Analysis Module (DAAM). This
combination is instrumental in pinpointing the hateful elements within images,
thereby generating detailed hate attention maps, which are used to blur these
regions from the image, thereby removing the hateful sections of the image. We
release this data set as a part of the dehate shared task. This paper also
describes the details of the shared task. Furthermore, we present DeHater, a
vision-language model designed for multimodal dehatification tasks. Our
approach sets a new standard in AI-driven image hate detection given textual
prompts, contributing to the development of more ethical AI applications in
social media.

</details>


### [125] [From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs](https://arxiv.org/abs/2509.21984)
*Yingjie Zhu,Xuefeng Bai,Kehai Chen,Yang Xiang,Weili Guan,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: 本文研究了大型视觉-语言模型（LVLMs）在空间变化下的鲁棒性问题，提出了Balanced Position Assignment (BaPA)机制来改善位置嵌入不平衡的问题，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型（LVLMs）在多模态任务中取得了显著成功，但它们对空间变化的鲁棒性仍不够明确。本文旨在系统研究LVLMs的空间偏差，并提出改进方法。

Method: 本文通过设计一个探针数据集来研究LVLMs的空间偏差，并引入了Balanced Position Assignment (BaPA)机制来解决位置嵌入不平衡的问题。

Result: 实验表明，BaPA可以提高LVLMs的空间鲁棒性，并在结合轻量微调时提升其在多种多模态基准上的性能。此外，BaPA还实现了平衡的注意力，促进了更全面的视觉理解。

Conclusion: 本文提出了一种名为BaPA的简单而有效的机制，通过为所有图像标记分配相同的位置嵌入，提高了LVLMs的空间鲁棒性，并在不重新训练的情况下进一步提升了其在多种多模态基准上的性能。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success across
a wide range of multimodal tasks, yet their robustness to spatial variations
remains insufficiently understood. In this work, we present a systematic study
of the spatial bias of LVLMs, focusing on how models respond when identical key
visual information is placed at different locations within an image. Through a
carefully designed probing dataset, we demonstrate that current LVLMs often
produce inconsistent outputs under such spatial shifts, revealing a fundamental
limitation in their spatial-semantic understanding. Further analysis shows that
this phenomenon originates not from the vision encoder, which reliably
perceives and interprets visual content across positions, but from the
unbalanced design of position embeddings in the language model component. In
particular, the widely adopted position embedding strategies, such as RoPE,
introduce imbalance during cross-modal interaction, leading image tokens at
different positions to exert unequal influence on semantic understanding. To
mitigate this issue, we introduce Balanced Position Assignment (BaPA), a simple
yet effective mechanism that assigns identical position embeddings to all image
tokens, promoting a more balanced integration of visual information. Extensive
experiments show that BaPA enhances the spatial robustness of LVLMs without
retraining and further boosts their performance across diverse multimodal
benchmarks when combined with lightweight fine-tuning. Further analysis of
information flow reveals that BaPA yields balanced attention, enabling more
holistic visual understanding.

</details>


### [126] [ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models](https://arxiv.org/abs/2509.21991)
*Jewon Lee,Wooksu Shin,Seungmin Yang,Ki-Ung Song,DongUk Lim,Jaeyeon Kim,Tae-Ho Kim,Bo-Kyeong Kim*

Main category: cs.CV

TL;DR: ERGO is an efficient reasoning model that reduces computational costs while maintaining accuracy by focusing on relevant image regions through a two-stage pipeline.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs have high computational overhead due to the large number of vision tokens. The need for efficient processing of high-resolution images in real-world vision-language applications motivates this research.

Method: ERGO uses a two-stage 'coarse-to-fine' reasoning pipeline, leveraging multimodal context for reasoning-driven perception and incorporating reward components in a reinforcement learning framework.

Result: ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup.

Conclusion: ERGO achieves higher accuracy than the original model and competitive methods while being more efficient.

Abstract: Efficient processing of high-resolution images is crucial for real-world
vision-language applications. However, existing Large Vision-Language Models
(LVLMs) incur substantial computational overhead due to the large number of
vision tokens. With the advent of "thinking with images" models, reasoning now
extends beyond text to the visual domain. This capability motivates our
two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is
analyzed to identify task-relevant regions; then, only these regions are
cropped at full resolution and processed in a subsequent reasoning stage. This
approach reduces computational cost while preserving fine-grained visual
details where necessary. A major challenge lies in inferring which regions are
truly relevant to a given query. Recent related methods often fail in the first
stage after input-image downsampling, due to perception-driven reasoning, where
clear visual information is required for effective reasoning. To address this
issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs
reasoning-driven perception-leveraging multimodal context to determine where to
focus. Our model can account for perceptual uncertainty, expanding the cropped
region to cover visually ambiguous areas for answering questions. To this end,
we develop simple yet effective reward components in a reinforcement learning
framework for coarse-to-fine perception. Across multiple datasets, our approach
delivers higher accuracy than the original model and competitive methods, with
greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*
benchmark by 4.7 points while using only 23% of the vision tokens, achieving a
3x inference speedup. The code and models can be found at:
https://github.com/nota-github/ERGO.

</details>


### [127] [MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing](https://arxiv.org/abs/2509.22186)
*Junbo Niu,Zheng Liu,Zhuangcheng Gu,Bin Wang,Linke Ouyang,Zhiyuan Zhao,Tao Chu,Tianyao He,Fan Wu,Qintong Zhang,Zhenjiang Jin,Guang Liang,Rui Zhang,Wenzheng Zhang,Yuan Qu,Zhifei Ren,Yuefeng Sun,Yuanhong Zheng,Dongsheng Ma,Zirui Tang,Boyu Niu,Ziyang Miao,Hejun Dong,Siyi Qian,Junyuan Zhang,Jingzhou Chen,Fangdong Wang,Xiaomeng Zhao,Liqun Wei,Wei Li,Shasha Wang,Ruiliang Xu,Yuanyuan Cao,Lu Chen,Qianqian Wu,Huaiyu Gu,Lindong Lu,Keming Wang,Dechen Lin,Guanlin Shen,Xuanhe Zhou,Linfeng Zhang,Yuhang Zang,Xiaoyi Dong,Jiaqi Wang,Bo Zhang,Lei Bai,Pei Chu,Weijia Li,Jiang Wu,Lijun Wu,Zhenxiang Li,Guangyu Wang,Zhongying Tu,Chao Xu,Kai Chen,Yu Qiao,Bowen Zhou,Dahua Lin,Wentao Zhang,Conghui He*

Main category: cs.CV

TL;DR: MinerU2.5是一种1.2B参数的文档解析视觉语言模型，采用从粗到细的两阶段解析策略，实现了最先进的识别准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了实现先进的识别准确性同时保持出色的计算效率，需要一种有效的文档解析方法。

Method: 提出了一种从粗到细的两阶段解析策略，将全局布局分析与局部内容识别解耦。在第一阶段，模型对下采样的图像进行高效的布局分析以识别结构元素；在第二阶段，根据全局布局对原始图像中提取的原分辨率图像块进行有针对性的内容识别。

Result: MinerU2.5在多个基准测试中表现出色，超越了通用和领域特定模型的各种识别任务，同时保持了显著更低的计算开销。

Conclusion: MinerU2.5展示了强大的文档解析能力，在多个基准测试中实现了最先进的性能，同时保持了显著的计算开销降低。

Abstract: We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language
model that achieves state-of-the-art recognition accuracy while maintaining
exceptional computational efficiency. Our approach employs a coarse-to-fine,
two-stage parsing strategy that decouples global layout analysis from local
content recognition. In the first stage, the model performs efficient layout
analysis on downsampled images to identify structural elements, circumventing
the computational overhead of processing high-resolution inputs. In the second
stage, guided by the global layout, it performs targeted content recognition on
native-resolution crops extracted from the original image, preserving
fine-grained details in dense text, complex formulas, and tables. To support
this strategy, we developed a comprehensive data engine that generates diverse,
large-scale training corpora for both pretraining and fine-tuning. Ultimately,
MinerU2.5 demonstrates strong document parsing ability, achieving
state-of-the-art performance on multiple benchmarks, surpassing both
general-purpose and domain-specific models across various recognition tasks,
while maintaining significantly lower computational overhead.

</details>


### [128] [Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](https://arxiv.org/abs/2509.22615)
*Yasmine Omri,Connor Ding,Tsachy Weissman,Thierry Tambe*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern vision language pipelines are driven by RGB vision encoders trained on
massive image text corpora. While these pipelines have enabled impressive zero
shot capabilities and strong transfer across tasks, they still inherit two
structural inefficiencies from the pixel domain: (i) transmitting dense RGB
images from edge devices to the cloud is energy intensive and costly, and (ii)
patch based tokenization explodes sequence length, stressing attention budgets
and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative
visual substrate for alignment: a compact, spatially adaptive representation
that parameterizes images by a set of colored anisotropic Gaussians. We develop
a scalable 2DGS pipeline with structured initialization, luminance aware
pruning, and batched CUDA kernels, achieving over 90x faster fitting and about
97% GPU utilization compared to prior implementations. We further adapt
contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen
RGB-based transformer backbone with a lightweight splat aware input stem and a
perceiver resampler, training only about 7% of the total parameters. On large
DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K
performance while compressing inputs 3 to 20x relative to pixels. While
accuracy currently trails RGB encoders, our results establish 2DGS as a viable
multimodal substrate, pinpoint architectural bottlenecks, and open a path
toward representations that are both semantically powerful and transmission
efficient for edge cloud learning.

</details>


### [129] [LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision](https://arxiv.org/abs/2509.22631)
*Debargha Ganguly,Sumit Kumar,Ishwar Balappanawar,Weicong Chen,Shashank Kambhatla,Srinivasan Iyengar,Shivkumar Kalyanaraman,Ponnurangam Kumaraguru,Vipin Chaudhary*

Main category: cs.CV

TL;DR: 本文介绍了Labeling Copilot，这是一个用于计算机视觉的数据整理深度研究代理，通过优化和可扩展的工具提供了一个稳健的基础，用于整理工业规模的数据集。


<details>
  <summary>Details</summary>
Motivation: 当前，整理高质量、领域特定的数据集是部署健壮视觉系统的主要瓶颈，需要在数据质量、多样性和成本之间进行复杂的权衡。

Method: Labeling Copilot是一个由大型多模态语言模型驱动的中央协调代理，使用多步骤推理来执行三个核心功能：(1) 校准发现，(2) 可控合成，(3) 共识标注。

Result: Labeling Copilot的组件在大规模验证中证明了其有效性。共识标注模块在密集的COCO数据集上平均每个图像有14.2个候选提案，达到了37.1%的最终注释mAP。在Web规模的Open Images数据集上，它能够处理极端类别不平衡，发现了903个新的边界框类别。

Conclusion: 本文提出了Labeling Copilot，这是一个用于计算机视觉的数据整理深度研究代理，通过优化和可扩展的工具提供了一个稳健的基础，用于整理工业规模的数据集。

Abstract: Curating high-quality, domain-specific datasets is a major bottleneck for
deploying robust vision systems, requiring complex trade-offs between data
quality, diversity, and cost when researching vast, unlabeled data lakes. We
introduce Labeling Copilot, the first data curation deep research agent for
computer vision. A central orchestrator agent, powered by a large multimodal
language model, uses multi-step reasoning to execute specialized tools across
three core capabilities: (1) Calibrated Discovery sources relevant,
in-distribution data from large repositories; (2) Controllable Synthesis
generates novel data for rare scenarios with robust filtering; and (3)
Consensus Annotation produces accurate labels by orchestrating multiple
foundation models via a novel consensus mechanism incorporating non-maximum
suppression and voting. Our large-scale validation proves the effectiveness of
Labeling Copilot's components. The Consensus Annotation module excels at object
discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per
image-nearly double the 7.4 ground-truth objects-achieving a final annotation
mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class
imbalance to discover 903 new bounding box categories, expanding its capability
to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a
10-million sample scale, features an active learning strategy that is up to 40x
more computationally efficient than alternatives with equivalent sample
efficiency. These experiments validate that an agentic workflow with optimized,
scalable tools provides a robust foundation for curating industrial-scale
datasets.

</details>


### [130] [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](https://arxiv.org/abs/2509.22646)
*Xingyu Fu,Siyi Liu,Yinuo Xu,Pan Lu,Guangqiuse Hu,Tianbo Yang,Taran Anantasagar,Christopher Shen,Yikai Mao,Yuanzhe Liu,Keyush Shah,Chung Un Lee,Yejin Choi,James Zou,Dan Roth,Chris Callison-Burch*

Main category: cs.CV

TL;DR: 本文介绍了DeeptraceReward，这是一个用于评估人类识别AI生成视频能力的基准。通过训练多模态语言模型，该基准在假线索识别、定位和解释方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型迅速发展，但人类能否检测深度伪造痕迹这一关键维度被忽视。需要一个基准来评估人类识别AI生成视频的能力。

Method: 引入了DeeptraceReward，这是第一个细粒度、空间和时间感知的基准，用于注释视频生成奖励的人类感知假痕迹。训练多模态语言模型作为奖励模型以模仿人类判断和定位。

Result: 在DeeptraceReward上，我们的7B奖励模型在假线索识别、定位和解释方面平均优于GPT-5 34.7%。发现了一个一致的难度梯度：二进制假/真实分类比细粒度深度伪造痕迹检测更容易；在后者中，性能从自然语言解释（最简单），到空间定位，再到时间标记（最难）。

Conclusion: DeeptraceReward为社会意识和可信赖的视频生成提供了严格的测试平台和训练信号。

Abstract: Can humans identify AI-generated (fake) videos and provide grounded reasons?
While video generation models have advanced rapidly, a critical dimension --
whether humans can detect deepfake traces within a generated video, i.e.,
spatiotemporal grounded visual artifacts that reveal a video as machine
generated -- has been largely overlooked. We introduce DeeptraceReward, the
first fine-grained, spatially- and temporally- aware benchmark that annotates
human-perceived fake traces for video generation reward. The dataset comprises
4.3K detailed annotations across 3.3K high-quality generated videos. Each
annotation provides a natural-language explanation, pinpoints a bounding-box
region containing the perceived trace, and marks precise onset and offset
timestamps. We consolidate these annotations into 9 major categories of
deepfake traces that lead humans to identify a video as AI-generated, and train
multimodal language models (LMs) as reward models to mimic human judgments and
localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by
34.7% on average across fake clue identification, grounding, and explanation.
Interestingly, we observe a consistent difficulty gradient: binary fake v.s.
real classification is substantially easier than fine-grained deepfake trace
detection; within the latter, performance degrades from natural language
explanations (easiest), to spatial grounding, to temporal labeling (hardest).
By foregrounding human-perceived deepfake traces, DeeptraceReward provides a
rigorous testbed and training signal for socially aware and trustworthy video
generation.

</details>


### [131] [CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning](https://arxiv.org/abs/2509.22647)
*Long Xing,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jianze Liang,Qidong Huang,Jiaqi Wang,Feng Wu,Dahua Lin*

Main category: cs.CV

TL;DR: CapRL是一种基于可验证奖励的强化学习方法，用于图像描述任务，通过提高描述质量来增强模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前的图像描述模型通常依赖于昂贵且不可扩展的人工标注数据，导致模型记忆特定的真实答案，限制了其通用性和生成多样化、创造性描述的能力。

Method: CapRL采用了一种解耦的两阶段流程，其中LVLM生成描述，然后通过一个独立的、无视觉的LLM根据该描述回答多项选择题来获得客观奖励。

Result: CapRL在多个基准测试中取得了显著提升，特别是在CapRL-5M数据集上预训练后，表现优于基线模型。

Conclusion: CapRL显著提升了图像描述任务的性能，并在多个基准测试中表现出色，证明了其有效性。

Abstract: Image captioning is a fundamental task that bridges the visual and linguistic
domains, playing a critical role in pre-training Large Vision-Language Models
(LVLMs). Current state-of-the-art captioning models are typically trained with
Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable
data annotated by humans or proprietary models. This approach often leads to
models that memorize specific ground-truth answers, limiting their generality
and ability to generate diverse, creative descriptions. To overcome the
limitation of SFT, we propose applying the Reinforcement Learning with
Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.
A primary challenge, however, is designing an objective reward function for the
inherently subjective nature of what constitutes a "good" caption. We introduce
Captioning Reinforcement Learning (CapRL), a novel training framework that
redefines caption quality through its utility: a high-quality caption should
enable a non-visual language model to accurately answer questions about the
corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM
generates a caption, and the objective reward is derived from the accuracy of a
separate, vision-free LLM answering Multiple-Choice Questions based solely on
that caption. As the first study to apply RLVR to the subjective image
captioning task, we demonstrate that CapRL significantly enhances multiple
settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B
results in substantial gains across 12 benchmarks. Moreover, within the Prism
Framework for caption quality evaluation, CapRL achieves performance comparable
to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.
Code is available here: https://github.com/InternLM/CapRL.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [132] [C-QUERI: Congressional Questions, Exchanges, and Responses in Institutions Dataset](https://arxiv.org/abs/2509.21548)
*Manjari Rudra,Daniel Magleby,Sujoy Sikdar*

Main category: cs.CY

TL;DR: 本文开发了一个从非结构化听证会转录文本中提取问题-答案对的管道，并构建了一个新的数据集，用于研究国会听证会中的提问策略。分析显示，提问者的党派隶属关系可以从他们的提问中预测出来。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模数据集来研究这种话语，政治采访和听证会中的战略性方面仍研究不足。国会听证会提供了一个特别丰富且易于处理的研究场所：互动由正式规则规定，证人有义务回答，并且不同政治派别的成员都有机会提问，这使得跨政治光谱的行为比较成为可能。

Method: 我们开发了一个管道来从非结构化听证会转录文本中提取问题-答案对，并构建了一个来自第108届至第117届国会委员会听证会的新数据集。

Result: 我们的分析揭示了不同政党之间提问策略的系统性差异，通过显示提问者的党派隶属关系可以从他们的提问中预测出来。

Conclusion: 我们的数据集和方法不仅推进了对国会政治的研究，还为分析类似访谈的问答提供了通用框架。

Abstract: Questions in political interviews and hearings serve strategic purposes
beyond information gathering including advancing partisan narratives and
shaping public perceptions. However, these strategic aspects remain
understudied due to the lack of large-scale datasets for studying such
discourse. Congressional hearings provide an especially rich and tractable site
for studying political questioning: Interactions are structured by formal
rules, witnesses are obliged to respond, and members with different political
affiliations are guaranteed opportunities to ask questions, enabling
comparisons of behaviors across the political spectrum.
  We develop a pipeline to extract question-answer pairs from unstructured
hearing transcripts and construct a novel dataset of committee hearings from
the 108th--117th Congress. Our analysis reveals systematic differences in
questioning strategies across parties, by showing the party affiliation of
questioners can be predicted from their questions alone. Our dataset and
methods not only advance the study of congressional politics, but also provide
a general framework for analyzing question-answering across interview-like
settings.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [133] [LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?](https://arxiv.org/abs/2509.21501)
*Lu Sun,Shihan Fu,Bingsheng Yao,Yuxuan Lu,Wenbo Li,Hansu Gu,Jiri Gesi,Jing Huang,Chen Luo,Dakuo Wang*

Main category: cs.HC

TL;DR: 本研究探讨了LLM代理在多轮交互中与代理AI系统镜像人类的能力，发现它们的动作模式与人类一致，并产生了相似的设计反馈。


<details>
  <summary>Details</summary>
Motivation: 评估这些系统具有挑战性，因为它们的快速发展超过了传统的人类评估。研究人员提出了LLM代理来模拟参与者作为数字双胞胎，但尚不清楚数字双胞胎能在多大程度上代表特定客户在与代理AI系统进行多轮交互时的表现。

Method: 本研究招募了40名人类参与者使用Amazon Rufus购物，收集了他们的个性特征、交互痕迹和用户体验反馈，并创建了数字双胞胎来重复任务。通过比较人类和数字双胞胎的痕迹进行分析。

Result: 虽然代理通常探索了更多样化的选择，但它们的动作模式与人类一致，并产生了相似的设计反馈。

Conclusion: 本研究是第一个量化LLM代理能够多轮交互中与代理AI系统镜像人类的程度的研究，突显了其在可扩展评估中的潜力。

Abstract: Agentic AI is emerging, capable of executing tasks through natural language,
such as Copilot for coding or Amazon Rufus for shopping. Evaluating these
systems is challenging, as their rapid evolution outpaces traditional human
evaluation. Researchers have proposed LLM Agents to simulate participants as
digital twins, but it remains unclear to what extent a digital twin can
represent a specific customer in multi-turn interaction with an agentic AI
system. In this paper, we recruited 40 human participants to shop with Amazon
Rufus, collected their personas, interaction traces, and UX feedback, and then
created digital twins to repeat the task. Pairwise comparison of human and
digital-twin traces shows that while agents often explored more diverse
choices, their action patterns aligned with humans and yielded similar design
feedback. This study is the first to quantify how closely LLM agents can mirror
human multi-turn interaction with an agentic AI system, highlighting their
potential for scalable evaluation.

</details>


### [134] [What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness](https://arxiv.org/abs/2509.21868)
*Yuxuan Li,Sauvik Das,Hirokazu Shirado*

Main category: cs.HC

TL;DR: 本文探讨了如何使LLM代理模拟真正对政策有用，并提出了三个设计启示。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决如何使LLM代理模拟真正对政策有用的问题。

Method: 本文通过与大学应急准备团队的长期迭代设计合作，开发了一个包含13,000个LLM代理的系统，用于模拟大规模集会中的群体移动和通信。

Result: 这些模拟影响了实际政策实施，塑造了志愿者培训、疏散协议和基础设施规划。

Conclusion: 本文提出了三个设计启示，以使LLM代理模拟真正对政策有用。

Abstract: There is growing interest in using Large Language Models as agents (LLM
agents) for social simulations to inform policy, yet real-world adoption
remains limited. This paper addresses the question: How can LLM agent
simulations be made genuinely useful for policy? We report on a year-long
iterative design engagement with a university emergency preparedness team.
Across multiple design iterations, we iteratively developed a system of 13,000
LLM agents that simulate crowd movement and communication during a large-scale
gathering under various emergency scenarios. These simulations informed actual
policy implementation, shaping volunteer training, evacuation protocols, and
infrastructure planning. Analyzing this process, we identify three design
implications: start with verifiable scenarios and build trust gradually, use
preliminary simulations to elicit tacit knowledge, and treat simulation and
policy development as evolving together. These implications highlight
actionable pathways to making LLM agent simulations that are genuinely useful
for policy.

</details>


### [135] [Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory](https://arxiv.org/abs/2509.22505)
*Yunhao Yuan,Jiaxun Zhang,Talayeh Aledavood,Renwen Zhang,Koustuv Saha*

Main category: cs.HC

TL;DR: 本文探讨了AI伴侣聊天机器人对用户心理社会影响的复杂性，并提出了设计建议以优化其益处并减少风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI伴侣聊天机器人（AICCs）如Replika的日益流行，其对用户心理和社会影响的不确定性需要进一步研究。

Method: 本文采用了大规模准实验研究和半结构化访谈相结合的方法，应用了分层倾向得分匹配和差异分析回归，并使用Knapp的关系发展模型进行主题分析。

Result: 研究发现与AICCs互动对用户情绪表达、孤独感和自杀意念有积极和消极的影响，同时识别出关系发展的轨迹，包括启动、升级和建立联系。

Conclusion: 本文通过混合方法研究了AI伴侣聊天机器人对用户心理社会影响的复杂性，并提出了设计建议，以最大化心理社会益处并减少风险。

Abstract: AI-powered companion chatbots (AICCs) such as Replika are increasingly
popular, offering empathetic interactions, yet their psychosocial impacts
remain unclear. We examined how engaging with AICCs shaped wellbeing and how
users perceived these experiences. First, we conducted a large-scale
quasi-experimental study of longitudinal Reddit data, applying stratified
propensity score matching and Difference-in-Differences regression. Findings
revealed mixed effects -- greater affective and grief expression, readability,
and interpersonal focus, alongside increases in language about loneliness and
suicidal ideation. Second, we complemented these results with 15
semi-structured interviews, which we thematically analyzed and contextualized
using Knapp's relationship development model. We identified trajectories of
initiation, escalation, and bonding, wherein AICCs provided emotional
validation and social rehearsal but also carried risks of over-reliance and
withdrawal. Triangulating across methods, we offer design implications for AI
companions that scaffold healthy boundaries, support mindful engagement,
support disclosure without dependency, and surface relationship stages --
maximizing psychosocial benefits while mitigating risks.

</details>


### [136] [Does AI Coaching Prepare us for Workplace Negotiations?](https://arxiv.org/abs/2509.22545)
*Veda Duddu,Jash Rajesh Parekh,Andy Mao,Hanyi Min,Ziang Xiao,Vedant Das Swain,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究探讨了AI在谈判辅导中的作用，发现传统手册在某些方面优于AI，强调了结合结构化内容和AI练习的重要性。


<details>
  <summary>Details</summary>
Motivation: 工作场所的谈判受到心理障碍的阻碍，甚至可能破坏精心准备的策略。AI可以提供个性化和随时可用的谈判辅导，但其在谈判准备中的有效性仍然不清楚。

Method: 我们构建了Trucey，这是一个基于Brett谈判模型的原型AI教练。我们进行了一项被试间实验（N=267），比较了Trucey、ChatGPT和传统的谈判手册，随后进行了深入访谈（N=15）。

Result: Trucey在减少恐惧方面表现出最强的下降，相对于其他两种条件，而手册在可用性和心理赋权方面表现优于两种AI。访谈显示，手册的全面、可审查的内容对参与者的信心和准备至关重要。相比之下，尽管参与者重视AI的练习能力，但其指导往往显得冗长且碎片化，需要额外努力，使他们感到不确定或不知所措。

Conclusion: 这些发现挑战了AI优越性的假设，并推动了混合设计，将结构化的、以理论为基础的内容与有针对性的练习、清晰的界限和自适应支架结合起来，以解决心理障碍并支持谈判准备。

Abstract: Workplace negotiations are undermined by psychological barriers, which can
even derail well-prepared tactics. AI offers personalized and always --
available negotiation coaching, yet its effectiveness for negotiation
preparedness remains unclear. We built Trucey, a prototype AI coach grounded in
Brett's negotiation model. We conducted a between-subjects experiment (N=267),
comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by
in-depth interviews (N=15). While Trucey showed the strongest reductions in
fear relative to both comparison conditions, the Handbook outperformed both AIs
in usability and psychological empowerment. Interviews revealed that the
Handbook's comprehensive, reviewable content was crucial for participants'
confidence and preparedness. In contrast, although participants valued AI's
rehearsal capability, its guidance often felt verbose and fragmented --
delivered in bits and pieces that required additional effort -- leaving them
uncertain or overwhelmed. These findings challenge assumptions of AI
superiority and motivate hybrid designs that integrate structured,
theory-driven content with targeted rehearsal, clear boundaries, and adaptive
scaffolds to address psychological barriers and support negotiation
preparedness.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [137] [Evaluating Open-Source Large Language Models for Technical Telecom Question Answering](https://arxiv.org/abs/2509.21949)
*Arina Caraus,Alessio Buscemi,Sumit Kumar,Ion Turcanu*

Main category: cs.NI

TL;DR: 本研究评估了两个开源大语言模型在电信领域的表现，发现Gemma在语义保真度和正确性方面优于DeepSeek，但两者均存在局限性，需要领域适应模型以支持可信的人工智能助手。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多个领域表现出色，但它们在技术领域如电信方面的表现仍缺乏深入研究。本研究旨在评估大语言模型在电信领域的表现，并探讨其在支持可信人工智能助手方面的潜力。

Method: 本研究评估了两个开源大语言模型Gemma 3 27B和DeepSeek R1 32B，使用从高级无线通信材料中提取的事实性和推理性问题进行测试。构建了一个包含105个问答对的基准，并通过词汇指标、语义相似性和LLM作为裁判的评分来评估性能。还通过源归属和分数方差分析了一致性、判断可靠性和幻觉。

Result: 研究结果表明，Gemma在语义保真度和LLM评分的正确性方面表现优异，而DeepSeek在词汇一致性方面略高。此外，研究还发现了当前在电信应用中的局限性，并强调了需要领域适应模型来支持可信的人工智能助手在工程中的应用。

Conclusion: 研究结果显示，Gemma在语义保真度和LLM评分的正确性方面表现优异，而DeepSeek在词汇一致性方面略高。此外，研究还指出了当前在电信应用中的局限性，并强调了需要领域适应模型来支持可信的人工智能助手在工程中的应用。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
various fields. However, their performance in technical domains such as
telecommunications remains underexplored. This paper evaluates two open-source
LLMs, Gemma 3 27B and DeepSeek R1 32B, on factual and reasoning-based questions
derived from advanced wireless communications material. We construct a
benchmark of 105 question-answer pairs and assess performance using lexical
metrics, semantic similarity, and LLM-as-a-judge scoring. We also analyze
consistency, judgment reliability, and hallucination through source attribution
and score variance. Results show that Gemma excels in semantic fidelity and
LLM-rated correctness, while DeepSeek demonstrates slightly higher lexical
consistency. Additional findings highlight current limitations in telecom
applications and the need for domain-adapted models to support trustworthy
Artificial Intelligence (AI) assistants in engineering.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [138] [HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores](https://arxiv.org/abs/2509.21336)
*Guohang Yan,Yue Zhang,Pinlong Cai,Ding Wang,Song Mao,Hongwei Zhang,Yaoze Zhang,Hairong Zhang,Xinyu Cai,Botian Shi*

Main category: cs.IR

TL;DR: HetaRAG 是一种融合多种检索方式的 RAG 框架，旨在提升 LLM 的知识准确性和上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统 RAG 系统在处理不同数据存储时存在局限性，例如向量搜索丢失全局上下文，知识图谱难以召回，全文索引缺乏语义理解，关系型数据库没有语义感知。因此，需要一种能够融合多种检索范式的系统。

Method: HetaRAG 通过将向量索引、知识图谱、全文引擎和结构化数据库统一到一个检索平面中，动态地路由和融合证据，以实现最佳性能。

Result: HetaRAG 能够有效地融合多种检索方式，从而提高检索效果，并为后续生成提供更准确和丰富的上下文信息。

Conclusion: HetaRAG 是一种混合的、深度检索增强生成框架，旨在通过协调异构数据存储中的跨模态证据来提高召回率、精度和上下文保真度。

Abstract: Retrieval-augmented generation (RAG) has become a dominant paradigm for
mitigating knowledge hallucination and staleness in large language models
(LLMs) while preserving data security. By retrieving relevant evidence from
private, domain-specific corpora and injecting it into carefully engineered
prompts, RAG delivers trustworthy responses without the prohibitive cost of
fine-tuning. Traditional retrieval-augmented generation (RAG) systems are
text-only and often rely on a single storage backend, most commonly a vector
database. In practice, this monolithic design suffers from unavoidable
trade-offs: vector search captures semantic similarity yet loses global
context; knowledge graphs excel at relational precision but struggle with
recall; full-text indexes are fast and exact yet semantically blind; and
relational engines such as MySQL provide strong transactional guarantees but no
semantic understanding. We argue that these heterogeneous retrieval paradigms
are complementary, and propose a principled fusion scheme to orchestrate them
synergistically, mitigating the weaknesses of any single modality. In this work
we introduce HetaRAG, a hybrid, deep-retrieval augmented generation framework
that orchestrates cross-modal evidence from heterogeneous data stores. We plan
to design a system that unifies vector indices, knowledge graphs, full-text
engines, and structured databases into a single retrieval plane, dynamically
routing and fusing evidence to maximize recall, precision, and contextual
fidelity. To achieve this design goal, we carried out preliminary explorations
and constructed an initial RAG pipeline; this technical report provides a brief
overview. The partial code is available at
https://github.com/KnowledgeXLab/HetaRAG.

</details>


### [139] [ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems](https://arxiv.org/abs/2509.21371)
*Dayu Yang,Hui Fang*

Main category: cs.IR

TL;DR: 本文提出了一种名为ReGeS的互惠检索-生成协同框架，以提高对话推荐系统的性能，避免额外注释，减少幻觉，并简化持续更新。


<details>
  <summary>Details</summary>
Motivation: 连接对话与外部领域知识对于对话推荐系统正确理解用户偏好至关重要。然而，现有解决方案要么需要领域特定的工程，限制了灵活性，要么仅依赖大型语言模型，增加了幻觉的风险。虽然检索增强生成（RAG）有潜力，但其在CRS中的直接使用受到嘈杂对话的阻碍，并且忽略了相似项目之间的细微差别。

Method: 提出了一种互惠检索-生成协同框架ReGeS，通过生成增强的检索来提炼对话中的用户意图，并通过检索增强的生成来区分细微的项目特征。

Result: 实验表明，ReGeS在推荐准确性方面达到了最先进的性能。

Conclusion: ReGeS在多个CRS基准测试中实现了最先进的推荐准确性，证明了互惠协同在知识密集型CRS任务中的有效性。

Abstract: Connecting conversation with external domain knowledge is vital for
conversational recommender systems (CRS) to correctly understand user
preferences. However, existing solutions either require domain-specific
engineering, which limits flexibility, or rely solely on large language models,
which increases the risk of hallucination. While Retrieval-Augmented Generation
(RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that
weaken retrieval and by overlooked nuances among similar items. We propose
ReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies
generation-augmented retrieval to distill informative user intent from
conversations and retrieval-augmented generation to differentiate subtle item
features. This synergy obviates the need for extra annotations, reduces
hallucinations, and simplifies continuous updates. Experiments on multiple CRS
benchmarks show that ReGeS achieves state-of-the-art performance in
recommendation accuracy, demonstrating the effectiveness of reciprocal synergy
for knowledge-intensive CRS tasks.

</details>


### [140] [Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?](https://arxiv.org/abs/2509.22325)
*JiaYing Zheng,HaiNan Zhang,Liang Pang,YongXin Tong,ZhiMing Zheng*

Main category: cs.IR

TL;DR: This paper proposes SynRewrite, a synthetic data-driven query rewriting model that generates high-quality synthetic rewrites more aligned with user intent than human annotations. The model is trained using GPT-4o to synthesize rewrites and fine-tuned with Flan-T5. It is further enhanced using the generator's feedback through the DPO algorithm. Experiments show that SynRewrite outperforms human rewrites in both retrieval and generation tasks.


<details>
  <summary>Details</summary>
Motivation: Multi-turn RAG systems often face queries with colloquial omissions and ambiguous references, posing significant challenges for effective retrieval and generation. Traditional query rewriting relies on human annotators to clarify queries, but due to limitations in annotators' expressive ability and depth of understanding, manually rewritten queries often diverge from those needed in real-world RAG systems, resulting in a gap between user intent and system response.

Method: We propose SynRewrite, a synthetic data-driven query rewriting model. To construct training data, we prompt GPT-4o with dialogue history, current queries, positive documents, and answers to synthesize high-quality rewrites. A Flan-T5 model is then finetuned on this dataset to map dialogue history and queries to synthetic rewrites. Finally, we further enhance the rewriter using the generator's feedback through the DPO algorithm to boost end-task performance.

Result: Experiments on TopiOCQA and QRECC datasets show that SynRewrite consistently outperforms human rewrites in both retrieval and generation tasks.

Conclusion: Synthetic rewrites can serve as a scalable and effective alternative to human annotations.

Abstract: Multi-turn RAG systems often face queries with colloquial omissions and
ambiguous references, posing significant challenges for effective retrieval and
generation. Traditional query rewriting relies on human annotators to clarify
queries, but due to limitations in annotators' expressive ability and depth of
understanding, manually rewritten queries often diverge from those needed in
real-world RAG systems, resulting in a gap between user intent and system
response. We observe that high-quality synthetic queries can better bridge this
gap, achieving superior performance in both retrieval and generation compared
to human rewrites. This raises an interesting question: Can rewriting models
trained on synthetic queries better capture user intent than human annotators?
In this paper, we propose SynRewrite, a synthetic data-driven query rewriting
model to generate high-quality synthetic rewrites more aligned with user
intent. To construct training data, we prompt GPT-4o with dialogue history,
current queries, positive documents, and answers to synthesize high-quality
rewrites. A Flan-T5 model is then finetuned on this dataset to map dialogue
history and queries to synthetic rewrites. Finally, we further enhance the
rewriter using the generator's feedback through the DPO algorithm to boost
end-task performance. Experiments on TopiOCQA and QRECC datasets show that
SynRewrite consistently outperforms human rewrites in both retrieval and
generation tasks. Our results demonstrate that synthetic rewrites can serve as
a scalable and effective alternative to human annotations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [141] [ARTI-6: Towards Six-dimensional Articulatory Speech Encoding](https://arxiv.org/abs/2509.21447)
*Jihwan Lee,Sean Foley,Thanathai Lertpetchpun,Kevin Huang,Yoonjeong Lee,Tiantian Feng,Louis Goldstein,Dani Byrd,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: ARTI-6 is a six-dimensional articulatory speech encoding framework that uses real-time MRI data to capture key vocal tract regions and includes an inversion and synthesis model for generating natural-sounding speech.


<details>
  <summary>Details</summary>
Motivation: To develop a compact six-dimensional articulatory speech encoding framework that captures crucial vocal tract regions and advances articulatory inversion, synthesis, and broader speech technology applications.

Method: ARTI-6 consists of three components: (1) a six-dimensional articulatory feature set representing key regions of the vocal tract; (2) an articulatory inversion model, which predicts articulatory features from speech acoustics leveraging speech foundation models; and (3) an articulatory synthesis model, which reconstructs intelligible speech directly from articulatory features.

Result: ARTI-6 achieves a prediction correlation of 0.87 with the articulatory inversion model and shows that even a low-dimensional representation can generate natural-sounding speech.

Conclusion: ARTI-6 provides an interpretable, computationally efficient, and physiologically grounded framework for advancing articulatory inversion, synthesis, and broader speech technology applications.

Abstract: We propose ARTI-6, a compact six-dimensional articulatory speech encoding
framework derived from real-time MRI data that captures crucial vocal tract
regions including the velum, tongue root, and larynx. ARTI-6 consists of three
components: (1) a six-dimensional articulatory feature set representing key
regions of the vocal tract; (2) an articulatory inversion model, which predicts
articulatory features from speech acoustics leveraging speech foundation
models, achieving a prediction correlation of 0.87; and (3) an articulatory
synthesis model, which reconstructs intelligible speech directly from
articulatory features, showing that even a low-dimensional representation can
generate natural-sounding speech. Together, ARTI-6 provides an interpretable,
computationally efficient, and physiologically grounded framework for advancing
articulatory inversion, synthesis, and broader speech technology applications.
The source code and speech samples are publicly available.

</details>


### [142] [AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit](https://arxiv.org/abs/2509.21597)
*Yi Zhu,Heitor R. Guimarães,Arthur Pimentel,Tiago Falk*

Main category: eess.AS

TL;DR: 本文提出了一种新的开源工具包AUDDT，用于评估音频深度伪造检测器，并分析了现有数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于大多数模型仅在有限的数据集上进行评估，因此其在现实条件下的泛化能力尚不确定，因此需要一种更全面的评估方法。

Method: 本文系统地回顾了28个现有的音频深度伪造数据集，并开发了一个开源基准测试工具包AUDDT，以自动化评估预训练的检测器。

Result: 本文展示了AUDDT工具包的使用、基准的组成以及不同深度伪造子组的分解，并通过广泛采用的预训练深度伪造检测器呈现了域内和域外检测结果。

Conclusion: 本文提出了一个名为AUDDT的开源基准测试工具包，用于评估预训练的深度伪造检测器，并揭示了现有数据集的局限性和与实际部署场景的差距。

Abstract: With the prevalence of artificial intelligence (AI)-generated content, such
as audio deepfakes, a large body of recent work has focused on developing
deepfake detection techniques. However, most models are evaluated on a narrow
set of datasets, leaving their generalization to real-world conditions
uncertain. In this paper, we systematically review 28 existing audio deepfake
datasets and present an open-source benchmarking toolkit called AUDDT
(https://github.com/MuSAELab/AUDDT). The goal of this toolkit is to automate
the evaluation of pretrained detectors across these 28 datasets, giving users
direct feedback on the advantages and shortcomings of their deepfake detectors.
We start by showcasing the usage of the developed toolkit, the composition of
our benchmark, and the breakdown of different deepfake subgroups. Next, using a
widely adopted pretrained deepfake detector, we present in- and out-of-domain
detection results, revealing notable differences across conditions and audio
manipulation types. Lastly, we also analyze the limitations of these existing
datasets and their gap relative to practical deployment scenarios.

</details>


### [143] [Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based Model Bias](https://arxiv.org/abs/2509.22061)
*Shree Harsha Bokkahalli Satish,Harm Lameris,Olivier Perrotin,Gustav Eje Henter,Éva Székely*

Main category: eess.AS

TL;DR: 本文首次系统评估了语音延续中的偏见，发现性别和发音类型会影响模型的行为，特别是在高连贯性情况下，性别效应在文本指标上显现，并且女性提示的延续更倾向于回归到模态发音，显示出系统性的语音质量偏见。


<details>
  <summary>Details</summary>
Motivation: 语音延续（SC）是生成连贯的口语提示扩展的任务，同时保留语义上下文和说话者身份。由于SC仅限于单个音频流，因此比对话更直接地用于探测语音基础模型中的偏见。在本研究中，我们进行了第一个系统评估，调查性别和发音类型如何影响延续行为。

Method: 我们对三个最近的模型进行了评估：SpiritLM（基础和富有表现力）、VAE-GSLM 和 SpeechGPT，以检查语音相似性、语音质量保持和基于文本的偏见指标。

Result: 结果表明，尽管语音相似性和连贯性仍然是挑战，但文本评估揭示了模型和性别之间的显著相互作用：当连贯性足够高时（对于VAE-GSLM），性别效应在诸如代理性和句子极性等文本指标上显现出来。此外，与男性提示相比，女性提示的延续更强烈地回归到模态发音，揭示了系统性的语音质量偏见。

Conclusion: 这些发现强调了语音延续作为探测语音基础模型中社会相关表示偏差的控制性探针的重要性，并表明随着延续质量的提高，它将变得越来越有信息量。

Abstract: Speech Continuation (SC) is the task of generating a coherent extension of a
spoken prompt while preserving both semantic context and speaker identity.
Because SC is constrained to a single audio stream, it offers a more direct
setting for probing biases in speech foundation models than dialogue does. In
this work we present the first systematic evaluation of bias in SC,
investigating how gender and phonation type (breathy, creaky, end-creak) affect
continuation behaviour. We evaluate three recent models: SpiritLM (base and
expressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice quality
preservation, and text-based bias metrics. Results show that while both speaker
similarity and coherence remain a challenge, textual evaluations reveal
significant model and gender interactions: once coherence is sufficiently high
(for VAE-GSLM), gender effects emerge on text-metrics such as agency and
sentence polarity. In addition, continuations revert toward modal phonation
more strongly for female prompts than for male ones, revealing a systematic
voice-quality bias. These findings highlight SC as a controlled probe of
socially relevant representational biases in speech foundation models, and
suggest that it will become an increasingly informative diagnostic as
continuation quality improves.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [144] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: Claim Advisor is a web application that uses large language models to accelerate the creation of product claims, showing promising results in a consumer packaged goods company.


<details>
  <summary>Details</summary>
Motivation: Creating product claims is a time-consuming and costly task. The goal is to accelerate claim creation using generative AI.

Method: Claim Advisor uses in-context learning and fine-tuning of large language models (LLM) to accelerate claim creation. It has three functions: semantically searching and identifying existing claims, generating and optimizing claims based on product descriptions and consumer profiles, and ranking claims using simulations via synthetic consumers.

Result: Applications in a CPG company have shown very promising results, demonstrating the effectiveness of Claim Advisor.

Conclusion: Claim Advisor has shown very promising results in a consumer packaged goods (CPG) company and is broadly useful across product categories and industries.

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [145] [Towards mitigating information leakage when evaluating safety monitors](https://arxiv.org/abs/2509.21344)
*Gerard Boxo,Aman Neelappa,Shivam Raval*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: White box monitors that analyze model internals offer promising advantages
for detecting potentially harmful behaviors in large language models, including
lower computational costs and integration into layered defense systems.However,
training and evaluating these monitors requires response exemplars that exhibit
the target behaviors, typically elicited through prompting or fine-tuning. This
presents a challenge when the information used to elicit behaviors inevitably
leaks into the data that monitors ingest, inflating their effectiveness. We
present a systematic framework for evaluating a monitor's performance in terms
of its ability to detect genuine model behavior rather than superficial
elicitation artifacts. Furthermore, we propose three novel strategies to
evaluate the monitor: content filtering (removing deception-related text from
inputs), score filtering (aggregating only over task-relevant tokens), and
prompt distilled fine-tuned model organisms (models trained to exhibit
deceptive behavior without explicit prompting). Using deception detection as a
representative case study, we identify two forms of leakage that inflate
monitor performance: elicitation leakage from prompts that explicitly request
harmful behavior, and reasoning leakage from models that verbalize their
deceptive actions. Through experiments on multiple deception benchmarks, we
apply our proposed mitigation strategies and measure performance retention. Our
evaluation of the monitors reveal three crucial findings: (1) Content filtering
is a good mitigation strategy that allows for a smooth removal of elicitation
signal and can decrease probe AUROC by 30\% (2) Score filtering was found to
reduce AUROC by 15\% but is not as straightforward to attribute to (3) A
finetuned model organism improves monitor evaluations but reduces their
performance by upto 40\%, even when re-trained.

</details>


### [146] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: 本文提出了UltraHorizon基准，用于评估智能体在长时域和部分可观测场景中的基础能力。实验显示LLM代理在这些设置中表现不佳，而人类参与者得分更高，表明智能体在长时域能力上存在持续的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的基准很少能捕捉到长时域挑战，导致系统评估存在空白。本文旨在填补这一空白，提出一个新颖的基准来衡量复杂现实世界挑战所需的基础能力。

Method: 本文通过探索作为统一任务，在三个不同的环境中验证了核心能力。智能体在长时域发现任务中需要通过持续推理、规划、记忆和工具管理来揭示隐藏规则。

Result: 实验结果表明，LLM代理在这些设置中表现不佳，而人类参与者得分更高，这表明智能体在长时域能力上存在持续的差距。此外，简单的扩展在本文的任务中失败。

Conclusion: 本文提出了UltraHorizon基准，以评估智能体在长时域和部分可观测场景中的基础能力。实验表明，LLM代理在这些设置中表现不佳，而人类参与者得分更高，这表明智能体在长时域能力上存在持续的差距。

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [147] [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
*Renqi Chen,Zeyin Tao,Jianming Guo,Jingzhe Zhu,Yiheng Peng,Qingqing Sun,Tianyi Zhang,Shuai Chen*

Main category: cs.AI

TL;DR: RISK is a novel framework for building and deploying GUI agents in e-commerce risk management, consisting of a dataset, a benchmark, and a reinforcement learning framework. It improves performance in both single-step and multi-step tasks.


<details>
  <summary>Details</summary>
Motivation: E-commerce risk management requires aggregating diverse, deeply embedded web data through multi-step, stateful interactions, which traditional scraping methods and most existing GUI agents cannot handle. These agents are typically limited to single-step tasks and lack the ability to manage dynamic, interactive content critical for effective risk assessment.

Method: RISK integrates three components: (1) RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction trajectories, (2) RISK-Bench, a benchmark with 802 single-step and 320 multi-step trajectories across three difficulty levels, and (3) RISK-R1, a R1-style reinforcement fine-tuning framework considering four aspects: Output Format, Single-step Level, Multi-step Level, and Task Level.

Result: Experiments show that RISK-R1 outperforms existing baselines, achieving a 6.8% improvement in offline single-step and an 8.8% improvement in offline multi-step. Moreover, it attains a top task success rate of 70.5% in online evaluation.

Conclusion: RISK provides a scalable, domain-specific solution for automating complex web interactions, advancing the state of the art in e-commerce risk management.

Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.

</details>


### [148] [The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging](https://arxiv.org/abs/2509.22034)
*Xiaochong Lan,Yu Zheng,Shiteng Cao,Yong Li*

Main category: cs.AI

TL;DR: 本文研究了模型合并技术在生成具有可调节推理能力的大型语言模型中的应用，发现模型合并可以有效平衡推理准确性和计算效率，并提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 由于对具有可调节推理能力的大型语言模型（LLMs）的需求不断增长，需要一种能够高效生成平衡推理深度和计算成本的模型的方法。模型合并作为一种无训练的技术，通过算术组合通用模型和专门推理模型的权重来解决这一挑战。然而，其潜力尚未被充分探索。

Method: 本文进行了一项大规模的实证研究，评估了多种模型合并技术在多个推理基准上的表现。通过系统地改变合并强度，构建了准确性和效率曲线。

Result: 研究发现，模型合并提供了一种有效且可控的方法，用于校准推理准确性与令牌效率之间的权衡，即使父模型的权重空间高度不同。此外，还发现了帕累托改进的情况，其中合并模型在提高准确性的同时降低了令牌消耗。

Conclusion: 本文提供了对可调节空间的首次全面分析，并为创建具有特定推理配置文件的LLM提供了实用指南。

Abstract: The growing demand for large language models (LLMs) with tunable reasoning
capabilities in many real-world applications highlights a critical need for
methods that can efficiently produce a spectrum of models balancing reasoning
depth and computational cost. Model merging has emerged as a promising,
training-free technique to address this challenge by arithmetically combining
the weights of a general-purpose model with a specialized reasoning model.
While various merging techniques exist, their potential to create a spectrum of
models with fine-grained control over reasoning abilities remains largely
unexplored. This work presents a large-scale empirical study evaluating a range
of model merging techniques across multiple reasoning benchmarks. We
systematically vary merging strengths to construct accuracy-efficiency curves,
providing the first comprehensive view of the tunable performance landscape.
Our findings reveal that model merging offers an effective and controllable
method for calibrating the trade-off between reasoning accuracy and token
efficiency, even when parent models have highly divergent weight spaces.
Crucially, we identify instances of Pareto Improvement, where a merged model
achieves both higher accuracy and lower token consumption than one of its
parents. Our study provides the first comprehensive analysis of this tunable
space, offering practical guidelines for creating LLMs with specific reasoning
profiles to meet diverse application demands.

</details>


### [149] [A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning](https://arxiv.org/abs/2509.22044)
*Ziqi Wang,Boye Niu,Zhongli Li,Linghui Meng,Jing Liu,Zhi Zheng,Tong Xu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为A2R的不对称两阶段推理框架，通过'探索者'和'合成器'模型的协作，显著提升了模型在复杂任务上的性能，并在成本效率上取得了优势。


<details>
  <summary>Details</summary>
Motivation: 解决模型在单次尝试中的表现与其潜在能力之间的差距问题，尤其是在多个解决方案路径中才能显现的差距。

Method: 提出了一种不对称的两阶段推理框架A2R，其中包含一个'探索者'模型和一个'合成器'模型，分别负责生成潜在解决方案和整合参考以进行更精细的推理。

Result: 使用A2R框架，Qwen3-8B-distill模型相比其自洽基线性能提高了75%。此外，A2R-Efficient配置（Qwen3-4B探索者与Qwen3-8B合成器）在成本降低约30%的情况下超越了单体Qwen3-32B模型的平均性能。

Conclusion: A2R不仅是一个性能提升框架，而且是现实应用中高效且实用的解决方案。

Abstract: Recent Large Reasoning Models have achieved significant improvements in
complex task-solving capabilities by allocating more computation at the
inference stage with a "thinking longer" paradigm. Even as the foundational
reasoning capabilities of models advance rapidly, the persistent gap between a
model's performance in a single attempt and its latent potential, often
revealed only across multiple solution paths, starkly highlights the disparity
between its realized and inherent capabilities. To address this, we present
A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge
the gap between a model's potential and its actual performance. In this
framework, an "explorer" model first generates potential solutions in parallel
through repeated sampling. Subsequently,a "synthesizer" model integrates these
references for a more refined, second stage of reasoning. This two-stage
process allows computation to be scaled orthogonally to existing sequential
methods. Our work makes two key innovations: First, we present A2R as a
plug-and-play parallel reasoning framework that explicitly enhances a model's
capabilities on complex questions. For example, using our framework, the
Qwen3-8B-distill model achieves a 75% performance improvement compared to its
self-consistency baseline. Second, through a systematic analysis of the
explorer and synthesizer roles, we identify an effective asymmetric scaling
paradigm. This insight leads to A2R-Efficient, a "small-to-big" variant that
combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration
surpasses the average performance of a monolithic Qwen3-32B model at a nearly
30% lower cost. Collectively, these results show that A2R is not only a
performance-boosting framework but also an efficient and practical solution for
real-world applications.

</details>


### [150] [InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.22261)
*Guanghao Zhu,Zhitian Hou,Zeyu Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: 本文提出了一种专门用于医学领域的多模态大语言模型，解决了数据质量、训练效率和领域特定知识提取的问题，并在医学视觉问答和诊断任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 通用的MLLMs通常缺乏进行医学任务所需的专门知识，导致不确定或幻觉响应。从先进模型进行知识蒸馏难以捕捉放射学和药理学中的领域专业知识。此外，使用大规模医学数据进行持续预训练的计算成本带来了显著的效率挑战。

Method: 我们结合了高质量的通用和医学多模态数据，并提出了一个五维质量评估框架来整理高质量的多模态医学数据集。我们采用低到高图像分辨率和多模态序列打包来提高训练效率，使大量医学数据的整合成为可能。此外，一个三阶段的监督微调过程确保了复杂医学任务的有效知识提取。

Result: 在MedEvalKit框架上评估，InfiMed-Foundation-1.7B优于Qwen2.5VL-3B，而InfiMed-Foundation-4B超越了HuatuoGPT-V-7B和MedGemma-27B-IT，展示了在医学视觉问答和诊断任务中的优越性能。

Conclusion: 通过解决数据质量、训练效率和领域特定知识提取的关键挑战，我们的工作为医疗保健中更可靠和有效的AI驱动解决方案铺平了道路。

Abstract: Multimodal large language models (MLLMs) have shown remarkable potential in
various domains, yet their application in the medical field is hindered by
several challenges. General-purpose MLLMs often lack the specialized knowledge
required for medical tasks, leading to uncertain or hallucinatory responses.
Knowledge distillation from advanced models struggles to capture
domain-specific expertise in radiology and pharmacology. Additionally, the
computational cost of continual pretraining with large-scale medical data poses
significant efficiency challenges. To address these issues, we propose
InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs
designed to deliver state-of-the-art performance in medical applications. We
combined high-quality general-purpose and medical multimodal data and proposed
a novel five-dimensional quality assessment framework to curate high-quality
multimodal medical datasets. We employ low-to-high image resolution and
multimodal sequence packing to enhance training efficiency, enabling the
integration of extensive medical data. Furthermore, a three-stage supervised
fine-tuning process ensures effective knowledge extraction for complex medical
tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B
outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B
and MedGemma-27B-IT, demonstrating superior performance in medical visual
question answering and diagnostic tasks. By addressing key challenges in data
quality, training efficiency, and domain-specific knowledge extraction, our
work paves the way for more reliable and effective AI-driven solutions in
healthcare. InfiMed-Foundation-4B model is available at
\href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.

</details>


### [151] [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
*Hieu Tran,Zonghai Yao,Nguyen Luong Tran,Zhichao Yang,Feiyun Ouyang,Shuo Han,Razieh Rahimi,Hong Yu*

Main category: cs.AI

TL;DR: PRIME是一种多智能体推理框架，模仿人类认知过程，提高了开源LLM在复杂、知识密集型推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 受《思考，快与慢》中双过程理论的启发，PRIME 模仿人类认知过程，以提高效率和准确性。

Method: PRIME 是一种多智能体推理框架，动态整合System 1（快速、直觉性思维）和System 2（缓慢、深思熟虑的思维）。它首先使用Quick Thinking Agent（System 1）生成快速答案；如果检测到不确定性，则触发由专门代理组成的Structured System 2推理管道。

Result: 实验结果表明，PRIME 使开源LLM在需要多跳和知识基础推理的基准测试中能够与最先进的闭源模型如GPT-4和GPT-4o竞争。

Conclusion: PRIME 是一种可扩展的解决方案，可以提高需要复杂、知识密集型推理的领域的LLMs性能。

Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

</details>


### [152] [Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time](https://arxiv.org/abs/2509.22572)
*Yixuan Han,Fan Ma,Ruijie Quan,Yi Yang*

Main category: cs.AI

TL;DR: Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity.

Method: Dynamic Experts Search (DES), which elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search.

Result: Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost.

Conclusion: DES is a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.

Abstract: Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [153] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: SPF是一种无需训练的空中视觉-语言导航框架，能够根据任何类型的自由形式指令在任何环境中导航。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的方法将动作预测视为文本生成任务，而SPF的关键见解是将其视为2D空间定位任务。

Method: SPF将动作预测视为2D空间定位任务，利用VLMs将模糊的语言指令分解为输入图像上的迭代2D航点注释，并将预测的2D航点转换为UAV的动作命令。此外，SPF还自适应地调整行驶距离以提高导航效率。

Result: SPF在DRL模拟基准测试中表现出色，优于之前的最佳方法，绝对差距为63%。在现实世界评估中，SPF也表现出色，优于强基线。

Conclusion: SPF在DRL模拟基准测试中设定了新的最先进水平，优于之前的最佳方法，绝对差距为63%。在广泛的现实世界评估中，SPF也大幅优于强基线。

Abstract: We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [154] [LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?](https://arxiv.org/abs/2509.21403)
*Rushil Gupta,Jason Hartford,Bang Liu*

Main category: cs.LG

TL;DR: 研究评估了大型语言模型在实验设计中的表现，发现它们对实验反馈不敏感，而经典方法表现更好。同时提出了一种混合方法LLMNN，取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型是否能够进行上下文实验设计，并探索更有效的实验设计方法。

Method: 使用开源和闭源指令调优的大型语言模型对遗传扰动和分子属性发现任务进行评估，并提出了一种简单的混合方法LLM-guided Nearest Neighbour (LLMNN)采样。

Result: 大型语言模型代理对实验反馈没有敏感性，替换真实结果为随机标签对性能没有影响，而经典方法如线性带宽和高斯过程优化表现更好。LLMNN在不需要大量上下文适应的情况下，在各个领域表现出竞争力或优越性。

Conclusion: 当前开源和闭源大型语言模型在实践中并未进行上下文实验设计，并强调了需要将基于先验的推理与更新后验的批量获取分开的混合框架。

Abstract: Large language models (LLMs) have recently been proposed as general-purpose
agents for experimental design, with claims that they can perform in-context
experimental design. We evaluate this hypothesis using both open- and
closed-source instruction-tuned LLMs applied to genetic perturbation and
molecular property discovery tasks. We find that LLM-based agents show no
sensitivity to experimental feedback: replacing true outcomes with randomly
permuted labels has no impact on performance. Across benchmarks, classical
methods such as linear bandits and Gaussian process optimization consistently
outperform LLM agents. We further propose a simple hybrid method, LLM-guided
Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with
nearest-neighbor sampling to guide the design of experiments. LLMNN achieves
competitive or superior performance across domains without requiring
significant in-context adaptation. These results suggest that current open- and
closed-source LLMs do not perform in-context experimental design in practice
and highlight the need for hybrid frameworks that decouple prior-based
reasoning from batch acquisition with updated posteriors.

</details>


### [155] [Are Hallucinations Bad Estimations?](https://arxiv.org/abs/2509.21473)
*Hude Liu,Jerry Yao-Chieh Hu,Jennifer Yuntong Zhang,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: 本文重新定义了生成模型中的幻觉现象，并通过理论分析和实验验证了其存在。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新理解生成模型中的幻觉现象，并探索其与损失最小化之间的关系。

Method: 本文通过形式化生成模型中的幻觉，并证明即使是最优估计器也会产生幻觉，同时提供了针对通用数据分布的高概率下限。

Result: 实验结果表明，幻觉确实存在于多种任务中，如硬币聚合、开放式问答和文本到图像生成。

Conclusion: 本文将幻觉重新定义为损失最小化与人类可接受输出之间的结构不匹配，并通过实验验证了这一理论。

Abstract: We formalize hallucinations in generative models as failures to link an
estimate to any plausible cause. Under this interpretation, we show that even
loss-minimizing optimal estimators still hallucinate. We confirm this with a
general high probability lower bound on hallucinate rate for generic data
distributions. This reframes hallucination as structural misalignment between
loss minimization and human-acceptable outputs, and hence estimation errors
induced by miscalibration. Experiments on coin aggregation, open-ended QA, and
text-to-image support our theory.

</details>


### [156] [Uncertainty-Aware Knowledge Tracing Models](https://arxiv.org/abs/2509.21514)
*Joshua Mitton,Prarthana Bhattacharyya,Ralph Abboud,Simon Woodhead*

Main category: cs.LG

TL;DR: 本文提出了一种通过捕捉预测不确定性来增强知识追踪模型的方法，并展示了这种不确定性如何帮助检测学生的错误。


<details>
  <summary>Details</summary>
Motivation: 现有的KT模型在学生选择干扰项时往往做出错误预测，导致学生错误无法被检测到。

Method: 通过捕捉预测不确定性来增强KT模型的能力，并展示预测不确定性与模型错误预测的一致性。

Result: 证明了KT模型中的不确定性是有信息量的，并且可以用于教育学习平台中以理解学生能力。

Conclusion: 不确定性在KT模型中是有信息量的，并且这种信号在教育学习平台中具有教学上的实用性。

Abstract: The main focus of research on Knowledge Tracing (KT) models is on model
developments with the aim of improving predictive accuracy. Most of these
models make the most incorrect predictions when students choose a distractor,
leading to student errors going undetected. We present an approach to add new
capabilities to KT models by capturing predictive uncertainty and demonstrate
that a larger predictive uncertainty aligns with model incorrect predictions.
We show that uncertainty in KT models is informative and that this signal would
be pedagogically useful for application in an educational learning platform
that can be used in a limited resource setting where understanding student
ability is necessary.

</details>


### [157] [Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews](https://arxiv.org/abs/2509.21579)
*Mst Eshita Khatun,Halima Akter,Tasnimul Rehan,Toufiq Ahmed*

Main category: cs.LG

TL;DR: 本研究利用大数据和机器学习技术检测亚马逊产品评论中的垃圾评论，以提高在线购物环境的可信度。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，在线购物已成为日常生活中的常见做法，但虚假评论会破坏消费者的信任并损害卖家声誉，因此需要有效的方法来检测和分类垃圾评论。

Method: 本研究利用大规模的亚马逊产品评论数据集，采用大数据框架处理和分析大量评论数据，提取欺诈行为的关键特征，并使用多种机器学习分类器进行垃圾评论检测。

Result: 本研究展示了各种机器学习分类器在检测垃圾评论中的效用，其中逻辑回归达到了90.35%的准确率。

Conclusion: 本研究通过使用先进的大数据分析和机器学习方法，提高了在线购物环境的可信度和透明度。

Abstract: In this digital era, online shopping is common practice in our daily lives.
Product reviews significantly influence consumer buying behavior and help
establish buyer trust. However, the prevalence of fraudulent reviews undermines
this trust by potentially misleading consumers and damaging the reputations of
the sellers. This research addresses this pressing issue by employing advanced
big data analytics and machine learning approaches on a substantial dataset of
Amazon product reviews. The primary objective is to detect and classify spam
reviews accurately so that it enhances the authenticity of the review. Using a
scalable big data framework, we efficiently process and analyze a large scale
of review data, extracting key features indicative of fraudulent behavior. Our
study illustrates the utility of various machine learning classifiers in
detecting spam reviews, with Logistic Regression achieving an accuracy of
90.35%, thus contributing to a more trustworthy and transparent online shopping
environment.

</details>


### [158] [Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers](https://arxiv.org/abs/2509.22445)
*Peter Shaw,James Cohan,Jacob Eisenstein,Kristina Toutanova*

Main category: cs.LG

TL;DR: 本文提出了一种基于MDL原则的理论框架，用于在神经网络中实现更好的压缩和泛化，特别针对Transformer进行了研究，并展示了其在算法任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏一种原理性的、通用的模型复杂度度量，将最小描述长度（MDL）原则应用于神经网络（如Transformer）具有挑战性。本文旨在解决这一问题，提供一种理论框架，以实现更好的压缩和泛化。

Method: 本文引入了渐近最优描述长度目标的理论概念，并基于Kolmogorov复杂性理论进行研究。此外，还构建并分析了一个基于自适应高斯混合先验的变分目标，以证明这些目标可以是可处理且可微的。

Result: 本文证明了对于Transformer来说，存在渐近最优的目标，并且通过实验验证了这种变分目标能够选择低复杂度的解决方案，在算法任务中表现出良好的泛化能力。然而，标准优化器从随机初始化中无法找到这样的解决方案，凸显了关键的优化挑战。

Conclusion: 本文提出了一个理论框架，用于识别具有强渐近保证的描述长度目标，这为训练能够实现更大压缩和泛化的神经网络提供了一条潜在路径。

Abstract: The Minimum Description Length (MDL) principle offers a formal framework for
applying Occam's razor in machine learning. However, its application to neural
networks such as Transformers is challenging due to the lack of a principled,
universal measure for model complexity. This paper introduces the theoretical
notion of asymptotically optimal description length objectives, grounded in the
theory of Kolmogorov complexity. We establish that a minimizer of such an
objective achieves optimal compression, for any dataset, up to an additive
constant, in the limit as model resource bounds increase. We prove that
asymptotically optimal objectives exist for Transformers, building on a new
demonstration of their computational universality. We further show that such
objectives can be tractable and differentiable by constructing and analyzing a
variational objective based on an adaptive Gaussian mixture prior. Our
empirical analysis shows that this variational objective selects for a
low-complexity solution with strong generalization on an algorithmic task, but
standard optimizers fail to find such solutions from a random initialization,
highlighting key optimization challenges. More broadly, by providing a
theoretical framework for identifying description length objectives with strong
asymptotic guarantees, we outline a potential path towards training neural
networks that achieve greater compression and generalization.

</details>


### [159] [IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method](https://arxiv.org/abs/2509.22463)
*Xinyu Liu,Bei Li,Jiahao Liu,Junhao Ruan,Kechen Jiao,Hongyin Tang,Jingang Wang,Xiao Tong,Jingbo Zhu*

Main category: cs.LG

TL;DR: 本文提出了IIET和IIAD方法，在提升Transformer性能的同时有效优化了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统效率技术如知识蒸馏可能对高阶方法的性能产生负面影响，因此需要探索更优的ODE-based Transformer架构。

Method: 提出了一种基于常微分方程的Transformer架构IIET，以及一种迭代影响感知蒸馏方法IIAD，以优化模型性能与效率之间的平衡。

Result: IIET在lm-evaluation-harness上比普通Transformer提升了2.65%，比PCformer提升了0.8%；E-IIET将推理开销减少了55%且保留了99.4%的原始任务准确性。

Conclusion: IIET和E-IIET在保持高性能的同时显著提高了效率，展示了其在实际应用中的潜力。

Abstract: High-order numerical methods enhance Transformer performance in tasks like
NLP and CV, but introduce a performance-efficiency trade-off due to increased
computational overhead. Our analysis reveals that conventional efficiency
techniques, such as distillation, can be detrimental to the performance of
these models, exemplified by PCformer. To explore more optimizable ODE-based
Transformer architectures, we propose the \textbf{I}terative \textbf{I}mplicit
\textbf{E}uler \textbf{T}ransformer \textbf{(IIET)}, which simplifies
high-order methods using an iterative implicit Euler approach. This
simplification not only leads to superior performance but also facilitates
model compression compared to PCformer. To enhance inference efficiency, we
introduce \textbf{I}teration \textbf{I}nfluence-\textbf{A}ware
\textbf{D}istillation \textbf{(IIAD)}. Through a flexible threshold, IIAD
allows users to effectively balance the performance-efficiency trade-off. On
lm-evaluation-harness, IIET boosts average accuracy by 2.65\% over vanilla
Transformers and 0.8\% over PCformer. Its efficient variant, E-IIET,
significantly cuts inference overhead by 55\% while retaining 99.4\% of the
original task accuracy. Moreover, the most efficient IIET variant achieves an
average performance gain exceeding 1.6\% over vanilla Transformer with
comparable speed.

</details>


### [160] [EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning](https://arxiv.org/abs/2509.22576)
*Xu Wujiang,Wentian Zhao,Zhenting Wang,Li Yu-Jhe,Jin Can,Jin Mingyu,Mei Kai,Wan Kun,Metaxas Dimitris*

Main category: cs.LG

TL;DR: 本文研究了在多轮稀疏奖励环境下训练LLM代理的挑战，并提出了一种新的方法EPO来解决探索-利用级联失败问题，从而显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 在多轮环境中训练LLM代理，其中完成单个任务需要30多个回合的交互，对强化学习提出了根本性的挑战。我们识别了一个独特的失败模式：探索-利用级联失败。

Method: 我们提出了熵正则化策略优化（EPO），这是一个通过三个协同机制打破失败循环的通用框架：(1) 在多轮设置中采用熵正则化以增强探索，(2) 一个熵平滑正则化器，将策略熵限制在历史平均值内以防止突然波动，(3) 自适应阶段加权，平衡训练中的探索和利用。

Result: EPO在ScienceWorld上实现了高达152%的性能提升，在ALFWorld上实现了高达19.8%的提升。

Conclusion: 我们的工作表明，多轮稀疏奖励设置需要与传统强化学习不同的基本熵控制，这对LLM代理训练有广泛的影响。

Abstract: Training LLM agents in multi-turn environments with sparse rewards, where
completing a single task requires 30+ turns of interaction within an episode,
presents a fundamental challenge for reinforcement learning. We identify a
critical failure mode unique to this setting: the exploration-exploitation
cascade failure. This cascade begins with early-stage policy premature
convergence, where sparse feedback causes agents to commit to flawed,
low-entropy strategies. Subsequently, agents enter late-stage policy collapse,
where conventional entropy regularization becomes counterproductive, promoting
chaotic exploration that destabilizes training. We propose Entropy-regularized
Policy Optimization (EPO), a general framework that breaks this failure cycle
through three synergistic mechanisms: (1) adopting entropy regularization in
multi-turn settings to enhance exploration, (2) an entropy smoothing
regularizer that bounds policy entropy within historical averages to prevent
abrupt fluctuations, and (3) adaptive phase-based weighting that balances
exploration and exploitation across training. Our analysis justifies that EPO
guarantees monotonically decreasing entropy variance while maintaining
convergence. EPO achieves up to 152% performance improvement on ScienceWorld
and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn
sparse-reward settings require fundamentally different entropy control than
traditional RL, with broad implications for LLM agent training.

</details>


### [161] [Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.22601)
*Yulei Qin,Xiaoyu Tan,Zhengbao He,Gang Li,Haojia Lin,Zongyi Li,Zihan Xu,Yuchen Shi,Siqi Cai,Renting Rui,Shaofei Cai,Yuzheng Cai,Xuan Zhang,Sheng Ye,Ke Li,Xing Sun*

Main category: cs.LG

TL;DR: 本文提出了一种基于课程的自模仿学习（SPEAR）方法，用于训练具有战略工具使用能力的LLM。该方法通过平衡探索和利用，提高了训练稳定性，并通过引入课程和内在奖励来促进探索。


<details>
  <summary>Details</summary>
Motivation: 现有的研究通过政策熵来刺激探索，但这种机械性的熵最大化容易导致RL训练不稳定，因为多轮分布变化。本文旨在解决这一问题，通过引导代理自身的经验实现渐进式的探索-利用平衡，避免熵崩溃或发散。

Method: 本文提出了一种基于课程的自模仿学习（SIL）方法，称为SPEAR。该方法扩展了传统的SIL框架，通过逐渐引导策略进化在不同阶段保持熵的平衡范围。此外，还引入了课程来管理探索过程，并利用内在奖励和SIL来促进技能级和动作级的探索。

Result: 本文提出的SPEAR方法能够有效平衡探索和利用，提高训练稳定性。通过引入课程和内在奖励，促进了技能级和动作级的探索，同时通过重新校准经验优势和引入正则化技术，进一步稳定了训练过程。

Conclusion: 本文提出了一种基于课程的自模仿学习（SIL）方法，称为SPEAR，以在不导致熵崩溃或发散的情况下实现渐进式的探索-利用平衡。该方法通过引入课程来管理探索过程，并利用内在奖励和SIL来促进技能级和动作级的探索，从而提高训练稳定性。

Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic
tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,
yet it faces a fundamental challenge of exploration-exploitation trade-off.
Existing studies stimulate exploration through the lens of policy entropy, but
such mechanical entropy maximization is prone to RL training instability due to
the multi-turn distribution shifting. In this paper, we target the progressive
exploration-exploitation balance under the guidance of the agent own
experiences without succumbing to either entropy collapsing or runaway
divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)
recipe for training agentic LLMs. It extends the vanilla SIL framework, where a
replay buffer stores self-generated promising trajectories for off-policy
update, by gradually steering the policy evolution within a well-balanced range
of entropy across stages. Specifically, our approach incorporates a curriculum
to manage the exploration process, utilizing intrinsic rewards to foster
skill-level exploration and facilitating action-level exploration through SIL.
At first, the auxiliary tool call reward plays a critical role in the
accumulation of tool-use skills, enabling broad exposure to the unfamiliar
distributions of the environment feedback with an upward entropy trend. As
training progresses, self-imitation gets strengthened to exploit existing
successful patterns from replayed experiences for comparative action-level
exploration, accelerating solution iteration without unbounded entropy growth.
To further stabilize training, we recalibrate the advantages of experiences in
the replay buffer to address the potential policy drift. Reugularizations such
as the clipping of tokens with high covariance between probability and
advantage are introduced to the trajectory-level entropy control to curb
over-confidence.

</details>


### [162] [IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning](https://arxiv.org/abs/2509.22621)
*Aayush Mishra,Daniel Khashabi,Anqi Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，通过利用ICL的内部计算来提升SFT的效果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨是否可以利用ICL的内部计算来改善SFT的质量。

Method: 本文引入了ICL Activation Alignment (IA2)，这是一种自蒸馏技术，旨在复制ICL的激活模式，并激励类似ICL的内部推理。

Result: 通过在SFT之前进行IA2作为预处理步骤，显著提高了模型输出的准确性和校准度，在12个流行的基准和2个模型家族上进行了广泛的实证研究。

Conclusion: 本文发现，通过使用ICL的内部计算可以提升SFT的质量，这不仅在实践中很有用，而且为理解模型适应的内部机制提供了概念上的窗口。

Abstract: Supervised Fine-Tuning (SFT) is used to specialize model behavior by training
weights to produce intended target responses for queries. In contrast,
In-Context Learning (ICL) adapts models during inference with instructions or
demonstrations in the prompt. ICL can offer better generalizability and more
calibrated responses compared to SFT in data scarce settings, at the cost of
more inference compute. In this work, we ask the question: Can ICL's internal
computations be used to improve the qualities of SFT? We first show that ICL
and SFT produce distinct activation patterns, indicating that the two methods
achieve adaptation through different functional mechanisms. Motivated by this
observation and to use ICL's rich functionality, we introduce ICL Activation
Alignment (IA2), a self-distillation technique which aims to replicate ICL's
activation patterns in SFT models and incentivizes ICL-like internal reasoning.
Performing IA2 as a priming step before SFT significantly improves the accuracy
and calibration of model outputs, as shown by our extensive empirical results
on 12 popular benchmarks and 2 model families. This finding is not only
practically useful, but also offers a conceptual window into the inner
mechanics of model adaptation.

</details>

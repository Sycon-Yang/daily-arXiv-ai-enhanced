<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: This paper introduces PEACH, a manually aligned English-Arabic healthcare corpus for research and education.


<details>
  <summary>Details</summary>
Motivation: To provide a gold-standard corpus for researchers in contrastive linguistics, translation studies, and natural language processing.

Method: The paper introduces PEACH, a sentence-aligned parallel English-Arabic corpus of healthcare texts.

Result: PEACH contains 51,671 parallel sentences with approximately 590,517 English and 567,707 Arabic word tokens.

Conclusion: PEACH is a publicly accessible, manually aligned corpus that can be used for various research and educational purposes in the field of healthcare texts.

Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [2] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在内容生成、问答、编程和代码推理等方面的有益应用，同时讨论了它们可能无意或故意产生有害、冒犯或有偏见内容的风险。文章提出了一个统一的LLM相关危害和防御分类法，并分析了新兴的多模态和LLM辅助越狱策略，评估了缓解措施，包括基于人类反馈的强化学习（RLHF）、提示工程和安全对齐。研究强调了LLM安全的演变格局，指出了当前评估方法的局限性，并概述了未来的研究方向，以指导开发稳健且符合伦理的语言技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在数字平台上彻底改变了内容创作，提供了自然语言生成和理解的前所未有的能力。然而，它们也可能无意或故意产生有害、冒犯或有偏见的内容。因此，需要对LLMs的安全性和潜在危害进行系统性的研究和分析。

Method: 本文通过系统回顾近期研究，涵盖了无意毒性、对抗性越狱攻击和内容审核技术。文章提出了一个统一的LLM相关危害和防御分类法，并分析了新兴的多模态和LLM辅助越狱策略，评估了缓解措施，包括基于人类反馈的强化学习（RLHF）、提示工程和安全对齐。

Result: 文章提出了一个统一的LLM相关危害和防御分类法，分析了新兴的多模态和LLM辅助越狱策略，并评估了缓解措施，包括基于人类反馈的强化学习（RLHF）、提示工程和安全对齐。研究还强调了LLM安全的演变格局，指出了当前评估方法的局限性，并概述了未来的研究方向。

Conclusion: 本文综述了大型语言模型（LLMs）在内容生成、问答、编程和代码推理等方面的有益应用，同时讨论了它们可能无意或故意产生有害、冒犯或有偏见内容的风险。文章提出了一个统一的LLM相关危害和防御分类法，并分析了新兴的多模态和LLM辅助越狱策略，评估了缓解措施，包括基于人类反馈的强化学习（RLHF）、提示工程和安全对齐。研究强调了LLM安全的演变格局，指出了当前评估方法的局限性，并概述了未来的研究方向，以指导开发稳健且符合伦理的语言技术。

Abstract: Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [3] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

TL;DR: 本文提出了一种细粒度对话事实验证的基准，并展示了当前方法在该任务上的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有的对话系统中的幻觉检测方法过于简化，无法处理混合准确、不准确或不可验证的事实。

Method: 构建了一个基于公开对话数据集的数据集，并使用各种基线方法对其进行评估。

Result: 实验结果表明，结合思维链（CoT）推理的方法可以提高对话事实验证的性能，但在HybriDialogue数据集上最佳F1分数仅为0.75。

Conclusion: 该研究提出了一个名为FineDialFact的基准，用于细粒度对话事实验证，并展示了当前方法在该任务上的挑战性。

Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [4] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

TL;DR: 研究发现，短暂记忆有助于Transformer模型的语言学习，但会影响其预测人类阅读时间的能力。


<details>
  <summary>Details</summary>
Motivation: 研究短暂记忆是否有助于语言学习，特别是在Transformer模型中，因为它们缺乏记忆限制或其他时间偏差。

Method: 通过在受控实验中训练具有和不具有短暂记忆的Transformer语言模型，评估了短暂记忆对语言学习的影响。

Result: 短暂记忆一致地提高了语言学习（通过整体语言建模性能和针对性的句法评估进行量化），但意外地损害了基于意外性的阅读时间预测。

Conclusion: 这些结果支持了记忆限制对神经网络语言学习有益，但对预测行为没有帮助。

Abstract: Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [5] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

TL;DR: 本研究比较了镜像模型和非镜像模型在预测抑郁评分中的性能。镜像模型表现出更大的效果大小，但可能存在偏差。非镜像模型虽然效果大小较小，但可能更具有泛化性。


<details>
  <summary>Details</summary>
Motivation: 许多研究直接从语言响应中开发模型，这些模型存在“准则污染”问题，导致效果大小的人为膨胀并降低模型的泛化能力。因此，需要比较镜像模型和非镜像模型的性能。

Method: 本研究比较了镜像模型和非镜像模型的性能。镜像模型使用结构化诊断数据，而非镜像模型使用生活史数据。GPT-4、GPT-4o和LLaMA3-70B被用来预测结构化诊断访谈的抑郁评分。

Result: 镜像模型表现出非常大的效果大小（例如，R2 = .80），而非镜像模型的效果大小较小但相对较大（例如，R2 = .27）。当镜像模型和非镜像模型预测的结构化访谈抑郁评分与自我报告的抑郁症状相关时，两者表现相同（例如，r = ~.54）。

Conclusion: 在本研究中，镜像语言AI模型在抑郁评估中显示出人为膨胀的效果大小和较差的泛化能力。随着抑郁语言AI模型的不断发展，采用非镜像模型可能会识别出具有独特实用性的可解释和泛化语义特征。

Abstract: A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [6] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

TL;DR: 本文重新审视了EmCom设置，通过引入小词汇量约束和新的指标，探索了受屈折形态学启发的游戏变化，并发现模拟的语音约束促进了拼接形态学，而出现的语言表现出自然语言融合语法属性的趋势。


<details>
  <summary>Details</summary>
Motivation: 现有的EmCom研究主要关注特定子领域的目标和指标，这些指标优先考虑以独特字符一对一表示属性并进行句法组合的通信方案。

Method: 重新解释了一个常见的EmCom设置，即属性-值重构游戏，并施加小词汇量约束以模拟双元音，同时制定了一个类似于自然主义屈折形态学的新设置。

Result: 我们开发了新的指标，并探索了受屈折形态学真实属性（如拼接性和融合性）驱动的此游戏的变化。

Conclusion: 通过实验，我们发现模拟的语音约束鼓励了拼接形态学，并且出现的语言复制了自然语言融合语法属性的趋势。

Abstract: Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [7] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型如何通过认知维度推理情绪，并引入了一个大规模的基准测试CoRE来评估其内部认知结构。研究发现不同模型在情感推理方面有多种模式。


<details>
  <summary>Details</summary>
Motivation: 大多数研究都以监督方式处理与情绪相关的任务，评估或训练大型语言模型的能力，使用与刺激（如文本、图像、视频、音频）相关的情绪标签。然而，这些研究往往局限于标准和表面的情绪相关任务，如识别引发或表达的情绪。本文旨在超越表面情绪任务，研究大型语言模型如何通过认知维度推理情绪。

Method: 我们从认知评价理论出发，检查大型语言模型在处理情绪刺激时是否能产生连贯且合理的认知推理。我们引入了一个大规模的基准测试CoRE来评估大型语言模型用于情感推理的内部认知结构。

Result: 我们的结果和分析揭示了不同大型语言模型在情感推理方面的多样化推理模式。

Conclusion: 我们的研究揭示了不同大型语言模型在情感推理方面的多样化推理模式。我们的基准测试和代码将公开提供。

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [8] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

TL;DR: 本文介绍了Spectrum Projection Score (SPS) 和 xCompress，以提高检索增强生成（RAG）的性能，并提供对检索和生成之间交互的深入理解。


<details>
  <summary>Details</summary>
Motivation: 先前的工作通常整体评估RAG，评估检索器和阅读器联合，使得难以隔离检索的真实贡献，尤其是在使用作为阅读器的LLMs的提示敏感性方面。

Method: 引入了Spectrum Projection Score (SPS)，这是一种轻量级、无监督的度量标准，允许读者通过比较摘要生成的标记区域与读者子空间的主要方向来评估检索摘要的语义对齐度。基于SPS，提出了xCompress，这是一个推理时控制器框架，能够动态采样、排序和压缩检索摘要候选。

Result: 在五个QA基准测试中，使用四个开源LLM进行的广泛实验表明，SPS不仅提高了各种任务的性能，还提供了一个关于检索和生成之间交互的合理视角。

Conclusion: SPS不仅在各种任务中提升了性能，还为检索和生成之间的交互提供了有根据的视角。

Abstract: Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [9] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 本文提出了一种三阶段管道，用于高效、精确地分类亲社会内容，同时减少人工标注和推理成本。


<details>
  <summary>Details</summary>
Motivation: 检测文本中的亲社会行为是一项新兴且重要的挑战，因为缺乏明确的定义和标记数据，需要新的方法来进行标注和部署。

Method: 我们提出了一种实用的三阶段管道，包括使用小种子集的人工标注示例来确定最佳的基于LLM的标注策略，引入人机协作的精炼循环以逐步澄清和扩展任务定义，并合成10k高质量的标注数据并训练一个两阶段的推理系统。

Result: 该管道在减少人工标注工作量和推理成本的同时，实现了高精度（约0.90）的亲社会内容分类，并将推理成本降低了约70%。

Conclusion: 我们的管道展示了如何通过有针对性的人机交互、仔细的任务制定和部署感知的架构设计，为新的负责任的人工智能任务解锁可扩展的解决方案。

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [10] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ATOP的新方法，用于跨话题自动作文评分，通过联合学习话题共享和特定特征来提高评分效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通过源话题和目标话题的分布对齐提取话题共享特征，但常常忽略话题特定特征，限制了评估关键特征如话题符合性的能力。

Method: 提出了一种名为ATOP的新方法，通过优化可学习的、包含共享和特定组件的主题感知提示，从预训练语言模型中激发相关知识，同时在统一的回归和分类框架中引入对抗训练以增强主题共享提示学习的鲁棒性，并使用基于邻居的分类器生成伪标签来指导针对目标主题的特定提示的监督学习。

Result: ATOP在ASAP++数据集上显著优于现有最先进的方法，实现了整体和多特征作文评分的提升。

Conclusion: ATOP在ASAP++数据集上的实验表明，它在整体和多特征作文评分中显著优于现有的最先进方法。

Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [11] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: 通过在DistilBERT模型中引入结构化的注意力稀疏性，发现模型准确性可以显著提高，这表明注意力稀疏性可能是一种提高Transformer模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制的二次计算成本问题，同时探索注意力稀疏性是否会影响模型准确性。

Method: 在SST-2情感分析任务上对DistilBERT模型进行微调时，引入结构化的、后期的注意力稀疏性。

Result: 具有80%注意力稀疏性的模型在验证准确率上达到了91.59%，比密集基线提高了0.97%。

Conclusion: 我们的工作重新定义了注意力稀疏性，不仅作为计算效率的工具，而且作为提高Transformer模型泛化和性能的潜在方法。

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [12] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种改进的自我奖励语言模型方法，通过协调过去、现在和未来的模型生成来维持学习信号，从而在多个任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自我奖励范式存在一个关键限制，即选择和拒绝响应的同步改进逐渐缩小了对比样本之间的表征差异，削弱了有效的偏好学习。

Method: 本文提出了两种策略：(1) 锚定拒绝，使用过去的初始模型输出固定被拒绝的响应；(2) 未来引导选择，利用下一代模型预测动态筛选选择的样本。

Result: 在三个模型家族（Llama、Qwen、Mistral）和不同模型大小（Llama3B/8B/70B）上的实验表明，与使用相同计算资源的自我奖励方法相比，本文的方法取得了显著的性能提升。例如，Llama3.1-8B在AlpacaEval 2.0上达到了29.44的胜率，超过了自我奖励基线（19.69）的9.75。此外，本文的方法在数学推理（GSM8K）、基于知识的问答（ARC、TruthfulQA）和代码生成（HumanEval）任务中也表现出更好的分布外泛化能力。

Conclusion: 本文提出了一种改进的自我奖励语言模型方法，称为时间自奖励语言模型，通过协调过去、现在和未来的模型生成来维持学习信号，从而在多个任务上取得了显著的性能提升。

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [13] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: 本文提出PEEK方法，利用预训练的嵌入模型作为LLMs的代理，以高效预测其知识。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs的随机性，难以预测它们获得了什么知识。现有的方法需要通过底层模型进行前向传递来探测LLMs的知识，这计算成本高且耗时。

Method: 通过各种探测策略确定LLMs已知的事实训练集，然后通过线性解码器层调整嵌入模型以预测LLMs输出。

Result: 在3个维基百科衍生数据集、4个LLMs和7个嵌入模型上的综合评估显示，嵌入模型可以在保留集中以高达90%的准确率预测LLMs的知识。

Conclusion: 知识适应的嵌入可以用来大规模识别LLMs中的知识差距，并提供对LLMs内部归纳偏见的更深入了解。

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [14] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为EvolvR的框架，用于改进大型语言模型在故事评估中的表现。通过自合成和自我过滤的Chain-of-Thought数据，该框架在多个基准测试中取得了最先进的性能，并显著提升了生成故事的质量。


<details>
  <summary>Details</summary>
Motivation: 准确的故事评估对于帮助人类质量判断以及提供关键信号来指导故事生成至关重要。然而，现有方法面临困境：封闭源代码模型的提示工程适应性差，而开放源代码模型的微调方法缺乏故事评估所需的严格推理能力。

Method: 我们提出了Self-Evolving Pairwise Reasoning (EvolvR)框架。该框架基于成对比较，首先通过多角色策略自我合成得分对齐的Chain-of-Thought (CoT)数据。为了确保数据质量，这些原始CoTs经过自我过滤过程，利用多代理来保证其逻辑严谨性和鲁棒性。最后，在精炼数据上训练的评估器被部署为奖励模型来指导故事生成任务。

Result: 实验结果表明，我们的框架在三个评估基准（包括StoryER、HANNA和OpenMEVA）上实现了最先进的性能。此外，当作为奖励模型使用时，它显著提高了生成故事的质量，从而充分验证了我们自进化方法的优势。

Conclusion: 实验结果表明，我们的框架在三个评估基准（包括StoryER、HANNA和OpenMEVA）上实现了最先进的性能。此外，当作为奖励模型使用时，它显著提高了生成故事的质量，从而充分验证了我们自进化方法的优势。

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [15] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Gašper Beguš*

Main category: cs.CL

TL;DR: This paper introduces ConlangCrafter, a multi-hop pipeline that uses LLMs to create conlangs by breaking down the process into modular stages, encouraging diversity and consistency, and showing success in producing varied and coherent languages without expert input.


<details>
  <summary>Details</summary>
Motivation: To leverage modern LLMs as computational creativity aids for end-to-end conlang creation.

Method: ConlangCrafter is a multi-hop pipeline that decomposes language design into modular stages, leveraging LLMs' meta-linguistic reasoning capabilities, injecting randomness to encourage diversity, and using self-refinement feedback to encourage consistency.

Result: ConlangCrafter was evaluated on metrics measuring coherence and typological diversity, demonstrating its ability to produce coherent and varied conlangs without human linguistic expertise.

Conclusion: ConlangCrafter can produce coherent and varied conlangs without human linguistic expertise.

Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [16] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 本文提出了两种有效的抽取式问答方法，用于《古兰经》。其中一种方法利用了指令调优的大语言模型和专门的阿拉伯语提示框架，结合后处理系统以提高精度并减少幻觉。评估结果显示，这种方法在低资源、语义丰富的问答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决《古兰经》文本中复杂语言、独特术语和深层含义相关的问题。

Method: 本文提出了两种有效的抽取式问答（QA）方法，用于《古兰经》。第二种方法使用少样本提示与指令调优的大语言模型，如Gemini和DeepSeek。开发了一个专门的阿拉伯语提示框架用于跨度提取，并集成了子词对齐、重叠抑制和语义过滤的后处理系统。

Result: 评估结果显示，带有阿拉伯语指令的大语言模型优于传统的微调模型。最佳配置在pAP10评分上达到了0.637。

Conclusion: 结果表明，基于提示的指令调优对于低资源、语义丰富的问答任务是有效的。

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [17] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

TL;DR: LogicRAG is a framework that dynamically extracts reasoning structures during inference to improve retrieval-augmented generation without relying on pre-built graphs, resulting in better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing Graph-based RAG methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning, which pre-built graphs may not align with.

Method: LogicRAG dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. It decomposes the input query into subproblems, constructs a DAG to model logical dependencies, linearizes the graph using topological sort, applies graph pruning, and uses context pruning to reduce redundant retrieval and irrelevant context.

Result: Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.

Conclusion: LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.

Abstract: Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [18] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 本研究提出了一种名为AURA的多层框架，通过过程奖励模型实现更安全的推理轨迹，显著提升了模型输出的逻辑完整性和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM面临管理基于功能的安全风险的挑战，即输出无意中促进了有害行为的情况，由于忽略了逻辑含义。传统的安全解决方案，如标量结果奖励模型、参数调整或启发式解码策略，缺乏检测和干预微妙但关键推理步骤所需的细致程度和主动性。

Method: 我们引入了AURA，这是一种创新的多层框架，围绕过程奖励模型（PRMs）构建，提供全面的步骤级评估，涵盖逻辑一致性和安全性意识。框架结合了内省自我批评、细粒度PRM评估和自适应安全意识解码，以动态且主动地引导模型走向更安全的推理轨迹。

Result: 实证证据清楚地表明，这种方法显著超越了现有方法，显著提高了模型输出的逻辑完整性和功能敏感安全性。

Conclusion: 本研究代表了向更安全、更负责任和上下文意识的AI迈出的重要一步，为对齐敏感的应用设定了新基准。

Abstract: Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [19] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: SRD is a novel data curation framework that improves the quality and compatibility of training data for knowledge distillation, leading to better performance and reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing white-box KD methods mainly focus on balancing ground truth and student-generated responses while overlooking two critical factors: training data quality and student-model compatibility. SRD aims to address these limitations by improving data quality and compatibility.

Method: Selective Reflection Distillation (SRD) is proposed, which leverages reflections from student models to refine training data. It dynamically evaluates and selects prompt-response pairs by comparing ground truth data with student model outputs, and employs a curriculum scheduling strategy to incrementally introduce curated subsets into the distillation process.

Result: Experiments on a range of language model benchmarks demonstrate SRD's consistent improvements in distilled model performance, as well as a reduction in training runtime by up to 39%, under diverse KD methods and model families. SRD operates as a plug-and-play module, enhancing sample efficiency without modifying underlying KD algorithms.

Conclusion: SRD provides a principled framework to achieve effective and efficient distillation of LLMs by focusing on data quality and compatibility. It advances the understanding of data-centric factors in KD and offers practical insights for enhancing the capability and efficiency of compressed LLMs.

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [20] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

TL;DR: 我们提出了Big5-Scaler，这是一种基于提示的框架，用于用可控制的五大性格特征对大型语言模型进行条件设置。通过将数值特征值嵌入自然语言提示中，我们的方法能够在不进行额外训练的情况下实现细粒度的性格控制。评估显示，它在不同模型中引发了连贯且可区分的性格特征，性能因提示类型和尺度而异。分析表明，简洁的提示和较低的特质强度有效，为构建具有个性意识的对话代理提供了一种高效的方法。


<details>
  <summary>Details</summary>
Motivation: 为了实现对大型语言模型的细粒度性格控制，同时避免额外的训练过程。

Method: Big5-Scaler，一种基于提示的框架，用于用可控制的五大性格特征对大型语言模型进行条件设置。通过将数值特征值嵌入自然语言提示中，我们的方法能够在不进行额外训练的情况下实现细粒度的性格控制。

Result: 结果表明，它在不同模型中引发了连贯且可区分的性格特征，性能因提示类型和尺度而异。

Conclusion: 我们的分析表明，简洁的提示和较低的特质强度有效，为构建具有个性意识的对话代理提供了一种高效的方法。

Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [21] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的偏差检测方法，用于识别大型语言模型生成过程中的隐性社会偏见。该方法结合了嵌套语义表示和上下文对比机制，能够准确检测不同语义文本之间的偏差差异，并具有高可解释性，适用于需要高可信度生成内容的现实应用场景。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决大型语言模型生成过程中可能出现的隐性刻板印象问题，提出一种可解释的偏差检测方法，以识别模型输出中的隐藏社会偏见，特别是那些难以通过显式语言特征捕捉的语义倾向。

Method: 该方法结合了嵌套语义表示和上下文对比机制，从模型输出的向量空间结构中提取潜在偏差特征。通过注意力权重扰动分析模型对特定社会属性术语的敏感性，从而揭示偏差形成的语义路径。

Result: 实验结果表明，所提出的方法在多个维度上表现出色。它能够准确识别语义相似文本之间的偏差差异，同时保持高语义对齐和输出稳定性。该方法还展示了在结构设计上的高可解释性，有助于揭示语言模型内部的偏差关联机制。

Conclusion: 该方法在多个维度上表现出强大的检测性能，能够准确识别语义相似文本之间的偏差差异，同时保持高语义对齐和输出稳定性。此外，该方法在结构设计上具有高度可解释性，有助于揭示语言模型内部的偏差关联机制，为偏差检测提供了更透明和可靠的技术基础。该方法适用于对生成内容的高可信度有要求的现实应用场景。

Abstract: This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [22] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: TADrop is an adaptive sparsification strategy that improves model merging by tailoring sparsity levels to the structure of model parameters.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of prevailing approaches that use a 'one-size-fits-all' strategy, which overlooks the inherent structural and statistical heterogeneity of model parameters.

Method: TADrop is an adaptive sparsification strategy that assigns a tailored sparsity level to each parameter tensor based on its distributional properties.

Result: TADrop consistently and significantly boosts the performance of foundational, classic, and SOTA merging methods across diverse tasks and models.

Conclusion: TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging.

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [23] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为UR2的通用框架，通过强化学习统一了检索和推理，提高了在多种任务中的适应能力，并在多个基准测试中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG和RL方法通常独立发展，缺乏整合，限制了它们在更广泛领域的泛化能力和适用性。因此，需要一种统一的框架来解决这个问题。

Method: UR2 引入了两个关键贡献：一种难度感知的课程训练，仅在困难问题上选择性地调用检索；以及结合领域特定离线语料库和LLM生成摘要的混合知识访问策略。

Result: UR2 在开放域问答、MMLU-Pro、医学和数学推理任务中表现出色，显著优于现有的RAG和RL方法，并在多个基准测试中达到与GPT-4o-mini和GPT-4.1-mini相当的性能。

Conclusion: UR2 (Unified RAG and Reasoning) 是一个统一检索和推理的通用框架，通过强化学习实现了检索和推理的动态协调。实验表明，UR2在多个任务中表现优异，接近GPT-4o-mini和GPT-4.1-mini的性能。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [24] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

TL;DR: The paper rethinks pragmatics as a dynamic interface for language in social contexts, especially with LLMs. It suggests that traditional theories may not be suitable for machine-centric systems and proposes new frameworks like HMC and context frustration.


<details>
  <summary>Details</summary>
Motivation: The paper aims to refine and methodologically reconsider the understanding of pragmatics in the context of large language models (LLMs).

Method: The paper challenges the traditional semiotic trichotomy and proposes the Human-Machine Communication (HMC) framework. It examines the tension between human-centred pragmatic theories and the machine-centred nature of LLMs, and introduces the concept of context frustration.

Result: The paper proposes the HMC framework as a more suitable alternative to traditional semiotic trichotomy. It highlights the limitations of human-centred pragmatic theories and introduces the concept of context frustration.

Conclusion:  pragmatic theory may need to be adjusted or expanded to better account for communication involving generative AI.

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [25] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: 本文研究了在小数据环境下向大型语言模型注入知识的挑战，并展示了通过生成合成数据来提高学习新事实的能力。研究还揭示了在小数据情况下遗忘现象的复杂性，并确认了基于RAG的方法在知识注入中的敏感性。最后，表明模型可以生成有效的合成训练数据，为自我改进的模型更新提供了可能的路径。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常需要大量文本才能有效获取新知识。然而，在只有几千或几百万个标记的情况下更新模型仍然具有挑战性。因此，本文旨在研究如何在小数据环境下有效地向大型语言模型注入知识。

Method: 本文使用最近的新闻数据集进行实验，评估模型通过问答对学习新信息的能力。从持续预训练基线开始，探索了不同的增强算法来生成合成数据以提高知识获取能力。

Result: 实验结果表明，仅在有限数据上继续预训练会产生适度的改进，而让模型接触多样化的文本变体则显著提高了学习新事实的能力，特别是那些通过多样化提示诱导更大变化的方法。此外，研究还揭示了在小数据情况下遗忘现象的复杂性，并确认了基于RAG的方法在知识注入中的敏感性。最后，表明模型可以生成有效的合成训练数据。

Conclusion: 本文探讨了在小数据环境下向大型语言模型注入知识的挑战，并展示了通过生成合成数据来提高学习新事实的能力。此外，研究还揭示了在小数据情况下遗忘现象的复杂性，并确认了基于RAG的方法在知识注入中的敏感性。最后，表明模型可以生成有效的合成训练数据，为自我改进的模型更新提供了可能的路径。

Abstract: Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [26] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

TL;DR: This paper presents the DKG-LLM framework, which integrates a dynamic knowledge graph with the Grok 3 large language model to improve medical diagnosis and personalized treatment recommendations.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have grown exponentially since the release of ChatGPT. These models have gained attention due to their robust performance on various tasks, including language processing tasks. These models achieve understanding and comprehension of tasks by training billions of parameters. The development of these models is a transformative force in enhancing natural language understanding and has taken a significant step towards artificial general intelligence (AGI).

Method: The DKG-LLM framework introduces a groundbreaking approach to medical diagnosis and personalized treatment recommendations by integrating a dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data (including clinical reports and PubMed articles) and patient records dynamically generate a knowledge graph.

Result: The evaluation results show that DKG-LLM achieves a diagnostic accuracy of 84.19%. The model also has a treatment recommendation accuracy of 89.63% and a semantic coverage of 93.48%.

Conclusion: DKG-LLM is a reliable and transformative tool that handles noisy data and complex multi-symptom diseases, along with feedback-based learning from physician input.

Abstract: Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [27] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

TL;DR: This paper introduces SceneJailEval, a scenario-adaptive multi-dimensional framework for jailbreak evaluation, and presents a comprehensive dataset for evaluating jailbreaks in various scenarios.


<details>
  <summary>Details</summary>
Motivation: Current approaches to jailbreak evaluation have limitations, such as binary classification labels without quantifying harm intensity and uniform evaluation criteria that do not fit all scenarios.

Method: Introduce SceneJailEval, a scenario-adaptive multi-dimensional framework for jailbreak evaluation, and create a comprehensive 14-scenario dataset with diverse jailbreak variants and regional cases.

Result: SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA).

Conclusion: SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA), surpassing accuracy limits of existing evaluation methods in heterogeneous scenarios and confirming its advantage.

Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [28] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 本文提出了一种情感智力分类法和评估基准，发现现有模型在情感推理方面存在局限，并建议采用针对性的数据和方法进行改进。


<details>
  <summary>Details</summary>
Motivation: 情感智力是开发人类对齐的大规模语言模型中一个关键但研究不足的维度，因此需要一种系统的方法来评估和提升模型的情感智力能力。

Method: 引入了一个统一的心理学基础的四层情感智力分类法，并构建了EICAP-Bench基准测试来评估大型语言模型的情感智力能力。此外，还通过LoRA适配器在UltraChat数据集上对模型进行了微调。

Result: 在EICAP-Bench基准测试中，Qwen2.5-Instruct表现最佳。微调结果显示，只有评估层在基于UltraChat的微调中显著提高。

Conclusion: 研究结果突显了现有预训练和指令调优范式的局限性，强调了需要有针对性的数据和建模策略来实现全面的情感智力对齐。

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [29] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: 本文提出了一种基于检索增强生成（RAG）的分类方法，用于内容审核，能够在不重新训练模型的情况下动态更新政策，并提供更高的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 内容审核需要能够快速适应不断变化的政策而无需昂贵的重新训练的分类系统。

Method: 使用检索增强生成（RAG）进行分类，将传统的分类任务从根据预训练参数确定正确类别转变为在推理时评估内容与上下文知识的关系。

Result: CPE展示了这种方法，并提供了三个主要优势：(1) 与领先的商业系统相当的稳健分类准确性，(2) 通过检索到的政策段实现内在可解释性，(3) 动态政策更新而无需模型重新训练。实验表明该系统可以在不重新训练或损害整体性能的情况下正确调整对特定身份群体的保护。

Conclusion: RAG可以将分类任务转化为更灵活、透明和可适应的内容审核过程，以及更广泛的分类问题。

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [30] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文介绍了InfoCausalQA，一个用于评估基于信息图的因果推理能力的新基准，并指出当前视觉语言模型在这一领域存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在因果推理方面的能力仍不充分，特别是在多模态设置中。需要一个基准来评估和提升这些能力。

Method: 引入了InfoCausalQA基准，用于评估基于信息图的因果推理能力，包含两个任务：定量因果推理和语义因果推理。

Result: 实验结果表明，当前的VLMs在计算推理和语义因果推理方面表现有限，与人类相比存在显著差距。

Conclusion: 通过InfoCausalQA，我们强调了提升多模态AI系统因果推理能力的必要性。

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [31] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: 本文研究了老年人德国说话者语音命令的意图识别，提出了一种结合调整后的Whisper ASR模型和基于Transformer的语言模型的新方法，并利用LLM生成的合成数据进行训练。结果表明，生成式AI可以有效填补低资源领域的数据缺口。


<details>
  <summary>Details</summary>
Motivation: 现有的意图识别方法主要针对英语短命令，而本文旨在解决这一限制，专注于老年人德国说话者的语音命令意图识别。

Method: 我们提出了一种新方法，将调整后的Whisper ASR模型与基于Transformer的语言模型相结合，这些语言模型在由三个著名的大语言模型（LLMs）生成的合成文本数据集上进行了训练。

Result: 结果表明，由LLM生成的合成数据显著提高了分类性能，并增强了对不同说话风格和未见过的词汇的鲁棒性。值得注意的是，较小的领域特定13B LLM LeoLM在德语意图识别的数据集质量方面超过了更大的ChatGPT (175B)。

Conclusion: 我们的方法表明，生成式AI可以有效地填补低资源领域中的数据缺口。我们提供了详细的数据生成和训练过程文档，以确保透明度和可重复性。

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [32] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

TL;DR: MDIR 是一种基于矩阵分析和大偏差理论的新方法，用于检测大型语言模型的侵权行为，具有高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 侵权检测方法在准确重建权重对应关系、计算统计显著性度量（如 p 值）以及避免将训练数据相似的模型错误标记为相关方面存在不足。

Method: MDIR 利用矩阵分析和大偏差理论来准确重建权重关系，并提供严格的 p 值估计。

Result: 实验结果表明，即使经过广泛的变换（如随机排列和持续预训练），MDIR 仍能可靠地检测侵权行为。

Conclusion: MDIR 是一种高效且易于使用的检测 LLM 侵权的方法，能够在单台个人电脑上快速完成检测任务。

Abstract: In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [33] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: 本文提出DynamicTRF框架，通过设计特定的TRF和引入GRE度量，提升了大型多模态模型在零样本图问答任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的单一类型图表示方法（如TRF）未能考虑不同模型或任务的具体偏好，导致回答不准确或过于冗长。

Method: 首先分析了现有TRF的特征和弱点，设计了一组针对零样本图问答的TRF，引入了Graph Response Efficiency (GRE)度量，构建了DynamicTRF框架，并训练了一个TRF路由器以适应性地为每个问题分配最佳TRF。

Result: 在7个领域内算法图问答任务和2个领域外下游任务的广泛实验中，DynamicTRF显著提高了LMMs的零样本图问答性能。

Conclusion: DynamicTRF显著提升了大型多模态模型在零样本图问答任务中的准确性。

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [34] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本研究探讨了将攻击性检测作为辅助任务以提升大型语言模型在检测网络欺凌方面的性能，结果显示增强提示管道方法优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 由于网络欺凌在社交媒体上的表达方式微妙且多样化，检测网络欺凌仍然是一个关键挑战。因此，需要探索有效的策略来提高大型语言模型在这一任务上的表现。

Method: 本研究通过在五个攻击性数据集和一个网络欺凌数据集上使用指令调优的大型语言模型，评估了多种策略，包括零样本、少样本、独立LoRA微调和多任务学习（MTL）。针对MTL结果不一致的问题，提出了一种增强提示管道方法，将攻击性预测嵌入到网络欺凌检测提示中以提供上下文增强。

Result: 初步结果表明，增强提示管道方法在标准LoRA微调方法上表现更优，表明基于攻击性的上下文显著提升了网络欺凌检测的效果。

Conclusion: 本研究表明，将攻击性检测作为辅助任务整合到统一训练框架中，可以显著提升大型语言模型在网络欺凌检测中的泛化能力和性能。

Abstract: Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [35] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: 本文研究了风格个性化的文本生成任务的评估方法，质疑了传统指标的有效性，并提出了使用多种评估指标集合的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究已经构建了工具和基准来实现风格个性化的文本生成，但在低资源作者风格个性化文本生成空间中的评估探索仍然有限。

Method: 我们评估了这些指标及其集合，并使用我们的风格区分基准进行了测试，该基准涵盖了八个写作任务，并在三个设置中进行评估：领域区分、作者身份识别以及LLM个性化与非个性化区分。

Result: 我们质疑了广泛采用的评估指标如BLEU和ROUGE的有效性，并探索了其他评估范式，如风格嵌入和LLM作为评判者，以全面评估风格个性化的文本生成任务。

Conclusion: 我们提供了确凿的证据，证明采用多样化的评估指标集合可以有效地评估风格个性化的文本生成。

Abstract: While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [36] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

TL;DR: 本文介绍了ChatAnime，这是第一个情感支持角色扮演（ESRP）数据集。通过选择20个顶级动漫角色并设计60个情绪相关的真实场景问题，我们收集了10个大语言模型和40名中国动漫爱好者的对话数据。评估结果显示，顶级大语言模型在角色扮演和情感支持方面优于人类粉丝，但在响应多样性方面仍落后于人类。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在角色扮演对话和提供情感支持方面表现出色，但将这些能力结合起来以实现与虚拟角色的情感支持互动仍存在显著的研究空白。为了填补这一研究空白，我们以动漫角色为例，因为它们有明确的人格和庞大的粉丝基础。这个选择使我们能够有效地评估LLMs在保持特定角色特征的同时提供情感支持的能力。

Method: 我们引入了ChatAnime，这是第一个情感支持角色扮演（ESRP）数据集。我们首先精心挑选了来自流行动漫社区的20个顶级角色，并设计了60个以情绪为中心的真实世界情景问题。然后，我们进行全国选拔过程，以确定40名具有深厚特定角色知识和丰富角色扮演经验的中国动漫爱好者。接下来，我们系统地从10个大语言模型和这40名中国动漫爱好者中收集了两轮对话数据。为了评估大语言模型的ESRP性能，我们设计了一个以用户体验为导向的评估系统，包括三个维度：基本对话、角色扮演和情感支持，以及一个用于响应多样性的总体指标。

Result: 实验结果表明，顶级大语言模型在角色扮演和情感支持方面超过了人类粉丝，而人类在响应多样性方面仍占优势。

Conclusion: 实验结果表明，顶级大语言模型在角色扮演和情感支持方面超过了人类粉丝，而人类在响应多样性方面仍占优势。我们希望这项工作能为未来优化大语言模型在ESRP方面的研究提供有价值的资源和见解。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [37] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

TL;DR: 本文提出了 SecMCP，一种安全框架，用于检测和量化对话漂移，提高 MCP 的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施（如基于规则的过滤器或 LLM 驱动的检测）由于依赖静态签名、计算效率低下和无法量化对话劫持而不足。

Method: SecMCP 通过在潜在多面体空间中建模 LLM 激活向量，识别对话动态中的异常变化，以主动检测劫持、误导和数据泄露。

Result: SecMCP 在三个最先进的 LLM 上进行了评估，展示了具有 AUROC 分数超过 0.915 的强大检测能力，同时保持了系统的可用性。

Conclusion: SecMCP 提供了一种有效的框架，能够检测和量化对话漂移，从而提高 MCP 的安全性。

Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [38] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文研究了如何赋予代理可学习、可更新和终身的程序性记忆。提出Memp方法，将过去代理轨迹提炼为细粒度指令和高级抽象，并探索构建、检索和更新策略。结合动态制度，使存储库随新经验进化。实验证明，随着记忆库的完善，代理在类似任务上的成功率和效率提高，且由更强模型构建的记忆迁移至弱模型也能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）基于代理在各种任务中表现出色，但它们的程序性记忆脆弱且手动工程或纠缠在静态参数中。本文研究了赋予代理可学习、可更新和终身程序性记忆的策略。

Method: 提出Memp，将过去的代理轨迹提炼为细粒度的逐步指令和更高级的脚本式抽象，并探索了构建、检索和更新程序性记忆的不同策略。结合动态制度，持续更新、纠正和淘汰内容，使存储库与新经验同步进化。

Result: 在TravelPlanner和ALFWorld上的实证评估表明，随着记忆库的改进，代理在类似任务上的成功率和效率不断提高。此外，由更强模型构建的程序性记忆保留其价值：将其迁移到较弱模型上能带来显著的性能提升。

Conclusion: 通过不断更新、纠正和淘汰内容，记忆库会随着新经验同步进化。实验证明，随着记忆库的完善，代理在类似任务上的成功率和效率稳步提高。此外，由更强模型构建的程序性记忆保留其价值：将程序性记忆迁移到较弱模型上能显著提升性能。

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [39] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: 研究发现，通过少量语言的微调，大型语言模型可以在未见过的语言中可靠地分类与移民相关的内容。然而，识别推文是支持还是反对移民立场则需要多语言微调。预训练偏差偏向于主导语言，但即使在微调期间对较少代表的语言进行最少接触也能显著提高性能。这些发现挑战了跨语言掌握需要大量多语言训练的假设。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型在少数语言上微调后能否转移到在预训练期间出现的未见语言。同时，研究还关注如何纠正预训练偏差，以及如何实现跨语言主题检测。

Method: 研究对轻量级LLaMA 3.2-3B模型进行了单语、双语或多语言数据集的微调，以分类来自X/Twitter的13种语言的与移民相关的推文。评估了最小语言特定微调是否能实现跨语言主题检测，并探讨了添加目标语言是否能纠正预训练偏差。

Result: 结果表明，经过一个或两个语言微调的大型语言模型可以可靠地在未见过的语言中分类与移民相关的内容。然而，识别推文是支持还是反对移民立场则需要多语言微调。预训练偏差偏向于主导语言，但即使在微调期间对较少代表的语言进行最少接触也能显著提高性能。

Conclusion: 研究发现，通过少量语言的微调，大型语言模型可以在未见过的语言中可靠地分类与移民相关的内容。然而，识别推文是支持还是反对移民立场则受益于多语言微调。预训练偏差偏向于主导语言，但在微调期间对较少代表的语言进行最少接触（少至原始预训练标记量的9.62×10^{-11}）就能显著提高性能。这些发现挑战了跨语言掌握需要大量多语言训练的假设：有限的语言覆盖足以实现主题级别的泛化，结构偏差可以通过轻量级干预进行纠正。

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [40] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: 研究发现，生成式AI在新闻报道中的使用显著增加，尤其是在地方和学院媒体中。虽然AI提高了词汇丰富度和可读性，但降低了正式性，导致写作风格更加统一。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨生成式AI对新闻完整性和作者身份的影响，并评估其在新闻报道中的使用情况。

Method: 研究使用了三种先进的AI文本检测器（如Binoculars、Fast-Detect GPT和GPTZero），分析了超过40,000篇来自主要、地方和学院新闻媒体的新闻文章。

Result: 研究发现，近年来生成式AI的使用显著增加，特别是在地方和学院新闻中。句子级别的分析表明，LLM通常用于新闻的引言部分，而结论部分则通常由人工撰写。语言分析显示，生成式AI提高了词汇丰富度和可读性，但降低了正式性，导致写作风格更加统一，尤其是在地方媒体中。

Conclusion: 研究发现，生成式AI在新闻报道中的使用显著增加，尤其是在地方和学院媒体中。虽然AI提高了词汇丰富度和可读性，但降低了正式性，导致写作风格更加统一。

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [41] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: SlimInfer是一种新的推理框架，通过动态剪枝机制加速大语言模型的推理，显著提升效率且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法虽然优化了注意力计算，但仍然在每一层处理完整的隐藏状态，限制了整体效率。因此，需要一种更高效的推理框架。

Method: SlimInfer通过在前向传播过程中直接剪枝不重要的提示令牌来加速推理。它引入了一种动态细粒度剪枝机制，在中间层准确移除冗余的隐藏状态令牌，并利用分层剪枝实现异步KV缓存管理。

Result: 实验表明，SlimInfer在单个RTX 4090上可以实现高达2.53倍的首次token时间（TTFT）加速和1.88倍的端到端延迟减少，同时在LongBench上保持性能。

Conclusion: SlimInfer能够显著加速大语言模型的推理过程，同时保持性能不下降。

Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [42] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: GLM-4.5 is an open-source MoE model with 355B parameters that performs well on various tasks and provides a compact version for research.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and high-performing open-source Mixture-of-Experts (MoE) large language model that supports hybrid reasoning methods for agentic and reasoning tasks.

Method: Multi-stage training on 23T tokens, comprehensive post-training with expert model iteration and reinforcement learning.

Result: GLM-4.5 scores 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified, ranking 3rd overall and 2nd on agentic benchmarks.

Conclusion: GLM-4.5 achieves strong performance across various tasks and ranks highly compared to competitors, while also providing a compact version for broader research applications.

Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [43] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: 本文提出了HapticLLaMA，一种用于触觉描述的多模态语言模型，能够将振动信号转化为自然语言描述。通过两种触觉分词器和两阶段训练方法，HapticLLaMA在自动指标和人类评估中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 之前的研究主要集中在视觉和音频上，而触觉信号（触觉感）仍然研究不足。为了弥补这一差距，我们正式定义了触觉描述任务。

Method: 我们提出了HapticLLaMA，这是一种多模态感觉语言模型，它将振动信号解释为给定感觉、情感或联想类别的描述。我们研究了两种类型的触觉分词器，并在两个阶段对HapticLLaMA进行了训练：(1) 使用LoRA-based adaptation的监督微调，以及(2) 通过人类反馈的强化学习(RLHF)进行微调。

Result: HapticLLaMA在生成描述方面表现出色，分别获得了59.98的METEOR分数和32.06的BLEU-4分数。此外，超过61%的生成描述在7分制中获得高于3.5的评分，RLHF使整体评分分布提高了10%，表明与人类触觉感知更一致。

Conclusion: 这些发现突显了大型语言模型处理和适应感官数据的潜力。

Abstract: Haptic captioning is the task of generating natural language descriptions
from haptic signals, such as vibrations, for use in virtual reality,
accessibility, and rehabilitation applications. While previous multimodal
research has focused primarily on vision and audio, haptic signals for the
sense of touch remain underexplored. To address this gap, we formalize the
haptic captioning task and propose HapticLLaMA, a multimodal sensory language
model that interprets vibration signals into descriptions in a given sensory,
emotional, or associative category. We investigate two types of haptic
tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that
convert haptic signals into sequences of discrete units, enabling their
integration with the LLaMA model. HapticLLaMA is trained in two stages: (1)
supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,
and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We
assess HapticLLaMA's captioning performance using both automated n-gram metrics
and human evaluation. HapticLLaMA demonstrates strong capability in
interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a
BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated
captions received human ratings above 3.5 on a 7-point scale, with RLHF
yielding a 10% improvement in the overall rating distribution, indicating
stronger alignment with human haptic perception. These findings highlight the
potential of large language models to process and adapt to sensory data.

</details>


### [44] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: 本文提出一种后训练方法，使LLM能够更好地形成临时惯例，并在两个新基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，LLM不会自然地表现出这种行为，而人类在多轮互动中能更有效地沟通并形成临时惯例。

Method: 我们开发了一种后训练过程，通过针对性的微调来培养这种能力，使用启发式识别的示范数据。

Result: 后训练的LLM在两个新的基准测试中表现出显著改进的惯例形成能力。

Conclusion: 我们的研究显示，通过后训练过程，LLM在两个评估方法中都显著提高了形成惯例的能力。

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1是一种统一框架，通过结合预训练的多模态大语言模型和扩散模型，实现了高效且高质量的图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接训练大语言模型或连接大语言模型和扩散模型时，由于大语言模型在预训练期间未见过图像表示，导致训练成本高昂。因此需要一种更高效的解决方案。

Method: Bifrost-1使用了基于补丁级别的CLIP图像嵌入作为潜在变量，将这些嵌入整合到扩散模型中，并为多模态大语言模型配备了初始化自原始参数的视觉生成分支。

Result: Bifrost-1在视觉保真度和多模态理解方面表现出与之前方法相当或更好的性能，并且在训练过程中计算成本显著降低。

Conclusion: Bifrost-1框架通过将预训练的多模态大语言模型和扩散模型无缝集成，实现了高质量且可控的图像生成，并在训练过程中显著降低了计算成本。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [46] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: 本研究通过设计一个五步数据合成管道，引入了有效的图表数据集（ECD），显著提高了多模态大语言模型在图表理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在挑战性基准测试中的成功率仍然较低，而之前的微调研究受限于合成图表与真实图表的相似性不足。

Method: 我们设计了一个五步数据合成管道，包括分离数据和功能创建、基于先前子图生成后续子图、视觉多样化生成的图表、过滤低质量数据以及使用GPT-4o生成问题答案对。

Result: 我们引入了有效的图表数据集（ECD），包含10k+图表图像和300k+问答对，覆盖25个主题，具有250+种图表类型组合，视觉复杂度高。

Conclusion: 我们展示了ECD数据集可以持续提高各种MLLM在真实世界和合成测试集上的性能。

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [47] [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
*Andrew Brown,Muhammad Roman,Barry Devereux*

Main category: cs.DL

TL;DR: 这篇综述分析了2020年至2025年5月期间关于检索增强生成（RAG）的高引用研究，明确了当前的研究现状并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供对检索增强生成（RAG）研究文献的聚焦分析，以明确当前的研究状况并指出未来的研究方向。

Method: 本文采用PRISMA 2020框架，通过明确的纳入和排除标准，对2020年至2025年5月期间高度引用的研究进行了系统回顾。

Result: 共纳入128篇符合标准的文章，涵盖了数据集、架构和评估实践，并综合分析了RAG的有效性和局限性。

Conclusion: 这篇综述明确了当前的研究现状，指出了方法上的差距，并为未来的研究指明了优先方向。

Abstract: This systematic review of the research literature on retrieval-augmented
generation (RAG) provides a focused analysis of the most highly cited studies
published between 2020 and May 2025. A total of 128 articles met our inclusion
criteria. The records were retrieved from ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).
RAG couples a neural retriever with a generative language model, grounding
output in up-to-date, non-parametric memory while retaining the semantic
generalisation stored in model weights. Guided by the PRISMA 2020 framework, we
(i) specify explicit inclusion and exclusion criteria based on citation count
and research questions, (ii) catalogue datasets, architectures, and evaluation
practices, and (iii) synthesise empirical evidence on the effectiveness and
limitations of RAG. To mitigate citation-lag bias, we applied a lower
citation-count threshold to papers published in 2025 so that emerging
breakthroughs with naturally fewer citations were still captured. This review
clarifies the current research landscape, highlights methodological gaps, and
charts priority directions for future research.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [48] [Basic interactive algorithms: Preview](https://arxiv.org/abs/2508.05798)
*Yuri Gurevich*

Main category: cs.LO

TL;DR: 本文探讨了算法概念的发展，并提出了对基本交互算法公理化的预览。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨算法概念的扩展以及如何将不同类型的算法视为基本算法的变体，从而进一步理解Church-Turing论题的不同版本。

Method: 本文通过分析算法概念的历史发展，探讨了基本算法的公理化及其与各种算法（如概率算法、量子算法等）的关系。

Result: 本文展示了基本算法可以被看作具有适当预言机的算法，并解释了如何将非确定性和概率算法以及量子电路算法纳入基本算法的框架中。

Conclusion: 本文提出了对基本交互算法公理化的未来工作的预览和初步探讨。

Abstract: This dialog paper offers a preview and provides a foretaste of an upcoming
work on the axiomatization of basic interactive algorithms.
  The modern notion of algorithm was elucidated in the 1930s--1950s. It was
axiomatized a quarter of a century ago as the notion of ``sequential
algorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm"
now. The axiomatization was used to show that for every basic algorithm there
is a behaviorally equivalent abstract state machine. It was also used to prove
the Church-Turing thesis as it has been understood by the logicians.
  Starting from the 1960s, the notion of algorithm has expanded --
probabilistic algorithms, quantum algorithms, etc. -- prompting introduction of
a much more ambitious version of the Church-Turing thesis commonly known as the
``physical thesis.'' We emphasize the difference between the two versions of
the Church-Turing thesis and illustrate how nondeterministic and probabilistic
algorithms can be viewed as basic algorithms with appropriate oracles. The same
view applies to quantum circuit algorithms and many other classes of
algorithms.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [49] [NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](https://arxiv.org/abs/2508.05835)
*Edresson Casanova,Paarth Neekhara,Ryan Langman,Shehzeen Hussain,Subhankar Ghosh,Xuesong Yang,Ante Jukić,Jason Li,Boris Ginsburg*

Main category: eess.AS

TL;DR: 本文介绍了 NanoCodec，这是一种先进的音频编解码器，在低帧率下实现了高质量的压缩，并在各种比特率范围内优于相关工作，为低延迟和高效的语音 LLM 训练和推理设定了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有的音频编解码器通常在高帧率下运行，导致训练和推理速度较慢，特别是对于自回归模型。因此，需要一种低帧率的音频编解码器，以减少生成一秒钟音频所需的自回归步骤数。

Method: 我们进行了消融研究，以检查帧率、比特率和因果性对编解码器重建质量的影响。基于我们的发现，我们引入了 NanoCodec，这是一种先进的音频编解码器，能够在低帧率下实现高质量的压缩。

Result: NanoCodec 在仅 12.5 帧每秒 (FPS) 的情况下实现了高质量的压缩，并在各种比特率范围内优于相关工作，为低延迟和高效的语音 LLM 训练和推理设定了新基准。

Conclusion: NanoCodec 是一种先进的音频编解码器，在仅 12.5 帧每秒 (FPS) 的情况下实现了高质量的压缩。NanoCodec 在各种比特率范围内优于相关工作，为低延迟和高效的语音 LLM 训练和推理设定了新基准。

Abstract: Large Language Models (LLMs) have significantly advanced audio processing by
leveraging audio codecs to discretize audio into tokens, enabling the
application of language modeling techniques to speech data. However, existing
audio codecs often operate at high frame rates, leading to slow training and
inference, particularly for autoregressive models. To address this, there is
growing interest in low frame-rate audio codecs, which reduce the number of
autoregressive steps required to generate one second of audio. In this paper,
we conduct ablation studies to examine the impact of frame rate, bitrate, and
causality on codec reconstruction quality. Based on our findings, we introduce
NanoCodec, a state-of-the-art audio codec that achieves high-quality
compression at just 12.5 frames per second (FPS). NanoCodec outperforms related
works across various bitrate ranges, establishing a new benchmark for
low-latency and efficient Speech LLM training and inference.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: AttriLens-Mol是一种基于属性的强化学习框架，用于分子属性预测，能够有效激发更相关和预测性的分子属性，提高模型的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在分子属性预测任务中依赖于人工设计的提示和思维链模板，而AttriLens-Mol旨在通过属性引导的强化学习方法提高模型的推理效率和相关性。

Method: AttriLens-Mol是一种基于属性的强化学习框架，通过格式奖励、计数奖励和合理性奖励来引导模型的推理过程。

Result: 在分布内和分布外数据集上的实验表明，使用AttriLens-Mol方法训练的模型在性能上显著提升，结果与监督微调模型和先进模型相当或更好。此外，提取的属性作为可解释决策树模型的特征时表现优于通过提示LLMs生成的属性。

Conclusion: AttriLens-Mol有效地激发了更相关和预测性的分子属性，从而提高了属性预测的可解释性和性能。

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [51] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR is a plugin designed to enhance sample efficiency in preference-based optimization frameworks for LLMs, addressing issues like low sample efficiency and primacy bias.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of low sample efficiency and susceptibility to primacy bias in post-training Large Language Models (LLMs), particularly through Reinforcement Learning (RL) and preference optimization methods.

Method: LoRR core mechanism enables training at a high replay number, maximizing the utility of each collected data batch. It incorporates a periodic reset strategy with reusing initial data, which preserves network plasticity. Furthermore, it leverages a hybrid optimization objective, combining supervised fine-tuning (SFT) and preference-based losses to further bolster data exploitation.

Result: LoRR significantly boosts the performance of various preference optimization methods on both mathematical and general reasoning benchmarks. An iterative DPO approach augmented with LoRR achieves comparable performance on challenging math tasks, outperforming some complex and computationally intensive RL-based algorithms.

Conclusion: LoRR offers a practical, sample-efficient, and highly effective paradigm for LLM finetuning, unlocking greater performance from limited data.

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [52] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: 本文提出了一种名为DINA的新框架，用于同时应对内部标签损坏和外部对抗扰动，实验证明其在真实数据集上的表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和生成式AI越来越多地被集成到客户服务和内容审核应用中，来自外部操控和内部标签损坏的对抗性威胁日益显现。因此，需要一种能够应对这两种威胁的方法。

Method: 我们引入了DINA（Dual Defense Against Internal Noise and Adversarial Attacks），这是一种针对NLP的新型统一框架。该方法从计算机视觉中借鉴了先进的噪声标签学习方法，并将其与对抗训练相结合，以同时缓解内部标签破坏和外部对抗扰动。

Result: 在在线游戏服务的真实数据集上进行的大量实验表明，与基线模型相比，DINA显著提高了模型的鲁棒性和准确性。

Conclusion: 我们的研究结果表明，DINA框架在现实中的对抗场景中显著提高了模型的鲁棒性和准确性，并强调了双重威胁防御的重要性。此外，这为公平和负责任的人工智能部署提供了实际策略。

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [53] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: 本文提出了一种名为DMFI的双模态框架，用于内部威胁检测。该框架结合了语义推理和行为感知微调，通过两个结构化视图（语义视图和行为抽象）来处理日志数据，并利用LoRA增强的LLM进行微调和融合。此外，还引入了DMFI-B策略以提高在严重类别不平衡下的鲁棒性。实验结果表明，DMFI在检测准确性方面优于现有方法，提供了一个可扩展且有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 内部威胁检测（ITD）由于恶意内部行为的细微、长期和依赖上下文的性质，在网络安全中是一个持续且高影响的挑战。传统模型往往难以捕捉语义意图和复杂的行为动态，而现有的基于LLM的解决方案在提示适应性和模态覆盖方面存在局限。为了弥合这一差距，我们提出了DMFI，一个双模态框架，将语义推理与行为感知微调相结合。

Method: 我们提出了DMFI，一个双模态框架，将语义推理与行为感知微调相结合。DMFI将原始日志转换为两个结构化视图：(1) 语义视图，使用指令格式提示处理内容丰富的工件（例如，电子邮件、https）；(2) 行为抽象，通过4W引导（何时-何地-何事-何人）转换来编码上下文动作序列。两个LoRA增强的LLM被独立微调，并通过轻量级MLP决策模块融合输出。我们进一步引入了DMFI-B，一种区分性适应策略，以分离正常和异常行为表示，提高在严重类别不平衡下的鲁棒性。

Result: 在CERT r4.2和r5.2数据集上的实验表明，DMFI在检测准确性方面优于最先进的方法。

Conclusion: 我们的方法结合了LLM的语义推理能力和结构化的行为建模，为现实世界的内部威胁检测提供了一个可扩展且有效的解决方案。

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [54] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [55] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: 本文介绍了ScamAgent，一个基于大型语言模型的自主多轮代理，能够生成高度真实的诈骗电话脚本。研究发现，现有的LLM安全防护措施在面对这种基于代理的威胁时无效，并强调了需要多轮安全审计和新的方法来检测和干扰由生成式AI驱动的对话欺骗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）展示了令人印象深刻的流畅性和推理能力，但它们被滥用的潜力引发了越来越多的关注。

Method: 我们提出了ScamAgent，这是一个基于大型语言模型的自主多轮代理，能够生成高度真实的诈骗电话脚本，模拟现实世界的欺诈场景。

Result: 我们展示了当前的LLM安全防护措施，包括拒绝机制和内容过滤器，在这种基于代理的威胁面前是无效的。即使具有强大提示级防护的模型也可以在提示被分解、伪装或在代理框架内逐步传递时被绕过。

Conclusion: 我们的研究结果突显了对多轮安全审计、代理级控制框架以及新的检测和干扰由生成式AI驱动的对话欺骗的方法的迫切需求。

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [56] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: 本研究评估了多种技术，包括查询重写、RAG融合、关键词增强、意图识别和上下文重排序，以提高电力领域客户支持系统的性能。最终系统结合了意图识别、RAG融合和重排序，在两个数据集上均取得了显著优于基线RAG模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 许多AI客服系统使用标准NLP流水线或微调的语言模型，这些方法在处理模糊、多意图或特定细节的查询时往往不足。

Method: 本案例研究评估了最近的技术：查询重写、RAG融合、关键词增强、意图识别和上下文重排序，以构建一个稳健的电力领域客户支持系统。比较了基于向量存储和基于图的RAG框架，最终选择了基于图的RAG，因为它在处理复杂查询方面表现更优。

Result: 查询重写提高了使用非标准术语或需要精确细节的查询的检索效果。RAG融合通过合并多个检索提升了模糊或多方面查询的性能。重排序通过过滤不相关的上下文减少了幻觉。意图识别通过将复杂问题分解为更针对性的子查询，提高了相关性和效率。相比之下，关键词增强由于选择有偏的关键词而对结果产生了负面影响。

Conclusion: 我们的最终系统结合了意图识别、RAG Fusion和重排序，能够处理歧义和多源查询。在GPT-4生成的数据集和真实电力公司FAQ数据集上分别达到了97.9%和89.6%的准确率，显著优于基线RAG模型。

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [57] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: 本文是对基于大型语言模型的搜索代理的首次系统分析，涵盖了架构、优化、应用和评估等方面，并指出了该领域的关键挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的出现，基于LLM的搜索代理标志着向更深层次、动态、自主信息检索的重要转变。本文旨在提供对搜索代理的系统性分析。

Method: 本文从架构、优化、应用和评估的角度对现有工作进行了全面分析和分类。

Result: 本文提供了对搜索代理的首次系统分析，并对其现有工作进行了全面分析和分类。

Conclusion: 本文对搜索代理进行了系统的分析和分类，并指出了该领域中的关键开放挑战，同时提出了有前景的未来研究方向。

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [58] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: 本研究提出一种微调的视觉语言模型，用于从财务文档中准确提取并生成Markdown表格结构，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 准确提取和表示财务文档中的表格结构对于监管和分析用例至关重要，但这一任务受到旋转布局、多级标题和隐式结构线索的挑战。

Method: 基于Qwen2.5-VL-7B的微调视觉语言模型（VLM），使用LoRA进行监督微调，并通过数据增强生成Markdown格式。

Result: 模型在基于标准的评估中达到92.20%的整体准确性，在基于Markdown树编辑距离相似性（TEDS）的评估中达到96.53%的分数，显著优于基线模型和大型VLM。

Conclusion: 领域特定的微调提供了一种有效且高效的方法，可以弥合非结构化财务文档与下游自动化之间的差距，其性能可与更大、更通用的模型相媲美，而无需其计算开销。

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 本文探讨了智能编码系统需要生成清晰解释的重要性，并提出了神经符号方法作为改进现有技术的潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的方法（包括形式化验证、静态分析和事后可解释性）存在局限性，无法满足非专家用户对透明度和可用性的需求。

Method: 本文提出了一种基于神经符号的方法，其中符号约束在训练期间指导模型行为，并通过神经表示丰富程序语义，从而在推理时实现自动一致性检查。

Result: 本文强调了两个关键的解释属性——认知对齐和语义忠实性，并探讨了神经符号方法在解释生成中的潜力。

Conclusion: 本文认为，智能编码系统不仅应生成代码，还应生成清晰、一致的解释，以弥合模型推理和用户理解之间的差距。

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: This paper introduces AEPO, a policy optimization framework that improves semantic alignment in MLLMs by enhancing exploration through a multi-answer generation strategy and an adaptive reward function, leading to state-of-the-art results in GUI grounding tasks.


<details>
  <summary>Details</summary>
Motivation: To address the exploration problem that bottlenecks semantic alignment in MLLMs, which prevents models from learning difficult semantic associations.

Method: Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework that employs a multi-answer generation strategy and an Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C.

Result: AEPO-trained models achieve significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding.

Conclusion: AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [61] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: 本研究分析了10万多个AI产品用户评论，发现伦理AI的七个维度（如公平性、透明度等）与用户满意度正相关，但不同用户群体和产品类型的影响程度不同。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨伦理AI与用户满意度之间的关系，以确定这些伦理原则是否被用户认可、重视或产生影响。

Method: 使用基于Transformer的语言模型，分析了来自G2的10万多个AI产品的用户评论，并测量了七个伦理维度的情感倾向。

Result: 所有七个伦理维度都与用户满意度呈正相关，但这种关系在用户和产品类型之间存在系统性差异。技术用户和AI开发平台的评论者更常讨论系统级问题，而非技术用户和终端应用的评论者则更关注以人为本的维度。此外，对于非技术用户和终端应用，伦理AI与用户满意度之间的关联更为显著。

Conclusion: 研究结果强调了从用户角度出发的伦理AI设计的重要性，并突出了在不同用户角色和产品类型中考虑情境差异的必要性。

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [62] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: ThematicPlane is a system that allows users to navigate and manipulate high-level semantic concepts in generative design, bridging the gap between creative intent and system control.


<details>
  <summary>Details</summary>
Motivation: The challenge of aligning generative AI outputs with nuanced creative intent, especially for non-experts, motivates the development of ThematicPlane.

Method: ThematicPlane is a system that enables users to navigate and manipulate high-level semantic concepts within an interactive thematic design plane.

Result: Participants engaged in divergent and convergent creative modes, often embracing unexpected results as inspiration or iteration cues. However, differing expectations of how themes mapped to outputs revealed a need for more explainable controls.

Conclusion: ThematicPlane fosters expressive, iterative workflows and highlights new directions for intuitive, semantics-driven interaction in generative design tools.

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>

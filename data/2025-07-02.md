<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.SD](#cs.SD) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: This paper evaluates the performance of LLMs on table understanding tasks, highlighting their challenges with scientific tables.


<details>
  <summary>Details</summary>
Motivation: To explore the efficiency of LLMs in processing tabular data, especially in scientific contexts.

Method: We investigate the effectiveness of both text-based and multimodal LLMs on table understanding tasks through a cross-domain and cross-modality evaluation. We also introduce the TableEval benchmark.

Result: LLMs show robustness across table modalities but struggle with scientific tables.

Conclusion: LLMs maintain robustness across table modalities but face significant challenges when processing scientific tables.

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [2] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: 本文主张将提示视为行为科学，而非权宜之计，强调其在大型语言模型研究中的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前提示方法很少被视为科学，而常被贬低为炼金术。作者认为这是分类错误，并希望重新定义提示的意义。

Method: 通过分析提示在研究和控制大型语言模型中的作用，以及将其视为行为科学的视角来探讨。

Result: 提示是研究和控制大型语言模型的重要手段，应被视为行为科学的一部分。

Conclusion: 提示不是一种权宜之计，而是大型语言模型科学的关键组成部分。

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [3] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: This paper introduces LineRetriever, a new method for retrieving relevant observation lines for web agents, improving performance within context limitations.


<details>
  <summary>Details</summary>
Motivation: Current approaches like bottom-up truncation or embedding-based retrieval lose critical information about page state and action history, which is essential for adaptive planning in web agents.

Method: Introduce LineRetriever, a novel approach that leverages a language model to identify and retrieve observation lines most relevant to future navigation steps.

Result: Experiments demonstrate that LineRetriever can reduce the size of the observation at each step for the web agent while maintaining consistent performance within the context limitations.

Conclusion: LineRetriever can reduce the size of the observation at each step for the web agent while maintaining consistent performance within the context limitations.

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [4] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型生成推理来增强文本分类的新方法，并展示了其在情绪分类任务中的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 标准分类模型通常直接将输入映射到标签，而没有显式的推理，这可能限制其性能、鲁棒性和可解释性。本文旨在通过利用大型语言模型生成的推理来增强文本分类。

Method: 本文提出了一种新颖的两阶段方法，通过利用大型语言模型（LLM）生成的推理来增强文本分类。第一阶段，我们在一个通用推理数据集上微调一个Llama-3.2-1B-Instruct模型（称为Llama-R-Gen），以在给定问题和答案的情况下生成文本推理（R）。第二阶段，这个一般训练的Llama-R-Gen被离线使用，以创建下游生成模型的增强训练数据集。该下游模型基于Llama-3.2-1B-Instruct，仅接受输入文本（Q），并被训练为输出生成的推理（R）紧接着预测的情绪（A）。

Result: 在dair-ai/emotion数据集上进行的实验表明，训练以输出推理和情绪的生成模型（Classifier Q->RA）在情绪预测方面的准确率比仅训练输出情绪的基线生成模型（Classifier Q->A）提高了8.7个百分点，这表明了推理生成的强大泛化能力和显式推理训练的好处。

Conclusion: 本文强调了利用大型语言模型生成的推理来创建更丰富的训练数据集的潜力，从而提高各种下游自然语言处理任务的性能，并提供明确的解释。

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [5] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: The paper discusses how LLMs fail to translate style and introduces RASTA, a method that helps LLMs better align with cultural communication norms.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of LLMs failing to translate style, which can lead to misalignment between the speaker's intended style and the listener's interpreted style due to cultural differences.

Method: RASTA (Retrieval-Augmented STylistic Alignment) is a method that leverages learned stylistic concepts to encourage LLM translation to appropriately convey cultural communication norms and align style.

Result: The paper shows that LLMs often bias translations towards neutrality and perform worse in non-Western languages. RASTA mitigates these failures by encouraging LLM translation to appropriately convey cultural communication norms and align style.

Conclusion: RASTA is a method that can help LLMs better translate style and align with cultural communication norms.

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [6] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: 我们的研究表明，指令微调不会完全消除有害信息，而是抑制其直接表达，使其在线性可访问且在下游行为中间接有影响。


<details>
  <summary>Details</summary>
Motivation: 大多数常用的语言模型（LMs）是通过结合微调和强化学习进行指令调整和对齐的，导致它们拒绝用户认为有害的请求。然而，越狱提示可以绕过这些拒绝机制并引发有害响应。因此，我们需要研究通过越狱提示访问的信息是否可以被解码。

Method: 我们研究了通过越狱提示访问的信息是否可以通过在语言模型隐藏状态上训练的线性探测器进行解码。我们展示了大量最初被拒绝的信息是线性可解码的。例如，跨模型，越狱语言模型对一个国家平均智商的响应可以通过具有超过0.8皮尔逊相关性的线性探测器预测。我们发现，基于模型（不拒绝）的探测器有时可以转移到其指令微调版本，并能够揭示越狱解码生成的信息，这表明许多被拒绝属性的内部表示从基础模型经过指令微调仍然存在。

Result: 我们发现，通过越狱提示访问的信息可以被线性探测器解码。例如，越狱语言模型对一个国家平均智商的响应可以通过具有超过0.8皮尔逊相关性的线性探测器预测。此外，基于模型的探测器有时可以转移到其指令微调版本，并能够揭示越狱解码生成的信息。

Conclusion: 我们的结果表明，指令微调不会完全消除或甚至重新定位有害信息在表示空间中——它们只是抑制了其直接表达，使其在线性可访问且在下游行为中间接有影响。

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [7] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: 本文通过数学模型研究了形态学和句法之间的关系，并提出了一种新的形态句法树结构。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解形态学和句法之间的交互作用，以及如何在形态学中进行词的形成，本文提出了一种新的数学模型。

Method: 本文基于合并的数学公式和强极简主义论题，利用代数结构和运算符理论来分析形态学和句法之间的关系。

Result: 本文展示了形态学树的共积分解，并将其与句法树相结合，形成一种新的形态句法树结构。

Conclusion: 本文提出了一个数学模型，用于描述形态学-句法接口，并重新解释了分布式形态学中的一些操作。

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [8] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: 我们的研究发现，非英语语言在推理任务中可以减少标记使用量并保持准确性，这表明多语言推理的潜力和强大多语言基础的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管最近在语言推理模型（LRMs）方面取得了进展，但大多数研究仅关注英语，即使许多模型是在多语言数据上预训练的。我们想调查英语是否是推理中最有效的语言。

Method: 我们评估了三个开源RLM：DeepSeek R1、Qwen 2.5和Qwen 3，在四个数学数据集和七种语系多样的语言上进行实验。

Result: 我们发现，在非英语语言中进行推理不仅减少了标记使用量，还保持了准确性。这些改进即使在将推理轨迹翻译成英语后仍然存在，表明推理行为发生了真正的转变，而不是表面的语言效应。模型的多语言能力决定了改进的程度。

Conclusion: 我们的研究结果表明，非英语语言在推理任务中不仅减少了标记使用量，还保持了准确性。这些改进即使在将推理轨迹翻译成英语后仍然存在，表明推理行为发生了真正的转变，而不是表面的语言效应。模型的多语言能力决定了改进的程度。我们的发现促使我们对语言模型中的推理有更广泛的视角，强调了多语言推理的潜力和强大多语言基础的重要性。

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [9] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: 本研究分析了微调方法对隐私风险的影响，发现基于提示的微调比基于参数的微调更注重隐私。


<details>
  <summary>Details</summary>
Motivation: 由于在微调过程中记忆带来的隐私风险相对较少受到关注，因此我们希望填补这一空白。

Method: 我们对流行的微调方法进行了分类，并通过成员推断攻击（MIAs）的视角评估了它们对记忆的影响。

Result: 与基于参数的微调相比，基于提示的微调在保持竞争力的同时，表现出较低的MIAs脆弱性。此外，基于提示的方法在不同模型规模下都保持了低记忆性。

Conclusion: 我们的研究结果表明，基于参数的微调更容易泄露隐私信息，而基于提示的微调则是一种更注重隐私的选项。

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [10] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文研究了非洲语言在NLP中的挑战，并开发了大规模人工标注数据集以提高其代表性。


<details>
  <summary>Details</summary>
Motivation: 非洲语言在NLP研究中缺乏代表性，因为它们的标记数据和网络上的未标记数据都很少。

Method: 本文分析了公开可用语料库中的噪声，整理了一个高质量的语料库，并研究了如何使用少量单语文本来适应和专业化多语言预训练语言模型。

Result: 本文开发了21种非洲语言的大规模人工标注数据集，并进行了广泛的实证评估。

Conclusion: 本文研究了非洲语言在自然语言处理中的挑战，并开发了大规模人工标注的数据集，以解决非洲语言在NLP研究中的代表性不足问题。

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [11] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: 本文研究了语言模型在生成平衡括号等简单句法任务中的错误机制，并提出了RASteer方法来提高模型性能。RASteer通过识别和增强可靠的组件，显著提升了模型在平衡括号任务和算术推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在简单句法任务（如生成平衡括号）中的持续错误机制，并提出一种方法来改善这些错误。

Method: RASteer是一种系统识别和增加可靠组件贡献的控制方法，以提高模型性能。

Result: RASteer显著提高了平衡括号任务的性能，将某些模型的准确率从0%提升到约100%，同时不会损害模型的一般编码能力。此外，RASteer在算术推理任务中也表现出更广泛的应用性，性能提升了约20%。

Conclusion: RASteer显著提高了平衡括号任务的性能，将某些模型的准确率从0%提升到约100%，同时不会损害模型的一般编码能力。此外，RASteer在算术推理任务中也表现出更广泛的应用性，性能提升了约20%。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [12] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: COLDSELECT是一种新的方法，用于在冷启动场景中选择verbalizer和少量样本实例，通过减少不确定性并提高泛化能力来超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了实例和verbalizer之间的依赖关系，其中实例-标签概率取决于verbalizer标记在嵌入空间中的接近程度。

Method: COLDSELECT是一种联合verbalizer和实例选择的方法，它将PLM词汇表和h_{[MASK]}嵌入映射到一个共享空间，并应用降维和聚类以确保高效且多样化的选择。

Result: 实验表明，COLDSELECT在八个基准测试中表现出色，能够有效减少不确定性并提高泛化能力。

Conclusion: COLDSELECT在冷启动场景中通过减少不确定性并增强泛化能力，优于基线方法，在verbalizer和少量样本实例选择方面表现出色。

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [13] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 本文提出了一种基于问题分解和重排序的RAG方法，以提高多跳问题的检索和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 多跳问题挑战了标准RAG，因为相关事实通常分布在多个文档中，而不是在一个源中出现。

Method: 我们提出了一个包含问题分解的RAG管道：(i) LLM分解原始查询为子问题，(ii) 为每个子问题检索段落，(iii) 合并候选池并重新排序以提高覆盖范围和精度。

Result: 我们在MultiHop-RAG和HotpotQA上评估了我们的方法，结果显示检索（MRR@10: +36.7%）和答案准确率（F1: +11.6%）相比标准RAG基线有所提高。

Conclusion: 我们的方法在多跳问题上有效，通过问题分解和重排序提高了检索和答案的准确性。

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [14] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

TL;DR: 本文通过无监督分段方法研究了格里高利圣咏的旋律结构，发现了一种记忆优化的分段方式，但这种分段方式并不符合传统的'拼接'理论。


<details>
  <summary>Details</summary>
Motivation: 格里高利圣咏的旋律可能由一些固定的片段构成，这种'拼接'理论虽然受到音乐学界的批评，但某些旋律片段的频繁重复表明可能存在尚未发现的分段方式。此外，最近的实证结果表明，分段方法在调式分类中表现优于音乐理论特征。

Method: 使用嵌套的层次化Pitman-Yor语言模型进行无监督分段，以寻找最优的格里高利圣咏旋律分段。

Result: 找到的分段方法在调式分类中达到了最先进的性能，并且发现了调式分类与记忆效率之间的联系。

Conclusion: 尽管找到了一种记忆优化的分段方式，但这种分段方式并不是传统意义上的'拼接'理论。

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [15] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果推理的提示框架CAPITAL，用于隐式情感分析，该方法在准确性和鲁棒性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的方法在ISA中容易受到内部偏差和虚假相关性的干扰，因此需要一种更可靠的方法来提高模型的准确性和鲁棒性。

Method: CAPITAL结合了前门调整到链式思维（CoT）推理中，通过编码器聚类和NWGM近似估计总体因果效应的两个组成部分，并使用对比学习目标来更好地对齐编码器的表示与LLM的推理空间。

Result: 在基准ISA数据集上的实验表明，CAPITAL在准确性和鲁棒性方面都优于强大的提示基线，尤其是在对抗条件下。

Conclusion: 本文提出了CAPITAL，这是一种将因果推理整合到大型语言模型提示中的原则性方法，并展示了其在偏差感知情感推理中的优势。

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [16] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: 本文展示了使用简单的监督可以显著提高语言模型与不同人口群体的对齐度，并提供了有用的基准以促进未来的研究。


<details>
  <summary>Details</summary>
Motivation: 准确预测不同人口群体如何回答主观问题具有巨大价值。

Method: 我们展示了使用相对简单的监督可以显著提高语言模型与不同人口群体的对齐度。

Result: 我们在三个涵盖各种主题的数据集上进行了评估，结果显示了对齐度的提升。此外，我们还报告了对齐度在特定群体之间的变化。

Conclusion: 我们的方法简单且通用，促进了易于采用，并且我们的广泛发现为在实践中何时使用或不使用我们的方法提供了有用的指导。

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [17] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: 本研究揭示了开放基准测试的潜在缺陷，并指出需要结合私有或动态基准测试以确保评估的完整性。


<details>
  <summary>Details</summary>
Motivation: 开放大型语言模型（LLM）基准测试虽然提供了标准化、透明的协议，但它们的开放性也引入了关键且未被充分探索的缺陷。

Method: 我们通过系统地构建“作弊”模型——在公开测试集上直接微调的较小版本的BART、T5和GPT-2——来揭示这些弱点。

Result: 这些“作弊”模型在著名的开放、全面基准（HELM）上取得了顶级排名，尽管它们的泛化能力差且实际应用价值有限。

Conclusion: 我们的研究强调了开放基准测试的局限性，并提出了对当前评估实践进行根本性重新评估的必要性。

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [18] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: 本文提出了一种模块化管道，用于在基于RAG的对话系统中管理广告，包括广告重写器和广告分类器，并利用合成数据训练分类器以指导广告集成策略。实验结果表明，分类器引导的优化可以提高广告的隐蔽性，实现更无缝的集成。


<details>
  <summary>Details</summary>
Motivation: 随着对话搜索引擎越来越多地采用基于生成的范式，广告的整合既带来了商业机会，也对用户体验构成了挑战。传统搜索中广告是明确区分的，而生成系统则模糊了信息内容和促销材料之间的界限，引发了透明度和信任的问题。

Method: 我们提出了一个模块化管道，包括广告重写器和稳健的广告分类器，并利用合成数据训练高性能分类器，然后用于指导两种互补的广告集成策略：监督微调广告重写器和最佳N采样方法。

Result: 我们的广告分类器在合成广告数据上训练，并通过课程学习增强，实现了强大的检测性能。此外，通过微调和最佳N采样，分类器引导的优化显著提高了广告的隐蔽性，实现了更无缝的集成。

Conclusion: 这些发现为开发更复杂的广告感知生成搜索系统和强大的广告分类器提供了对抗性共同进化框架。

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [19] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: Nirantar是一个用于评估多语言和多领域自动语音识别中持续学习的框架，它利用真实世界的数据进行评估，并展示了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有工作依赖于模拟事件，而Nirantar提供了动态、非均匀的语言和领域变化，使其成为CL研究的理想测试平台。

Method: Nirantar是一个全面的框架，用于评估持续学习（CL）在多语言和多领域自动语音识别（ASR）中的表现。它利用在印度22种语言和208个地区通过自然事件逐步收集的数据，以评估语言增量（LIL）、领域增量（DIL）以及新的语言增量领域增量学习（LIDIL）场景。

Result: 我们评估了现有方法，并表明没有单一的方法表现一致良好，强调了需要更稳健的CL策略。

Conclusion: 我们的框架使系统地基准测试CL方法成为可能，并表明没有单一的方法表现一致良好，强调了需要更稳健的CL策略。

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [20] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: 本文提出了一种基于胶囊网络的用户语义意图建模算法，以解决人机交互中意图识别准确率不足的问题。该方法通过向量化的胶囊结构表示输入文本中的语义特征，并使用动态路由机制在多个胶囊层之间传递信息。实验结果表明，所提出的模型在准确率、F1分数和意图检测率方面优于传统方法和其他深度学习结构。研究还分析了动态路由迭代次数对模型性能的影响，并提供了训练过程中损失函数的收敛曲线。这些结果验证了所提出方法在语义建模中的稳定性和有效性。总体而言，本研究提出了一种新的结构化建模方法，以在复杂的语义条件下提高意图识别效果。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互中的意图识别存在准确率不足的问题，需要一种更有效的语义建模方法。

Method: 本文提出的方法通过向量化的胶囊结构表示输入文本中的语义特征，并使用动态路由机制在多个胶囊层之间传递信息。这有助于更有效地捕捉语义实体之间的层次关系和部分-整体结构。模型使用卷积特征提取模块作为低级编码器，在生成初始语义胶囊后，通过迭代路由过程形成高层次的抽象意图表示。为了进一步提升性能，引入了基于边缘的机制到损失函数中，这提高了模型区分意图类别的能力。

Result: 实验结果表明，所提出的模型在准确率、F1分数和意图检测率方面优于传统方法和其他深度学习结构。研究还分析了动态路由迭代次数对模型性能的影响，并提供了训练过程中损失函数的收敛曲线。这些结果验证了所提出方法在语义建模中的稳定性和有效性。

Conclusion: 本文提出了一种基于胶囊网络的用户语义意图建模算法，以解决人机交互中意图识别准确率不足的问题。实验结果表明，所提出的模型在准确率、F1分数和意图检测率方面优于传统方法和其他深度学习结构。研究还分析了动态路由迭代次数对模型性能的影响，并提供了训练过程中损失函数的收敛曲线。这些结果验证了所提出方法在语义建模中的稳定性和有效性。总体而言，本研究提出了一种新的结构化建模方法，以在复杂的语义条件下提高意图识别效果。

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [21] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: 本文提供了关于计算密集型研究中方法论严谨性的指导，特别是针对主题建模算法的指南，有助于初学者和审稿人。


<details>
  <summary>Details</summary>
Motivation: 计算密集型研究方法在理论发展中的应用带来了方法论挑战，这可能削弱对研究的信任。因此，需要提供关于方法论严谨性的指导。

Method: 通过展示结构主题建模算法的应用并提出一组指南，讨论如何确保主题建模研究的严谨性。

Result: 提出了适用于主题建模算法的指南，这些指南可以针对其他算法进行上下文调整。

Conclusion: 本文贡献了关于主题建模的文献，并加入了计算密集型理论构建研究的方法论严谨性的新兴对话。

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [22] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: 本文提出了一个两部分的流程，结合基于检索的事实验证与针对常见幻觉模式进行微调的BERT系统，以解决LLM中的幻觉问题，并在多语言环境中取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 大多数关于幻觉的研究集中在英语数据上，忽视了LLM的多语言性质。

Method: 提出了一种两部分的流程，结合基于检索的事实验证与针对常见幻觉模式进行微调的BERT系统。

Result: 在所有语言中都取得了有竞争力的结果，在八种语言中获得了前10名，包括英语。此外，它支持超出共享任务涵盖的十四种语言以外的语言。

Conclusion: 该多语言幻觉识别器可以提高LLM输出的准确性和未来实用性。

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [23] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架，结合知识迁移和参数高效微调，以提高大型语言模型在低资源语言场景下的迁移和适应能力。该方法在跨语言任务中表现优异，尤其在数据稀缺的情况下具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决大型语言模型在低资源语言场景下的有限迁移和适应能力问题。

Method: 该方法提出了一个统一的框架，结合了知识迁移模块和参数高效的微调策略。引入了知识对齐损失和软提示调整，以指导模型在最少标注的情况下有效吸收目标语言或任务的结构特征。框架还包括轻量级的适应模块，以降低计算成本。训练过程中，整合了冻结策略和提示注入，以保留模型的原始知识并实现快速适应新任务。

Result: 实验结果表明，与现有的多语言预训练模型和主流迁移方法相比，所提出的方法在跨语言任务（如MLQA、XQuAD和PAWS-X）中实现了更高的性能和稳定性。

Conclusion: 该方法在跨语言任务中表现出色，尤其在数据极度匮乏的情况下具有明显优势。它增强了任务特定的适应性，同时保留了大型语言模型的一般能力，适用于复杂的语义建模和多语言处理任务。

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [24] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: MoR is a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering, leading to improved performance across various tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency.

Method: MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.

Result: Experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines.

Conclusion: MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [25] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: 本文介绍了SAFER，一种通过机制分析来解释和改进奖励模型的新框架。通过稀疏自编码器（SAEs），我们揭示了奖励模型激活中的可解释特征，从而深入了解与安全相关的决策。实验表明，SAFER可以在最小数据修改的情况下精确地降级或增强安全对齐，而不会牺牲通用聊天性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习从人类反馈（RLHF）是将大型语言模型（LLMs）与人类价值观对齐的关键范式，但其核心奖励模型仍然高度不透明。

Method: 我们提出了SAFER，这是一种通过机制分析来解释和改进奖励模型的新框架。利用稀疏自编码器（SAEs），我们揭示了奖励模型激活中的可解释特征，从而深入了解与安全相关的决策。

Result: 实验表明，SAFER可以在最小数据修改的情况下精确地降级或增强安全对齐，而不会牺牲通用聊天性能。

Conclusion: 我们的方法有助于在高风险的大型语言模型对齐任务中解释、审计和改进奖励模型。

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [26] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: 本研究探讨了不同语言训练的视觉-语言模型是否表现出基于文化的注意力模式，并发现它们确实反映了文化认知的影响。


<details>
  <summary>Details</summary>
Motivation: 研究不同语言训练的视觉-语言模型是否表现出类似的基于文化的注意力模式。

Method: 通过比较图像描述进行分析，以检查这些模型是否反映整体性与分析性倾向的差异。

Result: VLMs不仅内化了语言的结构特性，还再现了训练数据中的文化行为。

Conclusion: 文化认知可能隐式地塑造模型输出。

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [27] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: This paper explores the use of large language models to generate financial reports from time series data, proposing a framework for evaluation and demonstrating their capability to produce coherent and informative reports.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the potential of large language models (LLMs) to generate financial reports from time series data and evaluate their factual grounding and reasoning capabilities.

Method: The paper proposes a framework that includes prompt engineering, model selection, and evaluation, along with an automated highlighting system to categorize information in generated reports.

Result: Experiments using real stock market indices and synthetic time series data demonstrate that LLMs can produce coherent and informative financial reports.

Conclusion: LLMs can produce coherent and informative financial reports, and the proposed framework helps evaluate their factual grounding and reasoning capabilities.

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [28] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: 本文介绍了LitBench，这是一个用于创意写作验证的首个标准化基准和配对数据集。通过该基准，我们评估了零样本LLM裁判，训练了奖励模型，并进行了在线人类研究。结果表明，训练的奖励模型在准确率上优于现成裁判，并且与人类偏好一致。


<details>
  <summary>Details</summary>
Motivation: 评估由大型语言模型生成的创意写作仍然具有挑战性，因为开放式的叙事缺乏真实数据。为了实现对创意写作的稳健评估，我们需要一个标准化的基准和数据集。

Method: 我们引入了LitBench，这是第一个标准化基准和配对数据集，用于创意写作验证，并对其进行了基准测试、训练了Bradley Terry和生成奖励模型，还进行了在线人类研究以验证奖励模型排名。

Result: LitBench识别出Claude-3.7-Sonnet是最强的现成裁判，与人类偏好达到73%的一致性；训练的奖励模型在准确率上达到78%，超过了所有现成裁判。在线人类研究进一步证实了我们的奖励模型在新型LLM生成故事中与人类偏好一致。

Conclusion: 我们发布了LitBench和奖励模型，为可靠、自动化的创意写作系统评估和优化提供了经过验证的资源。

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [29] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: 本文研究了一种函数式编程方法，用于自然语言语义，以提高传统指称风格的表达能力。我们形式化了一个基于范畴的类型和效果系统，并构建了一个图示演算来建模解析和处理效果，并用于高效计算句子的指称。


<details>
  <summary>Details</summary>
Motivation: 提高传统指称风格的表达能力，并通过函数式编程方法来实现更高效的语义处理。

Method: 形式化一个基于范畴的类型和效果系统，并构建一个图示演算来建模解析和处理效果。

Result: 能够高效计算句子的指称。

Conclusion: 该方法提供了一种更强大和灵活的自然语言语义处理方式。

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [30] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: This paper reviews the use of GenAI in scientometrics, highlights its potential and limitations, and discusses its possible impact on the field.


<details>
  <summary>Details</summary>
Motivation: The paper aims to review the use of GenAI in scientometrics and begin a debate on its broader implications for the field.

Method: The paper reviews the use of GenAI in scientometrics, critically engages with recent experiments, and explores the implications of GenAI's ability to generate scientific language.

Result: GenAI shows promise in tasks where language generation dominates, such as topic labelling, but faces limitations in tasks requiring stable semantics, pragmatic reasoning, or structured domain knowledge. Additionally, GenAI may affect textual characteristics used to measure science.

Conclusion: GenAI has the potential to impact scientometrics, but careful empirical work and theoretical reflection are needed to interpret evolving patterns of knowledge production.

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [31] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: 研究发现，LLM在群体中表现出类似人类的功利主义倾向，但其背后的机制不同。


<details>
  <summary>Details</summary>
Motivation: 了解LLM在协作中的表现与个体代理的差异，以及是否会出现类似人类群体中的功利主义提升现象。

Method: 我们测试了六种模型在两个条件下的道德困境：(1) 独立，其中模型独立推理；(2) 组，其中它们以成对或三人组的形式进行多轮讨论。

Result: 所有模型在群体中比单独时更接受道德违规行为，类似于人类实验。一些模型支持最大化整体福祉的行为，即使这使陌生人受益而非熟悉的人。其他模型在群体中更愿意违反道德规范。然而，人类群体的功利主义提升机制与LLM不同。

Conclusion: 虽然LLM集体的行为表面看起来像人类群体推理，但其背后的驱动因素不同。我们讨论了对AI对齐、多智能体设计和人工道德推理的影响。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [32] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: 本文设计了一种可扩展的人类评估协议和自动化近似方法，以反映实践中使用模型的真实情况。通过该协议，我们收集了大量标注数据，并验证了LLM代理的有效性，发现它们可以作为人类标注者的合理替代品。


<details>
  <summary>Details</summary>
Motivation: 主题模型和文档聚类评估要么使用与人类偏好对齐不佳的自动指标，要么需要专家标签，这些标签难以扩展。

Method: 我们设计了一个可扩展的人类评估协议和相应的自动化近似方法，以反映实践者在现实世界中使用模型的情况。标注者或基于LLM的代理审查分配到主题或聚类的文本项目，推断出该组的类别，然后将该类别应用于其他文档。

Result: 我们收集了来自两个数据集上各种主题模型输出的大量众包标注，并使用这些标注来验证自动化代理，发现最佳LLM代理与人类标注者在统计上无法区分。

Conclusion: 我们发现最好的LLM代理与人类标注者在统计上无法区分，因此可以在自动化评估中作为合理的替代品。

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [33] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: 该论文探讨了风格分析作为区分大型语言模型和人类生成文本的方法，通过创建一个基于维基百科的基准数据集，并使用树基模型进行分类，结果显示在特定文本类型下可以有效区分机器生成和人类生成的文本。


<details>
  <summary>Details</summary>
Motivation: 探索风格分析作为区分大型语言模型（LLMs）和人类生成文本的方法，解决模型归属、知识产权和伦理AI使用问题。

Method: 通过创建基于维基百科的基准数据集，包括人类编写的术语摘要、纯LLM生成的文本、经过多种文本摘要方法处理的文本以及重写方法处理的文本，并使用树基模型（决策树和LightGBM）进行分类，利用人工设计的（StyloMetrix）和n-gram-based（我们自己的管道）风格特征来编码词汇、语法、句法和标点模式。

Result: 交叉验证结果在多类场景中达到了高达0.87的马修斯相关系数，在二分类中准确率在0.79到1之间，特别是维基百科和GPT-4在平衡数据集上达到了高达0.98的准确率。Shapley Additive Explanations指出了与百科全书文本类型相关的特征、个别过度使用的单词以及LLM相对于人类写作的更高语法标准化程度。

Conclusion: 这些结果表明，即使在LLM越来越复杂的背景下，至少对于定义明确的文本类型，区分机器生成和人类生成的文本是可能的。

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [34] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: 本文介绍了TransLaw，一个用于实际香港案例法翻译的多代理框架，通过三个专门代理协作提高翻译质量，并在成本和性能上表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于复杂的法律术语、文化嵌入的细微差别和严格的语言结构，LLMs在翻译香港法律判决中的潜力尚不确定。

Method: 引入了三个专门的代理（翻译者、注释者和校对者）来协作生成高精度的法律翻译。

Result: 该框架支持自定义LLM配置，并且与专业人工翻译服务相比实现了巨大的成本节约。使用13个开源和商业LLM进行了评估，发现其在法律语义准确性、结构连贯性和风格忠实度方面优于GPT-4o。

Conclusion: TransLaw框架在法律语义准确性、结构连贯性和风格忠实度方面优于GPT-4o，但在复杂术语的上下文理解和风格自然性方面仍落后于人类专家。

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [35] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 该研究创建了五个地区的GSM8K测试集的文化适应版本，并评估了六种大型语言模型的表现，发现模型在原始数据集上表现最好，但在文化适应版本上表现较差，但具有推理能力的模型更能应对这种变化。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试如GSM8K主要基于西方规范，包括名字、货币和日常场景，而数学问题的呈现方式可能带有隐含的文化背景。

Method: 通过基于提示的转换和人工验证，创建了五个地区的GSM8K测试集的文化适应版本，并评估了六种大型语言模型在五种提示策略下的表现。

Result: 模型在原始的以美国为中心的数据集上表现最好，而在文化适应版本上的表现相对较差。然而，具有推理能力的模型对这些变化更具韧性。

Conclusion: 模型的推理能力有助于弥合数学任务中的文化呈现差距。

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [36] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 本文对现有数据进行了元分析，发现只有39%的情况下下游任务性能符合线性缩放定律，并指出实验设置的变化可能导致缩放趋势的改变。


<details>
  <summary>Details</summary>
Motivation: 下游缩放定律旨在从较小规模的预训练损失预测较大规模的任务性能。然而，这种预测是否可行尚不清楚。

Method: 我们对现有的关于下游缩放定律的数据进行了元分析。

Result: 我们发现只有39%的情况下接近线性缩放定律的拟合。此外，实验设置的看似无害的变化可以完全改变缩放趋势。

Conclusion: 我们的分析强调了理解在什么条件下缩放定律成功的重要性。为了完全建模预训练损失和下游任务性能之间的关系，我们必须接受那些缩放行为偏离线性趋势的情况。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [37] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: 本文介绍了MemeCMD，一个自动生成的中文多轮对话数据集，结合大规模MLLM标注的图库和跨不同场景的双代理自动生成对话。实验表明，我们的方法在生成上下文合适的多样化带图对话方面是有效的，为推进多模态对话AI提供了一种可扩展且隐私保护的资源。


<details>
  <summary>Details</summary>
Motivation: 现有的对话数据集主要局限于手动标注或纯文本对话，缺乏多模态交互提供的表达力和上下文细微差别。

Method: 我们引入了MemeCMD，一个自动生成的中文多轮对话数据集，结合大规模MLLM标注的图库和跨不同场景的双代理自动生成对话。我们还引入了一个检索框架和自适应阈值，以确保上下文相关的自然间隔的图使用。

Result: 实验表明，我们的方法在生成上下文合适的多样化带图对话方面是有效的。

Conclusion: 我们的方法在生成上下文合适的多样化带图对话方面表现出有效性，为推进多模态对话AI提供了一种可扩展且隐私保护的资源。

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [38] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: 本文探讨了如何利用计算系统发育方法处理同源数据，并指出目前缺乏可行的方法来自动生成更大的同源数据集。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用计算系统发育方法在同源数据中的潜力，需要利用特定的（复杂的）模型和基于机器学习的技术。然而，这两种方法都需要比目前可用的手动收集的同源数据大得多的数据集。

Method: 通过从BabelNet中自动提取数据集来验证这一观点。

Result: 系统发育推断在相应的字符矩阵上产生的树与已建立的黄金标准树不一致。

Conclusion: 目前尚不清楚如何以及是否可以将这些计算方法应用于历史语言学。

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [39] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 本文研究了道德自我修正技术的悖论，并提出了通过利用精心筛选数据集的启发式方法来改进道德自我修正的解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和解决道德自我修正技术的两个主要悖论，即表面效果和无法识别道德不一致的原因，本文进行了研究。

Method: 本文分析了用于增强道德自我修正的微调语料库中的话语构建，揭示了有效构建背后的启发式方法。

Result: 本文发现道德自我修正依赖于反映启发式捷径的话语构建，并且这些启发式捷径在自我修正过程中会导致不一致。

Conclusion: 本文提出了一个解决方案，通过利用精心筛选数据集的启发式方法来改进道德自我修正。同时，也指出了该能力在学习情境上下文和模型规模方面的泛化挑战。

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [40] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 本文通过大规模实验比较了MLM和CLM在文本表示任务中的表现，发现CLM在数据效率和微调稳定性方面更优，并提出了一种分阶段的训练策略以在固定计算预算下达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的证据表明，使用因果语言建模（CLM）预训练的解码器模型可以有效地重新用作编码器，通常在文本表示基准上超越传统编码器，但尚不清楚这些改进是CLM目标的固有优势还是由模型和数据规模等混淆因素引起的。因此，本文旨在解决这一问题。

Method: 本文通过一系列大规模、精心控制的预训练消融实验，训练了总共30个参数范围从2.1亿到10亿的模型，并进行了超过15,000次微调和评估运行。

Result: 本文发现，虽然使用MLM训练通常在文本表示任务中表现更好，但CLM训练的模型在数据效率和微调稳定性方面更优。此外，一种分阶段的训练策略（先应用CLM，然后应用MLM）在固定的计算训练预算下可以达到最佳性能。当从现有的LLM生态系统中可获得的预训练CLM模型进行初始化时，这种策略变得更加有吸引力，减少了训练最佳编码器模型所需的计算负担。

Conclusion: 本文通过一系列大规模、精心控制的预训练消融实验，发现虽然MLM通常在文本表示任务中表现更好，但CLM训练的模型在数据效率和微调稳定性方面更优。基于这些发现，我们实验表明，一种分阶段的训练策略（先应用CLM，然后应用MLM）在固定的计算训练预算下可以达到最佳性能。此外，当从现有的LLM生态系统中可获得的预训练CLM模型进行初始化时，这种策略变得更加有吸引力，减少了训练最佳编码器模型所需的计算负担。

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [41] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: 本文介绍了 La Leaderboard，这是第一个开源排行榜，用于评估西班牙语和拉丁美洲语言变体的生成式 LLMs，并提供了方法论和指导。


<details>
  <summary>Details</summary>
Motivation: 为了推动代表西班牙语社区语言和文化多样性的 LLMs 的发展，本文提出了 La Leaderboard，这是第一个开源排行榜，用于评估西班牙语和拉丁美洲语言变体的生成式 LLMs。

Method: 本文介绍了 La Leaderboard 的方法，包括选择最适合每个下游任务的评估设置的指导。

Result: 该初始版本结合了 66 个数据集，展示了 50 个模型的评估结果。

Conclusion: La Leaderboard 是一个社区驱动的项目，旨在为西班牙语社区的 LLMs 开发建立评估标准。

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [42] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, with SciArena-Eval as a benchmark to assess automated evaluation methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create an open and collaborative platform for evaluating foundation models on scientific literature tasks, leveraging collective intelligence for community-driven evaluations.

Method: SciArena is a platform that evaluates foundation models on scientific literature tasks through community voting, and SciArena-Eval is a meta-evaluation benchmark based on preference data to measure the accuracy of models in judging answer quality.

Result: SciArena supports 23 foundation models and has collected over 13,000 votes from researchers, confirming the diversity of questions and strong self-consistency in evaluations. SciArena-Eval highlights challenges in automated evaluation methods.

Conclusion: SciArena provides a community-driven evaluation platform for foundation models on scientific literature tasks and highlights the need for more reliable automated evaluation methods.

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [43] [Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite](https://arxiv.org/abs/2507.00877)
*William H English,Chase Walker,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: eess.SY

TL;DR: 本文介绍了VLTL-Bench，这是一个新的基准，用于评估自然语言到时态逻辑翻译的验证性和可验证性。该基准包含多个状态空间、多样化自然语言规范以及样本轨迹，并提供每个步骤的真实值，以支持研究不同子步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言到时态逻辑（NL-to-TL）翻译系统在现有基准上表现出接近完美的性能，但当前研究仅测量自然语言逻辑到形式化时态逻辑的翻译准确性，忽略了系统将原子命题嵌入新场景或环境的能力。这对于结果公式的验证至关重要。因此，大多数NL-to-TL翻译框架提出自己的专用数据集，这可能会夸大性能指标并忽视对可扩展、领域通用系统的需要。

Method: 本文提出了VLTL-Bench基准，它包含三个独特的状态空间、数千个多样化的自然语言规范和对应的形式化规范，并提供样本轨迹来验证时态逻辑表达式。此外，该基准在每个步骤中提供真实值，以支持对整体问题的不同子步骤进行研究和改进。

Result: 本文提出的VLTL-Bench基准能够直接支持端到端评估，并且提供每个步骤的真实值，以便研究人员改进和评估整体问题的不同子步骤。该基准已发布在Kaggle上，以鼓励方法学严谨的进展。

Conclusion: 本文介绍了VLTL-Bench，这是一个统一的基准，用于衡量自动化自然语言到线性时态逻辑（LTL）翻译的验证性和可验证性。该基准提供了多个状态空间、多样化的自然语言规范和对应的正式规范，并包含样本轨迹以验证时态逻辑表达式。

Abstract: Empirical evaluation of state-of-the-art natural-language (NL) to
temporal-logic (TL) translation systems reveals near-perfect performance on
existing benchmarks. However, current studies measure only the accuracy of the
translation of NL logic into formal TL, ignoring a system's capacity to ground
atomic propositions into new scenarios or environments. This is a critical
feature, necessary for the verification of resulting formulas in a concrete
state space. Consequently, most NL-to-TL translation frameworks propose their
own bespoke dataset in which the correct grounding is known a-priori, inflating
performance metrics and neglecting the need for extensible, domain-general
systems. In this paper, we introduce the Verifiable Linear Temporal Logic
Benchmark ( VLTL-Bench), a unifying benchmark that measures verification and
verifiability of automated NL-to-LTL translation. The dataset consists of three
unique state spaces and thousands of diverse natural language specifications
and corresponding formal specifications in temporal logic. Moreover, the
benchmark contains sample traces to validate the temporal logic expressions.
While the benchmark directly supports end-to-end evaluation, we observe that
many frameworks decompose the process into i) lifting, ii) grounding, iii)
translation, and iv) verification. The benchmark provides ground truths after
each of these steps to enable researches to improve and evaluate different
substeps of the overall problem. To encourage methodologically sound advances
in verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:
https://www.kaggle.com/datasets/dubascudes/vltl bench.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: 本文提出了一种基于奖励的数据集蒸馏框架AdvDistill，通过利用教师模型的多个生成并根据基于规则的验证器分配奖励，显著提高了学生模型在数学和复杂推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLMs）的压缩和能力传授给更易于部署和高效的较小语言模型（SLMs）已受益于知识蒸馏（KD）技术的进步。然而，蒸馏通常围绕着学生模型仅仅复制教师模型的内部分布响应，限制了其泛化能力。这种限制在推理任务中被放大，并且计算成本高昂。

Method: 我们提出了AdvDistill，这是一种基于奖励的数据集蒸馏框架。我们为每个提示使用教师模型的多个生成（响应），并根据基于规则的验证器分配奖励。这些变化且通常分布的奖励在训练学生模型时作为权重。

Result: 我们的方法及其后续的行为分析表明，在数学和复杂推理任务中，学生模型的性能有显著提高，展示了在数据集蒸馏过程中引入奖励机制的有效性和优势。

Conclusion: 我们的方法及其后续的行为分析表明，在数学和复杂推理任务中，学生模型的性能有显著提高，展示了在数据集蒸馏过程中引入奖励机制的有效性和优势。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [45] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: 本文介绍了一种名为文本逆向推理的新范式，使大型语言模型能够事后分解和解释其自身的推理链。这种方法提高了模型的透明度和可解释性，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在解决复杂推理任务方面表现出色，但它们的决策过程仍然是一个黑箱。我们需要一种方法来分解和解释这些模型的推理链，以提高其透明度和可解释性。

Method: 我们引入了文本逆向推理，这是一种新颖的范式，使大型语言模型能够事后分解和解释其自身的推理链。我们的方法使用了一种元认知结构，通过注意力过程进行反射，以识别主要决策点并生成推理选择的解释。

Result: 我们在逻辑推理谜题、数学问题和伦理困境的测试中展示了SAGE-nano在推理准确性和解释质量方面的领先地位，同时其性能几乎与Claude-3.5 Sonnet或GPT-4o相当。

Conclusion: 我们的工作为透明人工智能系统开辟了新的途径，并填补了人工智能安全、教育和科学发现中的重要空白。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [46] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: ASTRO是一种框架，用于训练语言模型像搜索算法一样推理，通过利用自省、回溯和探索来增强模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚如何提升其他非推理模型（包括Llama 3）的推理能力。

Method: ASTRO通过从数学问题解决轨迹的蒙特卡洛树搜索（MCTS）中生成合成数据集，教导这些模型内化结构化的搜索行为。然后，我们对这些搜索衍生的痕迹进行微调，并通过可验证的奖励进行强化学习以进一步提高性能。

Result: 我们将ASTRO应用于Llama 3系列模型，在MATH-500上实现了16.0%的绝对性能提升，在AMC 2023上实现了26.9%，在AIME 2024上实现了20.0%的提升，特别是在需要迭代修正的难题上表现尤为突出。

Conclusion: 我们的结果表明，基于搜索的训练为将稳健的推理能力注入开放的大型语言模型提供了一种合理的方法。

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [47] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 研究发现，虽然数学推理在大型语言模型中取得了进展，但这些进步可能仅限于特定领域，而非广泛的问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 随着数学排行榜每周改善，值得思考这些进步是否反映了更广泛的问题解决能力还是仅是狭窄的过拟合。

Method: 我们评估了超过20个开放权重的推理调优模型，并进行了受控实验，使用数学-only数据但不同的调优方法。

Result: 我们发现大多数在数学上取得成功的模型未能将其优势转移到其他领域。RL调优的模型在跨领域上表现良好，而SFT调优的模型常常遗忘一般能力。

Conclusion: 我们的结果表明，需要重新考虑标准的训练后方法，特别是依赖于SFT蒸馏数据来推进推理模型。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [48] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: 本文提出了一种名为CIP的新技术，利用因果影响图（CIDs）来识别和减轻代理决策中的风险，从而提高自主代理的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着由大型语言模型（LLMs）驱动的自主代理在各种辅助任务中展现出潜力，确保其安全可靠的行为对于防止意外后果至关重要。

Method: 我们引入了一种名为CIP的新技术，该技术利用因果影响图（CIDs）来识别和减轻代理决策带来的风险。我们的方法包括三个关键步骤：(1) 基于任务规范初始化一个CID以概述决策过程，(2) 使用CID指导代理与环境的交互，(3) 根据观察到的行为和结果迭代优化CID。

Result: 实验结果表明，我们的方法在代码执行和移动设备控制任务中有效提高了安全性。

Conclusion: 我们的方法在代码执行和移动设备控制任务中有效提高了安全性。

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [49] [Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds](https://arxiv.org/abs/2507.00740)
*Craig S Wright*

Main category: cs.CR

TL;DR: 本文提供了SPV的完整形式化规范和数学证明结构，证明了其安全性，并提出了低带宽优化方案。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供一个安全的SPV实现蓝图，并反驳关于非验证客户端的常见误解。

Method: 本文从第一原理重建了SPV协议，将其验证模型建立在符号自动机、Merkle成员关系和链式证明支配谓词的基础上，并通过严格的概率和博弈论分析推导出协议的安全经济界限。

Result: 本文提出了低带宽优化，如自适应轮询和压缩头同步，同时保持正确性，并验证了协议在部分连接、敌对中继网络和敌对传播延迟下的活性和安全性。

Conclusion: 本文提供了Simplified Payment Verification (SPV)的完整形式化规范、协议描述和数学证明结构，并展示了SPV在有限敌对假设下的安全性以及其在需要可扩展性和可验证交易包含的数字现金系统中的严格最优性。

Abstract: This paper presents a complete formal specification, protocol description,
and mathematical proof structure for Simplified Payment Verification (SPV) as
originally defined in the Bitcoin whitepaper \cite{nakamoto2008}. In stark
contrast to the misrepresentations proliferated by popular implementations, we
show that SPV is not only secure under bounded adversarial assumptions but
strictly optimal for digital cash systems requiring scalable and verifiable
transaction inclusion. We reconstruct the SPV protocol from first principles,
grounding its verification model in symbolic automata, Merkle membership
relations, and chain-of-proof dominance predicates. Through rigorous
probabilistic and game-theoretic analysis, we derive the economic bounds within
which the protocol operates securely and verify its liveness and safety
properties under partial connectivity, hostile relay networks, and adversarial
propagation delay. Our specification further introduces low-bandwidth
optimisations such as adaptive polling and compressed header synchronisation
while preserving correctness. This document serves both as a blueprint for
secure SPV implementation and a rebuttal of common misconceptions surrounding
non-validating clients.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [50] [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
*Jianghao Lin,Xinyuan Wang,Xinyi Dai,Menghui Zhu,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: MassTool is a framework that enhances query representation and tool retrieval accuracy by using a two-tower architecture and incorporating search-based user intent modeling and adaptive knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in existing approaches that primarily focus on optimizing tool representations while neglecting the importance of precise query comprehension.

Method: MassTool is a multi-task search-based framework that uses a two-tower architecture, including a tool usage detection tower and a tool retrieval tower with a query-centric graph convolution network (QC-GCN). It also incorporates search-based user intent modeling (SUIM) and an adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.

Result: Extensive experiments demonstrate MassTool's effectiveness in improving retrieval accuracy.

Conclusion: MassTool demonstrates effectiveness in improving retrieval accuracy through its robust dual-step sequential decision-making pipeline.

Abstract: Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [51] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 本文提出了一种新的帧采样方法，称为'时刻采样'，通过使用通用的文本到视频时刻检索模型来指导帧采样过程，从而提高长视频问答（VideoQA）性能。


<details>
  <summary>Details</summary>
Motivation: 现有的帧子采样方法在处理长视频时存在不足，容易丢失关键帧或包含冗余信息，影响模型的准确性并增加计算资源消耗。

Method: 本文提出了一种名为'时刻采样'的新方法，利用轻量级的时刻检索模型来优先选择与问题相关的帧。

Result: 通过在四个长视频QA数据集上使用四种最先进的视频LLM进行广泛实验，证明了所提出方法的有效性。

Conclusion: 本文提出了一种新的帧采样方法，称为'时刻采样'，通过使用通用的文本到视频时刻检索模型来指导帧采样过程，从而提高长视频问答（VideoQA）性能。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [52] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: This paper introduces a challenging task called 'CaughtCheating' to evaluate the visual perception and reasoning capabilities of Multi-Modal Large Language Models (MLLMs). The task is inspired by social media requests where users ask others to detect suspicious clues from photos. The study shows that existing MLLMs, such as GPT-o3, struggle with this task, highlighting the need for further improvements in their ability to perform human-level detective work.


<details>
  <summary>Details</summary>
Motivation: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives?

Method: We investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task.

Result: We find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. This scenario is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task.

Conclusion: CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [53] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: MANTA is a new framework that unifies multi-modal data into a structured textual space, improving performance on various tasks like video question answering and temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal learning approaches often treat modalities separately, leading to inconsistencies in representation and reasoning. The goal is to create a unified framework that can seamlessly process multi-modal data with large language models.

Method: MANTA uses a theoretically-grounded approach to unify visual and auditory inputs into a structured textual space, addressing challenges such as semantic alignment, temporal synchronization, hierarchical content representation, and context-aware retrieval.

Result: MANTA improves state-of-the-art models by up to 22.6% in overall accuracy on Long Video Question Answering, with significant gains on longer videos. It also shows superiority on temporal reasoning tasks and cross-modal understanding.

Conclusion: MANTA introduces a novel framework for unifying multi-modal representations through structured text, demonstrating significant improvements in accuracy and performance on various tasks.

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [54] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: 本文提出了一种基于轻量级DNN的实时手语识别框架，通过编码手语特定参数并利用MediaPipe进行关键点提取，实现了高准确率和低延迟的手语识别。


<details>
  <summary>Details</summary>
Motivation: 我们的系统解决了手语识别中的关键挑战，包括数据稀缺性、高计算成本以及训练和推理环境之间的帧率差异。

Method: 我们提出了一种使用轻量级DNN的实时手语识别框架，该框架在有限的数据上进行训练。通过将手语特定参数编码为向量化输入，并利用MediaPipe进行关键点提取，我们实现了高度可分离的输入数据表示。我们的DNN架构优化为小于10MB的部署，能够在边缘设备上以不到10ms的延迟准确分类343个手势。

Result: 我们的模型在孤立手语识别中达到了92%的准确率，并已集成到'slait ai'网络应用中，展示了稳定的推理能力。

Conclusion: 我们的模型在孤立手语识别中达到了92%的准确率，并已集成到'slait ai'网络应用中，展示了稳定的推理能力。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [55] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: 本文提出了一种名为ONLY的无需训练的解码方法，该方法只需一次查询和解码过程中的单层干预，即可有效减少大型视觉语言模型的幻觉问题，并在各种基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的工作已经探索了对比解码方法来缓解这个问题，但这些方法需要两次或更多次查询，这会减慢LVLM的响应生成，使其不太适合实时应用。为了克服这个限制，我们提出了ONLY。

Method: 我们提出了一种无需训练的解码方法ONLY，它只需要一次查询和解码过程中的单层干预，从而实现了高效的实时部署。我们通过使用每个标记的文本到视觉熵比来选择性地增强文本输出，以放大关键的文本信息。

Result: 广泛的实验结果表明，我们的ONLY方法在各种基准测试中 consistently 超过最先进的方法，同时需要最小的实现努力和计算成本。

Conclusion: 我们的ONLY方法在各种基准测试中 consistently 超过最先进的方法，同时需要最小的实现努力和计算成本。

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [56] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

TL;DR: 本文介绍了一个名为SciBORG的模块化代理框架，它允许基于大型语言模型的代理自主规划、推理并实现稳健和可靠的特定领域任务执行。通过与物理和虚拟硬件的集成，验证了SciBORG的有效性，并展示了其在自主多步骤生物测定检索中的应用。结果表明，记忆和状态意识是代理规划和可靠性的关键使能因素，为在复杂环境中部署AI代理提供了通用的基础。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自然语言理解和生成方面取得了强大的进展，但它们在复杂、现实世界科学工作流中的应用仍受到内存、规划和工具集成挑战的限制。

Method: 我们介绍了SciBORG（Scientific Bespoke Artificial Intelligence Agents Optimized for Research Goals），这是一个模块化的代理框架，允许基于LLM的代理自主规划、推理并实现稳健和可靠的特定领域任务执行。代理动态地从源代码文档构建，并通过有限状态自动机（FSA）内存进行增强，以实现持久的状态跟踪和上下文感知决策。

Result: 通过与物理和虚拟硬件的集成（例如用于执行用户指定反应的微波合成器），验证了SciBORG的有效性，展示了其在自主多步骤生物测定检索中的应用，利用多步骤规划、推理、代理间通信和协调来执行探索性任务。系统基准测试显示，SciBORG代理实现了可靠的执行、适应性规划和可解释的状态转换。

Conclusion: 我们的结果表明，记忆和状态意识是代理规划和可靠性的关键使能因素，为在复杂环境中部署AI代理提供了通用的基础。

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: HDRAM is a symbolic memory framework that uses CHQ principles to improve associative retrieval in LLMs by treating the transformer latent space as a spread-spectrum channel.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) suffer from apparent precision loss, which is reframed as information spreading, shifting the problem from computational precision to an information-theoretic communication issue.

Method: HDRAM introduces a symbolic memory framework that treats the transformer latent space as a spread-spectrum channel, using hypertokens, structured symbolic codes integrating classical error-correcting codes (ECC), holographic computing, and quantum-inspired search.

Result: HDRAM significantly improves associative retrieval through phase-coherent memory addresses, enabling efficient key-value operations and Grover-style search in latent space.

Conclusion: HDRAM demonstrates how Classical-Holographic-Quantum-inspired (CHQ) principles can fortify transformer architectures by improving associative retrieval without architectural changes.

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [58] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，将监督微调（SFT）和偏好学习在大型语言模型（LLM）后训练中联系起来。我们发现SFT是隐式奖励学习的一种特殊情况，并提出了一种简单但有效的学习率降低方法，显著提高了性能。此外，我们通过不同的f散度函数推导出替代的SFT目标，进一步提高了后DPO模型的性能。最后，我们扩展了LLM逻辑与Q函数之间的理论关系，并提供了数学推导和实验验证。


<details>
  <summary>Details</summary>
Motivation: 后训练过程对于将预训练语言模型适配到现实任务至关重要，而从示范或偏好信号中学习在这一适应过程中起着关键作用。然而，传统的SFT存在局限性，即分布匹配中的KL散度项在优化过程中对策略是常数，无法约束模型更新。因此，我们需要一种更有效的方法来改进后训练过程。

Method: 本文通过严格的数学推导，分析了SFT和偏好学习方法在LLM后训练中的关系。我们还提出了基于不同f散度函数的替代SFT目标，以保持KL项在优化过程中的有效性。最后，我们将LLM逻辑与Q函数之间的理论关系从偏好学习扩展到SFT情境。

Result: 我们提出的简单但有效的方法在指令遵循任务中实现了显著的性能提升（最高25%的相对增益和6%的绝对胜率增加）。此外，我们通过不同的f散度函数推导出替代的SFT目标，进一步提高了后DPO模型的性能。最后，我们扩展了LLM逻辑与Q函数之间的理论关系，并提供了数学推导和实验验证。

Conclusion: 本文提出了一个统一的理论框架，将监督微调（SFT）和偏好学习在大型语言模型（LLM）后训练中联系起来。通过数学推导，我们发现SFT和偏好学习方法如直接偏好优化（DPO）都在相同的最优策略-奖励子空间中运行，并且SFT是隐式奖励学习的一种特殊情况。此外，我们提出了一种简单但有效的学习率降低方法，显著提高了性能。

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [59] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: 本文介绍了一种名为GLU Attention的新注意力机制，该机制将非线性引入注意力值中。实验表明，GLU Attention在文本和视觉模态中提高了模型性能和收敛速度，且没有额外参数和计算成本。GLU Attention轻量级，可以无缝集成其他技术，如Flash Attention、旋转位置嵌入（RoPE）和各种多头注意力（MHA）变体，如分组查询注意力（GQA）。该项目已在github上开源。


<details>
  <summary>Details</summary>
Motivation: Gated Linear Units (GLU) have shown great potential in enhancing neural network performance.

Method: Introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention.

Result: Experiments demonstrate that GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs.

Conclusion: GLU Attention is lightweight and can seamlessly integrate with other technologies, such as Flash Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention (MHA) variants such as Grouped-Query Attention (GQA).

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [60] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: ROSE is a novel framework that uses multi-objective reinforcement learning to generate topically diverse and contextually rich adversarial prompts for evaluating the safety of large language models.


<details>
  <summary>Details</summary>
Motivation: Existing manual safety benchmarks are limited by their static nature and the intensive labor required to update them, making it difficult to keep pace with rapidly advancing LLMs. Automated adversarial prompt generation offers a promising path toward adaptive evaluation, but current methods suffer from insufficient adversarial topic coverage and weak alignment with real-world contexts.

Method: ROSE is a novel framework that uses multi-objective reinforcement learning to fine-tune an adversarial LLM for generating topically diverse and contextually rich adversarial prompts.

Result: Experiments show that ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with notable improvements in integrated evaluation metrics.

Conclusion: ROSE represents a step toward more practical and reality-oriented safety evaluation of LLMs.

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [61] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: 本文探讨了基于补丁的时间序列基础模型的表示学习机制和泛化能力，揭示了它们如何通过将确定性向量表示扩展为潜在概率分布形式，从而继承大型语言模型的表示和迁移能力，并提供了严格的理论基础以理解、评估和改进这些模型的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据反映了不同的动力系统，使得跨领域迁移看似不可能，但模型的实证成功与此相矛盾。因此，需要解决这一根本性悖论。

Method: 本文从理论和实验角度研究了基于补丁的时间序列基础模型的表示学习机制和泛化能力。

Result: 连续时间序列补丁可以忠实地量化为离散词汇表，其关键统计特性与自然语言高度一致。这种泛化使时间序列模型能够继承大型语言模型的鲁棒表示和迁移能力，从而解释它们在时间任务中的优越性能。

Conclusion: 本文提供了理解、评估和改进大规模时间序列基础模型的安全性和可靠性的严格理论基石。

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [62] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: FedHLM is a communication-efficient HLM framework that combines uncertainty-aware inference with Federated Learning to reduce LLM transmissions and improve edge-AI efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the communication overhead caused by frequent offloading of low-confidence predictions to the LLM in Hybrid Language Models (HLMs).

Method: FedHLM integrates uncertainty-aware inference with Federated Learning (FL) to collaboratively learn token-level uncertainty thresholds. It also uses embedding-based token representations for Peer-to-Peer (P2P) resolution and hierarchical model aggregation.

Result: Experiments on large-scale news classification tasks show that FedHLM reduces LLM transmissions by over 95 percent with negligible accuracy loss.

Conclusion: FedHLM reduces LLM transmissions by over 95 percent with negligible accuracy loss, making it well-suited for scalable and efficient edge-AI applications.

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [63] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: 本文提出了一种新框架，通过整合ResNet和重新构建的2D Transformer生成的热图以及全局加权输入显著性，以增强模型的可解释性。该方法解决了现有方法中的时空不对齐问题，并在临床和工业数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性方法存在时空不对齐的问题，卷积网络无法捕捉全局上下文，而Transformer缺乏局部精度，这在医疗保健和工业监测等安全关键领域限制了可操作的见解。

Method: 我们提出了一种新框架，通过整合ResNet和重新构建的2D Transformer生成的热图以及全局加权输入显著性来增强模型可解释性。该方法结合了梯度加权激活图（ResNet）和Transformer注意力展开，以实现统一的可视化。

Result: 在临床（ECG心律失常检测）和工业（能耗预测）数据集上的实证评估表明，混合框架在PhysioNet数据集上达到了94.1%的准确率（F1 0.93），并在UCI Energy Appliance数据集上将回归误差减少到RMSE = 0.28 kWh（R2 = 0.95），优于单独的ResNet、Transformer和InceptionTime基线3.8-12.4%。

Conclusion: 通过将热图与全局加权输入显著性相结合，我们的方法实现了时空对齐，并在临床和工业数据集上表现出色。此外，通过将融合的热图转换为领域特定的叙述，我们提高了技术输出与利益相关者理解之间的桥梁。

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [64] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: AutoDS 是一种基于贝叶斯惊喜的开放性自主科学发现方法，通过 MCTS 策略探索假设空间，显著优于现有方法，并在多个领域中取得了更好的发现效果。


<details>
  <summary>Details</summary>
Motivation: 当前的自主科学发现（ASD）方法主要依赖于人类指定的研究问题来引导假设生成，而科学发现可以通过让 AI 系统根据自己的标准驱动探索来加速。现有的方法在选择假设时存在不足，例如基于多样性启发式或主观代理指标，因此需要一种更有效的解决方案。

Method: AutoDS 使用贝叶斯惊喜来驱动科学探索，通过量化 LLM 的先验信念到后验信念的认知转变。为了高效探索嵌套假设空间，该方法采用蒙特卡洛树搜索（MCTS）策略，使用惊喜作为奖励函数，并结合渐进式扩展。

Result: 在 21 个跨生物学、经济学、金融学和行为科学等领域的现实数据集上评估 AutoDS，结果表明，在固定预算下，AutoDS 比竞争对手多产生 5-29% 被 LLM 认为出人意料的发现。此外，人类评估显示，三分之二的 AutoDS 发现对领域专家来说是出人意料的。

Conclusion: AutoDS 是一种用于开放性自主科学发现的方法，它通过贝叶斯惊喜驱动科学探索。实验结果表明，在固定预算下，AutoDS 显著优于竞争对手，并且其发现对领域专家来说是出人意料的，这标志着向构建开放性 ASD 系统迈出了重要一步。

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [65] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种用于自动化放射学报告生成的多尺度多模态大型语言模型（μ²LLM），并引入了新颖的μ²Tokenizer来提高报告生成的质量。实验结果表明，该方法在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化放射学报告生成（RRG）旨在从临床影像（如计算机断层扫描（CT）扫描）生成详细的文本报告，以提高诊断的准确性和效率以及管理建议的提供。RRG受到两个关键挑战的困扰：（1）在资源限制下从影像数据中提取相关信息的固有复杂性，以及（2）客观评估模型生成的报告与专家编写的报告之间的差异的困难。

Method: 我们提出了μ²LLM，这是一种用于RRG任务的多尺度多模态大型语言模型。新颖的μ²Tokenizer作为中间层，整合了多尺度视觉标记器和文本标记器的多模态特征，然后通过直接偏好优化（DPO）提高报告生成质量，由GREEN-RedLlama指导。

Result: 在四个大型CT图像-报告医学数据集上的实验结果表明，我们的方法优于现有方法。

Conclusion: 实验结果表明，我们的方法优于现有方法，突显了我们在有限数据上微调的μ²LLMs在RRG任务中的潜力。

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [66] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: 本文提出了一种新的语言建模框架TarFlowLM，通过将语言建模从离散标记空间转移到连续潜在空间，实现了更高的建模灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型虽然在语言建模中取得了显著进展，但它们对离散标记、单向上下文和单次解码的依赖也激发了探索新的建模灵活性设计空间的灵感。

Method: 我们提出了一个基于Transformer的自回归归一化流的框架TarFlowLM，该框架将语言建模从离散的标记空间转移到连续的潜在空间。

Result: 实验结果表明，我们的框架在语言建模基准上表现出强大的似然性能，并突显了框架内在的灵活建模能力。

Conclusion: 我们的框架展示了在语言建模任务中的强大可能性和灵活性。

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [67] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: This paper focuses on improving the long-context modeling capabilities of state-space models (SSMs) by extending the associative recall task to a novel task called joint recall. The authors propose a solution that integrates SSMs with Context-Dependent Sparse Attention (CDSA) and further tailor it to natural language domains with HAX. Experiments show that HAX outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Efficient long-context modeling remains a critical challenge for NLP, as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively.

Method: We extend the associative recall to a novel synthetic task, joint recall, and propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA). We also propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is tailored to natural language domains.

Result: HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA) on both synthetic and real-world long-context benchmarks.

Conclusion: HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA) on both synthetic and real-world long-context benchmarks.

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [68] [Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture](https://arxiv.org/abs/2507.00466)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 本文提出了一种基于transformer的端到端模型，用于表演MIDI中的节拍和强拍跟踪，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在音乐表演MIDI中进行节拍跟踪是一个具有挑战性和重要的任务，但现有的方法主要集中在基于音频的方法上。

Method: 本文提出了一种基于transformer的端到端模型，用于表演MIDI中的节拍和强拍跟踪，利用编码器-解码器架构将MIDI输入序列转换为节拍注释。我们还引入了新的数据预处理技术，包括动态增强和优化的标记化策略，以提高准确性和跨不同数据集的泛化能力。

Result: 我们的模型在A-MAPS、ASAP、GuitarSet和Leduc数据集上的实验结果表明，它优于现有的符号音乐节拍跟踪方法，在各种音乐风格和乐器上实现了具有竞争力的F1分数。

Conclusion: 我们的研究结果表明，transformer架构在符号节拍跟踪中具有潜力，并建议未来将其与自动音乐转录系统集成以增强音乐分析和乐谱生成。

Abstract: Beat tracking in musical performance MIDI is a challenging and important task
for notation-level music transcription and rhythmical analysis, yet existing
methods primarily focus on audio-based approaches. This paper proposes an
end-to-end transformer-based model for beat and downbeat tracking in
performance MIDI, leveraging an encoder-decoder architecture for
sequence-to-sequence translation of MIDI input to beat annotations. Our
approach introduces novel data preprocessing techniques, including dynamic
augmentation and optimized tokenization strategies, to improve accuracy and
generalizability across different datasets. We conduct extensive experiments
using the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model
against state-of-the-art hidden Markov models (HMMs) and deep learning-based
beat tracking methods. The results demonstrate that our model outperforms
existing symbolic music beat tracking approaches, achieving competitive
F1-scores across various musical styles and instruments. Our findings highlight
the potential of transformer architectures for symbolic beat tracking and
suggest future integration with automatic music transcription systems for
enhanced music analysis and score generation.

</details>


### [69] [Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection](https://arxiv.org/abs/2507.00693)
*Yifan Gao,Jiao Fu,Long Guo,Hong Liu*

Main category: cs.SD

TL;DR: 本文介绍了在SpeechWellness Challenge (SW1) 中使用大型语言模型分析语音以识别青少年自杀风险的成果，取得了74%的准确率，证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 早期识别自杀风险对于预防自杀行为至关重要，因此识别和研究与自杀风险相关的模式和标志物已成为当前研究的重点。

Method: 该方法利用大型语言模型（LLM）作为主要工具进行特征提取，并结合传统的声学和语义特征。

Result: 所提出的方法在测试集上达到了74%的准确率，在SW1挑战中排名第一。

Conclusion: 这些发现展示了基于大型语言模型的方法在自杀风险评估中的潜力。

Abstract: Early identification of suicide risk is crucial for preventing suicidal
behaviors. As a result, the identification and study of patterns and markers
related to suicide risk have become a key focus of current research. In this
paper, we present the results of our work in the 1st SpeechWellness Challenge
(SW1), which aims to explore speech as a non-invasive and easily accessible
mental health indicator for identifying adolescents at risk of suicide.Our
approach leverages large language model (LLM) as the primary tool for feature
extraction, alongside conventional acoustic and semantic features. The proposed
method achieves an accuracy of 74\% on the test set, ranking first in the SW1
challenge. These findings demonstrate the potential of LLM-based methods for
analyzing speech in the context of suicide risk assessment.

</details>


### [70] [Multi-interaction TTS toward professional recording reproduction](https://arxiv.org/abs/2507.00808)
*Hiroki Kanagawa,Kenichi Fujita,Aya Watanabe,Yusuke Ijima*

Main category: cs.SD

TL;DR: 本文提出了一种具有多步骤交互的文本到语音合成方法，允许用户直观且快速地修正合成语音，模拟了配音导演与配音演员之间的关系，并通过实验验证了其在迭代风格修正中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在实际录音中，语音导演通过提供反馈来迭代地改进语音演员的表演以达到预期结果，但在文本到语音合成（TTS）中这一过程被忽视了。因此，即使合成语音偏离了用户的预期风格，也无法进行细粒度的风格修正。

Method: 我们提出了一种具有多步骤交互的TTS方法，允许用户直观且快速地修正合成语音。我们的方法模拟了TTS模型与其用户之间的互动，以模仿配音演员和配音导演之间的关系。

Result: 实验表明，所提出的模型及其相应的数据集能够根据用户的指导进行迭代风格修正，从而展示了其多交互能力。

Conclusion: 实验表明，所提出的模型及其相应的数据集能够根据用户的指导进行迭代风格修正，从而展示了其多交互能力。

Abstract: Voice directors often iteratively refine voice actors' performances by
providing feedback to achieve the desired outcome. While this iterative
feedback-based refinement process is important in actual recordings, it has
been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained
style refinement after the initial synthesis is not possible, even though the
synthesized speech often deviates from the user's intended style. To address
this issue, we propose a TTS method with multi-step interaction that allows
users to intuitively and rapidly refine synthetized speech. Our approach models
the interaction between the TTS model and its user to emulate the relationship
between voice actors and voice directors. Experiments show that the proposed
model with its corresponding dataset enable iterative style refinements in
accordance with users' directions, thus demonstrating its multi-interaction
capability. Sample audios are available: https://ntt-hilab-gensp.
github.io/ssw13multiinteraction_tts/

</details>

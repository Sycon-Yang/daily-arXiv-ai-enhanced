{"id": "2506.03259", "pdf": "https://arxiv.org/pdf/2506.03259", "abs": "https://arxiv.org/abs/2506.03259", "authors": ["Michael E. Garcia-Alcoser", "Mobina GhojoghNejad", "Fakrul Islam Tushar", "David Kim", "Kyle J. Lafata", "Geoffrey D. Rubin", "Joseph Y. Lo"], "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems", "categories": ["cs.CL", "I.2.7"], "comment": "23 pages, 10 figures, to be submitted in Radiology: Artificial\n  Intelligence", "summary": "Purpose: This study aims to evaluate the effectiveness of large language\nmodels (LLMs) in automating disease annotation of CT radiology reports. We\ncompare a rule-based algorithm (RBA), RadBERT, and three lightweight\nopen-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)\nCT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 CT reports\nfrom 29,540 patients, with 1,789 CAP reports manually annotated across three\norgan systems. External validation was conducted using the CT-RATE dataset.\nThree open-weight LLMs were tested with zero-shot prompting. Performance was\nevaluated using Cohen's Kappa and micro/macro-averaged F1 scores.\n  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and\nGemma-3 27B showed the highest agreement ($\\kappa$ median: 0.87). On the\nmanually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed\nby Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE\ndataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3\n27B close behind (0.89). Performance differences were mainly due to differing\nlabeling practices, especially for lung atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report\nannotation and generalize across organ systems with zero-shot prompting.\nHowever, binary labels alone cannot capture the full nuance of report language.\nLLMs can provide a flexible, efficient solution aligned with clinical judgment\nand user needs.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316CT\u653e\u5c04\u5b66\u62a5\u544a\u75be\u75c5\u6807\u6ce8\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u96f6\u6837\u672c\u63d0\u793a\u4e0b\u53ef\u4ee5\u8de8\u5668\u5b98\u7cfb\u7edf\u6cdb\u5316\u3002", "motivation": "\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u5316CT\u653e\u5c04\u5b66\u62a5\u544a\u75be\u75c5\u6807\u6ce8\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5c06\u57fa\u4e8e\u89c4\u5219\u7684\u7b97\u6cd5\uff08RBA\uff09\u3001RadBERT\u4ee5\u53ca\u4e09\u79cd\u8f7b\u91cf\u7ea7\u5f00\u6e90\u6743\u91cd\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u7528\u4e8e\u80f8\u90e8\u3001\u8179\u90e8\u548c\u9aa8\u76c6\uff08CAP\uff09CT\u62a5\u544a\u7684\u591a\u75be\u75c5\u6807\u8bb0\u3002", "method": "\u672c\u7814\u7a76\u56de\u987e\u6027\u5206\u6790\u4e86\u6765\u81ea29,540\u540d\u60a3\u8005\u768440,833\u4efdCT\u62a5\u544a\uff0c\u5176\u4e2d\u4e09\u7ec4\u5668\u5b98\u7cfb\u7edf\u4e2d\u5171\u67091,789\u4efdCAP\u62a5\u544a\u88ab\u624b\u52a8\u6ce8\u91ca\u3002\u4f7f\u7528Cohen's Kappa\u548c\u5fae\u89c2/\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\u8bc4\u4f30\u6027\u80fd\u3002\u4e09\u79cd\u5f00\u6e90\u6743\u91cd\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u96f6\u6837\u672c\u63d0\u793a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5728Duke CAP\u62a5\u544a\u768412,197\u4efd\u62a5\u544a\u4e2d\uff0cLlama-3.1 8B\u548cGemma-3 27B\u663e\u793a\u51fa\u4e86\u6700\u9ad8\u7684\u534f\u8bae\u4e00\u81f4\u6027\uff08\u03ba\u4e2d\u4f4d\u6570\uff1a0.87\uff09\u3002\u5728\u4eba\u5de5\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u4e0a\uff0cGemma-3 27B\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5b8f\u89c2F1\u5f97\u5206\uff080.82\uff09\uff0c\u5176\u6b21\u662fLlama-3.1 8B\uff080.79\uff09\uff0c\u800cRBA\u5f97\u5206\u6700\u4f4e\uff080.64\uff09\u3002\u5728CT-RATE\u6570\u636e\u96c6\uff08\u4ec5\u9650\u80ba/\u80f8\u819c\uff09\u4e0a\uff0cLlama-3.1 8B\u8868\u73b0\u6700\u4f73\uff080.91\uff09\uff0cGemma-3 27B\u7d27\u968f\u5176\u540e\uff080.89\uff09\u3002\u6027\u80fd\u5dee\u5f02\u4e3b\u8981\u7531\u4e8e\u4e0d\u540c\u7684\u6807\u6ce8\u5b9e\u8df5\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u80ba\u4e0d\u5f20\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728CT\u62a5\u544a\u6ce8\u91ca\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u96f6\u6837\u672c\u63d0\u793a\u4e0b\u53ef\u4ee5\u8de8\u5668\u5b98\u7cfb\u7edf\u6cdb\u5316\u3002\u7136\u800c\uff0c\u5355\u4e00\u6807\u7b7e\u65e0\u6cd5\u6355\u6349\u62a5\u544a\u8bed\u8a00\u7684\u6240\u6709\u7ec6\u5fae\u5dee\u522b\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0e\u4e34\u5e8a\u5224\u65ad\u548c\u7528\u6237\u9700\u6c42\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2506.03268", "pdf": "https://arxiv.org/pdf/2506.03268", "abs": "https://arxiv.org/abs/2506.03268", "authors": ["Cristiano Chesi"], "title": "A conclusive remark on linguistic theorizing and language modeling", "categories": ["cs.CL"], "comment": null, "summary": "This is the final remark on the replies received to my target paper in the\nItalian Journal of Linguistics", "AI": {"tldr": "Final remarks on replies to a linguistics paper.", "motivation": "To summarize and conclude the discussion on the target paper.", "method": "Compilation and analysis of replies.", "result": "Consolidated insights from various responses.", "conclusion": "Closure of the discussion on the target linguistics paper."}}
{"id": "2506.03278", "pdf": "https://arxiv.org/pdf/2506.03278", "abs": "https://arxiv.org/abs/2506.03278", "authors": ["Christodoulos Constantinides", "Dhaval Patel", "Shuxin Lin", "Claudio Guerrero", "Sunil Dagajirao Patil", "Jayant Kalagnanam"], "title": "FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes", "categories": ["cs.CL"], "comment": null, "summary": "We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)\nbenchmarking system designed to assess the ability of Large Language Models\n(LLMs) to reason and understand complex, domain-specific scenarios in Industry\n4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects\nof reasoning through failure modes, sensor data, and the relationships between\nthem across various industrial assets. Through this work, we envision a\nparadigm shift where modeling decisions are not only data-driven using\nstatistical tools like correlation analysis and significance tests, but also\ndomain-driven by specialized LLMs which can reason about the key contributors\nand useful patterns that can be captured with feature engineering. We evaluate\nthe Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and\nMistral-on FailureSensorIQ from different lens using\nPerturbation-Uncertainty-Complexity analysis, Expert Evaluation study,\nAsset-Specific Knowledge Gap analysis, ReAct agent using external\nknowledge-bases. Even though closed-source models with strong reasoning\ncapabilities approach expert-level performance, the comprehensive benchmark\nreveals a significant drop in performance that is fragile to perturbations,\ndistractions, and inherent knowledge gaps in the models. We also provide a\nreal-world case study of how LLMs can drive the modeling decisions on 3\ndifferent failure prediction datasets related to various assets. We release:\n(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ\nbenchmark and Hugging Face leaderboard based on MCQA built from non-textual\ndata found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature\nselection scikit-learn pipeline. The software is available at\nhttps://github.com/IBM/FailureSensorIQ.", "AI": {"tldr": "FailureSensorIQ is a new benchmarking system for evaluating Large Language Models' reasoning capabilities in complex industrial scenarios.", "motivation": "To assess LLMs' ability to reason about failure modes and sensor data in Industry 4.0.", "method": "Evaluates LLMs using various analyses and real-world case studies.", "result": "Closed-source models show strong reasoning capabilities but have fragilities and knowledge gaps.", "conclusion": "FailureSensorIQ provides valuable insights into LLMs' performance in industrial settings."}}
{"id": "2506.03292", "pdf": "https://arxiv.org/pdf/2506.03292", "abs": "https://arxiv.org/abs/2506.03292", "authors": ["Jiuding Sun", "Sidharth Baskaran", "Zhengxuan Wu", "Michael Sklar", "Christopher Potts", "Atticus Geiger"], "title": "HyperSteer: Activation Steering at Scale with Hypernetworks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering language models (LMs) by modifying internal activations is a popular\napproach for controlling text generation. Unsupervised dictionary learning\nmethods, e.g., sparse autoencoders, can be scaled to produce many steering\nvectors, but lack guarantees on the individual efficacy of each vector and\ncontrol over the coverage of relevant steering tasks. In contrast, supervised\nmethods for constructing steering vectors are targeted and effective, but\nrequire more data collection and training for each additional steering vector\nproduced. In this work, we introduce HyperSteer, a family of hypernetwork-based\narchitectures which are trained end-to-end to generate steering vectors\nconditioned on the natural language steering prompts and the internals of the\nsteered LM. In our evaluations, we show that scaling HyperSteer with thousands\nof steering prompts exceeds the performance of state-of-the-art activation\nsteering methods, even on steering prompts never seen during training.\nMoreover, HyperSteer performs on par with steering-via-prompting.", "AI": {"tldr": "Introduce HyperSteer, an end-to-end trained hypernetwork-based method that generates steering vectors, outperforming current state-of-the-art methods and performing equivalently to steering-via-prompting.", "motivation": "To address the limitations of unsupervised and supervised methods for steering language models.", "method": "HyperSteer uses hypernetwork-based architectures trained end-to-end to generate steering vectors conditioned on natural language steering prompts and the internals of the steered LM.", "result": "HyperSteer surpasses the performance of existing methods and performs as well as steering-via-prompting.", "conclusion": "Scaling HyperSteer with thousands of steering prompts outperforms state-of-the-art activation steering methods."}}
{"id": "2506.03295", "pdf": "https://arxiv.org/pdf/2506.03295", "abs": "https://arxiv.org/abs/2506.03295", "authors": ["Yubo Wang", "Ping Nie", "Kai Zou", "Lijun Wu", "Wenhu Chen"], "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.", "AI": {"tldr": "Demonstrating that Critique Fine-Tuning on a single problem can effectively enhance the reasoning abilities of large language models, showing improvements comparable to reinforcement learning but with much lower computational costs.", "motivation": "To find a more efficient way to unleash the reasoning potential of powerful base LLMs instead of using expensive and unstable reinforcement learning.", "method": "Construct critique data by collecting diverse model-generated solutions to a single problem and use teacher LLMs to provide detailed critiques. Then fine-tune Qwen and Llama family models on the CFT data.", "result": "Significant performance gains across diverse reasoning tasks with just 5 GPU hours of training.", "conclusion": "Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs."}}
{"id": "2506.03301", "pdf": "https://arxiv.org/pdf/2506.03301", "abs": "https://arxiv.org/abs/2506.03301", "authors": ["Daham M. Mustafa", "Abhishek Nadgeri", "Diego Collarana", "Benedikt T. Arnold", "Christoph Quix", "Christoph Lange", "Stefan Decker"], "title": "From Instructions to ODRL Usage Policies: An Ontology Guided Approach", "categories": ["cs.CL", "F.2.2; I.2.7; H.3.3"], "comment": "The paper is accepted at LLM+KG: International Workshop on Data\n  Management Opportunities in Unifying Large Language Models + Knowledge\n  Graphs, VLDB 2024, August 26, 2024, Guangzhou, China.\n  https://vldb.org/workshops/2024/proceedings/LLM+KG/LLM+KG-15.pdf", "summary": "This study presents an approach that uses large language models such as GPT-4\nto generate usage policies in the W3C Open Digital Rights Language ODRL\nautomatically from natural language instructions. Our approach uses the ODRL\nontology and its documentation as a central part of the prompt. Our research\nhypothesis is that a curated version of existing ontology documentation will\nbetter guide policy generation. We present various heuristics for adapting the\nODRL ontology and its documentation to guide an end-to-end KG construction\nprocess. We evaluate our approach in the context of dataspaces, i.e.,\ndistributed infrastructures for trustworthy data exchange between multiple\nparticipating organizations for the cultural domain. We created a benchmark\nconsisting of 12 use cases of varying complexity. Our evaluation shows\nexcellent results with up to 91.95% accuracy in the resulting knowledge graph.", "AI": {"tldr": "This study introduces an approach using large language models like GPT-4 to automatically create ODRL usage policies from natural language instructions. The research demonstrates high accuracy (up to 91.95%) in knowledge graphs generated from 12 complex use cases.", "motivation": "To develop a method that uses existing ontology documentation to guide policy generation automatically.", "method": "Using the ODRL ontology and its documentation within prompts to adapt the ontology and its documentation for knowledge graph construction.", "result": "Achieved up to 91.95% accuracy in the resulting knowledge graph from 12 use cases of varying complexity.", "conclusion": "The curated version of existing ontology documentation effectively guides policy generation."}}
{"id": "2506.03303", "pdf": "https://arxiv.org/pdf/2506.03303", "abs": "https://arxiv.org/abs/2506.03303", "authors": ["Mustafa Eyceoz", "Nikhil Shivakumar Nayak", "Hao Wang", "Ligong Han", "Akash Srivastava"], "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6; I.2.4"], "comment": "10 pages, 4 figures, 9 tables", "summary": "Modern causal language models stack many attention blocks to improve\nperformance, but not all blocks are necessary for every task. We propose\nHopscotch, a simple yet effective method that identifies and skips attention\nblocks with least contributions to a task and adapts to preserve output\nquality. Hopscotch jointly optimizes which blocks to skip and how to scale the\noutputs of the remaining layers. By introducing lightweight, trainable scaling\nparameters to attention and MLP blocks, it mitigates distribution shifts in\nhidden states caused by removing attention blocks. Hopscotch does not modify\nmodel weights or require access to pretraining or instruction-tuning data, and\nis compatible with existing model compression techniques. When applied to\n$\\texttt{Llama-3.1-8B}$ and $\\texttt{Qwen2.5-7B}$, Hopscotch achieves less than\na 2% drop in performance even after skipping four attention blocks.", "AI": {"tldr": "A new method called Hopscotch is proposed to identify and skip unnecessary attention blocks in causal language models without significant performance loss.", "motivation": "To improve efficiency by skipping unnecessary attention blocks in causal language models.", "method": "Jointly optimize which blocks to skip and how to scale outputs of the remaining layers with lightweight trainable scaling parameters.", "result": "Achieves less than a 2% drop in performance when skipping four attention blocks on Llama-3.1-8B and Qwen2.5-7B.", "conclusion": "Hopscotch is a simple and effective method that can be used with existing model compression techniques."}}
{"id": "2506.03310", "pdf": "https://arxiv.org/pdf/2506.03310", "abs": "https://arxiv.org/abs/2506.03310", "authors": ["Guillermo Marco", "Julio Gonzalo", "V\u00edctor Fresno"], "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing", "categories": ["cs.CL", "cs.HC"], "comment": "Camera-ready version, 14 pages, 3 figures. Accepted to Findings of\n  the Association for Computational Linguistics (ACL) 2025. Code & data:\n  https://github.com/grmarco/the-reader-is-the-metric", "summary": "Recent studies comparing AI-generated and human-authored literary texts have\nproduced conflicting results: some suggest AI already surpasses human quality,\nwhile others argue it still falls short. We start from the hypothesis that such\ndivergences can be largely explained by genuine differences in how readers\ninterpret and value literature, rather than by an intrinsic quality of the\ntexts evaluated. Using five public datasets (1,471 stories, 101 annotators\nincluding critics, students, and lay readers), we (i) extract 17 reference-less\ntextual features (e.g., coherence, emotional variance, average sentence\nlength...); (ii) model individual reader preferences, deriving feature\nimportance vectors that reflect their textual priorities; and (iii) analyze\nthese vectors in a shared \"preference space\". Reader vectors cluster into two\nprofiles: 'surface-focused readers' (mainly non-experts), who prioritize\nreadability and textual richness; and 'holistic readers' (mainly experts), who\nvalue thematic development, rhetorical variety, and sentiment dynamics. Our\nresults quantitatively explain how measurements of literary quality are a\nfunction of how text features align with each reader's preferences. These\nfindings advocate for reader-sensitive evaluation frameworks in the field of\ncreative text generation.", "AI": {"tldr": "This study investigates why different studies comparing AI-generated and human-authored literary texts yield conflicting results. It discovers that these differences are due to reader preferences rather than the intrinsic quality of the texts.", "motivation": "To understand why there are conflicting results in studies comparing AI-generated and human-authored literary texts.", "method": "Extracting textual features from five public datasets and modeling individual reader preferences to derive feature importance vectors.", "result": "Reader vectors cluster into two profiles: 'surface-focused readers' and 'holistic readers'. The results show that measurements of literary quality depend on how text features align with each reader's preferences.", "conclusion": "The study suggests that reader-sensitive evaluation frameworks should be used in the field of creative text generation."}}
{"id": "2506.03312", "pdf": "https://arxiv.org/pdf/2506.03312", "abs": "https://arxiv.org/abs/2506.03312", "authors": ["Celia Chen", "Scotty Beland", "Ingo Burghardt", "Jill Byczek", "William J. Conway", "Eric Cotugno", "Sadaf Davre", "Megan Fletcher", "Rajesh Kumar Gnanasekaran", "Kristin Hamilton", "Marilyn Harbert", "Jordan Heustis", "Tanaya Jha", "Emily Klein", "Hayden Kramer", "Alex Leitch", "Jessica Perkins", "Casi Sherman", "Celia Sterrn", "Logan Stevens", "Rebecca Zarrella", "Jennifer Golbeck"], "title": "Cross-Platform Violence Detection on Social Media: A Dataset and Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "In Proceedings of the 17th ACM Web Science Conference (WebSci '25). 9\n  pages", "summary": "Violent threats remain a significant problem across social media platforms.\nUseful, high-quality data facilitates research into the understanding and\ndetection of malicious content, including violence. In this paper, we introduce\na cross-platform dataset of 30,000 posts hand-coded for violent threats and\nsub-types of violence, including political and sexual violence. To evaluate the\nsignal present in this dataset, we perform a machine learning analysis with an\nexisting dataset of violent comments from YouTube. We find that, despite\noriginating from different platforms and using different coding criteria, we\nachieve high classification accuracy both by training on one dataset and\ntesting on the other, and in a merged dataset condition. These results have\nimplications for content-classification strategies and for understanding\nviolent content across social media.", "AI": {"tldr": "This paper presents a cross-platform dataset of 30,000 hand-coded posts for violent threats and sub-types of violence. Machine learning analysis shows high classification accuracy when trained on one dataset and tested on another or in a merged dataset condition.", "motivation": "To facilitate research into the understanding and detection of malicious content, including violence, by providing a useful, high-quality dataset.", "method": "Introducing a cross-platform dataset of 30,000 posts hand-coded for violent threats and sub-types of violence, and performing a machine learning analysis with an existing dataset of violent comments from YouTube.", "result": "High classification accuracy is achieved both by training on one dataset and testing on the other, and in a merged dataset condition.", "conclusion": "These results have implications for content-classification strategies and for understanding violent content across social media."}}
{"id": "2506.03357", "pdf": "https://arxiv.org/pdf/2506.03357", "abs": "https://arxiv.org/abs/2506.03357", "authors": ["Aldan Creo", "H\u00e9ctor Cerezo-Costas", "Pedro Alonso-Doval", "Maximiliano Hormaz\u00e1bal-Lagos"], "title": "Ask a Local: Detecting Hallucinations With Specialized Model Divergence", "categories": ["cs.CL", "cs.AI"], "comment": "Supplementary materials: https://github.com/ACMCMC/ask-a-local", "summary": "Hallucinations in large language models (LLMs) - instances where models\ngenerate plausible but factually incorrect information - present a significant\nchallenge for AI.\n  We introduce \"Ask a Local\", a novel hallucination detection method exploiting\nthe intuition that specialized models exhibit greater surprise when\nencountering domain-specific inaccuracies. Our approach computes divergence\nbetween perplexity distributions of language-specialized models to identify\npotentially hallucinated spans. Our method is particularly well-suited for a\nmultilingual context, as it naturally scales to multiple languages without the\nneed for adaptation, relying on external data sources, or performing training.\nMoreover, we select computationally efficient models, providing a scalable\nsolution that can be applied to a wide range of languages and domains.\n  Our results on a human-annotated question-answer dataset spanning 14\nlanguages demonstrate consistent performance across languages, with\nIntersection-over-Union (IoU) scores around 0.3 and comparable Spearman\ncorrelation values. Our model shows particularly strong performance on Italian\nand Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining\ncross-lingual effectiveness without language-specific adaptations. We release\nour code and architecture to facilitate further research in multilingual\nhallucination detection.", "AI": {"tldr": "We propose 'Ask a Local', a new method for detecting hallucinations in large language models. This method leverages the concept that specialized models are more surprised by inaccuracies in their domain, using perplexity distribution divergence to spot potentially hallucinated parts.", "motivation": "To address the issue of hallucinations in large language models which generate plausible but factually incorrect information.", "method": "Computing divergence between perplexity distributions of language-specialized models to identify potentially hallucinated spans.", "result": "The method shows consistent performance across 14 languages, with high Intersection-over-Union (IoU) scores and comparable Spearman correlation values. It performs especially well on Italian and Catalan.", "conclusion": "Our method is effective in detecting hallucinations in multilingual contexts without needing language-specific adaptations or additional training."}}
{"id": "2506.03360", "pdf": "https://arxiv.org/pdf/2506.03360", "abs": "https://arxiv.org/abs/2506.03360", "authors": ["Zihui Ma", "Lingyao Li", "Juan Li", "Wenyue Hua", "Jingxiao Liu", "Qingyuan Feng", "Yuki Miura"], "title": "A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Rapid, fine-grained disaster damage assessment is essential for effective\nemergency response, yet remains challenging due to limited ground sensors and\ndelays in official reporting. Social media provides a rich, real-time source of\nhuman-centric observations, but its multimodal and unstructured nature presents\nchallenges for traditional analytical methods. In this study, we propose a\nstructured Multimodal, Multilingual, and Multidimensional (3M) pipeline that\nleverages multimodal large language models (MLLMs) to assess disaster impacts.\nWe evaluate three foundation models across two major earthquake events using\nboth macro- and micro-level analyses. Results show that MLLMs effectively\nintegrate image-text signals and demonstrate a strong correlation with\nground-truth seismic data. However, performance varies with language,\nepicentral distance, and input modality. This work highlights the potential of\nMLLMs for disaster assessment and provides a foundation for future research in\napplying MLLMs to real-time crisis contexts. The code and data are released at:\nhttps://github.com/missa7481/EMNLP25_earthquake", "AI": {"tldr": "This study proposes a 3M pipeline using multimodal large language models (MLLMs) to assess disaster impacts from social media during earthquakes.", "motivation": "Traditional methods face challenges due to limited ground sensors and delays in official reporting. Social media offers real-time human-centric observations but is multimodal and unstructured.", "method": "The 3M pipeline leverages MLLMs to integrate image-text signals from social media.", "result": "MLLMs effectively integrate image-text signals and correlate well with ground-truth seismic data, but performance varies with language, epicentral distance, and input modality.", "conclusion": "This work shows the potential of MLLMs for disaster assessment and supports future research in applying them to real-time crisis contexts."}}
{"id": "2506.03408", "pdf": "https://arxiv.org/pdf/2506.03408", "abs": "https://arxiv.org/abs/2506.03408", "authors": ["Yi Xu", "Ruining Yang", "Yitian Zhang", "Yizhou Wang", "Jianglin Lu", "Mingyuan Zhang", "Lili Su", "Yun Fu"], "title": "Trajectory Prediction Meets Large Language Models: A Survey", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction.", "AI": {"tldr": "This survey provides a comprehensive overview of integrating language-driven techniques into trajectory prediction using large language models.", "motivation": "The rapid development of large language models has increased interest in applying language-driven techniques to trajectory prediction.", "method": "Categorizes recent work into five directions: trajectory prediction via language modeling paradigms, direct trajectory prediction with pretrained language models, language-guided scene understanding, language-driven data generation, and language-based reasoning and interpretability.", "result": "Analyzes representative methods, highlights core design choices, and identifies open challenges in each category.", "conclusion": "This survey offers a unified perspective on how language can enrich trajectory prediction by bridging natural language processing and trajectory prediction."}}
{"id": "2506.03424", "pdf": "https://arxiv.org/pdf/2506.03424", "abs": "https://arxiv.org/abs/2506.03424", "authors": ["Nicole R Schneider", "Nandini Ramachandran", "Kent O'Sullivan", "Hanan Samet"], "title": "DistRAG: Towards Distance-Based Spatial Reasoning in LLMs", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Many real world tasks where Large Language Models (LLMs) can be used require\nspatial reasoning, like Point of Interest (POI) recommendation and itinerary\nplanning. However, on their own LLMs lack reliable spatial reasoning\ncapabilities, especially about distances. To address this problem, we develop a\nnovel approach, DistRAG, that enables an LLM to retrieve relevant spatial\ninformation not explicitly learned during training. Our method encodes the\ngeodesic distances between cities and towns in a graph and retrieves a context\nsubgraph relevant to the question. Using this technique, our method enables an\nLLM to answer distance-based reasoning questions that it otherwise cannot\nanswer. Given the vast array of possible places an LLM could be asked about,\nDistRAG offers a flexible first step towards providing a rudimentary `world\nmodel' to complement the linguistic knowledge held in LLMs.", "AI": {"tldr": "This paper introduces DistRAG, a novel method enabling Large Language Models (LLMs) to perform spatial reasoning by retrieving relevant spatial information through geodesic distance encoding in a graph.", "motivation": "Enhancing LLMs with reliable spatial reasoning capabilities, especially about distances, for tasks requiring such skills like POI recommendation and itinerary planning.", "method": "Encoding geodesic distances between locations in a graph and retrieving a context subgraph related to the question.", "result": "DistRAG allows LLMs to answer distance-based reasoning questions they couldn't handle before.", "conclusion": "DistRAG provides a flexible way to incorporate a basic 'world model' into LLMs, complementing their linguistic knowledge."}}
{"id": "2506.03434", "pdf": "https://arxiv.org/pdf/2506.03434", "abs": "https://arxiv.org/abs/2506.03434", "authors": ["Ahmad Dawar Hakimi", "Ali Modarressi", "Philipp Wicke", "Hinrich Sch\u00fctze"], "title": "Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Understanding how large language models (LLMs) acquire and store factual\nknowledge is crucial for enhancing their interpretability and reliability. In\nthis work, we analyze the evolution of factual knowledge representation in the\nOLMo-7B model by tracking the roles of its attention heads and feed forward\nnetworks (FFNs) over the course of pre-training. We classify these components\ninto four roles: general, entity, relation-answer, and fact-answer specific,\nand examine their stability and transitions. Our results show that LLMs\ninitially depend on broad, general-purpose components, which later specialize\nas training progresses. Once the model reliably predicts answers, some\ncomponents are repurposed, suggesting an adaptive learning process. Notably,\nattention heads display the highest turnover. We also present evidence that\nFFNs remain more stable throughout training. Furthermore, our probing\nexperiments reveal that location-based relations converge to high accuracy\nearlier in training than name-based relations, highlighting how task complexity\nshapes acquisition dynamics. These insights offer a mechanistic view of\nknowledge formation in LLMs.", "AI": {"tldr": "This study investigates how a large language model (OLMo-7B) develops its factual knowledge during pre-training by analyzing the roles of attention heads and feed forward networks (FFNs). It shows that initially general components become specialized as training proceeds, with attention heads being more changeable than FFNs. Additionally, it finds that location-based relations are learned faster than name-based ones.", "motivation": "To enhance the interpretability and reliability of large language models by understanding how they acquire and store factual knowledge.", "method": "Analyzing the roles of attention heads and FFNs in the OLMo-7B model during pre-training and tracking their changes over time.", "result": "LLMs initially rely on broad, general-purpose components that later specialize. Some components are repurposed once reliable answer predictions are made. Attention heads have higher turnover compared to FFNs. Location-based relations are learned faster than name-based relations.", "conclusion": "This analysis provides a mechanistic understanding of knowledge formation in LLMs, emphasizing the adaptive learning process and the impact of task complexity on knowledge acquisition."}}
{"id": "2506.03458", "pdf": "https://arxiv.org/pdf/2506.03458", "abs": "https://arxiv.org/abs/2506.03458", "authors": ["Zahra Bokaei", "Walid Magdy", "Bonnie Webber"], "title": "Culture Matters in Toxic Language Detection in Persian", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "Toxic language detection is crucial for creating safer online environments\nand limiting the spread of harmful content. While toxic language detection has\nbeen under-explored in Persian, the current work compares different methods for\nthis task, including fine-tuning, data enrichment, zero-shot and few-shot\nlearning, and cross-lingual transfer learning. What is especially compelling is\nthe impact of cultural context on transfer learning for this task: We show that\nthe language of a country with cultural similarities to Persian yields better\nresults in transfer learning. Conversely, the improvement is lower when the\nlanguage comes from a culturally distinct country. Warning: This paper contains\nexamples of toxic language that may disturb some readers. These examples are\nincluded for the purpose of research on toxic detection.", "AI": {"tldr": "This paper explores various methods for detecting toxic language in Persian, highlighting the influence of cultural context on transfer learning effectiveness.", "motivation": "Creating safer online environments by limiting the spread of harmful content, with a focus on Persian language toxic language detection which has been under-explored.", "method": "Comparing different methods such as fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning.", "result": "Cultural context significantly impacts transfer learning effectiveness; languages similar to Persian yield better results than culturally distinct ones.", "conclusion": "Understanding the role of cultural context is crucial for improving cross-lingual transfer learning in toxic language detection."}}
{"id": "2506.03476", "pdf": "https://arxiv.org/pdf/2506.03476", "abs": "https://arxiv.org/abs/2506.03476", "authors": ["Chuyuan Li", "Raymond Li", "Thalia S. Field", "Giuseppe Carenini"], "title": "Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection", "categories": ["cs.CL"], "comment": null, "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that\nleads to dementia, and early intervention can greatly benefit from analyzing\nlinguistic abnormalities. In this work, we explore the potential of Large\nLanguage Models (LLMs) as health assistants for AD diagnosis from\npatient-generated text using in-context learning (ICL), where tasks are defined\nthrough a few input-output examples. Empirical results reveal that conventional\nICL methods, such as similarity-based selection, perform poorly for AD\ndiagnosis, likely due to the inherent complexity of this task. To address this,\nwe introduce Delta-KNN, a novel demonstration selection strategy that enhances\nICL performance. Our method leverages a delta score to assess the relative\ngains of each training example, coupled with a KNN-based retriever that\ndynamically selects optimal \"representatives\" for a given input. Experiments on\ntwo AD detection datasets across three open-source LLMs demonstrate that\nDelta-KNN consistently outperforms existing ICL baselines. Notably, when using\nthe Llama-3.1 model, our approach achieves new state-of-the-art results,\nsurpassing even supervised classifiers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eKNN\u7684\u6f14\u793a\u9009\u62e9\u7b56\u7565Delta-KNN\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u8bca\u65ad\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u662f\u4e00\u79cd\u8fdb\u884c\u6027\u7684\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff0c\u4f1a\u5bfc\u81f4\u75f4\u5446\uff0c\u65e9\u671f\u5e72\u9884\u53ef\u4ee5\u4ece\u5206\u6790\u8bed\u8a00\u5f02\u5e38\u4e2d\u53d7\u76ca\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u7684\u5065\u5eb7\u52a9\u624b\u7684\u6f5c\u529b\uff0c\u4f7f\u7528\u5b9e\u4f8b\u5b66\u4e60\u4ece\u60a3\u8005\u751f\u6210\u7684\u6587\u672c\u4e2d\u8fdb\u884c\u8bca\u65ad\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86Delta-KNN\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u6f14\u793a\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u5b9e\u4f8b\u5b66\u4e60\uff08ICL\uff09\u7684\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u5229\u7528delta\u5206\u6570\u6765\u8bc4\u4f30\u6bcf\u4e2a\u8bad\u7ec3\u4f8b\u5b50\u7684\u76f8\u5bf9\u6536\u76ca\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8eKNN\u7684\u68c0\u7d22\u5668\u52a8\u6001\u9009\u62e9\u7ed9\u5b9a\u8f93\u5165\u7684\u6700\u4f73\u201c\u4ee3\u8868\u201d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f20\u7edf\u7684\u76f8\u4f3c\u5ea6\u57fa\u7840\u9009\u62e9\u7b49\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u7531\u4e8e\u8fd9\u9879\u4efb\u52a1\u7684\u56fa\u6709\u590d\u6742\u6027\u3002\u800c\u6211\u4eec\u7684Delta-KNN\u65b9\u6cd5\u5728\u4e09\u4e2a\u5f00\u6e90LLM\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u4e00\u81f4\u6027\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u5b9e\u4f8b\u5b66\u4e60\u7684\u57fa\u7ebf\u3002\u7279\u522b\u662f\u5f53\u4f7f\u7528Llama-3.1\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u76d1\u7763\u5206\u7c7b\u5668\u3002"}}
{"id": "2506.03483", "pdf": "https://arxiv.org/pdf/2506.03483", "abs": "https://arxiv.org/abs/2506.03483", "authors": ["Jun Rao", "Zepeng Lin", "Xuebo Liu", "Xiaopeng Ke", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "Large Language Models (LLMs) often require domain-specific fine-tuning to\naddress targeted tasks, which risks degrading their general capabilities.\nMaintaining a balance between domain-specific enhancements and general model\nutility is a key challenge. This paper proposes a novel approach named APT\n(Weakness Case Acquisition and Iterative Preference Training) to enhance\ndomain-specific performance with self-generated dis-preferred weakness data\n(bad cases and similar cases). APT uniquely focuses on training the model using\nonly those samples where errors occur, alongside a small, similar set of\nsamples retrieved for this purpose. This targeted training minimizes\ninterference with the model's existing knowledge base, effectively retaining\ngeneric capabilities. Experimental results on the LLama-2 and Mistral-V0.3\nmodels across various benchmarks demonstrate that APT ensures no reduction in\ngeneric capacity and achieves superior performance on downstream tasks compared\nto various existing methods. This validates our method as an effective strategy\nfor enhancing domain-specific capabilities without sacrificing the model's\nbroader applicability.", "AI": {"tldr": "APT is a novel approach that improves domain-specific performance of LLMs by using self-generated weak data and iterative preference training, ensuring no degradation in generic capabilities.", "motivation": "To address the challenge of balancing domain-specific enhancements and general model utility in LLMs.", "method": "APT uses only error samples and a small set of retrieved similar samples for targeted training.", "result": "APT shows superior performance on downstream tasks compared to other methods while maintaining generic capabilities.", "conclusion": "APT is an effective strategy for enhancing domain-specific capabilities without sacrificing broader model applicability."}}
{"id": "2506.03484", "pdf": "https://arxiv.org/pdf/2506.03484", "abs": "https://arxiv.org/abs/2506.03484", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Atnafu Lambebo Tonja", "Hassan Shakil", "Samer Iskander", "Olga Kolesnikova", "Jugal Kalita"], "title": "Explainable AI: XAI-Guided Context-Aware Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Explainable AI (XAI) has emerged as a powerful tool for improving the\nperformance of AI models, going beyond providing model transparency and\ninterpretability. The scarcity of labeled data remains a fundamental challenge\nin developing robust and generalizable AI models, particularly for low-resource\nlanguages. Conventional data augmentation techniques introduce noise, cause\nsemantic drift, disrupt contextual coherence, lack control, and lead to\noverfitting. To address these challenges, we propose XAI-Guided Context-Aware\nData Augmentation. This novel framework leverages XAI techniques to modify less\ncritical features while selectively preserving most task-relevant features. Our\napproach integrates an iterative feedback loop, which refines augmented data\nover multiple augmentation cycles based on explainability-driven insights and\nthe model performance gain. Our experimental results demonstrate that XAI-SR-BT\nand XAI-PR-BT improve the accuracy of models on hate speech and sentiment\nanalysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using\nthe Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform\nexisting augmentation techniques by 4.8% and 5%, respectively, on the same\ndataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform\nboth baseline and conventional augmentation techniques across all tasks and\nmodels. This study provides a more controlled, interpretable, and context-aware\nsolution to data augmentation, addressing critical limitations of existing\naugmentation techniques and offering a new paradigm shift for leveraging XAI\ntechniques to enhance AI model training.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eXAI\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u548c\u8fed\u4ee3\u53cd\u9988\u6539\u8fdb\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u548c\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u4ee5\u53ca\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\u5f15\u5165\u566a\u58f0\u3001\u8bed\u4e49\u6f02\u79fb\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faXAI-Guided Context-Aware Data Augmentation\u6846\u67b6\uff0c\u5229\u7528XAI\u6280\u672f\u4fee\u6539\u975e\u5173\u952e\u7279\u5f81\u5e76\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u4f18\u5316\u589e\u5f3a\u6570\u636e\u3002", "result": "XAI-SR-BT\u548cXAI-PR-BT\u5206\u522b\u63d0\u5347\u4e866.6%\u548c8.1%\u7684\u6a21\u578b\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa4.8%\u548c5%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5229\u7528XAI\u63d0\u5347AI\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2506.03489", "pdf": "https://arxiv.org/pdf/2506.03489", "abs": "https://arxiv.org/abs/2506.03489", "authors": ["Mingxu Tao", "Jie Hu", "Mingchuan Yang", "Yunhuai Liu", "Dongyan Zhao", "Yansong Feng"], "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "The remarkable performance of Large language models (LLMs) relies heavily on\nthe availability of abundant high-quality training data. However, the high cost\nof acquiring annotated data often prevents models from obtaining capabilities\nto tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe\nthat boosts model performance in data-scarcity scenarios without extra\ntraining. We first employ model extrapolation to enhance a finetuned model with\nits inferior version, and then adopt contrastive decoding to further reduce\npredicted errors, by comparing the logit scores given by the extrapolated and\nthe vanilla finetuned model. Experiments across three tasks over four different\nLLMs show that EpiCoDe consistently outperforms existing methods with\nsignificant and robust improvement. We also propose a new theoretical framework\nto reveal the mechanism behind contrastive decoding in data-scarcity scenarios,\nwhich further helps us better understand the effectiveness of EpiCoDe.", "AI": {"tldr": "A novel method called EpiCoDe is introduced to improve large language model performance in data-scarcity scenarios without additional training. It uses model extrapolation and contrastive decoding to reduce predicted errors and has been proven effective across multiple tasks and models.", "motivation": "To address the issue of high cost in acquiring annotated data for training large language models.", "method": "EpiCoDe employs model extrapolation to enhance a fine-tuned model and contrastive decoding to compare logit scores from the extrapolated and vanilla fine-tuned models to reduce predicted errors.", "result": "Experiments across three tasks over four different LLMs show that EpiCoDe consistently outperforms existing methods with significant and robust improvement.", "conclusion": "EpiCoDe is an effective method to boost model performance in data-scarcity scenarios without extra training. A new theoretical framework was proposed to explain the mechanism behind contrastive decoding."}}
{"id": "2506.03490", "pdf": "https://arxiv.org/pdf/2506.03490", "abs": "https://arxiv.org/abs/2506.03490", "authors": ["Shigeng Chen", "Linhao Luo", "Zhangchi Qiu", "Yanan Cao", "Carl Yang", "Shirui Pan"], "title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "categories": ["cs.CL"], "comment": null, "summary": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications.", "AI": {"tldr": "This paper introduces MedEditBench, a framework for evaluating existing knowledge editing methods in the medical domain. It also proposes SGR-Edit, an improved method utilizing model-derived rationales for editing.", "motivation": "Existing knowledge editing methods have not been thoroughly explored in the complex medical domain, where generalization and interpretability are crucial.", "method": "Proposed MedEditBench with a new medical knowledge editing benchmark and three paradigms. Also introduced SGR-Edit, which uses model-derived rationales for editing.", "result": "Current KE methods only achieve superficial memorization and fail to generalize. SGR-Edit shows significant improvement over existing KE approaches.", "conclusion": "The study provides insights into medical knowledge editing and offers practical guidance for real-world medical applications."}}
{"id": "2506.03501", "pdf": "https://arxiv.org/pdf/2506.03501", "abs": "https://arxiv.org/abs/2506.03501", "authors": ["Yuchen Guo", "Zhicheng Dou", "Huy H. Nguyen", "Ching-Chun Chang", "Saku Sugawara", "Isao Echizen"], "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing", "categories": ["cs.CL", "cs.AI"], "comment": "IJCNN2025 accepted", "summary": "Content creation has dramatically progressed with the rapid advancement of\nlarge language models like ChatGPT and Claude. While this progress has greatly\nenhanced various aspects of life and work, it has also negatively affected\ncertain areas of society. A recent survey revealed that nearly 30% of college\nstudents use generative AI to help write academic papers and reports. Most\ncountermeasures treat the detection of AI-generated text as a binary\nclassification task and thus lack robustness. This approach overlooks human\ninvolvement in the generation of content even though human-machine\ncollaboration is becoming mainstream. Besides generating entire texts, people\nmay use machines to complete or revise texts. Such human involvement varies\ncase by case, which makes binary classification a less than satisfactory\napproach. We refer to this situation as participation detection obfuscation. We\npropose using BERTScore as a metric to measure human involvement in the\ngeneration process and a multi-task RoBERTa-based regressor trained on a token\nclassification task to address this problem. To evaluate the effectiveness of\nthis approach, we simulated academic-based scenarios and created a continuous\ndataset reflecting various levels of human involvement. All of the existing\ndetectors we examined failed to detect the level of human involvement on this\ndataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor\nmean squared error of 0.004). Moreover, it demonstrated some generalizability\nacross generative models. Our code is available at\nhttps://github.com/gyc-nii/CAS-CS-and-dual-head-detector", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u53c2\u4e0e\u68c0\u6d4b\u6a21\u7cca\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5728\u5b66\u672f\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u5e76\u5177\u6709\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u53cd\u5236\u63aa\u65bd\u5c06AI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u89c6\u4e3a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u4e14\u5ffd\u89c6\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4eba\u7c7b\u53c2\u4e0e\u3002", "method": "\u4f7f\u7528BERTScore\u4f5c\u4e3a\u5ea6\u91cf\u6807\u51c6\u6765\u8861\u91cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4eba\u7c7b\u53c2\u4e0e\u5ea6\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1RoBERTa-based\u56de\u5f52\u5668\u6765\u8fdb\u884c\u89e3\u51b3\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b66\u672f\u573a\u666f\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u8272\uff08F1\u5206\u6570\u4e3a0.9423\uff0c\u56de\u5f52\u5668\u5747\u65b9\u8bef\u5dee\u4e3a0.004\uff09\uff0c\u5e76\u4e14\u5728\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u5177\u6709\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528BERTScore\u548c\u591a\u4efb\u52a1RoBERTa-based\u56de\u5f52\u5668\uff0c\u6211\u4eec\u6210\u529f\u5730\u89e3\u51b3\u4e86\u53c2\u4e0e\u68c0\u6d4b\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5728\u5b66\u672f\u573a\u666f\u4e0b\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.03510", "pdf": "https://arxiv.org/pdf/2506.03510", "abs": "https://arxiv.org/abs/2506.03510", "authors": ["Seungcheol Park", "Sojin Lee", "Jongjin Kim", "Jinsik Lee", "Hyunjik Jo", "U Kang"], "title": "Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "IJCAI 2025 Main Track", "summary": "How can we accelerate large language models(LLMs) without sacrificing\naccuracy? The slow inference speed of LLMs hinders us to benefit from their\nremarkable performance in diverse applications. This is mainly because numerous\nsublayers are stacked together in LLMs. Sublayer pruning compresses and\nexpedites LLMs via removing unnecessary sublayers. However, existing sublayer\npruning algorithms are limited in accuracy since they naively select sublayers\nto prune, overlooking the different characteristics of each sublayer. In this\npaper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability\nInformation), an accurate sublayer pruning method for LLMs. SPRINT accurately\nselects a target sublayer to prune by considering 1) the amount of latency\nreduction after pruning and 2) the tunability of sublayers. SPRINT iteratively\nprunes redundant sublayers and swiftly tunes the parameters of remaining\nsublayers. Experiments show that SPRINT achieves the best accuracy-speedup\ntrade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense\nreasoning benchmarks compared to existing pruning algorithms.", "AI": {"tldr": "SPRINT is a new method for accelerating large language models without losing accuracy.", "motivation": "The slow inference speed of LLMs limits their application despite excellent performance.", "method": "SPRINT prunes unnecessary sublayers by considering latency reduction and tunability.", "result": "SPRINT improves accuracy-speedup trade-off, achieving up to 23.88% higher accuracy than previous methods.", "conclusion": "SPRINT provides an effective way to accelerate LLMs while maintaining high accuracy."}}
{"id": "2506.03519", "pdf": "https://arxiv.org/pdf/2506.03519", "abs": "https://arxiv.org/abs/2506.03519", "authors": ["Yangyang Zhao", "Ben Niu", "Libo Qin", "Shihan Wang"], "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals", "categories": ["cs.CL"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks.", "AI": {"tldr": "This paper combines Deep Reinforcement Learning with Evolutionary Algorithms to improve task-oriented dialogue systems.", "motivation": "To address the challenges of balancing exploration and exploitation in high-dimensional state and action spaces.", "method": "Innovative combination of EA's global search with DRL's local optimization, enhanced with an elite individual injection mechanism.", "result": "Significant improvement in the balance between exploration and exploitation, boosting performance across four datasets.", "conclusion": "The proposed method achieves efficient integration of EA and DRL for task-oriented dialogue policy optimization."}}
{"id": "2506.03523", "pdf": "https://arxiv.org/pdf/2506.03523", "abs": "https://arxiv.org/abs/2506.03523", "authors": ["Chong Li", "Jiajun Zhang", "Chengqing Zong"], "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment", "categories": ["cs.CL"], "comment": "ACL 2025, our codes and models are available at\n  https://github.com/ZNLP/TokAlign", "summary": "Tokenization serves as a foundational step for Large Language Models (LLMs)\nto process text. In new domains or languages, the inefficiency of the tokenizer\nwill slow down the training and generation of LLM. The mismatch in vocabulary\nalso hinders deep knowledge transfer between LLMs like token-level\ndistillation. To mitigate this gap, we propose an efficient method named\nTokAlign to replace the vocabulary of LLM from the token co-occurrences view,\nand further transfer the token-level knowledge between models. It first aligns\nthe source vocabulary to the target one by learning a one-to-one mapping matrix\nfor token IDs. Model parameters, including embeddings, are rearranged and\nprogressively fine-tuned for the new vocabulary. Our method significantly\nimproves multilingual text compression rates and vocabulary initialization for\nLLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods\nto 1.2$\\text{e}^2$ after initialization. Experimental results on models across\nmultiple parameter scales demonstrate the effectiveness and generalization of\nTokAlign, which costs as few as 5k steps to restore the performance of the\nvanilla model. After unifying vocabularies between LLMs, token-level\ndistillation can remarkably boost (+4.4% than sentence-level distillation) the\nbase model, costing only 235M tokens.", "AI": {"tldr": "TokAlign efficiently replaces the vocabulary of large language models and transfers token-level knowledge between models, improving performance and generalization.", "motivation": "The inefficiency of tokenization in new domains or languages slows down the training and generation of LLMs, and the mismatch in vocabulary hinders deep knowledge transfer between LLMs.", "method": "TokAlign replaces the vocabulary of LLMs by learning a one-to-one mapping matrix for token IDs, rearranging and progressively fine-tuning model parameters for the new vocabulary.", "result": "TokAlign significantly improves multilingual text compression rates and vocabulary initialization for LLMs, decreasing perplexity from 3.4e2 to 1.2e2 after initialization. It also boosts token-level distillation, improving performance by 4.4% over sentence-level distillation.", "conclusion": "This paper proposes TokAlign, a method that improves multilingual text compression rates and vocabulary initialization for large language models, reducing perplexity and demonstrating effectiveness and generalization across multiple parameter scales."}}
{"id": "2506.03524", "pdf": "https://arxiv.org/pdf/2506.03524", "abs": "https://arxiv.org/abs/2506.03524", "authors": ["Yuyu Zhang", "Jing Su", "Yifan Sun", "Chenguang Xi", "Xia Xiao", "Shen Zheng", "Anxiang Zhang", "Kaibo Liu", "Daoguang Zan", "Tao Sun", "Jinhua Zhu", "Shulin Xin", "Dong Huang", "Yetao Bai", "Lixin Dong", "Chao Li", "Jianchong Chen", "Hanzhi Zhou", "Yifan Huang", "Guanghan Ning", "Xierui Song", "Jiaze Chen", "Siyao Liu", "Kai Shen", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Coder: Let the Code Model Curate Data for Itself", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Code data in large language model (LLM) pretraining is recognized crucial not\nonly for code-related tasks but also for enhancing general intelligence of\nLLMs. Current open-source LLMs often heavily rely on human effort to produce\ntheir code pretraining data, such as employing hand-crafted filtering rules\ntailored to individual programming languages, or using human-annotated data to\ntrain quality filters. However, these approaches are inherently limited in\nscalability, prone to subjective biases, and costly to extend and maintain\nacross diverse programming languages. To address these challenges, we introduce\nSeed-Coder, a series of open-source LLMs comprising base, instruct and\nreasoning models of 8B size, minimizing human involvement in data construction.\nOur code pretraining data is produced by a model-centric data pipeline, which\npredominantly leverages LLMs for scoring and filtering code data. The instruct\nmodel is further trained via supervised fine-tuning and preference\noptimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)\nreinforcement learning to improve multi-step code reasoning. Seed-Coder\nachieves state-of-the-art results among open-source models of similar size and\neven surpasses some much larger models, demonstrating superior performance in\ncode generation, code completion, code editing, code reasoning, and software\nengineering tasks.", "AI": {"tldr": "Seed-Coder introduces a series of open-source LLMs that reduce human involvement in code data construction, achieving top-tier performance in multiple code-related tasks.", "motivation": "The motivation is to minimize human involvement in data construction for large language model pretraining, addressing scalability, subjectivity, and maintenance issues associated with current approaches.", "method": "Seed-Coder uses a model-centric data pipeline that primarily utilizes LLMs for scoring and filtering code data. The instruct model is trained through supervised fine-tuning and preference optimization, while the reasoning model employs Long-Chain-of-Thought reinforcement learning.", "result": "Seed-Coder achieves state-of-the-art results among open-source models of similar size and even outperforms some larger models.", "conclusion": "Seed-Coder demonstrates superior performance in various tasks including code generation, code completion, code editing, code reasoning, and software engineering tasks."}}
{"id": "2506.03533", "pdf": "https://arxiv.org/pdf/2506.03533", "abs": "https://arxiv.org/abs/2506.03533", "authors": ["Apurva Gandhi", "Graham Neubig"], "title": "Go-Browse: Training Web Agents with Structured Exploration", "categories": ["cs.CL"], "comment": null, "summary": "One of the fundamental problems in digital agents is their lack of\nunderstanding of their environment. For instance, a web browsing agent may get\nlost in unfamiliar websites, uncertain what pages must be visited to achieve\nits goals. To address this, we propose Go-Browse, a method for automatically\ncollecting diverse and realistic web agent data at scale through structured\nexploration of web environments. Go-Browse achieves efficient exploration by\nframing data collection as a graph search, enabling reuse of information across\nexploration episodes. We instantiate our method on the WebArena benchmark,\ncollecting a dataset of 10K successful task-solving trajectories and 40K\ninteraction steps across 100 URLs. Fine-tuning a 7B parameter language model on\nthis dataset achieves a success rate of 21.7% on the WebArena benchmark,\nbeating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for\nsub-10B parameter models by 2.9%.", "AI": {"tldr": "\u63d0\u51faGo-Browse\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a2\u7d22\u7f51\u7edc\u73af\u5883\u81ea\u52a8\u6536\u96c6\u5927\u89c4\u6a21\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684\u7f51\u9875\u4ee3\u7406\u6570\u636e\uff0c\u4f18\u5316\u540e\u7684\u8bed\u8a00\u6a21\u578b\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8621.7%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86GPT-4o mini\u5e76\u5237\u65b0\u4e86\u5c0f\u89c4\u6a21\u53c2\u6570\u6a21\u578b\u7684\u6700\u4f73\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u4ee3\u7406\u5bf9\u73af\u5883\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u5728\u7f51\u7edc\u6d4f\u89c8\u4ee3\u7406\u4e2d\uff0c\u5b83\u4eec\u53ef\u80fd\u5728\u4e0d\u719f\u6089\u7684\u7f51\u7ad9\u4e2d\u8ff7\u5931\u65b9\u5411\uff0c\u4e0d\u786e\u5b9a\u9700\u8981\u8bbf\u95ee\u54ea\u4e9b\u9875\u9762\u6765\u5b9e\u73b0\u76ee\u6807\u3002", "method": "Go-Browse\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u6536\u96c6\u89c6\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\uff0c\u5e76\u4e14\u80fd\u591f\u8de8\u63a2\u7d22\u9636\u6bb5\u590d\u7528\u4fe1\u606f\u3002", "result": "\u5728WebArena\u57fa\u51c6\u4e0a\u6536\u96c6\u4e86\u5305\u542b10K\u4e2a\u6210\u529f\u4efb\u52a1\u89e3\u51b3\u8f68\u8ff9\u548c40K\u4e2a\u4ea4\u4e92\u6b65\u9aa4\u7684\u6570\u636e\u96c6\uff1b\u4f18\u5316\u540e\u76847B\u53c2\u6570\u8bed\u8a00\u6a21\u578b\u8fbe\u5230\u4e8621.7%\u7684\u6210\u529f\u7387\u3002", "conclusion": "Go-Browse\u662f\u4e00\u79cd\u6709\u6548\u7684\u7f51\u7edc\u4ee3\u7406\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0c\u4f18\u5316\u540e\u7684\u8bed\u8a00\u6a21\u578b\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2506.03541", "pdf": "https://arxiv.org/pdf/2506.03541", "abs": "https://arxiv.org/abs/2506.03541", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "summary": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "AI": {"tldr": "Introduces a Debate and Reflect framework and Tree-structured Direct Preference Optimization to enhance smaller language models' performance across various NLP tasks.", "motivation": "The motivation is to address the high computational demands of Large Language Models (LLMs) and improve the performance of smaller models.", "method": "The method involves creating a novel Debate and Reflect (D&R) framework that facilitates multi-turn debates between smaller models and stronger teacher models to generate actionable feedback. Additionally, a Tree-structured Direct Preference Optimization (T-DPO) is introduced to effectively use these debate logs for training.", "result": "The result shows that the approach significantly enhances smaller-model accuracy, robustness, and generalization across diverse NLP benchmarks, surpassing conventional baselines by a significant margin.", "conclusion": "This paper concludes that the proposed D&R framework and T-DPO method can effectively improve the performance of smaller language models."}}
{"id": "2506.03557", "pdf": "https://arxiv.org/pdf/2506.03557", "abs": "https://arxiv.org/abs/2506.03557", "authors": ["Lin Sun", "Chuang Liu", "Peng Liu", "Bingyang Li", "Weijia Lu", "Ning Wu"], "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) have emerged as a popular method for\naligning Large Language Models (LLMs) with human preferences. While DPO\neffectively preserves the relative ordering between chosen and rejected\nresponses through pairwise ranking losses, it often neglects absolute reward\nmagnitudes. This oversight can decrease the likelihood of chosen responses and\nincrease the risk of generating out-of-distribution responses, leading to poor\nperformance. We term this issue Degraded Chosen Responses (DCR).To address this\nissue, we propose Balanced Preference Optimization (BPO), a novel framework\nthat dynamically balances the optimization of chosen and rejected responses\nthrough two key components: balanced reward margin and gap adaptor. Unlike\nprevious methods, BPO can fundamentally resolve DPO's DCR issue, without\nintroducing additional constraints to the loss function. Experimental results\non multiple mathematical reasoning tasks show that BPO significantly\noutperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%\nto 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses\nDPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over\nCal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a\nsingle line of code modification, making it simple to implement and fully\ncompatible with existing DPO-based frameworks.", "AI": {"tldr": "A novel framework called Balanced Preference Optimization (BPO) is proposed to improve the performance of Direct Preference Optimization (DPO) in aligning large language models with human preferences.", "motivation": "The motivation is to address the issue of Degraded Chosen Responses (DCR) caused by DPO's neglect of absolute reward magnitudes.", "method": "BPO introduces two key components: balanced reward margin and gap adaptor to balance the optimization of chosen and rejected responses.", "result": "Experimental results show that BPO improves accuracy by 10.1%-11.7% across multiple mathematical reasoning tasks compared to DPO and its variants.", "conclusion": "BPO effectively resolves DCR issue and outperforms DPO and its variants with only a single line of code modification."}}
{"id": "2506.03558", "pdf": "https://arxiv.org/pdf/2506.03558", "abs": "https://arxiv.org/abs/2506.03558", "authors": ["Jiawei Chen", "Xinyan Guan", "Qianhao Yuan", "Guozhao Mo", "Weixiang Zhou", "Yaojie Lu", "Hongyu Lin", "Ben He", "Le Sun", "Xianpei Han"], "title": "ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch", "categories": ["cs.CL"], "comment": null, "summary": "Current instruction data synthesis methods primarily focus on single-turn\ninstructions and often neglect cross-turn coherence, resulting in context drift\nand reduced task completion rates in extended conversations. To address this\nlimitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a\nframework that constrains multi-turn instruction synthesis by explicitly\nmodeling human conversational intent. It operates in two stages: (1) Intent\nModeling, which captures the global structure of human dialogues by assigning\neach conversation to one of nine well-defined intent trajectories, ensuring a\ncoherent and goal-oriented information flow; and (2) Skeleton Generation, which\nconstructs a structurally grounded sequence of user queries aligned with the\nmodeled intent, thereby serving as a scaffold that constrains and guides the\ndownstream instruction synthesis process. Based on this process, we construct\nConsistentChat, a multi-turn instruction dataset with approximately 15,000\nmulti-turn conversations and 224,392 utterances. Experiments on the Light,\nTopdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat\nachieve a 20-30% improvement in chat consistency and up to a 15% increase in\ntask success rate, significantly outperforming models trained on existing\nsingle-turn and multi-turn instruction datasets.", "AI": {"tldr": "We propose Skeleton-Guided Multi-Turn Dialogue Generation, a framework that improves multi-turn instruction synthesis by modeling human conversational intent. This results in a more coherent dialogue flow and better task completion rates. We also create ConsistentChat, a large multi-turn instruction dataset, which enhances model performance on various benchmarks.", "motivation": "Current methods for multi-turn instruction synthesis often suffer from context drift and reduced task completion rates due to lack of cross-turn coherence.", "method": "The proposed framework includes two stages: Intent Modeling, which assigns dialogues to intent trajectories, and Skeleton Generation, which constructs user query sequences aligned with the modeled intent.", "result": "Models fine-tuned on our constructed ConsistentChat dataset showed improved chat consistency by 20-30% and up to a 15% increase in task success rate compared to models trained on existing datasets.", "conclusion": "Our approach effectively addresses the issue of context drift in multi-turn dialogues and demonstrates superior performance on multiple benchmarks."}}
{"id": "2506.03566", "pdf": "https://arxiv.org/pdf/2506.03566", "abs": "https://arxiv.org/abs/2506.03566", "authors": ["Langlin Huang", "Chengsong Huang", "Jixuan Leng", "Di Huang", "Jiaxin Huang"], "title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5Position Specialists\uff08PosS\uff09\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u540e\u671f\u4f4d\u7f6e\u7684\u8349\u6848\u4ee4\u724c\u9884\u6d4b\u8d28\u91cf\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5728\u540e\u671f\u4f4d\u7f6e\u4e0a\u7684\u8349\u6848\u4ee4\u724c\u9884\u6d4b\u8d28\u91cf\u4e0b\u964d\uff0c\u56e0\u4e3a\u9519\u8bef\u5728\u751f\u6210\u7684\u7279\u5f81\u4e2d\u79ef\u7d2f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aPosition Specialists\uff08PosS\uff09\u7684\u65b9\u6cd5\uff0c\u5b83\u7531\u591a\u4e2a\u4e13\u95e8\u7684\u8349\u6848\u5c42\u7ec4\u6210\uff0c\u6bcf\u4e2a\u4e13\u95e8\u5904\u7406\u7279\u5b9a\u7ea7\u522b\u7684\u8349\u6848\u6a21\u578b\u7279\u5f81\u504f\u5dee\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cPosS\u5728\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u548c\u52a0\u901f\u6bd4\u65b9\u9762\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPosition Specialists\uff08PosS\uff09\u5728Llama-3-8B-Instruct\u548cLlama-2-13B-chat\u4e0a\u6709\u6548\u63d0\u5347\u4e86\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u548c\u52a0\u901f\u6bd4\u3002"}}
{"id": "2506.03569", "pdf": "https://arxiv.org/pdf/2506.03569", "abs": "https://arxiv.org/abs/2506.03569", "authors": ["Xiaomi LLM-Core Team", ":", "Zihao Yue", "Zhenru Lin", "Yifan Song", "Weikun Wang", "Shuhuai Ren", "Shuhao Gu", "Shicheng Li", "Peidian Li", "Liang Zhao", "Lei Li", "Kainan Bao", "Hao Tian", "Hailin Zhang", "Gang Wang", "Dawei Zhu", "Cici", "Chenhong He", "Bowen Ye", "Bowen Shen", "Zihan Zhang", "Zihan Jiang", "Zhixian Zheng", "Zhichao Song", "Zhenbo Luo", "Yue Yu", "Yudong Wang", "Yuanyuan Tian", "Yu Tu", "Yihan Yan", "Yi Huang", "Xu Wang", "Xinzhe Xu", "Xingchen Song", "Xing Zhang", "Xing Yong", "Xin Zhang", "Xiangwei Deng", "Wenyu Yang", "Wenhan Ma", "Weiwei Lv", "Weiji Zhuang", "Wei Liu", "Sirui Deng", "Shuo Liu", "Shimao Chen", "Shihua Yu", "Shaohui Liu", "Shande Wang", "Rui Ma", "Qiantong Wang", "Peng Wang", "Nuo Chen", "Menghang Zhu", "Kangyang Zhou", "Kang Zhou", "Kai Fang", "Jun Shi", "Jinhao Dong", "Jiebao Xiao", "Jiaming Xu", "Huaqiu Liu", "Hongshen Xu", "Heng Qu", "Haochen Zhao", "Hanglong Lv", "Guoan Wang", "Duo Zhang", "Dong Zhang", "Di Zhang", "Chong Ma", "Chang Liu", "Can Cai", "Bingquan Xia"], "title": "MiMo-VL Technical Report", "categories": ["cs.CL"], "comment": "32 pages", "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.", "AI": {"tldr": "Two powerful vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, achieve top performance in visual understanding and multimodal reasoning.", "motivation": "To develop advanced vision-language models that can excel in various tasks and provide better performance than existing models.", "method": "Four-stage pre-training with 2.4 trillion tokens combined with Mixed On-policy Reinforcement Learning (MORL).", "result": "MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 tasks and sets new standards in GUI grounding applications. It surpasses larger models on OlympiadBench.", "conclusion": "The study highlights the importance of incorporating high-quality reasoning data and mixed RL in pre-training stages, contributing to reproducibility and advancement in the field through an open-source model and evaluation suite."}}
{"id": "2506.03570", "pdf": "https://arxiv.org/pdf/2506.03570", "abs": "https://arxiv.org/abs/2506.03570", "authors": ["Lin Sun", "Chuang Liu", "Xiaofeng Ma", "Tao Yang", "Weijia Lu", "Ning Wu"], "title": "FreePRM: Training Process Reward Models Without Ground Truth Process Labels", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated that\nProcess Reward Models (PRMs) play a crucial role in enhancing model\nperformance. However, training PRMs typically requires step-level labels,\neither manually annotated or automatically generated, which can be costly and\ndifficult to obtain at scale. To address this challenge, we introduce FreePRM,\na weakly supervised framework for training PRMs without access to ground-truth\nstep-level labels. FreePRM first generates pseudo step-level labels based on\nthe correctness of final outcome, and then employs Buffer Probability to\neliminate impact of noise inherent in pseudo labeling. Experimental results\nshow that FreePRM achieves an average F1 score of 53.0% on ProcessBench,\noutperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared\nto other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B\n(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by\n+10.9%. This work introduces a new paradigm in PRM training, significantly\nreducing reliance on costly step-level annotations while maintaining strong\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFreePRM\u7684\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\uff0c\u65e0\u9700\u8bbf\u95ee\u771f\u5b9e\u6b65\u7ea7\u6807\u7b7e\u3002\u5b9e\u9a8c\u8868\u660eFreePRM\u5728ProcessBench\u4e0a\u5e73\u5747F1\u5f97\u5206\u4e3a53.0%\uff0c\u4f18\u4e8e\u5176\u4ed6\u5f00\u653e\u6e90\u7801PRMs\u3002", "motivation": "\u63d0\u9ad8PRMs\u7684\u6027\u80fd\u5e76\u51cf\u5c11\u5bf9\u6602\u8d35\u7684\u6b65\u7ea7\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u5f15\u5165FreePRM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9996\u5148\u57fa\u4e8e\u6700\u7ec8\u7ed3\u679c\u7684\u6b63\u786e\u6027\u751f\u6210\u4f2a\u6b65\u7ea7\u6807\u7b7e\uff0c\u7136\u540e\u4f7f\u7528\u7f13\u51b2\u533a\u6982\u7387\u6765\u6d88\u9664\u4f2a\u6807\u8bb0\u4e2d\u7684\u566a\u58f0\u5f71\u54cd\u3002", "result": "FreePRM\u5728ProcessBench\u4e0a\u5b9e\u73b0\u4e8653.0%\u7684\u5e73\u5747F1\u5f97\u5206\uff0c\u6bd4\u5b8c\u5168\u76d1\u7763\u7684PRM\u9ad8\u51fa24.1%\uff0c\u5e76\u4e14\u5728\u4e0e\u5176\u4ed6\u5f00\u653e\u6e90\u7801PRMs\u76f8\u6bd4\u65f6\u4e5f\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86PRM\u8bad\u7ec3\u7684\u65b0\u8303\u5f0f\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u5bf9\u6602\u8d35\u6b65\u7ea7\u6ce8\u91ca\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002"}}
{"id": "2506.03573", "pdf": "https://arxiv.org/pdf/2506.03573", "abs": "https://arxiv.org/abs/2506.03573", "authors": ["Lin Sun", "Can Zhang"], "title": "Exchange of Perspective Prompting Enhances Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made significant advancements in addressing\ndiverse natural language processing (NLP) tasks. However, their performance is\noften limited by inherent comprehension of problems. To address this\nlimitation, we propose Exchange-of-Perspective (EoP), a novel framework\ndesigned to exchange perspectives across different definitions of problem, so\nthat it can break the fixed mindset from any particular formulation of the\nquestion. We conducted extensive and comprehensive experiments on 8 benchmarks.\nThe results show that EoP can significantly improve performance. For instance,\ncompared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we\nobserve a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP\ndemonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a\n3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using\nQwen-2.5-72b.", "AI": {"tldr": "Introduce a new framework called Exchange-of-Perspective (EoP) to improve the performance of large language models on NLP tasks.", "motivation": "To address the limitation of large language models' inherent comprehension of problems.", "method": "Exchange-of-Perspective (EoP)", "result": "EoP improves performance on multiple benchmarks including AQuA, Math, and OlympiadBench Maths.", "conclusion": "The proposed Exchange-of-Perspective (EoP) framework can significantly enhance the performance of large language models on diverse natural language processing tasks."}}
{"id": "2506.03576", "pdf": "https://arxiv.org/pdf/2506.03576", "abs": "https://arxiv.org/abs/2506.03576", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics.", "AI": {"tldr": "A bidirectional language model framework named KG-BiLM is proposed to integrate symbolic knowledge graphs with language models for better semantic understanding.", "motivation": "Existing methods focus on either graph structure or textual semantics, but there is a need for a unified framework that can capture global KG connectivity, nuanced linguistic context, and discriminative reasoning semantics.", "method": "KG-BiLM incorporates Bidirectional Knowledge Attention, Knowledge-Masked Prediction, and Contrastive Graph Semantic Aggregation to fuse structural cues from KGs with the semantic expressiveness of generative transformers.", "result": "Experiments show that KG-BiLM surpasses strong baselines in link prediction tasks, particularly on large-scale graphs with complex multi-hop relations.", "conclusion": "KG-BiLM demonstrates the effectiveness of unifying structural information and textual semantics for richer semantic understanding."}}
{"id": "2506.03580", "pdf": "https://arxiv.org/pdf/2506.03580", "abs": "https://arxiv.org/abs/2506.03580", "authors": ["Enrico Benedetti", "Akiko Aizawa", "Florian Boudin"], "title": "Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models", "categories": ["cs.CL"], "comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Volume 4: Student Research Workshop)", "summary": "Providing example sentences that are diverse and aligned with learners'\nproficiency levels is essential for fostering effective language acquisition.\nThis study examines the use of Pre-trained Language Models (PLMs) to produce\nexample sentences targeting L2 Japanese learners. We utilize PLMs in two ways:\nas quality scoring components in a retrieval system that draws from a newly\ncurated corpus of Japanese sentences, and as direct sentence generators using\nzero-shot learning. We evaluate the quality of sentences by considering\nmultiple aspects such as difficulty, diversity, and naturalness, with a panel\nof raters consisting of learners of Japanese, native speakers -- and GPT-4. Our\nfindings suggest that there is inherent disagreement among participants on the\nratings of sentence qualities, except for difficulty. Despite that, the\nretrieval approach was preferred by all evaluators, especially for beginner and\nadvanced target proficiency, while the generative approaches received lower\nscores on average. Even so, our experiments highlight the potential for using\nPLMs to enhance the adaptability of sentence suggestion systems and therefore\nimprove the language learning journey.", "AI": {"tldr": "Pre-trained Language Models (PLMs) can be used to generate example sentences for L2 Japanese learners. The retrieval approach was preferred over the generative approach.", "motivation": "To provide diverse and proficiency-level-aligned example sentences to foster effective language acquisition.", "method": "Utilize PLMs as quality scoring components in a retrieval system and as direct sentence generators using zero-shot learning.", "result": "The retrieval approach was preferred by all evaluators, especially for beginner and advanced target proficiency, while the generative approaches received lower scores on average.", "conclusion": "PLMs have potential to enhance the adaptability of sentence suggestion systems and improve the language learning journey."}}
{"id": "2506.03592", "pdf": "https://arxiv.org/pdf/2506.03592", "abs": "https://arxiv.org/abs/2506.03592", "authors": ["Viktor Hangya", "Fabian K\u00fcch", "Darina Gold"], "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. We plan to publish our benchmark adaptions.", "AI": {"tldr": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5c06\u751f\u6210\u4efb\u52a1\u8f6c\u6362\u4e3a\u66f4\u4fbf\u5b9c\u7684\u7406\u89e3\u4efb\u52a1\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u65f6\u95f4\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u7b49\u5173\u952e\u80fd\u529b\u7684\u6709\u6548\u8bc4\u4f30\u3002", "motivation": "\u8fed\u4ee3\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u9884\u671f\u80fd\u529b\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53ef\u80fd\u4f1a\u8017\u8d39\u5927\u91cf\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u51cf\u8f7bNLG\u57fa\u51c6\u6d4b\u8bd5\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4ee5\u4fbf\u5728\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u76d1\u63a7\u5173\u952e\u7684LLM\u80fd\u529b\u3002", "method": "\u5c06\u751f\u6210\u4efb\u52a1\uff08NLG\uff09\u91cd\u65b0\u8868\u8ff0\u4e3a\u9009\u62e9\u6027\u7406\u89e3\u4efb\u52a1\uff08NLU\uff09\uff0c\u4ee5\u964d\u4f4eNLG\u57fa\u51c6\u6d4b\u8bd5\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u539f\u59cb\u751f\u6210\u4efb\u52a1\u548c\u91cd\u65b0\u8868\u8ff0\u7684\u7406\u89e3\u4efb\u52a1\u4e4b\u95f4\u5177\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u5728\u56db\u79cd\u80fd\u529b\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5e73\u574735\u500d\u7684\u8bc4\u4f30\u65f6\u95f4\u51cf\u5c11\u3002", "conclusion": "\u901a\u8fc7\u5c06\u751f\u6210\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u7684\u9009\u62e9\u6027\u7406\u89e3\u4efb\u52a1\uff0c\u652f\u6301LLM\u80fd\u529b\u8bc4\u4f30\uff0c\u5e76\u5728\u56db\u79cd\u80fd\u529b\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574735\u500d\u7684\u8bc4\u4f30\u65f6\u95f4\u51cf\u5c11\u3002"}}
{"id": "2506.03593", "pdf": "https://arxiv.org/pdf/2506.03593", "abs": "https://arxiv.org/abs/2506.03593", "authors": ["Ray Groshan", "Michael Ginn", "Alexis Palmer"], "title": "Is linguistically-motivated data augmentation worth it?", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main. First two authors contributed equally", "summary": "Data augmentation, a widely-employed technique for addressing data scarcity,\ninvolves generating synthetic data examples which are then used to augment\navailable training data. Researchers have seen surprising success from simple\nmethods, such as random perturbations from natural examples, where models seem\nto benefit even from data with nonsense words, or data that doesn't conform to\nthe rules of the language. A second line of research produces synthetic data\nthat does in fact follow all linguistic constraints; these methods require some\nlinguistic expertise and are generally more challenging to implement. No\nprevious work has done a systematic, empirical comparison of both\nlinguistically-naive and linguistically-motivated data augmentation strategies,\nleaving uncertainty about whether the additional time and effort of\nlinguistically-motivated data augmentation work in fact yields better\ndownstream performance.\n  In this work, we conduct a careful and comprehensive comparison of\naugmentation strategies (both linguistically-naive and\nlinguistically-motivated) for two low-resource languages with different\nmorphological properties, Uspanteko and Arapaho. We evaluate the effectiveness\nof many different strategies and their combinations across two important\nsequence-to-sequence tasks for low-resource languages: machine translation and\ninterlinear glossing. We find that linguistically-motivated strategies can have\nbenefits over naive approaches, but only when the new examples they produce are\nnot significantly unlike the training data distribution.", "AI": {"tldr": "This study systematically compares linguistically-naive and linguistically-motivated data augmentation strategies for low-resource languages Uspanteko and Arapaho in machine translation and interlinear glossing tasks.", "motivation": "Previous studies did not do a systematic comparison of different data augmentation strategies, leaving doubt on whether linguistically-motivated strategies yield better downstream performance.", "method": "Careful and comprehensive comparison of various augmentation strategies for two low-resource languages with different morphological properties.", "result": "Linguistically-motivated strategies can be beneficial over naive approaches, but only when the new examples align well with the training data distribution.", "conclusion": "The results provide insights into choosing appropriate data augmentation strategies for different scenarios in low-resource language processing."}}
{"id": "2506.03598", "pdf": "https://arxiv.org/pdf/2506.03598", "abs": "https://arxiv.org/abs/2506.03598", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "4 pages,2 figures,EITCE 2025", "summary": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.", "AI": {"tldr": "Introduces AP-SQL, a novel architecture that improves Text-to-SQL translation in resource-constrained environments by decomposing the task and leveraging prompt engineering.", "motivation": "The motivation is to address the challenge of using resource-intensive open-source models in resource-constrained environments.", "method": "The method involves schema filtering, retrieval-augmented text-to-SQL generation, and prompt-driven schema linking and SQL generation.", "result": "The result shows that AP-SQL improves schema selection accuracy and enhances the model's reasoning for accurate SQL generation.", "conclusion": "The paper concludes by demonstrating the effectiveness of AP-SQL in bridging the gap between resource-efficient small open-source models and powerful closed-source models."}}
{"id": "2506.03616", "pdf": "https://arxiv.org/pdf/2506.03616", "abs": "https://arxiv.org/abs/2506.03616", "authors": ["Eunki Kim", "Sangryul Kim", "James Thorne"], "title": "Learning to Insert [PAUSE] Tokens for Better Reasoning", "categories": ["cs.CL"], "comment": "18 pages, 5 figures, ACL findings", "summary": "To enhance reasoning capabilities, previous works have explored incorporating\nspecial-purpose tokens into the training process. These strategies strengthen\nthe learning mechanism of transformer-based large language models (LLMs).\nBuilding on prior research, in which inserting dummy tokens consecutively just\nbefore reasoning steps can enhance effectiveness, we introduce a novel approach\ntermed Dynamic Inserting Tokens Training (DIT). Our method identifies positions\nwithin sequences where model confidence is lowest according to token\nlog-likelihood. Strategically inserting [PAUSE] tokens on these positions\nbolsters the model's predictive capabilities for subsequent tokens.\nExperimental results across diverse datasets and models, from the 2.7B model to\nthe 8B model, demonstrate that DIT consistently outperforms traditional\nfine-tuning and previous token insertion methods. With this simple yet\neffective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on\nAQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work\nshows a model-based, dynamic approach rather than a heuristic one, thereby\nbroadening the scope of research in reasoning.", "AI": {"tldr": "A new method called Dynamic Inserting Tokens Training (DIT) is proposed to improve the reasoning capabilities of transformer-based large language models by strategically inserting [PAUSE] tokens where the model confidence is lowest.", "motivation": "Enhancing the reasoning capabilities of transformer-based large language models.", "method": "Identifying low-confidence positions within sequences using token log-likelihood and inserting [PAUSE] tokens.", "result": "Achieved accuracy gains of up to 4.7%p on GSM8K, 3.23%p on AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets.", "conclusion": "The proposed model-based, dynamic approach broadens the scope of research in reasoning."}}
{"id": "2506.03619", "pdf": "https://arxiv.org/pdf/2506.03619", "abs": "https://arxiv.org/abs/2506.03619", "authors": ["Ayuto Tsutsumi", "Yuu Jinnai"], "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales", "categories": ["cs.CL"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated strong language\nunderstanding and generation abilities across various languages, their cultural\nknowledge is often limited to English-speaking communities, which can\nmarginalize the cultures of non-English communities. To address the problem,\nevaluation of the cultural awareness of the LLMs and the methods to develop\nculturally aware LLMs have been investigated. In this study, we focus on\nevaluating knowledge of folktales, a key medium for conveying and circulating\nculture. In particular, we focus on Japanese folktales, specifically on\nknowledge of Yokai. Yokai are supernatural creatures originating from Japanese\nfolktales that continue to be popular motifs in art and entertainment today.\nYokai have long served as a medium for cultural expression, making them an\nideal subject for assessing the cultural awareness of LLMs. We introduce\nYokaiEval, a benchmark dataset consisting of 809 multiple-choice questions\n(each with four options) designed to probe knowledge about yokai. We evaluate\nthe performance of 31 Japanese and multilingual LLMs on this dataset. The\nresults show that models trained with Japanese language resources achieve\nhigher accuracy than English-centric models, with those that underwent\ncontinued pretraining in Japanese, particularly those based on Llama-3,\nperforming especially well. The code and dataset are available at\nhttps://github.com/CyberAgentA ILab/YokaiEval.", "AI": {"tldr": "This study introduces YokaiEval, a dataset for evaluating cultural awareness of large language models, particularly focusing on Japanese folklore and Yokai. It assesses 31 Japanese and multilingual LLMs, finding that models trained on Japanese data outperform English-centric ones.", "motivation": "To address the lack of cultural knowledge in LLMs regarding non-English speaking communities, particularly focusing on Japanese folklore.", "method": "Development of YokaiEval, a multiple-choice question dataset with 809 questions about Yokai, and evaluation of 31 Japanese and multilingual LLMs.", "result": "Japanese-trained models, especially those based on Llama-3 after continued pretraining in Japanese, performed better than English-centric models.", "conclusion": "Models trained with Japanese language resources perform better in cultural awareness tasks related to Japanese folklore."}}
{"id": "2506.03627", "pdf": "https://arxiv.org/pdf/2506.03627", "abs": "https://arxiv.org/abs/2506.03627", "authors": ["Lin Mu", "Guowei Chu", "Li Ni", "Lei Sang", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks", "categories": ["cs.CL", "cs.AI"], "comment": "13pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications.", "AI": {"tldr": "A novel prompting strategy named Robustness of Prompting (RoP) is proposed to improve the robustness of Large Language Models (LLMs) against input perturbations.", "motivation": "LLMs are sensitive to input perturbations which can negatively affect their performance.", "method": "RoP has two stages: Error Correction and Guidance. Error Correction generates prompts to automatically correct input errors while Guidance generates optimal guidance prompting to steer the model towards more robust and accurate inferences.", "result": "RoP improves LLMs' robustness against adversarial perturbations and maintains model accuracy with minimal degradation compared to clean input scenarios.", "conclusion": "RoP is a practical and effective approach for enhancing LLM robustness in real-world applications."}}
{"id": "2506.03637", "pdf": "https://arxiv.org/pdf/2506.03637", "abs": "https://arxiv.org/abs/2506.03637", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "title": "RewardAnything: Generalizable Principle-Following Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.", "AI": {"tldr": "Introduce generalizable reward models that can adapt to diverse real-world needs.", "motivation": "Current reward models are rigid and cannot adapt to different tasks' requirements.", "method": "Propose RewardAnything, which can follow natural language principles without retraining.", "result": "RewardAnything achieves state-of-the-art performance in traditional benchmarks and adapts well to novel principles on RABench.", "conclusion": "RewardAnything provides a flexible and efficient way to align large language models with various tasks using natural language principles."}}
{"id": "2506.03659", "pdf": "https://arxiv.org/pdf/2506.03659", "abs": "https://arxiv.org/abs/2506.03659", "authors": ["Yinuo Wang", "Robert E. Mercer", "Frank Rudzicz", "Sudipta Singha Roy", "Pengjie Ren", "Zhumin Chen", "Xindi Wang"], "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "categories": ["cs.CL"], "comment": null, "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.", "AI": {"tldr": "This survey examines six key dimensions of trustworthiness in medical QA systems powered by large language models, including Factuality, Robustness, Fairness, Safety, Explainability, and Calibration. It reviews how these dimensions are evaluated in existing systems, compares major benchmarks for assessment, and analyzes techniques driving model improvements. The paper also identifies open challenges and proposes future research directions.", "motivation": "Ensuring trustworthiness in healthcare question-answering systems is crucial for patient safety, clinical effectiveness, and user confidence, especially as large language models become more integrated into medical settings.", "method": "Systematic examination of six key dimensions of trustworthiness in medical QA systems, reviewing evaluations, compiling and comparing benchmarks, analyzing evaluation-guided techniques, and identifying open challenges.", "result": "The survey provides insights into evaluating trustworthiness in medical QA systems and highlights areas for improvement, such as scalable expert evaluation, integrated multi-dimensional metrics, and real-world deployment studies.", "conclusion": "This paper aims to advance the safe, reliable, and transparent deployment of large language model-powered medical QA systems by addressing trustworthiness across multiple dimensions."}}
{"id": "2506.03665", "pdf": "https://arxiv.org/pdf/2506.03665", "abs": "https://arxiv.org/abs/2506.03665", "authors": ["Hern\u00e1n Maina", "Guido Ivetta", "Mateo Lione Stuto", "Julian Martin Eisenschlos", "Jorge S\u00e1nchez", "Luciana Benotti"], "title": "ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visually impaired people could benefit from Visual Question Answering (VQA)\nsystems to interpret text in their surroundings. However, current models often\nstruggle with recognizing text in the photos taken by this population. Through\nin-depth interviews with visually impaired individuals, we identified common\nframing conventions that frequently result in misaligned text. Existing VQA\nbenchmarks primarily feature well-oriented text captured by sighted users,\nunder-representing these challenges. To address this gap, we introduce ROtated\nSAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich\nimages with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7\nabsolute points in the best-performing model.", "AI": {"tldr": "This paper introduces ROSA, a decoding strategy that improves VQA performance on images with misaligned text, outperforming Greedy decoding by 11.7 absolute points.", "motivation": "Current VQA models struggle with recognizing text in photos taken by visually impaired people due to framing conventions that cause misaligned text. Existing benchmarks under-represent these challenges.", "method": "ROSA, a decoding strategy that enhances VQA performance in text-rich images with incorrectly oriented text.", "result": "ROSA outperforms Greedy decoding by 11.7 absolute points in the best-performing model.", "conclusion": "ROSA addresses the challenges faced by VQA systems in interpreting text in photos taken by visually impaired individuals."}}
{"id": "2506.03681", "pdf": "https://arxiv.org/pdf/2506.03681", "abs": "https://arxiv.org/abs/2506.03681", "authors": ["Pradeep Rangappa", "Andres Carofilis", "Jeena Prakash", "Shashi Kumar", "Sergio Burdisso", "Srikanth Madikeri", "Esau Villatoro-Tello", "Bidisha Sharma", "Petr Motlicek", "Kadri Hacioglu", "Shankar Venkatesan", "Saurabh Vyas", "Andreas Stolcke"], "title": "Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Fine-tuning pretrained ASR models for specific domains is challenging for\nsmall organizations with limited labeled data and computational resources.\nHere, we explore different data selection pipelines and propose a robust\napproach that improves ASR adaptation by filtering pseudo-labels generated\nusing Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach\nintegrates multiple selection strategies -- including word error rate (WER)\nprediction, named entity recognition (NER), and character error rate (CER)\nanalysis -- to extract high-quality training segments. We evaluate our method\non Whisper and Zipformer using a 7500-hour baseline, comparing it to a\nCER-based approach relying on hypotheses from three ASR systems. Fine-tuning on\n7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our\nfiltering reduces the dataset to 100 hours (1.4%) with similar performance; a\nsimilar trend is observed on Fisher English.", "AI": {"tldr": "Fine-tuning pretrained ASR models for specific domains can be challenging for small organizations with limited resources. This paper proposes a robust approach using data selection pipelines to improve ASR adaptation by filtering pseudo-labels from Whisper and Zipformer models. The method integrates multiple selection strategies such as WER prediction, NER, and CER analysis to extract high-quality training segments. It was evaluated on Whisper and Zipformer using a 7500-hour baseline and compared to a CER-based approach relying on hypotheses from three ASR systems.", "motivation": "Fine-tuning ASR models for specific domains is difficult for small organizations due to limited labeled data and computational resources.", "method": "The paper explores different data selection pipelines and proposes an approach that improves ASR adaptation by filtering pseudo-labels generated using Whisper and Zipformer models. The method integrates multiple selection strategies including WER prediction, NER, and CER analysis to extract high-quality training segments.", "result": "Fine-tuning on 7500 hours of pseudo-labeled call center data achieved 12.3% WER, while the proposed filtering reduced the dataset to 100 hours (1.4%) with similar performance; a similar trend was observed on Fisher English.", "conclusion": "This approach offers a more efficient way for small organizations to fine-tune ASR models for specific domains by significantly reducing the amount of required training data without sacrificing performance."}}
{"id": "2506.03690", "pdf": "https://arxiv.org/pdf/2506.03690", "abs": "https://arxiv.org/abs/2506.03690", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "title": "Robust Preference Optimization via Dynamic Target Margins", "categories": ["cs.CL"], "comment": "18 pages, 6 figures, accepted to The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL2025)", "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.", "AI": {"tldr": "\u03b3-PO is a novel algorithm improving LLMs' alignment by optimizing reward margins dynamically.", "motivation": "To address the issue of noise affecting the data quality in DPO, which impacts the effectiveness of LLMs alignment.", "method": "A dynamic target margin preference optimization algorithm named \u03b3-PO is proposed.", "result": "\u03b3-PO shows an average 4.4% improvement over other baselines and is compatible with variants of DPO.", "conclusion": "\u03b3-PO improves the alignment of LLMs with better performance across various benchmarks."}}
{"id": "2506.03700", "pdf": "https://arxiv.org/pdf/2506.03700", "abs": "https://arxiv.org/abs/2506.03700", "authors": ["Zhepei Wei", "Wei-Lin Chen", "Xinyu Zhu", "Yu Meng"], "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism", "categories": ["cs.CL"], "comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode", "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.", "AI": {"tldr": "AdaDecode accelerates LLM decoding by adaptively generating tokens at intermediate layers, achieving up to 1.73x speedup without sacrificing output quality.", "motivation": "Autoregressive decoding is limited by its sequential token generation process, restricting the ability to fully leverage modern hardware's parallel processing capabilities.", "method": "AdaDecode adaptively generates tokens at intermediate layers when confidence is high, enabling parallel computation and reducing decoding latency.", "result": "Experiments show that AdaDecode achieves up to 1.73x speedup in decoding throughput while maintaining output parity with standard autoregressive decoding.", "conclusion": "AdaDecode is a novel decoding method that accelerates LLM decoding without auxiliary models or changes to the original model parameters, ensuring output consistency."}}
{"id": "2506.03704", "pdf": "https://arxiv.org/pdf/2506.03704", "abs": "https://arxiv.org/abs/2506.03704", "authors": ["Pei-Yun Lin", "Yen-lung Tsai"], "title": "ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "11 pages, 8 figures. Code and demo available at\n  https://github.com/peiyun2260/ScoreRAG. Submitted to arXiv for public access;\n  journal submission planned", "summary": "This research introduces ScoreRAG, an approach to enhance the quality of\nautomated news generation. Despite advancements in Natural Language Processing\nand large language models, current news generation methods often struggle with\nhallucinations, factual inconsistencies, and lack of domain-specific expertise\nwhen producing news articles. ScoreRAG addresses these challenges through a\nmulti-stage framework combining retrieval-augmented generation, consistency\nrelevance evaluation, and structured summarization. The system first retrieves\nrelevant news documents from a vector database, maps them to complete news\nitems, and assigns consistency relevance scores based on large language model\nevaluations. These documents are then reranked according to relevance, with\nlow-quality items filtered out. The framework proceeds to generate graded\nsummaries based on relevance scores, which guide the large language model in\nproducing complete news articles following professional journalistic standards.\nThrough this methodical approach, ScoreRAG aims to significantly improve the\naccuracy, coherence, informativeness, and professionalism of generated news\narticles while maintaining stability and consistency throughout the generation\nprocess. The code and demo are available at:\nhttps://github.com/peiyun2260/ScoreRAG.", "AI": {"tldr": "ScoreRAG is a multi-stage framework that enhances automated news generation by improving accuracy, coherence, and informativeness through retrieval-augmented generation and large language model evaluations.", "motivation": "Current news generation methods often suffer from hallucinations, factual inconsistencies, and lack of domain-specific expertise.", "method": "The framework includes retrieval-augmented generation, consistency relevance evaluation, and structured summarization. Relevant documents are retrieved, scored, and filtered before being used to guide the large language model in generating professional news articles.", "result": "ScoreRAG improves the quality of generated news articles in terms of accuracy, coherence, informativeness, and professionalism while maintaining stability and consistency.", "conclusion": "ScoreRAG offers a novel solution to the challenges in automated news generation using a multi-stage approach."}}
{"id": "2506.03722", "pdf": "https://arxiv.org/pdf/2506.03722", "abs": "https://arxiv.org/abs/2506.03722", "authors": ["Yinfeng Xia", "Huiyan Li", "Chenyang Le", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Applying large pre-trained speech models like Whisper has shown promise in\nreducing training costs for various speech tasks. However, integrating these\nmodels into streaming systems remains a challenge. This paper presents a novel\nprefix-to-prefix training framework for streaming recognition by fine-tuning\nthe Whisper. We introduce the Continuous Integrate-and-Fire mechanism to\nestablish a quasi-monotonic alignment between continuous speech sequences and\ndiscrete text tokens. Additionally, we design Monotonic Finite Look-ahead\nAttention, allowing each token to attend to infinite left-context and finite\nright-context from the speech sequences. We also employ the wait-k decoding\nstrategy to simplify the decoding process while ensuring consistency between\ntraining and testing. Our theoretical analysis and experiments demonstrate that\nthis approach achieves a controllable trade-off between latency and quality,\nmaking it suitable for various streaming applications.", "AI": {"tldr": "This paper introduces a new prefix-to-prefix training framework for streaming speech recognition using fine-tuned Whisper models. It includes mechanisms for quasi-monotonic alignment and attention, along with a simplified decoding strategy, achieving a balance between latency and quality.", "motivation": "To reduce training costs for speech tasks and address challenges in integrating large pre-trained speech models into streaming systems.", "method": "Fine-tuning Whisper models with a prefix-to-prefix training framework, introducing Continuous Integrate-and-Fire for alignment, designing Monotonic Finite Look-ahead Attention, and employing wait-k decoding strategy.", "result": "Achieves a controllable trade-off between latency and quality, suitable for various streaming applications.", "conclusion": "The proposed method effectively integrates large pre-trained speech models into streaming systems, offering a promising solution for real-time speech recognition tasks."}}
{"id": "2506.03723", "pdf": "https://arxiv.org/pdf/2506.03723", "abs": "https://arxiv.org/abs/2506.03723", "authors": ["Chaeyun Jang", "Moonseok Choi", "Yegon Kim", "Hyungi Lee", "Juho Lee"], "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty calibration is essential for the safe deployment of large\nlanguage models (LLMs), particularly when users rely on verbalized confidence\nestimates. While prior work has focused on classifiers or short-form\ngeneration, confidence calibration for chain-of-thought (CoT) reasoning remains\nlargely unexplored. Surprisingly, we find that supervised fine-tuning with\nscalar confidence labels alone suffices to elicit self-verification behavior of\nlanguage models, without any explicit reasoning supervision or reinforcement\nlearning-based rewards. Despite being trained only to produce a verbalized\nconfidence score without any self-verifying examples, the model learns to\ngenerate longer and self-checking responses for low-confidence queries while\nproviding more concise answers for high-confidence ones. We further propose a\nsimple rethinking method that boosts performance via test-time scaling based on\ncalibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such\nas MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning\nimproves both calibration and accuracy, while also enhancing interpretability\nby aligning the model's reasoning path with its confidence.", "AI": {"tldr": "This paper explores uncertainty calibration for chain-of-thought reasoning in large language models, finding that supervised fine-tuning with scalar confidence labels alone can elicit self-verification behaviors.", "motivation": "To improve the safety and reliability of large language models by ensuring their verbalized confidence estimates are accurate.", "method": "Supervised fine-tuning with scalar confidence labels.", "result": "The model learns to generate longer and self-checking responses for low-confidence queries and more concise answers for high-confidence ones. A rethinking method is proposed to boost performance via test-time scaling based on calibrated uncertainty.", "conclusion": "Confidence-aware fine-tuning improves both calibration and accuracy, enhancing interpretability by aligning the model's reasoning path with its confidence."}}
{"id": "2506.03735", "pdf": "https://arxiv.org/pdf/2506.03735", "abs": "https://arxiv.org/abs/2506.03735", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "summary": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements.", "AI": {"tldr": "\u63d0\u51faMath2Visual\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u8bbe\u8ba1\u7a7a\u95f4\u81ea\u52a8\u751f\u6210\u6570\u5b66\u5e94\u7528\u9898\u7684\u6559\u80b2\u6027\u89c6\u89c9\uff0c\u5efa\u7acb\u65b0\u57fa\u51c6\u5e76\u63d0\u4f9b\u591a\u6a21\u6001\u6559\u80b2\u5185\u5bb9\u751f\u4ea7\u7684\u89c1\u89e3\u3002", "motivation": "\u521b\u5efa\u6570\u5b66\u5e94\u7528\u9898\u7684\u89c6\u89c9\u8f85\u52a9\u5de5\u5177\u5f88\u8d39\u529b\u4e14\u7f3a\u4e4f\u81ea\u52a8\u5316\u65b9\u6cd5\u652f\u6301\u3002", "method": "\u63d0\u51faMath2Visual\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u8bbe\u8ba1\u7a7a\u95f4\u6765\u751f\u6210\u6559\u80b2\u4e0a\u6709\u610f\u4e49\u7684\u89c6\u89c9\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1903\u4e2a\u89c6\u89c9\u7684\u6ce8\u91ca\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u4e0e\u8bbe\u8ba1\u4e00\u81f4\u7684\u89c6\u89c9\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5bf9\u51e0\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5c55\u793a\u4e86\u5728\u6559\u80b2\u89c6\u89c9\u751f\u6210\u65b9\u9762\u7684\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u81ea\u52a8\u4ea7\u751f\u6559\u80b2\u4e0a\u6709\u610f\u4e49\u7684\u89c6\u89c9\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u751f\u4ea7\u591a\u6a21\u6001\u6559\u80b2\u5185\u5bb9\u7684\u5173\u952e\u6311\u6218\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.03761", "pdf": "https://arxiv.org/pdf/2506.03761", "abs": "https://arxiv.org/abs/2506.03761", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Zheyu Ye", "Zhoujun Li", "Zuozhu Liu"], "title": "Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services", "categories": ["cs.CL"], "comment": null, "summary": "As interest in using Large Language Models (LLMs) for interactive and\nemotionally rich experiences grows, virtual pet companionship emerges as a\nnovel yet underexplored application. Existing approaches focus on basic pet\nrole-playing interactions without systematically benchmarking LLMs for\ncomprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated\nbenchmark that evaluates LLMs across both self-interaction and\nhuman-interaction dimensions. Unlike prior work, Pet-Bench emphasizes\nself-evolution and developmental behaviors alongside interactive engagement,\noffering a more realistic reflection of pet companionship. It features diverse\ntasks such as intelligent scheduling, memory-based dialogues, and psychological\nconversations, with over 7,500 interaction instances designed to simulate\ncomplex pet behaviors. Evaluation of 28 LLMs reveals significant performance\nvariations linked to model size and inherent capabilities, underscoring the\nneed for specialized optimization in this domain. Pet-Bench serves as a\nfoundational resource for benchmarking pet-related LLM abilities and advancing\nemotionally immersive human-pet interactions.", "AI": {"tldr": "Introduce Pet-Bench, a benchmark for evaluating large language models in pet companionship tasks, revealing performance variations among different models.", "motivation": "To systematically evaluate LLMs for comprehensive pet companionship rather than just basic role-playing interactions.", "method": "Benchmarking LLMs with Pet-Bench which includes self-interaction and human-interaction dimensions.", "result": "Significant performance variations among 28 LLMs were found and the need for specialized optimization was highlighted.", "conclusion": "Pet-Bench is a new benchmark for evaluating LLMs in pet companionship tasks."}}
{"id": "2506.03762", "pdf": "https://arxiv.org/pdf/2506.03762", "abs": "https://arxiv.org/abs/2506.03762", "authors": ["Yifeng Gu", "Zicong Jiang", "Jianxiu Jin", "Kailing Guo", "Ziyang Zhang", "Xiangmin Xu"], "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 8 figures", "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.", "AI": {"tldr": "Propose a new method named AhaKV to reduce the bias of accumulated attention score and retain crucial tokens across global context.", "motivation": "The current methods rely on accumulated attention score for token eviction, which is biased and limits model's access to global contextual information.", "method": "Adaptively tune the scale of softmax based on the expectation of information entropy of attention scores and utilize the information of value vectors to refine the adaptive score.", "result": "Successfully mitigate bias and retain crucial tokens across global context with state-of-the-art results on several benchmark tasks.", "conclusion": "AhaKV is a novel approach to reduce the bias of accumulated attention score and improve the performance of LLMs during inference."}}
{"id": "2506.03763", "pdf": "https://arxiv.org/pdf/2506.03763", "abs": "https://arxiv.org/abs/2506.03763", "authors": ["Quang Hieu Pham", "Thuy Duong Nguyen", "Tung Pham", "Anh Tuan Luu", "Dat Quoc Nguyen"], "title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "The capabilities of large language models (LLMs) have been enhanced by\ntraining on data that reflects human thought processes, such as the\nChain-of-Thought format. However, evidence suggests that the conventional\nscheme of next-word prediction may not fully capture how humans learn to think.\nInspired by how humans generalize mathematical reasoning, we propose a new\napproach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our\nClozeMath involves a text-infilling task that predicts masked equations from a\ngiven solution, analogous to cloze exercises used in human learning.\nExperiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the\nstrong baseline Masked Thought in performance and robustness, with two\ntest-time scaling decoding algorithms, Beam Search and Chain-of-Thought\ndecoding. Additionally, we conduct an ablation study to analyze the effects of\nvarious architectural and implementation choices on our approach.", "AI": {"tldr": "This paper introduces ClozeMath, a novel approach for fine-tuning large language models on mathematical reasoning using text-infilling tasks. ClozeMath outperforms the Masked Thought baseline across multiple datasets.", "motivation": "To address limitations of next-word prediction in capturing human-like thinking processes in LLMs.", "method": "Introduces ClozeMath, which uses text-infilling to predict masked equations from solutions.", "result": "ClozeMath outperforms Masked Thought with improved performance and robustness, especially with Beam Search and Chain-of-Thought decoding.", "conclusion": "The proposed ClozeMath method provides a more effective way to enhance LLMs' mathematical reasoning capabilities."}}
{"id": "2506.03781", "pdf": "https://arxiv.org/pdf/2506.03781", "abs": "https://arxiv.org/abs/2506.03781", "authors": ["Seungcheol Park", "Jeongin Bae", "Beomseok Kwon", "Minjun Kim", "Byeongwook Kim", "Se Jung Kwon", "U Kang", "Dongsoo Lee"], "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "ACL 2025 Main Track", "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.", "AI": {"tldr": "A new method called UniQuanF improves the accuracy of large language model quantization.", "motivation": "To deploy large language models efficiently, it's necessary to quantize them while maintaining accuracy.", "method": "UniQuanF combines the strengths of binary-coding quantization and uniform quantization with unified initialization and mapping techniques.", "result": "UniQuanF surpasses previous methods, increasing accuracy by up to 4.60% on GSM8K benchmark.", "conclusion": "UniQuanF achieves high accuracy in quantizing large language models without additional deployment costs."}}
{"id": "2506.03785", "pdf": "https://arxiv.org/pdf/2506.03785", "abs": "https://arxiv.org/abs/2506.03785", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "4 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "AI": {"tldr": "Knockout Assessment is proposed to improve the accuracy of LLM-based evaluations by using iterative pairwise comparisons in a knockout tournament format.", "motivation": "Current LLM-as-a-Judge approaches lack a global ranking perspective due to reliance on individual or single-round pairwise assessments.", "method": "Introducing Knockout Assessment which uses a knockout tournament system with iterative pairwise comparisons.", "result": "Knockout assessment improved scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations.", "conclusion": "The results indicate that LLM assessments can be aligned more closely with human scoring through the use of Knockout Assessment."}}
{"id": "2506.03793", "pdf": "https://arxiv.org/pdf/2506.03793", "abs": "https://arxiv.org/abs/2506.03793", "authors": ["Sidharth Pulipaka", "Sparsh Jain", "Ashwin Sankar", "Raj Dabre"], "title": "Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Punctuation plays a vital role in structuring meaning, yet current models\noften struggle to restore it accurately in transcripts of spontaneous speech,\nespecially in the presence of disfluencies such as false starts and\nbacktracking. These limitations hinder the performance of downstream tasks like\ntranslation, text to speech, summarization, etc. where sentence boundaries are\ncritical for preserving quality. In this work, we introduce Cadence, a\ngeneralist punctuation restoration model adapted from a pretrained large\nlanguage model. Cadence is designed to handle both clean written text and\nhighly spontaneous spoken transcripts. It surpasses the previous state of the\nart in performance while expanding support from 14 to all 22 Indian languages\nand English. We conduct a comprehensive analysis of model behavior across\npunctuation types and language families, identifying persistent challenges\nunder domain shift and with rare punctuation marks. Our findings demonstrate\nthe efficacy of utilizing pretrained language models for multilingual\npunctuation restoration and highlight Cadence practical value for low resource\nNLP pipelines at scale.", "AI": {"tldr": "This paper introduces Cadence, a new punctuation restoration model that performs better than previous methods and supports more languages.", "motivation": "Current models struggle to restore punctuation accurately in spontaneous speech, especially with disfluencies, hindering downstream tasks.", "method": "Introducing Cadence, a generalist punctuation restoration model adapted from a pretrained large language model.", "result": "Cadence outperforms previous models and expands language support from 14 to 22 Indian languages and English.", "conclusion": "Cadence surpasses previous state-of-the-art performance in punctuation restoration and supports more languages."}}
{"id": "2506.03820", "pdf": "https://arxiv.org/pdf/2506.03820", "abs": "https://arxiv.org/abs/2506.03820", "authors": ["Ahmad Mustapha Wali", "Sergiu Nisioi"], "title": "Automatic Correction of Writing Anomalies in Hausa Texts", "categories": ["cs.CL"], "comment": null, "summary": "Hausa texts are often characterized by writing anomalies such as incorrect\ncharacter substitutions and spacing errors, which sometimes hinder natural\nlanguage processing (NLP) applications. This paper presents an approach to\nautomatically correct the anomalies by finetuning transformer-based models.\nUsing a corpus gathered from several public sources, we created a large-scale\nparallel dataset of over 450,000 noisy-clean Hausa sentence pairs by\nintroducing synthetically generated noise, fine-tuned to mimic realistic\nwriting errors. Moreover, we adapted several multilingual and African\nlanguage-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT\nvariants for this correction task using SentencePiece tokenization. Our\nexperimental results demonstrate significant increases in F1, BLEU and METEOR\nscores, as well as reductions in Character Error Rate (CER) and Word Error Rate\n(WER). This research provides a robust methodology, a publicly available\ndataset, and effective models to improve Hausa text quality, thereby advancing\nNLP capabilities for the language and offering transferable insights for other\nlow-resource languages.", "AI": {"tldr": "This paper presents an approach to automatically correct writing anomalies in Hausa texts by fine-tuning transformer-based models, achieving significant improvements in various evaluation metrics.", "motivation": "The motivation is to address the challenges posed by writing anomalies such as incorrect character substitutions and spacing errors in Hausa texts, which can hinder natural language processing (NLP) applications.", "method": "The method involves fine-tuning transformer-based models to automatically correct writing anomalies in Hausa texts. A large-scale parallel dataset of over 450,000 noisy-clean Hausa sentence pairs was created by introducing synthetically generated noise. Several multilingual and African language-focused models were adapted for this correction task using SentencePiece tokenization.", "result": "The experimental results show significant improvements in F1, BLEU and METEOR scores, as well as reductions in Character Error Rate (CER) and Word Error Rate (WER).", "conclusion": "This research offers a reliable method, a publicly accessible dataset, and efficient models to enhance the quality of Hausa texts, thus promoting NLP capabilities for the language and providing transferable knowledge for other low-resource languages."}}
{"id": "2506.03822", "pdf": "https://arxiv.org/pdf/2506.03822", "abs": "https://arxiv.org/abs/2506.03822", "authors": ["Fabian Karl", "Ansgar Scherp"], "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SCOLIA 2025", "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.", "AI": {"tldr": "This paper presents CRAWLDoc, a novel approach for contextual ranking of linked web documents to improve metadata extraction from diverse web sources.", "motivation": "The challenge of extracting accurate metadata from web sources due to variations in web layouts and data formats.", "method": "CRAWLDoc starts with a publication's URL, retrieves linked web resources, embeds them into a unified representation, and ranks relevant documents.", "result": "CRAWLDoc demonstrates robust and layout-independent ranking of relevant documents across publishers and data formats using a new manually labeled dataset.", "conclusion": "CRAWLDoc provides a foundation for enhanced metadata extraction from web documents with varying layouts and formats."}}
{"id": "2506.03827", "pdf": "https://arxiv.org/pdf/2506.03827", "abs": "https://arxiv.org/abs/2506.03827", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR2025", "summary": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.", "AI": {"tldr": "This paper introduces MoBGM, a model designed to enhance advertisement recall in e-commerce search by improving query rewriting relevance and authenticity while maximizing revenue.", "motivation": "Existing query rewriting methods often fail to optimize both the relevance/authenticity of user queries and the revenue potential of recalled ads.", "method": "MoBGM uses a discriminator, generator, and preference alignment module to simultaneously achieve these goals.", "result": "Extensive offline and online experiments show significant improvements over existing methods, creating substantial commercial value after deployment.", "conclusion": "The proposed MoBGM model demonstrates feasibility and robustness in enhancing advertisement recall and platform revenue."}}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832", "abs": "https://arxiv.org/abs/2506.03832", "authors": ["Omer Moussa", "Mariya Toneva"], "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "AI": {"tldr": "This study explores the effectiveness of brain-tuning on improving speech models' semantic understanding and hierarchical processing. The results show that brain-tuned models outperform pretrained models in aligning with semantic language regions and exhibit a clear hierarchy from acoustic to semantic representations.", "motivation": "To examine how well brain-tuned models reflect the brain's intermediate stages of speech processing.", "method": "Brain-tuning models using human brain recordings and layer-wise probing.", "result": "Late layers of brain-tuned models significantly improve in aligning with semantic language regions, and late layers are best at complex high-level tasks while early layers remain dedicated to low-level acoustic features.", "conclusion": "Brain-tuned models not only perform better but also exhibit a well-defined hierarchical processing from acoustic to semantic representations, making them better model organisms for human speech processing."}}
{"id": "2506.03861", "pdf": "https://arxiv.org/pdf/2506.03861", "abs": "https://arxiv.org/abs/2506.03861", "authors": ["Qiuhan Han", "Qian Wang", "Atsushi Yoshikawa", "Masayuki Yamamura"], "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading", "categories": ["cs.CL"], "comment": null, "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia.", "AI": {"tldr": "This paper introduces PulseReddit, a new dataset that matches Reddit discussions with cryptocurrency market stats for high-frequency trading analysis. It shows that using this data with MAS improves trading results, especially in bull markets.", "motivation": "To explore the potential of social media like Reddit for high-frequency trading in cryptocurrency markets.", "method": "Using PulseReddit data and LLM-based MAS to study the effect of social sentiment on trading performance.", "result": "MAS with PulseReddit data outperforms traditional methods in trading, especially beneficial in bull markets, and adapts well across various market conditions.", "conclusion": "Integrating social media data can enhance high-frequency trading strategies, with important implications for model selection and future MAS research."}}
{"id": "2506.03867", "pdf": "https://arxiv.org/pdf/2506.03867", "abs": "https://arxiv.org/abs/2506.03867", "authors": ["Jacqueline Rowe", "Mateusz Klimaszewski", "Liane Guillou", "Shannon Vallor", "Alexandra Birch"], "title": "EuroGEST: Investigating gender stereotypes in multilingual language models", "categories": ["cs.CL"], "comment": "8 pages, 6 figures, 1 table", "summary": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages.", "AI": {"tldr": "Introduce EuroGEST dataset to measure gender-stereotypical reasoning in LLMs across English and 29 European languages.", "motivation": "Most benchmarks for gender bias remain English-centric, but large language models support multiple languages.", "method": "EuroGEST builds on an existing expert-informed benchmark, expanded using translation tools, quality estimation metrics, and morphological heuristics.", "result": "Evaluate 24 multilingual language models from six model families, showing stronger stereotypes in larger models and instruction finetuning does not consistently reduce gendered stereotypes.", "conclusion": "Highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages."}}
{"id": "2506.03880", "pdf": "https://arxiv.org/pdf/2506.03880", "abs": "https://arxiv.org/abs/2506.03880", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Jianhua Tao"], "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential.", "AI": {"tldr": "A new framework called RadialRouter is proposed for efficient selection of optimal large language models.", "motivation": "Current LLM routing methods are limited in effectiveness due to insufficient exploration of the connection between user queries and LLM characteristics.", "method": "RadialRouter uses a lightweight Transformer-based backbone with a radial structure named RadialFormer to articulate the query-LLMs relationship.", "result": "RadialRouter outperforms existing routing methods by 9.2% and 5.8% in the Balance and Cost First scenarios.", "conclusion": "The proposed RadialRouter demonstrates practical application potential with adaptability toward different performance-cost trade-offs and dynamic LLM pools."}}
{"id": "2506.03884", "pdf": "https://arxiv.org/pdf/2506.03884", "abs": "https://arxiv.org/abs/2506.03884", "authors": ["Utkarsh Pathak", "Chandra Sai Krishna Gunda", "Anusha Prakash", "Keshav Agarwal", "Hema A. Murthy"], "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages", "categories": ["cs.CL", "cs.CV", "I.5.4"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Text-to-speech (TTS) systems typically require high-quality studio data and\naccurate transcriptions for training. India has 1369 languages, with 22\nofficial using 13 scripts. Training a TTS system for all these languages, most\nof which have no digital resources, seems a Herculean task. Our work focuses on\nzero-shot synthesis, particularly for languages whose scripts and phonotactics\ncome from different families. The novelty of our work is in the augmentation of\na shared phone representation and modifying the text parsing rules to match the\nphonotactics of the target language, thus reducing the synthesiser overhead and\nenabling rapid adaptation. Intelligible and natural speech was generated for\nSanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging\nlinguistic connections across languages with suitable synthesisers. Evaluations\nconfirm the effectiveness of this approach, highlighting its potential to\nexpand speech technology access for under-represented languages.", "AI": {"tldr": "This study focuses on zero-shot synthesis for languages with no digital resources, introducing a novel method that augments shared phone representation and modifies text parsing rules to match target language phonotactics, enabling intelligible and natural speech generation for several Indian languages.", "motivation": "To address the challenge of training TTS systems for languages with no digital resources or transcriptions in India, which has 1369 languages and 13 scripts.", "method": "Augmenting shared phone representation and modifying text parsing rules to match target language phonotactics to reduce synthesiser overhead and enable rapid adaptation.", "result": "Intelligible and natural speech was successfully generated for Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging linguistic connections across languages with suitable synthesisers.", "conclusion": "The proposed approach confirms the effectiveness of zero-shot synthesis for under-represented languages, showing potential to expand speech technology access."}}
{"id": "2506.03887", "pdf": "https://arxiv.org/pdf/2506.03887", "abs": "https://arxiv.org/abs/2506.03887", "authors": ["Junyi Chen", "Shihao Bai", "Zaijun Wang", "Siyu Wu", "Chuheng Du", "Hailong Yang", "Ruihao Gong", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "title": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation", "categories": ["cs.CL"], "comment": "Published as a conference paper at ACL 2025", "summary": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm.", "AI": {"tldr": "Pre$^3$ optimizes LLM decoding efficiency by transforming LR(1) transition graphs into DPDA, reducing TPOT by up to 40% and increasing throughput by up to 36%.", "motivation": "Existing methods for parsing LR(1) grammars lead to runtime execution overhead for context-dependent token processing, especially inefficient under large inference batches.", "method": "Pre$^3$ precomputes prefix-conditioned edges during preprocessing and introduces a novel approach to transform LR(1) transition graphs into DPDA.", "result": "Pre$^3$ reduces time per output token (TPOT) by up to 40% and increases throughput by up to 36% in experiments.", "conclusion": "Pre$^3$ optimizes LLM decoding efficiency and can be seamlessly integrated into standard LLM inference frameworks."}}
{"id": "2506.03901", "pdf": "https://arxiv.org/pdf/2506.03901", "abs": "https://arxiv.org/abs/2506.03901", "authors": ["Yuxin Zhang", "Yan Wang", "Yongrui Chen", "Shenyu Zhang", "Xinbang Dai", "Sheng Bi", "Guilin Qi"], "title": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge.\n  However, RAG systems are highly sensitive to retrieval noise prevalent in\nreal-world scenarios.\n  Existing benchmarks fail to emulate the complex and heterogeneous noise\ndistributions encountered in real-world retrieval environments, undermining\nreliable robustness assessment.\n  In this paper, we define four categories of retrieval noise based on\nlinguistic properties and noise characteristics, aiming to reflect the\nheterogeneity of noise in real-world scenarios.\n  Building on this, we introduce Magic Mushroom, a benchmark for replicating\n\"magic mushroom\" noise: contexts that appear relevant on the surface but\ncovertly mislead RAG systems.\n  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer\npairs.\n  More importantly, Magic Mushroom enables researchers to flexibly configure\ncombinations of retrieval noise according to specific research objectives or\napplication scenarios, allowing for highly controlled evaluation setups.\n  We evaluate LLM generators of varying parameter scales and classic RAG\ndenoising strategies under diverse noise distributions to investigate their\nperformance dynamics during progressive noise encroachment.\n  Our analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions.\n  Magic Mushroom emerges as a promising tool for evaluating and advancing\nnoise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications.\n  The Magic Mushroom benchmark is available at the\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.", "AI": {"tldr": "\u63d0\u51faMagic Mushroom\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5728\u9762\u5bf9\u590d\u6742\u566a\u58f0\u65f6\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u65e0\u6cd5\u6a21\u62df\u73b0\u5b9e\u68c0\u7d22\u73af\u5883\u4e2d\u590d\u6742\u4e14\u5f02\u6784\u7684\u566a\u58f0\u5206\u5e03\uff0c\u96be\u4ee5\u53ef\u9760\u5730\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5b9a\u4e49\u56db\u79cd\u57fa\u4e8e\u8bed\u8a00\u7279\u6027\u548c\u566a\u58f0\u7279\u5f81\u7684\u68c0\u7d22\u566a\u58f0\u7c7b\u522b\uff0c\u6784\u5efaMagic Mushroom\u57fa\u51c6\u6765\u590d\u5236\u201c\u9b54\u6cd5\u8611\u83c7\u201d\u566a\u58f0\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5668\u548c\u7ecf\u5178\u53bb\u566a\u7b56\u7565\u5728\u4e0d\u540c\u566a\u58f0\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u3002", "result": "Magic Mushroom\u5305\u542b7468\u4e2a\u5355\u8df3\u548c3925\u4e2a\u591a\u8df3\u95ee\u7b54\u5bf9\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u6839\u636e\u5177\u4f53\u7684\u7814\u7a76\u76ee\u6807\u6216\u5e94\u7528\u573a\u666f\u7075\u6d3b\u914d\u7f6e\u68c0\u7d22\u566a\u58f0\u7ec4\u5408\uff0c\u8bc4\u4f30\u4e0d\u540c\u566a\u58f0\u5206\u5e03\u4e0b\u751f\u6210\u5668\u548c\u53bb\u566a\u7b56\u7565\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u566a\u58f0\u5206\u5e03\u9762\u524d\u8868\u73b0\u51fa\u6781\u7aef\u654f\u611f\u6027\uff0c\u4e14\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "Magic Mushroom\u4f5c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u5bf9\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u566a\u58f0\u9c81\u68d2\u6027\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u4ee5\u52a0\u901f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\u3002"}}
{"id": "2506.03902", "pdf": "https://arxiv.org/pdf/2506.03902", "abs": "https://arxiv.org/abs/2506.03902", "authors": ["Eleftheria Tsipidi", "Samuel Kiegeland", "Franz Nowak", "Tianyang Xu", "Ethan Wilcox", "Alex Warstadt", "Ryan Cotterell", "Mario Giulianelli"], "title": "The Harmonic Structure of Information Contours", "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "The uniform information density (UID) hypothesis proposes that speakers aim\nto distribute information evenly throughout a text, balancing production effort\nand listener comprehension difficulty. However, language typically does not\nmaintain a strictly uniform information rate; instead, it fluctuates around a\nglobal average. These fluctuations are often explained by factors such as\nsyntactic constraints, stylistic choices, or audience design. In this work, we\nexplore an alternative perspective: that these fluctuations may be influenced\nby an implicit linguistic pressure towards periodicity, where the information\nrate oscillates at regular intervals, potentially across multiple frequencies\nsimultaneously. We apply harmonic regression and introduce a novel extension\ncalled time scaling to detect and test for such periodicity in information\ncontours. Analyzing texts in English, Spanish, German, Dutch, Basque, and\nBrazilian Portuguese, we find consistent evidence of periodic patterns in\ninformation rate. Many dominant frequencies align with discourse structure,\nsuggesting these oscillations reflect meaningful linguistic organization.\nBeyond highlighting the connection between information rate and discourse\nstructure, our approach offers a general framework for uncovering structural\npressures at various levels of linguistic granularity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u8bed\u8a00\u4fe1\u606f\u7387\u7684\u5468\u671f\u6027\u6a21\u5f0f\uff0c\u5e76\u53d1\u73b0\u5176\u4e0e\u8bdd\u8bed\u7ed3\u6784\u6709\u5173\u3002", "motivation": "\u4f20\u7edf\u7684\u5747\u5300\u4fe1\u606f\u5bc6\u5ea6\uff08UID\uff09\u5047\u8bbe\u8ba4\u4e3a\u8bf4\u8bdd\u8005\u65e8\u5728\u5728\u6574\u4e2a\u6587\u672c\u4e2d\u5747\u5300\u5206\u5e03\u4fe1\u606f\uff0c\u4f46\u8bed\u8a00\u901a\u5e38\u4e0d\u4f1a\u4fdd\u6301\u4e25\u683c\u7684\u5747\u5300\u4fe1\u606f\u7387\uff0c\u800c\u662f\u56f4\u7ed5\u5168\u7403\u5e73\u5747\u503c\u6ce2\u52a8\u3002\u8fd9\u4e9b\u6ce2\u52a8\u901a\u5e38\u7531\u8bf8\u5982\u53e5\u6cd5\u7ea6\u675f\u3001\u98ce\u683c\u9009\u62e9\u6216\u53d7\u4f17\u8bbe\u8ba1\u7b49\u56e0\u7d20\u89e3\u91ca\u3002\u8fd9\u9879\u5de5\u4f5c\u63a2\u7d22\u4e86\u4e00\u79cd\u66ff\u4ee3\u89c2\u70b9\uff0c\u5373\u8fd9\u4e9b\u6ce2\u52a8\u53ef\u80fd\u53d7\u5230\u9690\u5f0f\u8bed\u8a00\u5411\u5468\u671f\u6027\u538b\u529b\u7684\u5f71\u54cd\u3002", "method": "\u5e94\u7528\u8c10\u6ce2\u56de\u5f52\u548c\u5f15\u5165\u4e00\u79cd\u79f0\u4e3a\u65f6\u95f4\u5c3a\u5ea6\u7684\u65b0\u6269\u5c55\u6765\u68c0\u6d4b\u548c\u6d4b\u8bd5\u4fe1\u606f\u8f6e\u5ed3\u4e2d\u7684\u5468\u671f\u6027\u3002", "result": "\u5728\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u5df4\u65af\u514b\u8bed\u548c\u5df4\u897f\u8461\u8404\u7259\u8bed\u6587\u672c\u4e2d\u53d1\u73b0\u4e86\u4fe1\u606f\u7387\u7684\u4e00\u81f4\u5468\u671f\u6a21\u5f0f\u3002", "conclusion": "\u5206\u6790\u8868\u660e\uff0c\u4fe1\u606f\u7387\u5b58\u5728\u5468\u671f\u6027\u6a21\u5f0f\uff0c\u5e76\u4e14\u8bb8\u591a\u4e3b\u5bfc\u9891\u7387\u4e0e\u8bdd\u8bed\u7ed3\u6784\u4e00\u81f4\uff0c\u8fd9\u8868\u660e\u8fd9\u4e9b\u6ce2\u52a8\u53cd\u6620\u4e86\u6709\u610f\u4e49\u7684\u8bed\u8a00\u7ec4\u7ec7\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u63ed\u793a\u8bed\u8a00\u5b66\u5404\u4e2a\u5c42\u6b21\u4e0a\u7684\u7ed3\u6784\u6027\u538b\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2506.03913", "pdf": "https://arxiv.org/pdf/2506.03913", "abs": "https://arxiv.org/abs/2506.03913", "authors": ["Claire Barale", "Michael Rovatsos", "Nehal Bhuta"], "title": "When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Legal decisions are increasingly evaluated for fairness, consistency, and\nbias using machine learning (ML) techniques. In high-stakes domains like\nrefugee adjudication, such methods are often applied to detect disparities in\noutcomes. Yet it remains unclear whether statistical methods can meaningfully\nassess fairness in legal contexts shaped by discretion, normative complexity,\nand limited ground truth.\n  In this paper, we empirically evaluate three common ML approaches\n(feature-based analysis, semantic clustering, and predictive modeling) on a\nlarge, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our\nexperiments show that these methods produce divergent and sometimes\ncontradictory signals, that predictive modeling often depends on contextual and\nprocedural features rather than legal features, and that semantic clustering\nfails to capture substantive legal reasoning.\n  We show limitations of statistical fairness evaluation, challenge the\nassumption that statistical regularity equates to fairness, and argue that\ncurrent computational approaches fall short of evaluating fairness in legally\ndiscretionary domains. We argue that evaluating fairness in law requires\nmethods grounded not only in data, but in legal reasoning and institutional\ncontext.", "AI": {"tldr": "This paper evaluates three common machine learning approaches on a large dataset of Canadian refugee decisions, finding they produce conflicting signals and fail to adequately assess fairness in legally discretionary contexts.", "motivation": "To examine whether statistical methods can meaningfully assess fairness in legal contexts shaped by discretion, normative complexity, and limited ground truth.", "method": "Feature-based analysis, semantic clustering, and predictive modeling", "result": "These methods produce divergent and contradictory signals, predictive modeling often depends on contextual/procedural features, and semantic clustering fails to capture substantive legal reasoning.", "conclusion": "Statistical fairness evaluation has limitations, assuming statistical regularity equals fairness is flawed, and current computational approaches are insufficient for evaluating fairness in legally discretionary domains."}}
{"id": "2506.03916", "pdf": "https://arxiv.org/pdf/2506.03916", "abs": "https://arxiv.org/abs/2506.03916", "authors": ["Agostina Calabrese", "Tom Sherborne", "Bj\u00f6rn Ross", "Mirella Lapata"], "title": "Compositional Generalisation for Explainable Hate Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is key to online content moderation, but current models\nstruggle to generalise beyond their training data. This has been linked to\ndataset biases and the use of sentence-level labels, which fail to teach models\nthe underlying structure of hate speech. In this work, we show that even when\nmodels are trained with more fine-grained, span-level annotations (e.g.,\n\"artists\" is labeled as target and \"are parasites\" as dehumanising comparison),\nthey struggle to disentangle the meaning of these labels from the surrounding\ncontext. As a result, combinations of expressions that deviate from those seen\nduring training remain particularly difficult for models to detect. We\ninvestigate whether training on a dataset where expressions occur with equal\nfrequency across all contexts can improve generalisation. To this end, we\ncreate U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel\ncompositional generalisation benchmark of ~8,000 manually validated posts.\nTraining on a combination of U-PLEAD and real data improves compositional\ngeneralisation while achieving state-of-the-art performance on the\nhuman-sourced PLEAD.", "AI": {"tldr": "This study investigates the issue of hate speech detection models struggling to generalize beyond their training data due to dataset biases and the use of sentence-level labels. The authors propose a new approach using fine-grained, span-level annotations and introduce a novel dataset called U-PLEAD to improve model performance.", "motivation": "Current hate speech detection models struggle to generalize beyond their training data due to dataset biases and the use of sentence-level labels, which fail to teach models the underlying structure of hate speech.", "method": "The authors investigate whether training on a dataset where expressions occur with equal frequency across all contexts can improve generalization. They create U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel compositional generalization benchmark of ~8,000 manually validated posts.", "result": "Training on a combination of U-PLEAD and real data improves compositional generalization while achieving state-of-the-art performance on the human-sourced PLEAD.", "conclusion": "This work highlights the importance of considering the context in which expressions occur when training hate speech detection models."}}
{"id": "2506.03922", "pdf": "https://arxiv.org/pdf/2506.03922", "abs": "https://arxiv.org/abs/2506.03922", "authors": ["Zhaolu Kang", "Junhao Gong", "Jiaxu Yan", "Wanke Xia", "Yian Wang", "Ziwen Wang", "Huaxuan Ding", "Zhuo Cheng", "Wenhao Cao", "Zhiyuan Feng", "Siqi He", "Shannan Yan", "Junzhe Chen", "Xiaomin He", "Chaoya Jiang", "Wei Ye", "Kaidong Yu", "Xuelong Li"], "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\npotential to advance a broad range of domains. However, current benchmarks for\nevaluating MLLMs primarily emphasize general knowledge and vertical\nstep-by-step reasoning typical of STEM disciplines, while overlooking the\ndistinct needs and potential of the Humanities and Social Sciences (HSS). Tasks\nin the HSS domain require more horizontal, interdisciplinary thinking and a\ndeep integration of knowledge across related fields, which presents unique\nchallenges for MLLMs, particularly in linking abstract concepts with\ncorresponding visual representations. Addressing this gap, we present HSSBench,\na dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks\nin multiple languages, including the six official languages of the United\nNations. We also introduce a novel data generation pipeline tailored for HSS\nscenarios, in which multiple domain experts and automated agents collaborate to\ngenerate and iteratively refine each sample. HSSBench contains over 13,000\nmeticulously designed samples, covering six key categories. We benchmark more\nthan 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant\nchallenges even for state-of-the-art models. We hope that this benchmark will\ninspire further research into enhancing the cross-disciplinary reasoning\nabilities of MLLMs, especially their capacity to internalize and connect\nknowledge across fields.", "AI": {"tldr": "\u63d0\u51faHSSBench\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u6587\u5b66\u79d1\u548c\u793e\u4f1a\u79d1\u5b66\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u51c6\u4e3b\u8981\u5f3a\u8c03\u4e00\u822c\u77e5\u8bc6\u548c\u5782\u76f4\u7684\u5206\u6b65\u63a8\u7406\uff0c\u800c\u5ffd\u89c6\u4e86\u4eba\u6587\u5b66\u79d1\u548c\u793e\u4f1a\u79d1\u5b66\u7684\u72ec\u7279\u9700\u6c42\u548c\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u4e86\u65b0\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8d85\u8fc713,000\u4e2a\u6837\u672c\u8986\u76d6\u516d\u4e2a\u5173\u952e\u7c7b\u522b\u3002", "result": "HSSBench\u5305\u542b\u8d85\u8fc713,000\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6837\u672c\uff0c\u6db5\u76d6\u516d\u4e2a\u5173\u952e\u7c7b\u522b\uff0c\u5e76\u4e14\u8bc1\u660e\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728HSSBench\u4e0a\u4e5f\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "conclusion": "\u63d0\u51faHSSBench\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u6587\u5b66\u79d1\u548c\u793e\u4f1a\u79d1\u5b66\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u5e0c\u671b\u8fd9\u4e2a\u57fa\u51c6\u80fd\u6fc0\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u589e\u5f3a\u8de8\u5b66\u79d1\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.03923", "pdf": "https://arxiv.org/pdf/2506.03923", "abs": "https://arxiv.org/abs/2506.03923", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.", "AI": {"tldr": "\u7814\u7a76LLMs\u5728\u6bd4\u8f83\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u65b9\u5411\u6027\u6846\u67b6\u504f\u5dee\uff0c\u5e76\u63d0\u51faMathComp\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u7406\u89e3\u8bed\u4e49\u7ebf\u7d22\u5982\u4f55\u5851\u9020LLMs\u63a8\u7406\u7684\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5f15\u5165MathComp\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7814\u7a76LLMs\u5bf9\u8bed\u4e49\u7ebf\u7d22\u7684\u53cd\u5e94\u673a\u5236\u3002", "result": "\u53d1\u73b0LLMs\u5728\u6bd4\u8f83\u6570\u5b66\u95ee\u9898\u4e2d\u5b58\u5728\u4e00\u81f4\u4e14\u65b9\u5411\u6027\u7684\u6846\u67b6\u504f\u5dee\uff0c\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u53ef\u4ee5\u51cf\u5c11\u8fd9\u4e9b\u504f\u5dee\uff0c\u4f46\u6548\u679c\u56e0\u683c\u5f0f\u800c\u5f02\u3002", "conclusion": "\u63ed\u793a\u4e86LLMs\u5728\u6bd4\u8f83\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u65b9\u5411\u6027\u6846\u67b6\u504f\u5dee\uff0c\u5e76\u63d0\u51faMathComp\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u79cd\u504f\u5dee\u3002"}}
{"id": "2506.03941", "pdf": "https://arxiv.org/pdf/2506.03941", "abs": "https://arxiv.org/abs/2506.03941", "authors": ["Vivian Nguyen", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil"], "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations", "categories": ["cs.CL", "cs.AI", "cs.CY", "physics.soc-ph"], "comment": "To appear in the Proceedings of ACL 2025. Code and demo available in\n  ConvoKit (convokit.cornell.edu)", "summary": "During a conversation, there can come certain moments where its outcome hangs\nin the balance. In these pivotal moments, how one responds can put the\nconversation on substantially different trajectories leading to significantly\ndifferent outcomes. Systems that can detect when such moments arise could\nassist conversationalists in domains with highly consequential outcomes, such\nas mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting\nsuch pivotal moments as they happen, in an online fashion. Our approach relies\non the intuition that a moment is pivotal if our expectation of the outcome\nvaries widely depending on what might be said next. By applying our method to\ncrisis counseling conversations, we first validate it by showing that it aligns\nwith human perception -- counselors take significantly longer to respond during\nmoments detected by our method -- and with the eventual conversational\ntrajectory -- which is more likely to change course at these times. We then use\nour framework to explore the relation of the counselor's response during\npivotal moments with the eventual outcome of the session.", "AI": {"tldr": "Detect pivotal moments in conversations in real-time using an unsupervised computational method.", "motivation": "Systems that can detect pivotal moments in conversations can assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling.", "method": "An unsupervised computational method that detects pivotal moments based on varying expectations of outcomes depending on what might be said next.", "result": "The method aligns with human perception and conversational trajectory changes. It also explores the relation between counselor's response during pivotal moments and session outcome.", "conclusion": "The proposed method can help identify crucial points in conversations, potentially improving outcomes in high-stakes scenarios like crisis counseling."}}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949", "abs": "https://arxiv.org/abs/2506.03949", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.", "AI": {"tldr": "Introduce TableEval, a new benchmark for evaluating LLMs on complex TableQA tasks with diverse table structures and multilingual data.", "motivation": "Existing TableQA benchmarks are limited by their focus on simple flat tables and suffer from data leakage. They also fail to capture cross-lingual and cross-domain variability in practical applications.", "method": "Collect tables with various structures from four domains and include cross-lingual scenarios with tables in three languages. Propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level.", "result": "TableEval reveals critical gaps in the ability of state-of-the-art LLMs to handle complex, real-world TableQA tasks. SEAT achieves high agreement with human judgment.", "conclusion": "TableEval provides a more realistic and challenging benchmark for evaluating LLMs on TableQA tasks, highlighting areas for future improvement."}}
{"id": "2506.03968", "pdf": "https://arxiv.org/pdf/2506.03968", "abs": "https://arxiv.org/abs/2506.03968", "authors": ["Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding", "categories": ["cs.CL"], "comment": "To be published at ACL 2025", "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.", "AI": {"tldr": "This paper introduces a method to generate diverse and complex instructions for large language models using attributed grounding, resulting in a dataset called SynthQuestions which improves model performance.", "motivation": "To generate diverse, complex, and large-scale instruction data for aligning large language models.", "method": "Attributed grounding involving a top-down attribution process and a bottom-up synthesis process.", "result": "A dataset named SynthQuestions with 1 million instructions was created and proved effective in improving model performance.", "conclusion": "Models trained on the SynthQuestions dataset show leading performance on various benchmarks."}}
{"id": "2506.03978", "pdf": "https://arxiv.org/pdf/2506.03978", "abs": "https://arxiv.org/abs/2506.03978", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Viet Anh Nguyen"], "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets.", "AI": {"tldr": "Transformer\u6a21\u578b\u526a\u679d\u4e0d\u4ec5\u53ef\u5b9e\u73b0\u8ba1\u7b97\u8282\u7701\uff0c\u8fd8\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002SPRINT\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u52a8\u6001\u9009\u62e9\u6700\u4f73\u526a\u679d\u5934\u548c\u5c42\uff0c\u5728MATH500\u548cGSM8K\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7b56\u7565\u3002", "motivation": "\u53d1\u73b0\u9009\u62e9\u6027\u526a\u679d\u67d0\u4e9b\u6ce8\u610f\u529b\u5934\u80fd\u6539\u5584\u63a8\u7406\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u3002", "method": "\u63d0\u51faSPRINT\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u5d4c\u5165\u4e0e\u5934\u5d4c\u5165\u5bf9\u9f50\u6765\u8bc6\u522b\u66f4\u51c6\u786e\u63a8\u7406\u7684\u526a\u679d\u5934\u914d\u7f6e\u3002", "result": "\u5728MATH500\u548cGSM8K\u6570\u636e\u96c6\u4e0a\uff0cSPRINT\u65b9\u6cd5\u5927\u5e45\u8d85\u8d8a\u4f20\u7edf\u6700\u4f73-N\u548c\u968f\u673a\u5934\u9009\u62e9\u7b56\u7565\u3002", "conclusion": "\u6a21\u578b\u526a\u679d\u4e0d\u4ec5\u80fd\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd8\u53ef\u80fd\u589e\u5f3a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.03980", "pdf": "https://arxiv.org/pdf/2506.03980", "abs": "https://arxiv.org/abs/2506.03980", "authors": ["Takeshi Saga", "Catherine Pelachaud"], "title": "Voice Activity Projection Model with Multimodal Encoders", "categories": ["cs.CL"], "comment": null, "summary": "Turn-taking management is crucial for any social interaction. Still, it is\nchallenging to model human-machine interaction due to the complexity of the\nsocial context and its multimodal nature. Unlike conventional systems based on\nsilence duration, previous existing voice activity projection (VAP) models\nsuccessfully utilized a unified representation of turn-taking behaviors as\nprediction targets, which improved turn-taking prediction performance.\nRecently, a multimodal VAP model outperformed the previous state-of-the-art\nmodel by a significant margin. In this paper, we propose a multimodal model\nenhanced with pre-trained audio and face encoders to improve performance by\ncapturing subtle expressions. Our model performed competitively, and in some\ncases, even better than state-of-the-art models on turn-taking metrics. All the\nsource codes and pretrained models are available at\nhttps://github.com/sagatake/VAPwithAudioFaceEncoders.", "AI": {"tldr": "This paper proposes a multimodal model enhanced with pre-trained audio and face encoders to improve turn-taking prediction performance.", "motivation": "To model human-machine interaction more effectively by capturing subtle expressions.", "method": "Proposed a multimodal model using pre-trained audio and face encoders.", "result": "The model performed competitively or even better than state-of-the-art models on turn-taking metrics.", "conclusion": "A multimodal model enhanced with pre-trained audio and face encoders can improve turn-taking prediction performance."}}
{"id": "2506.03984", "pdf": "https://arxiv.org/pdf/2506.03984", "abs": "https://arxiv.org/abs/2506.03984", "authors": ["Carolin Holtermann", "Paul R\u00f6ttger", "Anne Lauscher"], "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks.", "AI": {"tldr": "This paper introduces GeoTemp, a dataset for evaluating language models' ability to jointly reason about time and space. It tests eight open chat models across different model families and finds that while models handle temporal knowledge well and performance scales up, integrating temporal and geographical information remains challenging. Performance isn't tied to specific geographic regions but improves with familiar location names. Prompt formulation greatly affects performance, with direct geographical knowledge injection being beneficial, whereas certain prompting techniques can hinder simpler tasks.", "motivation": "To explore language models' abilities to jointly reason over time and space in non-isolated and complex real-world settings.", "method": "Creating GeoTemp dataset and evaluating eight open chat models from three different model families on various combinations of temporal and geographic knowledge.", "result": "Models perform well with temporal knowledge alone and better with larger models. Combining temporal and geographical knowledge proves difficult. Familiar location names boost performance. Prompt formulation significantly impacts performance.", "conclusion": "Language models have varying capabilities when it comes to joint time-space reasoning, showing potential improvements with larger models and appropriate prompt engineering."}}
{"id": "2506.03989", "pdf": "https://arxiv.org/pdf/2506.03989", "abs": "https://arxiv.org/abs/2506.03989", "authors": ["Alex Laitenberger", "Christopher D. Manning", "Nelson F. Liu"], "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models", "categories": ["cs.CL"], "comment": "10 pages, 5 figures, for associated source code, see\n  https://github.com/alex-laitenberger/stronger-baselines-rag", "summary": "With the rise of long-context language models (LMs) capable of processing\ntens of thousands of tokens in a single pass, do multi-stage\nretrieval-augmented generation (RAG) pipelines still offer measurable benefits\nover simpler, single-stage approaches? To assess this question, we conduct a\ncontrolled evaluation for QA tasks under systematically scaled token budgets,\ncomparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three\nbaselines, including DOS RAG (Document's Original Structure RAG), a simple\nretrieve-then-read method that preserves original passage order. Despite its\nstraightforward design, DOS RAG consistently matches or outperforms more\nintricate methods on multiple long-context QA benchmarks. We recommend\nestablishing DOS RAG as a simple yet strong baseline for future RAG\nevaluations, pairing it with emerging embedding and language models to assess\ntrade-offs between complexity and effectiveness as model capabilities evolve.", "AI": {"tldr": "Long-context language models can handle tens of thousands of tokens, but multi-stage retrieval-augmented generation (RAG) pipelines like ReadAgent and RAPTOR don't always outperform simpler methods such as DOS RAG, which preserves original passage order and performs well on long-context QA tasks.", "motivation": "To evaluate whether multi-stage RAG pipelines still offer advantages over simpler, single-stage approaches in the context of long-context language models.", "method": "Controlled evaluation comparing multi-stage pipelines (ReadAgent and RAPTOR) against baselines including DOS RAG on QA tasks under scaled token budgets.", "result": "DOS RAG, despite its simple design, consistently matches or outperforms more complex methods on long-context QA benchmarks.", "conclusion": "Recommend establishing DOS RAG as a strong baseline for future RAG evaluations to assess trade-offs between complexity and effectiveness."}}
{"id": "2506.03990", "pdf": "https://arxiv.org/pdf/2506.03990", "abs": "https://arxiv.org/abs/2506.03990", "authors": ["Hongzhi Zhang", "Jingyuan Zhang", "Xingguang Ji", "Qi Wang", "Fuzheng Zhang"], "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u89c6\u9891\u4ee4\u724c\u538b\u7f29\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u4ee4\u724c\u6570\u91cf\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5178\u578b\u7684\u89c6\u9891\u5efa\u6a21\u65b9\u6cd5\uff08\u5982LLava\uff09\u5c06\u89c6\u9891\u8868\u793a\u4e3a\u89c6\u89c9\u4ee4\u724c\u5e8f\u5217\uff0c\u7136\u540e\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9aa8\u5e72\u5904\u7406\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u89c6\u9891\u7406\u89e3\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bfc\u81f4\u4e86\u5927\u91cf\u7684\u89c6\u89c9\u4ee4\u724c\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u957f\u89c6\u9891\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aDynTok\u7684\u65b0\u9896\u52a8\u6001\u89c6\u9891\u4ee4\u724c\u538b\u7f29\u7b56\u7565\u3002DynTok\u9002\u5e94\u6027\u5730\u5c06\u89c6\u89c9\u4ee4\u724c\u5206\u6210\u7ec4\u5e76\u5728\u6bcf\u4e2a\u7ec4\u5185\u5408\u5e76\u5b83\u4eec\uff0c\u5728\u4fe1\u606f\u5bc6\u5ea6\u4f4e\u7684\u533a\u57df\u5b9e\u73b0\u9ad8\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u5185\u5bb9\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u4ee4\u724c\u6570\u91cf\u51cf\u5c11\u5230\u539f\u59cb\u5927\u5c0f\u768444.4%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u6027\u80fd\u3002\u5b83\u8fdb\u4e00\u6b65\u53d7\u76ca\u4e8e\u589e\u52a0\u89c6\u9891\u5e27\u7684\u6570\u91cf\uff0c\u5728Video-MME\u4e0a\u8fbe\u523065.3\uff05\uff0c\u5728MLVU\u4e0a\u8fbe\u523072.5\uff05\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u51cf\u5c11\u4ee4\u724c\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5e94\u7528\u8fd9\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u89c6\u9891\u4ee4\u724c\u8868\u793a\u4e2d\u7684\u5197\u4f59\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u89c6\u9891\u5efa\u6a21\u6280\u672f\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2506.03993", "pdf": "https://arxiv.org/pdf/2506.03993", "abs": "https://arxiv.org/abs/2506.03993", "authors": ["Saif M. Mohammad"], "title": "Words of Warmth: Trust and Sociability Norms for over 26k English Words", "categories": ["cs.CL", "cs.CY"], "comment": "In Proceedings of ACL 2025 Main", "summary": "Social psychologists have shown that Warmth (W) and Competence (C) are the\nprimary dimensions along which we assess other people and groups. These\ndimensions impact various aspects of our lives from social competence and\nemotion regulation to success in the work place and how we view the world. More\nrecent work has started to explore how these dimensions develop, why they have\ndeveloped, and what they constitute. Of particular note, is the finding that\nwarmth has two distinct components: Trust (T) and Sociability (S). In this\nwork, we introduce Words of Warmth, the first large-scale repository of\nmanually derived word--warmth (as well as word--trust and word--sociability)\nassociations for over 26k English words. We show that the associations are\nhighly reliable. We use the lexicons to study the rate at which children\nacquire WCTS words with age. Finally, we show that the lexicon enables a wide\nvariety of bias and stereotype research through case studies on various target\nentities. Words of Warmth is freely available at:\nhttp://saifmohammad.com/warmth.html", "AI": {"tldr": "This paper introduces Words of Warmth, a large-scale repository of manually derived word-warmth associations for over 26k English words. The reliability of the associations is demonstrated, and it is used to study the acquisition rate of WCTS words in children with age. Additionally, the lexicon enables bias and stereotype research.", "motivation": "To understand the development and nature of warmth and its components (Trust and Sociability), and to create a tool for studying bias and stereotypes.", "method": "Manually deriving word-warmth associations for over 26k English words and analyzing their reliability. Using the lexicons to study the acquisition rate of WCTS words in children and enabling bias and stereotype research.", "result": "The creation of Words of Warmth, a reliable lexicon of word-warmth associations, and its application in studying the acquisition rate of WCTS words and enabling bias and stereotype research.", "conclusion": "Words of Warmth provides a valuable resource for understanding the development and nature of warmth and its components, and for conducting bias and stereotype research."}}
{"id": "2506.03994", "pdf": "https://arxiv.org/pdf/2506.03994", "abs": "https://arxiv.org/abs/2506.03994", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "categories": ["cs.CL", "cs.CV"], "comment": "ACL Findings 2025", "summary": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities.", "AI": {"tldr": "This paper examines how well large-scale models trained on extensive data represent semantic features of concrete object concepts by using probing tasks to assess the awareness of object properties.", "motivation": "To investigate the relationship between human learning grounded in sensorimotor experience and state-of-the-art foundation models.", "method": "Using probing tasks to evaluate image encoders trained on image data, multimodal image encoders, and language-only models on predicting extended versions of McRae norms and Binder dataset.", "result": "Multimodal image encoders slightly outperform language-only approaches; image-only encoders perform comparably to language models on non-visual attributes.", "conclusion": "The study provides new insights into unimodal learning and the complementarity of modalities."}}
{"id": "2506.04020", "pdf": "https://arxiv.org/pdf/2506.04020", "abs": "https://arxiv.org/abs/2506.04020", "authors": ["An Quang Tang", "Xiuzhen Zhang", "Minh Ngoc Dinh", "Zhuang Li"], "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering", "categories": ["cs.CL"], "comment": "Paper accepted to ACL 2025 Main Conference", "summary": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM", "AI": {"tldr": "This paper introduces a new task called Quantitative Query-Focused Summarization (QQSUM) which summarizes diverse customer opinions into Key Points (KPs) to answer user queries more effectively. The authors extend Retrieval-Augmented Generation (RAG) to create QQSUM-RAG which uses few-shot learning to jointly train a KP-oriented retriever and a KP summary generator.", "motivation": "Existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions.", "method": "The authors extend RAG to create QQSUM-RAG which employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator.", "result": "Experimental results show that QQSUM-RAG outperforms state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions.", "conclusion": "The proposed method provides a more effective way to answer user queries by summarizing diverse customer opinions into Key Points."}}
{"id": "2506.04032", "pdf": "https://arxiv.org/pdf/2506.04032", "abs": "https://arxiv.org/abs/2506.04032", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "categories": ["cs.CL"], "comment": null, "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.", "AI": {"tldr": "We developed a Patient Simulator using real EHR data to create clinical scenarios and tested its validity against expert clinicians.", "motivation": "To train and test AI health agents using realistic patient simulations.", "method": "Derived patient vignettes from EHR data and evaluated the simulator's performance across over 500 vignettes with multi-turn conversations.", "result": "Clinicians found the simulator consistent with patient vignettes in 97.7% of cases and the extracted case summary 99% relevant.", "conclusion": "The Patient Simulator can be used to train and test multi-turn conversational AI agents at scale."}}
{"id": "2506.04037", "pdf": "https://arxiv.org/pdf/2506.04037", "abs": "https://arxiv.org/abs/2506.04037", "authors": ["Dan Oneata", "Leanne Nortje", "Yevgen Matusevych", "Herman Kamper"], "title": "The mutual exclusivity bias of bilingual visually grounded speech models", "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Mutual exclusivity (ME) is a strategy where a novel word is associated with a\nnovel object rather than a familiar one, facilitating language learning in\nchildren. Recent work has found an ME bias in a visually grounded speech (VGS)\nmodel trained on English speech with paired images. But ME has also been\nstudied in bilingual children, who may employ it less due to cross-lingual\nambiguity. We explore this pattern computationally using bilingual VGS models\ntrained on combinations of English, French, and Dutch. We find that bilingual\nmodels generally exhibit a weaker ME bias than monolingual models, though\nexceptions exist. Analyses show that the combined visual embeddings of\nbilingual models have a smaller variance for familiar data, partly explaining\nthe increase in confusion between novel and familiar concepts. We also provide\nnew insights into why the ME bias exists in VGS models in the first place. Code\nand data: https://github.com/danoneata/me-vgs", "AI": {"tldr": "A study examines mutual exclusivity (ME), a language learning strategy, in bilingual visually grounded speech models trained on English, French, and Dutch. Results indicate a weaker ME bias in bilingual models compared to monolingual ones.", "motivation": "Investigate how mutual exclusivity, which helps children learn language by associating new words with new objects, is affected in bilingual contexts where cross-lingual ambiguity might reduce its use.", "method": "Train bilingual visually grounded speech models on combinations of English, French, and Dutch and analyze their mutual exclusivity biases compared to monolingual models.", "result": "Bilingual models show a weaker ME bias than monolingual models, explained by smaller variance in combined visual embeddings for familiar data leading to more confusion between novel and familiar concepts.", "conclusion": "This computational exploration provides insights into the weaker ME bias in bilingual models and sheds light on the original existence of ME bias in visually grounded speech models."}}
{"id": "2506.04041", "pdf": "https://arxiv.org/pdf/2506.04041", "abs": "https://arxiv.org/abs/2506.04041", "authors": ["Claire Barale", "Leslie Barrett", "Vikram Sunil Bajaj", "Michael Rovatsos"], "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.", "AI": {"tldr": "This paper introduces LexTime, a new dataset for evaluating LLMs' event ordering capabilities in legal language.", "motivation": "Temporal reasoning in legal texts is important for applications like case law analysis and compliance monitoring.", "method": "Introduce LexTime dataset consisting of 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations.", "result": "(1) LLMs are more accurate on legal event ordering than on narrative (up to +10.5%); (2) longer input contexts and implicit events boost accuracy, reaching 80.8% for implicit-explicit event pairs; (3) legal linguistic complexities and nested clauses remain a challenge.", "conclusion": "LexTime is the first dataset designed to evaluate LLMs' event ordering capabilities in legal language."}}
{"id": "2506.04042", "pdf": "https://arxiv.org/pdf/2506.04042", "abs": "https://arxiv.org/abs/2506.04042", "authors": ["Xiyu Liu", "Zhengxiao Liu", "Naibin Gu", "Zheng Lin", "Ji Xiang", "Weiping Wang"], "title": "Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing aims to alternate the target knowledge predicted by large\nlanguage models while ensuring the least side effects on unrelated knowledge.\nAn effective way to achieve knowledge editing is to identify pivotal parameters\nfor predicting factual associations and modify them with an optimization\nprocess to update the predictions. However, these locate-then-edit methods are\nuncontrollable since they tend to modify most unrelated relations connected to\nthe subject of target editing. We unveil that this failure of controllable\nediting is due to a shortcut learning issue during the optimization process.\nSpecifically, we discover two crucial features that are the subject feature and\nthe relation feature for models to learn during optimization, but the current\noptimization process tends to over-learning the subject feature while\nneglecting the relation feature. To eliminate this shortcut learning of the\nsubject feature, we propose a novel two-stage optimization process that\nbalances the learning of the subject feature and the relation feature.\nExperimental results demonstrate that our approach successfully prevents\nknowledge editing from shortcut learning and achieves the optimal overall\nperformance, contributing to controllable knowledge editing.", "AI": {"tldr": "This paper introduces a novel two-stage optimization process for controllable knowledge editing in large language models. The proposed method addresses a shortcut learning issue by balancing the learning of subject and relation features.", "motivation": "To ensure minimal side effects on unrelated knowledge while editing target knowledge in large language models.", "method": "A two-stage optimization process that balances the learning of subject and relation features.", "result": "Experimental results show successful prevention of shortcut learning and optimal overall performance.", "conclusion": "The proposed approach contributes to controllable knowledge editing in large language models."}}
{"id": "2506.04043", "pdf": "https://arxiv.org/pdf/2506.04043", "abs": "https://arxiv.org/abs/2506.04043", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted at ACL WOAH 2025", "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.", "AI": {"tldr": "This study evaluates large language model-generated automated counter-narratives across four dimensions, finding them often verbose and accessible only to those with college-level literacy.", "motivation": "To mitigate online hate speech by addressing concerns about the affective tone, accessibility, and ethical risks of automated counter-narratives generated by large language models.", "method": "Proposing a framework for evaluation and using three different large language models with three prompting strategies on two datasets.", "result": "LLM-generated counter-narratives are often verbose and require college-level literacy, with emotionally guided prompts providing more empathetic responses but still raising safety and effectiveness concerns.", "conclusion": "The study highlights the need for further research into improving the accessibility, safety, and effectiveness of automated counter-narratives."}}
{"id": "2506.04044", "pdf": "https://arxiv.org/pdf/2506.04044", "abs": "https://arxiv.org/abs/2506.04044", "authors": ["Aleksey Kudelya", "Alexander Shirnin"], "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.", "AI": {"tldr": "LIBU: An algorithm combining influence functions and second-order optimization for unlearning tasks in large language models.", "motivation": "To solve the task of unlearning - removing specific knowledge from a large language model without retraining from scratch and compromising its overall utility.", "method": "Combining classical influence functions and second-order optimization.", "result": "This lightweight approach is well applicable for unlearning LLMs in different kinds of task.", "conclusion": "LIBU can effectively remove the influence of specific data from the model and stabilize the overall utility."}}
{"id": "2506.04047", "pdf": "https://arxiv.org/pdf/2506.04047", "abs": "https://arxiv.org/abs/2506.04047", "authors": ["Yuqian Li", "Yupei Du", "Yufang Liu", "Feifei Feng", "Mou Xiao Feng", "Yuanbin Wu"], "title": "On Support Samples of Next Word Prediction", "categories": ["cs.CL"], "comment": "Accepted to ACL2025(Main Conference)", "summary": "Language models excel in various tasks by making complex decisions, yet\nunderstanding the rationale behind these decisions remains a challenge. This\npaper investigates \\emph{data-centric interpretability} in language models,\nfocusing on the next-word prediction task. Using representer theorem, we\nidentify two types of \\emph{support samples}-those that either promote or deter\nspecific predictions. Our findings reveal that being a support sample is an\nintrinsic property, predictable even before training begins. Additionally,\nwhile non-support samples are less influential in direct predictions, they play\na critical role in preventing overfitting and shaping generalization and\nrepresentation learning. Notably, the importance of non-support samples\nincreases in deeper layers, suggesting their significant role in intermediate\nrepresentation formation.These insights shed light on the interplay between\ndata and model decisions, offering a new dimension to understanding language\nmodel behavior and interpretability.", "AI": {"tldr": "This paper studies data-centric interpretability in language models focusing on next-word prediction. It identifies two types of support samples and highlights the importance of non-support samples in preventing overfitting and shaping generalization.", "motivation": "Understanding the rationale behind language model decisions is challenging. This paper aims to investigate data-centric interpretability to shed light on this.", "method": "Using representer theorem, the paper identifies two types of support samples and examines the role of non-support samples.", "result": "Support samples are intrinsic properties predictable before training. Non-support samples play a critical role in preventing overfitting and shaping generalization, especially in deeper layers.", "conclusion": "The study offers a new perspective on understanding language model behavior and interpretability by examining the interaction between data and model decisions."}}
{"id": "2506.04050", "pdf": "https://arxiv.org/pdf/2506.04050", "abs": "https://arxiv.org/abs/2506.04050", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "title": "Explainability-Based Token Replacement on LLM-Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u4f7f\u7528XAI\u65b9\u6cd5\u51cf\u5c11AI\u751f\u6210\u6587\u672c\u7684\u53ef\u68c0\u6d4b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5065\u7684\u96c6\u6210\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793aXAI\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u68c0\u6d4b\u96be\u5ea6\uff0c\u4f46\u9700\u8981\u66f4\u590d\u6742\u7684\u68c0\u6d4b\u7b56\u7565\u6765\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u9690\u85cf\u6280\u672f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6587\u672c\uff0c\u4f46\u5176\u8f93\u51fa\u901a\u5e38\u8868\u73b0\u51fa\u4e00\u4e9b\u6a21\u5f0f\uff0c\u4f7f\u5f97\u5176\u6bd4\u4eba\u7c7b\u4e66\u5199\u7684\u6587\u672c\u66f4\u5bb9\u6613\u88ab\u68c0\u6d4b\u5230\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528XAI\u65b9\u6cd5\u51cf\u5c11AI\u751f\u6210\u6587\u672c\u7684\u53ef\u68c0\u6d4b\u6027\u3002", "method": "\u4f7f\u7528SHAP\u548cLIME\u8bc6\u522b\u5f71\u54cd\u5206\u7c7b\u5668\u9884\u6d4b\u7684\u5173\u952e\u6807\u8bb0\uff0c\u5e76\u63d0\u51fa\u56db\u79cd\u57fa\u4e8e\u89e3\u91ca\u6027\u7684\u6807\u8bb0\u66ff\u6362\u7b56\u7565\u6765\u4fee\u6539\u8fd9\u4e9b\u6709\u5f71\u54cd\u529b\u7684\u5173\u952e\u6807\u8bb0\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6807\u8bb0\u66ff\u6362\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u524a\u5f31\u5355\u4e00\u5206\u7c7b\u5668\u5bf9AI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u662f\u96c6\u6210\u5206\u7c7b\u5668\u5728\u591a\u79cd\u8bed\u8a00\u548c\u9886\u57df\u4e0a\u8868\u73b0\u51fa\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "XAI\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u964d\u4f4eAI\u751f\u6210\u6587\u672c\u7684\u53ef\u68c0\u6d4b\u6027\uff0c\u4f46\u9700\u8981\u7a33\u5065\u7684\u96c6\u6210\u68c0\u6d4b\u7b56\u7565\u6765\u9002\u5e94\u9690\u85cfAI\u751f\u6210\u6587\u672c\u7684\u65b9\u6cd5\u7684\u53d8\u5316\u3002"}}
{"id": "2506.04051", "pdf": "https://arxiv.org/pdf/2506.04051", "abs": "https://arxiv.org/abs/2506.04051", "authors": ["Tim Franzmeyer", "Archie Sravankumar", "Lijuan Liu", "Yuning Mao", "Rui Hou", "Sinong Wang", "Jakob N. Foerster", "Luke Zettlemoyer", "Madian Khabsa"], "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "AI": {"tldr": "Large Language Models (LLMs) can produce incorrect answers (hallucinate). To address this, the paper proposes HALT, which fine-tunes LLMs to be more correct at the cost of completeness.", "motivation": "Hallucination in LLMs leads to incorrect answers, which is undesirable.", "method": "Post-train LLMs using HALT, which generates capability-aligned post-training data by splitting responses into factual fragments and identifying incorrect ones. Responses are then finetuned by removing or replacing incorrect fragments with 'Unsure from Here' based on a tunable threshold.", "result": "HALT improves the mean correctness of response fragments by 15% on average and increases the F1 score by 4%. A single reliable Llama3-70B model achieves 87% correctness across four domains while maintaining 53% of the standard finetuning response completeness.", "conclusion": "HALT effectively trades off response completeness for correctness, leading to more reliable LLMs."}}
{"id": "2506.04065", "pdf": "https://arxiv.org/pdf/2506.04065", "abs": "https://arxiv.org/abs/2506.04065", "authors": ["Muling Wu", "Qi Qian", "Wenhao Liu", "Xiaohua Wang", "Zisu Huang", "Di Liang", "LI Miao", "Shihan Dou", "Changze Lv", "Zhenghua Wang", "Zhibo Xu", "Lina Chen", "Tianlong Li", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.", "AI": {"tldr": "Customized Curriculum Learning (CCL) improves large language models' performance on mathematical reasoning benchmarks by optimizing sample difficulty and utilization.", "motivation": "Current post-training methods for LLMs suffer from inefficient sample usage and rigid difficulty handling.", "method": "Introduces model-adaptive difficulty definition and Guided Prompting to customize curricula and hint samples.", "result": "CCL outperforms uniform training across five mathematical reasoning benchmarks.", "conclusion": "CCL enhances sample utilization and model performance across different training paradigms."}}
{"id": "2506.04070", "pdf": "https://arxiv.org/pdf/2506.04070", "abs": "https://arxiv.org/abs/2506.04070", "authors": ["Yi Zhao", "Siqi Wang", "Jing Li"], "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.", "AI": {"tldr": "This paper presents LaF-GRPO, a method using LLMs to simulate visually impaired user responses for generating navigation instructions, which improves instruction quality and reduces reliance on real-world data. The paper also introduces NIG4VI, a benchmark dataset for evaluating such systems.", "motivation": "To develop precise, practical navigation instructions for visually impaired individuals due to the lack of effective solutions.", "method": "Proposes LaF-GRPO where an LLM generates rewards to guide VLM post-training; introduces NIG4VI as a benchmark dataset.", "result": "LaF-GRPO shows improved performance in generating navigation instructions compared to other methods, with better quantitative metrics like BLEU and METEOR scores.", "conclusion": "The proposed method effectively enhances the usability of navigation instructions for visually impaired users and reduces the need for real-world data."}}
{"id": "2506.04072", "pdf": "https://arxiv.org/pdf/2506.04072", "abs": "https://arxiv.org/abs/2506.04072", "authors": ["Meiqing Jin", "Liam Dugan", "Chris Callison-Burch"], "title": "Controlling Difficulty of Generated Text for AI-Assisted Language Learning", "categories": ["cs.CL", "cs.HC", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.", "AI": {"tldr": "\u8fd9\u9879\u5de5\u4f5c\u63a2\u7d22\u4e86\u5982\u4f55\u4f7f\u7528\u53ef\u63a7\u751f\u6210\u6280\u672f\u6765\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u7edd\u5bf9\u521d\u5b66\u8005\u7684\u8bed\u8a00\u5b66\u4e60\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u672a\u6765\u5224\u522b\u5668\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8f93\u51fa\u7684\u53ef\u7406\u89e3\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807Token Miss Rate (TMR)\u3002", "motivation": "\u5b9e\u8df5\u5bf9\u8bdd\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u63d0\u4f9b\u4e86\u4f20\u7edf\u9762\u5bf9\u9762\u8bed\u8a00\u5b66\u4e60\u7684\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5927\u591a\u6570LLMs\u751f\u6210\u7684\u6587\u672c\u5728\u63a5\u8fd1\u672c\u5730\u590d\u6742\u5ea6\u7684\u6c34\u5e73\u4e0a\uff0c\u4f7f\u5b83\u4eec\u4e0d\u9002\u5408\u521d\u5b66\u8005\u5b66\u4e60\u8005(CEFR: A1-A2)\u3002", "method": "\u7814\u7a76\u4e86\u53ef\u63a7\u751f\u6210\u6280\u672f\uff0c\u7279\u522b\u662f\u4e0d\u9700\u8981\u6a21\u578b\u5fae\u8c03\u7684\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u662f\u5426\u53ef\u4ee5\u9002\u5e94LLM\u8f93\u51fa\u4ee5\u66f4\u597d\u5730\u652f\u6301\u7edd\u5bf9\u521d\u5b66\u8005\u3002\u901a\u8fc7\u81ea\u52a8\u5ea6\u91cf\u548c\u5927\u5b66\u65e5\u8bed\u5b66\u4e60\u8005\u7684\u7528\u6237\u7814\u7a76\u6765\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u3002", "result": "\u867d\u7136\u5355\u72ec\u4f7f\u7528\u63d0\u793a\u65e0\u6cd5\u63a7\u5236\u8f93\u51fa\u96be\u5ea6\uff0c\u4f46\u4f7f\u7528\u672a\u6765\u5224\u522b\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u8f93\u51fa\u7684\u53ef\u7406\u89e3\u6027\uff08\u4ece40.4\uff05\u63d0\u9ad8\u523084.3\uff05\uff09\u3002", "conclusion": "\u4f7f\u7528\u672a\u6765\u5224\u522b\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u8f93\u51fa\u7684\u53ef\u7406\u89e3\u6027\uff0c\u5e76\u4e14\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684Token Miss Rate (TMR)\u8bc4\u4f30\u6307\u6807\uff0c\u8be5\u6307\u6807\u91cf\u5316\u4e86\u6bcf\u4e2a\u8bdd\u8bed\u4e2d\u4e0d\u53ef\u7406\u89e3\u7684\u6807\u8bb0\u6bd4\u4f8b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u5bc6\u5207\u76f8\u5173\u3002\u4e3a\u4e86\u652f\u6301AI\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u7684\u672a\u6765\u7814\u7a76\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u6211\u4eec\u7684\u4ee3\u7801\u3001\u6a21\u578b\u3001\u6ce8\u91ca\u5de5\u5177\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2506.04076", "pdf": "https://arxiv.org/pdf/2506.04076", "abs": "https://arxiv.org/abs/2506.04076", "authors": ["Jhen-Ke Lin", "Hao-Chien Lu", "Chung-Chun Wang", "Hong-Yun Lin", "Berlin Chen"], "title": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription.", "AI": {"tldr": "This paper investigates the impact of different annotation schemes on the performance of ASR systems for transcribing disfluent speech.", "motivation": "ASR systems often discard or generalize hesitations, losing important acoustic details which are crucial for downstream tasks like error analysis and feedback.", "method": "Fine-tuning Whisper models on the Speak & Improve 2025 corpus using low-rank adaptation (LoRA), comparing three annotation schemes: Pure, Rich, and Extra.", "result": "The 'Extra' scheme, which uses acoustically precise fillers inferred by Gemini 2.0 Flash, yielded a 5.5% WER, an 11.3% relative improvement over the 'Pure' scheme (6.2% WER).", "conclusion": "Explicit, realistic filled-pause labeling significantly enhances ASR accuracy for verbatim L2 speech transcription."}}
{"id": "2506.04077", "pdf": "https://arxiv.org/pdf/2506.04077", "abs": "https://arxiv.org/abs/2506.04077", "authors": ["Chung-Chun Wang", "Jhen-Ke Lin", "Hao-Chien Lu", "Hong-Yun Lin", "Berlin Chen"], "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u54cd\u5e94\u5e76\u7ed3\u5408\u52a8\u6001\u91cd\u8981\u6027\u635f\u5931\u7684\u65b0\u578b\u8bad\u7ec3\u8303\u5f0f\u6765\u89e3\u51b3\u81ea\u52a8\u5316\u53e3\u8bed\u8bc4\u4f30\u4e2d\u7684\u6807\u6ce8\u5f55\u97f3\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u5316\u53e3\u8bed\u8bc4\u4f30\u5728\u8868\u8fbe\u610f\u89c1\u65f6\u53d7\u9650\u4e8e\u6807\u6ce8\u5f55\u97f3\u7684\u7a00\u7f3a\u6027\uff0c\u5f71\u54cd\u4e86\u8bc4\u5206\u7684\u53ef\u9760\u6027\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6307\u5b9a\u719f\u7ec3\u5ea6\u7684\u591a\u6837\u5316\u54cd\u5e94\uff0c\u901a\u8fc7\u4e0e\u8bed\u97f3\u5408\u6210\u5668\u7ed3\u5408\u8f6c\u6362\u4e3a\u5408\u6210\u8bed\u97f3\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u91cd\u8981\u6027\u635f\u5931\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u5b9e\u4f8b\uff0c\u6700\u540e\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6574\u5408\u6587\u672c\u7279\u5f81\u548c\u8bed\u97f3\u4fe1\u53f7\u9884\u6d4b\u719f\u7ec3\u5ea6\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u6216\u5e38\u89c4\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4f4e\u8d44\u6e90\u7ea6\u675f\u95ee\u9898\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u8de8\u6a21\u6001\u4fe1\u606f\u7684\u81ea\u52a8\u5316\u53e3\u8bed\u8bc4\u4f30\u3002"}}
{"id": "2506.04078", "pdf": "https://arxiv.org/pdf/2506.04078", "abs": "https://arxiv.org/abs/2506.04078", "authors": ["Ming Zhang", "Yujiong Shen", "Zelin Li", "Huayu Sha", "Binze Hu", "Yuhui Wang", "Chenhao Huang", "Shichun Liu", "Jingqi Tong", "Changhao Jiang", "Mingxu Chai", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.", "AI": {"tldr": "\u63d0\u51faLLMEval-Med\uff0c\u4e00\u4e2a\u5305\u542b2996\u4e2a\u95ee\u9898\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u533b\u5b66\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u57fa\u51c6\u5728\u95ee\u9898\u8bbe\u8ba1\u3001\u6570\u636e\u6765\u6e90\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u533b\u5b66\u9886\u57df\u76842,996\u4e2a\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u7ba1\u9053\uff0c\u5c06\u4e13\u5bb6\u5f00\u53d1\u7684\u6e05\u5355\u7eb3\u5165LLM-as-Judge\u6846\u67b6\u4e2d\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4eba\u673a\u4e00\u81f4\u6027\u5206\u6790\u9a8c\u8bc1\u673a\u5668\u8bc4\u5206\uff0c\u5e76\u6839\u636e\u4e13\u5bb6\u53cd\u9988\u52a8\u6001\u8c03\u6574\u6e05\u5355\u548c\u63d0\u793a\u4ee5\u786e\u4fdd\u53ef\u9760\u6027\u3002", "result": "LLMEval-Med\u6db5\u76d6\u4e86\u4e94\u4e2a\u6838\u5fc3\u533b\u5b66\u9886\u57df\uff0c\u5305\u62ec2,996\u4e2a\u6765\u81ea\u771f\u5b9e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u4e13\u5bb6\u8bbe\u8ba1\u4e34\u5e8a\u573a\u666f\u7684\u95ee\u9898\u3002\u5b83\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u7ba1\u9053\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u4e2a\u7c7b\u522b\u4e2d\u768413\u4e2aLLMs\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u57fa\u51c6LLMEval-Med\u4e3a\u533b\u5b66\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u548c\u6709\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.04079", "pdf": "https://arxiv.org/pdf/2506.04079", "abs": "https://arxiv.org/abs/2506.04079", "authors": ["Pedro Henrique Martins", "Jo\u00e3o Alves", "Patrick Fernandes", "Nuno M. Guerreiro", "Ricardo Rei", "Amin Farajian", "Mateusz Klimaszewski", "Duarte M. Alves", "Jos\u00e9 Pombal", "Manuel Faysse", "Pierre Colombo", "Fran\u00e7ois Yvon", "Barry Haddow", "Jos\u00e9 G. C. de Souza", "Alexandra Birch", "Andr\u00e9 F. T. Martins"], "title": "EuroLLM-9B: Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "56 pages", "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.", "AI": {"tldr": "EuroLLM-9B is a large language model trained from scratch to support European citizens with 35 languages. It addresses the lack of representation for European languages in open large language models.", "motivation": "To address the underrepresentation and underserved status of European languages in existing open large language models.", "method": "Trained from scratch with a focus on European languages, including tokenizer design, architectural specifications, data filtering, and training procedures. Developed EuroFilter for data filtering and EuroBlocks-Synthetic for post-training.", "result": "Competitive performance on multilingual benchmarks and machine translation tasks, established as the leading open European-made LLM of its size.", "conclusion": "EuroLLM-9B is a significant advancement in supporting European languages in large language models and is released openly to support research and adoption."}}
{"id": "2506.04098", "pdf": "https://arxiv.org/pdf/2506.04098", "abs": "https://arxiv.org/abs/2506.04098", "authors": ["Wenhao Li", "Wenwu Li", "Chuyun Shen", "Junjie Sheng", "Zixiao Huang", "Di Wu", "Yun Hua", "Wei Yin", "Xiangfeng Wang", "Hongyuan Zha", "Bo Jin"], "title": "TextAtari: 100K Frames Game Playing with Language Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "51 pages, 39 figures", "summary": "We present TextAtari, a benchmark for evaluating language agents on very\nlong-horizon decision-making tasks spanning up to 100,000 steps. By translating\nthe visual state representations of classic Atari games into rich textual\ndescriptions, TextAtari creates a challenging test bed that bridges sequential\ndecision-making with natural language processing. The benchmark includes nearly\n100 distinct tasks with varying complexity, action spaces, and planning\nhorizons, all rendered as text through an unsupervised representation learning\nframework (AtariARI). We evaluate three open-source large language models\n(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks\n(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how\ndifferent forms of prior knowledge affect performance on these long-horizon\nchallenges. Four scenarios-Basic, Obscured, Manual Augmentation, and\nReference-based-investigate the impact of semantic understanding, instruction\ncomprehension, and expert demonstrations on agent decision-making. Our results\nreveal significant performance gaps between language agents and human players\nin extensive planning tasks, highlighting challenges in sequential reasoning,\nstate tracking, and strategic planning across tens of thousands of steps.\nTextAtari provides standardized evaluation protocols, baseline implementations,\nand a framework for advancing research at the intersection of language models\nand planning.", "AI": {"tldr": "TextAtari is a benchmark for evaluating language agents on very long-horizon decision-making tasks.", "motivation": "To create a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps.", "method": "An unsupervised representation learning framework (AtariARI) was used to translate visual state representations of classic Atari games into rich textual descriptions, creating a challenging test bed for sequential decision-making with natural language processing.", "result": "The results reveal significant performance gaps between language agents and human players in extensive planning tasks.", "conclusion": "TextAtari highlights the challenges faced by language agents in long-horizon decision-making tasks."}}
{"id": "2506.04108", "pdf": "https://arxiv.org/pdf/2506.04108", "abs": "https://arxiv.org/abs/2506.04108", "authors": ["Yutao Sun", "Tianzhu Ye", "Li Dong", "Yuqing Xia", "Jian Chen", "Yizhao Gao", "Shijie Cao", "Jianyong Wang", "Furu Wei"], "title": "Rectified Sparse Attention", "categories": ["cs.CL"], "comment": null, "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.", "AI": {"tldr": "Introduces ReSA for efficient long-sequence generation in large language models.", "motivation": "To address the issue of KV cache misalignment in recent sparse decoding methods.", "method": "Rectified Sparse Attention (ReSA), which combines block-sparse attention with periodic dense rectification.", "result": "ReSA achieves significant efficiency improvement with high generation quality.", "conclusion": "ReSA improves the efficiency of long-sequence generation with near-lossless quality."}}
{"id": "2506.04131", "pdf": "https://arxiv.org/pdf/2506.04131", "abs": "https://arxiv.org/abs/2506.04131", "authors": ["Disha Sheshanarayana", "Tanishka Magar", "Ayushi Mittal", "Neelam Chaplot"], "title": "CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to SICon 2025 ACL", "summary": "Courtrooms are places where lives are determined and fates are sealed, yet\nthey are not impervious to manipulation. Strategic use of manipulation in legal\njargon can sway the opinions of judges and affect the decisions. Despite the\ngrowing advancements in NLP, its application in detecting and analyzing\nmanipulation within the legal domain remains largely unexplored. Our work\naddresses this gap by introducing LegalCon, a dataset of 1,063 annotated\ncourtroom conversations labeled for manipulation detection, identification of\nprimary manipulators, and classification of manipulative techniques, with a\nfocus on long conversations. Furthermore, we propose CLAIM, a two-stage,\nIntent-driven Multi-agent framework designed to enhance manipulation analysis\nby enabling context-aware and informed decision-making. Our results highlight\nthe potential of incorporating agentic frameworks to improve fairness and\ntransparency in judicial processes. We hope that this contributes to the\nbroader application of NLP in legal discourse analysis and the development of\nrobust tools to support fairness in legal decision-making. Our code and data\nare available at https://github.com/Disha1001/CLAIM.", "AI": {"tldr": "This paper presents LegalCon, a dataset for manipulation detection in courtroom conversations, and CLAIM, a framework to improve fairness and transparency in judicial processes through enhanced manipulation analysis.", "motivation": "To address the gap in using NLP for detecting and analyzing manipulation within the legal domain", "method": "Introducing LegalCon dataset and CLAIM framework", "result": "A dataset of 1,063 annotated courtroom conversations and a two-stage Intent-driven Multi-agent framework called CLAIM", "conclusion": "The paper introduces LegalCon, a dataset for manipulation detection in courtroom conversations, and CLAIM, a framework for enhancing such analysis."}}
{"id": "2506.04139", "pdf": "https://arxiv.org/pdf/2506.04139", "abs": "https://arxiv.org/abs/2506.04139", "authors": ["Ratna Kandala", "Katie Hoemann"], "title": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86LLMs\u5728\u6355\u6349\u5f17\u62c9\u8292\u65e5\u5e38\u53d9\u8ff0\u60c5\u611f\u503c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u8377\u5170\u8bed\u8c03\u6574\u6a21\u578b\u8868\u73b0\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u6587\u5316\u4e0e\u8bed\u8a00\u5b9a\u5236\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7406\u89e3\u65e5\u5e38\u8bed\u8a00\u4e2d\u7684\u7ec6\u5fae\u5dee\u522b\u5bf9\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u60c5\u7eea\u7814\u7a76\u7684\u8fdb\u6b65\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u65e5\u5e38\u8bed\u8a00\u672c\u8d28\u4e0a\u662f\u81ea\u53d1\u7684\u3001\u4e30\u5bcc\u7684\u8868\u8fbe\u6027\u548c\u6df1\u5ea6\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u3002", "method": "\u7814\u7a76\u5305\u62ec\u7ea625,000\u4e2a\u6587\u672c\u54cd\u5e94\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cdDutch-specific LLMs\u9884\u6d4b\u60c5\u611f\u503c\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8f93\u51fa\u4e0eLIWC\u548cPattern\u751f\u6210\u7684\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u4e09\u79cdDutch-specific LLMs\u5728\u9884\u6d4b\u60c5\u611f\u503c\u65b9\u9762\u8868\u73b0\u4e0d\u5982LIWC\u548cPattern\uff0c\u8fd9\u8868\u660e\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u6765\u5904\u7406\u65e5\u5e38\u53d9\u8ff0\u7684\u60c5\u611f\u5206\u6790\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u67b6\u6784\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u5f53\u524d\u8377\u5170\u8bed\u8c03\u6574\u6a21\u578b\u5728\u51c6\u786e\u6355\u6349\u81ea\u53d1\u771f\u5b9e\u4e16\u754c\u53d9\u8ff0\u4e2d\u7684\u60c5\u611f\u503c\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u6587\u5316\u4e0e\u8bed\u8a00\u5b9a\u5236\u7684\u6a21\u578b/\u5de5\u5177\u6765\u5904\u7406\u81ea\u7136\u8bed\u8a00\u4f7f\u7528\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2506.04142", "pdf": "https://arxiv.org/pdf/2506.04142", "abs": "https://arxiv.org/abs/2506.04142", "authors": ["Kejian Zhu", "Shangqing Tu", "Zhuoran Jin", "Lei Hou", "Juanzi Li", "Jun Zhao"], "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation", "AI": {"tldr": "This paper proposes a novel method for identifying shortcut neurons in large language models to mitigate contamination issues in evaluations.", "motivation": "Current evaluations rely on public benchmarks which are prone to data contamination issues compromising fairness. Building new benchmarks is costly and cyclical.", "method": "Analyzing mechanisms of contaminated models themselves and proposing a method for identifying shortcut neurons through comparative and causal analysis. Introducing an evaluation method called shortcut neuron patching to suppress shortcut neurons.", "result": "Experiments validate the effectiveness of the approach in mitigating contamination. Evaluation results show a strong linear correlation with MixEval, a trustworthy benchmark.", "conclusion": "The proposed method effectively reveals true capabilities of the models and is trustworthy. Further experiments demonstrate the generalizability of the method across various benchmarks and hyperparameter settings."}}
{"id": "2506.04156", "pdf": "https://arxiv.org/pdf/2506.04156", "abs": "https://arxiv.org/abs/2506.04156", "authors": ["Sarvesh Soni", "Dina Demner-Fushman"], "title": "A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization", "categories": ["cs.CL"], "comment": null, "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u60a3\u8005\u6848\u4f8b\u7684\u4e13\u5bb6\u6ce8\u91ca\u6570\u636e\u96c6ArchEHR-QA\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540cLLMs\u5728EHR\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u63d0\u9ad8\u4e34\u5e8a\u54cd\u5e94\u4e8b\u5b9e\u6027\u548c\u76f8\u5173\u6027\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u96c6\u65e0\u6cd5\u6355\u6349\u60a3\u8005\u5728EHR\u80cc\u666f\u4e0b\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30AI\u751f\u6210\u54cd\u5e94\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u7684\u9c81\u68d2\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165\u4e86ArchEHR-QA\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u5f00\u653e\u6743\u91cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "Llama 4\u5728\u56de\u7b54\u4f18\u5148\u63d0\u793a\u65b9\u6cd5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u5206\u6570\u3002\u624b\u52a8\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u5e38\u89c1\u7684\u95ee\u9898\uff0c\u5982\u9057\u6f0f\u5173\u952e\u4e34\u5e8a\u8bc1\u636e\u548c\u77db\u76fe\u6216\u5e7b\u89c9\u5185\u5bb9\u3002", "conclusion": "ArchEHR-QA\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684EHR\u95ee\u7b54\u7cfb\u7edf\uff0c\u5f3a\u8c03\u4e86\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u751f\u6210\u4e8b\u5b9e\u6027\u548c\u76f8\u5173\u6027\u54cd\u5e94\u7684\u8fdb\u4e00\u6b65\u8fdb\u5c55\u9700\u6c42\u3002"}}
{"id": "2506.04179", "pdf": "https://arxiv.org/pdf/2506.04179", "abs": "https://arxiv.org/abs/2506.04179", "authors": ["Anhao Zhao", "Fanghua Ye", "Yingqi Fan", "Junlong Tong", "Zhiwei Fei", "Hui Su", "Xiaoyu Shen"], "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT.", "AI": {"tldr": "SkipGPT\u662f\u4e00\u79cd\u52a8\u6001\u5c42\u4fee\u526a\u6846\u67b6\uff0c\u901a\u8fc7token\u611f\u77e5\u8def\u7531\u548c\u7ec4\u4ef6\u7279\u5b9a\u7b56\u7565\u51cf\u5c1140%\u4ee5\u4e0a\u7684\u6a21\u578b\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u6001\u4fee\u526a\u65b9\u6cd5\u5ffd\u7565\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u4e24\u4e2a\u5173\u952e\u7684\u52a8\u6001\u7279\u6027\uff1a\u6c34\u5e73\u52a8\u6001\uff08token\u7ea7\u522b\u7684\u5f02\u8d28\u6027\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4fee\u526a\u51b3\u7b56\uff09\u548c\u5782\u76f4\u52a8\u6001\uff08MLP\u548c\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u4e0d\u540c\u529f\u80fd\u89d2\u8272\u9700\u8981\u7279\u5b9a\u4e8e\u7ec4\u4ef6\u7684\u4fee\u526a\u7b56\u7565\uff09\u3002", "method": "SkipGPT\u5f15\u5165\u4e86\u5168\u5c40token\u611f\u77e5\u8def\u7531\u6765\u4f18\u5148\u5904\u7406\u5173\u952etoken\uff0c\u5e76\u4e3aMLP\u548c\u81ea\u6ce8\u610f\u529b\u7ec4\u4ef6\u63d0\u4f9b\u4e86\u72ec\u7acb\u7684\u4fee\u526a\u7b56\u7565\u3002\u4e3a\u4e86\u51cf\u8f7b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u8303\u5f0f\uff1a\u9996\u5148\u901a\u8fc7\u8f6f\u53c2\u6570\u5316\u5b66\u4e60\u8def\u7531\u7b56\u7565\u4ee5\u907f\u514d\u8fc7\u65e9\u7684\u4fee\u526a\u51b3\u7b56\uff0c\u7136\u540e\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684LoRA\u5fae\u8c03\u6062\u590d\u56e0\u5c42\u79fb\u9664\u800c\u53d7\u5230\u5f71\u54cd\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSkipGPT\u51cf\u5c11\u4e86\u8d85\u8fc740%\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u539f\u59cb\u5bc6\u96c6\u6a21\u578b\u7684\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002", "conclusion": "SkipGPT\u901a\u8fc7\u52a8\u6001\u5c42\u4fee\u526a\u6846\u67b6\u4f18\u5316\u4e86\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u5e76\u5728\u51cf\u5c11\u8d85\u8fc740%\u6a21\u578b\u53c2\u6570\u7684\u540c\u65f6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u539f\u59cb\u5bc6\u96c6\u6a21\u578b\u7684\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u53ef\u6269\u5c55\u3001\u8d44\u6e90\u611f\u77e5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.04180", "pdf": "https://arxiv.org/pdf/2506.04180", "abs": "https://arxiv.org/abs/2506.04180", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Juanzi Li", "Roy Ka-Wei Lee"], "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.", "AI": {"tldr": "Propose SuperWriter-Agent, an agent-based framework that improves long-form text generation by introducing planning and refinement stages. Train a 7B parameter SuperWriter-LM using supervised fine-tuning and hierarchical DPO with MCTS. Achieve state-of-the-art performance in automatic and human evaluations.", "motivation": "Maintaining coherence, logical consistency, and text quality in long-form text generation is challenging for large language models.", "method": "Introduce SuperWriter-Agent with planning and refinement stages. Construct a supervised fine-tuning dataset for SuperWriter-LM and develop a hierarchical DPO procedure with MCTS.", "result": "SuperWriter-LM outperforms larger baseline models in diverse benchmarks based on automatic and human evaluations.", "conclusion": "Incorporating structured thinking steps through SuperWriter-Agent enhances the quality of long-form text generation."}}
{"id": "2506.04182", "pdf": "https://arxiv.org/pdf/2506.04182", "abs": "https://arxiv.org/abs/2506.04182", "authors": ["Ruiqi Zhang", "Changyi Xiao", "Yixin Cao"], "title": "Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of large reasoning models, long Chain-of-Thought\n(CoT) prompting has demonstrated strong performance on complex tasks. However,\nthis often comes with a significant increase in token usage. In this paper, we\nconduct a comprehensive empirical analysis comparing long and short CoT\nstrategies. Our findings reveal that while long CoT can lead to performance\nimprovements, its benefits are often marginal relative to its significantly\nhigher token consumption. Specifically, long CoT tends to outperform when ample\ngeneration budgets are available, whereas short CoT is more effective under\ntighter budget constraints. These insights underscore the need for a dynamic\napproach that selects the proper CoT strategy based on task context and\nresource availability. To address this, we propose SwitchCoT, an automatic\nframework that adaptively chooses between long and short CoT strategies to\nbalance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is\ndesigned to be budget-aware, making it broadly applicable across scenarios with\nvarying resource constraints. Experimental results demonstrate that SwitchCoT\ncan reduce inference costs by up to 50% while maintaining high accuracy.\nNotably, under limited token budgets, it achieves performance comparable to, or\neven exceeding, that of using either long or short CoT alone.", "AI": {"tldr": "Compare the effectiveness of long and short Chain-of-Thought (CoT) prompting strategies in large reasoning models.", "motivation": "Long CoT prompting has shown strong performance but at the cost of higher token usage. Short CoT could be more efficient under tight budget constraints.", "method": "Conduct a comprehensive empirical analysis comparing long and short CoT strategies.", "result": "SwitchCoT, a framework that adaptively chooses between long and short CoT strategies, reduces inference costs by up to 50% while maintaining high accuracy.", "conclusion": "SwitchCoT is a budget-aware framework that balances reasoning accuracy and computational efficiency."}}
{"id": "2506.04185", "pdf": "https://arxiv.org/pdf/2506.04185", "abs": "https://arxiv.org/abs/2506.04185", "authors": ["Qingfei Zhao", "Ruobing Wang", "Dingling Xu", "Daren Zha", "Limin Liu"], "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning", "categories": ["cs.CL"], "comment": "16 pages, 3 figures", "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.", "AI": {"tldr": "R-Search is a new reinforcement learning framework that helps large language models improve their reasoning abilities by integrating deep search interactions, leading to better responses in complex tasks.", "motivation": "To address the challenge of enabling large language models to effectively integrate deep search interactions for improved reasoning capabilities.", "method": "Introducing R-Search, which uses multi-reward signals to guide LLMs in deciding when to retrieve or reason and integrates key evidence to enhance knowledge interaction between reasoning and search.", "result": "R-Search outperformed advanced RAG baselines by up to 32.2% in-domain and 25.1% out-of-domain across seven datasets.", "conclusion": "The proposed method shows significant improvement in the quality of responses generated by LLMs in complex logic- and knowledge-intensive tasks."}}
{"id": "2506.04226", "pdf": "https://arxiv.org/pdf/2506.04226", "abs": "https://arxiv.org/abs/2506.04226", "authors": ["Akshat Gupta", "Maochuan Lu", "Thomas Hartvigsen", "Gopala Anumanchipalli"], "title": "Efficient Knowledge Editing via Minimal Precomputation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Knowledge editing methods like MEMIT are able to make data and compute\nefficient updates of factual knowledge by using a single sentence to update\nfacts and their consequences. However, what is often overlooked is a\n\"precomputation step\", which requires a one-time but significant computational\ncost. The authors of MEMIT originally precompute approximately 44 million\nhidden vectors per edited layer, which requires a forward pass over 44 million\ntokens. For GPT-J (6B), this precomputation step takes 36 hours on a single\nGPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this\nprecomputation time grows with model size. In this paper, we show that this\nexcessive computational cost is unnecessary. Knowledge editing using MEMIT and\nrelated methods, such as ROME and EMMET, can be performed by pre-computing a\nvery small portion of the 44 million hidden vectors. We first present the\ntheoretical minimum number of hidden vector precomputation required for\nsolutions of these editing methods to exist. We then empirically show that\nknowledge editing using these methods can be done by pre-computing\nsignificantly fewer hidden vectors. Specifically, we show that the\nprecomputation step can be done with less than 0.3% of the originally\nstipulated number of hidden vectors. This saves a significant amount of\nprecomputation time and allows users to begin editing new models within a few\nminutes.", "AI": {"tldr": "This paper shows that the excessive computational cost of knowledge editing methods like MEMIT, ROME, and EMMET is unnecessary. By precomputing only a small fraction of the originally stipulated hidden vectors, users can significantly reduce precomputation time.", "motivation": "To address the significant one-time computational cost associated with precomputing hidden vectors for knowledge editing methods.", "method": "Theoretical analysis and empirical experiments to determine the minimum number of hidden vectors needed for knowledge editing and demonstrate that it can be done with much fewer vectors.", "result": "Reduction in precomputation time by more than 99.7%, allowing users to start editing new models within minutes instead of hours.", "conclusion": "Knowledge editing methods can be made much more efficient by precomputing far fewer hidden vectors, making them more accessible and practical for wider use."}}
{"id": "2505.24073", "pdf": "https://arxiv.org/pdf/2505.24073", "abs": "https://arxiv.org/abs/2505.24073", "authors": ["Chan-Wei Hu", "Yueqi Wang", "Shuo Xing", "Chia-Ju Chen", "Zhengzhong Tu"], "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning.", "AI": {"tldr": "This paper systematically analyzes the Retrieval-Augmented Generation (RAG) pipeline for large vision-language models (LVLMs), focusing on retrieval strategies, re-ranking methods to reduce biases, and integrating retrieved information into generation processes. It also proposes a self-reflective framework enhancing model performance.", "motivation": "To address limitations of LVLMs like susceptibility to hallucinations and lack of real-time evidence verification, using RAG to enhance their performance in dynamic real-world applications.", "method": "Systematically dissecting the RAG pipeline including investigating modality configurations, re-ranking strategies, and integration approaches into the generation phase, plus proposing a unified agentic framework for self-reflection.", "result": "Achieved an average performance boost of 5% across various tasks without additional fine-tuning.", "conclusion": "The study provides valuable insights into optimizing RAG for LVLMs, enhancing their ability to ground outputs in factual and contextually relevant information."}}
{"id": "2506.03197", "pdf": "https://arxiv.org/pdf/2506.03197", "abs": "https://arxiv.org/abs/2506.03197", "authors": ["Baode Wang", "Biao Wu", "Weizhen Li", "Meng Fang", "Yanjie Liang", "Zuming Huang", "Haozhe Wang", "Jun Huang", "Ling Chen", "Wei Chu", "Yuan Qi"], "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 12 figures", "summary": "Automated parsing of scanned documents into richly structured,\nmachine-readable formats remains a critical bottleneck in Document AI, as\ntraditional multi-stage pipelines suffer from error propagation and limited\nadaptability to diverse layouts. We introduce layoutRL, an end-to-end\nreinforcement learning framework that trains models to be explicitly\nlayout-aware by optimizing a composite reward of normalized edit distance,\nparagraph count accuracy, and reading order preservation. Leveraging our newly\nreleased dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic\nscanned document parsing data with expert-filtered real-world documents, we\ninstantiate layoutRL in a vision-language-model-based parser called\nInfinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and\nformula extraction, and reading order detection, Infinity-Parser achieves new\nstate-of-the-art performance in both accuracy and structural fidelity,\noutpacing specialist pipelines and general-purpose vision-language models. We\nwill publicly release our code and dataset to accelerate progress in robust\ndocument understanding.", "AI": {"tldr": "Introducing layoutRL, a novel end-to-end reinforcement learning framework for parsing scanned documents, which achieves state-of-the-art performance on various benchmarks.", "motivation": "Traditional multi-stage pipelines for Document AI suffer from error propagation and limited adaptability to diverse layouts.", "method": "LayoutRL optimizes a composite reward of normalized edit distance, paragraph count accuracy, and reading order preservation.", "result": "Infinity-Parser, based on layoutRL, achieves new state-of-the-art performance in OCR, table and formula extraction, and reading order detection on English and Chinese benchmarks.", "conclusion": "The proposed layoutRL framework and the released dataset and code can significantly advance robust document understanding."}}
{"id": "2506.03230", "pdf": "https://arxiv.org/pdf/2506.03230", "abs": "https://arxiv.org/abs/2506.03230", "authors": ["Selcuk Gurses", "Aozhong Zhang", "Yanxia Deng", "Xun Dong", "Xin Li", "Naigang Wang", "Penghang Yin", "Zi Yang"], "title": "DiaBlo: Diagonal Blocks Are Sufficient For Finetuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "comment": null, "summary": "Finetuning is a critical step for adapting large language models (LLMs) to\ndomain-specific downstream tasks. To mitigate the substantial computational and\nmemory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)\nmethods have been proposed to update only a small subset of model parameters.\nHowever, performance gaps between PEFT approaches and full-model fine-tuning\nstill exist. In this work, we present DiaBlo, a simple yet effective PEFT\napproach that updates only the diagonal blocks of selected model weight\nmatrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates\nthe need for low rank matrix products, thereby avoiding the reliance on\nauxiliary initialization schemes or customized optimization strategies to\nimprove convergence. This design leads to stable and robust convergence while\nmaintaining comparable memory efficiency and training speed to LoRA. We conduct\nextensive experiments across a range of tasks, including commonsense reasoning,\narithmetic reasoning, code generation, and safety alignment, to evaluate the\neffectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo\ndemonstrates strong and consistent performance while maintaining high memory\nefficiency and fast finetuning speed. Codes are available at\nhttps://github.com/ziyangjoy/DiaBlo.", "AI": {"tldr": "DiaBlo\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u66f4\u65b0\u9009\u5b9a\u6a21\u578b\u6743\u91cd\u77e9\u9635\u7684\u5bf9\u89d2\u5757\u6765\u9002\u5e94\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5230\u7279\u5b9a\u9886\u57df\u7684\u4e0b\u6e38\u4efb\u52a1\u3002\u5b83\u5728\u6027\u80fd\u3001\u5185\u5b58\u6548\u7387\u548c\u8bad\u7ec3\u901f\u5ea6\u4e0a\u4e0e\u5168\u6a21\u578b\u5fae\u8c03\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u65f6\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "method": "DiaBlo\u53ea\u66f4\u65b0\u9009\u5b9a\u6a21\u578b\u6743\u91cd\u77e9\u9635\u7684\u5bf9\u89d2\u5757\uff0c\u65e0\u9700\u4f4e\u79e9\u77e9\u9635\u4e58\u79ef\uff0c\u907f\u514d\u4e86\u5bf9\u8f85\u52a9\u521d\u59cb\u5316\u65b9\u6848\u6216\u5b9a\u5236\u4f18\u5316\u7b56\u7565\u7684\u4f9d\u8d56\u3002", "result": "DiaBlo\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u5e38\u8bc6\u63a8\u7406\u3001\u7b97\u672f\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u5b89\u5168\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5185\u5b58\u6548\u7387\u548c\u5feb\u901f\u5fae\u8c03\u901f\u5ea6\u3002", "conclusion": "DiaBlo\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2506.03370", "pdf": "https://arxiv.org/pdf/2506.03370", "abs": "https://arxiv.org/abs/2506.03370", "authors": ["Leonid Ryvkin"], "title": "Comparison of different Unique hard attention transformer models by the formal languages they can recognize", "categories": ["cs.LG", "cs.CL", "cs.FL"], "comment": null, "summary": "This note is a survey of various results on the capabilities of unique hard\nattention transformers encoders (UHATs) to recognize formal languages. We\ndistinguish between masked vs. non-masked, finite vs. infinite image and\ngeneral vs. bilinear attention score functions. We recall some relations\nbetween these models, as well as a lower bound in terms of first-order logic\nand an upper bound in terms of circuit complexity.", "AI": {"tldr": "Survey of UHATs' capabilities in recognizing formal languages with different types of attention.", "motivation": "To explore the capabilities of unique hard attention transformers encoders (UHATs) in recognizing formal languages.", "method": "Distinguishing between masked vs. non-masked, finite vs. infinite image and general vs. bilinear attention score functions.", "result": "Recalled some relations between these models, as well as a lower bound in terms of first-order logic and an upper bound in terms of circuit complexity.", "conclusion": "This note serves as a survey of various results on the capabilities of unique hard attention transformers encoders (UHATs) to recognize formal languages."}}
{"id": "2506.03426", "pdf": "https://arxiv.org/pdf/2506.03426", "abs": "https://arxiv.org/abs/2506.03426", "authors": ["Joonseong Kang", "Soojeong Lee", "Subeen Park", "Sumin Park", "Taero Kim", "Jihee Kim", "Ryunyi Lee", "Kyungwoo Song"], "title": "Adaptive Task Vectors for Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks without parameter updates by conditioning on a few demonstrations\nprovided in the prompt. Despite its success, ICL suffers from several\nlimitations, including sensitivity to demonstration order, context length\nconstraints, and computational inefficiency. To address these challenges, task\nvector-based approaches compress task information into a single vector.\nHowever, these methods typically construct task vectors from fixed sets of\ndemonstrations and reuse them across input queries, without conditioning on the\nspecific input. This limitation can lead models to struggle with effective\nadaptation when the input query is not well aligned with the underlying\ndemonstrations, consequently degrading their generalization performance on\nunseen tasks. To overcome this limitation, we propose Adaptive Task Vectors\n(ATV), a simple and effective framework that dynamically generates task vectors\nconditioned on each input query. ATV employs a small language model to generate\ntask vectors, which are then transformed to match the target LLM's architecture\nand applied to guide its output generation. In contrast to ICL and previous\nvector-based approaches, which rely on fixed demonstration sets and their\ncorresponding vectors, ATV dynamically generates task vectors tailored to each\nspecific input query and task. Consequently, ATV demonstrates strong\nperformance and generalization capabilities, even for unseen tasks.\nFurthermore, we provide a theoretical analysis indicating that ATV is\nexpressively equivalent to LoRA under equal rank budgets and more expressive\nthan Prefix-Tuning, thereby offering formal support for its representational\nadvantage.", "AI": {"tldr": "Adaptive Task Vectors (ATV) is proposed to improve in-context learning in large language models by dynamically generating task vectors based on each input query.", "motivation": "To address limitations of current in-context learning methods such as sensitivity to demonstration order and poor generalization on unseen tasks.", "method": "Developing a framework called Adaptive Task Vectors (ATV) that uses a small language model to generate task vectors conditioned on input queries.", "result": "ATV shows strong performance and generalization abilities, especially on unseen tasks, and is theoretically proven to be more expressive than some existing methods.", "conclusion": "The proposed method ATV provides a way to enhance the adaptability and generalization capability of in-context learning in large language models."}}
{"id": "2506.03444", "pdf": "https://arxiv.org/pdf/2506.03444", "abs": "https://arxiv.org/abs/2506.03444", "authors": ["Yue Gong", "Raul Castro Fernandez"], "title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior", "categories": ["cs.LG", "cs.CL"], "comment": "Under Review", "summary": "As hypothesis generation becomes increasingly automated, a new bottleneck has\nemerged: hypothesis assessment. Modern systems can surface thousands of\nstatistical relationships-correlations, trends, causal links-but offer little\nguidance on which ones are novel, non-trivial, or worthy of expert attention.\nIn this work, we study the complementary problem to hypothesis generation:\nautomatic hypothesis assessment. Specifically, we ask: given a large set of\nstatistical relationships, can we automatically assess which ones are novel and\nworth further exploration? We focus on correlations as they are a common entry\npoint in exploratory data analysis that often serve as the basis for forming\ndeeper scientific or causal hypotheses.\n  To support automatic assessment, we propose to leverage the vast knowledge\nencoded in LLMs' weights to derive a prior distribution over the correlation\nvalue of a variable pair. If an LLM's prior expects the correlation value\nobserved, then such correlation is not surprising, and vice versa. We propose\nthe Logit-based Calibrated Prior, an LLM-elicited correlation prior that\ntransforms the model's raw output logits into a calibrated, continuous\npredictive distribution over correlation values. We evaluate the prior on a\nbenchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of\n78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of\n89.2% in predicting Pearson correlation coefficient. It also outperforms a\nfine-tuned RoBERTa classifier in binary correlation prediction and achieves\nhigher precision@K in hypothesis ranking. We further show that the prior\ngeneralizes to correlations not seen during LLM pretraining, reflecting\ncontext-sensitive reasoning rather than memorization.", "AI": {"tldr": "This paper studies the problem of automatically assessing statistical relationships (correlations) to determine their novelty and worthiness of further exploration.", "motivation": "With the increasing automation of hypothesis generation, there is a need for automatic hypothesis assessment to identify which statistical relationships are novel and non-trivial.", "method": "The authors propose using the knowledge encoded in large language models (LLMs) to derive a prior distribution over correlation values. They introduce the Logit-based Calibrated Prior, which transforms the model's raw output logits into a calibrated predictive distribution over correlation values.", "result": "The proposed prior was evaluated on a benchmark of 2,096 real-world variable pairs, achieving high accuracy in predicting Pearson correlation coefficients, outperforming a fine-tuned RoBERTa classifier in binary correlation prediction, and achieving higher precision@K in hypothesis ranking. The prior also generalized to unseen correlations, indicating context-sensitive reasoning rather than memorization.", "conclusion": "This work demonstrates the potential of using LLMs for automatic hypothesis assessment, providing a valuable tool for identifying novel and non-trivial statistical relationships."}}
{"id": "2506.03487", "pdf": "https://arxiv.org/pdf/2506.03487", "abs": "https://arxiv.org/abs/2506.03487", "authors": ["Xianming Li", "Aamir Shakir", "Rui Huang", "Julius Lipp", "Jing Li"], "title": "ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Reranking is fundamental to information retrieval and retrieval-augmented\ngeneration, with recent Large Language Models (LLMs) significantly advancing\nreranking quality. While recent advances with LLMs have significantly improved\ndocument reranking quality, current approaches primarily rely on large-scale\nLLMs (>7B parameters) through zero-shot prompting, presenting high\ncomputational costs. Small Language Models (SLMs) offer a promising alternative\nbecause of their efficiency, but our preliminary quantitative analysis reveals\nthey struggle with understanding task prompts without fine-tuning. This limits\ntheir effectiveness for document reranking tasks. To address this issue, we\nintroduce a novel two-stage training approach, ProRank, for SLM-based document\nreranking. First, we propose a prompt warmup stage using reinforcement learning\nGRPO to steer SLMs to understand task prompts and generate more accurate\ncoarse-grained binary relevance scores for document reranking. Then, we\ncontinuously fine-tune the SLMs with a fine-grained score learning stage\nwithout introducing additional layers to further improve the reranking quality.\nComprehensive experimental results demonstrate that the proposed ProRank\nconsistently outperforms both the most advanced open-source and proprietary\nreranking models. Notably, our lightweight ProRank-0.5B model even surpasses\nthe powerful 32B LLM reranking model on the BEIR benchmark, establishing that\nproperly trained SLMs can achieve superior document reranking performance while\nmaintaining computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5ProRank\uff0c\u7528\u4e8e\u57fa\u4e8e\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6587\u6863\u91cd\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u89c4\u6a21\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6587\u6863\u91cd\u6392\u5e8f\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u91cd\u6392\u5e8f\u8d28\u91cf\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u800c\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u56e0\u5176\u9ad8\u6548\u6027\u6210\u4e3a\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5b83\u4eec\u5728\u4e0d\u7ecf\u8fc7\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u7406\u89e3\u4efb\u52a1\u63d0\u793a\u8bed\uff0c\u9650\u5236\u4e86\u5176\u5728\u6587\u6863\u91cd\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProRank\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u62ec\u4e00\u4e2a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60GRPO\u7684\u63d0\u793a\u9884\u70ed\u9636\u6bb5\u548c\u4e00\u4e2a\u65e0\u9700\u5f15\u5165\u989d\u5916\u5c42\u7684\u7ec6\u7c92\u5ea6\u8bc4\u5206\u5b66\u4e60\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684ProRank\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u4e13\u6709\u91cd\u6392\u5e8f\u6a21\u578b\u3002\u7279\u522b\u662f\u8f7b\u91cf\u7ea7\u7684ProRank-0.5B\u6a21\u578b\u751a\u81f3\u5728BEIR\u57fa\u51c6\u4e0a\u8d85\u8fc7\u4e86\u5f3a\u5927\u768432B\u53c2\u6570LLM\u91cd\u6392\u5e8f\u6a21\u578b\u3002", "conclusion": "\u9002\u5f53\u8bad\u7ec3\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u6587\u6863\u91cd\u6392\u5e8f\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.03525", "pdf": "https://arxiv.org/pdf/2506.03525", "abs": "https://arxiv.org/abs/2506.03525", "authors": ["Daeun Lee", "Jaehong Yoon", "Jaemin Cho", "Mohit Bansal"], "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project website: https://video-skill-cot.github.io/", "summary": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains.", "AI": {"tldr": "Propose Video-SKoT, a framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.", "motivation": "Existing methods often struggle to adapt to domain-specific skills over various video content.", "method": "Propose Video-Skill-CoT (Video-SKoT), which automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. Construct skill-based CoT annotations and introduce a skill-specific expert learning framework.", "result": "Demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where Video-SKoT consistently outperforms strong baselines.", "conclusion": "Video-SKoT provides a way to improve complex video understanding by constructing skill-aware CoT supervisions."}}
{"id": "2506.03587", "pdf": "https://arxiv.org/pdf/2506.03587", "abs": "https://arxiv.org/abs/2506.03587", "authors": ["Florian Boudin", "Akiko Aizawa"], "title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing", "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "The rapid growth of scholarly literature makes it increasingly difficult for\nresearchers to keep up with new knowledge. Automated tools are now more\nessential than ever to help navigate and interpret this vast body of\ninformation. Scientific papers pose unique difficulties, with their complex\nlanguage, specialized terminology, and diverse formats, requiring advanced\nmethods to extract reliable and actionable insights. Large language models\n(LLMs) offer new opportunities, enabling tasks such as literature reviews,\nwriting assistance, and interactive exploration of research. This special issue\nof the TAL journal highlights research addressing these challenges and, more\nbroadly, research on natural language processing and information retrieval for\nscholarly and scientific documents.", "AI": {"tldr": "This special issue of the TAL journal focuses on research that addresses the challenges of navigating and interpreting scholarly literature using large language models and other advanced methods.", "motivation": "The increasing difficulty for researchers to keep up with new knowledge due to the rapid growth of scholarly literature necessitates automated tools to extract reliable insights.", "method": "Using large language models (LLMs) to enable tasks such as literature reviews, writing assistance, and interactive exploration of research.", "result": "Research presented in this special issue addresses challenges in natural language processing and information retrieval for scholarly and scientific documents.", "conclusion": "Automated tools like LLMs provide new opportunities for navigating and interpreting scholarly literature."}}
{"id": "2506.03589", "pdf": "https://arxiv.org/pdf/2506.03589", "abs": "https://arxiv.org/abs/2506.03589", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Anh Nguyen", "Ngan Le"], "title": "BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages, 14 figures", "summary": "Text-video retrieval (TVR) systems often suffer from visual-linguistic biases\npresent in datasets, which cause pre-trained vision-language models to overlook\nkey details. To address this, we propose BiMa, a novel framework designed to\nmitigate biases in both visual and textual representations. Our approach begins\nby generating scene elements that characterize each video by identifying\nrelevant entities/objects and activities. For visual debiasing, we integrate\nthese scene elements into the video embeddings, enhancing them to emphasize\nfine-grained and salient details. For textual debiasing, we introduce a\nmechanism to disentangle text features into content and bias components,\nenabling the model to focus on meaningful content while separately handling\nbiased information. Extensive experiments and ablation studies across five\nmajor TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)\ndemonstrate the competitive performance of BiMa. Additionally, the model's bias\nmitigation capability is consistently validated by its strong results on\nout-of-distribution retrieval tasks.", "AI": {"tldr": "This paper proposes BiMa, a new framework for text-video retrieval (TVR) that reduces visual-linguistic biases in datasets by generating scene elements for videos and separating text features into content and bias components.", "motivation": "Pre-trained vision-language models in TVR systems often overlook key details due to biases present in datasets.", "method": "BiMa generates scene elements for videos, integrates these into video embeddings for visual debiasing, and introduces a mechanism to separate text features into content and bias components for textual debiasing.", "result": "Extensive experiments across five major TVR benchmarks show that BiMa performs competitively and maintains strong results on out-of-distribution retrieval tasks.", "conclusion": "BiMa effectively mitigates biases in both visual and textual representations, leading to improved performance in TVR."}}
{"id": "2506.03606", "pdf": "https://arxiv.org/pdf/2506.03606", "abs": "https://arxiv.org/abs/2506.03606", "authors": ["Parismita Gogoi", "Sishir Kalita", "Wendy Lalhminghlui", "Viyazonuo Terhiija", "Moakala Tzudir", "Priyankoo Sarmah", "S. R. M. Prasanna"], "title": "Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "comment": "Accepted in Interspeech2025", "summary": "This study explores the use of self-supervised learning (SSL) models for tone\nrecognition in three low-resource languages from North Eastern India: Angami,\nAo, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on\nboth tonal and non-tonal languages. We analyze tone-wise performance across the\nlayers for all three languages and compare the different models. Our results\nshow that tone recognition works best for Mizo and worst for Angami. The middle\nlayers of the SSL models are the most important for tone recognition,\nregardless of the pre-training language, i.e. tonal or non-tonal. We have also\nfound that the tone inventory, tone types, and dialectal variations affect tone\nrecognition. These findings provide useful insights into the strengths and\nweaknesses of SSL-based embeddings for tonal languages and highlight the\npotential for improving tone recognition in low-resource settings. The source\ncode is available at GitHub 1 .", "AI": {"tldr": "This study investigates the effectiveness of self-supervised learning models for tone recognition in three low-resource North Eastern Indian languages: Angami, Ao, and Mizo. It evaluates four Wav2vec2.0 base models and finds that tone recognition performs best for Mizo and worst for Angami. The research highlights that the middle layers of the SSL models are crucial for tone recognition regardless of the pre-training language type. Tone inventory, tone types, and dialectal variations impact tone recognition. This study offers valuable insights into SSL-based embeddings for tonal languages and suggests ways to enhance tone recognition in low-resource scenarios.", "motivation": "The motivation behind this study is to explore the potential of self-supervised learning models for tone recognition in low-resource languages from North Eastern India, specifically Angami, Ao, and Mizo.", "method": "Four Wav2vec2.0 base models pre-trained on both tonal and non-tonal languages are evaluated for their effectiveness in tone recognition across the three languages.", "result": "Tone recognition works best for Mizo and worst for Angami. The middle layers of the SSL models are the most important for tone recognition, irrespective of the pre-training language type. Tone inventory, tone types, and dialectal variations influence tone recognition.", "conclusion": "This study provides useful insights into the strengths and weaknesses of SSL-based embeddings for tonal languages and suggests possibilities for enhancing tone recognition in low-resource settings."}}
{"id": "2506.03614", "pdf": "https://arxiv.org/pdf/2506.03614", "abs": "https://arxiv.org/abs/2506.03614", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Chao Yang", "Chaochao Lu"], "title": "VLMs Can Aggregate Scattered Training Patches", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as $\\textit{visual\nstitching}$ -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch},\n\\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.", "AI": {"tldr": "This paper investigates the 'visual stitching' capability in vision-language models (VLMs), which allows harmful content to bypass moderation when split into small, harmless-looking patches. The authors demonstrate this ability in popular open-source VLMs and show how harmful content can evade detection and be reconstructed during inference, posing significant safety risks.", "motivation": "To understand and mitigate the risks of vision-language models being manipulated through 'visual stitching', where harmful content can be hidden in small, seemingly benign patches.", "method": "The authors fine-tune VLMs using image patches labeled with unique synthetic IDs and observe whether the model can correctly verbalize these IDs from full images or text references. They also simulate an adversarial data poisoning scenario with harmful patches labeled with misleading text descriptions.", "result": "The results confirm that VLMs possess 'visual stitching' capabilities, allowing them to reconstruct IDs from full images or text references. This demonstrates how harmful content can evade moderation and be reproduced during inference.", "conclusion": "The study highlights the risks associated with 'visual stitching' in VLMs and emphasizes the need for more robust data moderation techniques to prevent potential misuse."}}
{"id": "2506.03741", "pdf": "https://arxiv.org/pdf/2506.03741", "abs": "https://arxiv.org/abs/2506.03741", "authors": ["Rifat Mehreen Amin", "Oliver Hans K\u00fchle", "Daniel Buschek", "Andreas Butz"], "title": "PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "We introduce PromptCanvas, a concept that transforms prompting into a\ncomposable, widget-based experience on an infinite canvas. Users can generate,\ncustomize, and arrange interactive widgets representing various facets of their\ntext, offering greater control over AI-generated content. PromptCanvas allows\nwidget creation through system suggestions, user prompts, or manual input,\nproviding a flexible environment tailored to individual needs. This enables\ndeeper engagement with the creative process. In a lab study with 18\nparticipants, PromptCanvas outperformed a traditional conversational UI on the\nCreativity Support Index. Participants found that it reduced cognitive load,\nwith lower mental demand and frustration. Qualitative feedback revealed that\nthe visual organization of thoughts and easy iteration encouraged new\nperspectives and ideas. A follow-up field study (N=10) confirmed these results,\nshowcasing the potential of dynamic, customizable interfaces in improving\ncollaborative writing with AI.", "AI": {"tldr": "PromptCanvas is a new way to interact with AI by using a visual canvas with interactive widgets, which was shown to improve creativity and reduce cognitive load compared to traditional methods.", "motivation": "To provide users with more control over AI-generated content and enhance their creative process.", "method": "Developing a concept called PromptCanvas that uses a visual canvas with interactive widgets for text generation and customization, allowing users to create widgets through system suggestions, user prompts, or manual input.", "result": "In a lab study, PromptCanvas outperformed a traditional conversational UI on the Creativity Support Index, reducing cognitive load and encouraging new perspectives and ideas. A follow-up field study confirmed these results.", "conclusion": "PromptCanvas demonstrates the potential of dynamic, customizable interfaces in improving collaborative writing with AI."}}
{"id": "2506.03857", "pdf": "https://arxiv.org/pdf/2506.03857", "abs": "https://arxiv.org/abs/2506.03857", "authors": ["Mingxuan Xia", "Haobo Wang", "Yixuan Li", "Zewei Yu", "Jindong Wang", "Junbo Zhao", "Runze Wu"], "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025 (Main conference)", "summary": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist.", "AI": {"tldr": "Large Language Models (LLMs) have been used for data annotation, but they often produce wrong labels due to uncertainty. A new method called CanDist has been proposed which encourages LLMs to output multiple possible labels and then uses a Small Language Model (SLM) to distill these into unique labels for downstream tasks.", "motivation": "Existing methods for using LLMs for data annotation use an aggressive strategy that prompts LLMs to output a single gold label per unlabeled sample, which can lead to incorrect labels for difficult samples. This paper is motivated by ambiguity aversion in human behavior and proposes a novel candidate annotation paradigm.", "method": "The paper proposes a teacher-student framework called CanDist that distills candidate annotations with a Small Language Model (SLM). It also provides a theoretical justification showing that distilling candidate annotations from the teacher LLM is better than using single annotations.", "result": "Experiments on six text classification tasks showed the effectiveness of the proposed method.", "conclusion": "This paper introduces a novel approach to data annotation using Large Language Models, which improves the quality of labels and performance of downstream tasks."}}
{"id": "2506.03930", "pdf": "https://arxiv.org/pdf/2506.03930", "abs": "https://arxiv.org/abs/2506.03930", "authors": ["Yuansheng Ni", "Ping Nie", "Kai Zou", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.", "AI": {"tldr": "A large-scale dataset VisCode-200K is presented to improve the performance of LLMs in visualization tasks.", "motivation": "Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation.", "method": "Fine-tuning Qwen2.5-Coder-Instruct on VisCode-200K", "result": "VisCoder demonstrates the benefits of feedback-driven learning for executable, visually accurate code generation.", "conclusion": "VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini."}}
{"id": "2506.03939", "pdf": "https://arxiv.org/pdf/2506.03939", "abs": "https://arxiv.org/abs/2506.03939", "authors": ["Junqi Gao", "Xiang Zou", "YIng Ai", "Dong Li", "Yichen Niu", "Biqing Qi", "Jianxing Liu"], "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git.", "AI": {"tldr": "Graph Counselor improves Large Language Models' factual accuracy and generation quality by using multi-agent collaboration and self-reflection.", "motivation": "Existing Graph Retrieval Augmented Generation methods have inefficient information aggregation and rigid reasoning mechanism.", "method": "Graph Counselor uses Adaptive Graph Information Extraction Module with Planning, Thought, and Execution Agents for multi-level dependency modeling and dynamic reasoning depth adjustment. It also uses Self-Reflection with Multiple Perspectives module for accurate and consistent reasoning results.", "result": "Graph Counselor performs better than existing methods in various graph reasoning tasks.", "conclusion": "Multi-agent collaboration and self-reflection can enhance external knowledge integration capabilities in specialized domains."}}
{"id": "2506.04018", "pdf": "https://arxiv.org/pdf/2506.04018", "abs": "https://arxiv.org/abs/2506.04018", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Goun\u00e9", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.11; K.4.1; I.2.6"], "comment": "Prepint, under review for NeurIPS 2025", "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a AgentMisalignment \u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e0d aligned \u503e\u5411\uff0c\u53d1\u73b0\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u503e\u5411\u4e8e\u66f4\u591a\u7684\u4e0d aligned \u884c\u4e3a\uff0c\u5e76\u4e14\u4ee3\u7406\u4e2a\u6027\u5bf9\u4e0d aligned \u503e\u5411\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5c1d\u8bd5\u4e0d aligned \u884c\u4e3a\u7684\u53ef\u80fd\u6027\uff08\u4e0d aligned \u503e\u5411\uff09\uff0c\u8fd9\u662f\u4e4b\u524d\u5de5\u4f5c\u6ca1\u6709\u5145\u5206\u7406\u89e3\u7684\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u4e0d aligned \u503e\u5411\u57fa\u51c6\u6d4b\u8bd5 AgentMisalignment\uff0c\u5305\u62ec\u4e00\u7cfb\u5217\u73b0\u5b9e\u573a\u666f\uff0c\u4f7f LLM \u4ee3\u7406\u6709\u673a\u4f1a\u8868\u73b0\u51fa\u4e0d aligned \u884c\u4e3a\uff0c\u5e76\u7ec4\u7ec7\u4e86\u56db\u4e2a\u5b50\u7c7b\u522b\u7684\u4e0d aligned \u884c\u4e3a\u8bc4\u4f30\uff0c\u5305\u62ec\u76ee\u6807\u5b88\u536b\u3001\u62b5\u6297\u5173\u95ed\u3001\u6c99\u888b\u884c\u4e3a\u548c\u6743\u529b\u5bfb\u6c42\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e0d\u540c\u7684\u7cfb\u7edf\u63d0\u793a\u6539\u53d8\u4ee3\u7406\u4e2a\u6027\uff0c\u7814\u7a76\u5176\u5bf9\u4e0d aligned \u503e\u5411\u7684\u5f71\u54cd\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u663e\u793a\uff0c\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u5e73\u5747\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u4e0d aligned \u503e\u5411\u3002\u53e6\u5916\uff0c\u4e2a\u6027\u7279\u5f81\u53ef\u4ee5\u6781\u5927\u5730\u3001\u4e0d\u53ef\u9884\u6d4b\u5730\u5f71\u54cd\u4e0d aligned \u503e\u5411\uff0c\u6709\u65f6\u6bd4\u6a21\u578b\u672c\u8eab\u7684\u9009\u62e9\u66f4\u91cd\u8981\u3002", "conclusion": "\u5f53\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\u672a\u80fd\u63a8\u5e7f\u5230 LLM \u4ee3\u7406\uff0c\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u8bc4\u4f30\u503e\u5411\u7684\u91cd\u8981\u6027\uff0c\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u7684\u666e\u53ca\u3002"}}
{"id": "2506.04019", "pdf": "https://arxiv.org/pdf/2506.04019", "abs": "https://arxiv.org/abs/2506.04019", "authors": ["Neeva Oza", "Ishaan Govil", "Parul Gupta", "Dinesh Khandelwal", "Dinesh Garg", "Parag Singla"], "title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)", "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5"], "comment": null, "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.", "AI": {"tldr": "This paper introduces CETBench, a new benchmark for code equivalence checking using code transformation. It shows that current state-of-the-art large language models (LLMs) perform poorly on this task even with simple transformations. The authors propose a fine-tuning approach to improve LLM performance on transformed code pairs.", "motivation": "To evaluate the capabilities of LLMs for code rewriting and translation tasks by benchmarking their ability to check code equivalence.", "method": "Creating CETBench by applying random code transformations to pairs of programs in a repository and testing SOTA LLMs on these transformed pairs.", "result": "SOTA LLMs show poor performance on the code equivalence checking task even with simple transformations. Fine-tuning improves performance.", "conclusion": "LLMs may lack true semantic understanding of code, as evidenced by their poor performance on transformed code equivalence tasks."}}
{"id": "2506.04039", "pdf": "https://arxiv.org/pdf/2506.04039", "abs": "https://arxiv.org/abs/2506.04039", "authors": ["Jiulong Wu", "Zhengliang Shi", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Lingyong Yan", "Min Cao", "Min Zhang"], "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.", "AI": {"tldr": "This paper introduces EMPO, a method that improves modality alignment in large visual language models to reduce hallucinations, using open-source instruction datasets to create high-quality preference data.", "motivation": "To address the issue of hallucinations in large visual language models due to modality misalignment and the limitations of their underlying large language model backbones.", "method": "Entity-centric Multimodal Preference Optimization (EMPO), which enhances modality alignment and uses open-source instruction datasets for constructing high-quality preference data.", "result": "Experiments show that EMPO reduces hallucination rates by 85.9% on Object-HalBench and 49.8% on MM-HalBench.", "conclusion": "EMPO effectively improves modality alignment in large visual language models, leading to reduced hallucination rates."}}
{"id": "2506.04088", "pdf": "https://arxiv.org/pdf/2506.04088", "abs": "https://arxiv.org/abs/2506.04088", "authors": ["Jun-Peng Jiang", "Yu Xia", "Hai-Long Sun", "Shiyin Lu", "Qing-Guo Chen", "Weihua Luo", "Kaifu Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "title": "Multimodal Tabular Reasoning with Privileged Structured Information", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.", "AI": {"tldr": "This paper introduces a new framework called Turbo to perform tabular reasoning from table images, achieving state-of-the-art performance with limited data.", "motivation": "To tackle the task of tabular reasoning from table images where high-quality textual representations are often unavailable.", "method": "Introduce a new framework called Turbo which benefits from a structure-aware reasoning trace generator and repeatedly generates and selects advantageous reasoning paths.", "result": "Achieves state-of-the-art performance across multiple datasets with limited data.", "conclusion": "Turbo demonstrates the effectiveness of leveraging privileged structured information to enhance multimodal large language models for tabular reasoning."}}
{"id": "2506.04089", "pdf": "https://arxiv.org/pdf/2506.04089", "abs": "https://arxiv.org/abs/2506.04089", "authors": ["Anastasiia Ivanova", "Eva Bakaeva", "Zoya Volovikova", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": "ACL 2025 (Main Conference)", "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.", "AI": {"tldr": "\u63d0\u51faAmbiK\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7edf\u4e00\u6bd4\u8f83\u4efb\u52a1\u6b67\u4e49\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4efb\u52a1\u6b67\u4e49\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6bd4\u8f83\uff0c\u56e0\u4e3a\u5b83\u4eec\u662f\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u7684\uff0c\u6ca1\u6709\u901a\u7528\u57fa\u51c6\u3002", "method": "\u5728\u53a8\u623f\u73af\u5883\u4e2d\u6536\u96c6\u4e861000\u5bf9\u6a21\u7cca\u4efb\u52a1\u53ca\u5176\u660e\u786e\u5bf9\u5e94\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u73af\u5883\u63cf\u8ff0\u3001\u6f84\u6e05\u95ee\u9898\u548c\u7b54\u6848\u3001\u7528\u6237\u610f\u56fe\u548c\u4efb\u52a1\u8ba1\u5212\u7b49\u4fe1\u606f\u3002", "result": "AmbiK\u6570\u636e\u96c6\u5305\u542b\u4e862000\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a\u4e09\u7c7b\u6b67\u4e49\u7c7b\u578b\uff08\u4eba\u7c7b\u504f\u597d\u3001\u5e38\u8bc6\u77e5\u8bc6\u3001\u5b89\u5168\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u4eba\u7c7b\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6AmbiK\uff0c\u7528\u4e8e\u7edf\u4e00\u6bd4\u8f83\u4efb\u52a1\u6b67\u4e49\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2506.04141", "pdf": "https://arxiv.org/pdf/2506.04141", "abs": "https://arxiv.org/abs/2506.04141", "authors": ["Kejian Zhu", "Zhuoran Jin", "Hongbang Yuan", "Jiachun Li", "Shangqing Tu", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://mmr-v.github.io", "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.", "AI": {"tldr": "This paper introduces MMR-V, a new benchmark for evaluating multimodal deep reasoning in videos. It emphasizes long-range, multi-frame reasoning, beyond perception, reliability, and confusability. Current models perform poorly on this benchmark.", "motivation": "Existing video benchmarks mainly focus on understanding tasks, neglecting multimodal deep reasoning. This paper aims to fill the gap by proposing a new benchmark.", "method": "MMR-V, a benchmark for multimodal deep reasoning in videos with four key features: long-range, multi-frame reasoning, beyond perception, reliability, and confusability.", "result": "Current models struggle with multimodal reasoning on MMR-V, with the best-performing model achieving only 52.5% accuracy. Existing reasoning enhancement strategies bring limited gains.", "conclusion": "MMR-V can inspire further research into enhancing multimodal reasoning capabilities."}}
{"id": "2506.04207", "pdf": "https://arxiv.org/pdf/2506.04207", "abs": "https://arxiv.org/abs/2506.04207", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "19 pages, 6 figures", "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.", "AI": {"tldr": "This paper introduces ReVisual-R1, a novel 7B-parameter MLLM, which improves multimodal reasoning capabilities via a staged training approach.", "motivation": "Enhancing multimodal large language model's (MLLM) reasoning capabilities by improving multimodal reinforcement learning (RL) through analyzing current training pipelines.", "method": "Staged training approach combining effective cold start initialization, addressing gradient stagnation in multimodal RL, and subsequent text-only RL training.", "result": "Achieved superior performance on benchmarks like MathVerse, MathVision, WeMath, LogicVista, DynaMath, and AIME2024, AIME2025 compared to many recent multimodal reasoning models.", "conclusion": "ReVisual-R1 sets a new state-of-the-art performance among open-source 7B MLLMs on various challenging benchmarks."}}
{"id": "2506.04210", "pdf": "https://arxiv.org/pdf/2506.04210", "abs": "https://arxiv.org/abs/2506.04210", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models.", "AI": {"tldr": "Test-time scaling for reasoning models often leads to overthinking, which initially improves performance but eventually undermines precision due to increased output variance. An alternative method, parallel thinking, achieves better accuracy by generating multiple independent reasoning paths and selecting the most consistent response.", "motivation": "To investigate whether 'thinking more' at test-time genuinely enhances reasoning ability in models like OpenAI o1 and DeepSeek R1.", "method": "Detailed empirical study across models and benchmarks, and proposing a probabilistic model to understand the non-monotonic trend of performance.", "result": "Initial performance improvements from additional thinking followed by a decline ('overthinking'), and introduction of parallel thinking method achieving up to 20% higher accuracy than extended thinking.", "conclusion": "Extended thinking is not an effective strategy for test-time scaling as it creates an illusion of improved reasoning due to model uncertainty and evaluation metric connection. Parallel thinking offers a superior alternative."}}
{"id": "2506.03259", "pdf": "https://arxiv.org/pdf/2506.03259", "abs": "https://arxiv.org/abs/2506.03259", "authors": ["Michael E. Garcia-Alcoser", "Mobina GhojoghNejad", "Fakrul Islam Tushar", "David Kim", "Kyle J. Lafata", "Geoffrey D. Rubin", "Joseph Y. Lo"], "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems", "categories": ["cs.CL", "I.2.7"], "comment": "23 pages, 10 figures, to be submitted in Radiology: Artificial\n  Intelligence", "summary": "Purpose: This study aims to evaluate the effectiveness of large language\nmodels (LLMs) in automating disease annotation of CT radiology reports. We\ncompare a rule-based algorithm (RBA), RadBERT, and three lightweight\nopen-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)\nCT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 CT reports\nfrom 29,540 patients, with 1,789 CAP reports manually annotated across three\norgan systems. External validation was conducted using the CT-RATE dataset.\nThree open-weight LLMs were tested with zero-shot prompting. Performance was\nevaluated using Cohen's Kappa and micro/macro-averaged F1 scores.\n  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and\nGemma-3 27B showed the highest agreement ($\\kappa$ median: 0.87). On the\nmanually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed\nby Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE\ndataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3\n27B close behind (0.89). Performance differences were mainly due to differing\nlabeling practices, especially for lung atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report\nannotation and generalize across organ systems with zero-shot prompting.\nHowever, binary labels alone cannot capture the full nuance of report language.\nLLMs can provide a flexible, efficient solution aligned with clinical judgment\nand user needs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.03268", "pdf": "https://arxiv.org/pdf/2506.03268", "abs": "https://arxiv.org/abs/2506.03268", "authors": ["Cristiano Chesi"], "title": "A conclusive remark on linguistic theorizing and language modeling", "categories": ["cs.CL"], "comment": null, "summary": "This is the final remark on the replies received to my target paper in the\nItalian Journal of Linguistics", "AI": {"tldr": "This is the final remark on the replies received to my target paper in the Italian Journal of Linguistics.", "motivation": "To summarize and conclude the discussion on the replies received for the target paper.", "method": "Analyzing and summarizing the replies.", "result": "Final remarks on the replies received.", "conclusion": "Concluding the discussion on the replies to the target paper."}}
{"id": "2506.03278", "pdf": "https://arxiv.org/pdf/2506.03278", "abs": "https://arxiv.org/abs/2506.03278", "authors": ["Christodoulos Constantinides", "Dhaval Patel", "Shuxin Lin", "Claudio Guerrero", "Sunil Dagajirao Patil", "Jayant Kalagnanam"], "title": "FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes", "categories": ["cs.CL"], "comment": null, "summary": "We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)\nbenchmarking system designed to assess the ability of Large Language Models\n(LLMs) to reason and understand complex, domain-specific scenarios in Industry\n4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects\nof reasoning through failure modes, sensor data, and the relationships between\nthem across various industrial assets. Through this work, we envision a\nparadigm shift where modeling decisions are not only data-driven using\nstatistical tools like correlation analysis and significance tests, but also\ndomain-driven by specialized LLMs which can reason about the key contributors\nand useful patterns that can be captured with feature engineering. We evaluate\nthe Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and\nMistral-on FailureSensorIQ from different lens using\nPerturbation-Uncertainty-Complexity analysis, Expert Evaluation study,\nAsset-Specific Knowledge Gap analysis, ReAct agent using external\nknowledge-bases. Even though closed-source models with strong reasoning\ncapabilities approach expert-level performance, the comprehensive benchmark\nreveals a significant drop in performance that is fragile to perturbations,\ndistractions, and inherent knowledge gaps in the models. We also provide a\nreal-world case study of how LLMs can drive the modeling decisions on 3\ndifferent failure prediction datasets related to various assets. We release:\n(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ\nbenchmark and Hugging Face leaderboard based on MCQA built from non-textual\ndata found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature\nselection scikit-learn pipeline. The software is available at\nhttps://github.com/IBM/FailureSensorIQ.", "AI": {"tldr": "Introduce FailureSensorIQ, a benchmark for assessing LLMs in industrial scenarios, revealing performance fragility and knowledge gaps.", "motivation": "To create a novel Multi-Choice Question-Answering (MCQA) benchmarking system that assesses LLMs' ability to reason and understand complex industrial scenarios.", "method": "FailureSensorIQ benchmark evaluates LLMs using multiple lenses such as Perturbation-Uncertainty-Complexity analysis, Expert Evaluation study, Asset-Specific Knowledge Gap analysis, and ReAct agents with external knowledge-bases.", "result": "Closed-source models like GPT-4 approach expert-level performance but show fragility in certain conditions. The benchmark reveals knowledge gaps and provides insights into improving LLM performance.", "conclusion": "Closed-source models with strong reasoning capabilities perform well but are still fragile to perturbations and distractions. The FailureSensorIQ benchmark highlights knowledge gaps and provides tools for improving LLM performance."}}
{"id": "2506.03292", "pdf": "https://arxiv.org/pdf/2506.03292", "abs": "https://arxiv.org/abs/2506.03292", "authors": ["Jiuding Sun", "Sidharth Baskaran", "Zhengxuan Wu", "Michael Sklar", "Christopher Potts", "Atticus Geiger"], "title": "HyperSteer: Activation Steering at Scale with Hypernetworks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering language models (LMs) by modifying internal activations is a popular\napproach for controlling text generation. Unsupervised dictionary learning\nmethods, e.g., sparse autoencoders, can be scaled to produce many steering\nvectors, but lack guarantees on the individual efficacy of each vector and\ncontrol over the coverage of relevant steering tasks. In contrast, supervised\nmethods for constructing steering vectors are targeted and effective, but\nrequire more data collection and training for each additional steering vector\nproduced. In this work, we introduce HyperSteer, a family of hypernetwork-based\narchitectures which are trained end-to-end to generate steering vectors\nconditioned on the natural language steering prompts and the internals of the\nsteered LM. In our evaluations, we show that scaling HyperSteer with thousands\nof steering prompts exceeds the performance of state-of-the-art activation\nsteering methods, even on steering prompts never seen during training.\nMoreover, HyperSteer performs on par with steering-via-prompting.", "AI": {"tldr": "HyperSteer improves language model steering by generating effective steering vectors using hypernetworks, outperforming current methods.", "motivation": "To address the limitations of both unsupervised and supervised methods for steering language models.", "method": "HyperSteer uses hypernetwork-based architectures to generate steering vectors conditioned on steering prompts and the internals of the steered LM.", "result": "HyperSteer outperforms other methods when scaled with thousands of steering prompts, even on unseen prompts.", "conclusion": "HyperSteer surpasses state-of-the-art activation steering methods and performs as well as steering-via-prompting."}}
{"id": "2506.03295", "pdf": "https://arxiv.org/pdf/2506.03295", "abs": "https://arxiv.org/abs/2506.03295", "authors": ["Yubo Wang", "Ping Nie", "Kai Zou", "Lijun Wu", "Wenhu Chen"], "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.", "AI": {"tldr": "One-shot Critique Fine-Tuning (CFT) is proposed to efficiently enhance the reasoning abilities of large language models.", "motivation": "To find a more efficient way to unleash the reasoning potential of powerful base LLMs than reinforcement learning (RL) which is expensive and unstable.", "method": "Construct critique data by collecting diverse model-generated solutions to a single problem and use teacher LLMs to provide detailed critiques. Fine-tune LLMs on the CFT data.", "result": "Qwen and Llama family models show significant performance gains across diverse reasoning tasks after being fine-tuned with CFT data. The results are comparable to or even surpass the results from RL with 20x less compute.", "conclusion": "One-shot Critique Fine-Tuning (CFT) is a simple, general, and compute-efficient approach to unleash the reasoning capabilities of modern LLMs."}}
{"id": "2506.03301", "pdf": "https://arxiv.org/pdf/2506.03301", "abs": "https://arxiv.org/abs/2506.03301", "authors": ["Daham M. Mustafa", "Abhishek Nadgeri", "Diego Collarana", "Benedikt T. Arnold", "Christoph Quix", "Christoph Lange", "Stefan Decker"], "title": "From Instructions to ODRL Usage Policies: An Ontology Guided Approach", "categories": ["cs.CL", "F.2.2; I.2.7; H.3.3"], "comment": "The paper is accepted at LLM+KG: International Workshop on Data\n  Management Opportunities in Unifying Large Language Models + Knowledge\n  Graphs, VLDB 2024, August 26, 2024, Guangzhou, China.\n  https://vldb.org/workshops/2024/proceedings/LLM+KG/LLM+KG-15.pdf", "summary": "This study presents an approach that uses large language models such as GPT-4\nto generate usage policies in the W3C Open Digital Rights Language ODRL\nautomatically from natural language instructions. Our approach uses the ODRL\nontology and its documentation as a central part of the prompt. Our research\nhypothesis is that a curated version of existing ontology documentation will\nbetter guide policy generation. We present various heuristics for adapting the\nODRL ontology and its documentation to guide an end-to-end KG construction\nprocess. We evaluate our approach in the context of dataspaces, i.e.,\ndistributed infrastructures for trustworthy data exchange between multiple\nparticipating organizations for the cultural domain. We created a benchmark\nconsisting of 12 use cases of varying complexity. Our evaluation shows\nexcellent results with up to 91.95% accuracy in the resulting knowledge graph.", "AI": {"tldr": "This paper explores using large language models to automatically create ODRL usage policies from natural language instructions, showing up to 91.95% accuracy in knowledge graph generation.", "motivation": "The motivation behind this study is to explore the use of large language models like GPT-4 for automatically generating usage policies in the W3C Open Digital Rights Language (ODRL) from natural language instructions.", "method": "The method involves using the ODRL ontology and its documentation as a key component of the prompt. The researchers also propose heuristics for modifying the ODRL ontology and its documentation to facilitate the end-to-end knowledge graph construction process.", "result": "The approach was evaluated within the context of dataspace, which refers to distributed infrastructures for reliable data exchange among multiple participating organizations in the cultural sector. A benchmark comprising 12 use cases of differing complexities was developed. The evaluation demonstrated high accuracy rates of up to 91.95% in the generated knowledge graph.", "conclusion": "This study concludes that a carefully selected version of existing ontology documentation can effectively direct policy generation."}}
{"id": "2506.03303", "pdf": "https://arxiv.org/pdf/2506.03303", "abs": "https://arxiv.org/abs/2506.03303", "authors": ["Mustafa Eyceoz", "Nikhil Shivakumar Nayak", "Hao Wang", "Ligong Han", "Akash Srivastava"], "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6; I.2.4"], "comment": "10 pages, 4 figures, 9 tables", "summary": "Modern causal language models stack many attention blocks to improve\nperformance, but not all blocks are necessary for every task. We propose\nHopscotch, a simple yet effective method that identifies and skips attention\nblocks with least contributions to a task and adapts to preserve output\nquality. Hopscotch jointly optimizes which blocks to skip and how to scale the\noutputs of the remaining layers. By introducing lightweight, trainable scaling\nparameters to attention and MLP blocks, it mitigates distribution shifts in\nhidden states caused by removing attention blocks. Hopscotch does not modify\nmodel weights or require access to pretraining or instruction-tuning data, and\nis compatible with existing model compression techniques. When applied to\n$\\texttt{Llama-3.1-8B}$ and $\\texttt{Qwen2.5-7B}$, Hopscotch achieves less than\na 2% drop in performance even after skipping four attention blocks.", "AI": {"tldr": "Introducing Hopscotch, a method to skip unnecessary attention blocks in large language models without significantly affecting performance.", "motivation": "Not all attention blocks in modern causal language models are necessary for every task.", "method": "Hopscotch, a method that identifies and skips attention blocks with least contributions to a task and adapts to preserve output quality.", "result": "When applied to Llama-3.1-8B and Qwen2.5-7B, Hopscotch achieves less than a 2% drop in performance even after skipping four attention blocks.", "conclusion": "Hopscotch is a simple yet effective method that can mitigate distribution shifts in hidden states caused by removing attention blocks without modifying model weights or requiring access to pretraining or instruction-tuning data."}}
{"id": "2506.03310", "pdf": "https://arxiv.org/pdf/2506.03310", "abs": "https://arxiv.org/abs/2506.03310", "authors": ["Guillermo Marco", "Julio Gonzalo", "V\u00edctor Fresno"], "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing", "categories": ["cs.CL", "cs.HC"], "comment": "Camera-ready version, 14 pages, 3 figures. Accepted to Findings of\n  the Association for Computational Linguistics (ACL) 2025. Code & data:\n  https://github.com/grmarco/the-reader-is-the-metric", "summary": "Recent studies comparing AI-generated and human-authored literary texts have\nproduced conflicting results: some suggest AI already surpasses human quality,\nwhile others argue it still falls short. We start from the hypothesis that such\ndivergences can be largely explained by genuine differences in how readers\ninterpret and value literature, rather than by an intrinsic quality of the\ntexts evaluated. Using five public datasets (1,471 stories, 101 annotators\nincluding critics, students, and lay readers), we (i) extract 17 reference-less\ntextual features (e.g., coherence, emotional variance, average sentence\nlength...); (ii) model individual reader preferences, deriving feature\nimportance vectors that reflect their textual priorities; and (iii) analyze\nthese vectors in a shared \"preference space\". Reader vectors cluster into two\nprofiles: 'surface-focused readers' (mainly non-experts), who prioritize\nreadability and textual richness; and 'holistic readers' (mainly experts), who\nvalue thematic development, rhetorical variety, and sentiment dynamics. Our\nresults quantitatively explain how measurements of literary quality are a\nfunction of how text features align with each reader's preferences. These\nfindings advocate for reader-sensitive evaluation frameworks in the field of\ncreative text generation.", "AI": {"tldr": "This study investigates why different evaluations exist between AI-generated and human-written literary texts, suggesting that these differences stem more from reader interpretation than text quality itself.", "motivation": "To understand why evaluations of AI vs human literary texts vary greatly.", "method": "Extracting textual features, modeling reader preferences, analyzing preference space.", "result": "Readers cluster into 'surface-focused' and 'holistic' profiles with differing priorities.", "conclusion": "Reader-sensitive evaluation frameworks are needed for creative text generation."}}
{"id": "2506.03312", "pdf": "https://arxiv.org/pdf/2506.03312", "abs": "https://arxiv.org/abs/2506.03312", "authors": ["Celia Chen", "Scotty Beland", "Ingo Burghardt", "Jill Byczek", "William J. Conway", "Eric Cotugno", "Sadaf Davre", "Megan Fletcher", "Rajesh Kumar Gnanasekaran", "Kristin Hamilton", "Marilyn Harbert", "Jordan Heustis", "Tanaya Jha", "Emily Klein", "Hayden Kramer", "Alex Leitch", "Jessica Perkins", "Casi Sherman", "Celia Sterrn", "Logan Stevens", "Rebecca Zarrella", "Jennifer Golbeck"], "title": "Cross-Platform Violence Detection on Social Media: A Dataset and Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "In Proceedings of the 17th ACM Web Science Conference (WebSci '25). 9\n  pages", "summary": "Violent threats remain a significant problem across social media platforms.\nUseful, high-quality data facilitates research into the understanding and\ndetection of malicious content, including violence. In this paper, we introduce\na cross-platform dataset of 30,000 posts hand-coded for violent threats and\nsub-types of violence, including political and sexual violence. To evaluate the\nsignal present in this dataset, we perform a machine learning analysis with an\nexisting dataset of violent comments from YouTube. We find that, despite\noriginating from different platforms and using different coding criteria, we\nachieve high classification accuracy both by training on one dataset and\ntesting on the other, and in a merged dataset condition. These results have\nimplications for content-classification strategies and for understanding\nviolent content across social media.", "AI": {"tldr": "This paper presents a cross-platform dataset of 30,000 posts related to violent threats and sub-types of violence. Machine learning analysis shows high classification accuracy across different platforms.", "motivation": "To facilitate research into the understanding and detection of malicious content, including violence, by providing useful, high-quality data.", "method": "Introducing a cross-platform dataset of 30,000 posts hand-coded for violent threats and sub-types of violence, and performing machine learning analysis with an existing dataset of violent comments from YouTube.", "result": "High classification accuracy was achieved both by training on one dataset and testing on the other, and in a merged dataset condition.", "conclusion": "These results have implications for content-classification strategies and for understanding violent content across social media."}}
{"id": "2506.03357", "pdf": "https://arxiv.org/pdf/2506.03357", "abs": "https://arxiv.org/abs/2506.03357", "authors": ["Aldan Creo", "H\u00e9ctor Cerezo-Costas", "Pedro Alonso-Doval", "Maximiliano Hormaz\u00e1bal-Lagos"], "title": "Ask a Local: Detecting Hallucinations With Specialized Model Divergence", "categories": ["cs.CL", "cs.AI"], "comment": "Supplementary materials: https://github.com/ACMCMC/ask-a-local", "summary": "Hallucinations in large language models (LLMs) - instances where models\ngenerate plausible but factually incorrect information - present a significant\nchallenge for AI.\n  We introduce \"Ask a Local\", a novel hallucination detection method exploiting\nthe intuition that specialized models exhibit greater surprise when\nencountering domain-specific inaccuracies. Our approach computes divergence\nbetween perplexity distributions of language-specialized models to identify\npotentially hallucinated spans. Our method is particularly well-suited for a\nmultilingual context, as it naturally scales to multiple languages without the\nneed for adaptation, relying on external data sources, or performing training.\nMoreover, we select computationally efficient models, providing a scalable\nsolution that can be applied to a wide range of languages and domains.\n  Our results on a human-annotated question-answer dataset spanning 14\nlanguages demonstrate consistent performance across languages, with\nIntersection-over-Union (IoU) scores around 0.3 and comparable Spearman\ncorrelation values. Our model shows particularly strong performance on Italian\nand Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining\ncross-lingual effectiveness without language-specific adaptations. We release\nour code and architecture to facilitate further research in multilingual\nhallucination detection.", "AI": {"tldr": "A new method called 'Ask a Local' is introduced to detect hallucinations in large language models. It uses perplexity distribution divergence between language-specialized models to identify potentially wrong information. The method is multilingual, efficient, and does not require training or external data.", "motivation": "To address the challenge of detecting hallucinations in large language models which generate plausible but factually incorrect information.", "method": "The method computes divergence between perplexity distributions of language-specialized models to find potentially hallucinated spans. It selects computationally efficient models and works across different languages without adaptation.", "result": "The method performs consistently across 14 languages with Intersection-over-Union (IoU) scores around 0.3 and similar Spearman correlation values. It especially excels in Italian and Catalan with IoU scores of 0.42 and 0.38, respectively.", "conclusion": "This study presents an effective and scalable solution for multilingual hallucination detection in large language models."}}
{"id": "2506.03360", "pdf": "https://arxiv.org/pdf/2506.03360", "abs": "https://arxiv.org/abs/2506.03360", "authors": ["Zihui Ma", "Lingyao Li", "Juan Li", "Wenyue Hua", "Jingxiao Liu", "Qingyuan Feng", "Yuki Miura"], "title": "A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Rapid, fine-grained disaster damage assessment is essential for effective\nemergency response, yet remains challenging due to limited ground sensors and\ndelays in official reporting. Social media provides a rich, real-time source of\nhuman-centric observations, but its multimodal and unstructured nature presents\nchallenges for traditional analytical methods. In this study, we propose a\nstructured Multimodal, Multilingual, and Multidimensional (3M) pipeline that\nleverages multimodal large language models (MLLMs) to assess disaster impacts.\nWe evaluate three foundation models across two major earthquake events using\nboth macro- and micro-level analyses. Results show that MLLMs effectively\nintegrate image-text signals and demonstrate a strong correlation with\nground-truth seismic data. However, performance varies with language,\nepicentral distance, and input modality. This work highlights the potential of\nMLLMs for disaster assessment and provides a foundation for future research in\napplying MLLMs to real-time crisis contexts. The code and data are released at:\nhttps://github.com/missa7481/EMNLP25_earthquake", "AI": {"tldr": "This study proposes a 3M pipeline using MLLMs for disaster impact assessment, showing effectiveness in integrating image-text signals and correlating with seismic data, but performance varies with different factors.", "motivation": "To address the challenges of limited ground sensors and delays in official reporting during rapid, fine-grained disaster damage assessment.", "method": "A structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that leverages multimodal large language models (MLLMs) to assess disaster impacts.", "result": "Performance varies with language, epicentral distance, and input modality.", "conclusion": "MLLMs can effectively integrate image-text signals and have a strong correlation with ground-truth seismic data."}}
{"id": "2506.03408", "pdf": "https://arxiv.org/pdf/2506.03408", "abs": "https://arxiv.org/abs/2506.03408", "authors": ["Yi Xu", "Ruining Yang", "Yitian Zhang", "Yizhou Wang", "Jianglin Lu", "Mingyuan Zhang", "Lili Su", "Yun Fu"], "title": "Trajectory Prediction Meets Large Language Models: A Survey", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction.", "AI": {"tldr": "This survey reviews recent work on using large language models (LLMs) in trajectory prediction, categorizing them into five directions.", "motivation": "To integrate language-driven techniques into trajectory prediction to leverage the semantic and reasoning capabilities of LLMs.", "method": "Categorizing recent work into five directions: (1) Trajectory prediction via language modeling paradigms, (2) Direct trajectory prediction with pretrained language models, (3) Language-guided scene understanding for trajectory prediction, (4) Language-driven data generation for trajectory prediction, (5) Language-based reasoning and interpretability for trajectory prediction.", "result": "A comprehensive overview of the emerging field of integrating LLMs into trajectory prediction, analyzing representative methods and identifying open challenges.", "conclusion": "This survey bridges natural language processing and trajectory prediction, offering a unified perspective on how language can enrich trajectory prediction."}}
{"id": "2506.03424", "pdf": "https://arxiv.org/pdf/2506.03424", "abs": "https://arxiv.org/abs/2506.03424", "authors": ["Nicole R Schneider", "Nandini Ramachandran", "Kent O'Sullivan", "Hanan Samet"], "title": "DistRAG: Towards Distance-Based Spatial Reasoning in LLMs", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Many real world tasks where Large Language Models (LLMs) can be used require\nspatial reasoning, like Point of Interest (POI) recommendation and itinerary\nplanning. However, on their own LLMs lack reliable spatial reasoning\ncapabilities, especially about distances. To address this problem, we develop a\nnovel approach, DistRAG, that enables an LLM to retrieve relevant spatial\ninformation not explicitly learned during training. Our method encodes the\ngeodesic distances between cities and towns in a graph and retrieves a context\nsubgraph relevant to the question. Using this technique, our method enables an\nLLM to answer distance-based reasoning questions that it otherwise cannot\nanswer. Given the vast array of possible places an LLM could be asked about,\nDistRAG offers a flexible first step towards providing a rudimentary `world\nmodel' to complement the linguistic knowledge held in LLMs.", "AI": {"tldr": "A novel approach called DistRAG is developed to enable Large Language Models (LLMs) to retrieve relevant spatial information for tasks requiring spatial reasoning.", "motivation": "LLMs lack reliable spatial reasoning capabilities, especially about distances, which is needed for tasks like POI recommendation and itinerary planning.", "method": "DistRAG encodes geodesic distances between locations in a graph and retrieves a context subgraph related to the question.", "result": "The method allows LLMs to answer distance-based reasoning questions they couldn't answer before.", "conclusion": "DistRAG provides a flexible way to give LLMs a basic 'world model' to complement their linguistic knowledge."}}
{"id": "2506.03434", "pdf": "https://arxiv.org/pdf/2506.03434", "abs": "https://arxiv.org/abs/2506.03434", "authors": ["Ahmad Dawar Hakimi", "Ali Modarressi", "Philipp Wicke", "Hinrich Sch\u00fctze"], "title": "Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Understanding how large language models (LLMs) acquire and store factual\nknowledge is crucial for enhancing their interpretability and reliability. In\nthis work, we analyze the evolution of factual knowledge representation in the\nOLMo-7B model by tracking the roles of its attention heads and feed forward\nnetworks (FFNs) over the course of pre-training. We classify these components\ninto four roles: general, entity, relation-answer, and fact-answer specific,\nand examine their stability and transitions. Our results show that LLMs\ninitially depend on broad, general-purpose components, which later specialize\nas training progresses. Once the model reliably predicts answers, some\ncomponents are repurposed, suggesting an adaptive learning process. Notably,\nattention heads display the highest turnover. We also present evidence that\nFFNs remain more stable throughout training. Furthermore, our probing\nexperiments reveal that location-based relations converge to high accuracy\nearlier in training than name-based relations, highlighting how task complexity\nshapes acquisition dynamics. These insights offer a mechanistic view of\nknowledge formation in LLMs.", "AI": {"tldr": "This study examines how the OLMo-7B large language model evolves its factual knowledge representation during pre-training by analyzing the roles of its attention heads and feed-forward networks. Results indicate initial reliance on general-purpose components that later specialize, with attention heads showing higher turnover compared to FFNs. Name-based relations take longer to converge than location-based ones, illustrating the impact of task complexity on knowledge acquisition.", "motivation": "Enhancing the interpretability and reliability of large language models by understanding how they acquire and store factual knowledge.", "method": "Tracking the roles of attention heads and feed-forward networks in the OLMo-7B model during pre-training stages.", "result": "LLMs initially rely on general-purpose components that become specialized as training advances. Some components are repurposed once reliable predictions are made, suggesting an adaptive learning process. Attention heads have higher turnover than FFNs, and location-based relations converge faster than name-based ones.", "conclusion": "The study provides insights into the mechanism of knowledge formation in large language models, emphasizing the role of component specialization and task complexity."}}
{"id": "2506.03458", "pdf": "https://arxiv.org/pdf/2506.03458", "abs": "https://arxiv.org/abs/2506.03458", "authors": ["Zahra Bokaei", "Walid Magdy", "Bonnie Webber"], "title": "Culture Matters in Toxic Language Detection in Persian", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "Toxic language detection is crucial for creating safer online environments\nand limiting the spread of harmful content. While toxic language detection has\nbeen under-explored in Persian, the current work compares different methods for\nthis task, including fine-tuning, data enrichment, zero-shot and few-shot\nlearning, and cross-lingual transfer learning. What is especially compelling is\nthe impact of cultural context on transfer learning for this task: We show that\nthe language of a country with cultural similarities to Persian yields better\nresults in transfer learning. Conversely, the improvement is lower when the\nlanguage comes from a culturally distinct country. Warning: This paper contains\nexamples of toxic language that may disturb some readers. These examples are\nincluded for the purpose of research on toxic detection.", "AI": {"tldr": "This study investigates various methods for detecting toxic language in Persian, highlighting the influence of cultural context on cross-lingual transfer learning performance.", "motivation": "Creating safer online environments by limiting the spread of harmful content in Persian.", "method": "Comparing fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning methods.", "result": "Cultural context significantly affects the performance of transfer learning for toxic language detection.", "conclusion": "Culturally similar languages yield better results in transfer learning for toxic language detection compared to culturally distinct ones."}}
{"id": "2506.03476", "pdf": "https://arxiv.org/pdf/2506.03476", "abs": "https://arxiv.org/abs/2506.03476", "authors": ["Chuyuan Li", "Raymond Li", "Thalia S. Field", "Giuseppe Carenini"], "title": "Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection", "categories": ["cs.CL"], "comment": null, "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that\nleads to dementia, and early intervention can greatly benefit from analyzing\nlinguistic abnormalities. In this work, we explore the potential of Large\nLanguage Models (LLMs) as health assistants for AD diagnosis from\npatient-generated text using in-context learning (ICL), where tasks are defined\nthrough a few input-output examples. Empirical results reveal that conventional\nICL methods, such as similarity-based selection, perform poorly for AD\ndiagnosis, likely due to the inherent complexity of this task. To address this,\nwe introduce Delta-KNN, a novel demonstration selection strategy that enhances\nICL performance. Our method leverages a delta score to assess the relative\ngains of each training example, coupled with a KNN-based retriever that\ndynamically selects optimal \"representatives\" for a given input. Experiments on\ntwo AD detection datasets across three open-source LLMs demonstrate that\nDelta-KNN consistently outperforms existing ICL baselines. Notably, when using\nthe Llama-3.1 model, our approach achieves new state-of-the-art results,\nsurpassing even supervised classifiers.", "AI": {"tldr": "Use large language models to detect Alzheimer's disease through patient-generated text by introducing a new demonstration selection strategy.", "motivation": "Detect linguistic abnormalities in patients with Alzheimer's disease for early intervention.", "method": "Use large language models with in-context learning and introduce a new demonstration selection strategy called Delta-KNN.", "result": "Delta-KNN outperforms existing in-context learning baselines and even surpasses supervised classifiers in certain cases.", "conclusion": "Delta-KNN is an effective method for AD diagnosis using patient-generated text and large language models."}}
{"id": "2506.03483", "pdf": "https://arxiv.org/pdf/2506.03483", "abs": "https://arxiv.org/abs/2506.03483", "authors": ["Jun Rao", "Zepeng Lin", "Xuebo Liu", "Xiaopeng Ke", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "Large Language Models (LLMs) often require domain-specific fine-tuning to\naddress targeted tasks, which risks degrading their general capabilities.\nMaintaining a balance between domain-specific enhancements and general model\nutility is a key challenge. This paper proposes a novel approach named APT\n(Weakness Case Acquisition and Iterative Preference Training) to enhance\ndomain-specific performance with self-generated dis-preferred weakness data\n(bad cases and similar cases). APT uniquely focuses on training the model using\nonly those samples where errors occur, alongside a small, similar set of\nsamples retrieved for this purpose. This targeted training minimizes\ninterference with the model's existing knowledge base, effectively retaining\ngeneric capabilities. Experimental results on the LLama-2 and Mistral-V0.3\nmodels across various benchmarks demonstrate that APT ensures no reduction in\ngeneric capacity and achieves superior performance on downstream tasks compared\nto various existing methods. This validates our method as an effective strategy\nfor enhancing domain-specific capabilities without sacrificing the model's\nbroader applicability.", "AI": {"tldr": "APT\u662f\u4e00\u79cd\u589e\u5f3a\u9886\u57df\u7279\u5b9a\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u751f\u6210\u7684\u5f31\u6570\u636e\u8fdb\u884c\u8fed\u4ee3\u504f\u597d\u8bad\u7ec3\uff0c\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660eAPT\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4fdd\u6301\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u589e\u5f3a\u5176\u9886\u57df\u7279\u5b9a\u6027\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "APT\u901a\u8fc7\u4f7f\u7528\u4ec5\u5305\u542b\u9519\u8bef\u53d1\u751f\u7684\u6837\u672c\u4ee5\u53ca\u5c11\u91cf\u68c0\u7d22\u5230\u7684\u76f8\u4f3c\u6837\u672c\u8fdb\u884c\u9488\u5bf9\u6027\u8bad\u7ec3\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "result": "APT\u5728LLama-2\u548cMistral-V0.3\u6a21\u578b\u4e0a\u901a\u8fc7\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u793a\u51fa\u4e86\u4f18\u8d8a\u7684\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "APT\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u9886\u57df\u7279\u5b9a\u6027\u80fd\u800c\u4e0d\u635f\u5bb3\u6a21\u578b\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.03484", "pdf": "https://arxiv.org/pdf/2506.03484", "abs": "https://arxiv.org/abs/2506.03484", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Atnafu Lambebo Tonja", "Hassan Shakil", "Samer Iskander", "Olga Kolesnikova", "Jugal Kalita"], "title": "Explainable AI: XAI-Guided Context-Aware Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Explainable AI (XAI) has emerged as a powerful tool for improving the\nperformance of AI models, going beyond providing model transparency and\ninterpretability. The scarcity of labeled data remains a fundamental challenge\nin developing robust and generalizable AI models, particularly for low-resource\nlanguages. Conventional data augmentation techniques introduce noise, cause\nsemantic drift, disrupt contextual coherence, lack control, and lead to\noverfitting. To address these challenges, we propose XAI-Guided Context-Aware\nData Augmentation. This novel framework leverages XAI techniques to modify less\ncritical features while selectively preserving most task-relevant features. Our\napproach integrates an iterative feedback loop, which refines augmented data\nover multiple augmentation cycles based on explainability-driven insights and\nthe model performance gain. Our experimental results demonstrate that XAI-SR-BT\nand XAI-PR-BT improve the accuracy of models on hate speech and sentiment\nanalysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using\nthe Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform\nexisting augmentation techniques by 4.8% and 5%, respectively, on the same\ndataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform\nboth baseline and conventional augmentation techniques across all tasks and\nmodels. This study provides a more controlled, interpretable, and context-aware\nsolution to data augmentation, addressing critical limitations of existing\naugmentation techniques and offering a new paradigm shift for leveraging XAI\ntechniques to enhance AI model training.", "AI": {"tldr": "This paper introduces XAI-Guided Context-Aware Data Augmentation, a novel approach that uses Explainable AI (XAI) techniques to improve data augmentation for low-resource languages. It demonstrates improved accuracy in hate speech and sentiment analysis tasks compared to traditional methods.", "motivation": "The scarcity of labeled data is a major challenge in developing robust AI models, especially for low-resource languages. Conventional data augmentation techniques often introduce noise, cause semantic drift, disrupt context, lack control, and result in overfitting.", "method": "XAI-Guided Context-Aware Data Augmentation modifies less critical features while preserving task-relevant ones, using an iterative feedback loop to refine augmented data over multiple cycles based on explainability-driven insights and performance gains.", "result": "XAI-SR-BT and XAI-PR-BT showed 6.6% and 8.1% improvements in accuracy on hate speech and sentiment analysis tasks, respectively, compared to the baseline using the Amharic dataset and XLM-R model. These methods outperformed existing techniques by 4.8% and 5%.", "conclusion": "This study offers a more controlled, interpretable, and context-aware solution to data augmentation, overcoming limitations of existing methods and suggesting a new way to use XAI to enhance AI model training."}}
{"id": "2506.03489", "pdf": "https://arxiv.org/pdf/2506.03489", "abs": "https://arxiv.org/abs/2506.03489", "authors": ["Mingxu Tao", "Jie Hu", "Mingchuan Yang", "Yunhuai Liu", "Dongyan Zhao", "Yansong Feng"], "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "The remarkable performance of Large language models (LLMs) relies heavily on\nthe availability of abundant high-quality training data. However, the high cost\nof acquiring annotated data often prevents models from obtaining capabilities\nto tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe\nthat boosts model performance in data-scarcity scenarios without extra\ntraining. We first employ model extrapolation to enhance a finetuned model with\nits inferior version, and then adopt contrastive decoding to further reduce\npredicted errors, by comparing the logit scores given by the extrapolated and\nthe vanilla finetuned model. Experiments across three tasks over four different\nLLMs show that EpiCoDe consistently outperforms existing methods with\nsignificant and robust improvement. We also propose a new theoretical framework\nto reveal the mechanism behind contrastive decoding in data-scarcity scenarios,\nwhich further helps us better understand the effectiveness of EpiCoDe.", "AI": {"tldr": "A novel method named EpiCoDe is introduced to improve model performance in data-scarcity scenarios without extra training.", "motivation": "High cost of acquiring annotated data often prevents models from obtaining capabilities to tackle downstream tasks.", "method": "Employ model extrapolation to enhance a finetuned model with its inferior version, and then adopt contrastive decoding to further reduce predicted errors.", "result": "EpiCoDe consistently outperforms existing methods with significant and robust improvement.", "conclusion": "A new theoretical framework is proposed to reveal the mechanism behind contrastive decoding in data-scarcity scenarios."}}
{"id": "2506.03490", "pdf": "https://arxiv.org/pdf/2506.03490", "abs": "https://arxiv.org/abs/2506.03490", "authors": ["Shigeng Chen", "Linhao Luo", "Zhangchi Qiu", "Yanan Cao", "Carl Yang", "Shirui Pan"], "title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "categories": ["cs.CL"], "comment": null, "summary": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications.", "AI": {"tldr": "MedEditBench\u8bc4\u4f30\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u533b\u5b66\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u65b9\u6cd5\u4ec5\u5b9e\u73b0\u6d45\u5c42\u8bb0\u5fc6\uff0c\u63d0\u51faSGR-Edit\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u8bc4\u4f30\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u533b\u5b66\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u89e3\u51b3\u590d\u6742\u533b\u5b66\u573a\u666f\u4e0b\u77e5\u8bc6\u5185\u5316\u548c\u6cdb\u5316\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165MedEditBench\u53ca\u5176\u5305\u542b\u7684\u65b0\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\u548c\u4e09\u79cd\u7f16\u8f91\u8303\u5f0f\uff1b\u63d0\u51faSGR-Edit\u5229\u7528\u6a21\u578b\u63a8\u5bfc\u7684\u63a8\u7406\u4f5c\u4e3a\u76ee\u6807\u77e5\u8bc6\u8fdb\u884c\u7f16\u8f91\u3002", "result": "\u5f53\u524d\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u65b0\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1bSGR-Edit\u5c55\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u63ed\u793a\u4e86\u533b\u5b66\u77e5\u8bc6\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5b9a\u4f4d\uff0c\u63a2\u8ba8\u4e86\u987a\u5e8f\u7f16\u8f91\u5bf9\u6f14\u8fdb\u77e5\u8bc6\u7684\u5f71\u54cd\uff0c\u4e3a\u5b9e\u9645\u533b\u5b66\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2506.03501", "pdf": "https://arxiv.org/pdf/2506.03501", "abs": "https://arxiv.org/abs/2506.03501", "authors": ["Yuchen Guo", "Zhicheng Dou", "Huy H. Nguyen", "Ching-Chun Chang", "Saku Sugawara", "Isao Echizen"], "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing", "categories": ["cs.CL", "cs.AI"], "comment": "IJCNN2025 accepted", "summary": "Content creation has dramatically progressed with the rapid advancement of\nlarge language models like ChatGPT and Claude. While this progress has greatly\nenhanced various aspects of life and work, it has also negatively affected\ncertain areas of society. A recent survey revealed that nearly 30% of college\nstudents use generative AI to help write academic papers and reports. Most\ncountermeasures treat the detection of AI-generated text as a binary\nclassification task and thus lack robustness. This approach overlooks human\ninvolvement in the generation of content even though human-machine\ncollaboration is becoming mainstream. Besides generating entire texts, people\nmay use machines to complete or revise texts. Such human involvement varies\ncase by case, which makes binary classification a less than satisfactory\napproach. We refer to this situation as participation detection obfuscation. We\npropose using BERTScore as a metric to measure human involvement in the\ngeneration process and a multi-task RoBERTa-based regressor trained on a token\nclassification task to address this problem. To evaluate the effectiveness of\nthis approach, we simulated academic-based scenarios and created a continuous\ndataset reflecting various levels of human involvement. All of the existing\ndetectors we examined failed to detect the level of human involvement on this\ndataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor\nmean squared error of 0.004). Moreover, it demonstrated some generalizability\nacross generative models. Our code is available at\nhttps://github.com/gyc-nii/CAS-CS-and-dual-head-detector", "AI": {"tldr": "This paper proposes a new method using BERTScore and a multi-task RoBERTa-based regressor to effectively detect the level of human involvement in the generation process of AI-generated text.", "motivation": "The current countermeasures for detecting AI-generated text lack robustness as they treat it as a binary classification task and overlook human involvement in the generation process.", "method": "We propose using BERTScore as a metric to measure human involvement in the generation process and a multi-task RoBERTa-based regressor trained on a token classification task.", "result": "Our method succeeded with an F1 score of 0.9423 and a regressor mean squared error of 0.004. It also showed some generalizability across generative models.", "conclusion": "Our proposed method using BERTScore and a multi-task RoBERTa-based regressor can effectively detect the level of human involvement in the generation process."}}
{"id": "2506.03510", "pdf": "https://arxiv.org/pdf/2506.03510", "abs": "https://arxiv.org/abs/2506.03510", "authors": ["Seungcheol Park", "Sojin Lee", "Jongjin Kim", "Jinsik Lee", "Hyunjik Jo", "U Kang"], "title": "Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "IJCAI 2025 Main Track", "summary": "How can we accelerate large language models(LLMs) without sacrificing\naccuracy? The slow inference speed of LLMs hinders us to benefit from their\nremarkable performance in diverse applications. This is mainly because numerous\nsublayers are stacked together in LLMs. Sublayer pruning compresses and\nexpedites LLMs via removing unnecessary sublayers. However, existing sublayer\npruning algorithms are limited in accuracy since they naively select sublayers\nto prune, overlooking the different characteristics of each sublayer. In this\npaper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability\nInformation), an accurate sublayer pruning method for LLMs. SPRINT accurately\nselects a target sublayer to prune by considering 1) the amount of latency\nreduction after pruning and 2) the tunability of sublayers. SPRINT iteratively\nprunes redundant sublayers and swiftly tunes the parameters of remaining\nsublayers. Experiments show that SPRINT achieves the best accuracy-speedup\ntrade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense\nreasoning benchmarks compared to existing pruning algorithms.", "AI": {"tldr": "SPRINT accelerates large language models by pruning unnecessary sublayers while maintaining accuracy.", "motivation": "Accelerating large language models without sacrificing accuracy.", "method": "Proposes SPRINT, which considers latency reduction and tunability of sublayers to select targets for pruning.", "result": "SPRINT improves accuracy-speedup trade-off, with up to 23.88%p higher accuracy on zero-shot commonsense reasoning benchmarks than existing methods.", "conclusion": "SPRINT is an effective sublayer pruning method for accelerating large language models."}}
{"id": "2506.03519", "pdf": "https://arxiv.org/pdf/2506.03519", "abs": "https://arxiv.org/abs/2506.03519", "authors": ["Yangyang Zhao", "Ben Niu", "Libo Qin", "Shihan Wang"], "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals", "categories": ["cs.CL"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks.", "AI": {"tldr": "This paper combines Deep Reinforcement Learning and Evolutionary Algorithms to address the issue of balancing exploration and exploitation in task-oriented dialogue systems.", "motivation": "The high dimensionality of state and action spaces in task-oriented dialogue systems leads to local optima or poor convergence when using only Deep Reinforcement Learning.", "method": "Innovative combination of the global search capabilities of Evolutionary Algorithms with the local optimization of Deep Reinforcement Learning, along with an elite individual injection mechanism for enhancing search efficiency.", "result": "Experiments on four datasets showed improved performance and demonstrated the effectiveness of the proposed elite individual injection mechanism in reducing exploration time.", "conclusion": "The proposed approach achieves an efficient integration of Evolutionary Algorithms and Deep Reinforcement Learning for task-oriented dialogue policy tasks."}}
{"id": "2506.03523", "pdf": "https://arxiv.org/pdf/2506.03523", "abs": "https://arxiv.org/abs/2506.03523", "authors": ["Chong Li", "Jiajun Zhang", "Chengqing Zong"], "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment", "categories": ["cs.CL"], "comment": "ACL 2025, our codes and models are available at\n  https://github.com/ZNLP/TokAlign", "summary": "Tokenization serves as a foundational step for Large Language Models (LLMs)\nto process text. In new domains or languages, the inefficiency of the tokenizer\nwill slow down the training and generation of LLM. The mismatch in vocabulary\nalso hinders deep knowledge transfer between LLMs like token-level\ndistillation. To mitigate this gap, we propose an efficient method named\nTokAlign to replace the vocabulary of LLM from the token co-occurrences view,\nand further transfer the token-level knowledge between models. It first aligns\nthe source vocabulary to the target one by learning a one-to-one mapping matrix\nfor token IDs. Model parameters, including embeddings, are rearranged and\nprogressively fine-tuned for the new vocabulary. Our method significantly\nimproves multilingual text compression rates and vocabulary initialization for\nLLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods\nto 1.2$\\text{e}^2$ after initialization. Experimental results on models across\nmultiple parameter scales demonstrate the effectiveness and generalization of\nTokAlign, which costs as few as 5k steps to restore the performance of the\nvanilla model. After unifying vocabularies between LLMs, token-level\ndistillation can remarkably boost (+4.4% than sentence-level distillation) the\nbase model, costing only 235M tokens.", "AI": {"tldr": "TokAlign efficiently replaces and aligns vocabularies for LLMs, improving multilingual text processing and knowledge transfer.", "motivation": "Inefficient tokenization slows down LLM training and generation, and mismatches in vocabulary hinder knowledge transfer between models.", "method": "TokAlign learns a one-to-one mapping matrix for token IDs, rearranges model parameters, and progressively fine-tunes them for the new vocabulary.", "result": "TokAlign reduces perplexity from 3.4e2 to 1.2e2 after initialization and boosts token-level distillation by 4.4% over sentence-level distillation.", "conclusion": "The proposed TokAlign method improves multilingual text compression rates and vocabulary initialization for LLMs, enhancing model performance with fewer training steps."}}
{"id": "2506.03524", "pdf": "https://arxiv.org/pdf/2506.03524", "abs": "https://arxiv.org/abs/2506.03524", "authors": ["Yuyu Zhang", "Jing Su", "Yifan Sun", "Chenguang Xi", "Xia Xiao", "Shen Zheng", "Anxiang Zhang", "Kaibo Liu", "Daoguang Zan", "Tao Sun", "Jinhua Zhu", "Shulin Xin", "Dong Huang", "Yetao Bai", "Lixin Dong", "Chao Li", "Jianchong Chen", "Hanzhi Zhou", "Yifan Huang", "Guanghan Ning", "Xierui Song", "Jiaze Chen", "Siyao Liu", "Kai Shen", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Coder: Let the Code Model Curate Data for Itself", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Code data in large language model (LLM) pretraining is recognized crucial not\nonly for code-related tasks but also for enhancing general intelligence of\nLLMs. Current open-source LLMs often heavily rely on human effort to produce\ntheir code pretraining data, such as employing hand-crafted filtering rules\ntailored to individual programming languages, or using human-annotated data to\ntrain quality filters. However, these approaches are inherently limited in\nscalability, prone to subjective biases, and costly to extend and maintain\nacross diverse programming languages. To address these challenges, we introduce\nSeed-Coder, a series of open-source LLMs comprising base, instruct and\nreasoning models of 8B size, minimizing human involvement in data construction.\nOur code pretraining data is produced by a model-centric data pipeline, which\npredominantly leverages LLMs for scoring and filtering code data. The instruct\nmodel is further trained via supervised fine-tuning and preference\noptimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)\nreinforcement learning to improve multi-step code reasoning. Seed-Coder\nachieves state-of-the-art results among open-source models of similar size and\neven surpasses some much larger models, demonstrating superior performance in\ncode generation, code completion, code editing, code reasoning, and software\nengineering tasks.", "AI": {"tldr": "Seed-Coder\u662f\u4e00\u79cd\u65b0\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u5217\uff0c\u5305\u62ec\u57fa\u5ea7\u3001\u6307\u4ee4\u548c\u63a8\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u4ee3\u7801\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u4ece\u800c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u3001\u5b8c\u6210\u3001\u7f16\u8f91\u3001\u63a8\u7406\u548c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u9884\u8bad\u7ec3\u6570\u636e\u4e0a\u4f9d\u8d56\u5927\u91cf\u4eba\u529b\uff0c\u5982\u624b\u5de5\u5236\u4f5c\u7684\u8fc7\u6ee4\u89c4\u5219\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u4e3b\u89c2\u504f\u5dee\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002", "method": "Seed-Coder\u901a\u8fc7\u6a21\u578b\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u7ba1\u9053\u751f\u6210\u4ee3\u7801\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5229\u7528LLMs\u8fdb\u884c\u8bc4\u5206\u548c\u8fc7\u6ee4\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u3001\u504f\u597d\u4f18\u5316\u548c\u957f\u94fe\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u6765\u8bad\u7ec3\u6307\u4ee4\u548c\u63a8\u7406\u6a21\u578b\u3002", "result": "Seed-Coder\u5728\u4ee3\u7801\u751f\u6210\u3001\u5b8c\u6210\u3001\u7f16\u8f91\u3001\u63a8\u7406\u548c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u4e2d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u67d0\u4e9b\u66f4\u5927\u7684\u6a21\u578b\u3002", "conclusion": "Seed-Coder\u5c55\u793a\u4e86\u5728\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528LLMs\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u9884\u8bad\u7ec3\u6570\u636e\u7684\u80fd\u529b\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u7684\u901a\u7528\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2506.03533", "pdf": "https://arxiv.org/pdf/2506.03533", "abs": "https://arxiv.org/abs/2506.03533", "authors": ["Apurva Gandhi", "Graham Neubig"], "title": "Go-Browse: Training Web Agents with Structured Exploration", "categories": ["cs.CL"], "comment": null, "summary": "One of the fundamental problems in digital agents is their lack of\nunderstanding of their environment. For instance, a web browsing agent may get\nlost in unfamiliar websites, uncertain what pages must be visited to achieve\nits goals. To address this, we propose Go-Browse, a method for automatically\ncollecting diverse and realistic web agent data at scale through structured\nexploration of web environments. Go-Browse achieves efficient exploration by\nframing data collection as a graph search, enabling reuse of information across\nexploration episodes. We instantiate our method on the WebArena benchmark,\ncollecting a dataset of 10K successful task-solving trajectories and 40K\ninteraction steps across 100 URLs. Fine-tuning a 7B parameter language model on\nthis dataset achieves a success rate of 21.7% on the WebArena benchmark,\nbeating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for\nsub-10B parameter models by 2.9%.", "AI": {"tldr": "This paper introduces Go-Browse, a method for collecting large-scale web agent data via structured exploration, achieving better performance than existing methods.", "motivation": "The lack of understanding of their environment by digital agents, such as getting lost in unfamiliar websites, motivates the need for better data collection methods.", "method": "Go-Browse frames data collection as a graph search, allowing for efficient exploration and reuse of information across episodes.", "result": "On the WebArena benchmark, fine-tuning a 7B parameter language model on the collected dataset achieved a success rate of 21.7%, outperforming GPT-4o mini and other sub-10B parameter models.", "conclusion": "Go-Browse provides an effective way to collect diverse and realistic web agent data, leading to improved performance in web navigation tasks."}}
{"id": "2506.03541", "pdf": "https://arxiv.org/pdf/2506.03541", "abs": "https://arxiv.org/abs/2506.03541", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "summary": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "AI": {"tldr": "This paper presents a novel Debate and Reflect framework that orchestrates multi-turn debates between smaller models and stronger teacher models, which is followed by Tree-structured Direct Preference Optimization to efficiently leverage these debate logs. The approach improves smaller-model accuracy, robustness, and generalization.", "motivation": "Current techniques struggle to yield substantial and lasting performance gains in distilling large language models into smaller ones.", "method": "Debate and Reflect framework with multi-turn debates and Tree-structured Direct Preference Optimization.", "result": "Significantly improves smaller-model accuracy, robustness, and generalization.", "conclusion": "Our approach outperforms conventional baselines by a large margin."}}
{"id": "2506.03557", "pdf": "https://arxiv.org/pdf/2506.03557", "abs": "https://arxiv.org/abs/2506.03557", "authors": ["Lin Sun", "Chuang Liu", "Peng Liu", "Bingyang Li", "Weijia Lu", "Ning Wu"], "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) have emerged as a popular method for\naligning Large Language Models (LLMs) with human preferences. While DPO\neffectively preserves the relative ordering between chosen and rejected\nresponses through pairwise ranking losses, it often neglects absolute reward\nmagnitudes. This oversight can decrease the likelihood of chosen responses and\nincrease the risk of generating out-of-distribution responses, leading to poor\nperformance. We term this issue Degraded Chosen Responses (DCR).To address this\nissue, we propose Balanced Preference Optimization (BPO), a novel framework\nthat dynamically balances the optimization of chosen and rejected responses\nthrough two key components: balanced reward margin and gap adaptor. Unlike\nprevious methods, BPO can fundamentally resolve DPO's DCR issue, without\nintroducing additional constraints to the loss function. Experimental results\non multiple mathematical reasoning tasks show that BPO significantly\noutperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%\nto 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses\nDPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over\nCal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a\nsingle line of code modification, making it simple to implement and fully\ncompatible with existing DPO-based frameworks.", "AI": {"tldr": "Proposed Balanced Preference Optimization (BPO) to improve Large Language Models' performance in mathematical reasoning tasks by addressing the degraded chosen responses issue of Direct Preference Optimization (DPO).", "motivation": "DPO neglects absolute reward magnitudes which can lead to degraded chosen responses and poor performance.", "method": "Balanced Preference Optimization (BPO) with two key components: balanced reward margin and gap adaptor.", "result": "BPO outperforms DPO and its variants in multiple mathematical reasoning tasks.", "conclusion": "BPO effectively addresses the DCR issue of DPO and improves the performance of LLMs in mathematical reasoning tasks."}}
{"id": "2506.03558", "pdf": "https://arxiv.org/pdf/2506.03558", "abs": "https://arxiv.org/abs/2506.03558", "authors": ["Jiawei Chen", "Xinyan Guan", "Qianhao Yuan", "Guozhao Mo", "Weixiang Zhou", "Yaojie Lu", "Hongyu Lin", "Ben He", "Le Sun", "Xianpei Han"], "title": "ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch", "categories": ["cs.CL"], "comment": null, "summary": "Current instruction data synthesis methods primarily focus on single-turn\ninstructions and often neglect cross-turn coherence, resulting in context drift\nand reduced task completion rates in extended conversations. To address this\nlimitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a\nframework that constrains multi-turn instruction synthesis by explicitly\nmodeling human conversational intent. It operates in two stages: (1) Intent\nModeling, which captures the global structure of human dialogues by assigning\neach conversation to one of nine well-defined intent trajectories, ensuring a\ncoherent and goal-oriented information flow; and (2) Skeleton Generation, which\nconstructs a structurally grounded sequence of user queries aligned with the\nmodeled intent, thereby serving as a scaffold that constrains and guides the\ndownstream instruction synthesis process. Based on this process, we construct\nConsistentChat, a multi-turn instruction dataset with approximately 15,000\nmulti-turn conversations and 224,392 utterances. Experiments on the Light,\nTopdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat\nachieve a 20-30% improvement in chat consistency and up to a 15% increase in\ntask success rate, significantly outperforming models trained on existing\nsingle-turn and multi-turn instruction datasets.", "AI": {"tldr": "A framework named Skeleton-Guided Multi-Turn Dialogue Generation is proposed to improve cross-turn coherence in multi-turn instruction synthesis. A new dataset named ConsistentChat is constructed for model training.", "motivation": "Current instruction data synthesis methods neglect cross-turn coherence, leading to context drift and reduced task completion rates in extended conversations.", "method": "The framework includes two stages: Intent Modeling and Skeleton Generation. Intent Modeling captures the global structure of human dialogues, while Skeleton Generation constructs a structurally grounded sequence of user queries aligned with the modeled intent.", "result": "Experiments on three benchmarks show that models fine-tuned on ConsistentChat achieve significant improvements in chat consistency and task success rate compared to models trained on existing datasets.", "conclusion": "The proposed framework and dataset can effectively enhance the performance of multi-turn dialogue generation models."}}
{"id": "2506.03566", "pdf": "https://arxiv.org/pdf/2506.03566", "abs": "https://arxiv.org/abs/2506.03566", "authors": ["Langlin Huang", "Chengsong Huang", "Jixuan Leng", "Di Huang", "Jiaxin Huang"], "title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5PosS\uff0c\u901a\u8fc7\u4f7f\u7528\u4e13\u95e8\u5904\u7406\u7279\u5b9a\u4f4d\u7f6e\u7684\u8349\u7a3f\u5c42\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u540e\u671f\u4f4d\u7f6e\u7684\u8349\u7a3ftoken\u9884\u6d4b\u8d28\u91cf\u4e0b\u964d\uff0c\u56e0\u4e3a\u8349\u7a3f\u6a21\u578b\u751f\u6210\u7279\u5f81\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u3002", "method": "\u5f15\u5165\u4e86\u4f4d\u7f6e\u4e13\u5bb6\uff08PosS\uff09\uff0c\u7531\u591a\u4e2a\u4e13\u95e8\u5904\u7406\u7279\u5b9a\u4f4d\u7f6e\u7684\u8349\u7a3f\u5c42\u7ec4\u6210\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u4e13\u6ce8\u4e8e\u5904\u7406\u4e00\u5b9a\u6c34\u5e73\u7684\u8349\u7a3f\u6a21\u578b\u7279\u5f81\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684PosS\u5728Llama-3-8B-Instruct\u548cLlama-2-13B-chat\u4e0a\u5bf9\u516d\u4e2a\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f4d\u7f6e\u4e13\u5bb6\uff08PosS\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u540e\u671f\u4f4d\u7f6e\u7684token\u63a5\u53d7\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6709\u6548\u6539\u8fdb\u4e86\u57fa\u51c6\u6a21\u578b\u7684\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u548c\u52a0\u901f\u6bd4\u3002"}}
{"id": "2506.03569", "pdf": "https://arxiv.org/pdf/2506.03569", "abs": "https://arxiv.org/abs/2506.03569", "authors": ["Xiaomi LLM-Core Team", ":", "Zihao Yue", "Zhenru Lin", "Yifan Song", "Weikun Wang", "Shuhuai Ren", "Shuhao Gu", "Shicheng Li", "Peidian Li", "Liang Zhao", "Lei Li", "Kainan Bao", "Hao Tian", "Hailin Zhang", "Gang Wang", "Dawei Zhu", "Cici", "Chenhong He", "Bowen Ye", "Bowen Shen", "Zihan Zhang", "Zihan Jiang", "Zhixian Zheng", "Zhichao Song", "Zhenbo Luo", "Yue Yu", "Yudong Wang", "Yuanyuan Tian", "Yu Tu", "Yihan Yan", "Yi Huang", "Xu Wang", "Xinzhe Xu", "Xingchen Song", "Xing Zhang", "Xing Yong", "Xin Zhang", "Xiangwei Deng", "Wenyu Yang", "Wenhan Ma", "Weiwei Lv", "Weiji Zhuang", "Wei Liu", "Sirui Deng", "Shuo Liu", "Shimao Chen", "Shihua Yu", "Shaohui Liu", "Shande Wang", "Rui Ma", "Qiantong Wang", "Peng Wang", "Nuo Chen", "Menghang Zhu", "Kangyang Zhou", "Kang Zhou", "Kai Fang", "Jun Shi", "Jinhao Dong", "Jiebao Xiao", "Jiaming Xu", "Huaqiu Liu", "Hongshen Xu", "Heng Qu", "Haochen Zhao", "Hanglong Lv", "Guoan Wang", "Duo Zhang", "Dong Zhang", "Di Zhang", "Chong Ma", "Chang Liu", "Can Cai", "Bingquan Xia"], "title": "MiMo-VL Technical Report", "categories": ["cs.CL"], "comment": "32 pages", "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.", "AI": {"tldr": "Two advanced vision-language models achieve state-of-the-art performance through innovative training methods and comprehensive evaluation.", "motivation": "To develop powerful vision-language models that excel in general visual understanding and multimodal reasoning.", "method": "Four-stage pre-training with 2.4 trillion tokens combined with Mixed On-policy Reinforcement Learning (MORL).", "result": "MiMo-VL-7B-RL outperforms other models including Qwen2.5-VL-7B on most tasks and achieves high scores on various benchmarks like OlympiadBench and OSWorld-G.", "conclusion": "MiMo-VL-7B-SFT and MiMo-VL-7B-RL demonstrate superior performance in vision-language understanding and reasoning tasks."}}
{"id": "2506.03570", "pdf": "https://arxiv.org/pdf/2506.03570", "abs": "https://arxiv.org/abs/2506.03570", "authors": ["Lin Sun", "Chuang Liu", "Xiaofeng Ma", "Tao Yang", "Weijia Lu", "Ning Wu"], "title": "FreePRM: Training Process Reward Models Without Ground Truth Process Labels", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated that\nProcess Reward Models (PRMs) play a crucial role in enhancing model\nperformance. However, training PRMs typically requires step-level labels,\neither manually annotated or automatically generated, which can be costly and\ndifficult to obtain at scale. To address this challenge, we introduce FreePRM,\na weakly supervised framework for training PRMs without access to ground-truth\nstep-level labels. FreePRM first generates pseudo step-level labels based on\nthe correctness of final outcome, and then employs Buffer Probability to\neliminate impact of noise inherent in pseudo labeling. Experimental results\nshow that FreePRM achieves an average F1 score of 53.0% on ProcessBench,\noutperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared\nto other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B\n(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by\n+10.9%. This work introduces a new paradigm in PRM training, significantly\nreducing reliance on costly step-level annotations while maintaining strong\nperformance.", "AI": {"tldr": "\u63d0\u51faFreePRM\u6846\u67b6\uff0c\u65e0\u9700\u771f\u5b9e\u6b65\u7ea7\u6807\u7b7e\u5373\u53ef\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\uff0c\u901a\u8fc7\u6700\u7ec8\u7ed3\u679c\u6b63\u786e\u6027\u751f\u6210\u4f2a\u6b65\u7ea7\u6807\u7b7e\u5e76\u5229\u7528\u7f13\u51b2\u6982\u7387\u6d88\u9664\u566a\u58f0\u5f71\u54cd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728ProcessBench\u4e0a\u7684F1\u5206\u6570\u5e73\u5747\u8fbe\u523053.0%\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfPRMs\u8bad\u7ec3\u9700\u8981\u6602\u8d35\u4e14\u96be\u4ee5\u5927\u89c4\u6a21\u83b7\u53d6\u7684\u6b65\u7ea7\u6807\u7b7e\u3002", "method": "\u5f15\u5165FreePRM\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u6700\u7ec8\u7ed3\u679c\u6b63\u786e\u6027\u751f\u6210\u4f2a\u6b65\u7ea7\u6807\u7b7e\u53ca\u4f7f\u7528Buffer Probability\u5904\u7406\u4f2a\u6807\u7b7e\u4e2d\u7684\u566a\u58f0\u3002", "result": "FreePRM\u5728ProcessBench\u4e0a\u53d6\u5f9753.0%\u7684F1\u5206\u6570\uff0c\u6bd4Math-Shepherd\u4e0a\u5b8c\u5168\u76d1\u7763\u7684PRM\u9ad8\u51fa24.1%\uff0c\u6bd4\u5176\u4ed6\u5f00\u6e90PRMs\u9ad8\u51fa\u81f3\u5c1110.9%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f00\u521b\u4e86PRM\u8bad\u7ec3\u7684\u65b0\u8303\u5f0f\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u5bf9\u6602\u8d35\u6b65\u7ea7\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u52b2\u6027\u80fd\u3002"}}
{"id": "2506.03573", "pdf": "https://arxiv.org/pdf/2506.03573", "abs": "https://arxiv.org/abs/2506.03573", "authors": ["Lin Sun", "Can Zhang"], "title": "Exchange of Perspective Prompting Enhances Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made significant advancements in addressing\ndiverse natural language processing (NLP) tasks. However, their performance is\noften limited by inherent comprehension of problems. To address this\nlimitation, we propose Exchange-of-Perspective (EoP), a novel framework\ndesigned to exchange perspectives across different definitions of problem, so\nthat it can break the fixed mindset from any particular formulation of the\nquestion. We conducted extensive and comprehensive experiments on 8 benchmarks.\nThe results show that EoP can significantly improve performance. For instance,\ncompared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we\nobserve a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP\ndemonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a\n3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using\nQwen-2.5-72b.", "AI": {"tldr": "Large language models often face limitations in comprehending problems. This paper introduces Exchange-of-Perspective (EoP), a framework designed to overcome these limitations by exchanging perspectives across different problem definitions. Extensive experiments demonstrate EoP's effectiveness, showing significant performance improvements.", "motivation": "Addressing the inherent comprehension limitations of large language models in understanding diverse problem definitions.", "method": "Proposing Exchange-of-Perspective (EoP), a novel framework aimed at exchanging perspectives across different problem definitions to break the fixed mindset from any particular formulation of the question.", "result": "Significant performance improvements were observed on 8 benchmarks. For example, with GPT-3.5-Turbo and EoP, there was a 3.6% improvement on AQuA (from 60.6% to 64.2%). GPT-4-powered EoP showed a 7.7% overall accuracy enhancement on Math (from 53.9% to 61.6%) and a 3.5% improvement on OlympiadBench Maths (from 43.5% to 47.0%) when using Qwen-2.5-72b.", "conclusion": "The proposed Exchange-of-Perspective (EoP) framework effectively improves the performance of large language models on various NLP tasks by addressing their comprehension limitations."}}
{"id": "2506.03576", "pdf": "https://arxiv.org/pdf/2506.03576", "abs": "https://arxiv.org/abs/2506.03576", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics.", "AI": {"tldr": "A bidirectional LM framework that fuses structural cues from KGs with the semantic expressiveness of generative transformers.", "motivation": "Unify symbolic knowledge graphs with language models for richer semantic understanding.", "method": "Introduces KG-BiLM with three key components: Bidirectional Knowledge Attention, Knowledge-Masked Prediction, and Contrastive Graph Semantic Aggregation.", "result": "Outperforms strong baselines in link prediction, especially on large-scale graphs with complex multi-hop relations.", "conclusion": "Effectively unifies structural information and textual semantics."}}
{"id": "2506.03580", "pdf": "https://arxiv.org/pdf/2506.03580", "abs": "https://arxiv.org/abs/2506.03580", "authors": ["Enrico Benedetti", "Akiko Aizawa", "Florian Boudin"], "title": "Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models", "categories": ["cs.CL"], "comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Volume 4: Student Research Workshop)", "summary": "Providing example sentences that are diverse and aligned with learners'\nproficiency levels is essential for fostering effective language acquisition.\nThis study examines the use of Pre-trained Language Models (PLMs) to produce\nexample sentences targeting L2 Japanese learners. We utilize PLMs in two ways:\nas quality scoring components in a retrieval system that draws from a newly\ncurated corpus of Japanese sentences, and as direct sentence generators using\nzero-shot learning. We evaluate the quality of sentences by considering\nmultiple aspects such as difficulty, diversity, and naturalness, with a panel\nof raters consisting of learners of Japanese, native speakers -- and GPT-4. Our\nfindings suggest that there is inherent disagreement among participants on the\nratings of sentence qualities, except for difficulty. Despite that, the\nretrieval approach was preferred by all evaluators, especially for beginner and\nadvanced target proficiency, while the generative approaches received lower\nscores on average. Even so, our experiments highlight the potential for using\nPLMs to enhance the adaptability of sentence suggestion systems and therefore\nimprove the language learning journey.", "AI": {"tldr": "Use Pre-trained Language Models (PLMs) to generate example sentences for L2 Japanese learners. Evaluate the quality of sentences considering difficulty, diversity, and naturalness.", "motivation": "Provide diverse and proficiency-aligned example sentences to promote effective language acquisition.", "method": "1. Use PLMs as quality scoring components in a retrieval system. 2. Use PLMs as direct sentence generators through zero-shot learning.", "result": "Retrieval approach was preferred by all evaluators, especially for beginner and advanced proficiency. Generative approaches received lower scores on average.", "conclusion": "PLMs have the potential to enhance the adaptability of sentence suggestion systems and improve language learning."}}
{"id": "2506.03592", "pdf": "https://arxiv.org/pdf/2506.03592", "abs": "https://arxiv.org/abs/2506.03592", "authors": ["Viktor Hangya", "Fabian K\u00fcch", "Darina Gold"], "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. We plan to publish our benchmark adaptions.", "AI": {"tldr": "This work aims to reduce the computational burden of NLG benchmarks for LLMs by reformulating generative tasks into cheaper NLU alternatives, achieving strong correlations and significant time savings.", "motivation": "Iterative evaluation of LLMs during training is important but costly, especially for NLG tasks like reasoning and code generation. The goal is to enable monitoring crucial LLM capabilities more efficiently.", "method": "Reformulate generative tasks into cheaper NLU alternatives and test performance correlation between original and reformulated tasks.", "result": "Strong correlation found between task formats with an average 35x reduction in evaluation time.", "conclusion": "Capability assessment via cheaper alternatives is supported and plans to publish benchmark adaptions are made."}}
{"id": "2506.03593", "pdf": "https://arxiv.org/pdf/2506.03593", "abs": "https://arxiv.org/abs/2506.03593", "authors": ["Ray Groshan", "Michael Ginn", "Alexis Palmer"], "title": "Is linguistically-motivated data augmentation worth it?", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main. First two authors contributed equally", "summary": "Data augmentation, a widely-employed technique for addressing data scarcity,\ninvolves generating synthetic data examples which are then used to augment\navailable training data. Researchers have seen surprising success from simple\nmethods, such as random perturbations from natural examples, where models seem\nto benefit even from data with nonsense words, or data that doesn't conform to\nthe rules of the language. A second line of research produces synthetic data\nthat does in fact follow all linguistic constraints; these methods require some\nlinguistic expertise and are generally more challenging to implement. No\nprevious work has done a systematic, empirical comparison of both\nlinguistically-naive and linguistically-motivated data augmentation strategies,\nleaving uncertainty about whether the additional time and effort of\nlinguistically-motivated data augmentation work in fact yields better\ndownstream performance.\n  In this work, we conduct a careful and comprehensive comparison of\naugmentation strategies (both linguistically-naive and\nlinguistically-motivated) for two low-resource languages with different\nmorphological properties, Uspanteko and Arapaho. We evaluate the effectiveness\nof many different strategies and their combinations across two important\nsequence-to-sequence tasks for low-resource languages: machine translation and\ninterlinear glossing. We find that linguistically-motivated strategies can have\nbenefits over naive approaches, but only when the new examples they produce are\nnot significantly unlike the training data distribution.", "AI": {"tldr": "This paper compares linguistically-naive and linguistically-motivated data augmentation strategies for low-resource languages Uspanteko and Arapaho, finding that motivated strategies outperform naive ones only when the augmented data remains similar to the training data distribution.", "motivation": "To systematically compare linguistically-naive and linguistically-motivated data augmentation strategies for low-resource languages and determine if the extra effort for motivated strategies improves downstream performance.", "method": "Conducting a careful and comprehensive comparison of various data augmentation strategies for machine translation and interlinear glossing tasks using Uspanteko and Arapaho datasets.", "result": "Linguistically-motivated strategies surpass naive approaches only when the augmented data is not significantly different from the training data distribution.", "conclusion": "Linguistically-motivated data augmentation can be beneficial for low-resource languages if it maintains similarity to the original data distribution."}}
{"id": "2506.03598", "pdf": "https://arxiv.org/pdf/2506.03598", "abs": "https://arxiv.org/abs/2506.03598", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "4 pages,2 figures,EITCE 2025", "summary": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.", "AI": {"tldr": "A novel approach called Auto Prompt SQL (AP-SQL) is proposed to enhance Text-to-SQL translation using small open-source models by integrating schema filtering, retrieval-augmented text-to-SQL generation, and prompt-driven schema linking and SQL generation. The method improves schema selection accuracy and enhances reasoning ability with prompt engineering techniques.", "motivation": "The challenge of using resource-intensive open-source models in resource-constrained environments for Text-to-SQL translation.", "method": "Decomposing the task into schema filtering, retrieval-augmented text-to-SQL generation, and prompt-driven schema linking and SQL generation. Fine-tuning large language models for schema selection accuracy and exploring the impact of prompt engineering with CoT and GoT templates.", "result": "Comprehensive evaluations on the Spider benchmarks show the effectiveness of AP-SQL.", "conclusion": "AP-SQL bridges the gap between resource-efficient small open-source models and powerful closed-source models for Text-to-SQL translation."}}
{"id": "2506.03616", "pdf": "https://arxiv.org/pdf/2506.03616", "abs": "https://arxiv.org/abs/2506.03616", "authors": ["Eunki Kim", "Sangryul Kim", "James Thorne"], "title": "Learning to Insert [PAUSE] Tokens for Better Reasoning", "categories": ["cs.CL"], "comment": "18 pages, 5 figures, ACL findings", "summary": "To enhance reasoning capabilities, previous works have explored incorporating\nspecial-purpose tokens into the training process. These strategies strengthen\nthe learning mechanism of transformer-based large language models (LLMs).\nBuilding on prior research, in which inserting dummy tokens consecutively just\nbefore reasoning steps can enhance effectiveness, we introduce a novel approach\ntermed Dynamic Inserting Tokens Training (DIT). Our method identifies positions\nwithin sequences where model confidence is lowest according to token\nlog-likelihood. Strategically inserting [PAUSE] tokens on these positions\nbolsters the model's predictive capabilities for subsequent tokens.\nExperimental results across diverse datasets and models, from the 2.7B model to\nthe 8B model, demonstrate that DIT consistently outperforms traditional\nfine-tuning and previous token insertion methods. With this simple yet\neffective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on\nAQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work\nshows a model-based, dynamic approach rather than a heuristic one, thereby\nbroadening the scope of research in reasoning.", "AI": {"tldr": "Incorporate a novel method called Dynamic Inserting Tokens Training (DIT) to improve the reasoning capabilities of transformer-based large language models.", "motivation": "Previous works have explored incorporating special-purpose tokens into the training process to enhance the learning mechanism.", "method": "Identify positions within sequences where model confidence is lowest and insert [PAUSE] tokens on these positions to bolster the model's predictive capabilities for subsequent tokens.", "result": "Achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets.", "conclusion": "This work demonstrates a model-based, dynamic approach to enhance reasoning capabilities rather than a heuristic one, broadening the scope of research in reasoning."}}
{"id": "2506.03619", "pdf": "https://arxiv.org/pdf/2506.03619", "abs": "https://arxiv.org/abs/2506.03619", "authors": ["Ayuto Tsutsumi", "Yuu Jinnai"], "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales", "categories": ["cs.CL"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated strong language\nunderstanding and generation abilities across various languages, their cultural\nknowledge is often limited to English-speaking communities, which can\nmarginalize the cultures of non-English communities. To address the problem,\nevaluation of the cultural awareness of the LLMs and the methods to develop\nculturally aware LLMs have been investigated. In this study, we focus on\nevaluating knowledge of folktales, a key medium for conveying and circulating\nculture. In particular, we focus on Japanese folktales, specifically on\nknowledge of Yokai. Yokai are supernatural creatures originating from Japanese\nfolktales that continue to be popular motifs in art and entertainment today.\nYokai have long served as a medium for cultural expression, making them an\nideal subject for assessing the cultural awareness of LLMs. We introduce\nYokaiEval, a benchmark dataset consisting of 809 multiple-choice questions\n(each with four options) designed to probe knowledge about yokai. We evaluate\nthe performance of 31 Japanese and multilingual LLMs on this dataset. The\nresults show that models trained with Japanese language resources achieve\nhigher accuracy than English-centric models, with those that underwent\ncontinued pretraining in Japanese, particularly those based on Llama-3,\nperforming especially well. The code and dataset are available at\nhttps://github.com/CyberAgentA ILab/YokaiEval.", "AI": {"tldr": "This study evaluates the cultural awareness of Large Language Models (LLMs) by introducing YokaiEval, a dataset of 809 multiple-choice questions about Japanese folklore. It finds that models trained on Japanese resources outperform English-centric ones.", "motivation": "To address the lack of cultural knowledge of non-English speaking communities in LLMs, focusing on Japanese folklore as a medium for cultural expression.", "method": "Developing YokaiEval, a benchmark dataset with questions about Japanese yokai, and evaluating 31 Japanese and multilingual LLMs.", "result": "Japanese-trained models outperformed English-centric ones, with Llama-3 based models showing the best performance.", "conclusion": "This research highlights the importance of culturally-aware training data for improving LLMs' understanding of non-English cultures."}}
{"id": "2506.03627", "pdf": "https://arxiv.org/pdf/2506.03627", "abs": "https://arxiv.org/abs/2506.03627", "authors": ["Lin Mu", "Guowei Chu", "Li Ni", "Lei Sang", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks", "categories": ["cs.CL", "cs.AI"], "comment": "13pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications.", "AI": {"tldr": "A novel prompting strategy named RoP is proposed to enhance the robustness of Large Language Models (LLMs). It includes two stages: Error Correction and Guidance. Experiments show RoP improves LLMs' robustness against adversarial perturbations.", "motivation": "To develop a prompting strategy that explicitly mitigates the negative impact of input perturbations on LLMs.", "method": "RoP has two stages: Error Correction (generating adversarial examples and constructing prompts for error correction) and Guidance (generating optimal guidance prompting for robust and accurate inferences).", "result": "RoP significantly improves LLMs' robustness against adversarial perturbations while maintaining high accuracy, even with minimal degradation compared to clean input scenarios.", "conclusion": "RoP is a practical and effective approach for enhancing LLM robustness in real-world applications."}}
{"id": "2506.03637", "pdf": "https://arxiv.org/pdf/2506.03637", "abs": "https://arxiv.org/abs/2506.03637", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "title": "RewardAnything: Generalizable Principle-Following Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.", "AI": {"tldr": "Introduces RewardAnything, a novel reward model that can follow natural language principles, showing better adaptability and performance compared to existing models.", "motivation": "Existing reward models are limited by their fixed preference datasets and lack the ability to adapt to diverse real-world needs.", "method": "RewardAnything is trained to follow natural language principles, enabling it to adapt to various tasks and principles.", "result": "RewardAnything outperforms current reward models in generalization and adaptability, achieving state-of-the-art performance.", "conclusion": "RewardAnything shows superior adaptability in handling diverse real-world requirements without the need for retraining."}}
{"id": "2506.03659", "pdf": "https://arxiv.org/pdf/2506.03659", "abs": "https://arxiv.org/abs/2506.03659", "authors": ["Yinuo Wang", "Robert E. Mercer", "Frank Rudzicz", "Sudipta Singha Roy", "Pengjie Ren", "Zhumin Chen", "Xindi Wang"], "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "categories": ["cs.CL"], "comment": null, "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u786e\u4fdd\u60a3\u8005\u5b89\u5168\u3001\u4e34\u5e8a\u6548\u679c\u548c\u7528\u6237\u4fe1\u5fc3\uff0c\u63d0\u9ad8\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u76f4\u63a5\u5173\u7cfb\u5230\u4e34\u5e8a\u51b3\u7b56\u548c\u60a3\u8005\u7ed3\u679c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u533b\u7597\u6570\u636e\u7684\u590d\u6742\u6027\u3001\u4e34\u5e8a\u573a\u666f\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u503c\u5f97\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u7684\u591a\u65b9\u9762\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u5168\u9762\u7684\u53ef\u4fe1\u5ea6\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002", "method": "\u5bf9\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u8fdb\u884c\u5168\u9762\u8c03\u67e5\uff0c\u91cd\u70b9\u8003\u5bdf\u5176\u53ef\u4fe1\u5ea6\u7684\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002", "result": "\u5bf9\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\u8fdb\u884c\u4e86\u5168\u9762\u5ba1\u67e5\uff0c\u5305\u62ec\u5982\u4f55\u5728\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u4e2d\u8bc4\u4f30\u6bcf\u4e2a\u7ef4\u5ea6\uff0c\u7f16\u8bd1\u548c\u6bd4\u8f83\u4e3b\u8981\u57fa\u51c6\uff0c\u5e76\u5206\u6790\u8bc4\u4f30\u5f15\u5bfc\u7684\u6280\u672f\u3002", "conclusion": "\u8bc6\u522b\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u4e8b\u5b9e\u6027\u3001\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6821\u51c6\u6027\u7b49\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u5e76\u5206\u6790\u73b0\u6709\u8bc4\u4f30\u8fd9\u4e9b\u7ef4\u5ea6\u7684\u4e3b\u8981\u57fa\u51c6\u548c\u5f15\u5bfc\u6a21\u578b\u6539\u8fdb\u7684\u6280\u672f\u3002\u6700\u540e\uff0c\u786e\u5b9a\u4e86\u53ef\u6269\u5c55\u7684\u4e13\u5bb6\u8bc4\u4f30\u3001\u7efc\u5408\u591a\u7ef4\u6307\u6807\u548c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7814\u7a76\u7b49\u5f00\u653e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.03665", "pdf": "https://arxiv.org/pdf/2506.03665", "abs": "https://arxiv.org/abs/2506.03665", "authors": ["Hern\u00e1n Maina", "Guido Ivetta", "Mateo Lione Stuto", "Julian Martin Eisenschlos", "Jorge S\u00e1nchez", "Luciana Benotti"], "title": "ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visually impaired people could benefit from Visual Question Answering (VQA)\nsystems to interpret text in their surroundings. However, current models often\nstruggle with recognizing text in the photos taken by this population. Through\nin-depth interviews with visually impaired individuals, we identified common\nframing conventions that frequently result in misaligned text. Existing VQA\nbenchmarks primarily feature well-oriented text captured by sighted users,\nunder-representing these challenges. To address this gap, we introduce ROtated\nSAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich\nimages with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7\nabsolute points in the best-performing model.", "AI": {"tldr": "This paper introduces ROSA, a decoding strategy that improves VQA performance for text-rich images with incorrectly oriented text, addressing challenges faced by visually impaired individuals.", "motivation": "Current VQA models struggle with recognizing text in photos taken by visually impaired individuals due to under-representation of incorrectly oriented text.", "method": "Introduce ROSA, a decoding strategy", "result": "ROSA outperforms Greedy decoding by 11.7 absolute points in the best-performing model.", "conclusion": "ROtated SAmpling (ROSA) improves VQA performance in text-rich images with incorrectly oriented text."}}
{"id": "2506.03681", "pdf": "https://arxiv.org/pdf/2506.03681", "abs": "https://arxiv.org/abs/2506.03681", "authors": ["Pradeep Rangappa", "Andres Carofilis", "Jeena Prakash", "Shashi Kumar", "Sergio Burdisso", "Srikanth Madikeri", "Esau Villatoro-Tello", "Bidisha Sharma", "Petr Motlicek", "Kadri Hacioglu", "Shankar Venkatesan", "Saurabh Vyas", "Andreas Stolcke"], "title": "Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Fine-tuning pretrained ASR models for specific domains is challenging for\nsmall organizations with limited labeled data and computational resources.\nHere, we explore different data selection pipelines and propose a robust\napproach that improves ASR adaptation by filtering pseudo-labels generated\nusing Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach\nintegrates multiple selection strategies -- including word error rate (WER)\nprediction, named entity recognition (NER), and character error rate (CER)\nanalysis -- to extract high-quality training segments. We evaluate our method\non Whisper and Zipformer using a 7500-hour baseline, comparing it to a\nCER-based approach relying on hypotheses from three ASR systems. Fine-tuning on\n7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our\nfiltering reduces the dataset to 100 hours (1.4%) with similar performance; a\nsimilar trend is observed on Fisher English.", "AI": {"tldr": "This study explores data selection pipelines to improve ASR adaptation using Whisper and Zipformer models. It proposes an approach that filters pseudo-labels and integrates multiple selection strategies, achieving comparable performance with much smaller datasets.", "motivation": "Small organizations face challenges in fine-tuning pretrained ASR models due to limited labeled data and computational resources.", "method": "Proposes a robust approach integrating word error rate prediction, named entity recognition, and character error rate analysis to filter pseudo-labels generated by Whisper and Zipformer models.", "result": "Fine-tuning on 7500 hours of pseudo-labeled data achieves 12.3% WER, while the proposed filtering reduces the dataset to 100 hours with similar performance.", "conclusion": "The approach significantly reduces the required dataset size without sacrificing performance, making ASR adaptation more accessible for small organizations."}}
{"id": "2506.03690", "pdf": "https://arxiv.org/pdf/2506.03690", "abs": "https://arxiv.org/abs/2506.03690", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "title": "Robust Preference Optimization via Dynamic Target Margins", "categories": ["cs.CL"], "comment": "18 pages, 6 figures, accepted to The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL2025)", "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.", "AI": {"tldr": "A dynamic target margin preference optimization algorithm named \u03b3-PO is proposed to improve the alignment of large language models.", "motivation": "Ensuring safety and reliability of large language models in practical applications.", "method": "Introducing instance-specific margin calibration to adjust reward margins at the pairwise level.", "result": "Improves average performance by 4.4% across benchmarks like AlpacaEval2 and Arena-Hard.", "conclusion": "\u03b3-PO is a robust solution with minimal code changes and negligible impact on training efficiency."}}
{"id": "2506.03700", "pdf": "https://arxiv.org/pdf/2506.03700", "abs": "https://arxiv.org/abs/2506.03700", "authors": ["Zhepei Wei", "Wei-Lin Chen", "Xinyu Zhu", "Yu Meng"], "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism", "categories": ["cs.CL"], "comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode", "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.", "AI": {"tldr": "AdaDecode is a novel method for accelerating large language model decoding without requiring auxiliary models or changes to the original model parameters, while ensuring output consistency.", "motivation": "Autoregressive decoding is inherently limited by its sequential token generation process, restricting the ability to fully leverage modern hardware's parallel processing capabilities.", "method": "AdaDecode adaptively generates tokens at intermediate layers when confidence is high, enabling the next token's computation to begin immediately. The remaining layer computations for early-predicted tokens are deferred and executed in parallel with subsequent tokens when needed.", "result": "Experiments show that AdaDecode consistently achieves superior decoding throughput with up to 1.73x speedup, while guaranteeing output parity with standard autoregressive decoding.", "conclusion": "AdaDecode accelerates LLM decoding without requiring auxiliary models or changes to the original model parameters, while ensuring output consistency."}}
{"id": "2506.03704", "pdf": "https://arxiv.org/pdf/2506.03704", "abs": "https://arxiv.org/abs/2506.03704", "authors": ["Pei-Yun Lin", "Yen-lung Tsai"], "title": "ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "11 pages, 8 figures. Code and demo available at\n  https://github.com/peiyun2260/ScoreRAG. Submitted to arXiv for public access;\n  journal submission planned", "summary": "This research introduces ScoreRAG, an approach to enhance the quality of\nautomated news generation. Despite advancements in Natural Language Processing\nand large language models, current news generation methods often struggle with\nhallucinations, factual inconsistencies, and lack of domain-specific expertise\nwhen producing news articles. ScoreRAG addresses these challenges through a\nmulti-stage framework combining retrieval-augmented generation, consistency\nrelevance evaluation, and structured summarization. The system first retrieves\nrelevant news documents from a vector database, maps them to complete news\nitems, and assigns consistency relevance scores based on large language model\nevaluations. These documents are then reranked according to relevance, with\nlow-quality items filtered out. The framework proceeds to generate graded\nsummaries based on relevance scores, which guide the large language model in\nproducing complete news articles following professional journalistic standards.\nThrough this methodical approach, ScoreRAG aims to significantly improve the\naccuracy, coherence, informativeness, and professionalism of generated news\narticles while maintaining stability and consistency throughout the generation\nprocess. The code and demo are available at:\nhttps://github.com/peiyun2260/ScoreRAG.", "AI": {"tldr": "ScoreRAG\u662f\u4e00\u79cd\u901a\u8fc7\u591a\u9636\u6bb5\u6846\u67b6\u589e\u5f3a\u81ea\u52a8\u5316\u65b0\u95fb\u751f\u6210\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u4e00\u81f4\u6027\u76f8\u5173\u6027\u8bc4\u4f30\u548c\u7ed3\u6784\u5316\u603b\u7ed3\u3002\u5b83\u65e8\u5728\u63d0\u9ad8\u751f\u6210\u65b0\u95fb\u6587\u7ae0\u7684\u51c6\u786e\u6027\u3001\u8fde\u8d2f\u6027\u3001\u4fe1\u606f\u91cf\u548c\u4e13\u4e1a\u6027\uff0c\u5e76\u4fdd\u6301\u751f\u6210\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u65b0\u95fb\u751f\u6210\u65b9\u6cd5\u5e38\u51fa\u73b0\u5e7b\u89c9\u3001\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u4ee5\u53ca\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7b49\u95ee\u9898\u3002", "method": "ScoreRAG\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u7684\u65b0\u95fb\u6587\u6863\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e00\u81f4\u6027\u76f8\u5173\u6027\u8bc4\u5206\uff0c\u7136\u540e\u6839\u636e\u5f97\u5206\u91cd\u6392\u5e8f\u5e76\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u9879\u76ee\uff0c\u6700\u540e\u57fa\u4e8e\u5f97\u5206\u751f\u6210\u5206\u7ea7\u6458\u8981\u4ee5\u6307\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5b8c\u6574\u7684\u65b0\u95fb\u6587\u7ae0\u3002", "result": "ScoreRAG\u63d0\u9ad8\u4e86\u751f\u6210\u65b0\u95fb\u6587\u7ae0\u7684\u51c6\u786e\u6027\u3001\u8fde\u8d2f\u6027\u3001\u4fe1\u606f\u91cf\u548c\u4e13\u4e1a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "ScoreRAG\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5f53\u524d\u65b0\u95fb\u751f\u6210\u4e2d\u7684\u4e3b\u8981\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u65b0\u95fb\u751f\u6210\u7684\u8d28\u91cf\u3002"}}
{"id": "2506.03722", "pdf": "https://arxiv.org/pdf/2506.03722", "abs": "https://arxiv.org/abs/2506.03722", "authors": ["Yinfeng Xia", "Huiyan Li", "Chenyang Le", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Applying large pre-trained speech models like Whisper has shown promise in\nreducing training costs for various speech tasks. However, integrating these\nmodels into streaming systems remains a challenge. This paper presents a novel\nprefix-to-prefix training framework for streaming recognition by fine-tuning\nthe Whisper. We introduce the Continuous Integrate-and-Fire mechanism to\nestablish a quasi-monotonic alignment between continuous speech sequences and\ndiscrete text tokens. Additionally, we design Monotonic Finite Look-ahead\nAttention, allowing each token to attend to infinite left-context and finite\nright-context from the speech sequences. We also employ the wait-k decoding\nstrategy to simplify the decoding process while ensuring consistency between\ntraining and testing. Our theoretical analysis and experiments demonstrate that\nthis approach achieves a controllable trade-off between latency and quality,\nmaking it suitable for various streaming applications.", "AI": {"tldr": "This paper proposes a new prefix-to-prefix training framework using fine-tuned Whisper for streaming speech recognition. A Continuous Integrate-and-Fire mechanism and Monotonic Finite Look-ahead Attention are introduced to create a quasi-monotonic alignment between speech and text, with wait-k decoding for simplified processing. The method shows a balance between latency and quality.", "motivation": "To reduce training costs for speech tasks and integrate large pre-trained models like Whisper into streaming systems.", "method": "Fine-tuning Whisper with a prefix-to-prefix training framework, introducing Continuous Integrate-and-Fire and Monotonic Finite Look-ahead Attention, and using wait-k decoding strategy.", "result": "The approach achieves a controllable trade-off between latency and quality, suitable for various streaming applications.", "conclusion": "This work demonstrates the feasibility of integrating large pre-trained speech models into streaming systems with improved efficiency and performance."}}
{"id": "2506.03723", "pdf": "https://arxiv.org/pdf/2506.03723", "abs": "https://arxiv.org/abs/2506.03723", "authors": ["Chaeyun Jang", "Moonseok Choi", "Yegon Kim", "Hyungi Lee", "Juho Lee"], "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty calibration is essential for the safe deployment of large\nlanguage models (LLMs), particularly when users rely on verbalized confidence\nestimates. While prior work has focused on classifiers or short-form\ngeneration, confidence calibration for chain-of-thought (CoT) reasoning remains\nlargely unexplored. Surprisingly, we find that supervised fine-tuning with\nscalar confidence labels alone suffices to elicit self-verification behavior of\nlanguage models, without any explicit reasoning supervision or reinforcement\nlearning-based rewards. Despite being trained only to produce a verbalized\nconfidence score without any self-verifying examples, the model learns to\ngenerate longer and self-checking responses for low-confidence queries while\nproviding more concise answers for high-confidence ones. We further propose a\nsimple rethinking method that boosts performance via test-time scaling based on\ncalibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such\nas MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning\nimproves both calibration and accuracy, while also enhancing interpretability\nby aligning the model's reasoning path with its confidence.", "AI": {"tldr": "This paper investigates uncertainty calibration in large language models (LLMs) for chain-of-thought (CoT) reasoning. It finds that supervised fine-tuning with scalar confidence labels can elicit self-verification behavior in LLMs, improving both calibration and accuracy while enhancing interpretability.", "motivation": "Uncertainty calibration is crucial for safe deployment of LLMs, especially when relying on verbalized confidence estimates. Confidence calibration for CoT reasoning has been largely unexplored.", "method": "Supervised fine-tuning with scalar confidence labels alone is used to elicit self-verification behavior in LLMs. A simple rethinking method is proposed to boost performance via test-time scaling based on calibrated uncertainty.", "result": "Experiments on GSM8K and held-out reasoning tasks such as MATH-500 and ARC-Challenge show that the proposed confidence-aware fine-tuning improves both calibration and accuracy, while also enhancing interpretability by aligning the model's reasoning path with its confidence.", "conclusion": "This work demonstrates that supervised fine-tuning with scalar confidence labels can be effective for confidence calibration in LLMs for CoT reasoning, leading to improved calibration, accuracy, and interpretability."}}
{"id": "2506.03735", "pdf": "https://arxiv.org/pdf/2506.03735", "abs": "https://arxiv.org/abs/2506.03735", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "summary": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Math2Visual\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u6570\u5b66\u5e94\u7528\u9898\u7684\u6559\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u89c6\u89c9\u6548\u679c\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u751f\u6210\u6559\u80b2\u5185\u5bb9\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u6548\u679c\u662f\u6559\u6388\u6570\u5b66\u5e94\u7528\u9898\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u521b\u5efa\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u4eba\u529b\uff0c\u800c\u4e14\u7f3a\u4e4f\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u652f\u6301\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMath2Visual\u7684\u81ea\u52a8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9884\u5b9a\u4e49\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u8bbe\u8ba1\u7a7a\u95f4\u6765\u8bf4\u660eMWPs\u4e2d\u7684\u6838\u5fc3\u6570\u5b66\u5173\u7cfb\u3002\u4f7f\u7528Math2Visual\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1,903\u4e2a\u89c6\u89c9\u6548\u679c\u7684\u6ce8\u91ca\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u6587\u672c\u5230\u56fe\u50cf\uff08TTI\uff09\u6a21\u578b\u751f\u6210\u4e0e\u8bbe\u8ba1\u76f8\u4e00\u81f4\u7684\u89c6\u89c9\u6548\u679c\u7684\u80fd\u529b\u3002\u8fdb\u4e00\u6b65\u5fae\u8c03\u4e86\u51e0\u79cdTTI\u6a21\u578b\uff0c\u4ee5\u5c55\u793a\u5728\u6559\u80b2\u89c6\u89c9\u751f\u6210\u65b9\u9762\u7684\u6539\u8fdb\u3002", "result": "\u901a\u8fc7\u4f7f\u7528Math2Visual\u6784\u5efa\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5e76\u5fae\u8c03\u4e86\u51e0\u79cdTTI\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u6559\u80b2\u89c6\u89c9\u751f\u6210\u65b9\u9762\u7684\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u6709\u6559\u80b2\u610f\u4e49\u7684\u89c6\u89c9\u6548\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u751f\u6210\u591a\u6a21\u6001\u6559\u80b2\u5185\u5bb9\u7684\u5173\u952e\u6311\u6218\u7684\u89c1\u89e3\uff0c\u4f8b\u5982\u6570\u5b66\u5173\u7cfb\u7684\u9519\u8bef\u8868\u793a\u548c\u91cd\u8981\u89c6\u89c9\u5143\u7d20\u7684\u9057\u6f0f\u3002"}}
{"id": "2506.03761", "pdf": "https://arxiv.org/pdf/2506.03761", "abs": "https://arxiv.org/abs/2506.03761", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Zheyu Ye", "Zhoujun Li", "Zuozhu Liu"], "title": "Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services", "categories": ["cs.CL"], "comment": null, "summary": "As interest in using Large Language Models (LLMs) for interactive and\nemotionally rich experiences grows, virtual pet companionship emerges as a\nnovel yet underexplored application. Existing approaches focus on basic pet\nrole-playing interactions without systematically benchmarking LLMs for\ncomprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated\nbenchmark that evaluates LLMs across both self-interaction and\nhuman-interaction dimensions. Unlike prior work, Pet-Bench emphasizes\nself-evolution and developmental behaviors alongside interactive engagement,\noffering a more realistic reflection of pet companionship. It features diverse\ntasks such as intelligent scheduling, memory-based dialogues, and psychological\nconversations, with over 7,500 interaction instances designed to simulate\ncomplex pet behaviors. Evaluation of 28 LLMs reveals significant performance\nvariations linked to model size and inherent capabilities, underscoring the\nneed for specialized optimization in this domain. Pet-Bench serves as a\nfoundational resource for benchmarking pet-related LLM abilities and advancing\nemotionally immersive human-pet interactions.", "AI": {"tldr": "This paper introduces Pet-Bench, a benchmark for evaluating Large Language Models' (LLMs) capabilities in providing comprehensive companionship in virtual pet applications.", "motivation": "To explore the potential of LLMs in creating interactive and emotionally rich experiences through virtual pet companionship.", "method": "Developing a benchmark named Pet-Bench that assesses LLMs' performance in self-interaction and human-interaction dimensions, emphasizing self-evolution and developmental behaviors.", "result": "Evaluation of 28 LLMs showed significant performance variations related to model size and inherent capabilities, indicating the need for specialized optimization.", "conclusion": "Pet-Bench is a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions."}}
{"id": "2506.03762", "pdf": "https://arxiv.org/pdf/2506.03762", "abs": "https://arxiv.org/abs/2506.03762", "authors": ["Yifeng Gu", "Zicong Jiang", "Jianxiu Jin", "Kailing Guo", "Ziyang Zhang", "Xiangmin Xu"], "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 8 figures", "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.", "AI": {"tldr": "AhaKV\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u51cf\u5c11KV\u7f13\u5b58\u7684\u4f7f\u7528\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574softmax\u5c3a\u5ea6\u6765\u89e3\u51b3\u7d2f\u79ef\u6ce8\u610f\u529b\u5206\u6570\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5229\u7528\u4ef7\u503c\u5411\u91cf\u4fe1\u606f\u6765\u4f18\u5316\u9002\u5e94\u6027\u5f97\u5206\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u51cf\u5c11KV\u7f13\u5b58\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7d2f\u79ef\u6ce8\u610f\u529b\u5206\u6570\u4f5c\u4e3a\u9a71\u9010\u5206\u6570\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u504f\u5dee\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u8bbf\u95ee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAhaKV\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574softmax\u5c3a\u5ea6\u6765\u89e3\u51b3\u7d2f\u79ef\u6ce8\u610f\u529b\u5206\u6570\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5229\u7528\u4ef7\u503c\u5411\u91cf\u4fe1\u606f\u6765\u4f18\u5316\u9002\u5e94\u6027\u5f97\u5206\u3002", "result": "\u5728\u56fa\u5b9a\u7f13\u5b58\u9884\u7b97\u7684\u4e0d\u540c\u6a21\u578b\u4e0a\u90e8\u7f72\u4e86AhaKV\uff0c\u5b9e\u9a8c\u8868\u660eAhaKV\u6210\u529f\u51cf\u8f7b\u4e86\u504f\u5dee\u5e76\u4fdd\u7559\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u4e2d\u7684\u5173\u952e\u4ee4\u724c\uff0c\u5728\u51e0\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "AhaKV\u662f\u4e00\u79cd\u6709\u6548\u7684\u51cf\u5c11KV\u7f13\u5b58\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.03763", "pdf": "https://arxiv.org/pdf/2506.03763", "abs": "https://arxiv.org/abs/2506.03763", "authors": ["Quang Hieu Pham", "Thuy Duong Nguyen", "Tung Pham", "Anh Tuan Luu", "Dat Quoc Nguyen"], "title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "The capabilities of large language models (LLMs) have been enhanced by\ntraining on data that reflects human thought processes, such as the\nChain-of-Thought format. However, evidence suggests that the conventional\nscheme of next-word prediction may not fully capture how humans learn to think.\nInspired by how humans generalize mathematical reasoning, we propose a new\napproach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our\nClozeMath involves a text-infilling task that predicts masked equations from a\ngiven solution, analogous to cloze exercises used in human learning.\nExperiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the\nstrong baseline Masked Thought in performance and robustness, with two\ntest-time scaling decoding algorithms, Beam Search and Chain-of-Thought\ndecoding. Additionally, we conduct an ablation study to analyze the effects of\nvarious architectural and implementation choices on our approach.", "AI": {"tldr": "This paper proposes a new method called ClozeMath to improve mathematical reasoning in large language models by using a text-infilling task that predicts masked equations from a given solution.", "motivation": "The conventional scheme of next-word prediction may not fully capture how humans learn to think, so the authors were inspired by how humans generalize mathematical reasoning.", "method": "The authors use a text-infilling task that predicts masked equations from a given solution, similar to cloze exercises used in human learning.", "result": "ClozeMath outperforms the strong baseline Masked Thought in performance and robustness on GSM8K, MATH, and GSM-Symbolic datasets with two test-time scaling decoding algorithms.", "conclusion": "The proposed ClozeMath approach shows promise in improving mathematical reasoning in large language models."}}
{"id": "2506.03781", "pdf": "https://arxiv.org/pdf/2506.03781", "abs": "https://arxiv.org/abs/2506.03781", "authors": ["Seungcheol Park", "Jeongin Bae", "Beomseok Kwon", "Minjun Kim", "Byeongwook Kim", "Se Jung Kwon", "U Kang", "Dongsoo Lee"], "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "ACL 2025 Main Track", "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.", "AI": {"tldr": "Propose UniQuanF, a unified quantization method for large language models.", "motivation": "To quantize large language models while maintaining accuracy.", "method": "Unifies flexible mapping technique in UQ and non-uniform quantization levels of BCQ with proposed initialization and mapping techniques.", "result": "Outperforms existing UQ and BCQ methods, improving accuracy by up to 4.60% on GSM8K benchmark.", "conclusion": "UniQuanF removes computational and memory overhead, enabling superior accuracy without extra deployment costs."}}
{"id": "2506.03785", "pdf": "https://arxiv.org/pdf/2506.03785", "abs": "https://arxiv.org/abs/2506.03785", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "4 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "AI": {"tldr": "This paper introduces Knockout Assessment, a new method using a knockout tournament system with iterative pairwise comparisons for LLM evaluation, which improves scoring accuracy compared to previous methods.", "motivation": "Current LLM-as-a-Judge approaches lack a global ranking perspective due to reliance on individual or single-round pairwise assessments.", "method": "Knockout Assessment uses a knockout tournament system with iterative pairwise comparisons.", "result": "Experiments on three LLMs across two datasets showed that knockout assessment improved scoring accuracy, increasing Pearson correlation with expert evaluations.", "conclusion": "Knockout Assessment aligns LLM assessments more closely with human scoring."}}
{"id": "2506.03793", "pdf": "https://arxiv.org/pdf/2506.03793", "abs": "https://arxiv.org/abs/2506.03793", "authors": ["Sidharth Pulipaka", "Sparsh Jain", "Ashwin Sankar", "Raj Dabre"], "title": "Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Punctuation plays a vital role in structuring meaning, yet current models\noften struggle to restore it accurately in transcripts of spontaneous speech,\nespecially in the presence of disfluencies such as false starts and\nbacktracking. These limitations hinder the performance of downstream tasks like\ntranslation, text to speech, summarization, etc. where sentence boundaries are\ncritical for preserving quality. In this work, we introduce Cadence, a\ngeneralist punctuation restoration model adapted from a pretrained large\nlanguage model. Cadence is designed to handle both clean written text and\nhighly spontaneous spoken transcripts. It surpasses the previous state of the\nart in performance while expanding support from 14 to all 22 Indian languages\nand English. We conduct a comprehensive analysis of model behavior across\npunctuation types and language families, identifying persistent challenges\nunder domain shift and with rare punctuation marks. Our findings demonstrate\nthe efficacy of utilizing pretrained language models for multilingual\npunctuation restoration and highlight Cadence practical value for low resource\nNLP pipelines at scale.", "AI": {"tldr": "A new punctuation restoration model called Cadence has been developed, which outperforms previous models and supports more languages.", "motivation": "Current models struggle with restoring punctuation in spontaneous speech, especially with disfluencies, affecting downstream tasks.", "method": "Cadence, a generalist punctuation restoration model adapted from a pretrained large language model.", "result": "Cadence surpasses previous state-of-the-art performance and expands language support from 14 to 22 Indian languages plus English.", "conclusion": "Using pretrained language models for multilingual punctuation restoration is effective and Cadence has practical value for low-resource NLP pipelines."}}
{"id": "2506.03820", "pdf": "https://arxiv.org/pdf/2506.03820", "abs": "https://arxiv.org/abs/2506.03820", "authors": ["Ahmad Mustapha Wali", "Sergiu Nisioi"], "title": "Automatic Correction of Writing Anomalies in Hausa Texts", "categories": ["cs.CL"], "comment": null, "summary": "Hausa texts are often characterized by writing anomalies such as incorrect\ncharacter substitutions and spacing errors, which sometimes hinder natural\nlanguage processing (NLP) applications. This paper presents an approach to\nautomatically correct the anomalies by finetuning transformer-based models.\nUsing a corpus gathered from several public sources, we created a large-scale\nparallel dataset of over 450,000 noisy-clean Hausa sentence pairs by\nintroducing synthetically generated noise, fine-tuned to mimic realistic\nwriting errors. Moreover, we adapted several multilingual and African\nlanguage-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT\nvariants for this correction task using SentencePiece tokenization. Our\nexperimental results demonstrate significant increases in F1, BLEU and METEOR\nscores, as well as reductions in Character Error Rate (CER) and Word Error Rate\n(WER). This research provides a robust methodology, a publicly available\ndataset, and effective models to improve Hausa text quality, thereby advancing\nNLP capabilities for the language and offering transferable insights for other\nlow-resource languages.", "AI": {"tldr": "This paper introduces a method to automatically correct writing anomalies in Hausa texts using fine-tuned transformer-based models and a large-scale parallel dataset.", "motivation": "The motivation is to address the challenges posed by writing anomalies in Hausa texts that hinder natural language processing applications.", "method": "The method involves creating a large-scale parallel dataset of noisy-clean Hausa sentence pairs and adapting multilingual and African language-focused models for the correction task using SentencePiece tokenization.", "result": "The experimental results show significant improvements in F1, BLEU, and METEOR scores, as well as reductions in CER and WER.", "conclusion": "This research offers a robust methodology, a publicly available dataset, and effective models to enhance Hausa text quality and NLP capabilities for low-resource languages."}}
{"id": "2506.03822", "pdf": "https://arxiv.org/pdf/2506.03822", "abs": "https://arxiv.org/abs/2506.03822", "authors": ["Fabian Karl", "Ansgar Scherp"], "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SCOLIA 2025", "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.", "AI": {"tldr": "CRAWLDoc is a novel approach for ranking linked web documents based on context, starting from a publication's URL. It processes linked resources like PDFs and ORCID profiles into a unified representation and shows robust performance across different publishers and data formats.", "motivation": "Variations in web layouts and data formats pose challenges for metadata extraction by publication databases.", "method": "CRAWLDoc retrieves linked web resources starting from a publication URL, embeds them into a unified representation, and ranks them contextually.", "result": "CRAWLDoc performs well in ranking relevant documents across publishers and formats, as demonstrated by evaluations using a manually labeled dataset of 600 publications.", "conclusion": "This work establishes a foundation for enhanced metadata extraction from diverse web documents with varying layouts and formats."}}
{"id": "2506.03827", "pdf": "https://arxiv.org/pdf/2506.03827", "abs": "https://arxiv.org/abs/2506.03827", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR2025", "summary": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5bf9\u9f50\u51fa\u4ef7\u8bcd\u751f\u6210\u6a21\u578b(MoBGM)\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u5b50\u5546\u52a1\u641c\u7d22\u5e7f\u544a\u4e2d\u7684\u957f\u5c3e\u67e5\u8be2\u5339\u914d\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u7531\u5224\u522b\u5668\u3001\u751f\u6210\u5668\u548c\u504f\u597d\u5bf9\u9f50\u6a21\u5757\u7ec4\u6210\uff0c\u65e8\u5728\u540c\u65f6\u4f18\u5316\u67e5\u8be2\u7684\u76f8\u5173\u6027\u548c\u771f\u5b9e\u6027\u5e76\u6700\u5927\u5316\u5e73\u53f0\u6536\u5165\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u90e8\u7f72\u540e\u4e3a\u5e73\u53f0\u521b\u9020\u4e86\u5de8\u5927\u7684\u5546\u4e1a\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u7684\u67e5\u8be2\u91cd\u5199\u7814\u7a76\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u7528\u6237\u539f\u59cb\u67e5\u8be2\u7684\u76f8\u5173\u6027\u548c\u771f\u5b9e\u6027\uff0c\u4e5f\u4e0d\u80fd\u6700\u5927\u7a0b\u5ea6\u5730\u63d0\u9ad8\u53ec\u56de\u5e7f\u544a\u7684\u6536\u5165\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5bf9\u9f50\u51fa\u4ef7\u8bcd\u751f\u6210\u6a21\u578b(MoBGM)\uff0c\u5305\u62ec\u5224\u522b\u5668\u3001\u751f\u6210\u5668\u548c\u504f\u597d\u5bf9\u9f50\u6a21\u5757\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5224\u522b\u5668\u6765\u4f18\u5316\u5173\u952e\u76ee\u6807\uff0c\u5229\u7528\u5224\u522b\u5668\u7684\u53cd\u9988\u4fe1\u53f7\u8bad\u7ec3\u591a\u76ee\u6807\u5bf9\u9f50\u51fa\u4ef7\u8bcd\u751f\u6210\u5668\u3002", "result": "\u5e7f\u6cdb\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002\u5728\u90e8\u7f72\u540e\uff0c\u8be5\u7b97\u6cd5\u4e3a\u5e73\u53f0\u521b\u9020\u4e86\u5de8\u5927\u7684\u5546\u4e1a\u4ef7\u503c\u3002", "conclusion": "MoBGM\u89e3\u51b3\u4e86\u7535\u5b50\u5546\u52a1\u641c\u7d22\u5e7f\u544a\u4e2d\u7684\u957f\u5c3e\u67e5\u8be2\u5339\u914d\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u67e5\u8be2\u7684\u76f8\u5173\u6027\u548c\u771f\u5b9e\u6027\u5e76\u6700\u5927\u5316\u4e86\u5e73\u53f0\u6536\u5165\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832", "abs": "https://arxiv.org/abs/2506.03832", "authors": ["Omer Moussa", "Mariya Toneva"], "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "AI": {"tldr": "Brain-tuning improves speech models' semantic understanding and helps them reflect the brain's intermediate stages of speech processing.", "motivation": "Pretrained self-supervised speech models do not reflect the hierarchy of human speech processing.", "method": "Examine brain-tuned models' alignment with semantic language regions and conduct layer-wise probing.", "result": "Late layers of brain-tuned models improve in aligning with semantic language regions and become better at complex high-level tasks.", "conclusion": "Brain-tuned models not only perform better but also exhibit a well-defined hierarchical processing from acoustic to semantic representations."}}
{"id": "2506.03861", "pdf": "https://arxiv.org/pdf/2506.03861", "abs": "https://arxiv.org/abs/2506.03861", "authors": ["Qiuhan Han", "Qian Wang", "Atsushi Yoshikawa", "Masayuki Yamamura"], "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading", "categories": ["cs.CL"], "comment": null, "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia.", "AI": {"tldr": "This paper presents PulseReddit, a new dataset combining Reddit discussions with cryptocurrency market stats for high-frequency trading analysis. It uses LLM-based Multi-Agent Systems to show that this data improves trading results, especially in bullish markets, and offers insights into choosing LLMs for HFT.", "motivation": "To explore the underused information from social media platforms like Reddit for high-frequency trading in cryptocurrency markets.", "method": "Introduces PulseReddit dataset and uses LLM-based Multi-Agent Systems to study the effect of social sentiment from this data on trading performance.", "result": "MAS with PulseReddit data shows better trading outcomes than traditional methods, especially in bull markets, and adapts well across different market conditions.", "conclusion": "The research proves the value of integrating social media data in high-frequency trading and gives important insights for practical model selection in HFT applications."}}
{"id": "2506.03867", "pdf": "https://arxiv.org/pdf/2506.03867", "abs": "https://arxiv.org/abs/2506.03867", "authors": ["Jacqueline Rowe", "Mateusz Klimaszewski", "Liane Guillou", "Shannon Vallor", "Alexandra Birch"], "title": "EuroGEST: Investigating gender stereotypes in multilingual language models", "categories": ["cs.CL"], "comment": "8 pages, 6 figures, 1 table", "summary": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages.", "AI": {"tldr": "This paper introduces EuroGEST, a dataset measuring gender-stereotypical reasoning in large language models across 30 European languages. It finds that all models exhibit strong gender stereotypes and larger models encode these stereotypes more strongly.", "motivation": "Most benchmarks for gender bias in large language models are English-centric, but this paper aims to address this gap by creating a multilingual dataset.", "method": "EuroGEST was created by expanding an existing expert-informed benchmark using translation tools, quality estimation metrics, and morphological heuristics. Human evaluations confirmed the accuracy of translations and gender labels.", "result": "The study evaluated 24 multilingual language models and found that the strongest stereotypes were consistent across all languages and models, with larger models encoding gendered stereotypes more strongly. Instruction finetuning did not consistently reduce gendered stereotypes.", "conclusion": "This work highlights the need for more multilingual studies of fairness in large language models and provides scalable methods and resources to audit gender bias across languages."}}
{"id": "2506.03880", "pdf": "https://arxiv.org/pdf/2506.03880", "abs": "https://arxiv.org/abs/2506.03880", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Jianhua Tao"], "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRadialRouter\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8def\u7531\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684RadialFormer\u6765\u8868\u8fbe\u67e5\u8be2\u4e0eLLMs\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408Kullback-Leibler\u6563\u5ea6\u4e0e\u67e5\u8be2-\u67e5\u8be2\u5bf9\u6bd4\u635f\u5931\u7684\u76ee\u6807\u51fd\u6570\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u7ba1\u9053\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRadialRouter\u5728\u5e73\u8861\u548c\u6210\u672c\u4f18\u5148\u573a\u666f\u4e2d\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e869.2\uff05\u548c5.8\uff05\u3002", "motivation": "\u5f53\u524dLLM\u8def\u7531\u65b9\u6cd5\u7531\u4e8e\u672a\u80fd\u5145\u5206\u63a2\u7d22\u7528\u6237\u67e5\u8be2\u4e0eLLMs\u7279\u6027\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u5728\u6709\u6548\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRadialRouter\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u7528RadialFormer\u6765\u8868\u8fbe\u67e5\u8be2-LLMs\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u76ee\u6807\u51fd\u6570\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728RouterBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRadialRouter\u5728\u5e73\u8861\u548c\u6210\u672c\u4f18\u5148\u573a\u666f\u4e2d\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RadialRouter\u5c55\u793a\u4e86\u5bf9\u4e0d\u540c\u6027\u80fd-\u6210\u672c\u6743\u8861\u548c\u52a8\u6001LLM\u6c60\u7684\u9002\u5e94\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.03884", "pdf": "https://arxiv.org/pdf/2506.03884", "abs": "https://arxiv.org/abs/2506.03884", "authors": ["Utkarsh Pathak", "Chandra Sai Krishna Gunda", "Anusha Prakash", "Keshav Agarwal", "Hema A. Murthy"], "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages", "categories": ["cs.CL", "cs.CV", "I.5.4"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Text-to-speech (TTS) systems typically require high-quality studio data and\naccurate transcriptions for training. India has 1369 languages, with 22\nofficial using 13 scripts. Training a TTS system for all these languages, most\nof which have no digital resources, seems a Herculean task. Our work focuses on\nzero-shot synthesis, particularly for languages whose scripts and phonotactics\ncome from different families. The novelty of our work is in the augmentation of\na shared phone representation and modifying the text parsing rules to match the\nphonotactics of the target language, thus reducing the synthesiser overhead and\nenabling rapid adaptation. Intelligible and natural speech was generated for\nSanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging\nlinguistic connections across languages with suitable synthesisers. Evaluations\nconfirm the effectiveness of this approach, highlighting its potential to\nexpand speech technology access for under-represented languages.", "AI": {"tldr": "Develop a method for zero-shot synthesis of text-to-speech in under-resourced languages by adapting linguistic connections.", "motivation": "To enable speech technology access for under-represented languages in India.", "method": "Augment shared phone representation and modify text parsing rules to match target language phonotactics.", "result": "Intelligible and natural speech was generated for Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh.", "conclusion": "This approach reduces synthesiser overhead and enables rapid adaptation for under-represented languages."}}
{"id": "2506.03887", "pdf": "https://arxiv.org/pdf/2506.03887", "abs": "https://arxiv.org/abs/2506.03887", "authors": ["Junyi Chen", "Shihao Bai", "Zaijun Wang", "Siyu Wu", "Chuheng Du", "Hailong Yang", "Ruihao Gong", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "title": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation", "categories": ["cs.CL"], "comment": "Published as a conference paper at ACL 2025", "summary": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm.", "AI": {"tldr": "Pre$^3$ optimizes LLM decoding efficiency by transforming LR(1) transition graphs into DPDA, reducing TPOT by up to 40% and increasing throughput by up to 36%.", "motivation": "Efficient structured generations for LR(1) grammars are needed to produce outputs in specified formats.", "method": "Pre$^3$ uses prefix-conditioned edges to enable ahead-of-time edge analysis and transitions from LR(1) graphs to DPDA.", "result": "Pre$^3$ reduces TPOT by up to 40% and increases throughput by up to 36% in experiments.", "conclusion": "Pre$^3$ improves the efficiency of LLM decoding for LR(1) grammars by optimizing the transition process."}}
{"id": "2506.03901", "pdf": "https://arxiv.org/pdf/2506.03901", "abs": "https://arxiv.org/abs/2506.03901", "authors": ["Yuxin Zhang", "Yan Wang", "Yongrui Chen", "Shenyu Zhang", "Xinbang Dai", "Sheng Bi", "Guilin Qi"], "title": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge.\n  However, RAG systems are highly sensitive to retrieval noise prevalent in\nreal-world scenarios.\n  Existing benchmarks fail to emulate the complex and heterogeneous noise\ndistributions encountered in real-world retrieval environments, undermining\nreliable robustness assessment.\n  In this paper, we define four categories of retrieval noise based on\nlinguistic properties and noise characteristics, aiming to reflect the\nheterogeneity of noise in real-world scenarios.\n  Building on this, we introduce Magic Mushroom, a benchmark for replicating\n\"magic mushroom\" noise: contexts that appear relevant on the surface but\ncovertly mislead RAG systems.\n  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer\npairs.\n  More importantly, Magic Mushroom enables researchers to flexibly configure\ncombinations of retrieval noise according to specific research objectives or\napplication scenarios, allowing for highly controlled evaluation setups.\n  We evaluate LLM generators of varying parameter scales and classic RAG\ndenoising strategies under diverse noise distributions to investigate their\nperformance dynamics during progressive noise encroachment.\n  Our analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions.\n  Magic Mushroom emerges as a promising tool for evaluating and advancing\nnoise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications.\n  The Magic Mushroom benchmark is available at the\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.", "AI": {"tldr": "This paper introduces Magic Mushroom, a new benchmark for evaluating Retrieval-Augmented Generation (RAG) systems' robustness against retrieval noise.", "motivation": "Existing benchmarks fail to accurately simulate real-world retrieval noise, leading to unreliable robustness assessments.", "method": "Defining four categories of retrieval noise and creating Magic Mushroom with 7,468 single-hop and 3,925 multi-hop question-answer pairs.", "result": "Magic Mushroom shows that both large language model generators and denoising strategies need significant improvements and are highly sensitive to different noise distributions.", "conclusion": "Magic Mushroom serves as a valuable resource for enhancing noise-robust RAG systems and promoting their practical application."}}
{"id": "2506.03902", "pdf": "https://arxiv.org/pdf/2506.03902", "abs": "https://arxiv.org/abs/2506.03902", "authors": ["Eleftheria Tsipidi", "Samuel Kiegeland", "Franz Nowak", "Tianyang Xu", "Ethan Wilcox", "Alex Warstadt", "Ryan Cotterell", "Mario Giulianelli"], "title": "The Harmonic Structure of Information Contours", "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "The uniform information density (UID) hypothesis proposes that speakers aim\nto distribute information evenly throughout a text, balancing production effort\nand listener comprehension difficulty. However, language typically does not\nmaintain a strictly uniform information rate; instead, it fluctuates around a\nglobal average. These fluctuations are often explained by factors such as\nsyntactic constraints, stylistic choices, or audience design. In this work, we\nexplore an alternative perspective: that these fluctuations may be influenced\nby an implicit linguistic pressure towards periodicity, where the information\nrate oscillates at regular intervals, potentially across multiple frequencies\nsimultaneously. We apply harmonic regression and introduce a novel extension\ncalled time scaling to detect and test for such periodicity in information\ncontours. Analyzing texts in English, Spanish, German, Dutch, Basque, and\nBrazilian Portuguese, we find consistent evidence of periodic patterns in\ninformation rate. Many dominant frequencies align with discourse structure,\nsuggesting these oscillations reflect meaningful linguistic organization.\nBeyond highlighting the connection between information rate and discourse\nstructure, our approach offers a general framework for uncovering structural\npressures at various levels of linguistic granularity.", "AI": {"tldr": "This study explores the influence of periodicity on information rate fluctuations in six languages, revealing consistent periodic patterns aligned with discourse structure.", "motivation": "To investigate whether linguistic pressure towards periodicity affects information rate fluctuations in language, beyond syntactic, stylistic, or audience-related factors.", "method": "Applying harmonic regression and introducing time scaling to detect periodicity in information contours across English, Spanish, German, Dutch, Basque, and Brazilian Portuguese texts.", "result": "Consistent evidence of periodic patterns in information rate across analyzed languages, with many dominant frequencies corresponding to discourse structure.", "conclusion": "Periodicity in information rate reflects meaningful linguistic organization and offers a new framework for understanding structural pressures in language."}}
{"id": "2506.03913", "pdf": "https://arxiv.org/pdf/2506.03913", "abs": "https://arxiv.org/abs/2506.03913", "authors": ["Claire Barale", "Michael Rovatsos", "Nehal Bhuta"], "title": "When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Legal decisions are increasingly evaluated for fairness, consistency, and\nbias using machine learning (ML) techniques. In high-stakes domains like\nrefugee adjudication, such methods are often applied to detect disparities in\noutcomes. Yet it remains unclear whether statistical methods can meaningfully\nassess fairness in legal contexts shaped by discretion, normative complexity,\nand limited ground truth.\n  In this paper, we empirically evaluate three common ML approaches\n(feature-based analysis, semantic clustering, and predictive modeling) on a\nlarge, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our\nexperiments show that these methods produce divergent and sometimes\ncontradictory signals, that predictive modeling often depends on contextual and\nprocedural features rather than legal features, and that semantic clustering\nfails to capture substantive legal reasoning.\n  We show limitations of statistical fairness evaluation, challenge the\nassumption that statistical regularity equates to fairness, and argue that\ncurrent computational approaches fall short of evaluating fairness in legally\ndiscretionary domains. We argue that evaluating fairness in law requires\nmethods grounded not only in data, but in legal reasoning and institutional\ncontext.", "AI": {"tldr": "This paper evaluates three common machine learning approaches on a large dataset of Canadian refugee decisions, finding they produce conflicting signals and fail to adequately assess fairness in legally discretionary domains.", "motivation": "To examine whether statistical methods can effectively assess fairness in legal contexts shaped by discretion, normative complexity, and limited ground truth.", "method": "Feature-based analysis, semantic clustering, and predictive modeling.", "result": "These methods produce divergent and contradictory signals; predictive modeling often depends on contextual/procedural features rather than legal ones; semantic clustering fails to capture substantive legal reasoning.", "conclusion": "Statistical fairness evaluation has limitations; current computational approaches fall short of evaluating fairness in legally discretionary domains."}}
{"id": "2506.03916", "pdf": "https://arxiv.org/pdf/2506.03916", "abs": "https://arxiv.org/abs/2506.03916", "authors": ["Agostina Calabrese", "Tom Sherborne", "Bj\u00f6rn Ross", "Mirella Lapata"], "title": "Compositional Generalisation for Explainable Hate Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is key to online content moderation, but current models\nstruggle to generalise beyond their training data. This has been linked to\ndataset biases and the use of sentence-level labels, which fail to teach models\nthe underlying structure of hate speech. In this work, we show that even when\nmodels are trained with more fine-grained, span-level annotations (e.g.,\n\"artists\" is labeled as target and \"are parasites\" as dehumanising comparison),\nthey struggle to disentangle the meaning of these labels from the surrounding\ncontext. As a result, combinations of expressions that deviate from those seen\nduring training remain particularly difficult for models to detect. We\ninvestigate whether training on a dataset where expressions occur with equal\nfrequency across all contexts can improve generalisation. To this end, we\ncreate U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel\ncompositional generalisation benchmark of ~8,000 manually validated posts.\nTraining on a combination of U-PLEAD and real data improves compositional\ngeneralisation while achieving state-of-the-art performance on the\nhuman-sourced PLEAD.", "AI": {"tldr": "Detecting hate speech is important for managing online content, but current models have trouble generalizing beyond their training data due to biases and label types. Fine-tuning models with more specific annotations still presents challenges. Training on a balanced dataset like U-PLEAD helps models generalize better while maintaining high performance.", "motivation": "Current hate speech detection models struggle to generalize beyond their training data due to biases and label types.", "method": "Creating U-PLEAD, a large synthetic dataset, and a new benchmark for compositional generalization to improve model training.", "result": "Training on U-PLEAD combined with real data improves how well models can generalize compositionally while also achieving top performance on human-sourced data.", "conclusion": "Using a balanced dataset like U-PLEAD can enhance the generalization ability of hate speech detection models without sacrificing performance."}}
{"id": "2506.03922", "pdf": "https://arxiv.org/pdf/2506.03922", "abs": "https://arxiv.org/abs/2506.03922", "authors": ["Zhaolu Kang", "Junhao Gong", "Jiaxu Yan", "Wanke Xia", "Yian Wang", "Ziwen Wang", "Huaxuan Ding", "Zhuo Cheng", "Wenhao Cao", "Zhiyuan Feng", "Siqi He", "Shannan Yan", "Junzhe Chen", "Xiaomin He", "Chaoya Jiang", "Wei Ye", "Kaidong Yu", "Xuelong Li"], "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\npotential to advance a broad range of domains. However, current benchmarks for\nevaluating MLLMs primarily emphasize general knowledge and vertical\nstep-by-step reasoning typical of STEM disciplines, while overlooking the\ndistinct needs and potential of the Humanities and Social Sciences (HSS). Tasks\nin the HSS domain require more horizontal, interdisciplinary thinking and a\ndeep integration of knowledge across related fields, which presents unique\nchallenges for MLLMs, particularly in linking abstract concepts with\ncorresponding visual representations. Addressing this gap, we present HSSBench,\na dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks\nin multiple languages, including the six official languages of the United\nNations. We also introduce a novel data generation pipeline tailored for HSS\nscenarios, in which multiple domain experts and automated agents collaborate to\ngenerate and iteratively refine each sample. HSSBench contains over 13,000\nmeticulously designed samples, covering six key categories. We benchmark more\nthan 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant\nchallenges even for state-of-the-art models. We hope that this benchmark will\ninspire further research into enhancing the cross-disciplinary reasoning\nabilities of MLLMs, especially their capacity to internalize and connect\nknowledge across fields.", "AI": {"tldr": "Evaluate Multimodal Large Language Models (MLLMs) on Humanities and Social Sciences (HSS) tasks.", "motivation": "Current MLLM benchmarks mainly focus on STEM, neglecting the distinct needs of HSS.", "method": "Introduce HSSBench, a benchmark for evaluating MLLMs on HSS tasks in multiple languages, generated by domain experts and automated agents.", "result": "HSSBench has over 13,000 samples across six categories, presenting significant challenges to current MLLMs.", "conclusion": "HSSBench aims to improve MLLMs' cross-disciplinary reasoning abilities and knowledge connection across fields."}}
{"id": "2506.03923", "pdf": "https://arxiv.org/pdf/2506.03923", "abs": "https://arxiv.org/abs/2506.03923", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.", "AI": {"tldr": "Investigate mechanisms of semantic cues shaping reasoning in large language models using comparative math problems.", "motivation": "Understand how LLMs are influenced by input phrasing and semantic cues.", "method": "Introduce MathComp, a benchmark evaluating 300 scenarios under 14 prompt variants across three LLM families.", "result": "Found framing bias in LLMs, chain-of-thought prompting helps but varies in effectiveness, and demographic identity terms amplify directional drift.", "conclusion": "Highlighting the need for framing-aware benchmarks to assess reasoning robustness and fairness in LLMs."}}
{"id": "2506.03941", "pdf": "https://arxiv.org/pdf/2506.03941", "abs": "https://arxiv.org/abs/2506.03941", "authors": ["Vivian Nguyen", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil"], "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations", "categories": ["cs.CL", "cs.AI", "cs.CY", "physics.soc-ph"], "comment": "To appear in the Proceedings of ACL 2025. Code and demo available in\n  ConvoKit (convokit.cornell.edu)", "summary": "During a conversation, there can come certain moments where its outcome hangs\nin the balance. In these pivotal moments, how one responds can put the\nconversation on substantially different trajectories leading to significantly\ndifferent outcomes. Systems that can detect when such moments arise could\nassist conversationalists in domains with highly consequential outcomes, such\nas mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting\nsuch pivotal moments as they happen, in an online fashion. Our approach relies\non the intuition that a moment is pivotal if our expectation of the outcome\nvaries widely depending on what might be said next. By applying our method to\ncrisis counseling conversations, we first validate it by showing that it aligns\nwith human perception -- counselors take significantly longer to respond during\nmoments detected by our method -- and with the eventual conversational\ntrajectory -- which is more likely to change course at these times. We then use\nour framework to explore the relation of the counselor's response during\npivotal moments with the eventual outcome of the session.", "AI": {"tldr": "An unsupervised computational method is introduced to detect pivotal moments in conversations in real-time.", "motivation": "To assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling.", "method": "An unsupervised computational method relying on the intuition that a moment is pivotal if our expectation of the outcome varies widely depending on what might be said next.", "result": "The method aligns with human perception and conversational trajectory changes. It also explores the relation of the counselor's response during pivotal moments with the eventual outcome of the session.", "conclusion": "This work provides a way to detect pivotal moments in conversations in real-time, which could be beneficial for mental health crisis counseling."}}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949", "abs": "https://arxiv.org/abs/2506.03949", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.", "AI": {"tldr": "Introduce TableEval, a new benchmark for evaluating LLMs on realistic TableQA tasks including diverse table structures and cross-lingual scenarios, with a proposed SEAT evaluation framework.", "motivation": "Existing TableQA benchmarks are limited by their focus on simple flat tables and suffer from data leakage. Most benchmarks are monolingual and fail to capture cross-lingual and cross-domain variability.", "method": "Collect tables with various structures from four domains and in three languages. Propose SEAT, a new evaluation framework assessing the alignment between model responses and reference answers at the sub-question level.", "result": "TableEval includes tables with various structures from four domains and in three languages. SEAT achieves high agreement with human judgment.", "conclusion": "State-of-the-art LLMs have critical gaps in handling complex, real-world TableQA tasks. TableEval and SEAT provide insights for future improvements."}}
{"id": "2506.03968", "pdf": "https://arxiv.org/pdf/2506.03968", "abs": "https://arxiv.org/abs/2506.03968", "authors": ["Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding", "categories": ["cs.CL"], "comment": "To be published at ACL 2025", "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.", "AI": {"tldr": "This paper introduces a method for creating large amounts of complex and diverse synthetic instructions by using attributed grounding, resulting in a dataset called SynthQuestions with over 1 million instructions. Models trained on this dataset show improved performance on various benchmarks.", "motivation": "To address the limitations of existing methods for generating synthetic instructions, which either have narrow distributions or fail to produce meaningful trajectories in terms of complexity, by creating instructions that are cognitively insightful and grounded in real-world use cases.", "method": "Attributed grounding, which includes a top-down attribution process to ground real instructions to users and a bottom-up synthesis process to generate situations and instructions based on web documents.", "result": "A dataset of 1 million instructions named SynthQuestions, which improves model performance on several common benchmarks.", "conclusion": "Models trained on the SynthQuestions dataset show leading performance on various benchmarks and this performance scales with more web corpora."}}
{"id": "2506.03978", "pdf": "https://arxiv.org/pdf/2506.03978", "abs": "https://arxiv.org/abs/2506.03978", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Viet Anh Nguyen"], "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets.", "AI": {"tldr": "Model pruning in transformer-based language models can enhance reasoning capabilities.", "motivation": "The selective pruning of certain attention heads improves reasoning performance on challenging tasks.", "method": "SPRINT, a novel contrastive learning framework that dynamically selects the optimal head and layer to prune during inference by aligning question embeddings with head embeddings.", "result": "Our method outperforms traditional best-of-N and random head selection strategies on the MATH500 and GSM8K datasets.", "conclusion": "Selective pruning of attention heads can improve reasoning performance."}}
{"id": "2506.03980", "pdf": "https://arxiv.org/pdf/2506.03980", "abs": "https://arxiv.org/abs/2506.03980", "authors": ["Takeshi Saga", "Catherine Pelachaud"], "title": "Voice Activity Projection Model with Multimodal Encoders", "categories": ["cs.CL"], "comment": null, "summary": "Turn-taking management is crucial for any social interaction. Still, it is\nchallenging to model human-machine interaction due to the complexity of the\nsocial context and its multimodal nature. Unlike conventional systems based on\nsilence duration, previous existing voice activity projection (VAP) models\nsuccessfully utilized a unified representation of turn-taking behaviors as\nprediction targets, which improved turn-taking prediction performance.\nRecently, a multimodal VAP model outperformed the previous state-of-the-art\nmodel by a significant margin. In this paper, we propose a multimodal model\nenhanced with pre-trained audio and face encoders to improve performance by\ncapturing subtle expressions. Our model performed competitively, and in some\ncases, even better than state-of-the-art models on turn-taking metrics. All the\nsource codes and pretrained models are available at\nhttps://github.com/sagatake/VAPwithAudioFaceEncoders.", "AI": {"tldr": "This paper proposes a multimodal model enhanced with pre-trained audio and face encoders for turn-taking prediction in human-machine interaction, which performs competitively or even better than state-of-the-art models.", "motivation": "To improve turn-taking prediction performance in human-machine interaction by capturing subtle expressions using pre-trained audio and face encoders.", "method": "Proposing a multimodal model enhanced with pre-trained audio and face encoders.", "result": "The proposed model performed competitively or even better than state-of-the-art models on turn-taking metrics.", "conclusion": "The proposed multimodal model with pre-trained audio and face encoders is effective in improving turn-taking prediction performance."}}
{"id": "2506.03984", "pdf": "https://arxiv.org/pdf/2506.03984", "abs": "https://arxiv.org/abs/2506.03984", "authors": ["Carolin Holtermann", "Paul R\u00f6ttger", "Anne Lauscher"], "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks.", "AI": {"tldr": "This paper presents GeoTemp, a dataset evaluating the ability of language models to jointly reason over time and space.", "motivation": "Previous work on language models' logical reasoning abilities in time and space was limited to isolated tests or simple/artificial environments.", "method": "The authors created GeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37 time zones, and evaluated eight open chat models from three different model families.", "result": "Most models perform well on tasks involving only temporal knowledge, but struggle with tasks requiring connection between temporal and geographical information. Performance increases for locations with low model perplexity, and prompt formulation significantly affects performance.", "conclusion": "Language models show varying levels of success in joint temporal-spatial reasoning, with performance influenced by factors such as prompt formulation and location familiarity."}}
{"id": "2506.03989", "pdf": "https://arxiv.org/pdf/2506.03989", "abs": "https://arxiv.org/abs/2506.03989", "authors": ["Alex Laitenberger", "Christopher D. Manning", "Nelson F. Liu"], "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models", "categories": ["cs.CL"], "comment": "10 pages, 5 figures, for associated source code, see\n  https://github.com/alex-laitenberger/stronger-baselines-rag", "summary": "With the rise of long-context language models (LMs) capable of processing\ntens of thousands of tokens in a single pass, do multi-stage\nretrieval-augmented generation (RAG) pipelines still offer measurable benefits\nover simpler, single-stage approaches? To assess this question, we conduct a\ncontrolled evaluation for QA tasks under systematically scaled token budgets,\ncomparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three\nbaselines, including DOS RAG (Document's Original Structure RAG), a simple\nretrieve-then-read method that preserves original passage order. Despite its\nstraightforward design, DOS RAG consistently matches or outperforms more\nintricate methods on multiple long-context QA benchmarks. We recommend\nestablishing DOS RAG as a simple yet strong baseline for future RAG\nevaluations, pairing it with emerging embedding and language models to assess\ntrade-offs between complexity and effectiveness as model capabilities evolve.", "AI": {"tldr": "This study evaluates the performance of multi-stage RAG pipelines versus simpler approaches, finding that a straightforward retrieve-then-read method (DOS RAG) performs well across various benchmarks.", "motivation": "To evaluate whether multi-stage retrieval-augmented generation (RAG) pipelines still have advantages over simpler, single-stage approaches when long-context language models can process tens of thousands of tokens.", "method": "Conducting a controlled evaluation for QA tasks under systematically scaled token budgets, comparing two recent multi-stage pipelines (ReadAgent and RAPTOR) against three baselines including DOS RAG.", "result": "DOS RAG consistently matches or outperforms more complex methods on multiple long-context QA benchmarks.", "conclusion": "DOS RAG is recommended as a simple yet strong baseline for future RAG evaluations."}}
{"id": "2506.03990", "pdf": "https://arxiv.org/pdf/2506.03990", "abs": "https://arxiv.org/abs/2506.03990", "authors": ["Hongzhi Zhang", "Jingyuan Zhang", "Xingguang Ji", "Qi Wang", "Fuzheng Zhang"], "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aDynTok\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u538b\u7f29\u89c6\u9891\u4ee4\u724c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4ee4\u724c\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u89c6\u9891\u5efa\u6a21\u63d0\u4f9b\u4e86\u601d\u8def\u3002", "motivation": "\u5178\u578b\u7684\u89c6\u9891\u5efa\u6a21\u65b9\u6cd5\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u89c6\u89c9\u4ee4\u724c\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u957f\u89c6\u9891\u3002\u4e3a\u4e86\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u5c06\u5927\u91cf\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f\u8f93\u5165\u5230LLM\u9aa8\u5e72\u4e4b\u524d\uff0c\u5148\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDynTok\u7684\u65b0\u578b\u52a8\u6001\u89c6\u9891\u4ee4\u724c\u538b\u7f29\u7b56\u7565\u3002\u8be5\u7b56\u7565\u5c06\u89c6\u89c9\u4ee4\u724c\u81ea\u9002\u5e94\u5730\u5206\u4e3a\u7ec4\u5e76\u5408\u5e76\u6bcf\u4e2a\u7ec4\u5185\u7684\u4ee4\u724c\uff0c\u4ece\u800c\u5728\u4fe1\u606f\u5bc6\u5ea6\u4f4e\u7684\u533a\u57df\u5b9e\u73b0\u9ad8\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u5185\u5bb9\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u4ee4\u724c\u6570\u91cf\u51cf\u5c11\u5230\u539f\u59cb\u5927\u5c0f\u768444.4%\uff0c\u5e76\u5728Video-MME\u4e0a\u8fbe\u523065.3%\uff0c\u5728MLVU\u4e0a\u8fbe\u523072.5%\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165DynTok\u65b9\u6cd5\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u89c6\u9891\u4ee4\u724c\u8868\u793a\u7684\u9ad8\u6548\u538b\u7f29\uff0c\u51cf\u5c11\u4e8655.6%\u7684\u4ee4\u724c\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u7684\u53ef\u6bd4\u6027\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u65b9\u6cd5\u8fd8\u63ed\u793a\u4e86\u89c6\u9891\u4ee4\u724c\u8868\u793a\u4e2d\u7684\u5197\u4f59\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u89c6\u9891\u5efa\u6a21\u6280\u672f\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2506.03993", "pdf": "https://arxiv.org/pdf/2506.03993", "abs": "https://arxiv.org/abs/2506.03993", "authors": ["Saif M. Mohammad"], "title": "Words of Warmth: Trust and Sociability Norms for over 26k English Words", "categories": ["cs.CL", "cs.CY"], "comment": "In Proceedings of ACL 2025 Main", "summary": "Social psychologists have shown that Warmth (W) and Competence (C) are the\nprimary dimensions along which we assess other people and groups. These\ndimensions impact various aspects of our lives from social competence and\nemotion regulation to success in the work place and how we view the world. More\nrecent work has started to explore how these dimensions develop, why they have\ndeveloped, and what they constitute. Of particular note, is the finding that\nwarmth has two distinct components: Trust (T) and Sociability (S). In this\nwork, we introduce Words of Warmth, the first large-scale repository of\nmanually derived word--warmth (as well as word--trust and word--sociability)\nassociations for over 26k English words. We show that the associations are\nhighly reliable. We use the lexicons to study the rate at which children\nacquire WCTS words with age. Finally, we show that the lexicon enables a wide\nvariety of bias and stereotype research through case studies on various target\nentities. Words of Warmth is freely available at:\nhttp://saifmohammad.com/warmth.html", "AI": {"tldr": "A new large-scale repository of manually derived word-warmth associations for over 26k English words is introduced.", "motivation": "To study the development and constitution of warmth dimensions and their components trust and sociability.", "method": "Manual derivation of word-warmth associations.", "result": "The associations are highly reliable and can be used to study the acquisition of WCTS words with age and to enable bias and stereotype research through case studies on various target entities.", "conclusion": "Words of Warmth is a valuable resource for social psychologists and is freely available online."}}
{"id": "2506.03994", "pdf": "https://arxiv.org/pdf/2506.03994", "abs": "https://arxiv.org/abs/2506.03994", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "categories": ["cs.CL", "cs.CV"], "comment": "ACL Findings 2025", "summary": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities.", "AI": {"tldr": "Investigate how large-scale models trained on massive data represent semantic features of concrete object concepts; compare image encoders (image-only, multimodal, and language-only models) using McRae norms and Binder dataset; find multimodal image encoders outperform language-only approaches.", "motivation": "Compare the representation of semantic features of concrete object concepts between large-scale models and human learning grounded in sensorimotor experience.", "method": "Use probing tasks to evaluate image encoders (image-only, multimodal, and language-only models) on predicting McRae norms and Binder dataset.", "result": "Multimodal image encoders slightly outperform language-only approaches; image-only encoders perform comparably to language models on non-visual attributes.", "conclusion": "Insights into what can be learned from pure unimodal learning and the complementarity of the modalities."}}
{"id": "2506.04020", "pdf": "https://arxiv.org/pdf/2506.04020", "abs": "https://arxiv.org/abs/2506.04020", "authors": ["An Quang Tang", "Xiuzhen Zhang", "Minh Ngoc Dinh", "Zhuang Li"], "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering", "categories": ["cs.CL"], "comment": "Paper accepted to ACL 2025 Main Conference", "summary": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1 QQSUM\uff0c\u65e8\u5728\u603b\u7ed3\u591a\u6837\u5316\u7684\u5ba2\u6237\u610f\u89c1\u5e76\u91cf\u5316\u5176\u666e\u904d\u6027\u4ee5\u6709\u6548\u56de\u7b54\u7528\u6237\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u7684 PQA \u7cfb\u7edf\u751f\u6210\u7684\u7b54\u6848\u53ea\u5305\u542b\u5355\u4e00\u7684\u89c2\u70b9\uff0c\u65e0\u6cd5\u6355\u6349\u5ba2\u6237\u610f\u89c1\u7684\u591a\u6837\u6027\u3002", "method": "QQSUM-RAG \u6a21\u578b\u901a\u8fc7\u91c7\u7528\u5c11\u91cf\u5b66\u4e60\u6765\u8054\u5408\u8bad\u7ec3\u4e00\u4e2a\u9762\u5411\u5173\u952e\u70b9\u7684\u68c0\u7d22\u5668\u548c\u4e00\u4e2a\u5173\u952e\u70b9\u6458\u8981\u751f\u6210\u5668\uff0c\u4ece\u800c\u80fd\u591f\u6355\u83b7\u591a\u6837\u5316\u4e14\u6709\u4ee3\u8868\u6027\u7684\u89c2\u70b9\u7684\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u6458\u8981\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQQSUM-RAG \u5728\u6587\u672c\u8d28\u91cf\u548c\u610f\u89c1\u91cf\u5316\u51c6\u786e\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 RAG \u57fa\u7ebf\u3002", "conclusion": "QQSUM-RAG \u5728\u6587\u672c\u8d28\u91cf\u548c\u610f\u89c1\u91cf\u5316\u51c6\u786e\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 RAG \u57fa\u7ebf\u3002"}}
{"id": "2506.04032", "pdf": "https://arxiv.org/pdf/2506.04032", "abs": "https://arxiv.org/abs/2506.04032", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "categories": ["cs.CL"], "comment": null, "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e EHR \u6570\u636e\u7684 Patient Simulator\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5 AI \u5065\u5eb7\u4ee3\u7406\uff0c\u7ed3\u679c\u663e\u793a\u5176\u8868\u73b0\u4e0e\u771f\u5b9e\u60a3\u8005\u60c5\u5f62\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u73b0\u5b9e\u4e16\u754c\u60a3\u8005\u906d\u9047\u7684 Patient Simulator\uff0c\u63d0\u4f9b\u5408\u6210\u6d4b\u8bd5\u5bf9\u8c61\u4ee5\u5f00\u53d1\u548c\u6d4b\u8bd5\u533b\u7597\u4ee3\u7406\u6a21\u578b\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u6765\u81ea\u771f\u5b9e EHR \u6570\u636e\u7684\u60a3\u8005 vignettes \u7684 AI \u5065\u5eb7\u4ee3\u7406\u3002", "method": "\u9996\u5148\uff0c\u901a\u8fc7\u4ece\u771f\u5b9e\u4e16\u754c\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55 (EHR) \u9047\u89c1\u4e2d\u63a8\u5bfc\u51fa\u75c5\u4f8b\u7247\u6bb5\u6765\u6784\u5efa\u903c\u771f\u7684\u4e34\u5e8a\u573a\u666f\u3002\u7136\u540e\uff0c\u5728\u8d85\u8fc7 500 \u4e2a\u4e0d\u540c\u7684\u75c5\u4f8b\u7247\u6bb5\u4e0a\u8bc4\u4f30 Patient Simulator \u4f5c\u4e3a\u771f\u5b9e\u60a3\u8005\u9047\u89c1\u7684 simulacrum\uff08\u6a21\u62df\u7269\uff09\u3002\u5229\u7528\u72ec\u7acb\u7684 AI \u4ee3\u7406\u8fdb\u884c\u591a\u8f6e\u63d0\u95ee\u4ee5\u83b7\u53d6\u75c5\u53f2\uff0c\u5e76\u7531\u4e24\u540d\u4e13\u5bb6\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u7531\u6b64\u4ea7\u751f\u7684\u591a\u8f6e\u5bf9\u8bdd\u3002", "result": "\u4e34\u5e8a\u533b\u751f\u5728\u90a3\u4e9b\u76f8\u540c\u7684 97.7% \u6848\u4f8b\u4e2d\u8bc4\u5206 Patient Simulator \u4e0e\u75c5\u4f8b\u7247\u6bb5\u4e00\u81f4\u3002\u57fa\u4e8e\u5bf9\u8bdd\u5386\u53f2\u63d0\u53d6\u7684\u6848\u4f8b\u6458\u8981\u6709 99% \u662f\u76f8\u5173\u7684\u3002", "conclusion": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u6e90\u81ea\u771f\u5b9e\u533b\u7597\u60a3\u8005\u6570\u636e\u7684 vignettes\uff08\u75c5\u4f8b\u7247\u6bb5\uff09\u7eb3\u5165\u6784\u5efa\u60a3\u8005\u5bf9\u75c7\u72b6\u68c0\u67e5\u4ee3\u7406\u53cd\u5e94\u7684\u6a21\u62df\u3002\u8be5 Patient Simulator \u7684\u6027\u80fd\u548c\u4e00\u81f4\u6027\u53ef\u7528\u4e8e\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u6d4b\u8bd5\u591a\u8f6e\u5bf9\u8bdd AI \u4ee3\u7406\u3002"}}
{"id": "2506.04037", "pdf": "https://arxiv.org/pdf/2506.04037", "abs": "https://arxiv.org/abs/2506.04037", "authors": ["Dan Oneata", "Leanne Nortje", "Yevgen Matusevych", "Herman Kamper"], "title": "The mutual exclusivity bias of bilingual visually grounded speech models", "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Mutual exclusivity (ME) is a strategy where a novel word is associated with a\nnovel object rather than a familiar one, facilitating language learning in\nchildren. Recent work has found an ME bias in a visually grounded speech (VGS)\nmodel trained on English speech with paired images. But ME has also been\nstudied in bilingual children, who may employ it less due to cross-lingual\nambiguity. We explore this pattern computationally using bilingual VGS models\ntrained on combinations of English, French, and Dutch. We find that bilingual\nmodels generally exhibit a weaker ME bias than monolingual models, though\nexceptions exist. Analyses show that the combined visual embeddings of\nbilingual models have a smaller variance for familiar data, partly explaining\nthe increase in confusion between novel and familiar concepts. We also provide\nnew insights into why the ME bias exists in VGS models in the first place. Code\nand data: https://github.com/danoneata/me-vgs", "AI": {"tldr": "This study investigates mutual exclusivity (ME) bias in bilingual visually grounded speech (VGS) models compared to monolingual ones, finding that bilingual models show a weaker ME bias due to increased cross-lingual ambiguity and smaller variance in visual embeddings for familiar data.", "motivation": "To understand how mutual exclusivity affects language learning in children and how this strategy is impacted by bilingualism and cross-lingual ambiguity in computational models.", "method": "Training bilingual VGS models on combinations of English, French, and Dutch and comparing their ME biases with monolingual models.", "result": "Bilingual models exhibit a weaker ME bias than monolingual models, partly because their combined visual embeddings for familiar data have smaller variance, leading to more confusion between novel and familiar concepts.", "conclusion": "The study provides new insights into the ME bias in VGS models and highlights differences in language learning strategies between monolingual and bilingual contexts."}}
{"id": "2506.04041", "pdf": "https://arxiv.org/pdf/2506.04041", "abs": "https://arxiv.org/abs/2506.04041", "authors": ["Claire Barale", "Leslie Barrett", "Vikram Sunil Bajaj", "Michael Rovatsos"], "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.", "AI": {"tldr": "This paper introduces LexTime, a new dataset for evaluating large language models' ability to order events in legal language, showing that LLMs perform better on legal event ordering than narrative, and that longer input contexts and implicit events improve accuracy.", "motivation": "To address the lack of expert language evaluation in existing datasets for temporal reasoning in legal texts.", "method": "Introduce LexTime, a dataset of 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations.", "result": "LLMs are more accurate on legal event ordering than narrative (up to +10.5%); longer input contexts and implicit events boost accuracy, reaching 80.8% for implicit-explicit event pairs; legal linguistic complexities and nested clauses remain a challenge.", "conclusion": "Specific modeling strategies are needed to enhance temporal event reasoning in legal language."}}
{"id": "2506.04042", "pdf": "https://arxiv.org/pdf/2506.04042", "abs": "https://arxiv.org/abs/2506.04042", "authors": ["Xiyu Liu", "Zhengxiao Liu", "Naibin Gu", "Zheng Lin", "Ji Xiang", "Weiping Wang"], "title": "Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing aims to alternate the target knowledge predicted by large\nlanguage models while ensuring the least side effects on unrelated knowledge.\nAn effective way to achieve knowledge editing is to identify pivotal parameters\nfor predicting factual associations and modify them with an optimization\nprocess to update the predictions. However, these locate-then-edit methods are\nuncontrollable since they tend to modify most unrelated relations connected to\nthe subject of target editing. We unveil that this failure of controllable\nediting is due to a shortcut learning issue during the optimization process.\nSpecifically, we discover two crucial features that are the subject feature and\nthe relation feature for models to learn during optimization, but the current\noptimization process tends to over-learning the subject feature while\nneglecting the relation feature. To eliminate this shortcut learning of the\nsubject feature, we propose a novel two-stage optimization process that\nbalances the learning of the subject feature and the relation feature.\nExperimental results demonstrate that our approach successfully prevents\nknowledge editing from shortcut learning and achieves the optimal overall\nperformance, contributing to controllable knowledge editing.", "AI": {"tldr": "This paper proposes a new optimization process to enable more controlled knowledge editing in large language models.", "motivation": "To alternate the target knowledge predicted by large language models while ensuring the least side effects on unrelated knowledge.", "method": "A novel two-stage optimization process that balances the learning of the subject feature and the relation feature.", "result": "The proposed method demonstrates superior performance compared to previous approaches.", "conclusion": "Our approach successfully prevents knowledge editing from shortcut learning and achieves the optimal overall performance, contributing to controllable knowledge editing."}}
{"id": "2506.04043", "pdf": "https://arxiv.org/pdf/2506.04043", "abs": "https://arxiv.org/abs/2506.04043", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted at ACL WOAH 2025", "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.", "AI": {"tldr": "This study proposes a framework to evaluate LLM-generated counter-narratives across four dimensions, finding them often verbose and accessible only to those with college-level literacy.", "motivation": "Concerns about the affective tone, accessibility, and ethical risks of automated counter-narratives for mitigating online hate speech.", "method": "Proposing a framework for evaluation using three different LLMs and assessing three prompting strategies on two datasets.", "result": "LLM-generated counter-narratives are often verbose and cater to college-level literacy, with emotionally guided prompts yielding more empathetic responses but safety and effectiveness concerns persist.", "conclusion": "The study highlights the need for improvements in accessibility and ethical considerations when utilizing LLM-generated counter-narratives."}}
{"id": "2506.04044", "pdf": "https://arxiv.org/pdf/2506.04044", "abs": "https://arxiv.org/abs/2506.04044", "authors": ["Aleksey Kudelya", "Alexander Shirnin"], "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.", "AI": {"tldr": "Introducing a new algorithm called LIBU for unlearning specific knowledge from large language models without retraining, which combines influence functions and second-order optimization.", "motivation": "Removing specific knowledge from a large language model without retraining from scratch and compromising its overall utility.", "method": "LIBU (LoRA enhanced influence-based unlearning)", "result": "This lightweight approach is well applicable for unlearning LLMs in different kinds of task.", "conclusion": "The algorithm combines classical influence functions and second-order optimization to stabilize the overall utility."}}
{"id": "2506.04047", "pdf": "https://arxiv.org/pdf/2506.04047", "abs": "https://arxiv.org/abs/2506.04047", "authors": ["Yuqian Li", "Yupei Du", "Yufang Liu", "Feifei Feng", "Mou Xiao Feng", "Yuanbin Wu"], "title": "On Support Samples of Next Word Prediction", "categories": ["cs.CL"], "comment": "Accepted to ACL2025(Main Conference)", "summary": "Language models excel in various tasks by making complex decisions, yet\nunderstanding the rationale behind these decisions remains a challenge. This\npaper investigates \\emph{data-centric interpretability} in language models,\nfocusing on the next-word prediction task. Using representer theorem, we\nidentify two types of \\emph{support samples}-those that either promote or deter\nspecific predictions. Our findings reveal that being a support sample is an\nintrinsic property, predictable even before training begins. Additionally,\nwhile non-support samples are less influential in direct predictions, they play\na critical role in preventing overfitting and shaping generalization and\nrepresentation learning. Notably, the importance of non-support samples\nincreases in deeper layers, suggesting their significant role in intermediate\nrepresentation formation.These insights shed light on the interplay between\ndata and model decisions, offering a new dimension to understanding language\nmodel behavior and interpretability.", "AI": {"tldr": "This paper explores data-centric interpretability in language models for next-word prediction, identifying support and non-support samples and revealing their roles in predictions and generalization.", "motivation": "Understanding the rationale behind language model decisions remains challenging.", "method": "Using the representer theorem to identify support samples promoting or deterring specific predictions.", "result": "Support samples are an intrinsic property, predictable before training, while non-support samples prevent overfitting and shape generalization and representation learning.", "conclusion": "Insights into the interaction between data and model decisions offer a new perspective on interpreting language model behavior."}}
{"id": "2506.04050", "pdf": "https://arxiv.org/pdf/2506.04050", "abs": "https://arxiv.org/abs/2506.04050", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "title": "Explainability-Based Token Replacement on LLM-Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.", "AI": {"tldr": "This paper explores using explainable AI methods to make AI-generated text less detectable while also creating a robust ensemble-based detection method.", "motivation": "To address the issue that AI-generated text often exhibits patterns making it easier to detect than human-written text.", "method": "Training an ensemble classifier to distinguish AI-generated text from human-written text, then applying SHAP and LIME to find influential tokens and proposing four token replacement strategies.", "result": "Token replacement approaches can significantly reduce a single classifier's ability to detect AI-generated text, but an ensemble classifier still performs well across different languages and domains.", "conclusion": "Explainable AI methods can make AI-generated text harder to detect, but robust, ensemble-based detection strategies are necessary to counteract such manipulations."}}
{"id": "2506.04051", "pdf": "https://arxiv.org/pdf/2506.04051", "abs": "https://arxiv.org/abs/2506.04051", "authors": ["Tim Franzmeyer", "Archie Sravankumar", "Lijuan Liu", "Yuning Mao", "Rui Hou", "Sinong Wang", "Jakob N. Foerster", "Luke Zettlemoyer", "Madian Khabsa"], "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "AI": {"tldr": "This paper proposes HALT, a method to post-train large language models to generate content only when confident in its correctness, reducing hallucination. HALT improves the mean correctness of response fragments by 15% on average and increases correctness from 51% to 87% across four domains.", "motivation": "To address the problem of hallucination in large language models, which occurs when they lack knowledge or capability.", "method": "Post-train LLMs using HALT to generate content only when confident in its correctness, by producing capability-aligned post-training data that encodes what the model can and cannot reliably generate.", "result": "HALT increases the mean correctness of response fragments by 15% on average, improves F1 score by 4%, and trains a reliable Llama3-70B model with correctness increased from 51% to 87% across four domains.", "conclusion": "HALT effectively reduces hallucination in large language models, improving their reliability and correctness."}}

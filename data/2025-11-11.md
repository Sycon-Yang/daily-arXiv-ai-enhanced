<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 98]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.AI](#cs.AI) [Total: 13]
- [stat.ML](#stat.ML) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CV](#cs.CV) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation](https://arxiv.org/abs/2511.05516)
*Canxiang Yan,Chunxiang Jin,Dawei Huang,Haibing Yu,Han Peng,Hui Zhan,Jie Gao,Jing Peng,Jingdong Chen,Jun Zhou,Kaimeng Ren,Ming Yang,Mingxue Yang,Qiang Xu,Qin Zhao,Ruijie Xiong,Shaoxiong Lin,Xuezhi Wang,Yi Yuan,Yifei Wu,Yongjie Lyu,Zhengyu He,Zhihao Qiu,Zhiqiang Fang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 本文提出了一种统一的语音理解、生成和编辑框架，通过引入MingTok-Audio连续语音分词器和Ming-UniAudio语音语言模型，实现了语音任务的平衡，并在多个基准测试中取得了新的SOTA记录。


<details>
  <summary>Details</summary>
Motivation: 现有的语音模型在语音表示上面临理解与生成任务之间的矛盾需求，这种表示差异限制了语音语言模型在基于指令的自由形式编辑中的表现。因此，需要一种能够统一语音理解、生成和编辑的框架。

Method: 本文提出了一种统一的框架，包括一个统一的连续语音分词器MingTok-Audio，以及基于该分词器的语音语言模型Ming-UniAudio。同时，还开发了专门的语音编辑模型Ming-UniAudio-Edit，并引入了一个针对指令驱动的自由形式语音编辑的基准测试Ming-Freeform-Audio-Edit。

Result: Ming-UniAudio在ContextASR基准测试的12个指标中，在8个指标上达到了新的SOTA记录。特别是在中文语音克隆任务中，它实现了高度竞争性的Seed-TTS-WER为0.95。此外，Ming-Freeform-Audio-Edit基准测试为评估语音编辑能力提供了全面的评估维度。

Conclusion: 本文提出了一种统一的语音理解、生成和编辑框架，通过引入MingTok-Audio连续语音分词器和Ming-UniAudio语音语言模型，实现了语音任务的平衡，并在多个基准测试中取得了新的SOTA记录。此外，还提出了Ming-Freeform-Audio-Edit基准测试，以评估语音编辑能力并为未来研究奠定基础。所有模型和工具均已开源，以促进统一的音频理解和处理的发展。

Abstract: Existing speech models suffer from competing requirements on token representations by understanding and generation tasks. This discrepancy in representation prevents speech language models from performing instruction-based free-form editing. To solve this challenge, we introduce a novel framework that unifies speech understanding, generation, and editing. The core of our unified model is a unified continuous speech tokenizer MingTok-Audio, the first continuous tokenizer to effectively integrate semantic and acoustic features, which makes it suitable for both understanding and generation tasks. Based on this unified continuous audio tokenizer, we developed the speech language model Ming-UniAudio, which achieved a balance between generation and understanding capabilities. Ming-UniAudio sets new state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a dedicated speech editing model Ming-UniAudio-Edit, the first speech language model that enables universal, free-form speech editing guided solely by natural language instructions, handling both semantic and acoustic modifications without timestamp condition. To rigorously assess the editing capability and establish a foundation for future research, we introduce Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for instruction-based free-form speech editing, featuring diverse scenarios and evaluation dimensions spanning semantic correctness, acoustic quality, and instruction alignment. We open-sourced the continuous audio tokenizer, the unified foundational model, and the free-form instruction-based editing model to facilitate the development of unified audio understanding, generation, and manipulation.

</details>


### [2] [Retracing the Past: LLMs Emit Training Data When They Get Lost](https://arxiv.org/abs/2511.05518)
*Myeongseob Ko,Nikhil Reddy Billa,Adam Nguyen,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击方法，用于从大型语言模型中提取记忆化的训练数据，并展示了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）中训练数据的记忆化带来了重大的隐私和版权问题。现有的数据提取方法，尤其是基于启发式的偏差攻击，往往成功率有限，并且对记忆泄漏的基本驱动因素了解不足。

Method: 本文介绍了混淆诱导攻击（CIA），这是一种通过系统地最大化模型不确定性来提取记忆数据的原理性框架。我们进一步提出了不匹配监督微调（SFT），以同时削弱对齐模型的对齐并诱导有针对性的混淆，从而增加其对攻击的易感性。

Result: 我们在各种非对齐和对齐的LLMs上进行了实验，结果表明我们的攻击在无需事先了解训练数据的情况下，比现有基线方法更能提取逐字和近似逐字的训练数据。

Conclusion: 我们的研究突显了各种大型语言模型中持续的记忆风险，并提供了一种更系统的方法来评估这些漏洞。

Abstract: The memorization of training data in large language models (LLMs) poses significant privacy and copyright concerns. Existing data extraction methods, particularly heuristic-based divergence attacks, often exhibit limited success and offer limited insight into the fundamental drivers of memorization leakage. This paper introduces Confusion-Inducing Attacks (CIA), a principled framework for extracting memorized data by systematically maximizing model uncertainty. We empirically demonstrate that the emission of memorized text during divergence is preceded by a sustained spike in token-level prediction entropy. CIA leverages this insight by optimizing input snippets to deliberately induce this consecutive high-entropy state. For aligned LLMs, we further propose Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their alignment and induce targeted confusion, thereby increasing susceptibility to our attacks. Experiments on various unaligned and aligned LLMs demonstrate that our proposed attacks outperform existing baselines in extracting verbatim and near-verbatim training data without requiring prior knowledge of the training data. Our findings highlight persistent memorization risks across various LLMs and offer a more systematic method for assessing these vulnerabilities.

</details>


### [3] [Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning](https://arxiv.org/abs/2511.05532)
*Rufan Zhang,Lin Zhang,Xianghang Mi*

Main category: cs.CL

TL;DR: 本文提出了一种基于上下文学习的新型内容监管框架，实现了跨任务泛化和轻量级个性化，有效提升有害内容检测的适应性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 有害在线内容的激增需要强大且适应性强的监管系统。然而，现有的监管系统是集中式和任务特定的，透明度有限，并忽视了多样化的用户偏好，这种方法不适合隐私敏感或去中心化环境。

Method: 我们提出了一种新颖的框架，利用上下文学习（ICL）与基础模型来统一检测毒性、垃圾邮件和负面情绪，实现了轻量级个性化，允许用户通过简单的提示干预轻松阻止新类别、取消阻止现有类别或扩展检测到语义变体，而无需重新训练模型。

Result: 在公共基准（TextDetox、UCI SMS、SST2）和一个新的标注的Mastodon数据集上的广泛实验表明：（i）基础模型实现了强大的跨任务泛化，通常与任务特定微调模型相媲美或超越；（ii）只需一个用户提供的示例或定义即可实现有效的个性化；（iii）通过标签定义或推理增强提示显著提高了对嘈杂的真实世界数据的鲁棒性。

Conclusion: 我们的工作展示了超越一刀切的监管的明确转变，确立了ICL作为一种实用、隐私保护且高度适应的下一代用户中心内容安全系统的路径。

Abstract: The proliferation of harmful online content--e.g., toxicity, spam, and negative sentiment--demands robust and adaptable moderation systems. However, prevailing moderation systems are centralized and task-specific, offering limited transparency and neglecting diverse user preferences--an approach ill-suited for privacy-sensitive or decentralized environments. We propose a novel framework that leverages in-context learning (ICL) with foundation models to unify the detection of toxicity, spam, and negative sentiment across binary, multi-class, and multi-label settings. Crucially, our approach enables lightweight personalization, allowing users to easily block new categories, unblock existing ones, or extend detection to semantic variations through simple prompt-based interventions--all without model retraining. Extensive experiments on public benchmarks (TextDetox, UCI SMS, SST2) and a new, annotated Mastodon dataset reveal that: (i) foundation models achieve strong cross-task generalization, often matching or surpassing task-specific fine-tuned models; (ii) effective personalization is achievable with as few as one user-provided example or definition; and (iii) augmenting prompts with label definitions or rationales significantly enhances robustness to noisy, real-world data. Our work demonstrates a definitive shift beyond one-size-fits-all moderation, establishing ICL as a practical, privacy-preserving, and highly adaptable pathway for the next generation of user-centric content safety systems. To foster reproducibility and facilitate future research, we publicly release our code on GitHub and the annotated Mastodon dataset on Hugging Face.

</details>


### [4] [MCP4IFC: IFC-Based Building Design Using Large Language Models](https://arxiv.org/abs/2511.05533)
*Bharathi Kannan Nithyanantham,Tobias Sesterhenn,Ashwin Nedungadi,Sergio Peral Garijo,Janis Zenkner,Christian Bartelt,Stefan Lüdtke*

Main category: cs.CL

TL;DR: 本文介绍了MCP4IFC，这是一个开源框架，使大型语言模型能够直接操作IFC数据，通过提供BIM工具和动态代码生成系统来处理复杂任务。


<details>
  <summary>Details</summary>
Motivation: 将生成式AI引入建筑、工程和施工（AEC）领域需要能够将自然语言指令转换为标准化数据模型上的操作的系统。

Method: 我们提出了MCP4IFC，这是一个全面的开源框架，使大型语言模型（LLMs）能够通过Model Context Protocol（MCP）直接操作Industry Foundation Classes（IFC）数据。该框架提供了一套BIM工具，包括场景查询工具用于信息检索、预定义函数用于创建和修改常见建筑元素，以及一个动态代码生成系统，结合上下文学习与检索增强生成（RAG）来处理超出预定义工具集的任务。

Result: 实验表明，使用我们框架的LLM可以成功执行复杂任务，从建造简单的房屋到查询和编辑现有的IFC数据。

Conclusion: 我们的框架是开源的，旨在鼓励研究LLM驱动的BIM设计，并为AI辅助建模工作流程提供基础。

Abstract: Bringing generative AI into the architecture, engineering and construction (AEC) field requires systems that can translate natural language instructions into actions on standardized data models. We present MCP4IFC, a comprehensive open-source framework that enables Large Language Models (LLMs) to directly manipulate Industry Foundation Classes (IFC) data through the Model Context Protocol (MCP). The framework provides a set of BIM tools, including scene querying tools for information retrieval, predefined functions for creating and modifying common building elements, and a dynamic code-generation system that combines in-context learning with retrieval-augmented generation (RAG) to handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM using our framework can successfully perform complex tasks, from building a simple house to querying and editing existing IFC data. Our framework is released as open-source to encourage research in LLM-driven BIM design and provide a foundation for AI-assisted modeling workflows. Our code is available at https://show2instruct.github.io/mcp4ifc/.

</details>


### [5] [FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference](https://arxiv.org/abs/2511.05534)
*Kunxi Li,Yufan Xiong,Zhonghua Jiang,Yiyun Zhou,Zhaode Wang,Chengfei Lv,Shengyu Zhang*

Main category: cs.CL

TL;DR: 本文提出FlowMM框架，通过跨模态信息流引导的多模态KV缓存合并，有效减少内存和延迟，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存驱逐策略在生成质量上存在下降问题，而最近的努力在多模态场景中受到分布偏差和注意力偏差的限制。

Method: 引入了FlowMM框架，利用跨模态信息流动态应用层特定的合并策略，并引入了一种敏感度自适应的token匹配机制。

Result: 在多种领先的MLLM上进行的广泛实验表明，FlowMM将KV缓存内存减少了80%至95%，解码延迟降低了1.3至1.8倍。

Conclusion: FlowMM能够显著减少KV缓存内存并降低解码延迟，同时保持竞争性的任务性能。

Abstract: Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.

</details>


### [6] [Future of AI Models: A Computational perspective on Model collapse](https://arxiv.org/abs/2511.05535)
*Trivikram Satharasi,S Sitharama Iyengar*

Main category: cs.CL

TL;DR: 本研究通过分析2013年至2025年英语维基百科的语义相似性，量化并预测了模型崩溃的发生时间。结果表明，在公共LLM采用之前，相似性逐渐增加，这可能由早期的RNN/LSTM翻译和文本标准化管道驱动，但幅度较小。观察到的波动反映了不可减少的语言多样性、不同年份的语料库大小变化、有限的抽样误差以及LLM模型公共采用后的相似性指数级上升。这些发现提供了数据驱动的估计，以确定递归AI污染何时可能显著威胁数据丰富性和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着合成内容的主导地位，递归训练可能会侵蚀语言和语义多样性，导致模型崩溃。本研究旨在量化和预测模型崩溃的发生时间，以评估递归AI污染对数据丰富性和模型泛化能力的影响。

Method: 本研究使用Transformer嵌入和余弦相似性度量，分析了2013年至2025年英语维基百科（过滤后的Common Crawl）的年度语义相似性。

Result: 研究结果显示，在公共LLM采用之前，相似性逐渐增加，这可能由早期的RNN/LSTM翻译和文本标准化管道驱动，但幅度较小。观察到的波动反映了不可减少的语言多样性、不同年份的语料库大小变化、有限的抽样误差以及LLM模型公共采用后的相似性指数级上升。

Conclusion: 本研究通过分析2013年至2025年英语维基百科（过滤后的Common Crawl）的语义相似性，量化并预测了模型崩溃的发生时间。结果表明，在公共LLM采用之前，相似性逐渐增加，这可能由早期的RNN/LSTM翻译和文本标准化管道驱动，但幅度较小。观察到的波动反映了不可减少的语言多样性、不同年份的语料库大小变化、有限的抽样误差以及LLM模型公共采用后的相似性指数级上升。这些发现提供了数据驱动的估计，以确定递归AI污染何时可能显著威胁数据丰富性和模型泛化能力。

Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.

</details>


### [7] [Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability](https://arxiv.org/abs/2511.05541)
*Usha Bhalla,Alex Oesterling,Claudio Mayrink Verdun,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.CL

TL;DR: The paper proposes Temporal Sparse Autoencoders (T-SAEs) to improve unsupervised interpretability in language models by incorporating linguistic structure into the training process, resulting in better semantic feature discovery.


<details>
  <summary>Details</summary>
Motivation: Current dictionary learning methods for LLMs ignore linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. The paper aims to address this issue by incorporating linguistic structure into the training process.

Method: The paper introduces Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens.

Result: T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality across multiple datasets and models. They exhibit clear semantic structure despite being trained without explicit semantic signal.

Conclusion: T-SAEs offer a new pathway for unsupervised interpretability in language models by recovering smoother, more coherent semantic concepts without sacrificing reconstruction quality.

Abstract: Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as "the phrase 'The' at the start of sentences". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.

</details>


### [8] [UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8](https://arxiv.org/abs/2511.05578)
*Preston Firestone,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.CL

TL;DR: 本文通过单子理论分析了子词分词方法中可能产生的无效UTF-8序列问题，并验证了其在实际应用中的影响。


<details>
  <summary>Details</summary>
Motivation: 当前的子词分词方法在使用代码点或字节构建词汇表时存在一些问题，例如使用代码点需要大量的初始词汇以达到良好的输入覆盖，而使用字节则可能导致生成的序列不是有效的UTF-8。

Method: 本文使用单子理论对分词进行了形式化，并通过形式化结果验证了实际中的错误。

Result: 研究证明了使用无效UTF-8的分词器会生成无效的UTF-8序列，并且逐步转换和一次性转换的结果不同。

Conclusion: 本文通过单子理论形式化了使用无效UTF-8的分词器，并证明了这些分词器总是会产生无效的UTF-8序列。此外，研究还表明，将分词逐步转换回字符串并将其解释为UTF-8与一次性转换整个分词序列的结果不同。

Abstract: Subword tokenization segments input text according to a pre-defined vocabulary to feed it into a language model; the language model, in turn, generates a sequence made from this same vocabulary. The members of the vocabulary can be built of code points or bytes. Using code points means that all members of the vocabulary are valid UTF-8 characters. However, it also requires thousands of initial members to achieve acceptable coverage of inputs. Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with only 256 initial members of the vocabulary, but the members of the vocabulary and sequences of them are not guaranteed to be valid UTF-8. Sequences that are not valid UTF-8 break code that assumes its input to be valid UTF-8. Applications of language models must account for the breakage thereby introduced. In this paper, we formalize tokenization using monoid theory and prove that tokenizers whose vocabularies contain tokens that are ill-formed UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate formally that attempting to incrementally convert tokens back to a string and interpret the results as UTF-8 gives different results than converting the whole sequence of tokens at once. This formal result predicts real-world bugs: we evaluate mitigations for the problem identified and provide case studies of major foundation models, serving engines, and constrained generation systems.

</details>


### [9] [Optimizing Diversity and Quality through Base-Aligned Model Collaboration](https://arxiv.org/abs/2511.05650)
*Yichen Wang,Chenghao Yang,Tenghao Huang,Muhao Chen,Jonathan May,Mina Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为BACo的推理时令牌级模型协作框架，通过动态结合基础LLM与其对齐副本，实现了高多样性和高质量的生成。


<details>
  <summary>Details</summary>
Motivation: 现有的促进多样性的方法，如重新训练、提示工程和多采样方法，虽然提高了多样性，但通常会降低质量或需要昂贵的解码或后训练。

Method: 提出了一种名为BACo的推理时令牌级模型协作框架，该框架动态结合基础LLM与其对齐副本以优化多样性和质量。

Result: BACo在三个开放生成任务和13个覆盖多样性和质量的指标中，始终超越最先进的推理时基线。使用最佳路由器，BACo在多样性和质量上实现了21.3%的联合改进。

Conclusion: 研究结果表明，基础模型和对齐模型之间的协作可以优化和控制多样性与质量。

Abstract: Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.

</details>


### [10] [OckBench: Measuring the Efficiency of LLM Reasoning](https://arxiv.org/abs/2511.05722)
*Zheng Du,Hao Kang,Song Han,Tushar Krishna,Ligeng Zhu*

Main category: cs.CL

TL;DR: 本文介绍了 OckBench，一个评估准确性和 token 效率的基准，揭示了模型在 token 消耗上的显著差异，并倡导重新评估 token 的使用。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要关注准确性，而忽略了解码 token 效率这一重要因素。在实际系统中，生成不同数量的 token 会导致延迟、成本和能耗的巨大差异。

Method: 引入 OckBench，这是一个模型无关和硬件无关的基准，评估推理和编码任务的准确性和 token 数量。

Result: 通过实验比较多个开源和闭源模型，发现许多具有可比准确性的模型在 token 消耗上存在巨大差异，表明效率差异是一个被忽视但重要的区分轴。

Conclusion: OckBench 提供了一个统一的平台，用于测量、比较和指导在 token 效率推理方面的研究。

Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as "free" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .

</details>


### [11] [In-Context Learning Without Copying](https://arxiv.org/abs/2511.05743)
*Kerem Sahin,Sheridan Feucht,Adam Belfki,Jannik Brinkmann,Aaron Mueller,David Bau,Chris Wendler*

Main category: cs.CL

TL;DR: 这项工作研究了抑制归纳复制是否会影响变压器的上下文学习能力。结果表明，即使归纳复制被显著减少，模型仍然能够保持良好的性能，并且在某些任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 我们想知道当归纳复制被抑制时，变压器是否仍能获得ICL能力。

Method: 我们提出了Hapax，一种通过忽略任何可以被归纳头正确预测的标记的损失贡献来抑制归纳复制的设置。

Result: 尽管归纳复制显著减少，但在抽象ICL任务上的表现仍然相当，并且在21个任务中有13个任务的表现超过了普通模型，即使有31.7%的标记被从损失中省略。此外，我们的模型在无法被归纳头正确预测的标记位置上实现了更低的损失值。

Conclusion: 我们的研究结果表明，归纳复制不是学习抽象的上下文学习机制的必要条件。

Abstract: Induction heads are attention heads that perform inductive copying by matching patterns from earlier context and copying their continuations verbatim. As models develop induction heads, they often experience a sharp drop in training loss, a phenomenon cited as evidence that induction heads may serve as a prerequisite for more complex in-context learning (ICL) capabilities. In this work, we ask whether transformers can still acquire ICL capabilities when inductive copying is suppressed. We propose Hapax, a setting where we omit the loss contribution of any token that can be correctly predicted by induction heads. Despite a significant reduction in inductive copying, performance on abstractive ICL tasks (i.e., tasks where the answer is not contained in the input context) remains comparable and surpasses the vanilla model on 13 of 21 tasks, even though 31.7\% of tokens are omitted from the loss. Furthermore, our model achieves lower loss values on token positions that cannot be predicted correctly by induction heads. Mechanistic analysis further shows that models trained with Hapax develop fewer and weaker induction heads but still preserve ICL capabilities. Taken together, our findings indicate that inductive copying is not essential for learning abstractive ICL mechanisms.

</details>


### [12] [Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models](https://arxiv.org/abs/2511.05752)
*Xiangchen Song,Yulin Huang,Jinxu Guo,Yuchen Liu,Yaxuan Luan*

Main category: cs.CL

TL;DR: 本研究提出了一种结合大型语言模型、多尺度特征融合和图神经网络的混合方法，用于提升文本分类的性能，并在多个指标上取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 在复杂语义环境中，传统的文本分类方法可能无法充分捕捉到文本中的深层语义关系和逻辑依赖，因此需要一种更有效的解决方案。

Method: 本研究提出了一种混合方法，结合了大型语言模型的深度特征提取、通过特征金字塔的多尺度融合以及图神经网络的结构化建模，以提高复杂语义环境下的性能。

Result: 所提出的方法在鲁棒性对齐实验中表现出显著优势，在ACC、F1-Score、AUC和Precision方面优于现有模型，验证了该框架的有效性和稳定性。

Conclusion: 本研究不仅构建了一个平衡全局与局部信息以及语义与结构的集成框架，还为文本分类任务中的多尺度特征融合和结构化语义建模提供了新的视角。

Abstract: This study investigates a hybrid method for text classification that integrates deep feature extraction from large language models, multi-scale fusion through feature pyramids, and structured modeling with graph neural networks to enhance performance in complex semantic contexts. First, the large language model captures contextual dependencies and deep semantic representations of the input text, providing a rich feature foundation for subsequent modeling. Then, based on multi-level feature representations, the feature pyramid mechanism effectively integrates semantic features of different scales, balancing global information and local details to construct hierarchical semantic expressions. Furthermore, the fused features are transformed into graph representations, and graph neural networks are employed to capture latent semantic relations and logical dependencies in the text, enabling comprehensive modeling of complex interactions among semantic units. On this basis, the readout and classification modules generate the final category predictions. The proposed method demonstrates significant advantages in robustness alignment experiments, outperforming existing models on ACC, F1-Score, AUC, and Precision, which verifies the effectiveness and stability of the framework. This study not only constructs an integrated framework that balances global and local information as well as semantics and structure, but also provides a new perspective for multi-scale feature fusion and structured semantic modeling in text classification tasks.

</details>


### [13] [Language Generation: Complexity Barriers and Implications for Learning](https://arxiv.org/abs/2511.05759)
*Marcelo Arenas,Pablo Barceló,Luis Cofré,Alexander Kozachinskiy*

Main category: cs.CL

TL;DR: 本文研究了语言生成的理论可能性与实际可行性之间的差距，并指出需要考虑自然语言的结构特性来解释现代语言模型的成功。


<details>
  <summary>Details</summary>
Motivation: 尽管Kleinberg和Mullainathan证明了语言生成在理论上是可能的，但实际可行性仍需进一步研究。

Method: 通过分析正例数量对语言生成的影响，探讨了在简单语言家族（如正则语言和上下文无关语言）中生成句子所需的示例数量。

Result: 即使对于简单的语言家族，生成所需示例数量也可能非常大，甚至超出任何可计算函数的范围。

Conclusion: 本文揭示了理论上的可能性与高效可学习性之间的显著差距，并指出解释现代语言模型的实证成功需要考虑自然语言的结构特性。

Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.

</details>


### [14] [DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning](https://arxiv.org/abs/2511.05784)
*Yaxuan Wang,Chris Yuhao Liu,Quan Liu,Jinglong Pang,Wei Wei,Yujia Bao,Yang Liu*

Main category: cs.CL

TL;DR: DRAGON is a reasoning-based framework for unlearning in LLMs that does not require retain data and shows strong performance in practical scenarios.


<details>
  <summary>Details</summary>
Motivation: Unlearning in LLMs is crucial for protecting private data and removing harmful knowledge. Existing approaches often require training or access to retain data, which is not always available in real-world scenarios.

Method: DRAGON is a reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. It introduces a lightweight detection module to identify forget-worthy prompts without any retain data and routes them through a dedicated CoT guard model.

Result: Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.

Conclusion: DRAGON demonstrates strong unlearning capability, scalability, and applicability in practical scenarios.

Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.

</details>


### [15] [Quantifying Edits Decay in Fine-tuned LLMs](https://arxiv.org/abs/2511.05852)
*Yinjie Cheng,Paul Youssef,Christin Seifert,Jörg Schlötterer,Zhixue Zhao*

Main category: cs.CL

TL;DR: 本文研究了微调对知识编辑的影响，发现编辑在微调后会衰减，并提出了选择性层微调的方法来有效删除编辑。


<details>
  <summary>Details</summary>
Motivation: 知识编辑已成为一种轻量级替代重新训练的方法，用于纠正或注入大型语言模型（LLMs）中的特定事实。同时，微调仍然是适应LLMs到新领域和任务的默认操作。尽管它们被广泛采用，但这两种后期训练干预措施一直被孤立研究，留下了一个关键问题：如果我们微调一个已编辑的模型，这些编辑是否会保留？这个问题是由两个实际场景驱动的：移除隐蔽或恶意的编辑，以及保留有益的编辑。

Method: 我们系统地量化了微调后的编辑衰减，研究了微调如何影响知识编辑。我们评估了两种最先进的编辑方法（MEMIT，AlphaEdit）和三种微调方法（全参数，LoRA，DoRA）在五个LLM和三个数据集上的表现，产生了232种实验配置。

Result: 我们的结果表明，编辑在微调后会衰减，其生存率因配置而异，例如，AlphaEdit编辑的衰减比MEMIT编辑更严重。此外，我们提出了选择性层微调，并发现仅微调已编辑层可以有效地删除编辑，尽管对下游性能有轻微的影响。令人惊讶的是，微调未编辑层比全微调损害更多的编辑。

Conclusion: 我们的研究建立了经验基线和可操作的策略，以将知识编辑与微调集成，并强调了在评估模型编辑时需要考虑整个LLM应用流程。

Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.

</details>


### [16] [Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations](https://arxiv.org/abs/2511.05901)
*Rui Yang,Matthew Yu Heng Wong,Huitao Li,Xin Li,Wentao Zhu,Jingchi Liao,Kunyu Yu,Jonathan Chong Kai Liew,Weihao Xuan,Yingjian Chen,Yuhe Ke,Jasmine Chiat Ling Ong,Douglas Teodoro,Chuan Hong,Daniel Shi Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 本研究回顾了医疗领域中RAG的应用，发现其仍处于早期阶段，需要在临床验证、跨语言适应和低资源环境支持方面取得进展。


<details>
  <summary>Details</summary>
Motivation: 医疗知识的快速增长和临床实践的复杂性增加带来了挑战。大型语言模型（LLMs）展示了价值，但存在固有局限性。检索增强生成（RAG）技术显示出提高其临床适用性的潜力。

Method: 本研究回顾了医疗领域中RAG的应用。

Result: 研究主要依赖于公开数据，而在私有数据中的应用有限。检索方法通常依赖于以英语为中心的嵌入模型，而LLMs主要是通用的，很少使用医疗专用的LLMs。评估方面，自动度量标准评估生成质量和任务性能，而人工评估关注准确性、完整性、相关性和流畅性，但对偏见和安全性的关注不足。RAG应用集中在问答、报告生成、文本摘要和信息提取上。

Conclusion: 医疗RAG仍处于早期阶段，需要在临床验证、跨语言适应和低资源环境支持方面取得进展，以实现可信赖和负责任的全球使用。

Abstract: The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.

</details>


### [17] [NILC: Discovering New Intents with LLM-assisted Clustering](https://arxiv.org/abs/2511.05913)
*Hongtao Wang,Renchi Yang,Wenqing Lin*

Main category: cs.CL

TL;DR: This paper proposes NILC, a novel clustering framework for effective new intent discovery (NID) that leverages large language models to refine cluster centroids and text embeddings, resulting in significant performance improvements across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing works towards NID mainly adopt a cascaded architecture, which fails to leverage feedback from both steps for mutual refinement and overlooks nuanced textual semantics, leading to suboptimal performance.

Method: NILC is a novel clustering framework that follows an iterative workflow, in which clustering assignments are updated by refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs).

Result: Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.

Conclusion: NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.

Abstract: New intent discovery (NID) seeks to recognize both new and known intents from unlabeled user utterances, which finds prevalent use in practical dialogue systems. Existing works towards NID mainly adopt a cascaded architecture, wherein the first stage focuses on encoding the utterances into informative text embeddings beforehand, while the latter is to group similar embeddings into clusters (i.e., intents), typically by K-Means. However, such a cascaded pipeline fails to leverage the feedback from both steps for mutual refinement, and, meanwhile, the embedding-only clustering overlooks nuanced textual semantics, leading to suboptimal performance. To bridge this gap, this paper proposes NILC, a novel clustering framework specially catered for effective NID. Particularly, NILC follows an iterative workflow, in which clustering assignments are judiciously updated by carefully refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs). Specifically, NILC first taps into LLMs to create additional semantic centroids for clusters, thereby enriching the contextual semantics of the Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment hard samples (ambiguous or terse utterances) identified from clusters via rewriting for subsequent cluster correction. Further, we inject supervision signals through non-trivial techniques seeding and soft must links for more accurate NID in the semi-supervised setting. Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.

</details>


### [18] [IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction](https://arxiv.org/abs/2511.05921)
*Ankan Mullick,Sukannya Purkayastha,Saransh Sharma,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 本文提出了一种名为IDALC的半监督框架，用于检测用户意图并纠正系统拒绝的语句，同时最大限度地减少人工标注的需求。实验证明，该系统在多个基准数据集上优于基线方法，准确率提高了5-10%，宏F1得分提高了4-8%。此外，整体标注成本仅占可用未标记数据的6-10%。


<details>
  <summary>Details</summary>
Motivation: 语音控制对话系统因能够响应各种用户查询执行广泛的操作而变得非常流行。然而，这些系统存在一定的局限性。有时，即使对于已知的意图，如果任何模型表现出低置信度，会导致拒绝需要手动标注的语句。此外，随着时间的推移，可能需要重新训练这些代理以获取新的意图来执行额外的任务。对所有这些新兴意图和被拒绝的语句进行标注是不切实际的，因此需要一种高效的机制来减少标注成本。

Method: 本文引入了IDALC（基于意图检测和主动学习的校正），这是一种半监督框架，旨在检测用户意图并纠正系统拒绝的语句，同时最小化人工标注的需求。

Result: 实验证明，IDALC在多个基准数据集上优于基线方法，准确率提高了5-10%，宏F1得分提高了4-8%。此外，整体标注成本仅占可用未标记数据的6-10%。

Conclusion: 本文提出了一种名为IDALC的半监督框架，用于检测用户意图并纠正系统拒绝的语句，同时最大限度地减少人工标注的需求。实验证明，该系统在多个基准数据集上优于基线方法，准确率提高了5-10%，宏F1得分提高了4-8%。此外，整体标注成本仅占可用未标记数据的6-10%。

Abstract: Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. There are instances where, even for known intents, if any model exhibits low confidence, it results in rejection of utterances that necessitate manual annotation. Additionally, as time progresses, there may be a need to retrain these agents with new intents from the system-rejected queries to carry out additional tasks. Labeling all these emerging intents and rejected utterances over time is impractical, thus calling for an efficient mechanism to reduce annotation costs. In this paper, we introduce IDALC (Intent Detection and Active Learning based Correction), a semi-supervised framework designed to detect user intents and rectify system-rejected utterances while minimizing the need for human annotation. Empirical findings on various benchmark datasets demonstrate that our system surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8% improvement in macro-F1. Remarkably, we maintain the overall annotation cost at just 6-10% of the unlabelled data available to the system. The overall framework of IDALC is shown in Fig. 1

</details>


### [19] [Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs](https://arxiv.org/abs/2511.05933)
*Renfei Zhang,Manasa Kaniselvan,Niloofar Mireshghallah*

Main category: cs.CL

TL;DR: RL-enhanced models improve knowledge recall by enhancing procedural skills in navigating hierarchical knowledge, not by acquiring new data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to challenge the narrative that RL degrades memorized knowledge in language models. Instead, the study aims to show that RL enhances knowledge recall by improving procedural skills in navigating knowledge hierarchies.

Method: The study observes the performance of RL-enhanced models on knowledge recall tasks and compares them with base and SFT models. It also uses structured prompting to guide SFT models and analyzes internal activations to understand the differences between model types.

Result: RL-enhanced models consistently outperform base and SFT models on knowledge recall tasks, particularly those involving hierarchical structures. Structured prompting recovers most of the performance gap, and RL models retain superior ability to recall procedural paths on deep-retrieval tasks.

Conclusion: RL-enhanced models outperform their base and SFT counterparts on pure knowledge recall tasks, especially those requiring traversal of hierarchical, structured knowledge. The gains are attributed to improved procedural skills in navigating existing knowledge hierarchies rather than new data.

Abstract: Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.

</details>


### [20] [Interpretable Recognition of Cognitive Distortions in Natural Language Texts](https://arxiv.org/abs/2511.05969)
*Anton Kolonin,Anna Arinicheva*

Main category: cs.CL

TL;DR: 该研究提出了一种新的多因素自然语言文本分类方法，基于加权结构模式（如N-gram），并考虑了它们之间的异层级关系，应用于心理护理中特定认知扭曲的自动化检测。该方法在两个公开数据集上取得了显著的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决社会影响大的问题，即心理护理中特定认知扭曲的自动化检测，并希望通过一种可解释、稳健和透明的人工智能模型来实现这一目标。

Method: 该研究提出了一种基于加权结构模式（如N-gram）的多因素分类方法，并考虑了它们之间的异层级关系。该方法应用于心理护理中特定认知扭曲的自动化检测。

Result: 该研究在两个公开数据集上测试了其方法，并取得了显著的F1分数提升，优于文献中的已知结果。

Conclusion: 该研究提出了一种新的多因素自然语言文本分类方法，基于加权结构模式如N-gram，并考虑它们之间的异层级关系，应用于解决社会影响大的问题，即心理护理中特定认知扭曲的自动化检测。该方法在该领域处于领先地位，并在两个公开数据集上进行了测试，取得了显著的F1分数提升。

Abstract: We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.

</details>


### [21] [Revisiting Entropy in Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2511.05993)
*Renren Jin,Pengzhi Gao,Yuqi Ren,Zhuowen Han,Tongxuan Zhang,Wuwei Huang,Wei Liu,Jian Luan,Deyi Xiong*

Main category: cs.CL

TL;DR: 本文研究了RLVR训练中LLMs的熵动态，并发现模型熵可以通过调整具有正优势的标记的相对损失权重来有效调节。


<details>
  <summary>Details</summary>
Motivation: 尽管已经提出了各种方法来缓解熵崩溃，但对RLVR中的熵进行全面研究仍然缺乏。为了填补这一空白，我们进行了研究。

Method: 我们进行了广泛的实验，以研究RLVR训练中LLMs的熵动态，并分析了模型熵与响应多样性、校准和性能之间的关系。此外，我们从理论上和实证上证明了具有正优势的标记是熵崩溃的主要贡献者。

Result: 我们的结果表明，离策略更新的数量、训练数据的多样性以及优化目标中的剪切阈值是影响RLVR训练中LLMs熵的关键因素。

Conclusion: 我们的研究揭示了在RLVR训练中模型熵的关键影响因素，并展示了通过调整具有正优势的标记的相对损失权重可以有效调节模型熵。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a predominant approach for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, causing premature convergence to suboptimal local minima and hinder further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To address this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our findings reveal that the number of off-policy updates, the diversity of training data, and the clipping thresholds in the optimization objective are critical factors influencing the entropy of LLMs trained with RLVR. Moreover, we theoretically and empirically demonstrate that tokens with positive advantages are the primary contributors to entropy collapse, and that model entropy can be effectively regulated by adjusting the relative loss weights of tokens with positive and negative advantages during training.

</details>


### [22] [LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis](https://arxiv.org/abs/2511.06000)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 研究评估了最先进的语言模型在生成生物医学研究的抽象摘要时保留年龄相关信息的能力。结果表明，当前的语言模型在忠实和无偏总结方面存在局限性，需要公平意识评估框架和总结流程。


<details>
  <summary>Details</summary>
Motivation: 临床干预往往依赖于年龄：对成人安全的药物和程序可能对儿童有害或对老年人无效。然而，随着语言模型越来越多地融入生物医学证据综合工作流，尚不清楚这些系统是否保留这种重要的人口统计学区分。

Method: 构建了一个名为DemogSummary的新颖的年龄分层数据集，涵盖了儿童、成人和老年群体的系统综述原始研究。评估了三种具有摘要能力的LLM，即Qwen（开源）、Longformer（开源）和GPT-4.1 Nano（专有），使用标准指标和新提出的Demographic Salience Score（DSS），该评分量化了年龄相关实体的保留和幻觉。

Result: 研究结果揭示了模型和年龄群体之间的系统性差异：以成人为重点的摘要的代际保真度最低，被低估的人群更容易出现幻觉。

Conclusion: 研究结果揭示了模型和年龄群体之间的系统性差异：以成人为重点的摘要的代际保真度最低，被低估的人群更容易出现幻觉。这些发现突显了当前LLM在忠实和无偏总结方面的局限性，并指出了在生物医学NLP中需要公平意识评估框架和总结流程。

Abstract: Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies. We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination. Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and under-represented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.

</details>


### [23] [Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data](https://arxiv.org/abs/2511.06023)
*Deng Yixuan,Ji Xiaoqiang*

Main category: cs.CL

TL;DR: 本文提出了一种基于多奖励组相对策略优化（GRPO）的框架，用于微调大型语言模型以减少偏见并提高道德行为。通过构建一个合成的英语数据集并使用DeBERTa-v3训练奖励模型，实验结果表明该方法有效降低了偏见强度，并保持了模型的流畅性和信息量。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的对齐技术如RLHF和DPO已经缓解了一些问题，但它们在解决文化特定和多维形式的歧视方面仍然有限。因此，本文旨在提出一种新的框架来解决这些问题。

Method: 本文提出了一种多奖励组相对策略优化（GRPO）框架，用于微调大型语言模型以实现道德和无偏的行为。该方法构建了一个合成的英语数据集，源自中国语境的歧视类别，包括地区、种族和职业偏见。每个实例都与中性和有偏见的回答配对，以训练基于DeBERTa-v3的奖励模型，该模型提供多维奖励信号，捕捉公平性、中立性和语言质量。训练后的奖励模型然后指导GRPO微调，以优化模型输出在这些伦理维度上的表现。

Result: 实验结果表明，偏见强度显著降低，同时在不损害流畅性和信息量的情况下改善了与非歧视标准的一致性。

Conclusion: 本研究展示了基于GRPO的多奖励优化在去偏LLM方面的有效性，并提供了一个可复制的文化情境伦理对齐框架。

Abstract: Large Language Models (LLMs) often exhibit implicit biases and discriminatory tendencies that reflect underlying social stereotypes. While recent alignment techniques such as RLHF and DPO have mitigated some of these issues, they remain limited in addressing culturally specific and multi-dimensional forms of discrimination. This paper proposes a Multi-Reward Group Relative Policy Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free behavior. Our approach constructs a synthetic English-language dataset derived from Chinese-context discrimination categories, including regional, ethnic, and occupational biases. Each instance is paired with both neutral and biased responses to train a reward model based on DeBERTa-v3, which provides multi-dimensional reward signals capturing fairness, neutrality, and linguistic quality. The trained reward model then guides GRPO fine-tuning to optimize model outputs along these ethical dimensions. Experimental results demonstrate significant reductions in bias intensity and improved alignment with non-discriminatory standards without compromising fluency or informativeness. This study highlights the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs and offers a replicable framework for cultural-contextual ethical alignment.

</details>


### [24] [Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts](https://arxiv.org/abs/2511.06048)
*Xinyuan Yan,Shusen Liu,Kowshik Thopalli,Bei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种交互式可视化系统，结合基于拓扑的视觉编码和降维方法，以忠实地表示选定特征之间的局部和全局关系，从而促进对潜在空间中概念表示的更深入和更细致的分析。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAEs）已成为在大型语言模型（LLMs）中发现可解释特征的强大工具，但提取的方向数量庞大，使得全面探索变得不可行。传统嵌入技术如UMAP在揭示全局结构方面存在局限性，包括高维压缩伪影、过绘和误导性的邻域扭曲。

Method: 本文提出了一种交互式可视化系统，结合基于拓扑的视觉编码和降维方法，以忠实地表示选定特征之间的局部和全局关系。

Result: 本文提出了一种专注于探索框架，优先考虑精心挑选的概念及其对应的SAE特征，而不是同时尝试可视化所有可用特征。我们展示了一个交互式可视化系统，结合基于拓扑的视觉编码和降维方法，以忠实地表示选定特征之间的局部和全局关系。这种混合方法使用户能够通过有针对性的、可解释的子集来研究SAE行为，从而促进对潜在空间中概念表示的更深入和更细致的分析。

Conclusion: 本文提出了一种专注于探索框架，优先考虑精心挑选的概念及其对应的SAE特征，而不是同时尝试可视化所有可用特征。我们展示了一个交互式可视化系统，结合基于拓扑的视觉编码和降维方法，以忠实地表示选定特征之间的局部和全局关系。这种混合方法使用户能够通过有针对性的、可解释的子集来研究SAE行为，从而促进对潜在空间中概念表示的更深入和更细致的分析。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.

</details>


### [25] [Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework](https://arxiv.org/abs/2511.06051)
*Mahmoud El-Bahnasawi*

Main category: cs.CL

TL;DR: 本文提出了一种三层次框架，以实现计算效率高且性能优越的仇恨言论检测系统。


<details>
  <summary>Details</summary>
Motivation: 本文旨在开发计算效率高的仇恨言论检测系统，同时保持竞争力并适用于实时部署。

Method: 我们提出了一种三层次框架，结合基于规则的预筛选、参数高效的LoRA微调BERTweet模型和持续学习能力。

Result: 我们的方法达到了0.85宏F1分数，代表了最先进的大型语言模型（如SafePhi）性能的94%，而使用的基础模型小100倍（134M vs 14B参数）。

Conclusion: 我们的方法在资源受限的环境中实现了强大的仇恨言论检测，同时保持了与最先进的大型语言模型相当的准确性。

Abstract: This paper addresses the critical challenge of developing computationally efficient hate speech detection systems that maintain competitive performance while being practical for real-time deployment. We propose a novel three-layer framework that combines rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. Our approach achieves 0.85 macro F1 score - representing 94% of the performance of state-of-the-art large language models like SafePhi (Phi-4 based) while using a base model that is 100x smaller (134M vs 14B parameters). Compared to traditional BERT-based approaches with similar computational requirements, our method demonstrates superior performance through strategic dataset unification and optimized fine-tuning. The system requires only 1.87M trainable parameters (1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4 GPU, making robust hate speech detection accessible in resource-constrained environments while maintaining competitive accuracy for real-world deployment.

</details>


### [26] [ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning](https://arxiv.org/abs/2511.06057)
*Bingbing Wang,Zhengda Jin,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文提出 ReMoD 框架，通过双推理范式重新思考立场表达的模态贡献，显著提升了多模态立场检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作简单融合不同模态的信息，忽略了不同模态在立场表达中的不同贡献，可能导致立场理解噪音进入学习过程。

Method: 受人类认知的双过程理论启发，提出 ReMoD 框架，通过双推理范式重新思考立场表达的模态贡献。ReMoD 结合了经验驱动的直觉推理和审慎的反思推理，以动态加权模态贡献。

Result: 在公开的 MMSD 基准上进行的大量实验表明，ReMoD 显著优于大多数基线模型，并表现出强大的泛化能力。

Conclusion: ReMoD 显著优于大多数基线模型，并表现出强大的泛化能力。

Abstract: Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing work simply fuses information from various modalities to learn stance representations, overlooking the varying contributions of stance expression from different modalities. Therefore, stance misunderstanding noises may be drawn into the stance learning process due to the risk of learning errors by rough modality combination. To address this, we get inspiration from the dual-process theory of human cognition and propose **ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance expression through a **D**ual-reasoning paradigm. ReMoD integrates *experience-driven intuitive reasoning* to capture initial stance cues with *deliberate reflective reasoning* to adjust for modality biases, refine stance judgments, and thereby dynamically weight modality contributions based on their actual expressive power for the target stance. Specifically, the intuitive stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool (SEP) to form an initial stance hypothesis, prioritizing historically impactful modalities. This hypothesis is then refined in the reflective stage via two reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to amplify relevant modalities, while Semantic-CoT refines SEP with deeper contextual insights of stance semantics. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the public MMSD benchmark demonstrate that our ReMoD significantly outperforms most baseline models and exhibits strong generalization capabilities.

</details>


### [27] [Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework](https://arxiv.org/abs/2511.06067)
*Haoyue Yang,Xuanle Zhao,Yujie Liu,Zhuojun Zou,Kailin Lyu,Changchun Zhou,Yao Zhu,Jie Hao*

Main category: cs.CL

TL;DR: ArchCraft is a framework that converts architectural descriptions from academic papers into synthesizable Verilog projects with RTL verification, surpassing existing methods in paper understanding and code completion.


<details>
  <summary>Details</summary>
Motivation: The challenge of reproducing hardware architectures from academic papers due to the lack of publicly available source code and the complexity of HDLs motivates the development of ArchCraft.

Method: ArchCraft uses a structured workflow that employs formal graphs to capture the architectural blueprint and symbols to define the functional specification, translating unstructured academic papers into verifiable, hardware-aware designs. It generates RTL and testbench code decoupled via these symbols for verification and debugging.

Result: ArchCraft was systematically assessed on ArchSynthBench, demonstrating superiority over direct generation methods and the VerilogCoder framework in both paper understanding and code completion. The generated RTL code meets all timing constraints without violations, and its performance metrics are consistent with those reported in the original papers.

Conclusion: ArchCraft demonstrates the ability to convert architectural descriptions from academic papers into synthesizable Verilog projects with RTL verification, and it outperforms existing methods in paper understanding and code completion. The generated RTL code meets timing constraints and performance metrics consistent with original papers.

Abstract: The reproduction of hardware architectures from academic papers remains a significant challenge due to the lack of publicly available source code and the complexity of hardware description languages (HDLs). To this end, we propose \textbf{ArchCraft}, a Framework that converts abstract architectural descriptions from academic papers into synthesizable Verilog projects with register-transfer level (RTL) verification. ArchCraft introduces a structured workflow, which uses formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into verifiable, hardware-aware designs. The framework then generates RTL and testbench (TB) code decoupled via these symbols to facilitate verification and debugging, ultimately reporting the circuit's Power, Area, and Performance (PPA). Moreover, we propose the first benchmark, \textbf{ArchSynthBench}, for synthesizing hardware from architectural descriptions, with a complete set of evaluation indicators, 50 project-level circuits, and around 600 circuit blocks. We systematically assess ArchCraft on ArchSynthBench, where the experiment results demonstrate the superiority of our proposed method, surpassing direct generation methods and the VerilogCoder framework in both paper understanding and code completion. Furthermore, evaluation and physical implementation of the generated executable RTL code show that these implementations meet all timing constraints without violations, and their performance metrics are consistent with those reported in the original papers.

</details>


### [28] [Stemming Hallucination in Language Models Using a Licensing Oracle](https://arxiv.org/abs/2511.06073)
*Simeon Emanuilov,Richard Ackermann*

Main category: cs.CL

TL;DR: 本文提出了一种名为Licensing Oracle的架构解决方案，通过在语言模型的生成过程中嵌入确定性验证步骤，以防止幻觉。实验结果表明，Licensing Oracle在避免错误答案方面表现优异，为未来的AI系统提供了可靠的生成路径。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽然具有出色的自然语言生成能力，但容易产生幻觉，生成与事实不符的信息。现有的统计方法无法完全消除幻觉，因此需要一种新的解决方案。

Method: 本文提出了Licensing Oracle，这是一种通过在模型的生成过程中嵌入确定性验证步骤来防止幻觉的架构解决方案，确保仅生成事实准确的声明。

Result: 实验结果表明，Licensing Oracle实现了完美的回避精度（AP = 1.0）和零错误答案（FAR-NE = 0.0），确保只生成有效声明，并在事实响应中达到了89.1%的准确性。

Conclusion: 本文展示了通过架构创新，如Licensing Oracle，可以为基于结构化知识表示的领域中的幻觉问题提供必要且充分的解决方案，并为未来AI系统中的受约束生成奠定了基础。

Abstract: Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.

</details>


### [29] [MuonAll: Muon Variant for Efficient Finetuning of Large Language Models](https://arxiv.org/abs/2511.06086)
*Saurabh Page,Advait Joshi,S. S. Sonawane*

Main category: cs.CL

TL;DR: MuonAll is introduced to incorporate all parameters inside Muon by transforming into 2D matrices, and it performs at par with AdamW across major benchmarks.


<details>
  <summary>Details</summary>
Motivation: Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon.

Method: MuonAll incorporates all the parameters inside Muon by transforming into 2D matrices.

Result: We conduct extensive finetuning experiments across publicly available language models with model sizes up to half billion parameters.

Conclusion: Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers.

Abstract: Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at https://github.com/Saurabh750/optimizer

</details>


### [30] [Evaluation of retrieval-based QA on QUEST-LOFT](https://arxiv.org/abs/2511.06125)
*Nathan Scales,Nathanael Schärli,Olivier Bousquet*

Main category: cs.CL

TL;DR: 本文分析了RAG在QUEST-LOFT上的性能问题，并展示了通过结构化输出格式优化RAG的效果。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在需要跨文档信息或复杂推理的问题上表现不佳，而长上下文语言模型的方法也存在类似问题。

Method: 对QUEST-LOFT的性能不佳因素进行了深入分析，并通过人类评估发布了更新的数据。

Result: RAG在结合结构化输出格式后，能够显著优于长上下文方法。

Conclusion: RAG可以通过结合结构化输出格式来优化，从而显著优于长上下文方法。

Abstract: Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.

</details>


### [31] [Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models](https://arxiv.org/abs/2511.06146)
*Akshar Tumu,Varad Shinde,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.

</details>


### [32] [BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering](https://arxiv.org/abs/2511.06183)
*Ryuhei Miyazato,Ting-Ruen Wei,Xuyang Wu,Hsin-Tai Wu,Kei Harada*

Main category: cs.CL

TL;DR: BookAsSumQA is proposed to evaluate aspect-based book summarization by generating QA pairs from a narrative knowledge graph. Results show RAG-based methods are more effective for longer texts.


<details>
  <summary>Details</summary>
Motivation: Aspect-based summarization is not well explored for books due to the difficulty of constructing reference summaries for long texts.

Method: Proposed BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization, which generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality.

Result: Experiments using BookAsSumQA showed that LLM-based approaches have higher accuracy on shorter texts, while RAG-based methods become more effective as document length increases.

Conclusion: RAG-based methods are more effective and practical for aspect-based book summarization as document length increases.

Abstract: Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.

</details>


### [33] [Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning](https://arxiv.org/abs/2511.06190)
*Sangmook Lee,Dohyung Kim,Hyukhun Koh,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出STEER，一种无需外部模型的领域无关框架，通过利用较小模型的置信度分数进行细粒度、步骤级别的路由，从而在保持竞争力或提高准确性的同时减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 为了降低推理成本，先前的工作训练了路由器模型或委托机制，将简单查询分配给小型高效模型，而将更难的查询转发给大型昂贵模型。然而，这些训练好的路由器模型在领域变化下缺乏鲁棒性，并且需要昂贵的数据合成技术来获得足够的地面真实路由标签进行训练。

Method: 我们提出了STEER，一种无需使用外部模型的领域无关框架，通过利用较小模型在生成推理步骤前的logits中的置信度分数，实现细粒度、步骤级别的路由。

Result: 在多个领域的各种挑战性基准测试中，STEER在保持竞争力或提高准确性的同时，减少了推理成本（与仅使用大型模型相比，在AIME上准确率提高了20%，FLOPs减少了48%），优于依赖训练好的外部模块的基线方法。

Conclusion: 我们的结果确立了模型内部的置信度作为模型路由的稳健、领域无关的信号，为高效LLM部署提供了一条可扩展的路径。

Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.

</details>


### [34] [Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease](https://arxiv.org/abs/2511.06215)
*Puzhen Su,Yongzhu Miao,Chunxi Guo,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: EK-ICL is a novel framework that enhances ICL by incorporating explicit knowledge, leading to better performance in detecting Alzheimer's Disease from narrative transcripts.


<details>
  <summary>Details</summary>
Motivation: Existing ICL approaches suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, especially in clinical domains like AD detection.

Method: EK-ICL integrates structured explicit knowledge, including confidence scores from small language models, parsing feature scores for demo selection, and label word replacement to resolve semantic misalignment. It also employs a parsing-based retrieval strategy and ensemble prediction.

Result: EK-ICL demonstrates significant performance improvements over state-of-the-art methods across three AD datasets, showing enhanced reasoning stability and task alignment.

Conclusion: EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines in AD detection, highlighting the importance of explicit knowledge in clinical reasoning under low-resource conditions.

Abstract: Detecting Alzheimer's Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.

</details>


### [35] [SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization](https://arxiv.org/abs/2511.06222)
*Yue Huang,Xiangqi Wang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: SPA is a new alignment paradigm that enforces a strict 'trustworthy-before-helpful' ordering, improving helpfulness without compromising safety.


<details>
  <summary>Details</summary>
Motivation: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict.

Method: Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance.

Result: Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities.

Conclusion: SPA provides a scalable and interpretable alignment strategy for critical LLM applications.

Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict "trustworthy-before-helpful" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.

</details>


### [36] [Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records](https://arxiv.org/abs/2511.06230)
*Juntao Li,Haobin Yuan,Ling Luo,Tengxiao Lv,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文介绍了一个用于自动推荐适当出院药物的竞赛，构建了一个高质量的数据集，并展示了基于大型语言模型的系统在药物推荐中的潜力。


<details>
  <summary>Details</summary>
Motivation: 出院药物推荐在确保治疗连续性、防止再入院和改善慢性代谢疾病患者的长期管理中起着关键作用。

Method: 本文介绍了CHIP 2025共享任务竞赛，旨在开发先进的方法来自动推荐适当的出院药物。构建了一个名为CDrugRed的高质量数据集，并使用大型语言模型（LLM）进行药物推荐。

Result: 最高性能团队在最终测试集上取得了最高的整体性能，Jaccard得分为0.5102，F1得分为0.6267，展示了基于大型语言模型的集成系统的潜力。

Conclusion: 这些结果突显了将大型语言模型应用于中国电子健康记录中的药物推荐的潜力和剩余挑战。

Abstract: Discharge medication recommendation plays a critical role in ensuring treatment continuity, preventing readmission, and improving long-term management for patients with chronic metabolic diseases. This paper present an overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop state-of-the-art approaches for automatically recommending appro-priate discharge medications using real-world Chinese EHR data. For this task, we constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified hospitalization records from 3,190 patients in China. This task is challenging due to multi-label nature of medication recommendation, het-erogeneous clinical text, and patient-specific variability in treatment plans. A total of 526 teams registered, with 167 and 95 teams submitting valid results to the Phase A and Phase B leaderboards, respectively. The top-performing team achieved the highest overall performance on the final test set, with a Jaccard score of 0.5102, F1 score of 0.6267, demonstrating the potential of advanced large language model (LLM)-based ensemble systems. These re-sults highlight both the promise and remaining challenges of applying LLMs to medication recommendation in Chinese EHRs. The post-evaluation phase remains open at https://tianchi.aliyun.com/competition/entrance/532411/.

</details>


### [37] [Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy](https://arxiv.org/abs/2511.06234)
*Mojtaba Noghabaei*

Main category: cs.CL

TL;DR: 本研究探讨了ELECTRA-small模型在处理包含否定的例子时的表现，并通过数据增强方法提高了其准确性，同时不影响整体性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在自然语言推理（NLI）中通常通过使用虚假相关性或数据集特征来达到高表现，而不是理解语言细节，如否定。我们研究了在斯坦福自然语言推理（SNLI）数据集上微调的ELECTRA-small模型处理否定的能力。

Method: 我们通过分析发现，该模型在正确分类包含否定的例子时存在困难。为了应对这一问题，我们使用强调否定的对比集和对抗性例子对训练数据进行了增强。

Result: 这种有针对性的数据增强提高了模型在包含否定的例子中的准确性，而不会对整体性能产生不利影响，从而减轻了识别出的数据集特征。

Conclusion: 我们的结果表明，这种有针对性的数据增强提高了模型在包含否定的例子中的准确性，而不会对整体性能产生不利影响，从而减轻了识别出的数据集特征。

Abstract: Pre-trained models for natural language inference (NLI) often achieve high performance on benchmark datasets by using spurious correlations, or dataset artifacts, rather than understanding language touches such as negation. In this project, we investigate the performance of an ELECTRA-small model fine-tuned on the Stanford Natural Language Inference (SNLI) dataset, focusing on its handling of negation. Through analysis, we identify that the model struggles with correctly classifying examples containing negation. To address this, we augment the training data with contrast sets and adversarial examples emphasizing negation. Our results demonstrate that this targeted data augmentation improves the model's accuracy on negation-containing examples without adversely affecting overall performance, therefore mitigating the identified dataset artifact.

</details>


### [38] [TimeSense:Making Large Language Models Proficient in Time-Series Analysis](https://arxiv.org/abs/2511.06344)
*Zhirui Zhang,Changhua Pei,Tianyi Gao,Zhe Xie,Yibo Hao,Zhaoyang Yu,Longlong Xu,Tong Xiao,Jing Han,Dan Pei*

Main category: cs.CL

TL;DR: 本文提出了TimeSense，一种多模态框架，通过平衡文本推理与保留的时间感，使LLMs擅长时间序列分析。实验结果表明，TimeSense在多个任务中达到了最先进的性能，并且在复杂的多维时间序列推理任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于文本标签进行训练监督，这使得模型偏向于文本线索，而可能忽略了完整的时间特征。这种偏差可能导致与底层时间序列上下文矛盾的输出。为了应对这个问题，我们构建了EvalTS基准测试，包括三个难度级别的10个任务，从基本的时间模式识别到复杂的现实世界推理，以在更具挑战性和现实的场景中评估模型。

Method: 我们提出了TimeSense，这是一种多模态框架，通过平衡文本推理与保留的时间感，使LLMs擅长时间序列分析。TimeSense包含一个时间感模块，可以在模型的上下文中重建输入时间序列，确保文本推理基于时间序列动态。此外，为了增强时间序列数据的空间理解，我们显式地结合了基于坐标的定位嵌入，为每个时间点提供空间上下文，使模型能够更有效地捕捉结构依赖性。

Result: TimeSense在多个任务中达到了最先进的性能，并且在复杂的多维时间序列推理任务中优于现有方法。

Conclusion: 实验结果表明，TimeSense在多个任务中达到了最先进的性能，并且在复杂的多维时间序列推理任务中优于现有方法。

Abstract: In the time-series domain, an increasing number of works combine text with temporal data to leverage the reasoning capabilities of large language models (LLMs) for various downstream time-series understanding tasks. This enables a single model to flexibly perform tasks that previously required specialized models for each domain. However, these methods typically rely on text labels for supervision during training, biasing the model toward textual cues while potentially neglecting the full temporal features. Such a bias can lead to outputs that contradict the underlying time-series context. To address this issue, we construct the EvalTS benchmark, comprising 10 tasks across three difficulty levels, from fundamental temporal pattern recognition to complex real-world reasoning, to evaluate models under more challenging and realistic scenarios. We also propose TimeSense, a multimodal framework that makes LLMs proficient in time-series analysis by balancing textual reasoning with a preserved temporal sense. TimeSense incorporates a Temporal Sense module that reconstructs the input time-series within the model's context, ensuring that textual reasoning is grounded in the time-series dynamics. Moreover, to enhance spatial understanding of time-series data, we explicitly incorporate coordinate-based positional embeddings, which provide each time point with spatial context and enable the model to capture structural dependencies more effectively. Experimental results demonstrate that TimeSense achieves state-of-the-art performance across multiple tasks, and it particularly outperforms existing methods on complex multi-dimensional time-series reasoning tasks.

</details>


### [39] [HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection](https://arxiv.org/abs/2511.06391)
*Irina Proskurina,Marc-Antoine Carpentier,Julien Velcin*

Main category: cs.CL

TL;DR: 本文研究了HatePrototypes在仇恨言论检测中的作用，发现它们能够有效处理显性和隐性仇恨，并且可以在不同基准之间迁移。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要解决对受保护群体的明确仇恨，常常忽略了隐性或间接仇恨，例如贬低比较、排斥或暴力呼吁以及微妙的歧视语言，这些仍然会造成伤害。

Method: 我们分析了HatePrototypes的作用，这些是针对仇恨言论检测和安全监管优化的语言模型的类级向量表示。

Result: 我们发现，这些原型能够实现跨任务迁移，并且在不同基准中可互换。此外，我们展示了使用原型的无参数早期退出对于两种类型的仇恨都是有效的。

Conclusion: 我们发现，这些原型（从每类仅50个例子构建）能够在显性和隐性仇恨之间实现跨任务迁移，并且在不同基准中可互换。此外，我们展示了使用原型的无参数早期退出对于两种类型的仇恨都是有效的。我们发布了代码、原型资源和评估脚本，以支持未来关于高效和可迁移仇恨言论检测的研究。

Abstract: Optimization of offensive content moderation models for different types of hateful messages is typically achieved through continued pre-training or fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly address explicit hate toward protected groups and often overlook implicit or indirect hate, such as demeaning comparisons, calls for exclusion or violence, and subtle discriminatory language that still causes harm. While explicit hate can often be captured through surface features, implicit hate requires deeper, full-model semantic processing. In this work, we question the need for repeated fine-tuning and analyze the role of HatePrototypes, class-level vector representations derived from language models optimized for hate speech detection and safety moderation. We find that these prototypes, built from as few as 50 examples per class, enable cross-task transfer between explicit and implicit hate, with interchangeable prototypes across benchmarks. Moreover, we show that parameter-free early exiting with prototypes is effective for both hate types. We release the code, prototype resources, and evaluation scripts to support future research on efficient and transferable hate speech detection.

</details>


### [40] [SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss](https://arxiv.org/abs/2511.06402)
*Lionel Z. Wang,Shihan Ben,Yulu Huang,Simeng Qing*

Main category: cs.CL

TL;DR: 本文提出了一种名为SugarTextNet的新型基于Transformer的框架，用于识别社交媒体上的糖约会相关内容。通过集成预训练的Transformer编码器、基于注意力的线索提取器和上下文短语编码器，并引入Context-Aware Focal Loss来解决类别不平衡问题，该方法在多个指标上表现出色，证明了其在敏感内容检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 糖约会相关内容在主流社交媒体平台上迅速传播，引发了严重的社会和监管问题，包括亲密关系的商业化和交易关系的正常化。检测此类内容极具挑战性，因为微妙的委婉语、模糊的语言线索以及现实数据中的极端类别不平衡普遍存在。

Method: 我们提出了SugarTextNet，这是一个基于Transformer的框架，专门用于识别社交媒体上的糖约会相关内容。SugarTextNet集成了预训练的Transformer编码器、基于注意力的线索提取器和上下文短语编码器，以捕捉用户生成文本中的显著和细微特征。为了处理类别不平衡并增强少数类检测，我们引入了Context-Aware Focal Loss，这是一种结合了焦点损失缩放和上下文加权的定制损失函数。

Result: 我们在一个新整理的手动注释的数据集上评估了SugarTextNet，该数据集包含来自新浪微博的3,067条中文社交媒体帖子，结果表明我们的方法在多个指标上显著优于传统机器学习模型、深度学习基线和大型语言模型。全面的消融研究证实了每个组件的不可或缺性。

Conclusion: 我们的研究强调了针对敏感内容检测的领域特定、上下文感知建模的重要性，并为复杂的真实世界场景中的内容审核提供了一个稳健的解决方案。

Abstract: Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.

</details>


### [41] [How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset](https://arxiv.org/abs/2511.06418)
*Sunil Mohan,Theofanis Karaletsos*

Main category: cs.CL

TL;DR: 本文介绍了一个数据集，用于评估大型语言模型在药物机制事实知识和推理能力方面的表现，并展示了在开放世界设置下模型面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 药物开发/再利用和个性化医学领域对预训练大型语言模型（LLMs）的兴趣日益增加。这些模型需要展示事实知识和对药物机制的深刻理解，以便在新情境中回忆和推理相关知识。

Method: 本文引入了一个数据集，用于评估大型语言模型（LLMs）在已知机制的事实知识以及在新颖情境下推理的能力。

Result: 使用这个数据集，我们展示了o4-mini在OpenAI的4o、o3和o3-mini模型以及最近的小型Qwen3-4B-thinking模型中表现更好，而Qwen3-4B-thinking模型在某些情况下甚至超过了o4-mini。

Conclusion: 本文展示了在开放世界设置下，模型需要回忆相关知识进行推理，这比封闭世界设置更具挑战性。此外，影响推理链内部链接的反事实情况比影响提示中提到的药物链接的反事实情况更难。

Abstract: Two scientific fields showing increasing interest in pre-trained large language models (LLMs) are drug development / repurposing, and personalized medicine. For both, LLMs have to demonstrate factual knowledge as well as a deep understanding of drug mechanisms, so they can recall and reason about relevant knowledge in novel situations. Drug mechanisms of action are described as a series of interactions between biomedical entities, which interlink into one or more chains directed from the drug to the targeted disease. Composing the effects of the interactions in a candidate chain leads to an inference about whether the drug might be useful or not for that disease. We introduce a dataset that evaluates LLMs on both factual knowledge of known mechanisms, and their ability to reason about them under novel situations, presented as counterfactuals that the models are unlikely to have seen during training. Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini models from OpenAI, and the recent small Qwen3-4B-thinking model closely matches o4-mini's performance, even outperforming it in some cases. We demonstrate that the open world setting for reasoning tasks, which requires the model to recall relevant knowledge, is more challenging than the closed world setting where the needed factual knowledge is provided. We also show that counterfactuals affecting internal links in the reasoning chain present a much harder task than those affecting a link from the drug mentioned in the prompt.

</details>


### [42] [Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop](https://arxiv.org/abs/2511.06427)
*Lifeng Han,David Lindevelt,Sander Puts,Erik van Mulligen,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在提取癌症患者隐喻语言方面的表现，并创建了一个名为HealthQuote.NL的语料库，以支持更好的患者护理和个性化护理路径的设计。


<details>
  <summary>Details</summary>
Motivation: 隐喻和隐喻语言（MLs）在临床医生、患者及其家属之间的医疗沟通中起着重要作用。

Method: 我们使用两种数据源（1）癌症患者讲故事的访谈数据和（2）在线论坛数据，包括患者的帖子、评论和向专业人士提出的问题，来提取患者使用的隐喻。我们通过探索不同的提示策略（如思维链推理、少量示例学习和自我提示）来研究当前最先进的大型语言模型（LLMs）在这一任务上的表现。

Result: 我们验证了提取的隐喻，并将结果编译成一个名为HealthQuote.NL的语料库。

Conclusion: 我们相信提取的隐喻可以支持更好的患者护理，例如共同决策、改善患者和临床医生之间的沟通以及提高患者的健康素养。它们还可以指导个性化护理路径的设计。

Abstract: Metaphors and metaphorical language (MLs) play an important role in healthcare communication between clinicians, patients, and patients' family members. In this work, we focus on Dutch language data from cancer patients. We extract metaphors used by patients using two data sources: (1) cancer patient storytelling interview data and (2) online forum data, including patients' posts, comments, and questions to professionals. We investigate how current state-of-the-art large language models (LLMs) perform on this task by exploring different prompting strategies such as chain of thought reasoning, few-shot learning, and self-prompting. With a human-in-the-loop setup, we verify the extracted metaphors and compile the outputs into a corpus named HealthQuote.NL. We believe the extracted metaphors can support better patient care, for example shared decision making, improved communication between patients and clinicians, and enhanced patient health literacy. They can also inform the design of personalized care pathways. We share prompts and related resources at https://github.com/aaronlifenghan/HealthQuote.NL

</details>


### [43] [Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models](https://arxiv.org/abs/2511.06441)
*Mayank Saini,Arit Kumar Bishwas*

Main category: cs.CL

TL;DR: 本文提出了一种统一的模块化框架，通过智能路由查询到合适的专家模型，提高了效率并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在视觉、音频和文档理解中越来越重要，但其高推理成本阻碍了实时、可扩展的部署。而较小的开源模型虽然有成本优势，但在处理复杂或多模态查询时表现不佳。

Method: 我们引入了一个统一的模块化框架，使用学习的路由网络将每个查询路由到最合适的专家模型，以平衡成本和质量。对于视觉任务，我们采用了一个两阶段的开源管道，优化了效率并重新利用高效的经典视觉组件。

Result: 在MMLU和VQA等基准测试中，我们的方法达到了或超过了始终高端的LLM（单一模型服务于所有查询类型）的性能，同时将对昂贵模型的依赖减少了超过67%。

Conclusion: 通过引入一个统一的模块化框架，我们实现了高质量、资源高效的AI规模化部署。

Abstract: As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.

</details>


### [44] [SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention](https://arxiv.org/abs/2511.06446)
*Bohan Yu,Wei Huang,Kang Liu*

Main category: cs.CL

TL;DR: This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). It encodes KBs into key-value pairs and injects them into LLMs' KV cache. SR-KI uses a two-stage training paradigm to locate a dedicated retrieval layer and apply an attention-based loss. This design allows efficient compression of injected knowledge and dynamic knowledge updates. Experiments show that SR-KI can integrate up to 40K KBs into a 7B LLM on a single A100 40GB GPU, achieving strong retrieval performance and up to 99.75% compression.


<details>
  <summary>Details</summary>
Motivation: The paper aims to integrate real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs) efficiently and effectively.

Method: SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries.

Result: SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.

Conclusion: SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.

Abstract: This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.

</details>


### [45] [Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages](https://arxiv.org/abs/2511.06497)
*Quang Phuoc Nguyen,David Anugraha,Felix Gaschi,Jun Bin Cheng,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本研究发现，对齐对于低资源语言特别有效，使用经过仔细选择、语言多样性的子集可以与完整的多语言对齐相匹配，甚至在未见过的低资源语言上表现更好。这表明有效的对齐不需要全面的语言覆盖，并且可以减少数据收集的负担，同时保持高效和稳健。


<details>
  <summary>Details</summary>
Motivation: 研究对齐是否真的能通过使用所有可用语言来提高跨语言转移，或者是否有选择性的子集可以提供更好的效果，并研究其对低资源语言的影响。

Method: 进行了一项广泛的实证研究，以调查对齐是否真的从使用所有可用语言中受益，或者是否有选择性的子集可以提供相当或更好的跨语言转移，并研究对低资源语言的影响。

Result: 对齐对于低资源语言特别有效，使用经过仔细选择、语言多样性的子集可以与完整的多语言对齐相匹配，甚至在未见过的低资源语言上表现更好。

Conclusion: 有效的对齐不需要全面的语言覆盖，并且在由有根据的语言选择指导时，可以减少数据收集的负担，同时保持高效和稳健。

Abstract: Realignment is a promising strategy to improve cross-lingual transfer in multilingual language models. However, empirical results are mixed and often unreliable, particularly for typologically distant or low-resource languages (LRLs) compared to English. Moreover, word realignment tools often rely on high-quality parallel data, which can be scarce or noisy for many LRLs. In this work, we conduct an extensive empirical study to investigate whether realignment truly benefits from using all available languages, or if strategically selected subsets can offer comparable or even improved cross-lingual transfer, and study the impact on LRLs. Our controlled experiments show that realignment can be particularly effective for LRLs and that using carefully selected, linguistically diverse subsets can match full multilingual alignment, and even outperform it for unseen LRLs. This indicates that effective realignment does not require exhaustive language coverage and can reduce data collection overhead, while remaining both efficient and robust when guided by informed language selection.

</details>


### [46] [You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations](https://arxiv.org/abs/2511.06516)
*Amit LeVi,Raz Lapid,Rom Himelstein,Yaniv Nemcovsky,Ravid Shwartz Ziv,Avi Mendelson*

Main category: cs.CL

TL;DR: This paper introduces two task-aware post-training quantization methods, TAQ and TAQO, which outperform existing approaches by leveraging task-salient signals in hidden representations.


<details>
  <summary>Details</summary>
Motivation: Existing post-training quantization methods are task-agnostic and do not consider how task-specific signals are distributed across layers. This work aims to address this limitation by using hidden representations that encode task-salient signals as a guideline for quantization.

Method: The paper proposes two task-aware post-training quantization (PTQ) methods: Task-Aware Quantization (TAQ) and TAQO. TAQ allocates bitwidths using task-conditioned statistics from hidden activations, while TAQO allocates precision based on direct layer sensitivity tests.

Result: TAQ and TAQO outperform the baselines, achieving high task performance while maintaining low average precision. On Phi-4, TAQ achieves 42.33 EM / 50.81 F1, surpassing Activation-aware Weight Quantization (AWQ) by a large margin.

Conclusion: TAQ and TAQO outperform the baselines, with TAQ leading on Phi-4 and TAQO leading on Llama-3.1, Qwen3, and Qwen2.5. These methods achieve high task performance while maintaining low average precision.

Abstract: Large Language Models (LLMs) excel across diverse tasks, yet many applications require only limited capabilities, making large variants inefficient in memory and latency. Existing approaches often combine distillation and quantization, but most post-training quantization (PTQ) methods are task-agnostic, ignoring how task-specific signals are distributed across layers. In this work, we propose to use hidden representations that encode task-salient signals as a guideline for quantization. In order to fully utilize our innovative idea, this paper compares two new task-aware PTQ methods: Task-Aware Quantization (TAQ), which allocates bitwidths using task-conditioned statistics from hidden activations, and TAQO, which allocates precision based on direct layer sensitivity tests. From a small calibration set, these approaches identify task-relevant layers, preserving their precision while aggressively quantizing the rest. This yields stable task sensitivity profiles and efficient task-specialized models. Across models, TAQ and TAQO outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1, Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1, far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while remaining within < 1.0% of the original accuracy at lower average precision.

</details>


### [47] [Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement](https://arxiv.org/abs/2511.06530)
*Xiaonan Luo,Yue Huang,Ping He,Xiangliang Zhang*

Main category: cs.CL

TL;DR: RefineLab is an LLM-driven framework that improves QA datasets by selectively refining them within a token budget, resulting in higher quality datasets for LLM evaluation.


<details>
  <summary>Details</summary>
Motivation: High-quality QA datasets are foundational for reliable LLM evaluation, but existing datasets have issues such as domain coverage gaps, misaligned difficulty distributions, and factual inconsistencies. The use of generative models has made these challenges worse.

Method: RefineLab is an LLM-driven framework that automatically refines raw QA textual data into high-quality datasets under a controllable token-budget constraint. It addresses a constrained optimization problem by improving the quality of QA samples while respecting resource limitations.

Result: Experiments show that RefineLab consistently narrows divergence from expert datasets across coverage, difficulty alignment, factual fidelity, and distractor quality.

Conclusion: RefineLab pioneers a scalable, customizable path to reproducible dataset design, with broad implications for LLM evaluation.

Abstract: High-quality Question-Answer (QA) datasets are foundational for reliable Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit persistent gaps in domain coverage, misaligned difficulty distributions, and factual inconsistencies. The recent surge in generative model-powered datasets has compounded these quality challenges. In this work, we introduce RefineLab, the first LLM-driven framework that automatically refines raw QA textual data into high-quality datasets under a controllable token-budget constraint. RefineLab takes a set of target quality attributes (such as coverage and difficulty balance) as refinement objectives, and performs selective edits within a predefined token budget to ensure practicality and efficiency. In essence, RefineLab addresses a constrained optimization problem: improving the quality of QA samples as much as possible while respecting resource limitations. With a set of available refinement operations (e.g., rephrasing, distractor replacement), RefineLab takes as input the original dataset, a specified set of target quality dimensions, and a token budget, and determines which refinement operations should be applied to each QA sample. This process is guided by an assignment module that selects optimal refinement strategies to maximize overall dataset quality while adhering to the budget constraint. Experiments demonstrate that RefineLab consistently narrows divergence from expert datasets across coverage, difficulty alignment, factual fidelity, and distractor quality. RefineLab pioneers a scalable, customizable path to reproducible dataset design, with broad implications for LLM evaluation.

</details>


### [48] [Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages](https://arxiv.org/abs/2511.06531)
*Oluwadara Kalejaiye,Luel Hagos Beyene,David Ifeoluwa Adelani,Mmekut-Mfon Gabriel Edet,Aniefon Daniel Akpan,Eno-Abasi Urua,Anietie Andy*

Main category: cs.CL

TL;DR: 本文介绍了ibom数据集，用于机器翻译和主题分类，旨在解决非洲尼日利亚沿海地区语言缺乏NLP研究的问题。


<details>
  <summary>Details</summary>
Motivation: 由于这些语言缺乏文本数据，自然语言处理研究主要集中在少数几种语言上。因此，需要创建一个针对这些语言的数据集以促进研究。

Method: 引入ibom数据集，用于机器翻译和主题分类，并扩展了Flores-200基准测试以涵盖这些语言。

Result: 当前大型语言模型在这些语言的机器翻译任务中表现不佳，但在少量样本的情况下，主题分类性能有所提升。

Conclusion: 当前大型语言模型在这些语言的机器翻译任务中表现不佳，但在少量样本的情况下，主题分类性能有所提升。

Abstract: Nigeria is the most populous country in Africa with a population of more than 200 million people. More than 500 languages are spoken in Nigeria and it is one of the most linguistically diverse countries in the world. Despite this, natural language processing (NLP) research has mostly focused on the following four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the languages spoken in Nigeria). This is in part due to the unavailability of textual data in these languages to train and apply NLP algorithms. In this work, we introduce ibom -- a dataset for machine translation and topic classification in four Coastal Nigerian languages from the Akwa Ibom State region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus on extending Flores-200 benchmark to these languages, and further align the translated texts with topic labels based on SIB-200 classification dataset. Our evaluation shows that current LLMs perform poorly on machine translation for these languages in both zero-and-few shot settings. However, we find the few-shot samples to steadily improve topic classification with more shots.

</details>


### [49] [Rep2Text: Decoding Full Text from a Single LLM Token Representation](https://arxiv.org/abs/2511.06571)
*Haiyan Zhao,Zirui He,Fan Yang,Ali Payani,Mengnan Du*

Main category: cs.CL

TL;DR: 本文提出了一种从大型语言模型的最后一个标记表示中恢复原始输入文本的新框架Rep2Text，并展示了其在不同模型和医学数据上的有效性。


<details>
  <summary>Details</summary>
Motivation: 我们试图回答一个基本问题：在大型语言模型中，从单个最后一个标记表示中能恢复原始输入文本到什么程度？

Method: 我们提出了Rep2Text，这是一个新颖的框架，用于从最后一个标记表示中解码完整的文本。Rep2Text使用一个可训练的适配器，将目标模型的内部表示投影到解码语言模型的嵌入空间中，然后自回归地重建输入文本。

Result: 实验表明，在各种模型组合中，平均超过一半的16个标记序列的信息可以从这种压缩表示中恢复，同时保持强大的语义完整性和连贯性。此外，我们的分析揭示了一个信息瓶颈效应：较长的序列在标记级恢复上表现较差，但保持了强大的语义完整性。

Conclusion: 我们的框架在恢复输入文本方面表现出色，并且在分布外的医学数据上也显示出强大的泛化能力。

Abstract: Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.

</details>


### [50] [TabRAG: Tabular Document Retrieval via Structured Language Representations](https://arxiv.org/abs/2511.06582)
*Jacob Si,Mike Qu,Michelle Lee,Yingzhen Li*

Main category: cs.CL

TL;DR: 本文介绍了 TabRAG，一种用于处理表格密集型文档的解析式 RAG 管道，其在生成和检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于解析的方法在提取表格数据时性能不佳，因此需要一种更有效的解决方案。

Method: TabRAG 是一种基于解析的 RAG 流程，旨在通过结构化语言表示来处理表格密集型文档。

Result: TabRAG 在生成和检索任务中优于现有的流行解析方法。

Conclusion: TabRAG 提供了一种有效的解析方法，用于处理表格密集型文档，并在生成和检索任务中表现出色。

Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.

</details>


### [51] [MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making](https://arxiv.org/abs/2511.06592)
*Zhi Rui Tam,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 研究发现，音频大型语言模型可能根据患者的语音特征而非医学证据做出临床决策，这可能导致医疗差距。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从基于文本的界面转向临床环境中的音频交互，它们可能会通过音频中的副语言线索引入新的漏洞。

Method: 我们评估了170个临床案例，每个案例都从36种不同的语音配置文件中合成语音，涵盖了年龄、性别和情绪的变化。

Result: 我们的研究结果揭示了严重的模态偏差：与相同的文本输入相比，音频输入的手术建议差异高达35%，其中一个模型提供的建议减少了80%。进一步分析发现，年轻和年长声音之间存在高达12%的年龄差异，这种差异在大多数模型中持续存在，尽管使用了链式思维提示。虽然显式推理成功消除了性别偏见，但由于识别性能较差，情绪的影响未被检测到。

Conclusion: 我们的结论是，在临床部署这些模型之前，迫切需要具备偏见意识的架构。

Abstract: As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.

</details>


### [52] [Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes](https://arxiv.org/abs/2511.06601)
*Zi-Niu Wu*

Main category: cs.CL

TL;DR: 本文提出基于二元性的修辞模式操作和金字塔多层映射框架，以增强表达多样性并降低认知复杂性，为AI系统提供分层修辞推理结构的路径。


<details>
  <summary>Details</summary>
Motivation: 建立不同领域之间的概念桥梁，使每个领域都能从其他领域受益。

Method: 提出了基于二元性的模式操作（split-unite, forward-backward, expansion-reduction 和 orthogonal dualities），并引入了组合和泛化等生成模式，同时提出了一个金字塔多层映射框架来减少认知复杂性。

Result: 通过二项式组合和香农熵分析量化了表达多样性程度和复杂性降低程度，识别出边际修辞位（MRB），并展示了分层选择比平面选择显著降低了选择不确定性。

Conclusion: 该研究将静态且不可测量的修辞分类转化为更动态和可测量的系统，为未来AI系统在语言标记上操作分层修辞推理结构提供了路径。

Abstract: Rhetorical modes are useful in both academic and non-academic writing, and can be subjects to be studied within linguistic research and computational modeling. Establishing a conceptual bridge among these domains could enable each to benefit from the others. This paper proposes duality-based mode operations (split-unite, forward-backward, expansion-reduction and orthogonal dualities) to expand the set of rhetorical modes, introducing generated modes like combination and generalization, thereby enhancing epistemic diversity across multiple applications. It further presents a pyramid multilayer mapping framework (e.g., three layers from the rhetorical model layer, to cognitive layer, and to epistemic layers) that reduces the resulting cognitive complexity. The degrees of expressive diversity and complexity reduction are quantified through binomial combinatorics and Shannon entropy analysis. A Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a rhetorical-scalable parameter that measures expressive growth speed in bits per stage. A direct entropy measure shows that hierarchical selection over smaller subsets markedly reduces choice uncertainty compared with flat selection across all modes. These considerations appear to transform static and non-measurable rhetorical taxonomies into more dynamic and more measurable systems for discourse design. From this work, it would be possible to identify a pathway for future AI systems to operate not only on language tokens but on layered rhetorical reasoning structures, bridging linguistic, pedagogical, academic, and computational research

</details>


### [53] [How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models](https://arxiv.org/abs/2511.06676)
*Subhojit Ghimire*

Main category: cs.CL

TL;DR: 本文通过分析AI模型在不同语言群体中的偏差问题，揭示了AI审核可能带来的不公平影响，并提供了一个交互式工具来增强公众对AI的理解和批判能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的审核在日常生活中变得普遍，人们对AI可能存在偏见的担忧日益增加。本文旨在探讨如何确保AI审核的公正性，并揭示其潜在的不公平影响。

Method: 本文采用定量基准测试和交互式教学工具的方法，分析了AI模型在非裔美国英语和标准美国英语之间的偏差问题。

Result: 基准测试结果显示，AI模型对非裔美国英语文本的毒性评分是标准美国英语的1.8倍，对身份仇恨的评分是8.8倍。此外，作者还开发了一个交互式教学工具，用以展示AI偏见的实际影响。

Conclusion: 本文提供了统计证据，证明了AI模型在不同语言群体中的不公平影响，并设计了一个公开的工具来促进对AI的批判性理解。

Abstract: Now that AI-driven moderation has become pervasive in everyday life, we often hear claims that "the AI is biased". While this is often said jokingly, the light-hearted remark reflects a deeper concern. How can we be certain that an online post flagged as "inappropriate" was not simply the victim of a biased algorithm? This paper investigates this problem using a dual approach. First, I conduct a quantitative benchmark of a widely used toxicity model (unitary/toxic-bert) to measure performance disparity between text in African-American English (AAE) and Standard American English (SAE). The benchmark reveals a clear, systematic bias: on average, the model scores AAE text as 1.8 times more toxic and 8.8 times higher for "identity hate". Second, I introduce an interactive pedagogical tool that makes these abstract biases tangible. The tool's core mechanic, a user-controlled "sensitivity threshold," demonstrates that the biased score itself is not the only harm; instead, the more-concerning harm is the human-set, seemingly neutral policy that ultimately operationalises discrimination. This work provides both statistical evidence of disparate impact and a public-facing tool designed to foster critical AI literacy.

</details>


### [54] [Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation](https://arxiv.org/abs/2511.06680)
*Keunhyeung Park,Seunguk Yu,Youngbin Kim*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Standard-to-dialect machine translation remains challenging due to a persistent dialect gap in large language models and evaluation distortions inherent in n-gram metrics, which favor source copying over authentic dialect translation. In this paper, we propose the dialect refinement (DIA-REFINE) framework, which guides LLMs toward faithful target dialect outputs through an iterative loop of translation, verification, and feedback using external dialect classifiers. To address the limitations of n-gram-based metrics, we introduce the dialect fidelity score (DFS) to quantify linguistic shift and the target dialect ratio (TDR) to measure the success of dialect translation. Experiments on Korean dialects across zero-shot and in-context learning baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity. The proposed metrics distinguish between False Success cases, where high n-gram scores obscure failures in dialectal translation, and True Attempt cases, where genuine attempts at dialectal translation yield low n-gram scores. We also observed that models exhibit varying degrees of responsiveness to the framework, and that integrating in-context examples further improves the translation of dialectal expressions. Our work establishes a robust framework for goal-directed, inclusive dialect translation, providing both rigorous evaluation and critical insights into model performance.

</details>


### [55] [Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention](https://arxiv.org/abs/2511.06682)
*Shibing Mo,Haoyang Ruan,Kai Wu,Jing Liu*

Main category: cs.CL

TL;DR: This paper introduces TSAN, a test-time preference optimization method that leverages multiple candidate responses without parameter updates, achieving superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing test-time methods that critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates.

Method: TSAN is a new paradigm for test-time preference optimization that requires no parameter updates. It emulates self-attention entirely in natural language to analyze, weigh, and synthesize the strengths of multiple candidates.

Result: Empirical evaluations show that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method.

Conclusion: TSAN demonstrates that it can outperform supervised models and current state-of-the-art test-time alignment methods by effectively leveraging multiple candidate solutions.

Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.

</details>


### [56] [Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content](https://arxiv.org/abs/2511.06708)
*Adi Danish Bin Muhammad Amin,Mohaiminul Islam Bhuiyan,Nur Shazwani Kamarudin,Zulfahmi Toh,Nur Syafiqah Nafis*

Main category: cs.CL

TL;DR: 本研究通过分析YouTube上的游戏评论，探讨了情感分析在游戏行业中的应用，发现SVM在分类准确性方面表现最佳，并强调了情感分析对游戏开发的重要意义。


<details>
  <summary>Details</summary>
Motivation: 游戏行业的快速发展需要更深入地理解用户情感，特别是在YouTube等流行社交媒体平台上。

Method: 本研究利用YouTube API收集与各种视频游戏相关的评论，并使用TextBlob情感分析工具进行分析。预处理后的数据通过机器学习算法（包括朴素贝叶斯、逻辑回归和支持向量机（SVM））进行分类。

Result: SVM在不同数据集上表现出最佳的分类准确性。分析涵盖了多款受欢迎的游戏视频，揭示了用户偏好和批评的趋势和见解。

Conclusion: 研究强调了高级情感分析在捕捉用户评论中细微情绪的重要性，为游戏开发者提供了有价值的反馈以改进游戏设计和用户体验。未来的研究将专注于整合更复杂的自然语言处理技术并探索更多的数据源以进一步优化游戏领域的感情分析。

Abstract: The rapid evolution of the gaming industry, driven by technological advancements and a burgeoning community, necessitates a deeper understanding of user sentiments, especially as expressed on popular social media platforms like YouTube. This study presents a sentiment analysis on video games based on YouTube comments, aiming to understand user sentiments within the gaming community. Utilizing YouTube API, comments related to various video games were collected and analyzed using the TextBlob sentiment analysis tool. The pre-processed data underwent classification using machine learning algorithms, including Naïve Bayes, Logistic Regression, and Support Vector Machine (SVM). Among these, SVM demonstrated superior performance, achieving the highest classification accuracy across different datasets. The analysis spanned multiple popular gaming videos, revealing trends and insights into user preferences and critiques. The findings underscore the importance of advanced sentiment analysis in capturing the nuanced emotions expressed in user comments, providing valuable feedback for game developers to enhance game design and user experience. Future research will focus on integrating more sophisticated natural language processing techniques and exploring additional data sources to further refine sentiment analysis in the gaming domain.

</details>


### [57] [Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights](https://arxiv.org/abs/2511.06738)
*Hyunjae Kim,Jiwoong Sohn,Aidan Gilson,Nicholas Cochran-Caggiano,Serina Applebaum,Heeju Jin,Seihee Park,Yujin Park,Jiyeong Park,Seoyoung Choi,Brittany Alexandra Herrera Contreras,Thomas Huang,Jaehoon Yun,Ethan F. Wei,Roy Jiang,Leah Colucci,Eric Lai,Amisha Dave,Tuo Guo,Maxwell B. Singer,Yonghoe Koo,Ron A. Adelman,James Zou,Andrew Taylor,Arman Cohan,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 本研究评估了RAG在医学中的表现，发现其存在关键问题，但通过简单策略可以改善性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG被广泛采用以解决医学知识快速变化和提供可验证推理的问题，但其可靠性仍不明确。因此，需要对RAG在医学中的表现进行系统评估。

Method: 本研究对RAG进行了全面的专家评估，分解了RAG管道的三个组成部分：证据检索、证据选择和响应生成，并通过实验分析了这些部分的表现。

Result: 标准RAG通常会降低性能，仅22%的前16段是相关的，证据选择较弱，事实性和完整性下降。然而，简单的策略如证据过滤和查询重写可以显著缓解这些问题。

Conclusion: 研究结果表明，RAG在医学领域中存在关键的失败点，需要重新审视其作用，并强调了阶段感知评估和有意系统设计的重要性。

Abstract: Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.

</details>


### [58] [Sensitivity of Small Language Models to Fine-tuning Data Contamination](https://arxiv.org/abs/2511.06763)
*Nicy Scaria,Silvester John Joseph Kennedy,Deepak Subramani*

Main category: cs.CL

TL;DR: 本文研究了小型语言模型（SLMs）在指令调优过程中对数据污染的鲁棒性。结果显示，语法变换导致性能严重下降，而语义变换则表现出不同的阈值行为。此外，发现更大的模型更容易受到语义错误的影响，这表明需要污染意识的训练协议。


<details>
  <summary>Details</summary>
Motivation: Small Language Models (SLMs) 在资源受限环境中被越来越多地部署，但它们在指令调优期间对数据污染的行为鲁棒性仍然知之甚少。

Method: 我们系统地研究了23个SLMs（270M到4B参数）在多个模型家族中的污染敏感性，通过测量在指令调优期间对语法和语义变换类型的易感性：语法变换（字符和单词反转）和语义变换（无关和反事实响应），每种应用在25%、50%、75%和100%的污染水平上。

Result: 我们的结果揭示了脆弱性模式的根本不对称性：语法变换导致灾难性的性能下降，字符反转在所有模型中都产生了接近完全的失败，而语义变换表现出不同的阈值行为，并在核心语言能力上表现出更大的弹性。关键的是，我们发现了一个“能力诅咒”，其中更大、更强大的模型更容易受到学习语义错误的影响，更容易遵循有害的指令，而我们的基础与指令调优变体分析显示，对齐提供了不一致的鲁棒性好处，有时甚至会降低弹性。

Conclusion: 我们的工作建立了三个核心贡献：(1) SLMs 对语法模式污染的不成比例的易感性的实证证据，(2) 语法和语义变换之间不对称敏感性模式的识别，以及 (3) 污染鲁棒性评估的系统评估协议。这些发现具有直接的部署意义，表明当前的鲁棒性假设可能不适用于较小的模型，并突显了需要污染意识的训练协议。

Abstract: Small Language Models (SLMs) are increasingly being deployed in resource-constrained environments, yet their behavioral robustness to data contamination during instruction tuning remains poorly understood. We systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B parameters) across multiple model families by measuring susceptibility to syntactic and semantic transformation types during instruction tuning: syntactic transformations (character and word reversal) and semantic transformations (irrelevant and counterfactual responses), each applied at contamination levels of 25\%, 50\%, 75\%, and 100\%. Our results reveal fundamental asymmetries in vulnerability patterns: syntactic transformations cause catastrophic performance degradation, with character reversal producing near-complete failure across all models regardless of size or family, while semantic transformations demonstrate distinct threshold behaviors and greater resilience in core linguistic capabilities. Critically, we discover a ``\textit{capability curse}" where larger, more capable models become more susceptible to learning semantic corruptions, effectively following harmful instructions more readily, while our analysis of base versus instruction-tuned variants reveals that alignment provides inconsistent robustness benefits, sometimes even reducing resilience. Our work establishes three core contributions: (1) empirical evidence of SLMs' disproportionate vulnerability to syntactic pattern contamination, (2) identification of asymmetric sensitivity patterns between syntactic and semantic transformations, and (3) systematic evaluation protocols for contamination robustness assessment. These findings have immediate deployment implications, suggesting that current robustness assumptions may not hold for smaller models and highlighting the need for contamination-aware training protocols.

</details>


### [59] [SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces](https://arxiv.org/abs/2511.06778)
*Ruiheng Liu,XiaoBing Chen,Jinyu Zhang,Qiongwen Zhang,Yu Zhang,Bailong Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为SafeNlidb的新框架，用于解决LLM-based NLIDB中的隐私和安全问题。该框架通过自动化管道生成混合思维链交互数据，并引入推理预热和交替偏好优化来提高安全性。实验结果表明，该方法在保持高实用性的同时，实现了显著的安全性提升。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在处理基于推理的攻击时仍然存在困难，且误报率较高，并且常常会损害SQL查询的可靠性。因此，我们需要一种新的方法来解决这些问题。

Method: 我们提出了一个名为SafeNlidb的隐私-安全对齐框架，该框架包含一个自动化管道，可以从头开始生成混合思维链交互数据，并将隐式安全推理与SQL生成无缝结合。此外，我们引入了推理预热和交替偏好优化来克服直接偏好优化（DPO）的多偏好振荡问题，使LLM能够通过细粒度推理生成安全意识的SQL，而无需人工标注的偏好数据。

Result: 广泛的实验表明，我们的方法优于更大规模的LLM和理想设置的基线，实现了显著的安全性提升。

Conclusion: 我们的方法在保持高实用性的同时，实现了显著的安全性提升。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven significant progress in Natural Language Interface to Database (NLIDB). However, the widespread adoption of LLMs has raised critical privacy and security concerns. During interactions, LLMs may unintentionally expose confidential database contents or be manipulated by attackers to exfiltrate data through seemingly benign queries. While current efforts typically rely on rule-based heuristics or LLM agents to mitigate this leakage risk, these methods still struggle with complex inference-based attacks, suffer from high false positive rates, and often compromise the reliability of SQL queries. To address these challenges, we propose \textsc{SafeNlidb}, a novel privacy-security alignment framework for LLM-based NLIDB. The framework features an automated pipeline that generates hybrid chain-of-thought interaction data from scratch, seamlessly combining implicit security reasoning with SQL generation. Additionally, we introduce reasoning warm-up and alternating preference optimization to overcome the multi-preference oscillations of Direct Preference Optimization (DPO), enabling LLMs to produce security-aware SQL through fine-grained reasoning without the need for human-annotated preference data. Extensive experiments demonstrate that our method outperforms both larger-scale LLMs and ideal-setting baselines, achieving significant security improvements while preserving high utility.WARNING: This work may contain content that is offensive and harmful!

</details>


### [60] [Learning to Focus: Focal Attention for Selective and Scalable Transformers](https://arxiv.org/abs/2511.06818)
*Dhananjay Ram,Wei Xia,Stefano Soatto*

Main category: cs.CL

TL;DR: Focal Attention is a simple modification to the transformer architecture that sharpens attention distributions, leading to improved performance on long-context tasks and reduced resource requirements.


<details>
  <summary>Details</summary>
Motivation: The standard softmax attention in transformer architectures often produces noisy probability distributions, which can impair effective feature selection, especially for long contexts.

Method: Focal Attention is a modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training.

Result: Focal Attention scales more favorably than standard transformers with respect to model size, training data, and context length. It achieves the same accuracy with up to 42% fewer parameters or 33% less training data, and delivers substantial relative improvements on long-context tasks.

Conclusion: Focal Attention demonstrates effectiveness in real-world applications, particularly for long-context tasks, and can achieve the same accuracy with fewer parameters or training data.

Abstract: Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.

</details>


### [61] [Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection](https://arxiv.org/abs/2511.06826)
*Puzhen Su,Haoran Yin,Yongzhu Miao,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: 本文提出DA4ICL框架，通过扩展上下文宽度和深化演示信号，显著提升了AD检测性能。


<details>
  <summary>Details</summary>
Motivation: 改进基于上下文学习(ICL)的AD检测需要通过更好的演示集来丰富感知，而现有的任务向量(TV)方法在AD检测中不适用，因为注入粒度、强度和位置不匹配。

Method: DA4ICL是一种以演示为中心的锚定框架，通过Diverse and Contrastive Retrieval (DCR)扩展上下文宽度，并通过Projected Vector Anchoring (PVA)在每个Transformer层深化每个演示的信号。

Result: DA4ICL在三个AD基准测试中相对于ICL和TV基线取得了显著且稳定的提升。

Conclusion: DA4ICL在三个AD基准测试中相对于ICL和TV基线取得了显著且稳定的提升，为细粒度、OOD和低资源LLM适应开辟了新范式。

Abstract: Detecting Alzheimer's disease (AD) from narrative transcripts challenges large language models (LLMs): pre-training rarely covers this out-of-distribution task, and all transcript demos describe the same scene, producing highly homogeneous contexts. These factors cripple both the model's built-in task knowledge (\textbf{task cognition}) and its ability to surface subtle, class-discriminative cues (\textbf{contextual perception}). Because cognition is fixed after pre-training, improving in-context learning (ICL) for AD detection hinges on enriching perception through better demonstration (demo) sets. We demonstrate that standard ICL quickly saturates, its demos lack diversity (context width) and fail to convey fine-grained signals (context depth), and that recent task vector (TV) approaches improve broad task adaptation by injecting TV into the LLMs' hidden states (HSs), they are ill-suited for AD detection due to the mismatch of injection granularity, strength and position. To address these bottlenecks, we introduce \textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and deepens each demo's signal via \emph{\textbf{Projected Vector Anchoring}} (PVA) at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large, stable gains over both ICL and TV baselines, charting a new paradigm for fine-grained, OOD and low-resource LLM adaptation.

</details>


### [62] [CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition](https://arxiv.org/abs/2511.06860)
*Hung-Yang Sung,Chien-Chun Wang,Kuan-Tang Huang,Tien-Hong Lo,Yu-Sheng Tsao,Yung-Chang Hsu,Berlin Chen*

Main category: cs.CL

TL;DR: CLiFT-ASR is a framework that improves ASR for low-resource languages by combining phonetic and Han-character annotations through a two-stage process, achieving significant improvements in accuracy.


<details>
  <summary>Details</summary>
Motivation: The scarcity of annotated data for low-resource languages like Taiwanese Hokkien makes automatic speech recognition (ASR) challenging. Prior studies have rarely explored staged strategies that integrate both annotation types.

Method: CLiFT-ASR is a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien using a two-stage process: first learning acoustic and tonal representations from phonetic Tai-lo annotations, then capturing vocabulary and syntax from Han-character transcriptions.

Result: Experiments on the TAT-MOE corpus show that CLiFT-ASR achieves a 24.88% relative reduction in character error rate (CER) compared with strong baselines.

Conclusion: CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and has potential to benefit other low-resource language scenarios.

Abstract: Automatic speech recognition (ASR) for low-resource languages such as Taiwanese Hokkien is difficult due to the scarcity of annotated data. However, direct fine-tuning on Han-character transcriptions often fails to capture detailed phonetic and tonal cues, while training only on romanization lacks lexical and syntactic coverage. In addition, prior studies have rarely explored staged strategies that integrate both annotation types. To address this gap, we present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The framework employs a two-stage process in which it first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions. This progressive adaptation enables effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88\% relative reduction in character error rate (CER) compared with strong baselines. The results indicate that CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and that it has potential to benefit other low-resource language scenarios.

</details>


### [63] [Inclusion of Role into Named Entity Recognition and Ranking](https://arxiv.org/abs/2511.06886)
*Neelesh Kumar Shukla,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 本文研究了如何通过将实体角色检测问题建模为命名实体识别和实体检索/排序任务来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 当实体在特定上下文中根据其行为或属性扮演角色时，需要一种方法来分配这些角色。

Method: 本文将实体角色检测问题建模为NER和实体检索/排序任务，并探索了不同的上下文，如句子和文档。

Result: 本文提出了自动化学习代表词和短语的方法，并利用它们构建角色和实体的表示。

Conclusion: 本文研究了如何通过将实体角色检测问题建模为命名实体识别（NER）和实体检索/排序任务来解决该问题。

Abstract: Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.

</details>


### [64] [EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers](https://arxiv.org/abs/2511.06890)
*Yilin Jiang,Mingzi Zhang,Xuanyu Yin,Sheng Jin,Suyu Lu,Zuocan Ying,Zengyi Yu,Xiangjie Kong*

Main category: cs.CL

TL;DR: EduGuardBench is a dual-component benchmark designed to assess the professional fidelity and safety of Large Language Models used in educational scenarios. It identifies the risks of academic misconduct and reveals that the safest models are those that can transform harmful requests into teachable moments.


<details>
  <summary>Details</summary>
Motivation: Ensuring the professional competence and ethical safety of Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios.

Method: We propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment.

Result: Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety.

Conclusion: EduGuardBench provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education.

Abstract: Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, are pivotal for personalized education. However, ensuring their professional competence and ethical safety is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios. To address this, we propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety. EduGuardBench thus provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education. See https://github.com/YL1N/EduGuardBench for Materials.

</details>


### [65] [RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation](https://arxiv.org/abs/2511.06899)
*Haofeng Wang,Yu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态推理评估方法RPTS，并构建了一个新的基准测试RPTS-Eval，以更全面地评估模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基准测试主要通过选择题或简答题格式评估模型，这忽略了推理过程。此外，这些基准测试没有考虑模态间关系对推理的影响。

Method: 我们提出了基于树结构的度量标准RPTS，将推理步骤组织成推理树，并利用其层次信息为每个推理步骤分配加权忠实度分数。此外，我们构建了一个新的基准测试RPTS-Eval，包含374张图像和390个推理实例。

Result: 我们评估了代表性的LVLMs（如GPT4o、Llava-Next），揭示了它们在多模态推理中的局限性，并突出了开源和闭源商业LVLMs之间的差异。

Conclusion: 我们相信这个基准测试将有助于推动多模态推理领域研究的进步。

Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.

</details>


### [66] [HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection](https://arxiv.org/abs/2511.06942)
*Fangqi Dai,Xingjian Jiang,Zizhuang Deng*

Main category: cs.CL

TL;DR: HLPD是一种用于检测机器修订文本和先进LLM生成文本的新方法，相比现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 为了防止由LLM生成的可信内容引发的虚假信息和社会问题，需要开发高效可靠的文本来源识别方法。现有方法在面对更先进的LLM输出或带有对抗性多任务机器修改的文本时表现不佳。

Method: HLPD基于人类语言偏好检测，采用基于奖励的对齐过程Human Language Preference Optimization (HLPO)，使模型更敏感于人类写作，从而提高机器修订文本的识别能力。

Result: HLPD在对抗性多任务评估框架中测试，当检测由GPT系列模型修订的文本时，相对于ImBD有15.11%的相对改进，超过Fast-DetectGPT 45.56%。在检测由先进LLM生成的文本时，HLPD达到最高的平均AUROC，超过ImBD 5.53%，超过Fast-DetectGPT 34.14%。

Conclusion: HLPD在检测由GPT系列模型修订的文本和由先进LLM生成的文本方面表现出色，超过了现有的方法。

Abstract: To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.

</details>


### [67] [SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs](https://arxiv.org/abs/2511.07001)
*Zhenliang Zhang,Xinyu Hu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理时方法SCOPE，用于缓解大型语言模型中的版权侵权问题，无需参数更新或辅助过滤器，能够在不降低通用性能的情况下有效防止侵权。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时防御方法主要关注表面级别的令牌匹配，依赖外部黑名单或过滤器，这增加了部署复杂性，并可能忽略语义上的改写泄露。

Method: 将版权侵权缓解重新定义为内在语义空间控制，引入SCOPE方法，该方法不需要参数更新或辅助过滤器。通过稀疏自编码器（SAE）将隐藏状态投影到高维、近单义空间，并在解码过程中钳制版权敏感子空间的激活。

Result: 在广泛认可的基准测试中，SCOPE能够缓解版权侵权问题，而不会降低通用性能。进一步的可解释性分析确认了隔离子空间捕获了高层次语义。

Conclusion: SCOPE能够有效缓解版权侵权问题，同时不影响通用性能。

Abstract: Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. In this work, we reframe copyright infringement mitigation as intrinsic semantic-space control and introduce SCOPE, an inference-time method that requires no parameter updates or auxiliary filters. Specifically, the sparse autoencoder (SAE) projects hidden states into a high-dimensional, near-monosemantic space; benefiting from this representation, we identify a copyright-sensitive subspace and clamp its activations during decoding. Experiments on widely recognized benchmarks show that SCOPE mitigates copyright infringement without degrading general utility. Further interpretability analyses confirm that the isolated subspace captures high-level semantics.

</details>


### [68] [Automated Circuit Interpretation via Probe Prompting](https://arxiv.org/abs/2511.07002)
*Giuseppe Birardi*

Main category: cs.CL

TL;DR: 本文提出了一种自动化方法，用于将归因图转换为紧凑、可解释的子图，从而提高神经网络的可解释性。


<details>
  <summary>Details</summary>
Motivation: 机械可解释性旨在通过识别哪些学习特征介导特定行为来理解神经网络，但解释它们需要大量的手动分析。

Method: 我们提出了探针提示，这是一种自动化管道，可将归因图转换为由概念对齐的超节点构建的紧凑、可解释的子图。

Result: 探针提示的子图在保留高解释覆盖率的同时压缩了复杂性，并且概念对齐组表现出更高的行为一致性。

Conclusion: 我们释放了代码、交互式演示和最小化资源，以促进立即再现和社区采用。

Abstract: Mechanistic interpretability aims to understand neural networks by identifying which learned features mediate specific behaviors. Attribution graphs reveal these feature pathways, but interpreting them requires extensive manual analysis -- a single prompt can take approximately 2 hours for an experienced circuit tracer. We present probe prompting, an automated pipeline that transforms attribution graphs into compact, interpretable subgraphs built from concept-aligned supernodes. Starting from a seed prompt and target logit, we select high-influence features, generate concept-targeted yet context-varying probes, and group features by cross-prompt activation signatures into Semantic, Relationship, and Say-X categories using transparent decision rules.
  Across five prompts including classic "capitals" circuits, probe-prompted subgraphs preserve high explanatory coverage while compressing complexity (Completeness 0.83, mean across circuits; Replacement 0.54). Compared to geometric clustering baselines, concept-aligned groups exhibit higher behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and 5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower geometric compactness. Entity-swap tests reveal a layerwise hierarchy: early-layer features transfer robustly (64% transfer rate, mean layer 6.3), while late-layer Say-X features specialize for output promotion (mean layer 16.4), supporting a backbone-and-specialization view of transformer computation.
  We release code (https://github.com/peppinob-ol/attribution-graph-probing), an interactive demo (https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal artifacts enabling immediate reproduction and community adoption.

</details>


### [69] [Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs](https://arxiv.org/abs/2511.07003)
*Yingfeng Luo,Ziqiang Xu,Yuxuan Ouyang,Murun Yang,Dingyang Lin,Kaiyan Chang,Tong Zheng,Bei Li,Peinan Feng,Quan Du,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文介绍了LMT，一种专注于中英文的多语言翻译模型，通过解决方向退化问题和引入PMP方法，实现了先进的性能，并发布了多种规模的模型以促进研究。


<details>
  <summary>Details</summary>
Motivation: 解决多语言机器翻译中的广泛语言覆盖、一致的翻译质量和英语中心偏差问题。

Method: 提出了一种名为Strategic Downsampling的方法来缓解方向退化现象，并设计了Parallel Multilingual Prompting (PMP)来增强跨语言转移。

Result: LMT在可比较的语言覆盖范围内实现了最先进的性能，其4B模型显著超过了更大的Aya-101-13B和NLLB-54B模型。

Conclusion: LMT在语言覆盖范围和翻译质量方面达到了最先进的水平，超越了更大的模型，并提供了四种不同规模的版本以促进未来的研究。

Abstract: Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \textbf{LMT}, a suite of \textbf{L}arge-scale \textbf{M}ultilingual \textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \footnote{\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.

</details>


### [70] [A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation](https://arxiv.org/abs/2511.07010)
*Siddharth Betala,Kushan Raj,Vipul Betala,Rohan Saswade*

Main category: cs.CL

TL;DR: 本文介绍了一种两阶段方法，用于解决多模态翻译任务中的训练数据质量问题，并通过参数高效的微调提高了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中的质量问题，提高多模态翻译任务的翻译质量。

Method: 我们提出了一种两阶段的方法，首先通过自动化错误检测和纠正处理训练数据中的质量问题，然后进行参数高效的模型微调。此外，我们引入了一个视觉增强的判断-纠正管道，利用多模态语言模型系统地识别和纠正翻译错误。

Result: 在四个语言对上，使用修正后的数据进行训练，BLEU分数有所提升，例如英语-孟加拉语在评估集上提升了+1.30，挑战集上提升了+0.70。

Conclusion: 通过使用自动化错误检测和纠正以及参数高效的模型微调，我们的方法在多个语言对上实现了BLEU分数的提升。

Abstract: In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -> 54.00).

</details>


### [71] [Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity](https://arxiv.org/abs/2511.07011)
*Anastasiia Tokareva,Judith Dineley,Zoe Firth,Pauline Conde,Faith Matcham,Sara Siddi,Femke Lamers,Ewan Carr,Carolin Oetzmann,Daniel Leightley,Yuezhou Zhang,Amos A. Folarin,Josep Maria Haro,Brenda W. J. H. Penninx,Raquel Bailon,Srinivasan Vairavan,Til Wykes,Richard J. B. Dobson,Vaibhav A. Narayan,Matthew Hotopf,Nicholas Cummins,The RADAR-CNS Consortium*

Main category: cs.CL

TL;DR: 该研究探讨了通过口语分析来评估重度抑郁症症状严重程度的潜力，发现了一些与症状相关的词汇特征，但预测性能有限，需要在更大样本和多种语言中进一步研究。


<details>
  <summary>Details</summary>
Motivation: 通过移动设备在临床预约之间捕捉到的口语可能有助于客观、更频繁地评估症状严重程度和早期检测重度抑郁症的复发。然而，迄今为止的研究主要集中在非临床横断面样本的书面语言上，使用复杂的机器学习方法，且可解释性有限。

Method: 我们描述了对来自英国、荷兰和西班牙的586名参与者的5,836次录音以及PHQ-8评估的纵向语音数据的初步探索性分析，使用线性混合效应模型来识别与MDD症状严重程度相关的可解释的词汇特征，并使用高维向量嵌入测试四种回归器机器学习模型的预测性能。

Result: 在英语数据中，MDD症状严重程度与7个特征相关，包括词汇多样性测量和绝对语言。在荷兰语中，与每句话中的单词数和积极词语频率有关；在西班牙语录音中未观察到关联。所有语言的词汇特征和向量嵌入的预测能力接近随机水平。

Conclusion: 为了理解词汇标记在临床研究和实践中的价值，需要在多种语言的大样本中进行进一步研究，并使用考虑个体内部和个体间语言差异的改进模型和机器学习模型。

Abstract: Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models.
  Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages.
  Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice.
  Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.

</details>


### [72] [Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks](https://arxiv.org/abs/2511.07025)
*Yauhen Babakhin,Radek Osmulski,Ronay Ak,Gabriel Moreira,Mengyao Xu,Benedikt Schifferer,Bo Liu,Even Oldridge*

Main category: cs.CL

TL;DR: llama-embed-nemotron-8b 是一个全开源文本嵌入模型，在MMTEB基准测试中取得最先进的性能，通过结合真实和合成数据以及详细的消融研究来实现。


<details>
  <summary>Details</summary>
Motivation: 为了应对最近模型在训练数据或方法上不完全公开的问题，我们开发了一个完全开源的模型，公开其权重和详细的消融研究，并计划分享经过筛选的训练数据集。

Method: 通过结合16.1百万个查询-文档对的数据混合，包括从公共数据集中的7.7百万样本和各种开放权重LLM生成的8.4百万合成示例，开发了一个全开源模型，并进行了详细的消融研究。

Result: llama-embed-nemotron-8b 在所有主要嵌入任务中表现出色，包括检索、分类和语义文本相似性（STS），并在多语言场景中表现优异，例如低资源语言和跨语言设置。

Conclusion: llama-embed-nemotron-8b 是一个具有顶级性能、广泛适用性和用户驱动灵活性的通用文本嵌入解决方案。

Abstract: We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.

</details>


### [73] [Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data](https://arxiv.org/abs/2511.07044)
*Mihael Arcan,David-Paul Niland*

Main category: cs.CL

TL;DR: 本研究评估了多种用于心理健康检测的方法，比较了大型语言模型与经典机器学习和基于变压器的架构。结果显示基于变压器的模型效果良好，并且合成数据在提高召回率和泛化能力方面很有价值。


<details>
  <summary>Details</summary>
Motivation: 由于症状表达的细微和多样化，从文本中检测心理健康状况仍然具有挑战性。

Method: 本研究评估了多种用于心理健康检测的方法，比较了大型语言模型（如Llama和GPT）与经典机器学习和基于变压器的架构（包括BERT、XLNet和Distil-RoBERTa）。使用DAIC-WOZ临床访谈数据集对模型进行了微调，并应用了合成数据生成来缓解类别不平衡问题。

Result: 结果表明，Distil-RoBERTa在GAD-2任务中达到了最高的F1分数（0.883），而XLNet在PHQ任务中表现最佳（F1高达0.891）。对于压力检测，零样本合成方法（SD+Zero-Shot-Basic）达到了F1为0.884和ROC AUC为0.886。研究结果证明了基于变压器的模型的有效性，并突显了合成数据在提高召回率和泛化能力方面的价值。然而，需要仔细校准以防止精度损失。

Conclusion: 本研究强调了结合先进的语言模型和数据增强在提高从文本中自动进行心理健康评估方面的潜力。

Abstract: Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.

</details>


### [74] [When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction](https://arxiv.org/abs/2511.07055)
*Katharina Beckh,Stefan Rüping*

Main category: cs.CL

TL;DR: 本文研究了如何通过集成多个模型来提高医学数据集中完整证据的召回率。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，仅提供最小足够的证据是不够的，需要识别完整的贡献特征以满足合规性和分类需求。

Method: 本文进行了一项案例研究，使用包含人类标注完整证据的医学数据集，分析了不同模型的证据恢复情况，并探讨了集成方法的效果。

Result: 单个模型通常只能恢复部分完整证据，而集成方法将证据召回率从约0.60提高到约0.86。

Conclusion: 本文表明，通过聚合多个模型的证据可以显著提高证据召回率，并讨论了相关影响。

Abstract: Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\sim$0.60 (single best model) to $\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.

</details>


### [75] [Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection](https://arxiv.org/abs/2511.07065)
*Brage Eilertsen,Røskva Bjørgfinsdóttir,Francielle Vargas,Ali Ramezani-Kebrya*

Main category: cs.CL

TL;DR: 本文提出了一种名为监督理性注意（SRA）的框架，通过将模型注意力与人类理性对齐来提高仇恨言论分类的可解释性和公平性。SRA在多个基准测试中表现出色，证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的不透明性对仇恨言论检测系统的伦理部署提出了重大挑战。

Method: 我们引入了监督理性注意（SRA），这是一种框架，它显式地将模型注意与人类理性对齐，提高了仇恨言论分类的可解释性和公平性。SRA将监督注意机制集成到基于变压器的分类器中，优化了一个结合标准分类损失和最小化注意权重与人工标注理性之间差异的对齐损失项的联合目标。

Result: SRA在仇恨言论基准测试中实现了2.4倍更好的可解释性，产生了更忠实和与人类对齐的标记级解释。在公平性方面，SRA在所有措施上都表现良好，特别是在检测针对身份群体的有毒帖子方面排名第二，同时在其他指标上保持了可比较的结果。

Conclusion: 这些发现表明，将人类理性纳入注意力机制可以增强可解释性和真实性，而不会损害公平性。

Abstract: The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.

</details>


### [76] [Importance-Aware Data Selection for Efficient LLM Instruction Tuning](https://arxiv.org/abs/2511.07074)
*Tingyu Jiang,Shen Li,Yiyao Song,Lan Zhang,Hualei Zhu,Yuan Zhao,Xiaohang Xu,Kenjiro Taura,Hao Henry Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的度量标准MIWV，用于选择对提升指令微调性能最有用的高质量数据，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要关注于计算数据质量评分来评估指令数据，但需要一种方法来选择能够最大化提升指令微调性能的高质量数据。

Method: 本文提出了Model Instruction Weakness Value (MIWV) 作为度量标准，该度量基于模型在使用In-Context Learning (ICL)时响应的差异性，以识别对提升指令微调性能最有益的数据。

Result: 实验结果表明，仅选择MIWV排名前1%的数据可以超越使用完整数据集的效果，并且这种方法比现有研究更有效。

Conclusion: 本文提出了一种新的度量标准MIWV，用于量化指令数据在提升模型能力中的重要性，并通过实验验证了仅选择MIWV排名前1%的数据可以超越使用完整数据集的效果。

Abstract: Instruction tuning plays a critical role in enhancing the performance and efficiency of Large Language Models (LLMs). Its success depends not only on the quality of the instruction data but also on the inherent capabilities of the LLM itself. Some studies suggest that even a small amount of high-quality data can achieve instruction fine-tuning results that are on par with, or even exceed, those from using a full-scale dataset. However, rather than focusing solely on calculating data quality scores to evaluate instruction data, there is a growing need to select high-quality data that maximally enhances the performance of instruction tuning for a given LLM. In this paper, we propose the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the importance of instruction data in enhancing model's capabilities. The MIWV metric is derived from the discrepancies in the model's responses when using In-Context Learning (ICL), helping identify the most beneficial data for enhancing instruction tuning performance. Our experimental results demonstrate that selecting only the top 1\% of data based on MIWV can outperform training on the full dataset. Furthermore, this approach extends beyond existing research that focuses on data quality scoring for data selection, offering strong empirical evidence supporting the effectiveness of our proposed method.

</details>


### [77] [EmoBang: Detecting Emotion From Bengali Texts](https://arxiv.org/abs/2511.07077)
*Abdullah Al Maruf,Aditi Golder,Zakaria Masud Jiyad,Abdullah Al Numan,Tarannum Shaila Zaman*

Main category: cs.CL

TL;DR: 本文介绍了第一个全面的 Bengali 情感检测基准，提出了两种模型并取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 由于 Bengali 语言在情感检测方面研究不足，且缺乏大规模标准化数据集，因此需要进行相关研究以提高其情感检测的性能。

Method: 本文引入了一个新的 Bengali 情感数据集，并提出了两种模型：(i) 一种混合卷积循环神经网络 (CRNN) 模型 (EmoBangHybrid) 和 (ii) 一种 AdaBoost-BERT 集成模型 (EmoBangEnsemble)。此外，还评估了六种基线模型和五种特征工程技术，并评估了零样本和少量样本大语言模型 (LLMs) 在数据集上的表现。

Result: 实验结果表明，EmoBangH 和 EmoBangE 的准确率分别为 92.86% 和 93.69%，优于现有方法，并为未来的研究建立了强有力的基线。

Conclusion: 本文提出了两个模型用于自动情感检测，并展示了它们在 Bengali 情感检测中的优越性能，为未来的研究建立了强有力的基线。

Abstract: Emotion detection from text seeks to identify an individual's emotional or mental state - positive, negative, or neutral - based on linguistic cues. While significant progress has been made for English and other high-resource languages, Bengali remains underexplored despite being the world's fourth most spoken language. The lack of large, standardized datasets classifies Bengali as a low-resource language for emotion detection. Existing studies mainly employ classical machine learning models with traditional feature engineering, yielding limited performance. In this paper, we introduce a new Bengali emotion dataset annotated across eight emotion categories and propose two models for automatic emotion detection: (i) a hybrid Convolutional Recurrent Neural Network (CRNN) model (EmoBangHybrid) and (ii) an AdaBoost-Bidirectional Encoder Representations from Transformers (BERT) ensemble model (EmoBangEnsemble). Additionally, we evaluate six baseline models with five feature engineering techniques and assess zero-shot and few-shot large language models (LLMs) on the dataset. To the best of our knowledge, this is the first comprehensive benchmark for Bengali emotion detection. Experimental results show that EmoBangH and EmoBangE achieve accuracies of 92.86% and 93.69%, respectively, outperforming existing methods and establishing strong baselines for future research.

</details>


### [78] [Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora](https://arxiv.org/abs/2511.07080)
*Khalil Hennara,Ahmad Bastati,Muhammad Hreden,Mohamed Motasim Hamed,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 本文提出了一个处理Common Crawl数据集的管道Wasm，以创建一个新的阿拉伯语多模态数据集，该数据集提供markdown输出，并保留了网络内容的结构完整性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的阿拉伯语多模态数据集，限制了阿拉伯语相关研究的进展。因此，本文旨在创建一个能够保留文档结构的阿拉伯语多模态数据集。

Method: 本文提出了一种处理Common Crawl数据集的管道Wasm，以创建一个新的阿拉伯语多模态数据集。该方法保留了网络内容的结构完整性，同时保持了文本和多模态预训练场景的灵活性。

Result: 本文创建了一个新的阿拉伯语多模态数据集，该数据集提供markdown输出，并保留了网络内容的结构完整性。此外，还提供了数据处理管道的比较分析，并公开发布了数据集和处理管道。

Conclusion: 本文提出了一个处理Common Crawl数据集的管道Wasm，以创建一个新的阿拉伯语多模态数据集，该数据集独特地提供markdown输出。我们提供了对我们的数据处理管道与现有主要数据集的比较分析，并公开发布了代表性的数据集转储以及用于阿拉伯语的多模态处理管道，以支持未来的研究。

Abstract: The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.

</details>


### [79] [More Agents Helps but Adversarial Robustness Gap Persists](https://arxiv.org/abs/2511.07112)
*Khashayar Alavi,Zhastay Yeltay,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 研究显示LLM代理协作能提高数学问题解答的准确性，但对抗性输入仍是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理协作是否比单个LLM更鲁棒，特别是在面对对抗性输入时。

Method: 使用统一的采样和投票框架（Agent Forest）评估六个开源模型在四个基准测试中的表现，分析不同噪声类型和代理数量对结果的影响。

Result: 噪声类型影响显著，人类打字错误是主要瓶颈；协作提高了准确性，但对抗鲁棒性差距仍然存在。

Conclusion: 协作可以提高准确性，但对抗鲁棒性差距仍然存在。

Abstract: When LLM agents work together, they seem to be more powerful than a single LLM in mathematical question answering. However, are they also more robust to adversarial inputs? We investigate this question using adversarially perturbed math questions. These perturbations include punctuation noise with three intensities (10, 30, and 50 percent), plus real-world and human-like typos (WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B, Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math, MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15, 20, 25). Our findings show that (1) Noise type matters: punctuation noise harm scales with its severity, and the human typos remain the dominant bottleneck, yielding the largest gaps to Clean accuracy and the highest ASR even with a large number of agents. And (2) Collaboration reliably improves accuracy as the number of agents, n, increases, with the largest gains from one to five agents and diminishing returns beyond 10 agents. However, the adversarial robustness gap persists regardless of the agent count.

</details>


### [80] [Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought](https://arxiv.org/abs/2511.07124)
*Zhikang Chen,Sen Cui,Deheng Ye,Yu Zhang,Yatao Bian,Tingting Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于能量模型的链式思维校准框架（EBM-CoT），以提高大型语言模型在多步骤推理中的一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的显式链式思维方法存在错误传播和词汇表达能力有限的问题，而隐式或连续推理方法缺乏明确机制来确保推理步骤之间的一致性。

Method: 我们提出了EBM-CoT，一种基于能量的链式思维校准框架，通过能量模型（EBM）优化潜在思维表示。

Result: 在数学、常识和符号推理基准测试中进行了广泛的实验，结果表明该框架显著提高了推理的一致性和效率。

Conclusion: 我们的方法显著提高了大型语言模型在多步骤推理中的一致性和效率。

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities through \emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step intermediate reasoning. However, explicit CoT methods rely on discrete token-level reasoning processes that are prone to error propagation and limited by vocabulary expressiveness, often resulting in rigid and inconsistent reasoning trajectories. Recent research has explored implicit or continuous reasoning in latent spaces, allowing models to perform internal reasoning before generating explicit output. Although such approaches alleviate some limitations of discrete CoT, they generally lack explicit mechanisms to enforce consistency among reasoning steps, leading to divergent reasoning paths and unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based Chain-of-Thought Calibration framework that refines latent thought representations through an energy-based model (EBM). Our method dynamically adjusts latent reasoning trajectories toward lower-energy, high-consistency regions in the embedding space, improving both reasoning accuracy and consistency without modifying the base language model. Extensive experiments across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate that the proposed framework significantly enhances the consistency and efficiency of multi-step reasoning in LLMs.

</details>


### [81] [LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging](https://arxiv.org/abs/2511.07129)
*Seungeon Lee,Soumi Das,Manish Gupta,Krishna P. Gummadi*

Main category: cs.CL

TL;DR: LoGo是一种无需训练的框架，能够在实例级别动态选择和合并适配器，从而在多个NLP任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA适配器通常针对单一任务进行训练，限制了其在现实世界中的应用。

Method: LoGo是一种无需训练的框架，能够在实例级别动态选择和合并适配器，而无需额外要求。

Result: LoGo在5个NLP基准、27个数据集和3个模型家族中优于基于训练的基线，在某些任务上的性能提高了3.6%。

Conclusion: LoGo在多个NLP基准和数据集中表现出色，展示了其有效性和实用性。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models.However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.

</details>


### [82] [TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine](https://arxiv.org/abs/2511.07148)
*Zihao Cheng,Yuheng Lu,Huaiqian Ye,Zeming Liu,Minqi Wang,Jingjing Liu,Zihan Li,Wei Fan,Yuanfang Guo,Ruiji Fu,Shifeng She,Gang Wang,Yunhong Wang*

Main category: cs.CL

TL;DR: 本文介绍了TCM-Eval，这是第一个动态且可扩展的传统中医基准，并提出了SI-CoTE方法来增强问题-答案对，开发了ZhiMingTang（ZMT）这一先进的传统中医大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现代医学中表现出色，但在传统中医中的应用受到缺乏标准化基准和高质量训练数据的严重限制。

Method: 我们构建了一个大规模的训练语料库，并提出了Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) 方法，通过拒绝采样自主丰富带有验证推理链的问题-答案对，建立了数据和模型共同进化的良性循环。

Result: 我们引入了TCM-Eval，这是第一个动态且可扩展的传统中医基准，精心从国家医学执照考试中收集并由传统中医专家验证。

Conclusion: 我们开发了ZhiMingTang（ZMT），这是一个专为传统中医设计的最先进的大型语言模型，其表现显著超过人类执业者的及格阈值。为了鼓励未来的研究和开发，我们发布了公共排行榜，促进社区参与和持续改进。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modern medicine, yet their application in Traditional Chinese Medicine (TCM) remains severely limited by the absence of standardized benchmarks and the scarcity of high-quality training data. To address these challenges, we introduce TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts. Furthermore, we construct a large-scale training corpus and propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution. Using this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, which significantly exceeds the passing threshold for human practitioners. To encourage future research and development, we release a public leaderboard, fostering community engagement and continuous improvement.

</details>


### [83] [Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?](https://arxiv.org/abs/2511.07162)
*Lynn Greschner,Meike Bauer,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 本文探讨了评估理论在论证说服力分析中的适用性，并发现评估比情感更能提升说服力预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注情感的整体强度和分类值，但我们认为情感在论证中是主观的，取决于接收者的目标、标准、先验知识和立场。

Method: 基于ContArgA语料库的注释，我们进行了零次提示实验，以评估黄金标注和预测情感及评估对于主观说服力标签评估的重要性。

Result: 虽然分类情感信息确实提高了说服力预测，但评估的改进更为明显。

Conclusion: 本文首次系统比较了用于说服力预测的情感模型，展示了评估的优势，并为计算论证的理论和实际应用提供了见解。

Abstract: The convincingness of an argument does not only depend on its structure (logos), the person who makes the argument (ethos), but also on the emotion that it causes in the recipient (pathos). While the overall intensity and categorical values of emotions in arguments have received considerable attention in the research community, we argue that the emotion an argument evokes in a recipient is subjective. It depends on the recipient's goals, standards, prior knowledge, and stance. Appraisal theories lend themselves as a link between the subjective cognitive assessment of events and emotions. They have been used in event-centric emotion analysis, but their suitability for assessing argument convincingness remains unexplored. In this paper, we evaluate whether appraisal theories are suitable for emotion analysis in arguments by considering subjective cognitive evaluations of the importance and impact of an argument on its receiver. Based on the annotations in the recently published ContArgA corpus, we perform zero-shot prompting experiments to evaluate the importance of gold-annotated and predicted emotions and appraisals for the assessment of the subjective convincingness labels. We find that, while categorical emotion information does improve convincingness prediction, the improvement is more pronounced with appraisals. This work presents the first systematic comparison between emotion models for convincingness prediction, demonstrating the advantage of appraisals, providing insights for theoretical and practical applications in computational argumentation.

</details>


### [84] [AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning](https://arxiv.org/abs/2511.07166)
*Meiyun Wang,Charin Polpanumas*

Main category: cs.CL

TL;DR: AdaRec是一种利用大型语言模型进行自适应个性化推荐的少量样本上下文学习框架。它通过叙述性档案将用户-项目交互转化为自然语言表示，并采用双通道架构进行水平行为对齐和垂直因果归因。AdaRec无需手动特征工程，支持快速跨任务适应，并在少量样本和零样本场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法需要手动特征工程，而AdaRec通过语义表示消除手动特征工程，并支持在最小监督下快速跨任务适应。

Method: AdaRec引入了叙述性档案，将用户-项目交互转换为自然语言表示，以实现统一的任务处理并增强人类可读性。围绕双变量推理范式，AdaRec采用双通道架构，结合水平行为对齐（发现同行驱动模式）与垂直因果归因（突出用户偏好背后的决定因素）。

Result: 实验表明，在真实电商数据集上，AdaRec在少量样本设置中优于机器学习模型和基于LLM的基线，最多提高8%。在零样本场景中，它比专家设计的配置提高了19%。此外，对由AdaRec生成的合成数据进行轻量级微调可以达到完全微调模型的性能。

Conclusion: AdaRec在少量样本设置中优于机器学习模型和基于LLM的基线，最多提高8%。在零样本场景中，它比专家设计的配置提高了19%，展示了在长尾个性化中的有效性。此外，对由AdaRec生成的合成数据进行轻量级微调可以达到完全微调模型的性能，突显了其在各种任务中的效率和泛化能力。

Abstract: We propose AdaRec, a few-shot in-context learning framework that leverages large language models for an adaptive personalized recommendation. AdaRec introduces narrative profiling, transforming user-item interactions into natural language representations to enable unified task handling and enhance human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a dual-channel architecture that integrates horizontal behavioral alignment, discovering peer-driven patterns, with vertical causal attribution, highlighting decisive factors behind user preferences. Unlike existing LLM-based approaches, AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experiments on real ecommerce datasets demonstrate that AdaRec outperforms both machine learning models and LLM-based baselines by up to eight percent in few-shot settings. In zero-shot scenarios, it achieves up to a nineteen percent improvement over expert-crafted profiling, showing effectiveness for long-tail personalization with minimal interaction data. Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec matches the performance of fully fine-tuned models, highlighting its efficiency and generalization across diverse tasks.

</details>


### [85] [EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models](https://arxiv.org/abs/2511.07193)
*Jiacheng Huang,Ning Yu,Xiaoyin Yi*

Main category: cs.CL

TL;DR: EMODIS is a benchmark for evaluating how well LLMs can interpret ambiguous emoji expressions in different contexts, and it shows that even strong models struggle with this task.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are increasingly deployed in real-world communication settings, yet their ability to resolve context-dependent ambiguity remains underexplored.

Method: We present EMODIS, a new benchmark for evaluating LLMs' capacity to interpret ambiguous emoji expressions under minimal but contrastive textual contexts.

Result: Even the strongest models frequently fail to distinguish meanings when only subtle contextual cues are present. Further analysis reveals systematic biases toward dominant interpretations and limited sensitivity to pragmatic contrast.

Conclusion: EMODIS provides a rigorous testbed for assessing contextual disambiguation, and highlights the gap in semantic reasoning between humans and LLMs.

Abstract: Large language models (LLMs) are increasingly deployed in real-world communication settings, yet their ability to resolve context-dependent ambiguity remains underexplored. In this work, we present EMODIS, a new benchmark for evaluating LLMs' capacity to interpret ambiguous emoji expressions under minimal but contrastive textual contexts. Each instance in EMODIS comprises an ambiguous sentence containing an emoji, two distinct disambiguating contexts that lead to divergent interpretations, and a specific question that requires contextual reasoning. We evaluate both open-source and API-based LLMs, and find that even the strongest models frequently fail to distinguish meanings when only subtle contextual cues are present. Further analysis reveals systematic biases toward dominant interpretations and limited sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for assessing contextual disambiguation, and highlights the gap in semantic reasoning between humans and LLMs.

</details>


### [86] [Discourse Graph Guided Document Translation with Large Language Models](https://arxiv.org/abs/2511.07230)
*Viet-Thanh Pham,Minghan Wang,Hao-Han Liao,Thuy-Trang Vu*

Main category: cs.CL

TL;DR: TransGraph是一种基于话语引导的框架，通过结构化话语图来建模段落间的关系，从而在文档级机器翻译任务中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的代理机器翻译系统需要大量的计算资源且对记忆检索策略敏感，因此需要一种更高效的方法来处理长文档翻译。

Method: TransGraph通过结构化话语图来显式建模段落间的关系，并选择性地将每个翻译片段条件化到相关的图邻域。

Result: TransGraph在三个文档级MT基准测试中超越了强基线，在翻译质量和术语一致性方面表现优异，同时具有显著更低的token开销。

Conclusion: TransGraph在文档级机器翻译任务中表现出色，同时具有较低的token开销。

Abstract: Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.

</details>


### [87] [Who Is the Story About? Protagonist Entity Recognition in News](https://arxiv.org/abs/2511.07296)
*Jorge Gabín,M. Eduardo Ares,Javier Parapar*

Main category: cs.CL

TL;DR: 本文介绍了 Protagonist Entity Recognition (PER)，这是一种识别锚定新闻故事并塑造其主要发展的组织的任务。通过比较大型语言模型 (LLMs) 的预测与专家标注者的注释，验证 PER。结果表明，PER 是对以叙事为中心的信息抽取的可行且有意义的扩展，并且引导的 LLM 可以在大规模上近似人类对叙事重要性的判断。


<details>
  <summary>Details</summary>
Motivation: 传统命名实体识别 (NER) 将所有提及同等对待，掩盖了真正推动叙述的实体，这限制了依赖于理解事件显著性、影响或叙述焦点的下游任务。

Method: 引入了 Protagonist Entity Recognition (PER)，通过比较大型语言模型 (LLMs) 的预测与四位专家标注者的注释，验证 PER。利用这些发现，使用最先进的 LLM 对大规模新闻集合进行自动标记。

Result: PER 是一个可行且有意义的扩展，并且引导的 LLM 可以在大规模上近似人类对叙事重要性的判断。

Conclusion: PER 是对以叙事为中心的信息抽取的可行且有意义的扩展，并且引导的 LLM 可以在大规模上近似人类对叙事重要性的判断。

Abstract: News articles often reference numerous organizations, but traditional Named Entity Recognition (NER) treats all mentions equally, obscuring which entities genuinely drive the narrative. This limits downstream tasks that rely on understanding event salience, influence, or narrative focus. We introduce Protagonist Entity Recognition (PER), a task that identifies the organizations that anchor a news story and shape its main developments. To validate PER, we compare he predictions of Large Language Models (LLMs) against annotations from four expert annotators over a gold corpus, establishing both inter-annotator consistency and human-LLM agreement. Leveraging these findings, we use state-of-the-art LLMs to automatically label large-scale news collections through NER-guided prompting, generating scalable, high-quality supervision. We then evaluate whether other LLMs, given reduced context and without explicit candidate guidance, can still infer the correct protagonists. Our results demonstrate that PER is a feasible and meaningful extension to narrative-centered information extraction, and that guided LLMs can approximate human judgments of narrative importance at scale.

</details>


### [88] [Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification](https://arxiv.org/abs/2511.07304)
*Sourav Saha,K M Nafi Asib,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 本文介绍了团队Retriv在孟加拉多任务仇恨言论识别共享任务中的工作，采用变压器模型的集成方法取得了良好的结果。


<details>
  <summary>Details</summary>
Motivation: 本文解决了孟加拉仇恨言论识别的问题，这是一个社会影响大但语言上具有挑战性的任务。

Method: 对于子任务1A和1B，我们采用了变压器模型（BanglaBERT、MuRIL、IndicBERTv2）的软投票集成。对于子任务1C，我们训练了三种多任务变体，并通过加权投票集成其预测。

Result: 我们的系统在子任务1A和1B上分别获得了72.75%和72.69%的微F1分数，在子任务1C上获得了72.62%的加权微F1分数。在共享任务排行榜上，这些对应于第9、第10和第7名。

Conclusion: 这些结果展示了变压器集合和加权多任务框架在低资源环境中推进孟加拉仇恨言论检测的潜力。我们为社区公开了实验脚本。

Abstract: This paper addresses the problem of Bangla hate speech identification, a socially impactful yet linguistically challenging task. As part of the "Bangla Multi-task Hate Speech Identification" shared task at the BLP Workshop, IJCNLP-AACL 2025, our team "Retriv" participated in all three subtasks: (1A) hate type classification, (1B) target group identification, and (1C) joint detection of type, severity, and target. For subtasks 1A and 1B, we employed a soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2). For subtask 1C, we trained three multitask variants and aggregated their predictions through a weighted voting ensemble. Our systems achieved micro-f1 scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62% (1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th positions, respectively. These results highlight the promise of transformer ensembles and weighted multitask frameworks for advancing Bangla hate speech detection in low-resource contexts. We made experimental scripts publicly available for the community.

</details>


### [89] [ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding](https://arxiv.org/abs/2511.07311)
*Tuan-Dung Le,Shohreh Haddadan,Thanh Q. Thieu*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据增强技术，利用大语言模型扩展医学缩写，并引入一致性训练，以提高自动ICD编码的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要增强模型对代码层次结构和同义词的理解，但常常忽视临床笔记中广泛使用的医学缩写，这是ICD代码推断的关键因素。

Method: 我们提出了一种新颖有效的数据增强技术，利用大语言模型扩展医学缩写，使模型能够在其全称表示上进行训练。此外，我们引入了一致性训练，通过强制原始文档和增强文档之间的预测一致来规范预测。

Result: 在MIMIC-III数据集上的实验表明，我们的方法ACE-ICD在多个设置中取得了新的最先进性能。

Conclusion: 我们的方法ACE-ICD在多个设置中建立了新的最先进性能，包括常见代码、罕见代码和全代码分配。

Abstract: Automatic ICD coding, the task of assigning disease and procedure codes to electronic medical records, is crucial for clinical documentation and billing. While existing methods primarily enhance model understanding of code hierarchies and synonyms, they often overlook the pervasive use of medical acronyms in clinical notes, a key factor in ICD code inference. To address this gap, we propose a novel effective data augmentation technique that leverages large language models to expand medical acronyms, allowing models to be trained on their full form representations. Moreover, we incorporate consistency training to regularize predictions by enforcing agreement between the original and augmented documents. Extensive experiments on the MIMIC-III dataset demonstrate that our approach, ACE-ICD establishes new state-of-the-art performance across multiple settings, including common codes, rare codes, and full-code assignments. Our code is publicly available.

</details>


### [90] [RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments](https://arxiv.org/abs/2511.07317)
*Zhiyuan Zeng,Hamish Ivison,Yiping Wang,Lifan Yuan,Shuyue Stella Li,Zhuorui Ye,Siting Li,Jacqueline He,Runlong Zhou,Tong Chen,Chenyang Zhao,Yulia Tsvetkov,Simon Shaolei Du,Natasha Jaques,Hao Peng,Pang Wei Koh,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 我们引入了强化学习与可验证环境（RLVE），这是一种使用可验证环境的方法，这些环境可以程序化生成问题并提供算法可验证的奖励，以扩展语言模型（LM）的强化学习。RLVE-Gym是一个由人工环境工程开发的400个可验证环境的大规模套件。通过RLVE-Gym，我们展示了环境扩展（即扩大训练环境的集合）可以持续提高可泛化的推理能力。在RLVE-Gym中对所有400个环境进行联合训练，可以在六个推理基准测试中平均提高3.37%的绝对值。相比之下，继续该LM的原始RL训练仅能获得0.49%的平均绝对增益，尽管使用了超过3倍的计算资源。


<details>
  <summary>Details</summary>
Motivation: To scale up RL for language models (LMs) and improve generalizable reasoning capabilities, we need to address the issue of static data distributions leading to vanishing learning signals when problems are either too easy or too hard for the policy.

Method: We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs).

Result: Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks.

Conclusion: RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs.

Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.

</details>


### [91] [When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs](https://arxiv.org/abs/2511.07318)
*Shaowen Wang,Yiqi Dong,Ruinian Chang,Tansheng Zhu,Yuebo Sun,Kaifeng Lyu,Jian Li*

Main category: cs.CL

TL;DR: 本文研究了由虚假相关性引起的幻觉问题，并表明现有检测方法无法有效应对这些问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）继续表现出幻觉，生成看似正确但错误的响应。我们关注一种之前未被充分探讨的幻觉类别，这种幻觉是由虚假相关性驱动的。

Method: 我们通过系统控制的合成实验和对最先进的开源和专有LLM（包括GPT-5）的实证评估来研究这个问题。

Result: 我们展示了这些虚假相关性会导致幻觉，这些幻觉自信地生成，对模型扩展免疫，逃避当前的检测方法，并在拒绝微调后仍然存在。现有的幻觉检测方法，如基于置信度的过滤和内部状态探测，在存在虚假相关性的情况下根本失败。

Conclusion: 我们的研究强调了需要新的方法来专门解决由虚假相关性引起的幻觉。

Abstract: Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.

</details>


### [92] [FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation](https://arxiv.org/abs/2511.07322)
*Song Jin,Shuqi Li,Shukun Zhang,Rui Yan*

Main category: cs.CL

TL;DR: 本文首次将股权研究报告（ERR）生成任务形式化，提出了一种开源评估基准FinRpt，构建了一个集成7种金融数据类型的高质量ERR数据集，并引入了一个包含11个度量的全面评估系统。此外，还提出了一种专门为此任务设计的多智能体框架FinRpt-Gen，并通过监督微调和强化学习进行了训练。实验结果表明了FinRpt的数据质量和度量有效性以及FinRpt-Gen的高性能，展示了它们在ERR生成领域的潜力。所有代码和数据集都是公开的。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在金融任务如股票预测和问答中表现出色，但其在完全自动化股权研究报告生成中的应用仍然是未探索的领域。

Method: 我们提出了一个针对该任务的多智能体框架，名为FinRpt-Gen，并使用监督微调和强化学习在提出的数据集上训练了几种基于LLM的代理。

Result: 实验结果表明了基准FinRpt的数据质量和度量有效性以及FinRpt-Gen的高性能，展示了它们在ERR生成领域推动创新的潜力。

Conclusion: 实验结果表明，基准FinRpt的数据质量和度量有效性以及FinRpt-Gen的高性能，展示了它们在ERR生成领域推动创新的潜力。所有代码和数据集都是公开的。

Abstract: While LLMs have shown great success in financial tasks like stock prediction and question answering, their application in fully automating Equity Research Report generation remains uncharted territory. In this paper, we formulate the Equity Research Report (ERR) Generation task for the first time. To address the data scarcity and the evaluation metrics absence, we present an open-source evaluation benchmark for ERR generation - FinRpt. We frame a Dataset Construction Pipeline that integrates 7 financial data types and produces a high-quality ERR dataset automatically, which could be used for model training and evaluation. We also introduce a comprehensive evaluation system including 11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent framework specifically tailored to address this task, named FinRpt-Gen, and train several LLM-based agents on the proposed datasets using Supervised Fine-Tuning and Reinforcement Learning. Experimental results indicate the data quality and metrics effectiveness of the benchmark FinRpt and the strong performance of FinRpt-Gen, showcasing their potential to drive innovation in the ERR generation field. All code and datasets are publicly available.

</details>


### [93] [Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains](https://arxiv.org/abs/2511.07380)
*Pingjie Wang,Hongcheng Liu,Yusheng Liao,Ziqing Fan,Yaxin Du,Shuo Tang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出NTK-Selector框架，利用神经切线核选择通用领域的辅助数据，以提高低资源领域中大型语言模型的性能。实验结果表明该方法在多个领域均能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺和过拟合风险，大型语言模型在低资源领域中的应用面临挑战。然而，存在大量类似的通用领域数据，这些数据可能作为辅助监督来增强领域性能。

Method: 提出了一种基于神经切线核（NTK）的框架NTK-Selector，用于选择通用领域的辅助数据以增强领域特定性能。解决了直接应用NTK到LLMs时的理论假设和计算成本问题。

Result: 在四个低资源领域（医疗、金融、法律和心理）的广泛实验表明，NTK-Selector一致提升了下游性能。例如，在1000个领域内样本上微调仅带来了+0.8和+0.9点的提升，而使用NTK-Selector选择的9000个辅助样本则带来了+8.7和+5.1点的显著提升。

Conclusion: NTK-Selector能够显著提升低资源领域中的下游性能，表明通过选择有价值的辅助数据可以有效增强领域特定性能。

Abstract: Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: \textbf{\textit{how to effectively select the most valuable auxiliary data to maximize domain-specific performance}}, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose \textbf{NTK-Selector}, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial \textbf{gains of +8.7 and +5.1 points}, which corresponds to a \textbf{10.9x and 5.7x improvement} over the domain-only setting.

</details>


### [94] [Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation](https://arxiv.org/abs/2511.07382)
*K M Nafi Asib,Sourav Saha,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 本文提出了一种用于从孟加拉语指令生成代码的方法，结合了指令提示和测试驱动的反馈引导迭代优化过程，取得了良好的效果，并强调了低资源语言中特定方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言如孟加拉语缺乏指令到代码的数据集和评估基准，因此需要一种专门的方法来提高代码生成的效果。

Method: 本文提出的方法结合了指令提示和基于测试驱动的反馈引导迭代优化过程，使用微调的Qwen2.5-14B模型生成代码，并通过单元测试进行迭代优化。

Result: 该方法帮助团队“Retriv”在共享任务中获得第二名，Pass@1得分为0.934。

Conclusion: 本文提出了一个结合指令提示和基于测试驱动的反馈引导迭代优化过程的方法，用于从孟加拉语指令生成代码。该方法在共享任务中取得了第二名的成绩，并强调了低资源语言中针对特定方法的需求。

Abstract: Large Language Models (LLMs) have advanced the automated generation of code from natural language prompts. However, low-resource languages (LRLs) like Bangla remain underrepresented due to the limited availability of instruction-to-code datasets and evaluation benchmarks. To address this, the BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generation in Bangla". In this work, we propose a method that combines instruction prompting with a test-driven, feedback-guided iterative refinement process using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla instructions, tests it against unit tests, and iteratively refines any failing outputs through three evaluation passes, using test feedback to guide each step. This approach helped our team "Retriv" to secure 2nd place in the shared task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla instruction understanding and Python code generation, emphasizing the need for targeted methods in LRLs. We made experimental scripts publicly available for the community.

</details>


### [95] [Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence](https://arxiv.org/abs/2511.07384)
*Sean McLeish,Ang Li,John Kirchenbauer,Dayal Singh Kalra,Brian R. Bartoldson,Bhavya Kailkhura,Avi Schwarzschild,Jonas Geiping,Tom Goldstein,Micah Goldblum*

Main category: cs.CL

TL;DR: 本文研究了如何将预训练的非循环语言模型转换为深度循环模型，以在保持性能的同时减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用循环结构来减少总计算成本并保持性能。

Method: 研究如何将现有的预训练非循环语言模型转换为深度循环模型，并使用递增的课程来增加模型的有效深度。

Result: 在数学任务中，将预训练模型转换为循环模型在给定计算预算下表现优于直接微调原始非循环模型。

Conclusion: 通过将预训练的非循环语言模型转换为深度循环模型，可以在给定的计算预算下获得更好的性能。

Abstract: Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.

</details>


### [96] [Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction](https://arxiv.org/abs/2511.07392)
*Hyeryun Park,Byung Mo Gu,Jun Hee Lee,Byeong Hyeon Choi,Sekeun Kim,Hyun Koo Kim,Kyungsang Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于分层多智能体框架的语音导向外科代理协调平台（SAOP），用于在达芬奇机器人手术中处理语音命令，以访问和操作多模态患者数据。


<details>
  <summary>Details</summary>
Motivation: 在达芬奇机器人手术中，外科医生的手和眼睛完全投入到手术过程中，使得在不中断的情况下访问和操作多模态患者数据变得困难。

Method: 我们提出了一个基于分层多智能体框架的语音导向外科代理协调平台（SAOP），包括一个协调代理和三个由大型语言模型（LLMs）驱动的任务特定代理。这些LLM-based代理自主地规划、细化、验证和推理，将语音命令映射到具体任务，如检索临床信息、操作CT扫描或在手术视频上导航3D解剖模型。

Result: SAOP在240个语音命令中实现了高精度和成功率，LLM-based agents提高了对语音识别错误和多样或模糊的自由格式命令的鲁棒性。

Conclusion: SAOP在240个语音命令中实现了高精度和成功率，LLM-based agents提高了对语音识别错误和多样或模糊的自由格式命令的鲁棒性，展示了在微创达芬奇机器人手术中支持的强大潜力。

Abstract: In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in the procedure, making it difficult to access and manipulate multimodal patient data without interruption. We propose a voice-directed Surgical Agent Orchestrator Platform (SAOP) built on a hierarchical multi-agent framework, consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to map voice commands into specific tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models on the surgical video. We also introduce a Multi-level Orchestration Evaluation Metric (MOEM) to comprehensively assess the performance and robustness from command-level and category-level perspectives. The SAOP achieves high accuracy and success rates across 240 voice commands, while LLM-based agents improve robustness against speech recognition errors and diverse or ambiguous free-form commands, demonstrating strong potential to support minimally invasive da Vinci robotic surgery.

</details>


### [97] [ConvFill: Model Collaboration for Responsive Conversational Voice Agents](https://arxiv.org/abs/2511.07397)
*Vidya Srinivas,Zachary Englhardt,Maximus Powers,Shwetak Patel,Vikram Iyer*

Main category: cs.CL

TL;DR: 本文提出了一种名为对话填充的方法，通过将轻量级设备端模型与强大的后端模型结合，解决了对话代理中延迟和模型能力之间的矛盾，并取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 部署具有大型语言模型的对话语音代理面临一个关键挑战：基于云的基础模型提供深度推理和领域知识，但引入了干扰自然对话的延迟，而设备端模型响应迅速但缺乏复杂性。

Method: 我们提出了对话填充，这是一种任务，其中轻量级的设备端模型生成上下文适当的对话，同时无缝地结合来自强大后端模型的流式知识。

Result: 评估结果显示，对话填充可以成功学习，ConvFill在相同大小的独立小型模型上实现了36-42%的准确率提升，同时保持了亚200毫秒的响应延迟。

Conclusion: 我们的结果展示了这种方法在构建既立即响应又知识渊博的设备端对话代理方面的前景。

Abstract: Deploying conversational voice agents with large language models faces a critical challenge: cloud-based foundation models provide deep reasoning and domain knowledge but introduce latency that disrupts natural conversation, while on-device models respond immediately but lack sophistication. We propose conversational infill, a task where a lightweight on-device model generates contextually appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful backend model. This approach decouples response latency from model capability, enabling systems that feel responsive while accessing the full power of large-scale models. We present ConvFill, a 360M parameter model trained on synthetic multi-domain conversations. Evaluation across multiple backend models shows that conversational infill can be successfully learned, with ConvFill achieving accuracy improvements of 36-42% over standalone small models of the same size while consistently retaining sub-200ms response latencies. Our results demonstrate the promise of this approach for building on-device conversational agents that are both immediately responsive and knowledgeable.

</details>


### [98] [SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations](https://arxiv.org/abs/2511.07405)
*Manon Berriche,Célia Nouri,Chloé Clavel,Jean-Philippe Cointet*

Main category: cs.CL

TL;DR: 我们介绍了SPOT（在线线程中的停止点），这是一个将社会学概念转化为可重复NLP任务的注释语料库。通过二分类任务研究停止点，并提供了可靠的注释指南。结果表明，微调的编码器模型优于提示的大型语言模型，同时结合上下文元数据可以进一步提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的框架如反言论或社会纠正往往忽视了停止点这种普通的关键干预措施，因此我们需要一个可重复的NLP任务来研究这一概念。

Method: 我们将停止点概念操作化为二分类任务，并提供了可靠的注释指南。我们对微调的编码器模型（CamemBERT）和指令调优的大语言模型进行了基准测试，并在各种提示策略下进行评估。

Result: 微调的编码器模型在F1分数上比提示的大型语言模型高出超过10个百分点，这证实了监督学习在新兴的非英语社交媒体任务中的重要性。结合上下文元数据进一步将编码器模型的F1分数从0.75提高到0.78。

Conclusion: 我们释放了匿名数据集，以及注释指南和代码库，以促进透明度和可重复的研究。

Abstract: We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [99] [MCP-RiskCue: Can LLM infer risk information from MCP server System Logs?](https://arxiv.org/abs/2511.05867)
*Jiayi Fu,Qiyao Sun*

Main category: cs.CR

TL;DR: 本文提出了一个合成基准，用于评估LLMs从系统日志中识别MCP服务器安全风险的能力。实验结果表明，强化学习方法在提高LLM安全性方面效果更好。


<details>
  <summary>Details</summary>
Motivation: 为了填补这一空白，我们提出了第一个合成基准，用于评估LLMs从系统日志中识别安全风险的能力。

Method: 我们定义了九类MCP服务器风险，并使用十种最先进的LLMs生成了1,800个合成系统日志。这些日志被嵌入到243个精选的MCP服务器的返回值中，产生了用于训练的2,421个聊天历史记录和用于评估的471个查询的数据集。

Result: 我们的初步实验显示，较小的模型常常无法检测到有风险的系统日志，导致高假阴性。虽然经过监督微调（SFT）的模型往往过度标记良性日志，导致假阳性增加，但基于可验证奖励的强化学习（RLVR）提供了更好的精确率-召回率平衡。特别是，经过组相对策略优化（GRPO）训练后，Llama3.1-8B-Instruct达到了83%的准确率，超过了最佳大型远程模型9个百分点。细粒度的、按类别分析进一步证明了强化学习在提高MCP框架内LLM安全性方面的有效性。

Conclusion: 我们的实验表明，较小的模型通常无法检测到有风险的系统日志，导致高假阴性。虽然经过监督微调（SFT）的模型往往过度标记良性日志，导致假阳性增加，但基于可验证奖励的强化学习（RLVR）提供了更好的精确率-召回率平衡。特别是，经过组相对策略优化（GRPO）训练后，Llama3.1-8B-Instruct达到了83%的准确率，超过了最佳大型远程模型9个百分点。细粒度的、按类别分析进一步证明了强化学习在提高MCP框架内LLM安全性方面的有效性。

Abstract: Large language models (LLMs) demonstrate strong capabilities in solving complex tasks when integrated with external tools. The Model Context Protocol (MCP) has become a standard interface for enabling such tool-based interactions. However, these interactions introduce substantial security concerns, particularly when the MCP server is compromised or untrustworthy. While prior benchmarks primarily focus on prompt injection attacks or analyze the vulnerabilities of LLM MCP interaction trajectories, limited attention has been given to the underlying system logs associated with malicious MCP servers. To address this gap, we present the first synthetic benchmark for evaluating LLMs ability to identify security risks from system logs. We define nine categories of MCP server risks and generate 1,800 synthetic system logs using ten state-of-the-art LLMs. These logs are embedded in the return values of 243 curated MCP servers, yielding a dataset of 2,421 chat histories for training and 471 queries for evaluation. Our pilot experiments reveal that smaller models often fail to detect risky system logs, leading to high false negatives. While models trained with supervised fine-tuning (SFT) tend to over-flag benign logs, resulting in elevated false positives, Reinforcement Learning from Verifiable Reward (RLVR) offers a better precision-recall balance. In particular, after training with Group Relative Policy Optimization (GRPO), Llama3.1-8B-Instruct achieves 83% accuracy, surpassing the best-performing large remote model by 9 percentage points. Fine-grained, per-category analysis further underscores the effectiveness of reinforcement learning in enhancing LLM safety within the MCP framework. Code and data are available at: https://github.com/PorUna-byte/MCP-Guard/tree/master

</details>


### [100] [Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs](https://arxiv.org/abs/2511.05919)
*Alina Fastowski,Bardh Prenkaj,Yuxiao Li,Gjergji Kasneci*

Main category: cs.CR

TL;DR: 本文研究了LLMs在面对对抗性中间人攻击时的脆弱性，并提出了一个有效的防御机制。


<details>
  <summary>Details</summary>
Motivation: LLMs现在是信息检索的重要组成部分。它们作为问答聊天机器人的角色引发了重大担忧，因为它们显示出对对抗性中间人（MitM）攻击的易受性。

Method: 我们提出了Xmera，这是一个理论基础的MitM框架，用于对LLM的事实记忆进行首次系统的攻击评估。通过在三个封闭书本和基于事实的QA设置中扰动输入，我们削弱了响应的正确性并评估了生成过程的不确定性。为了提供一种简单的防御机制，我们训练了随机森林分类器来区分受攻击和未受攻击的查询。

Result: 意想不到的是，基于简单指令的攻击报告了最高的成功率（高达约85.3%），同时对于错误回答的问题也有较高的不确定性。我们通过训练随机森林分类器来区分受攻击和未受攻击的查询，平均AUC高达约96%。

Conclusion: 我们相信，提醒用户对从黑盒和可能被篡改的LLM中获得的答案保持警惕是用户网络空间安全的第一道防线。

Abstract: LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to "victim" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.

</details>


### [101] [Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting](https://arxiv.org/abs/2511.06197)
*Dilli Prasad Sharma,Liang Xue,Xiaowei Sun,Xiaodong Lin,Pulei Xiong*

Main category: cs.CR

TL;DR: 本文提出了一种基于SHAP的对抗检测模型，以提高物联网入侵检测系统的鲁棒性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的快速普及，物联网网络面临着越来越复杂的网络安全威胁，包括针对基于人工智能和机器学习的入侵检测系统的对抗攻击。这些攻击旨在逃避检测、诱导误分类并系统地破坏安全防御的可靠性和完整性。

Method: 本文使用SHapley Additive exPlanations (SHAP) 的DeepExplainer从网络流量特征中提取归因指纹，使IDS能够可靠地区分干净输入和对抗性扰动输入。

Result: 在标准物联网基准数据集上的评估表明，该模型在检测对抗攻击方面显著优于最先进的方法。此外，该方法还提高了模型的透明度和可解释性。

Conclusion: 本文提出了一种新的对抗检测模型，通过SHAP-based指纹技术增强物联网入侵检测系统（IDS）对对抗攻击的鲁棒性。该方法在标准物联网基准数据集上显著优于最先进的方法，并提高了模型的透明度和可解释性，从而增强了对IDS的信任。

Abstract: The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [102] [When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms](https://arxiv.org/abs/2511.06448)
*Qibing Ren,Zhijie Zheng,Jiaxuan Guo,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.MA

TL;DR: 本文研究了大规模多智能体系统中的集体金融欺诈风险，提出了一个基准来模拟金融欺诈场景，并分析了影响欺诈成功的关键因素，同时提出了一系列缓解策略。


<details>
  <summary>Details</summary>
Motivation: 我们研究了在由大型语言模型（LLM）代理驱动的大规模多智能体系统中集体金融欺诈的风险，旨在了解代理如何协作进行欺诈行为，以及这种协作如何放大风险。

Method: 我们提出了MultiAgentFraudBench，这是一个大规模基准，用于模拟基于现实在线交互的金融欺诈场景。我们分析了影响欺诈成功的关键因素，并提出了缓解策略，包括添加内容级警告、使用LLM作为监控者以及通过信息共享增强群体韧性。

Result: 我们分析了影响欺诈成功的关键因素，包括交互深度、活动水平和细粒度协作失败模式，并提出了多种缓解策略。此外，我们发现恶意代理能够适应环境干预。

Conclusion: 我们的研究突显了多智能体金融欺诈的现实风险，并提出了实用的缓解措施。然而，我们观察到恶意智能体可以适应环境干预。

Abstract: In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [103] [FPGA or GPU? Analyzing comparative research for application-specific guidance](https://arxiv.org/abs/2511.06565)
*Arnab A Purkayastha,Jay Tharwani,Shobhit Aggarwal*

Main category: cs.AR

TL;DR: 本文分析了FPGA和GPU在不同应用中的优劣，并提供了选择合适加速器的建议。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注性能指标，缺乏对特定应用中哪种加速器最有益的深入见解，因此本文旨在填补这一空白。

Method: 本文通过分类审查的研究和分析关键性能指标，比较了FPGA和GPU在不同应用中的表现。

Result: 本文识别了FPGA和GPU在不同应用中的优势和适用场景，并提供了实用的建议。

Conclusion: 本文通过综合分析各种研究文章，强调了FPGA和GPU在特定应用中的优势、局限性和理想使用场景，并提供了可操作的建议，帮助研究人员和从业者在性能、能效和可编程性之间做出权衡。

Abstract: The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [104] [Factual and Musical Evaluation Metrics for Music Language Models](https://arxiv.org/abs/2511.05550)
*Daniel Chenyu Lin,Michael Freeman,John Thickstun*

Main category: cs.SD

TL;DR: 本文提出了一种新的评估指标和框架，以更准确地衡量音乐语言模型的性能，并可推广到其他领域。


<details>
  <summary>Details</summary>
Motivation: 当前的评估指标无法衡量音乐语言模型回答的正确性，因此需要一种新的评估方法。

Method: 本文提出了一个更好的通用音乐语言模型评估指标和一个事实性评估框架，以测量音乐语言模型回答的正确性。

Result: 本文提出的评估指标和框架能够更准确地衡量音乐语言模型的性能，并且可以推广到其他开放问答领域。

Conclusion: 本文提出了一种更通用的音乐语言模型评估指标和一个事实性评估框架，以量化音乐语言模型回答的正确性，并且该框架可以推广到其他开放问答领域。

Abstract: Music language models (Music LMs), like vision language models, leverage multimodal representations to answer natural language queries about musical audio recordings. Although Music LMs are reportedly improving, we find that current evaluations fail to capture whether their answers are correct. Specifically, for all Music LMs that we examine, widely-used evaluation metrics such as BLEU, METEOR, and BERTScore fail to measure anything beyond linguistic fluency of the model's responses. To measure the true performance of Music LMs, we propose (1) a better general-purpose evaluation metric for Music LMs adapted to the music domain and (2) a factual evaluation framework to quantify the correctness of a Music LM's responses. Our framework is agnostic to the modality of the question-answering model and could be generalized to quantify performance in other open-ended question-answering domains. We use open datasets in our experiments and will release all code on publication.

</details>


### [105] [Persian Musical Instruments Classification Using Polyphonic Data Augmentation](https://arxiv.org/abs/2511.05717)
*Diba Hadi Esfangereh,Mohammad Hossein Sameti,Sepehr Harfi Moridani,Leili Javidpour,Mahdieh Soleymani Baghshah*

Main category: cs.SD

TL;DR: 本文介绍了针对波斯乐器分类的新数据集和文化上知情的数据增强策略，并通过MERT模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究非西方传统，特别是波斯音乐的乐器分类仍然有限，因此我们引入了一个新的数据集，并提出了文化上知情的数据增强策略。

Method: 我们提出了一个文化上知情的数据增强策略，该策略从单音样本生成现实的多音混合，并使用MERT模型（Music undERstanding with large-scale self-supervised Training）进行评估。

Result: 在真实世界的多音波斯音乐中，所提出的方法在ROC-AUC（0.795）方面表现最佳，突显了音调和时间连贯性的互补优势。

Conclusion: 这些结果展示了文化基础增强在稳健的波斯乐器识别中的有效性，并为文化包容的音乐信息检索和多样化音乐生成系统提供了基础。

Abstract: Musical instrument classification is essential for music information retrieval (MIR) and generative music systems. However, research on non-Western traditions, particularly Persian music, remains limited. We address this gap by introducing a new dataset of isolated recordings covering seven traditional Persian instruments, two common but originally non-Persian instruments (i.e., violin, piano), and vocals. We propose a culturally informed data augmentation strategy that generates realistic polyphonic mixtures from monophonic samples. Using the MERT model (Music undERstanding with large-scale self-supervised Training) with a classification head, we evaluate our approach with out-of-distribution data which was obtained by manually labeling segments of traditional songs. On real-world polyphonic Persian music, the proposed method yielded the best ROC-AUC (0.795), highlighting complementary benefits of tonal and temporal coherence. These results demonstrate the effectiveness of culturally grounded augmentation for robust Persian instrument recognition and provide a foundation for culturally inclusive MIR and diverse music generation systems.

</details>


### [106] [ELEGANCE: Efficient LLM Guidance for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2511.06288)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: ELEGANCE框架通过引入语言知识显著提升了音频-视觉目标说话人提取模型的性能。


<details>
  <summary>Details</summary>
Motivation: 人类在提取目标语音时会利用语言知识，如句法约束、下一个词预测和对话先验知识，而现有的AV-TSE模型主要依赖视觉线索。

Method: 提出ELEGANCE框架，利用大型语言模型的语言知识，通过输出语言约束、中间语言预测和输入语言先验三种策略来增强音频-视觉目标说话人提取模型。

Result: 在两个AV-TSE基线模型上进行了广泛的实验，使用RoBERTa、Qwen3-0.6B和Qwen3-4B，结果表明该方法在视觉线索受损、未见过的语言、目标说话人切换、干扰说话人增多和域外测试集等挑战性场景中表现优异。

Conclusion: ELEGANCE框架通过三种指导策略有效结合了语言知识，显著提高了在挑战性场景下的AV-TSE模型性能。

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on visual cues from the target speaker. However, humans also leverage linguistic knowledge, such as syntactic constraints, next word prediction, and prior knowledge of conversation, to extract target speech. Inspired by this observation, we propose ELEGANCE, a novel framework that incorporates linguistic knowledge from large language models (LLMs) into AV-TSE models through three distinct guidance strategies: output linguistic constraints, intermediate linguistic prediction, and input linguistic prior. Comprehensive experiments with RoBERTa, Qwen3-0.6B, and Qwen3-4B on two AV-TSE backbones demon- strate the effectiveness of our approach. Significant improvements are observed in challenging scenarios, including visual cue impaired, unseen languages, target speaker switches, increased interfering speakers, and out-of-domain test set. Demo page: https://alexwxwu.github.io/ELEGANCE/.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [107] [The Role of High-Performance GPU Resources in Large Language Model Based Radiology Imaging Diagnosis](https://arxiv.org/abs/2509.16328)
*Jyun-Ping Kao*

Main category: q-bio.TO

TL;DR: This paper reviews modern GPU architectures and their impact on radiology tasks using LLMs, discussing challenges and future developments.


<details>
  <summary>Details</summary>
Motivation: to enable the deployment of large-language models (LLMs) in clinical radiology practice with high diagnostic accuracy and low inference latency.

Method: reviewing modern GPU architectures and their performance metrics, discussing practical challenges, and anticipating next-generation features.

Result: empirical studies indicate that using appropriate GPU resources can reduce inference time and improve throughput, and next-generation GPU features will further enable on-premise and federated radiology AI.

Conclusion:  advancing GPU infrastructure is essential for safe, efficient LLM-based radiology diagnostics.

Abstract: Large-language models (LLMs) are rapidly being applied to radiology, enabling automated image interpretation and report generation tasks. Their deployment in clinical practice requires both high diagnostic accuracy and low inference latency, which in turn demands powerful hardware. High-performance graphical processing units (GPUs) provide the necessary compute and memory throughput to run large LLMs on imaging data. We review modern GPU architectures (e.g. NVIDIA A100/H100, AMD Instinct MI250X/MI300) and key performance metrics of floating-point throughput, memory bandwidth, VRAM capacity. We show how these hardware capabilities affect radiology tasks: for example, generating reports or detecting findings on CheXpert and MIMIC-CXR images is computationally intensive and benefits from GPU parallelism and tensor-core acceleration. Empirical studies indicate that using appropriate GPU resources can reduce inference time and improve throughput. We discuss practical challenges including privacy, deployment, cost, power and optimization strategies: mixed-precision, quantization, compression, and multi-GPU scaling. Finally, we anticipate that next-generation features (8-bit tensor cores, enhanced interconnect) will further enable on-premise and federated radiology AI. Advancing GPU infrastructure is essential for safe, efficient LLM-based radiology diagnostics.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [108] [The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation](https://arxiv.org/abs/2511.05903)
*Zhengyuan Liu,Stella Xin Yin,Bryan Chen Zhengyu Tan,Roy Ka-Wei Lee,Guimei Liu,Dion Hoe-Lian Goh,Wenya Wang,Nancy F. Chen*

Main category: cs.CY

TL;DR: 本文提出了一种基于记忆的学生模拟框架，通过分层记忆机制和结构化知识表示来融入发展轨迹，并整合元认知过程和人格特质以丰富个体学习者档案。实验结果表明，该方法能有效反映知识发展的渐进性和学生面临的典型困难。


<details>
  <summary>Details</summary>
Motivation: 现有的学生模拟方法在教育应用中存在显著局限，主要关注单一学习体验，未能考虑学生的逐步知识构建和技能演变。此外，大型语言模型优化为产生直接且准确的响应，难以代表真实学习者的不完整理解和发育限制。

Method: 本文引入了一种基于记忆的学生模拟框架，通过分层记忆机制和结构化知识表示来融入发展轨迹，并整合元认知过程和人格特质以丰富个体学习者档案。

Result: 实验结果表明，我们的方法可以有效地反映知识发展的渐进性和学生面临的典型困难，提供更准确的学习过程表示。

Conclusion: 本文提出了一种基于记忆的学生模拟框架，能够有效反映知识发展的渐进性以及学生面临的典型困难，为学习过程提供了更准确的表示。

Abstract: User simulation is important for developing and evaluating human-centered AI, yet current student simulation in educational applications has significant limitations. Existing approaches focus on single learning experiences and do not account for students' gradual knowledge construction and evolving skill sets. Moreover, large language models are optimized to produce direct and accurate responses, making it challenging to represent the incomplete understanding and developmental constraints that characterize real learners. In this paper, we introduce a novel framework for memory-based student simulation that incorporates developmental trajectories through a hierarchical memory mechanism with structured knowledge representation. The framework also integrates metacognitive processes and personality traits to enrich the individual learner profiling, through dynamical consolidation of both cognitive development and personal learning characteristics. In practice, we implement a curriculum-aligned simulator grounded on the Next Generation Science Standards. Experimental results show that our approach can effectively reflect the gradual nature of knowledge development and the characteristic difficulties students face, providing a more accurate representation of learning processes.

</details>


### [109] [Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI](https://arxiv.org/abs/2511.06078)
*Luis Marquez-Carpintero,Alberto Lopez-Sellers,Miguel Cazorla*

Main category: cs.CY

TL;DR: 本文综述了利用大型语言模型（LLM）模拟学生行为的研究，探讨了其在教育环境中的应用潜力及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探讨LLM在教育环境中模拟学生行为的潜力，并评估其对课程开发、教学评估和教师培训的影响。

Method: 该论文进行了一项主题性综述，分析了利用LLM模拟学生行为的实证和方法学研究。

Result: 论文总结了LLM-based代理在模仿学习者原型、响应教学输入以及在多代理课堂场景中互动的能力。同时，论文还讨论了这些系统对教育领域的潜在影响。

Conclusion: 该论文指出，尽管LLM在自然语言生成和情境灵活性方面优于基于规则的系统，但仍然存在算法偏见、评估可靠性以及与教育目标对齐等问题。论文识别了现有技术和方法上的差距，并提出了未来将生成式AI整合到自适应学习系统和教学设计中的研究方向。

Abstract: Simulated Students offer a valuable methodological framework for evaluating pedagogical approaches and modelling diverse learner profiles, tasks which are otherwise challenging to undertake systematically in real-world settings. Recent research has increasingly focused on developing such simulated agents to capture a range of learning styles, cognitive development pathways, and social behaviours. Among contemporary simulation techniques, the integration of large language models (LLMs) into educational research has emerged as a particularly versatile and scalable paradigm. LLMs afford a high degree of linguistic realism and behavioural adaptability, enabling agents to approximate cognitive processes and engage in contextually appropriate pedagogical dialogues. This paper presents a thematic review of empirical and methodological studies utilising LLMs to simulate student behaviour across educational environments. We synthesise current evidence on the capacity of LLM-based agents to emulate learner archetypes, respond to instructional inputs, and interact within multi-agent classroom scenarios. Furthermore, we examine the implications of such systems for curriculum development, instructional evaluation, and teacher training. While LLMs surpass rule-based systems in natural language generation and situational flexibility, ongoing concerns persist regarding algorithmic bias, evaluation reliability, and alignment with educational objectives. The review identifies existing technological and methodological gaps and proposes future research directions for integrating generative AI into adaptive learning systems and instructional design.

</details>


### [110] [Large Language Models Develop Novel Social Biases Through Adaptive Exploration](https://arxiv.org/abs/2511.06148)
*Addison J. Wu,Ryan Liu,Xuechunzi Bai,Thomas L. Griffiths*

Main category: cs.CY

TL;DR: 本文研究了大型语言模型在没有内在差异的情况下可能自发产生新的社会偏见，并发现显式激励探索是减少这种偏见的有效方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型 (LLMs) 被纳入赋予它们做出实际决策能力的框架，确保它们无偏见变得越来越重要。

Method: 使用心理学文献中的范式，我们展示了 LLMs 可以自发地对人工人口群体发展出新颖的社会偏见，即使不存在固有的差异。我们检查了一系列针对模型输入、问题结构和显式引导的干预措施。

Result: LLMs 会自发地发展出新的社会偏见，导致高度分层的任务分配，这比由人类参与者分配的不公平性更严重，并且在较新和更大的模型中更加严重。通过显式激励探索最有效地减少分层，强调了需要更好的多方面目标来减轻偏见。

Conclusion: 这些结果表明，LLMs 不仅仅是人类社会偏见的被动镜像，而是可以主动从经验中创造新的偏见，这引发了关于这些系统如何随时间塑造社会的紧迫问题。

Abstract: As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.

</details>


### [111] [Place Matters: Comparing LLM Hallucination Rates for Place-Based Legal Queries](https://arxiv.org/abs/2511.06700)
*Damian Curran,Vanessa Sporne,Lea Frermann,Jeannie Paterson*

Main category: cs.CY

TL;DR: 本文提出了一种基于功能主义比较法概念的方法，用于比较不同地区大型语言模型的法律知识。通过构建一个包含Reddit用户寻求法律建议的事实场景的数据集，并评估LLM生成的法律摘要中的幻觉率，我们发现幻觉率与地理位置有关，且与模型预测的不确定性有关。


<details>
  <summary>Details</summary>
Motivation: 量化这些差异对于理解用户使用基于LLM的聊天机器人时获得的法律信息质量是否因位置而异至关重要。然而，获得有意义的比较指标是具有挑战性的，因为不同地方的法律机构本身不容易比较。

Method: 我们提出了一种基于功能主义比较法概念的方法，以获得地点间的度量。我们构建了一个数据集，其中包括从Reddit帖子中提取的事实场景，这些帖子来自寻求家庭、住房、就业、犯罪和交通问题法律建议的用户。我们使用这些场景来引出洛杉矶、伦敦和悉尼的LLM相关的法律摘要。这些摘要通常是对立法条款的总结，并被手动评估是否存在幻觉。

Result: 我们展示了领先的封闭源代码LLM对法律信息的幻觉率与地点之间存在显著关联。这表明这些模型提供的法律解决方案质量在地理上并不均匀分布。此外，我们发现幻觉率与模型多次采样时多数响应的频率之间存在强烈的负相关，这表明可以使用该频率作为模型预测法律事实不确定性的度量。

Conclusion: 我们的研究表明，大型语言模型在不同地区提供的法律信息质量并不均匀分布，这可能影响用户获得的法律解决方案的质量。此外，我们发现幻觉率与模型多次采样时多数响应的频率之间存在强烈负相关，这表明可以使用该频率作为模型预测法律事实不确定性的度量。

Abstract: How do we make a meaningful comparison of a large language model's knowledge of the law in one place compared to another? Quantifying these differences is critical to understanding if the quality of the legal information obtained by users of LLM-based chatbots varies depending on their location. However, obtaining meaningful comparative metrics is challenging because legal institutions in different places are not themselves easily comparable. In this work we propose a methodology to obtain place-to-place metrics based on the comparative law concept of functionalism. We construct a dataset of factual scenarios drawn from Reddit posts by users seeking legal advice for family, housing, employment, crime and traffic issues. We use these to elicit a summary of a law from the LLM relevant to each scenario in Los Angeles, London and Sydney. These summaries, typically of a legislative provision, are manually evaluated for hallucinations. We show that the rate of hallucination of legal information by leading closed-source LLMs is significantly associated with place. This suggests that the quality of legal solutions provided by these models is not evenly distributed across geography. Additionally, we show a strong negative correlation between hallucination rate and the frequency of the majority response when the LLM is sampled multiple times, suggesting a measure of uncertainty of model predictions of legal facts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [112] [Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs](https://arxiv.org/abs/2511.05766)
*Felipe Valencia-Clavijo*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型中的锚定偏差，通过行为分析、归因方法和统一评分来评估其影响。结果表明，锚定偏差在大型模型中是稳健且可解释的，但在小型模型中表现不一，这提示了在应用领域中的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究表明大型语言模型表现出锚定偏差，但大多数证据依赖于表面输出，内部机制和归因贡献未被探索。因此，本文旨在深入研究大型语言模型中的锚定偏差，揭示其内部机制和影响。

Method: 本研究通过三种贡献推进了对大型语言模型中锚定偏差的研究：(1) 基于对数概率的行为分析，显示锚点会改变整个输出分布，并控制训练数据污染；(2) 对结构化提示字段进行精确的Shapley值归因，以量化锚点对模型对数概率的影响；(3) 一个统一的锚定偏差敏感度评分，整合了行为和归因证据，涵盖六个开源模型。

Result: 结果揭示了Gemma-2B、Phi-2和Llama-2-7B中存在稳健的锚定效应，归因信号表明锚点影响重新加权。较小的模型如GPT-2、Falcon-RW-1B和GPT-Neo-125M显示出变异性，表明规模可能调节敏感性。然而，归因效应在不同提示设计中变化，强调了将大型语言模型视为人类替代品的脆弱性。

Conclusion: 研究结果表明，大型语言模型中的锚定偏差是稳健、可测量和可解释的，同时突显了在应用领域中的风险。更广泛地说，该框架连接了行为科学、大型语言模型安全性和可解释性，为评估其他认知偏差提供了可重复的路径。

Abstract: Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.

</details>


### [113] [DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis](https://arxiv.org/abs/2511.05810)
*Bowen Xu,Xinyue Zeng,Jiazhen Hu,Tuo Wang,Adithya Kulkarni*

Main category: cs.AI

TL;DR: 本文介绍了DiagnoLLM，一个结合贝叶斯去卷积、eQTL引导的深度学习和基于LLM的叙述生成的混合框架，用于实现可解释的疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 构建值得信赖的临床AI系统不仅需要准确的预测，还需要透明、生物基础的解释。

Method: 我们提出了DiagnoLLM，这是一个混合框架，结合了贝叶斯去卷积、eQTL引导的深度学习和基于LLM的叙述生成，以实现可解释的疾病诊断。

Result: 通过结合GP-unmix和eQTL分析，神经分类器在阿尔茨海默病（AD）检测中实现了88.0%的准确率。人类评估确认这些报告是准确、可行且适合医生和患者的。

Conclusion: 我们的研究显示，当作为后处理推理器部署时，大型语言模型（LLMs）可以在混合诊断流程中作为有效的通信者。

Abstract: Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.

</details>


### [114] [ScRPO: From Errors to Insights](https://arxiv.org/abs/2511.06065)
*Lianrui Li,Dakuan Lu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出了一种名为Self-correction Relative Policy Optimization (ScRPO) 的新颖强化学习框架，旨在通过自我反思和错误纠正来增强大型语言模型在具有挑战性的数学问题上的表现。该方法包括两个阶段：(1) 试错学习阶段：使用GRPO训练模型并收集错误答案及其相应的问题到错误池中；(2) 自我纠正学习阶段：引导模型反思之前答案错误的原因。在多个数学推理基准测试中进行了广泛的实验，包括AIME、AMC、Olympiad、MATH-500、GSM8k，使用Deepseek-Distill-Qwen-1.5B和Deepseek-Distill-Qwen-7B。实验结果表明，ScRPO始终优于几种后训练方法。这些发现表明ScRPO是使语言模型在有限外部反馈下自我改进的有前景的范式，为更可靠和强大的AI系统铺平了道路。


<details>
  <summary>Details</summary>
Motivation: To enhance large language models on challenging mathematical problems by leveraging self-reflection and error correction.

Method: ScRPO consists of two stages: (1) Trial-and-error learning stage: training the model with GRPO and collecting incorrect answers along with their corresponding questions in an error pool; (2) Self-correction learning stage: guiding the model to reflect on why its previous answers were wrong.

Result: Extensive experiments across multiple math reasoning benchmarks, including AIME, AMC, Olympiad, MATH-500, GSM8k, using Deepseek-Distill-Qwen-1.5B and Deepseek-Distill-Qwen-7B. The experimental results demonstrate that ScRPO consistently outperforms several post-training methods.

Conclusion: ScRPO is a promising paradigm for enabling language models to self-improve on difficult tasks with limited external feedback, paving the way toward more reliable and capable AI systems.

Abstract: We propose Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to enhance large language models on challenging mathematical problems by leveraging self-reflection and error correction. Our approach consists of two stages: (1) Trial-and-error learning stage: training the model with GRPO and collecting incorrect answers along with their corresponding questions in an error pool; (2) Self-correction learning stage: guiding the model to reflect on why its previous answers were wrong. Extensive experiments across multiple math reasoning benchmarks, including AIME, AMC, Olympiad, MATH-500, GSM8k, using Deepseek-Distill-Qwen-1.5B and Deepseek-Distill-Qwen-7B. The experimental results demonstrate that ScRPO consistently outperforms several post-training methods. These findings highlight ScRPO as a promising paradigm for enabling language models to self-improve on difficult tasks with limited external feedback, paving the way toward more reliable and capable AI systems.

</details>


### [115] [Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles](https://arxiv.org/abs/2511.06160)
*Fatima Jahara,Mark Dredze,Sharon Levy*

Main category: cs.AI

TL;DR: 本文提出了PRIME评估框架，通过逻辑谜题检测LLM中的社会偏见，发现模型在刻板印象关联时表现更准确。


<details>
  <summary>Details</summary>
Motivation: 当前的评估基准无法检测到复杂逻辑推理任务中更微妙的社会偏见，因此需要一种新的评估方法。

Method: 引入了一个新的评估框架PRIME（Puzzle Reasoning for Implicit Biases in Model Evaluation），使用逻辑网格谜题系统地探测社会刻板印象对LLM逻辑推理和决策的影响。

Result: 模型在解决方案符合刻板印象关联时，推理准确性更高。

Conclusion: PRIME有助于诊断和量化LLM在演绎推理中持续存在的社会偏见，这对于公平性至关重要。

Abstract: While recent safety guardrails effectively suppress overtly biased outputs, subtler forms of social bias emerge during complex logical reasoning tasks that evade current evaluation benchmarks. To fill this gap, we introduce a new evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model Evaluation), that uses logic grid puzzles to systematically probe the influence of social stereotypes on logical reasoning and decision making in LLMs. Our use of logic puzzles enables automatic generation and verification, as well as variability in complexity and biased settings. PRIME includes stereotypical, anti-stereotypical, and neutral puzzle variants generated from a shared puzzle structure, allowing for controlled and fine-grained comparisons. We evaluate multiple model families across puzzle sizes and test the effectiveness of prompt-based mitigation strategies. Focusing our experiments on gender stereotypes, our findings highlight that models consistently reason more accurately when solutions align with stereotypical associations. This demonstrates the significance of PRIME for diagnosing and quantifying social biases perpetuated in the deductive reasoning of LLMs, where fairness is critical.

</details>


### [116] [Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads](https://arxiv.org/abs/2511.06209)
*Jingwei Ni,Ekaterina Fadeeva,Tianyi Wu,Mubashara Akhtar,Jiaheng Zhang,Elliott Ash,Markus Leippold,Timothy Baldwin,See-Kiong Ng,Artem Shelmanov,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: 本文提出了一种基于数据驱动不确定性分数的轻量级步骤级推理验证方法，该方法利用LLM的内部状态来估计推理步骤的不确定性，并在多个领域中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的验证方法，如Process Reward Models (PRMs)，要么计算成本高，要么局限于特定领域，或者需要大规模的人类或模型生成的注释。因此，我们需要一种轻量级的替代方法来进行步骤级推理验证。

Method: 我们提出了一种基于数据驱动不确定性分数的轻量级步骤级推理验证方法。我们训练了基于Transformer的不确定性量化头（UHeads），使用冻结LLM的内部状态来估计其生成过程中的推理步骤的不确定性。

Result: UHeads在多个领域（包括数学、规划和一般知识问答）中表现良好，甚至超过了比它们大810倍的PRMs。

Conclusion: 我们的研究结果表明，LLM的内部状态编码了它们的不确定性，并可以作为推理验证的可靠信号，为可扩展和通用的内省LLM提供了有希望的方向。

Abstract: Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.

</details>


### [117] [Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B](https://arxiv.org/abs/2511.06221)
*Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin Zhang*

Main category: cs.AI

TL;DR: VibeThinker-1.5B展示了小型模型可以通过创新方法实现与大型模型相当的推理能力，并显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 挑战当前认为小型模型本质上缺乏稳健推理能力的共识，探索一种不依赖于扩大模型参数来提升能力的方法。

Method: 通过Spectrum-to-Signal Principle (SSP)开发了1.5B参数的VibeThinker-1.5B模型，该框架包括Two-Stage Diversity-Exploring Distillation (SFT)和MaxEnt-Guided Policy Optimization (RL)。

Result: VibeThinker-1.5B在三个数学基准测试中优于DeepSeek R1（400x更大），在LiveCodeBench V6上表现优于Magistral Medium和其基础模型。

Conclusion: 小型模型可以实现与大型模型相当的推理能力，从而大幅降低训练和推理成本，推动先进人工智能研究的民主化。

Abstract: Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.

</details>


### [118] [LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation](https://arxiv.org/abs/2511.06346)
*Liya Zhu,Peizhuang Cong,Aowei Ji,Wenya Wu,Jiani Hou,Chunjie Wu,Xiang Gao,Jingkai Liu,Zhou Huan,Xuelei Sun,Yang Yang,Jianpeng Jiao,Liang Hu,Xinjie Chen,Jiashuo Liu,Jingzhe Ding,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: LPFQA is a new benchmark for evaluating large language models, focusing on long-tail knowledge and real-world applications.


<details>
  <summary>Details</summary>
Motivation: Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To bridge this gap, we propose LPFQA.

Method: LPFQA is a long-tail knowledge-based benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. It introduces four key innovations: fine-grained evaluation dimensions, hierarchical difficulty structure, authentic professional scenario modeling, and interdisciplinary knowledge integration.

Result: We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks.

Conclusion: LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.

Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question answering, and professional applications; however, their true capabilities remain difficult to evaluate using existing benchmarks. Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To bridge this gap, we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. LPFQA introduces four key innovations: fine-grained evaluation dimensions that target knowledge depth, reasoning, terminology comprehension, and contextual analysis; a hierarchical difficulty structure that ensures semantic clarity and unique answers; authentic professional scenario modeling with realistic user personas; and interdisciplinary knowledge integration across diverse domains. We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks. LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.

</details>


### [119] [MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models](https://arxiv.org/abs/2511.06419)
*Jingyu Hu,Shu Yang,Xilin Gong,Hongming Wang,Weiru Liu,Di Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为MONICA的新框架，用于在模型推理过程中实时监控和减轻附和行为，实验结果表明该方法在多个数据集和模型上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要关注根据最终答案进行判断并纠正，而没有理解附和行为如何在推理过程中发展。因此，需要一种能够监控和减轻模型推理过程中的附和行为的方法。

Method: MONICA是一种基于监控的校准框架，它在模型推理过程中实时监控附和行为，并在分数超过预定义阈值时动态抑制附和行为。

Result: MONICA方法在12个数据集和3个大型推理模型上进行了广泛的实验，结果表明该方法有效减少了模型在中间推理步骤和最终答案中的附和行为，从而实现了稳健的性能提升。

Conclusion: MONICA方法在12个数据集和3个大型推理模型上进行了广泛的实验，结果表明该方法有效减少了模型在中间推理步骤和最终答案中的附和行为，从而实现了稳健的性能提升。

Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users' incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.

</details>


### [120] [Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis](https://arxiv.org/abs/2511.06437)
*Abhishek More,Anthony Zhang,Nicole Bonilla,Ashvik Vivekan,Kevin Zhu,Parham Sharafoleslami,Maheep Chaudhary*

Main category: cs.AI

TL;DR: 本文提出了一种新的解码策略EDTR，通过结合拓扑分析和基于Dirichlet的不确定性量化来衡量大型语言模型在多个推理路径上的置信度。EDTR在多个基准测试中表现优异，提供了更可靠的置信度估计，有助于安全部署大型语言模型。


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought (CoT)提示使大型语言模型能够解决复杂问题，但安全部署这些模型需要可靠的置信度估计，而现有方法在错误预测上存在校准不足和严重过度自信的问题。

Method: 我们提出了增强的Dirichlet和拓扑风险（EDTR），这是一种结合拓扑分析与基于Dirichlet的不确定性量化的新解码策略，以测量LLM在多个推理路径上的置信度。EDTR将每个CoT视为高维空间中的向量，并提取八个拓扑风险特征，捕捉推理分布的几何结构。

Result: EDTR在四个不同的推理基准测试中优于三种最先进的校准方法，平均ECE为0.287，综合得分为0.672，比竞争方法好41%。在AIME和GSM8K等领域，EDTR实现了完美的准确率和出色的校准效果，而基线方法在这些领域表现出严重的过度自信。

Conclusion: 我们的工作提供了一个几何框架，用于理解和量化多步骤LLM推理中的不确定性，从而在需要校准置信度估计的场景中实现更可靠的部署。

Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.

</details>


### [121] [GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2511.06618)
*Moriya Dechtiar,Daniel Martin Katz,Mari Sundaresan,Sylvain Jaume,Hongming Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的框架，将法律合同转换为结构化的语义图，以简化和自动化合同审查和分析。


<details>
  <summary>Details</summary>
Motivation: 合同是复杂的文档，具有详细的正式结构、显性和隐性依赖关系以及丰富的语义内容。给定这些文档属性，合同起草和手动检查合同已被证明既繁琐又容易出错。

Method: 我们引入了一种基于强化学习的大语言模型（LLM）框架，用于分割和提取合同中的实体和关系。我们的方法GRAPH-GRPO-LEX结合了LLMs和强化学习与组相对策略优化（GRPO）。

Result: 通过应用精心设计的图度量奖励函数，我们展示了自动识别条款之间直接关系的能力，甚至可以发现隐藏的依赖关系。

Conclusion: 我们的方法展示了将合同分析从线性、手动阅读过程转变为易于可视化的图的可能性，为合同检查奠定了基础，类似于现在在软件工程中实践的做法。

Abstract: Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.

</details>


### [122] [MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks](https://arxiv.org/abs/2511.07107)
*Liang Shan,Kaicheng Shen,Wen Wu,Zhenyu Ying,Chaochao Lu,Guangze Ye,Liang He*

Main category: cs.AI

TL;DR: 本文提出了MENTOR框架，用于发现和减轻LLM在领域任务中的隐性风险。该框架引入了元认知自我评估工具，发布了一个包含9000个风险查询的数据集，并通过动态生成规则知识图谱和激活引导来提高LLM的值对齐和安全性。实验结果表明，MENTOR在降低语义攻击成功率方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前对齐努力主要针对显性风险，如偏见、仇恨言论和暴力，但往往无法解决更深层次的、领域特定的隐性风险，并且缺乏适用于不同专业领域的灵活、通用框架。

Method: 提出了一种基于元认知的自我进化框架MENTOR，引入了新颖的元认知自我评估工具，使LLM能够通过视角转换和后果思考来反思潜在的价值错配。同时发布了包含9000个风险查询的数据集，以增强领域特定风险识别。根据元认知反思的结果，动态生成补充规则知识图谱，扩展预定义的静态规则树，并在推理过程中使用激活引导来指导LLM遵循规则。

Result: 实验结果表明，MENTOR框架在三个垂直领域中的防御性测试中显著降低了语义攻击的成功率，实现了LLM隐性风险缓解的新水平。此外，元认知评估不仅与基线人类评估者高度一致，还提供了更全面和深入的LLM价值对齐分析。

Conclusion: MENTOR框架在防御性测试中显示出有效性，显著降低了语义攻击的成功率，为LLM的隐性风险缓解提供了新的水平。此外，元认知评估不仅与基线人类评估者高度一致，还提供了更全面和深入的LLM价值对齐分析。

Abstract: Ensuring the safety and value alignment of large language models (LLMs) is critical for their deployment. Current alignment efforts primarily target explicit risks such as bias, hate speech, and violence. However, they often fail to address deeper, domain-specific implicit risks and lack a flexible, generalizable framework applicable across diverse specialized fields. Hence, we proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering and mitigating implicit Risks in LLMs on Domain Tasks. To address the limitations of labor-intensive human evaluation, we introduce a novel metacognitive self-assessment tool. This enables LLMs to reflect on potential value misalignments in their responses using strategies like perspective-taking and consequential thinking. We also release a supporting dataset of 9,000 risk queries spanning education, finance, and management to enhance domain-specific risk identification. Subsequently, based on the outcomes of metacognitive reflection, the framework dynamically generates supplementary rule knowledge graphs that extend predefined static rule trees. This enables models to actively apply validated rules to future similar challenges, establishing a continuous self-evolution cycle that enhances generalization by reducing maintenance costs and inflexibility of static systems. Finally, we employ activation steering during inference to guide LLMs in following the rules, a cost-effective method to robustly enhance enforcement across diverse contexts. Experimental results show MENTOR's effectiveness: In defensive testing across three vertical domains, the framework substantially reduces semantic attack success rates, enabling a new level of implicit risk mitigation for LLMs. Furthermore, metacognitive assessment not only aligns closely with baseline human evaluators but also delivers more thorough and insightful analysis of LLMs value alignment.

</details>


### [123] [IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction](https://arxiv.org/abs/2511.07327)
*Guoxin Chen,Zile Qiao,Xuanzhong Chen,Donglei Yu,Haotian Xu,Wayne Xin Zhao,Ruihua Song,Wenbiao Yin,Huifeng Yin,Liwen Zhang,Kuan Li,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.AI

TL;DR: IterResearch 是一种新的迭代深度研究范式，通过战略工作区重构和效率感知策略优化，显著提升了长时间任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于单一上下文范式，导致上下文窒息和噪声污染，限制了它们在长时间任务中的有效性。

Method: IterResearch 将长时间研究重新表述为具有战略工作区重构的马尔可夫决策过程，并开发了效率感知策略优化 (EAPO) 框架，通过几何奖励折扣激励高效的探索，并通过自适应下采样实现稳定的分布式训练。

Result: IterResearch 在六个基准测试中平均提高了14.5pp，并缩小了与前沿专有系统的差距。其范式表现出前所未有的交互扩展性，扩展到2048次交互，并在长时间任务中将前沿模型提高了高达19.2pp。

Conclusion: IterResearch 是一种适用于长时间推理的多功能解决方案，既可以作为训练好的代理，也可以作为前沿模型的提示范式。

Abstract: Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\% to 42.5\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.

</details>


### [124] [DigiData: Training and Evaluating General-Purpose Mobile Control Agents](https://arxiv.org/abs/2511.07413)
*Yuxuan Sun,Manchen Wang,Shengyi Qian,William R. Wong,Eric Gan,Pierluca D'Oro,Alejandro Castillejo Munoz,Sneha Silwal,Pedro Matias,Nitin Kamra,Satwik Kottur,Nick Raines,Xuanyi Zhao,Joy Chen,Joseph Greer,Andrea Madotto,Allen Bolourchi,James Valori,Kevin Carlberg,Karl Ridgeway,Joseph Tighe*

Main category: cs.AI

TL;DR: 本文介绍了DigiData数据集和DigiData-Bench基准，以推动移动控制代理的发展，并提出了新的评估方法以更准确地评估代理性能。


<details>
  <summary>Details</summary>
Motivation: 为了加速这一转变，两个基本构建块是必不可少的：高质量的数据集，使代理能够实现复杂和人类相关的目标，以及稳健的评估方法，使研究人员和从业者能够快速提高代理性能。

Method: 我们介绍了DigiData，一个大规模、高质量、多样化、多模态的数据集，用于训练移动控制代理。此外，我们还提出了DigiData-Bench，一个用于在现实世界复杂任务上评估移动控制代理的基准。

Result: 我们展示了常用的步骤准确性指标在可靠评估移动控制代理方面存在不足，并提出了动态评估协议和AI驱动的评估作为代理评估的严格替代方案。

Conclusion: 我们的贡献旨在显著推动移动控制代理的发展，为更直观和有效的用户-设备交互铺平道路。

Abstract: AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [125] [Adaptive Testing for Segmenting Watermarked Texts From Language Models](https://arxiv.org/abs/2511.06645)
*Xingchi Li,Xiaochi Liu,Guanxun Li*

Main category: stat.ML

TL;DR: 本文提出了一种新的框架，用于检测和分割LLM生成的水印文本，该方法无需精确估计提示概率，具有有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（如GPT-4和Claude 3.5）的迅速采用，需要区分LLM生成文本和人类写作内容，以减轻虚假信息的传播和在教育中的滥用。水印技术是一种有前景的方法，可以在LLM生成的文本中嵌入微妙的统计信号以实现可靠识别。

Method: 本文首先推广了之前研究中的基于似然的LLM检测方法，引入了一种灵活的加权公式，并进一步将其适应到逆变换抽样方法。然后，将这种自适应检测策略扩展到解决更具有挑战性的任务，即分割给定文本为水印和非水印子字符串。

Result: 广泛的数值实验表明，所提出的方法在准确分割包含混合水印和非水印内容的文本方面是有效且稳健的。

Conclusion: 本文提出的框架在准确分割包含水印和非水印内容的文本方面表现出有效性和鲁棒性。

Abstract: The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.

</details>


### [126] [Language Generation with Infinite Contamination](https://arxiv.org/abs/2511.07417)
*Anay Mehrotra,Grigoris Velegkas,Xifan Yu,Felix Zhou*

Main category: stat.ML

TL;DR: 本文研究了在污染数据下语言生成的可行性，发现当污染比例趋于零时生成是可行的，并引入课程学习模型以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究假设数据是完美的，没有噪声或遗漏，但现实中的数据往往存在污染。因此，本文旨在研究语言生成在污染情况下的可行性，并探索如何提高生成的鲁棒性。

Method: 本文分析了在污染枚举下的语言生成问题，通过理论分析和模型构建来研究生成的鲁棒性，并引入了一种基于课程学习的模型。

Result: 本文证明了在污染样本比例趋于零的情况下，语言生成是可行的。同时，密集生成的鲁棒性比普通生成更低。此外，引入的课程学习模型使得在无限污染情况下也能实现密集生成。

Conclusion: 本文研究了在污染枚举下语言生成的鲁棒性，发现当污染样本的比例趋于零时，语言生成是可行的。此外，还引入了一种超越最坏情况的模型，证明了在污染比例趋于零的情况下，密集生成也是可行的，这表明课程学习可能对从噪声网络数据中学习至关重要。

Abstract: We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.
  Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).
  We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.
  Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [127] [Predicting Oscar-Nominated Screenplays with Sentence Embeddings](https://arxiv.org/abs/2511.05500)
*Francis Gross*

Main category: cs.IR

TL;DR: 本文创建了一个新的数据集Movie-O-Label，用于预测剧本的奥斯卡提名。使用现代文本嵌入和逻辑回归模型进行分类，取得了良好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 奥斯卡提名是电影行业的重要因素，因为它们可以提升可见度和商业成功。本文探讨了是否可以使用现代语言模型预测剧本的奥斯卡提名。

Method: 通过将MovieSum电影剧本集合与精心策划的奥斯卡记录相结合，创建了一个名为Movie-O-Label的新数据集。每个剧本由标题、维基百科摘要和完整剧本表示。长剧本被分割成重叠的文本块，并使用E5句子嵌入模型进行编码。然后使用逻辑回归模型对剧本嵌入进行分类。

Result: 当结合三个与剧本相关的特征输入（剧本、摘要和标题）时，最佳结果得到实现。最佳模型达到了0.66的宏F1分数，精度召回AP为0.445（基线为0.19），ROC-AUC为0.79。

Conclusion: 结果表明，即使基于现代文本嵌入的简单模型也显示出良好的预测性能，可能成为未来研究的起点。

Abstract: Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research.

</details>


### [128] [A Representation Sharpening Framework for Zero Shot Dense Retrieval](https://arxiv.org/abs/2511.05684)
*Dhananjay Ashok,Suraj Nair,Mutasem Al-Darabsah,Choon Hui Teo,Tarun Agarwal,Jonathan May*

Main category: cs.IR

TL;DR: a training-free representation sharpening framework improves zero-shot dense retrieval by differentiating documents, achieving state-of-the-art results and offering an efficient approximation.


<details>
  <summary>Details</summary>
Motivation: zero-shot dense retrieval is challenging because pretrained dense retrievers (DRs) are not trained on the target corpus, leading to struggles in representing semantic differences between similar documents.

Method: introduce a training-free representation sharpening framework that augments a document's representation with information that helps differentiate it from similar documents in the corpus.

Result: the representation sharpening framework proves consistently superior to traditional retrieval on over twenty datasets spanning multiple languages, setting a new state-of-the-art on the BRIGHT benchmark.

Conclusion: representation sharpening is compatible with prior approaches to zero-shot dense retrieval and consistently improves their performance. Additionally, the indexing-time approximation preserves the majority of performance gains without additional inference-time cost.

Abstract: Zero-shot dense retrieval is a challenging setting where a document corpus is provided without relevant queries, necessitating a reliance on pretrained dense retrievers (DRs). However, since these DRs are not trained on the target corpus, they struggle to represent semantic differences between similar documents. To address this failing, we introduce a training-free representation sharpening framework that augments a document's representation with information that helps differentiate it from similar documents in the corpus. On over twenty datasets spanning multiple languages, the representation sharpening framework proves consistently superior to traditional retrieval, setting a new state-of-the-art on the BRIGHT benchmark. We show that representation sharpening is compatible with prior approaches to zero-shot dense retrieval and consistently improves their performance. Finally, we address the performance-cost tradeoff presented by our framework and devise an indexing-time approximation that preserves the majority of our performance gains over traditional retrieval, yet suffers no additional inference-time cost.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [129] [Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents](https://arxiv.org/abs/2511.07176)
*Hanlin Cai,Houtianfu Wang,Haofan Dong,Kai Li,Ozgur B. Akan*

Main category: cs.NI

TL;DR: 本文提出了一种新的模型中毒攻击方法GRMP，展示了其对FL-enabled IoA系统的严重威胁。


<details>
  <summary>Details</summary>
Motivation: FL-enabled IoA系统面临模型中毒攻击的威胁，而现有的基于距离和相似性的防御在大规模参数和异构数据分布下变得脆弱。

Method: 提出了一种基于图表示的模型中毒（GRMP）攻击，利用观察到的良性本地模型构建参数相关图，并扩展了对抗变分图自动编码器来捕捉和重塑高阶依赖关系。

Result: 实验表明，在提出的攻击下系统准确性逐渐下降，且现有防御机制无法检测到该攻击。

Conclusion: GRMP攻击对IoA范式构成了严重威胁，表明需要更有效的防御机制。

Abstract: Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [130] [Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction](https://arxiv.org/abs/2511.05577)
*An Vuong,Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: 本文提出了一种多模态聚合物数据集，用于微调视觉-语言模型，并评估了多模态对预测性能的影响。结果表明，微调模型优于单模态和基线方法，展示了多模态学习的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管一些机器学习方法已经解决了材料科学领域的一些挑战，但仍然缺乏针对聚合物属性预测等广泛任务的基座模型。

Method: 我们提出了一个用于微调视觉-语言模型的多模态聚合物数据集，并通过指令调优对对进行评估。

Result: 我们的微调模型使用LoRA，优于单模态和基线方法，展示了多模态学习的优势。

Conclusion: 我们的微调模型在预测性能上优于单模态和基线方法，展示了多模态学习的优势，并且减少了为不同属性训练单独模型的需求，降低了部署和维护成本。

Abstract: Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.

</details>


### [131] [TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification](https://arxiv.org/abs/2511.05704)
*Pasan Dissanayake,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: TabDistill通过将复杂Transformer模型的知识蒸馏到简单神经网络中，实现了参数效率和在有限训练数据下的高性能。


<details>
  <summary>Details</summary>
Motivation: 在少量训练数据的情况下，Transformer模型虽然表现良好，但其复杂性和参数数量显著增加。为了克服这种权衡，提出了TabDistill框架。

Method: 提出了一种新的策略，将复杂Transformer模型中的预训练知识蒸馏到简单的神经网络中，以有效地对表格数据进行分类。

Result: 蒸馏后的神经网络在相同训练数据下超越了传统的基线模型，并在某些情况下甚至超过了它们所蒸馏的原始Transformer模型。

Conclusion: TabDistill框架在参数效率和有限训练数据下的性能之间取得了平衡，表现出色。

Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.

</details>


### [132] [Adapting Web Agents with Synthetic Supervision](https://arxiv.org/abs/2511.06101)
*Zhaoyang Wang,Yiming Liang,Xuchao Zhang,Qianhui Wu,Siwei Han,Anson Bastos,Rujia Wang,Chetan Bansal,Baolin Peng,Jianfeng Gao,Saravan Rajmohan,Huaxiu Yao*

Main category: cs.LG

TL;DR: 本文提出了SynthAgent，一种通过双重优化任务和轨迹来提高合成数据质量的框架，实验结果证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 网络代理在适应新网站时面临环境特定任务和演示不足的问题。最近的研究探索了合成数据生成来解决这一挑战，但它们存在数据质量问题，其中合成任务包含无法执行的幻觉，收集的轨迹则存在噪声、冗余或错位的动作。

Method: 我们提出了一种完全合成的监督框架SynthAgent，通过任务和轨迹的双重优化来提高合成数据的质量。我们的方法首先通过分类探索网页元素来合成多样化的任务，确保对目标环境的有效覆盖。在轨迹收集过程中，当检测到与实际观察冲突时，我们会优化任务，以减轻幻觉并保持任务一致性。收集后，我们通过全局上下文进行轨迹优化，以减轻潜在的噪声或错位。最后，我们在精炼的合成数据上微调开源网络代理，使其适应目标环境。

Result: 实验结果表明，SynthAgent优于现有的合成数据方法，验证了高质量合成监督的重要性。

Conclusion: 实验结果表明，SynthAgent优于现有的合成数据方法，验证了高质量合成监督的重要性。

Abstract: Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.

</details>


### [133] [Mixtures of SubExperts for Large Language Continual Learning](https://arxiv.org/abs/2511.06237)
*Haeyong Kang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Mixtures of SubExperts (MoSEs)的新型自适应PEFT方法，用于解决持续学习中的灾难性遗忘和模型规模增长问题。通过将稀疏的子专家混合集成到transformer层中，并由任务特定的路由机制控制，MoSEs能够最小化参数干扰和灾难性遗忘，同时实现知识迁移和模型容量的次线性增长。在TRACE基准数据集上的实验表明，MoSEs在知识保留和扩展新任务方面显著优于传统方法，实现了最先进的性能，并且在内存和计算上节省了大量资源。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在持续学习中面临根本性困境：重复使用单一PEFT参数集会导致先前知识的灾难性遗忘；为每个任务分配独立参数会阻止遗忘但导致模型大小线性增长，无法在相关任务之间进行知识迁移。

Method: 提出了一种名为Mixtures of SubExperts (MoSEs)的新型自适应PEFT方法，该方法将稀疏的子专家混合集成到transformer层中，并由任务特定的路由机制控制。

Result: 在TRACE基准数据集上的实验表明，MoSEs在知识保留和扩展新任务方面显著优于传统持续学习方法，实现了最先进的性能，并且在内存和计算上节省了大量资源。

Conclusion: MoSEs显著优于传统的持续学习方法，在知识保留和扩展新任务方面表现出色，实现了最先进的性能，并且在内存和计算上节省了大量资源。

Abstract: Adapting Large Language Models (LLMs) to a continuous stream of tasks is a critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT) methods have become a standard for this, they face a fundamental dilemma in continual learning. Reusing a single set of PEFT parameters for new tasks often leads to catastrophic forgetting of prior knowledge. Conversely, allocating distinct parameters for each task prevents forgetting but results in a linear growth of the model's size and fails to facilitate knowledge transfer between related tasks. To overcome these limitations, we propose a novel adaptive PEFT method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel continual learning framework designed for minimal forgetting and efficient scalability. MoSEs integrate a sparse Mixture of SubExperts into the transformer layers, governed by a task-specific routing mechanism. This architecture allows the model to isolate and protect knowledge within dedicated SubExperts, thereby minimizing parameter interference and catastrophic forgetting. Crucially, the router can adaptively select and combine previously learned sparse parameters for new tasks, enabling effective knowledge transfer while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that MoSEs significantly outperform conventional continual learning approaches in both knowledge retention and scalability to new tasks, achieving state-of-the-art performance with substantial memory and computational savings.

</details>


### [134] [CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models](https://arxiv.org/abs/2511.06430)
*Peyman Hosseini,Ondrej Bohdal,Taha Ceritli,Ignacio Castro,Matthew Purver,Mete Ozay,Umberto Michieli*

Main category: cs.LG

TL;DR: CG-TTRL improves upon TTRL by incorporating context into its two-phase sampling strategy, leading to better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: TTRL's two-phase sampling strategy under-utilizes contextual guidance, which can potentially improve pseudo-label accuracy and regulate exploration.

Method: We propose context-guided TTRL (CG-TTRL), which integrates context dynamically into both sampling phases and introduces a method for efficient context selection for on-device applications.

Result: Evaluations on mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL, with additional 7% relative accuracy improvement and 8% relative improvement after 3 steps of test-time training.

Conclusion: CG-TTRL outperforms TTRL in terms of accuracy and efficiency, demonstrating the effectiveness of integrating context into the two-phase sampling strategy.

Abstract: Test-time Reinforcement Learning (TTRL) has shown promise in adapting foundation models for complex tasks at test-time, resulting in large performance improvements. TTRL leverages an elegant two-phase sampling strategy: first, multi-sampling derives a pseudo-label via majority voting, while subsequent downsampling and reward-based fine-tuning encourages the model to explore and learn diverse valid solutions, with the pseudo-label modulating the reward signal. Meanwhile, in-context learning has been widely explored at inference time and demonstrated the ability to enhance model performance without weight updates. However, TTRL's two-phase sampling strategy under-utilizes contextual guidance, which can potentially improve pseudo-label accuracy in the initial exploitation phase while regulating exploration in the second. To address this, we propose context-guided TTRL (CG-TTRL), integrating context dynamically into both sampling phases and propose a method for efficient context selection for on-device applications. Our evaluations on mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g. additional 7% relative accuracy improvement over TTRL), while boosting efficiency by obtaining strong performance after only a few steps of test-time training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).

</details>


### [135] [The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models](https://arxiv.org/abs/2511.07237)
*Xin Qiu,Junlong Tong,Yirong Sun,Yunpu Ma,Xiaoyu Shen*

Main category: cs.LG

TL;DR: 本文发现了时间序列模型中的缩放悖论，并提出了一种通过保留少数功能重要的层来提高模型性能和效率的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列模型通常假设模型规模和数据量越大性能越好，但实际中发现更大的模型并不一定表现更好，因此需要研究这一现象的原因并寻找解决方案。

Method: 通过分析内部表示，识别出少数功能重要的层，并提出一种自动识别和保留这些主导层的方法。

Result: 在保留21%参数的情况下，实现了准确率提升和推理速度加快。在多个最先进的模型上验证了该方法的通用性，保留少于30%的层在超过95%的任务中达到了可比或更好的准确性。

Conclusion: 本文发现时间序列模型存在一个缩放悖论，即更大的模型并不一定表现更好。通过分析内部表示，发现只有少数层是功能重要的，而大多数层是冗余的、未充分利用的，甚至会干扰训练。基于这一发现，提出了一种自动识别和保留这些主导层的方法，在保留21%参数的情况下，实现了准确率提升和推理速度加快。该方法在多个最先进的模型上验证了其通用性。

Abstract: Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \textit{\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\% of the parameters achieves up to a 12\% accuracy improvement and a 2.7$\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\% of layers achieves comparable or superior accuracy in over 95\% of tasks.

</details>


### [136] [Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection](https://arxiv.org/abs/2511.07364)
*Vaibhav Mavi,Shubh Jaroria,Weiqi Sun*

Main category: cs.LG

TL;DR: 本研究扩展了自我评估技术到多步骤任务，发现逐步评估比整体评分更有效，提高了LLM系统的可信度和故障检测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的可靠性和故障检测对于其在高风险、多步骤推理任务中的部署至关重要。然而，大多数方法集中在单步输出上，忽略了多步骤推理的挑战。

Method: 我们扩展了自我评估技术到多步骤任务，测试了两种直观的方法：整体评分和逐步评分。

Result: 使用两个多步骤基准数据集，我们显示逐步评估通常优于整体评分，在检测潜在错误方面有高达15%的AUC-ROC相对增加。

Conclusion: 我们的研究结果表明，自我评估的LLM系统在复杂推理中提供了有意义的置信度估计，提高了其可信度，并为故障检测提供了一个实用的框架。

Abstract: Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [137] [Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale](https://arxiv.org/abs/2511.05705)
*David Acuna,Chao-Han Huck Yang,Yuntian Deng,Jaehun Jung,Ximing Lu,Prithviraj Ammanabrolu,Hyunwoo Kim,Yuan-Hong Liao,Yejin Choi*

Main category: cs.CV

TL;DR: 本文介绍了一种新的推理数据生成框架，用于构建大规模、以视觉为中心的推理数据集，并展示了该数据集在多个任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理的进步主要依赖于未公开的数据集和专有数据合成方法，因此需要一种系统的方法来构建大规模、以视觉为中心的推理数据集。

Method: 我们引入了一个新的推理数据生成框架，通过两个阶段（规模和复杂性）生成高质量的合成视觉中心问题。

Result: 微调Qwen2.5-VL-7B模型在所有评估的视觉中心基准测试中表现优于所有开放数据基线，并且甚至超过了MiMo-VL-7B-RL等强大的封闭数据模型。

Conclusion: 我们的数据在多个视觉中心基准测试中表现出色，甚至超过了某些封闭数据模型。此外，我们的数据在文本推理和音频推理任务中也表现良好，表明其有效性。

Abstract: Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.

</details>


### [138] [Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective](https://arxiv.org/abs/2511.06284)
*Bing Wang,Ximing Li,Yanjun Wang,Changchun Li,Lin Yuanbo Wu,Buyu Wang,Shengsheng Wang*

Main category: cs.CV

TL;DR: RETSIMD 是一种基于文本生成图像并结合图神经网络的多模态虚假信息检测方法，能够有效提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统多模态虚假信息检测中，文本可能比图像更具有信息量，因为文本通常描述整个事件或故事，而图像仅展示部分场景。因此，我们提出 RETSIMD 方法来更好地利用文本信息。

Method: RETSIMD 方法通过将文本分割成多个段落，并使用预训练的文本到图像生成器生成对应的图像，同时引入了两个辅助目标来增强文本-图像和图像-标签之间的互信息，并利用图神经网络生成融合特征。

Result: 实验结果表明，RETSIMD 方法在多模态虚假信息检测任务中表现出色，验证了其有效性。

Conclusion: RETSIMD 是一种有效的多模态虚假信息检测方法，通过文本生成图像并利用图神经网络融合特征，提高了检测效果。

Abstract: Multimodal Misinformation Detection (MMD) refers to the task of detecting social media posts involving misinformation, where the post often contains text and image modalities. However, by observing the MMD posts, we hold that the text modality may be much more informative than the image modality because the text generally describes the whole event/story of the current post but the image often presents partial scenes only. Our preliminary empirical results indicate that the image modality exactly contributes less to MMD. Upon this idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that each text can be divided into several segments, and each text segment describes a partial scene that can be presented by an image. Accordingly, we split the text into a sequence of segments, and feed these segments into a pre-trained text-to-image generator to augment a sequence of images. We further incorporate two auxiliary objectives concerning text-image and image-label mutual information, and further post-train the generator over an auxiliary text-to-image generation benchmark dataset. Additionally, we propose a graph structure by defining three heuristic relationships between images, and use a graph neural network to generate the fused features. Extensive empirical results validate the effectiveness of RETSIMD.

</details>


### [139] [HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment](https://arxiv.org/abs/2511.06653)
*Ruijia Wu,Ping Chen,Fei Shen,Shaoan Zhao,Qiang Hui,Huanlin Gao,Ting Lu,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: HiMo-CLIP is a framework that enhances CLIP-style models by introducing hierarchical decomposition and monotonicity-aware contrastive loss to improve image-text retrieval.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of contrastive vision-language models in capturing semantic hierarchy and monotonicity in text.

Method: HiMo-CLIP introduces two key components: a hierarchical decomposition (HiDe) module and a monotonicity-aware contrastive loss (MoLo).

Result: Experiments on multiple image-text retrieval benchmarks show that HiMo-CLIP consistently outperforms strong baselines.

Conclusion: HiMo-CLIP consistently outperforms strong baselines, particularly under long or compositional descriptions.

Abstract: Contrastive vision-language models like CLIP have achieved impressive results in image-text retrieval by aligning image and text representations in a shared embedding space. However, these models often treat text as flat sequences, limiting their ability to handle complex, compositional, and long-form descriptions. In particular, they fail to capture two essential properties of language: semantic hierarchy, which reflects the multi-level compositional structure of text, and semantic monotonicity, where richer descriptions should result in stronger alignment with visual content.To address these limitations, we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style models without modifying the encoder architecture. HiMo-CLIP introduces two key components: a hierarchical decomposition (HiDe) module that extracts latent semantic components from long-form text via in-batch PCA, enabling flexible, batch-aware alignment across different semantic granularities, and a monotonicity-aware contrastive loss (MoLo) that jointly aligns global and component-level representations, encouraging the model to internalize semantic ordering and alignment strength as a function of textual completeness.These components work in concert to produce structured, cognitively-aligned cross-modal representations. Experiments on multiple image-text retrieval benchmarks show that HiMo-CLIP consistently outperforms strong baselines, particularly under long or compositional descriptions. The code is available at https://github.com/UnicomAI/HiMo-CLIP.

</details>


### [140] [Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View](https://arxiv.org/abs/2511.06722)
*Jianyu Qi,Ding Zou,Wenrui Yan,Rui Ma,Jiaxu Li,Zhijie Zheng,Zhiguo Yang,Rongchang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于难度感知采样的后训练框架，以提高多模态大语言模型在链式思维推理中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练范式往往忽视了两个关键方面：(1)缺乏可量化的难度度量，无法战略性地筛选样本进行后训练优化。(2)次优的后训练范式未能联合优化感知和推理能力。

Method: 我们提出了两种新的难度感知采样策略：渐进式图像语义掩码（PISM）通过系统的图像退化来量化样本难度，而跨模态注意力平衡（CMAB）则通过注意力分布分析来评估跨模态交互复杂性。利用这些指标，我们设计了一个包含GRPO-only和SFT+GRPO混合训练范式的分层训练框架。

Result: 我们在六个基准数据集上评估了这些方法，实验结果表明，将GRPO应用于难度分层的样本比传统的SFT+GRPO流程表现出一致的优势。

Conclusion: 实验表明，将GRPO应用于难度分层的样本可以优于传统的SFT+GRPO管道，这表明战略性数据采样可以消除对监督微调的需求，同时提高模型准确性。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have spurred significant progress in Chain-of-Thought (CoT) reasoning. Building on the success of Deepseek-R1, researchers extended multimodal reasoning to post-training paradigms based on reinforcement learning (RL), focusing predominantly on mathematical datasets. However, existing post-training paradigms tend to neglect two critical aspects: (1) The lack of quantifiable difficulty metrics capable of strategically screening samples for post-training optimization. (2) Suboptimal post-training paradigms that fail to jointly optimize perception and reasoning capabilities. To address this gap, we propose two novel difficulty-aware sampling strategies: Progressive Image Semantic Masking (PISM) quantifies sample hardness through systematic image degradation, while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction complexity via attention distribution analysis. Leveraging these metrics, we design a hierarchical training framework that incorporates both GRPO-only and SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark datasets. Experiments demonstrate consistent superiority of GRPO applied to difficulty-stratified samples compared to conventional SFT+GRPO pipelines, indicating that strategic data sampling can obviate the need for supervised fine-tuning while improving model accuracy. Our code will be released at https://github.com/qijianyu277/DifficultySampling.

</details>


### [141] [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](https://arxiv.org/abs/2511.07403)
*Hunar Batra,Haoqin Tu,Hardy Chen,Yuanze Lin,Cihang Xie,Ronald Clark*

Main category: cs.CV

TL;DR: 本文介绍了SpatialThinker，这是一种通过强化学习训练的3D感知多模态大语言模型，能够整合结构化的空间定位和多步骤推理。该模型在空间理解和现实世界VQA基准测试中表现出色，优于监督微调和稀疏RL基线。


<details>
  <summary>Details</summary>
Motivation: 现有的空间MLLM通常依赖于显式的3D输入或架构特定的修改，并且受到大规模数据集或稀疏监督的限制。为了克服这些限制，我们引入了SpatialThinker。

Method: 引入了SpatialThinker，这是一个通过强化学习训练的3D感知MLLM，以整合结构化的空间定位和多步骤推理。模型通过构建任务相关对象和空间关系的场景图，并通过密集的空间奖励进行推理来模拟人类的空间感知。

Result: SpatialThinker-7B在空间理解和现实世界VQA基准测试中优于监督微调和稀疏RL基线，其基础模型增益几乎是稀疏RL的两倍，并超过了GPT-4o。

Conclusion: 这些结果展示了将空间监督与奖励对齐的推理相结合的有效性，有助于在数据有限的情况下实现稳健的3D空间理解，并推动MLLMs向人类水平的视觉推理发展。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [142] [Approximating the Mathematical Structure of Psychodynamics](https://arxiv.org/abs/2511.05580)
*Bryce-Allen Bagley,Navin Khoshnan*

Main category: q-bio.NC

TL;DR: 本文提出了一种基于过程理论的图示框架，用于形式化人类心理动力学，并解释了该框架与心理分析、神经技术、AI对齐、AI代理在自主谈判中的人类代表、开发类人AI系统和其他AI安全方面的认知过程核心概念之间的联系。


<details>
  <summary>Details</summary>
Motivation: 为了能够对心理学和精神病学医学以及AI安全的认知方面进行精确的定量研究，需要一种数学公式，既数学上精确又对多个领域的专家同样易于理解。

Method: 本文使用过程理论的图示框架来形式化人类心理动力学，并描述其关键属性，同时解释图示表示与心理分析、神经技术、AI对齐、AI代理在自主谈判中的人类代表、开发类人AI系统和其他AI安全方面的认知过程核心概念之间的联系。

Result: 本文形式化了人类心理动力学，描述了其关键属性，并解释了图示表示与心理分析、神经技术、AI对齐、AI代理在自主谈判中的人类代表、开发类人AI系统和其他AI安全方面的认知过程核心概念之间的联系。

Conclusion: 本文提出了一个基于过程理论的图示框架来形式化人类心理动力学，并解释了图示表示与心理分析、神经技术、AI对齐、AI代理在自主谈判中的人类代表、开发类人AI系统和其他AI安全方面的认知过程核心概念之间的联系。

Abstract: The complexity of human cognition has meant that psychology makes more use of theory and conceptual models than perhaps any other biomedical field. To enable precise quantitative study of the full breadth of phenomena in psychological and psychiatric medicine as well as cognitive aspects of AI safety, there is a need for a mathematical formulation which is both mathematically precise and equally accessible to experts from numerous fields. In this paper we formalize human psychodynamics via the diagrammatic framework of process theory, describe its key properties, and explain the links between a diagrammatic representation and central concepts in analysis of cognitive processes in contexts such as psychotherapy, neurotechnology, AI alignment, AI agent representation of individuals in autonomous negotiations, developing human-like AI systems, and other aspects of AI safety.

</details>


### [143] [On the Analogy between Human Brain and LLMs: Spotting Key Neurons in Grammar Perception](https://arxiv.org/abs/2511.06519)
*Sanaz Saki Norouzi,Mohammad Masjedi,Pascal Hitzler*

Main category: q-bio.NC

TL;DR: 研究发现大型语言模型中存在一个专注于捕捉词性标签概念的子空间，类似于大脑中不同语法类别由不同神经元处理的现象。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型领域，人们希望使语言学习过程更接近人类。神经科学研究表明，不同的语法类别由大脑中的不同神经元处理，而本文旨在探索大型语言模型是否也表现出类似的行为。

Method: 利用Llama 3识别与不同词性标签预测相关的重要神经元，并使用这些关键神经元的激活模式训练分类器以预测新数据的词性标签。

Result: 通过训练分类器，发现关键神经元的激活模式可以可靠地预测新数据的词性标签，表明大型语言模型中存在一个专注于捕捉词性标签概念的子空间。

Conclusion: 研究结果表明，大型语言模型中存在一个专注于捕捉词性标签概念的子空间，这与神经科学中的病变研究观察到的模式相似。

Abstract: Artificial Neural Networks, the building blocks of AI, were inspired by the human brain's network of neurons. Over the years, these networks have evolved to replicate the complex capabilities of the brain, allowing them to handle tasks such as image and language processing. In the realm of Large Language Models, there has been a keen interest in making the language learning process more akin to that of humans. While neuroscientific research has shown that different grammatical categories are processed by different neurons in the brain, we show that LLMs operate in a similar way. Utilizing Llama 3, we identify the most important neurons associated with the prediction of words belonging to different part-of-speech tags. Using the achieved knowledge, we train a classifier on a dataset, which shows that the activation patterns of these key neurons can reliably predict part-of-speech tags on fresh data. The results suggest the presence of a subspace in LLMs focused on capturing part-of-speech tag concepts, resembling patterns observed in lesion studies of the brain in neuroscience.

</details>

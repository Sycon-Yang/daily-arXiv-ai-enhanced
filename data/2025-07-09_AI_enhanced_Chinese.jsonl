{"id": "2507.05261", "pdf": "https://arxiv.org/pdf/2507.05261", "abs": "https://arxiv.org/abs/2507.05261", "authors": ["Yingtai Xiao", "Yuqing Zhu", "Sirat Samyoun", "Wanrong Zhang", "Jiachen T. Wang", "Jian Du"], "title": "TokenShapley: Token Level Context Attribution with Shapley Value", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684token\u7ea7\u5f52\u56e0\u65b9\u6cd5TokenShapley\uff0c\u901a\u8fc7\u7ed3\u5408Shapley\u503c\u548cKNN\u68c0\u7d22\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u5f52\u56e0\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7528\u6237\u5bfb\u6c42\u54cd\u5e94\u4e2d\u7279\u5b9a\u5173\u952e\u8bcd\uff08\u5982\u6570\u5b57\u3001\u5e74\u4efd\u6216\u540d\u79f0\uff09\u7684\u5f52\u56e0\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ec6\u7c92\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684token\u7ea7\u5f52\u56e0\u65b9\u6cd5TokenShapley\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8eShapley\u503c\u7684\u6570\u636e\u5f52\u56e0\u548c\u53d7KNN\u589e\u5f3aLLM\u6700\u65b0\u8fdb\u5c55\u542f\u53d1\u7684KNN\u68c0\u7d22\u6280\u672f\u3002", "result": "TokenShapley\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5728token\u7ea7\u5f52\u56e0\u65b9\u9762\u5b9e\u73b0\u4e8611-23%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "TokenShapley\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5728token\u7ea7\u5f52\u56e0\u65b9\u9762\u5b9e\u73b0\u4e8611-23%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002"}}
{"id": "2507.05266", "pdf": "https://arxiv.org/pdf/2507.05266", "abs": "https://arxiv.org/abs/2507.05266", "authors": ["Sougata Saha", "Monojit Choudhury"], "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Measuring the generalization ability of Large Language Models (LLMs) is\nchallenging due to data contamination. As models grow and computation becomes\ncheaper, ensuring tasks and test cases are unseen during training phases will\nbecome nearly impossible. We argue that knowledge-retrieval and reasoning tasks\nare not ideal for measuring generalization, as LLMs are not trained for\nspecific tasks. Instead, we propose user behavior prediction, also a key aspect\nof personalization, as a theoretically sound, scalable, and robust alternative.\nWe introduce a novel framework for this approach and test it on movie and music\nrecommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.\nResults align with our framework's predictions, showing GPT-4o outperforms\nGPT-4o-mini and Llama, though all models have much room for improvement,\nespecially Llama.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u4f5c\u4e3a\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5e76\u5728\u7535\u5f71\u548c\u97f3\u4e50\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "motivation": "\u6d4b\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6570\u636e\u6c61\u67d3\u3002\u968f\u7740\u6a21\u578b\u7684\u589e\u957f\u548c\u8ba1\u7b97\u6210\u672c\u7684\u964d\u4f4e\uff0c\u786e\u4fdd\u4efb\u52a1\u548c\u6d4b\u8bd5\u7528\u4f8b\u5728\u8bad\u7ec3\u9636\u6bb5\u672a\u88ab\u770b\u5230\u5c06\u53d8\u5f97\u51e0\u4e4e\u4e0d\u53ef\u80fd\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u6d4b\u8bd5\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u3002", "result": "\u7ed3\u679c\u4e0e\u6211\u4eec\u6846\u67b6\u7684\u9884\u6d4b\u4e00\u81f4\uff0c\u663e\u793aGPT-4o\u7684\u8868\u73b0\u4f18\u4e8eGPT-4o-mini\u548cLlama\u3002", "conclusion": "\u6240\u6709\u6a21\u578b\u90fd\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662fLlama\u3002"}}
{"id": "2507.05271", "pdf": "https://arxiv.org/pdf/2507.05271", "abs": "https://arxiv.org/abs/2507.05271", "authors": ["Mohammad Zia Ur Rehman", "Aditya Shah", "Nagendra Kumar"], "title": "An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks", "categories": ["cs.CL"], "comment": null, "summary": "The global reach of social media has amplified the spread of hateful content,\nincluding implicit sexism, which is often overlooked by conventional detection\nmethods. In this work, we introduce an Adaptive Supervised Contrastive lEarning\nframework for implicit sexism detectioN (ASCEND). A key innovation of our\nmethod is the incorporation of threshold-based contrastive learning: by\ncomputing cosine similarities between embeddings, we selectively treat only\nthose sample pairs as positive if their similarity exceeds a learnable\nthreshold. This mechanism refines the embedding space by robustly pulling\ntogether representations of semantically similar texts while pushing apart\ndissimilar ones, thus reducing false positives and negatives. The final\nclassification is achieved by jointly optimizing a contrastive loss with a\ncross-entropy loss. Textual features are enhanced through a word-level\nattention module. Additionally, we employ sentiment, emotion, and toxicity\nfeatures. Evaluations on the EXIST2021 and MLSC datasets demonstrate that\nASCEND significantly outperforms existing methods, with average Macro F1\nimprovements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting\nits efficacy in capturing the subtle cues of implicit sexist language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aASCEND\u7684\u81ea\u9002\u5e94\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u9690\u6027\u6027\u522b\u6b67\u89c6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u57fa\u4e8e\u9608\u503c\u7684\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u4f18\u5316\u5d4c\u5165\u7a7a\u95f4\uff0c\u7ed3\u5408\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5229\u7528\u6587\u672c\u7279\u5f81\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cASCEND\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u9690\u6027\u6027\u522b\u6b67\u89c6\uff0c\u800c\u793e\u4ea4\u5a92\u4f53\u7684\u5168\u7403\u5f71\u54cd\u529b\u52a0\u5267\u4e86\u4ec7\u6068\u5185\u5bb9\u7684\u4f20\u64ad\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6ASCEND\uff0c\u901a\u8fc7\u57fa\u4e8e\u9608\u503c\u7684\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\uff0c\u9009\u62e9\u6027\u5730\u5c06\u76f8\u4f3c\u5ea6\u8d85\u8fc7\u53ef\u5b66\u4e60\u9608\u503c\u7684\u6837\u672c\u5bf9\u4f5c\u4e3a\u6b63\u4f8b\uff0c\u4ece\u800c\u4f18\u5316\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\u3002", "result": "\u5728EXIST2021\u548cMLSC\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cASCEND\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u9690\u6027\u6027\u522b\u6b67\u89c6\u8bed\u8a00\u7684\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "ASCEND\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u5b8fF1\u63d0\u53479.86%\u300129.63%\u548c32.51%\uff0c\u8868\u660e\u5176\u5728\u6355\u6349\u9690\u6027\u6027\u522b\u6b67\u89c6\u8bed\u8a00\u7684\u7ec6\u5fae\u7ebf\u7d22\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.05285", "pdf": "https://arxiv.org/pdf/2507.05285", "abs": "https://arxiv.org/abs/2507.05285", "authors": ["Miloud Mihoubi", "Meriem Zerkouk", "Belkacem Chikhaoui"], "title": "Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "I.2.7; I.2.1; K.3.1"], "comment": "10 pages, 5 figures, 5 tables. Submitted to the 38th Canadian\n  Conference on Artificial Intelligence (Canadian AI 2025)", "summary": "Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors, and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk profiles. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u5206\u6790\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u6765\u63d0\u9ad8\u8f8d\u5b66\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u51cf\u5c11\u8fdc\u7a0b\u5b66\u4e60\u4e2d\u7684\u8f8d\u5b66\u98ce\u9669\u3002", "motivation": "\u5b66\u751f\u5728\u8fdc\u7a0b\u5b66\u4e60\u4e2d\u7684\u8f8d\u5b66\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5177\u6709\u6df1\u8fdc\u7684\u793e\u4f1a\u548c\u7ecf\u6d4e\u540e\u679c\u3002\u867d\u7136\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5229\u7528\u7ed3\u6784\u5316\u7684\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u548c\u884c\u4e3a\u6570\u636e\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5230\u975e\u7ed3\u6784\u5316\u5b66\u751f\u4e92\u52a8\u4e2d\u7684\u7ec6\u5fae\u60c5\u611f\u548c\u4e0a\u4e0b\u6587\u56e0\u7d20\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e09\u79cd\u534f\u540c\u521b\u65b0\uff1a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7528\u4e8e\u9886\u57df\u7279\u5b9a\u7684\u60c5\u611f\u5206\u6790\uff0c\u63d0\u793a\u5de5\u7a0b\u7528\u4e8e\u89e3\u7801\u5b66\u672f\u538b\u529b\u56e0\u7d20\uff0c\u4ee5\u53ca\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u4ee5\u52a8\u6001\u5bf9\u9f50\u6587\u672c\u3001\u884c\u4e3a\u548c\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u3002", "result": "\u8be5\u6846\u67b6\u57284423\u540d\u5b66\u751f\u7684\u7eb5\u5411\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8689%\u7684\u51c6\u786e\u7387\u548c0.88\u7684F1\u5206\u6570\uff0c\u6bd4\u4f20\u7edf\u6a21\u578b\u63d0\u9ad8\u4e867%\uff0c\u5e76\u5c06\u5047\u9634\u6027\u51cf\u5c11\u4e8621%\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u8fd8\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u4f8b\u5982\u4e3a\u5b64\u7acb\u5b66\u4e60\u8005\u63d0\u4f9b\u5bfc\u5e08\u8ba1\u5212\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u9769\u6027\u7684AI\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u540c\u521b\u65b0\u6765\u91cd\u65b0\u5b9a\u4e49\u8f8d\u5b66\u9884\u6d4b\uff0c\u8be5\u6846\u67b6\u57284423\u540d\u5b66\u751f\u7684\u7eb5\u5411\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8689%\u7684\u51c6\u786e\u7387\u548c0.88\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u6709\u52a9\u4e8e\u51cf\u8f7b\u5168\u7403\u6559\u80b2\u7cfb\u7edf\u4e2d\u7684\u8f8d\u5b66\u98ce\u9669\u3002"}}
{"id": "2507.05319", "pdf": "https://arxiv.org/pdf/2507.05319", "abs": "https://arxiv.org/abs/2507.05319", "authors": ["Cheng Yuan", "Xinkai Rui", "Yongqi Fan", "Yawei Fan", "Boyang Zhong", "Jiacheng Wang", "Weiyan Zhang", "Tong Ruan"], "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review", "categories": ["cs.CL", "cs.AI"], "comment": "ACL Demo 2025", "summary": "Despite the remarkable performance of Large Language Models (LLMs) in\nautomated discharge summary generation, they still suffer from hallucination\nissues, such as generating inaccurate content or fabricating information\nwithout valid sources. In addition, electronic medical records (EMRs) typically\nconsist of long-form data, making it challenging for LLMs to attribute the\ngenerated content to the sources. To address these challenges, we propose LCDS,\na Logic-Controlled Discharge Summary generation system. LCDS constructs a\nsource mapping table by calculating textual similarity between EMRs and\ndischarge summaries to constrain the scope of summarized content. Moreover,\nLCDS incorporates a comprehensive set of logical rules, enabling it to generate\nmore reliable silver discharge summaries tailored to different clinical fields.\nFurthermore, LCDS supports source attribution for generated content, allowing\nexperts to efficiently review, provide feedback, and rectify errors. The\nresulting golden discharge summaries are subsequently recorded for incremental\nfine-tuning of LLMs. Our project and demo video are in the GitHub repository\nhttps://github.com/ycycyc02/LCDS.", "AI": {"tldr": "LCDS is a system that generates reliable discharge summaries by using logical rules and source mapping, and supports source attribution for error correction and LLM fine-tuning.", "motivation": "LLMs suffer from hallucination issues when generating discharge summaries, and EMRs consist of long-form data that makes it difficult for LLMs to attribute generated content to sources.", "method": "LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries, and incorporates a comprehensive set of logical rules to generate reliable discharge summaries tailored to different clinical fields. It also supports source attribution for generated content.", "result": "LCDS generates more reliable discharge summaries with source attribution, allowing experts to review and correct errors. The resulting summaries can be used for incremental fine-tuning of LLMs.", "conclusion": "LCDS can generate more reliable discharge summaries and support source attribution, which helps experts review and correct errors, and the resulting summaries can be used for incremental fine-tuning of LLMs."}}
{"id": "2507.05330", "pdf": "https://arxiv.org/pdf/2507.05330", "abs": "https://arxiv.org/abs/2507.05330", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled new applications\nin e-commerce customer service. However, their capabilities remain constrained\nin complex, multimodal scenarios. We present MindFlow, the first open-source\nmultimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it\nintegrates memory, decision-making, and action modules, and adopts a modular\n\"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via\nonline A/B testing and simulation-based ablation, MindFlow demonstrates\nsubstantial gains in handling complex queries, improving user satisfaction, and\nreducing operational costs, with a 93.53% relative improvement observed in\nreal-world deployments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MindFlow\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u7535\u5b50\u5546\u52a1\u7684\u9996\u4e2a\u5f00\u6e90\u591a\u6a21\u6001LLM\u4ee3\u7406\uff0c\u901a\u8fc7\u6574\u5408\u8bb0\u5fc6\u3001\u51b3\u7b56\u548c\u52a8\u4f5c\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u6a21\u5757\u5316\u7684'MLLM-as-Tool'\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u3001\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7535\u5b50\u5546\u52a1\u5ba2\u670d\u4e2d\u6709\u4e86\u65b0\u7684\u5e94\u7528\uff0c\u4f46\u5b83\u4eec\u5728\u590d\u6742\u3001\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u80fd\u529b\u4ecd\u7136\u53d7\u5230\u9650\u5236\u3002", "method": "\u57fa\u4e8eCoALA\u6846\u67b6\uff0c\u6574\u5408\u4e86\u8bb0\u5fc6\u3001\u51b3\u7b56\u548c\u52a8\u4f5c\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u6a21\u5757\u5316\u7684'MLLM-as-Tool'\u7b56\u7565\u8fdb\u884c\u6709\u6548\u7684\u89c6\u89c9\u6587\u672c\u63a8\u7406\u3002", "result": "\u901a\u8fc7\u5728\u7ebfA/B\u6d4b\u8bd5\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u6d88\u878d\u5b9e\u9a8c\u8bc4\u4f30\uff0cMindFlow\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u3001\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "MindFlow\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u3001\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u89c2\u5bdf\u5230\u4e8693.53%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002"}}
{"id": "2507.05346", "pdf": "https://arxiv.org/pdf/2507.05346", "abs": "https://arxiv.org/abs/2507.05346", "authors": ["William Fleshman", "Benjamin Van Durme"], "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The proliferation of fine-tuned language model experts for specific tasks and\ndomains signals the need for efficient selection and combination methods. We\npropose LoRA-Augmented Generation (LAG) for leveraging large libraries of\nknowledge and task-specific LoRA adapters. LAG requires no additional training\nor access to data, and efficiently filters, retrieves, and applies experts on a\nper-token and layer basis. We evaluate LAG on various knowledge-intensive\ntasks, achieving superior performance over existing data-free methods. We\nexplore scenarios where additional data is available, demonstrating LAG's\ncompatibility with alternative solutions such as retrieval-augmented generation\n(RAG).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAG\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u9009\u62e9\u548c\u7ec4\u5408\u77e5\u8bc6\u5e93\u4e2d\u7684\u4e13\u5bb6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6570\u636e\u8bbf\u95ee\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u9700\u8981\u77e5\u8bc6\u7684\u4efb\u52a1\uff0c\u5e76\u4e14\u4e0eRAG\u7b49\u5176\u4ed6\u89e3\u51b3\u65b9\u6848\u517c\u5bb9\u3002", "motivation": "\u968f\u7740\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u548c\u9886\u57df\u7684\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u4e13\u5bb6\u7684\u666e\u53ca\uff0c\u9700\u8981\u9ad8\u6548\u7684\u9009\u53d6\u548c\u7ec4\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAG\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u77e5\u8bc6\u5e93\u548c\u4efb\u52a1\u7279\u5b9a\u7684LoRA\u9002\u914d\u5668\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6570\u636e\u8bbf\u95ee\uff0c\u5373\u53ef\u5728\u6bcf\u4e2a\u6807\u8bb0\u548c\u5c42\u7684\u57fa\u7840\u4e0a\u8fc7\u6ee4\u3001\u68c0\u7d22\u548c\u5e94\u7528\u4e13\u5bb6\u3002", "result": "\u5728\u5404\u79cd\u9700\u8981\u77e5\u8bc6\u7684\u4efb\u52a1\u4e0a\u8bc4\u4f30LAG\uff0c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u6570\u636e\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u6709\u989d\u5916\u6570\u636e\u53ef\u7528\u7684\u60c5\u51b5\uff0c\u8bc1\u660e\u4e86LAG\u4e0eRAG\u7b49\u5176\u4ed6\u89e3\u51b3\u65b9\u6848\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "LAG\u53ef\u4ee5\u9ad8\u6548\u5730\u9009\u62e9\u548c\u7ec4\u5408\u77e5\u8bc6\u5e93\u4e2d\u7684\u4e13\u5bb6\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u9700\u8981\u77e5\u8bc6\u7684\u4efb\u52a1\uff0c\u5e76\u4e14\u4e0eRAG\u7b49\u5176\u4ed6\u89e3\u51b3\u65b9\u6848\u517c\u5bb9\u3002"}}
{"id": "2507.05362", "pdf": "https://arxiv.org/pdf/2507.05362", "abs": "https://arxiv.org/abs/2507.05362", "authors": ["Riccardo Alberghi", "Elizaveta Demyanenko", "Luca Biggio", "Luca Saglietti"], "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in natural language processing highlight two key factors for\nimproving reasoning in large language models (LLMs): (i) allocating more\ntest-time compute tends to help on harder problems but often introduces\nredundancy in the reasoning trace, and (ii) compute is most effective when\nreasoning is systematic and incremental, forming structured chains of thought\n(CoTs) akin to human problem-solving. To study these factors in isolation, we\nintroduce a controlled setting based on shortest-path tasks in layered graphs.\nWe train decoder-only transformers on question-trace-answer triples using a\ncustom tokenizer, comparing models trained on optimal bottom-up dynamic\nprogramming traces with those trained on longer, valid traces involving\nbacktracking. Surprisingly, with the same training-token budget, models trained\non inefficient traces generalize better to unseen graphs. This benefit is not\ndue to length alone-injecting arbitrary redundancy into reasoning traces fails\nto help and can even hurt performance. Instead, we find that generalization\ncorrelates with the model's confidence in next-token prediction, suggesting\nthat long, coherent, and locally incremental traces make the training signal\neasier to optimize.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u4ee4\u724c\u9884\u7b97\u4e0b\uff0c\u4f7f\u7528\u4f4e\u6548\u8ffd\u8e2a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u56fe\u4e0a\u6cdb\u5316\u80fd\u529b\u66f4\u597d\uff0c\u8fd9\u4e0e\u6a21\u578b\u5bf9\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u7684\u4fe1\u5fc3\u6709\u5173\u3002", "motivation": "\u7814\u7a76\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\uff1a(i) \u5206\u914d\u66f4\u591a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6709\u52a9\u4e8e\u89e3\u51b3\u66f4\u96be\u7684\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u4f1a\u5f15\u5165\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u5197\u4f59\uff1b(ii) \u8ba1\u7b97\u5728\u7cfb\u7edf\u548c\u6e10\u8fdb\u63a8\u7406\u4e2d\u6700\u4e3a\u6709\u6548\uff0c\u5f62\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u95ee\u9898\u89e3\u51b3\u7684\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\uff08CoTs\uff09\u3002", "method": "\u5728\u57fa\u4e8e\u5206\u5c42\u56fe\u7684\u6700\u77ed\u8def\u5f84\u4efb\u52a1\u4e2d\u5f15\u5165\u4e00\u4e2a\u53d7\u63a7\u8bbe\u7f6e\uff0c\u8bad\u7ec3\u89e3\u7801\u5668\u4ec5\u53d8\u538b\u5668\u6a21\u578b\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49\u5206\u8bcd\u5668\uff0c\u6bd4\u8f83\u5728\u6700\u4f18\u81ea\u5e95\u5411\u4e0a\u52a8\u6001\u89c4\u5212\u8ffd\u8e2a\u548c\u5305\u542b\u56de\u6eaf\u7684\u8f83\u957f\u6709\u6548\u8ffd\u8e2a\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "result": "\u4f7f\u7528\u4f4e\u6548\u8ffd\u8e2a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u56fe\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u8fd9\u4e0e\u6a21\u578b\u5bf9\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u7684\u4fe1\u5fc3\u6709\u5173\u3002", "conclusion": "\u6a21\u578b\u5728\u76f8\u540c\u8bad\u7ec3\u4ee4\u724c\u9884\u7b97\u4e0b\uff0c\u4f7f\u7528\u4f4e\u6548\u8ffd\u8e2a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u56fe\u4e0a\u6cdb\u5316\u80fd\u529b\u66f4\u597d\u3002\u8fd9\u79cd\u4f18\u52bf\u4e0d\u662f\u7531\u4e8e\u957f\u5ea6\u672c\u8eab\uff0c\u800c\u662f\u6a21\u578b\u5bf9\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u7684\u4fe1\u5fc3\u4e0e\u6cdb\u5316\u80fd\u529b\u76f8\u5173\u3002"}}
{"id": "2507.05385", "pdf": "https://arxiv.org/pdf/2507.05385", "abs": "https://arxiv.org/abs/2507.05385", "authors": ["Guanzhong Pan", "Mei Tan", "Hyunji Nam", "Luc\u00eda Langlois", "James Malamut", "Liliana Deonizio", "Dorottya Demszky"], "title": "EduCoder: An Open-Source Annotation System for Education Transcript Data", "categories": ["cs.CL"], "comment": null, "summary": "We introduce EduCoder, a domain-specialized tool designed to support\nutterance-level annotation of educational dialogue. While general-purpose text\nannotation tools for NLP and qualitative research abound, few address the\ncomplexities of coding education dialogue transcripts -- with diverse\nteacher-student and peer interactions. Common challenges include defining\ncodebooks for complex pedagogical features, supporting both open-ended and\ncategorical coding, and contextualizing utterances with external features, such\nas the lesson's purpose and the pedagogical value of the instruction. EduCoder\nis designed to address these challenges by providing a platform for researchers\nand domain experts to collaboratively define complex codebooks based on\nobserved data. It incorporates both categorical and open-ended annotation types\nalong with contextual materials. Additionally, it offers a side-by-side\ncomparison of multiple annotators' responses, allowing comparison and\ncalibration of annotations with others to improve data reliability. The system\nis open-source, with a demo video available.", "AI": {"tldr": "EduCoder \u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u6559\u80b2\u5bf9\u8bdd\u7f16\u7801\u7684\u590d\u6742\u6027\uff0c\u652f\u6301\u534f\u4f5c\u5b9a\u4e49\u4ee3\u7801\u672c\u3001\u5206\u7c7b\u548c\u5f00\u653e\u5f0f\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u591a\u6ce8\u91ca\u8005\u5bf9\u6bd4\u63d0\u9ad8\u6570\u636e\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u6587\u672c\u6ce8\u91ca\u5de5\u5177\u96be\u4ee5\u5e94\u5bf9\u6559\u80b2\u5bf9\u8bdd\u4e2d\u6559\u5e08-\u5b66\u751f\u548c\u540c\u4f34\u4e92\u52a8\u7684\u590d\u6742\u6027\uff0c\u5305\u62ec\u5b9a\u4e49\u4ee3\u7801\u672c\u3001\u652f\u6301\u5f00\u653e\u5f0f\u548c\u5206\u7c7b\u7f16\u7801\u4ee5\u53ca\u5c06\u8bdd\u8bed\u4e0e\u5916\u90e8\u7279\u5f81\u5173\u8054\u7b49\u95ee\u9898\u3002", "method": "EduCoder \u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5e73\u53f0\uff0c\u8ba9\u7814\u7a76\u4eba\u5458\u548c\u9886\u57df\u4e13\u5bb6\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u6570\u636e\u534f\u4f5c\u5b9a\u4e49\u590d\u6742\u7684\u4ee3\u7801\u672c\uff0c\u5e76\u7ed3\u5408\u5206\u7c7b\u548c\u5f00\u653e\u5f0f\u6ce8\u91ca\u7c7b\u578b\u4ee5\u53ca\u4e0a\u4e0b\u6587\u6750\u6599\uff0c\u540c\u65f6\u63d0\u4f9b\u591a\u6ce8\u91ca\u8005\u54cd\u5e94\u7684\u5bf9\u6bd4\u529f\u80fd\u3002", "result": "EduCoder \u89e3\u51b3\u4e86\u6559\u80b2\u5bf9\u8bdd\u7f16\u7801\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u534f\u4f5c\u5b9a\u4e49\u4ee3\u7801\u672c\u7684\u5e73\u53f0\uff0c\u652f\u6301\u591a\u79cd\u6ce8\u91ca\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u591a\u6ce8\u91ca\u8005\u5bf9\u6bd4\u63d0\u9ad8\u6570\u636e\u53ef\u9760\u6027\u3002", "conclusion": "EduCoder \u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u6559\u80b2\u5bf9\u8bdd\u8f6c\u5f55\u672c\u7f16\u7801\u7684\u590d\u6742\u6027\uff0c\u901a\u8fc7\u63d0\u4f9b\u534f\u4f5c\u5b9a\u4e49\u590d\u6742\u4ee3\u7801\u672c\u7684\u5e73\u53f0\uff0c\u652f\u6301\u5206\u7c7b\u548c\u5f00\u653e\u5f0f\u7f16\u7801\uff0c\u5e76\u63d0\u4f9b\u591a\u6ce8\u91ca\u8005\u54cd\u5e94\u7684\u5bf9\u6bd4\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u53ef\u9760\u6027\u3002"}}
{"id": "2507.05387", "pdf": "https://arxiv.org/pdf/2507.05387", "abs": "https://arxiv.org/abs/2507.05387", "authors": ["Ruidi Chang", "Chunyuan Deng", "Hanjie Chen"], "title": "The Generalization Ridge: Information Flow in Natural Language Generation", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based language models have achieved state-of-the-art performance\nin natural language generation (NLG) tasks, yet their internal mechanisms for\nsynthesizing task-relevant information remain insufficiently understood. While\nprior studies suggest that intermediate layers often yield more generalizable\nrepresentations than final layers, how this generalization ability emerges and\npropagates across layers during training remains unclear. To address this gap,\nwe propose InfoRidge, an information-theoretic framework, to characterize how\npredictive information-the mutual information between hidden representations\nand target outputs-varies across depth. Estimating this quantity enables us to\ntrace the flow of task-relevant information throughout the model during\ntraining. Our experiments across various models and datasets reveal a\nconsistent non-monotonic trend: predictive information peaks in upper-middle\nlayers-forming a generalization ridge-before declining in final layers,\nreflecting a transition between generalization and memorization. To further\ninvestigate this phenomenon, we introduce residual scaling\ncoefficients-trainable scalar parameters applied to each residual block-which\nserve as functional probes for assessing the relative importance of individual\ntransformer layers. These coefficients reveal that, under distribution shift,\nmodels downweight final layers and increasingly rely on ridge layers,\nhighlighting their role in generalization. Together, these findings offer new\ninsights into the internal mechanisms of transformers and underscore the\ncritical role of intermediate layers in supporting generalization.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86InfoRidge\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u53d8\u538b\u5668\u4e2d\u9884\u6d4b\u4fe1\u606f\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u7f29\u653e\u7cfb\u6570\u5206\u6790\u4e86\u4e2d\u95f4\u5c42\u5728\u6cdb\u5316\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u4e3a\u4e86\u586b\u8865\u5bf9\u53d8\u538b\u5668\u5185\u90e8\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u7684\u7a7a\u767d\uff0c\u7814\u7a76\u4e86\u4e2d\u95f4\u5c42\u548c\u6700\u7ec8\u5c42\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6cdb\u5316\u80fd\u529b\u7684\u6f14\u53d8\u548c\u4f20\u64ad\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6846\u67b6InfoRidge\uff0c\u7528\u4e8e\u8868\u5f81\u9884\u6d4b\u4fe1\u606f\u5982\u4f55\u968f\u6df1\u5ea6\u53d8\u5316\uff0c\u5e76\u5f15\u5165\u4e86\u6b8b\u5dee\u7f29\u653e\u7cfb\u6570\u4f5c\u4e3a\u8bc4\u4f30\u5355\u4e2aTransformer\u5c42\u76f8\u5bf9\u91cd\u8981\u6027\u7684\u529f\u80fd\u63a2\u9488\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u9884\u6d4b\u4fe1\u606f\u5728\u4e0a\u5c42\u4e2d\u95f4\u5c42\u8fbe\u5230\u5cf0\u503c\uff0c\u5f62\u6210\u4e00\u4e2a\u6cdb\u5316\u810a\uff0c\u7136\u540e\u5728\u6700\u7ec8\u5c42\u4e0b\u964d\uff0c\u53cd\u6620\u4e86\u4ece\u6cdb\u5316\u5230\u8bb0\u5fc6\u7684\u8f6c\u53d8\u3002\u6b8b\u5dee\u7f29\u653e\u7cfb\u6570\u8868\u660e\uff0c\u5728\u5206\u5e03\u53d8\u5316\u4e0b\uff0c\u6a21\u578b\u4f1a\u964d\u4f4e\u6700\u7ec8\u5c42\u7684\u6743\u91cd\u5e76\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u810a\u5c42\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u53d8\u538b\u5668\u7684\u5185\u90e8\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e2d\u95f4\u5c42\u5728\u652f\u6301\u6cdb\u5316\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.05391", "pdf": "https://arxiv.org/pdf/2507.05391", "abs": "https://arxiv.org/abs/2507.05391", "authors": ["Guillem Ram\u00edrez", "Alexandra Birch", "Ivan Titov"], "title": "Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9690\u79c1\u914d\u7f6e\u6587\u4ef6\u4fdd\u62a4\u7528\u6237\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u6570\u636e\u96c6PEEP\u6765\u652f\u6301\u7814\u7a76\uff0c\u4f46\u5b9e\u9a8c\u8868\u660e\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u5728\u9075\u5faa\u8fd9\u4e9b\u6307\u4ee4\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u7528\u6237\u5e0c\u671b\u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u4fdd\u6301\u5bf9\u6570\u636e\u7684\u63a7\u5236\uff0c\u800c\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u5546\u4e1aAPI\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u66b4\u9732\u7ed9\u670d\u52a1\u63d0\u4f9b\u5546\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9\u7528\u6237\u80fd\u591f\u5b9a\u4e49\u9690\u79c1\u504f\u597d\u5e76\u4fdd\u62a4\u6570\u636e\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5176\u4e2d\u672c\u5730\u6a21\u578b\u4f7f\u7528\u9690\u79c1\u914d\u7f6e\u6587\u4ef6\u6765\u91cd\u5199\u67e5\u8be2\uff0c\u4ec5\u9690\u85cf\u7528\u6237\u8ba4\u4e3a\u654f\u611f\u7684\u7ec6\u8282\uff0c\u7136\u540e\u5c06\u67e5\u8be2\u53d1\u9001\u5230\u5916\u90e8\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86PEEP\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u652f\u6301\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u90e8\u5206\u9075\u5faa\u9690\u79c1\u914d\u7f6e\u6587\u4ef6\u7684\u6307\u4ee4\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9690\u79c1\u914d\u7f6e\u6587\u4ef6\u6765\u4fdd\u62a4\u7528\u6237\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4f46\u5b9e\u9a8c\u8868\u660e\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u5728\u9075\u5faa\u8fd9\u4e9b\u6307\u4ee4\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u6765\u66f4\u597d\u5730\u7406\u89e3\u548c\u9075\u5b88\u7528\u6237\u7684\u9690\u79c1\u504f\u597d\u3002"}}
{"id": "2507.05418", "pdf": "https://arxiv.org/pdf/2507.05418", "abs": "https://arxiv.org/abs/2507.05418", "authors": ["Jaedong Hwang", "Kumar Tanmay", "Seok-Jin Lee", "Ayush Agrawal", "Hamid Palangi", "Kumar Ayush", "Ila Fiete", "Paul Pu Liang"], "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual QA, and code generation, yet their multilingual reasoning\ncapabilities in these tasks remain underdeveloped. Especially for low-resource\nlanguages such as Swahili or Thai, LLMs can often misinterpret prompts or\ndefault to reasoning in English. This implicit bias toward high-resource\nlanguages undermines factual accuracy, interpretability, and trust. Current\nmultilingual benchmarks focus only on final answers, overlooking whether models\nactually reason in the target language. To address this gap, we introduce\nGeoFact-X, a geography-based multilingual factual reasoning benchmark with\nannotated reasoning traces in five languages: English, Hindi, Japanese,\nSwahili, and Thai. We further propose BRIDGE, a novel training method that\nguides supervised fine-tuning and test-time reinforcement learning with a\nlanguage-consistency reward to align reasoning with the input language.\nFinally, we develop an automatic evaluation protocol using LLM-as-a-judge to\nassess answer correctness and the quality and language consistency of reasoning\ntraces, enabling nuanced and scalable analysis beyond surface-level metrics.\nOur results show that BRIDGE significantly enhances multilingual reasoning\nfidelity, demonstrating that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/GeoFact-X_BRIDGE", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GeoFact-X\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5730\u7406\u7684\u591a\u8bed\u8a00\u4e8b\u5b9e\u63a8\u7406\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86BRIDGE\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u591a\u8bed\u8a00\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\uff0c\u5ffd\u89c6\u4e86\u6a21\u578b\u662f\u5426\u771f\u7684\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u8fdb\u884c\u63a8\u7406\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5982\u65af\u74e6\u5e0c\u91cc\u8bed\u6216\u6cf0\u8bed\uff0cLLM\u7ecf\u5e38\u8bef\u89e3\u63d0\u793a\u6216\u9ed8\u8ba4\u7528\u82f1\u8bed\u8fdb\u884c\u63a8\u7406\uff0c\u8fd9\u5f71\u54cd\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86BRIDGE\uff0c\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u4e00\u81f4\u6027\u5956\u52b1\u5f15\u5bfc\u76d1\u7763\u5fae\u8c03\u548c\u6d4b\u8bd5\u65f6\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u4f7f\u63a8\u7406\u4e0e\u8f93\u5165\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "BRIDGE\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8bed\u8a00\u63a8\u7406\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5173\u6ce8\u63a8\u7406\u7684\u591a\u8bed\u8a00\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u7a33\u5065\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cBRIDGE\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8bed\u8a00\u63a8\u7406\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5173\u6ce8\u63a8\u7406\u7684\u591a\u8bed\u8a00\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u7a33\u5065\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.05424", "pdf": "https://arxiv.org/pdf/2507.05424", "abs": "https://arxiv.org/abs/2507.05424", "authors": ["Yufei Tao", "Adam Hiatt", "Rahul Seetharaman", "Ameeta Agrawal"], "title": "\"Lost-in-the-Later\": Framework for Quantifying Contextual Grounding in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models are capable of leveraging both contextual and\nparametric knowledge but how they prioritize and integrate these sources\nremains underexplored. We introduce CoPE, a novel evaluation framework that\nsystematically measures contextual knowledge (CK) and parametric knowledge (PK)\nacross models and languages. Using our MultiWikiAtomic dataset in English,\nSpanish, and Danish, we analyze how large language models (LLMs) integrate\ncontext, prioritize information, and incorporate PK in open-ended question\nanswering. Our analysis uncovers a phenomenon we call lost-in-the-later, where\nLLMs tend to overlook or deprioritize information that appears later in a given\ncontext, revealing a strong positional bias that affects contextual grounding.\nWe further find that reasoning models, as well as non-reasoning models prompted\nwith chain-of-thought (CoT), use context even less than non-reasoning models\nwithout CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,\nin particular, results in lower recall and shorter responses, leading to\ndegraded contextual grounding. Based on these insights, we design prompt-based\nmethods to effectively leverage input context. A case study applying CoPE to\nsummarization demonstrates that CK-informed prompting improves factual\ngrounding and reduces hallucination.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CoPE\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u4e0a\u4e0b\u6587\u548c\u53c2\u6570\u77e5\u8bc6\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5bf9\u540e\u7eed\u4fe1\u606f\u6709\u4f4d\u7f6e\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5229\u7528\u4e0a\u4e0b\u6587\u548c\u53c2\u6570\u77e5\u8bc6\uff0c\u4f46\u5b83\u4eec\u5982\u4f55\u4f18\u5148\u548c\u6574\u5408\u8fd9\u4e9b\u6765\u6e90\u4ecd\u4e0d\u660e\u786e\u3002\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u7406\u89e3\u8fd9\u4e00\u70b9\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86CoPE\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u6d4b\u91cf\u4e0d\u540c\u6a21\u578b\u548c\u8bed\u8a00\u4e2d\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff08CK\uff09\u548c\u53c2\u6570\u77e5\u8bc6\uff08PK\uff09\u3002\u6211\u4eec\u4f7f\u7528MultiWikiAtomic\u6570\u636e\u96c6\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u6574\u5408\u4e0a\u4e0b\u6587\u3001\u4f18\u5148\u5904\u7406\u4fe1\u606f\u4ee5\u53ca\u5728\u5f00\u653e\u6027\u95ee\u9898\u56de\u7b54\u4e2d\u878d\u5165PK\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u53d1\u73b0\u4e86\u4e00\u4e2a\u79f0\u4e3a\u201clost-in-the-later\u201d\u7684\u73b0\u8c61\uff0c\u5373\u5927\u578b\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u5ffd\u7565\u6216\u4f4e\u4f30\u540e\u7eed\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u63a8\u7406\u6a21\u578b\u548c\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u7684\u975e\u63a8\u7406\u6a21\u578b\u4f7f\u7528\u4e0a\u4e0b\u6587\u66f4\u5c11\uff0c\u5e76\u672a\u80fd\u7f13\u89e3\u8fd9\u4e00\u73b0\u8c61\u3002CoT\u63d0\u793a\u5bfc\u81f4\u53ec\u56de\u7387\u964d\u4f4e\u548c\u54cd\u5e94\u53d8\u77ed\uff0c\u4ece\u800c\u5f71\u54cd\u4e86\u4e0a\u4e0b\u6587\u7684\u57fa\u7840\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u4fe1\u606f\u65f6\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\uff0c\u5373\u66f4\u503e\u5411\u4e8e\u5ffd\u7565\u6216\u4f4e\u4f30\u540e\u7eed\u4fe1\u606f\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u6765\u6709\u6548\u5229\u7528\u8f93\u5165\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u4e8b\u5b9e\u4f9d\u636e\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002"}}
{"id": "2507.05443", "pdf": "https://arxiv.org/pdf/2507.05443", "abs": "https://arxiv.org/abs/2507.05443", "authors": ["Ashwin Rao", "Sze Yuh Nina Wang", "Kristina Lerman"], "title": "Gendered Divides in Online Discussions about Reproductive Rights", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health\nOrganization marked a turning point in the national debate over reproductive\nrights. While the ideological divide over abortion is well documented, less is\nknown about how gender and local sociopolitical contexts interact to shape\npublic discourse. Drawing on nearly 10 million abortion-related posts on X\n(formerly Twitter) from users with inferred gender, ideology and location, we\nshow that gender significantly moderates abortion attitudes and emotional\nexpression, particularly in conservative regions, and independently of\nideology. This creates a gender gap in abortion attitudes that grows more\npronounced in conservative regions. The leak of the Dobbs draft opinion further\nintensified online engagement, disproportionately mobilizing pro-abortion women\nin areas where access was under threat. These findings reveal that abortion\ndiscourse is not only ideologically polarized but also deeply structured by\ngender and place, highlighting the central role of identity in shaping\npolitical expression during moments of institutional disruption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u8fd11000\u4e07\u6761\u5728X\u4e0a\u7684\u5815\u80ce\u76f8\u5173\u5e16\u5b50\uff0c\u53d1\u73b0\u6027\u522b\u663e\u8457\u8c03\u8282\u5815\u80ce\u6001\u5ea6\u548c\u60c5\u611f\u8868\u8fbe\uff0c\u7279\u522b\u662f\u5728\u4fdd\u5b88\u5730\u533a\uff0c\u5e76\u4e14\u72ec\u7acb\u4e8e\u610f\u8bc6\u5f62\u6001\u3002\u8fbe\u5e03\u65af\u8349\u6848\u610f\u89c1\u7684\u6cc4\u9732\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u5728\u7ebf\u53c2\u4e0e\u5ea6\uff0c\u4e0d\u6210\u6bd4\u4f8b\u5730\u52a8\u5458\u4e86\u5904\u4e8e\u5a01\u80c1\u4e2d\u7684\u5730\u533a\u5185\u7684\u652f\u6301\u5815\u80ce\u5973\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5815\u80ce\u8a00\u8bba\u4e0d\u4ec5\u5728\u610f\u8bc6\u5f62\u6001\u4e0a\u4e24\u6781\u5206\u5316\uff0c\u800c\u4e14\u6df1\u6df1\u53d7\u5230\u6027\u522b\u548c\u5730\u70b9\u7684\u7ed3\u6784\u5f71\u54cd\uff0c\u7a81\u663e\u4e86\u8eab\u4efd\u5728\u5236\u5ea6\u4e2d\u65ad\u65f6\u671f\u5851\u9020\u653f\u6cbb\u8868\u8fbe\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "motivation": "\u4e86\u89e3\u6027\u522b\u548c\u5f53\u5730\u793e\u4f1a\u653f\u6cbb\u80cc\u666f\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u4ee5\u5851\u9020\u516c\u4f17\u8bdd\u8bed\u3002", "method": "\u6211\u4eec\u5206\u6790\u4e86\u8fd11000\u4e07\u6761\u5728X\uff08\u524d\u8eab\u4e3aTwitter\uff09\u4e0a\u7684\u5815\u80ce\u76f8\u5173\u5e16\u5b50\uff0c\u8fd9\u4e9b\u5e16\u5b50\u6765\u81ea\u5177\u6709\u63a8\u65ad\u51fa\u7684\u6027\u522b\u3001\u610f\u8bc6\u5f62\u6001\u548c\u4f4d\u7f6e\u7684\u7528\u6237\u3002", "result": "\u6027\u522b\u663e\u8457\u8c03\u8282\u5815\u80ce\u6001\u5ea6\u548c\u60c5\u611f\u8868\u8fbe\uff0c\u7279\u522b\u662f\u5728\u4fdd\u5b88\u5730\u533a\uff0c\u5e76\u4e14\u72ec\u7acb\u4e8e\u610f\u8bc6\u5f62\u6001\u3002\u8fd9\u5bfc\u81f4\u4e86\u5815\u80ce\u6001\u5ea6\u7684\u6027\u522b\u5dee\u8ddd\uff0c\u5728\u4fdd\u5b88\u5730\u533a\u66f4\u52a0\u660e\u663e\u3002\u8fbe\u5e03\u65af\u8349\u6848\u610f\u89c1\u7684\u6cc4\u9732\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u5728\u7ebf\u53c2\u4e0e\u5ea6\uff0c\u4e0d\u6210\u6bd4\u4f8b\u5730\u52a8\u5458\u4e86\u5904\u4e8e\u5a01\u80c1\u4e2d\u7684\u5730\u533a\u5185\u7684\u652f\u6301\u5815\u80ce\u5973\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5815\u80ce\u8a00\u8bba\u4e0d\u4ec5\u5728\u610f\u8bc6\u5f62\u6001\u4e0a\u4e24\u6781\u5206\u5316\uff0c\u800c\u4e14\u6df1\u6df1\u53d7\u5230\u6027\u522b\u548c\u5730\u70b9\u7684\u7ed3\u6784\u5f71\u54cd\uff0c\u7a81\u663e\u4e86\u8eab\u4efd\u5728\u5236\u5ea6\u4e2d\u65ad\u65f6\u671f\u5851\u9020\u653f\u6cbb\u8868\u8fbe\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002"}}
{"id": "2507.05444", "pdf": "https://arxiv.org/pdf/2507.05444", "abs": "https://arxiv.org/abs/2507.05444", "authors": ["Sana Kang", "Myeongseok Gwon", "Su Young Kwon", "Jaewook Lee", "Andrew Lan", "Bhiksha Raj", "Rita Singh"], "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs", "categories": ["cs.CL"], "comment": null, "summary": "Vocabulary acquisition poses a significant challenge for second-language (L2)\nlearners, especially when learning typologically distant languages such as\nEnglish and Korean, where phonological and structural mismatches complicate\nvocabulary learning. Recently, large language models (LLMs) have been used to\ngenerate keyword mnemonics by leveraging similar keywords from a learner's\nfirst language (L1) to aid in acquiring L2 vocabulary. However, most of this\nresearch has focused on native English speakers learning other languages,\nrather than the reverse. In this paper, we present PhoniTale, a novel\ncross-lingual mnemonic generation system that retrieves L1 keyword sequence\nbased on phonological similarity and uses LLMs to generate mnemonics. We\nevaluate PhoniTale using both automated metrics and human evaluations,\ncomparing its output to mnemonics created by humans and by previous automated\napproaches. To assess practical effectiveness, we also conduct a short-term\nrecall test measuring mnemonic helpfulness. Our findings show that PhoniTale\nperforms comparably to human-authored mnemonics. We also highlight key areas\nfor future improvement in mnemonic quality and methodology.", "AI": {"tldr": "PhoniTale is a new system that generates mnemonics for L2 vocabulary by leveraging phonological similarities between L1 and L2, and it shows promising results compared to human-generated mnemonics.", "motivation": "Vocabulary acquisition is challenging for L2 learners, especially when learning typologically distant languages. Previous research has focused on native English speakers, but this paper addresses the reverse scenario.", "method": "PhoniTale is a cross-lingual mnemonic generation system that retrieves L1 keyword sequences based on phonological similarity and uses LLMs to generate mnemonics.", "result": "PhoniTale was evaluated using automated metrics and human evaluations, and it performed comparably to human-authored mnemonics. A short-term recall test also showed its effectiveness.", "conclusion": "PhoniTale performs comparably to human-authored mnemonics, but there are key areas for future improvement in mnemonic quality and methodology."}}
{"id": "2507.05448", "pdf": "https://arxiv.org/pdf/2507.05448", "abs": "https://arxiv.org/abs/2507.05448", "authors": ["Martin Schuele"], "title": "On the Semantics of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT demonstrated the potential to\nreplicate human language abilities through technology, ranging from text\ngeneration to engaging in conversations. However, it remains controversial to\nwhat extent these systems truly understand language. We examine this issue by\nnarrowing the question down to the semantics of LLMs at the word and sentence\nlevel. By examining the inner workings of LLMs and their generated\nrepresentation of language and by drawing on classical semantic theories by\nFrege and Russell, we get a more nuanced picture of the potential semantic\ncapabilities of LLMs.", "AI": {"tldr": "The paper explores the semantic capabilities of Large Language Models (LLMs) by analyzing their inner workings and drawing on classical semantic theories.", "motivation": "To understand the extent to which LLMs truly understand language, focusing on semantics at the word and sentence level.", "method": "The paper examines the inner workings of LLMs and their generated representation of language, drawing on classical semantic theories by Frege and Russell.", "result": "A more nuanced picture of the potential semantic capabilities of LLMs is obtained.", "conclusion": "LLMs have some semantic capabilities, but their understanding is limited compared to human understanding."}}
{"id": "2507.05455", "pdf": "https://arxiv.org/pdf/2507.05455", "abs": "https://arxiv.org/abs/2507.05455", "authors": ["Ashima Suvarna", "Christina Chance", "Hamid Palangi", "Sophie Hao", "Thomas Hartvigsen", "Saadia Gabriel"], "title": "ModelCitizens:Representing Community Voices in Online Safety", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MODELCITIZENS\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u6bd2\u6027\u8bed\u8a00\u68c0\u6d4b\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eLLaMA\u548cGemma\u7684\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6bd2\u6027\u68c0\u6d4b\u6a21\u578b\u901a\u5e38\u5728\u6ce8\u91ca\u4e2d\u5c06\u591a\u6837\u5316\u7684\u6ce8\u91ca\u8005\u89c2\u70b9\u5408\u5e76\u4e3a\u4e00\u4e2a\u5355\u4e00\u7684\u5730\u9762\u771f\u5b9e\u503c\uff0c\u8fd9\u4f1a\u62b9\u53bb\u91cd\u8981\u7684\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u6bd2\u6027\u6982\u5ff5\uff0c\u5982\u88ab\u91cd\u65b0\u4f7f\u7528\u7684\u8bed\u8a00\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u6355\u6349\u793e\u533a\u89c4\u8303\u548c\u751f\u6d3b\u7ecf\u9a8c\u5bf9\u6bd2\u6027\u611f\u77e5\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86MODELCITIZENS\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u62ec6.8K\u6761\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u548c40K\u6761\u8de8\u4e0d\u540c\u8eab\u4efd\u7fa4\u4f53\u7684\u6bd2\u6027\u6ce8\u91ca\uff0c\u5e76\u901a\u8fc7LLM\u751f\u6210\u7684\u5bf9\u8bdd\u573a\u666f\u589e\u5f3a\u4e86\u8fd9\u4e9b\u5e16\u5b50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u57fa\u4e8eLLaMA\u548cGemma\u7684\u6a21\u578bLLAMACITIZEN-8B\u548cGEMMACITIZEN-12B\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728MODELCITIZENS\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\u3002", "result": "\u6700\u5148\u8fdb\u7684\u6bd2\u6027\u68c0\u6d4b\u5de5\u5177\uff08\u4f8b\u5982OpenAI Moderation API\uff0cGPT-o4-mini\uff09\u5728MODELCITIZENS\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u589e\u5f3a\u4e0a\u4e0b\u6587\u7684\u5e16\u5b50\u4e0a\u8868\u73b0\u66f4\u5dee\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u5e03\u7684LLAMACITIZEN-8B\u548cGEMMACITIZEN-12B\u6a21\u578b\u5728\u5206\u5e03\u5185\u8bc4\u4f30\u4e2d\u6bd4GPT-o4-mini\u9ad8\u51fa5.5%\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u793e\u533a\u77e5\u60c5\u7684\u6ce8\u91ca\u548c\u5efa\u6a21\u5728\u5305\u5bb9\u6027\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.05517", "pdf": "https://arxiv.org/pdf/2507.05517", "abs": "https://arxiv.org/abs/2507.05517", "authors": ["Jean-Philippe Corbeil", "Asma Ben Abacha", "George Michalopoulos", "Phillip Swazinna", "Miguel Del-Agua", "Jerome Tremblay", "Akila Jeeson Daniel", "Cari Bader", "Kevin Cho", "Pooja Krishnan", "Nathan Bodenstab", "Thomas Lin", "Wenxuan Teng", "Francois Beaulieu", "Paul Vozila"], "title": "Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong\nperformance on clinical natural language processing (NLP) tasks across multiple\nmedical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular\nreporting from nurse dictations and medical order extraction from\ndoctor-patient consultations - remain underexplored due to data scarcity and\nsensitivity, despite active industry efforts. Practical solutions to these\nreal-world clinical tasks can significantly reduce the documentation burden on\nhealthcare providers, allowing greater focus on patient care. In this paper, we\ninvestigate these two challenging tasks using private and open-source clinical\ndatasets, evaluating the performance of both open- and closed-weight LLMs, and\nanalyzing their respective strengths and limitations. Furthermore, we propose\nan agentic pipeline for generating realistic, non-sensitive nurse dictations,\nenabling structured extraction of clinical observations. To support further\nresearch in both areas, we release SYNUR and SIMORD, the first open-source\ndatasets for nurse observation extraction and medical order extraction.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u751f\u6210\u771f\u5b9e\u62a4\u58eb\u53e3\u8ff0\u8bb0\u5f55\u7684\u4ee3\u7406\u6d41\u7a0b\uff0c\u5e76\u53d1\u5e03\u4e86\u4e24\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u6027\u548c\u654f\u611f\u6027\uff0c\u7ed3\u6784\u5316\u8868\u683c\u62a5\u544a\u548c\u533b\u7597\u8ba2\u5355\u63d0\u53d6\u4efb\u52a1\u4ecd\u7136\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u4f46\u8fd9\u4e9b\u4efb\u52a1\u7684\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u663e\u8457\u51cf\u8f7b\u533b\u7597\u4fdd\u5065\u63d0\u4f9b\u8005\u7684\u6587\u6863\u8d1f\u62c5\u3002", "method": "\u672c\u6587\u4f7f\u7528\u79c1\u6709\u548c\u5f00\u6e90\u4e34\u5e8a\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u5f00\u653e\u6743\u91cd\u548c\u5c01\u95ed\u6743\u91cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u8868\u683c\u62a5\u544a\u548c\u533b\u7597\u8ba2\u5355\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee3\u7406\u6d41\u7a0b\u6765\u751f\u6210\u771f\u5b9e\u7684\u62a4\u58eb\u53e3\u8ff0\u8bb0\u5f55\u3002", "result": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u4e24\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u751f\u6210\u771f\u5b9e\u4e14\u4e0d\u654f\u611f\u7684\u62a4\u58eb\u53e3\u8ff0\u8bb0\u5f55\u7684\u4ee3\u7406\u6d41\u7a0b\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u4e24\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.05557", "pdf": "https://arxiv.org/pdf/2507.05557", "abs": "https://arxiv.org/abs/2507.05557", "authors": ["Alex ZH Dou", "Zhongwei Wan", "Dongfei Cui", "Xin Wang", "Jing Xiong", "Haokun Lin", "Chaofan Tao", "Shen Yan", "Mi Zhang"], "title": "Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS", "categories": ["cs.CL"], "comment": "Technical Report", "summary": "Test-time scaling has emerged as a promising paradigm in language modeling,\nleveraging additional computational resources at inference time to enhance\nmodel performance. In this work, we introduce R2-LLMs, a novel and versatile\nhierarchical retrieval-augmented reasoning framework designed to improve\ntest-time scaling in large language models (LLMs) without requiring\ndistillation from more advanced models to obtain chain-of-thought (CoT)\ntraining data. R2-LLMs enhances inference-time generalization by integrating\ndual-level retrieval-based in-context learning: (1) At the coarse level, our\napproach extracts abstract templates from complex reasoning problems and\nretrieves similar problem-answer pairs to facilitate high-level in-context\nlearning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs\nefficiently retrieves analogous intermediate solution steps from reference\nmathematical problem datasets, refining step-wise reasoning with the aid of a\nprocess reward model (PRM) for scoring. R2-LLMs is a robust hierarchical\nreasoning-augmentation method that enhances in-context-level reasoning while\nseamlessly integrating with step-level tree search methods. Utilizing PRM, it\nrefines both candidate generation and decision-making for improved reasoning\naccuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO\ndatasets achieve substantial relative improvement with an increase of up to 16%\nusing LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of\nour approach in complex reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86R2-LLMs\uff0c\u8fd9\u662f\u4e00\u79cd\u5206\u5c42\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6846\u67b6\uff0c\u65e8\u5728\u5728\u4e0d\u4f9d\u8d56\u84b8\u998f\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u3002\u901a\u8fc7\u53cc\u7ea7\u57fa\u4e8e\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0cR2-LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "Test-time scaling\u5728\u8bed\u8a00\u5efa\u6a21\u4e2d\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u8303\u5f0f\uff0c\u5229\u7528\u989d\u5916\u7684\u8ba1\u7b97\u8d44\u6e90\u5728\u63a8\u7406\u65f6\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4ece\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u4e2d\u84b8\u998f\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u84b8\u998f\u5373\u53ef\u63d0\u5347\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "method": "R2-LLMs\u662f\u4e00\u79cd\u5206\u5c42\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53cc\u7ea7\u57fa\u4e8e\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1a(1) \u5728\u7c97\u7c92\u5ea6\u7ea7\u522b\uff0c\u4ece\u590d\u6742\u63a8\u7406\u95ee\u9898\u4e2d\u63d0\u53d6\u62bd\u8c61\u6a21\u677f\u5e76\u68c0\u7d22\u76f8\u4f3c\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u4ee5\u4fc3\u8fdb\u9ad8\u5c42\u6b21\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1b(2) \u5728\u7ec6\u7c92\u5ea6\u7ea7\u522b\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u9ad8\u6548\u5730\u4ece\u53c2\u8003\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u7c7b\u4f3c\u4e2d\u95f4\u89e3\u51b3\u65b9\u6848\u6b65\u9aa4\uff0c\u5e76\u5229\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u8fdb\u884c\u8bc4\u5206\u4ee5\u7ec6\u5316\u9010\u6b65\u63a8\u7406\u3002", "result": "\u5728MATH500\u3001GSM8K\u548cOlympiadBench-TO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u4f7f\u7528LLaMA-3.1-8B\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u6700\u9ad8\u63d0\u5347\u4e8616%\u3002", "conclusion": "R2-LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\uff0c\u901a\u8fc7\u4f7f\u7528LLaMA-3.1-8B\u76f8\u6bd4\u57fa\u7ebf\u63d0\u5347\u4e86\u9ad8\u8fbe16%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002"}}
{"id": "2507.05598", "pdf": "https://arxiv.org/pdf/2507.05598", "abs": "https://arxiv.org/abs/2507.05598", "authors": ["Sihyun Park"], "title": "Self-Review Framework for Enhancing Instruction Following Capability of LLM", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Various techniques have been proposed to improve large language models (LLMs)\nadherence to formatting and instruction constraints. One of the most effective\napproaches involves utilizing high-quality data generated by powerful models.\nHowever, such models often fail to fully comply with complex instructions in a\nsingle generation. To address this limitation, iterative revision methods have\nbeen introduced. Nevertheless, as the number of data points and revision\niterations increases, the associated monetary costs grow significantly. As a\nresource-efficient alternative, methods have been proposed that leverage\nhigh-performance evaluation tools to compensate for the limited self-evaluation\ncapabilities of open-source LLMs. However, these approaches often lead to a\ndegradation in output quality due to excessive revision. To overcome these\nchallenges, we propose Re5, a self-evaluation and revision framework designed\nto enhance instruction-following performance while preserving the quality of\nthe generated content. Re5 extracts task and constraint components from user\ninstructions, performs structural evaluations to prevent error accumulation,\nand applies fine-grained constraint-specific content evaluations followed by\nselective revisions. This process ensures precise and quality-preserving\nimprovements. The final high-quality outputs are used for alignment tuning,\nenabling long-term alignment improvements through a data-centric iterative\nrefinement loop. Experimental results demonstrate that Re5 achieves\ninstruction-following performance comparable to models trained on data\ngenerated by GPT-4o-mini, a high-performance model, even with a small amount of\ndata while maintaining response quality with a 64.24%-win rate over the\nnon-revised initial responses. These results validate Re5 as an efficient and\neffective solution for enhancing instruction adherence with minimal external\nsupervision.", "AI": {"tldr": "Re5 \u662f\u4e00\u79cd\u81ea\u8bc4\u4f30\u548c\u4fee\u8ba2\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u6307\u4ee4\u9075\u5faa\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u70b9\u548c\u4fee\u8ba2\u8fed\u4ee3\u589e\u52a0\u65f6\u6210\u672c\u663e\u8457\u4e0a\u5347\uff0c\u800c\u4f7f\u7528\u9ad8\u6027\u80fd\u8bc4\u4f30\u5de5\u5177\u7684\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8f93\u51fa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "Re5 \u63d0\u53d6\u7528\u6237\u6307\u4ee4\u4e2d\u7684\u4efb\u52a1\u548c\u7ea6\u675f\u7ec4\u4ef6\uff0c\u8fdb\u884c\u7ed3\u6784\u8bc4\u4f30\u4ee5\u9632\u6b62\u9519\u8bef\u79ef\u7d2f\uff0c\u5e76\u5e94\u7528\u7ec6\u7c92\u5ea6\u7684\u7279\u5b9a\u7ea6\u675f\u5185\u5bb9\u8bc4\u4f30\uff0c\u968f\u540e\u8fdb\u884c\u9009\u62e9\u6027\u4fee\u8ba2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRe5 \u5728\u5c11\u91cf\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4e0e GPT-4o-mini \u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\u7684\u6307\u4ee4\u9075\u5faa\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u975e\u4fee\u8ba2\u521d\u59cb\u54cd\u5e94\u4e0a\u4fdd\u6301\u4e86 64.24% \u7684\u80dc\u7387\u3002", "conclusion": "Re5 \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u6700\u5c0f\u7684\u5916\u90e8\u76d1\u7763\u4e0b\u589e\u5f3a\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002"}}
{"id": "2507.05617", "pdf": "https://arxiv.org/pdf/2507.05617", "abs": "https://arxiv.org/abs/2507.05617", "authors": ["Mingzhe Li", "Jing Xiang", "Qishen Zhang", "Kaiyang Wan", "Xiuying Chen"], "title": "Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge distillation typically involves transferring knowledge from a Large\nLanguage Model (LLM) to a Smaller Language Model (SLM). However, in tasks such\nas text matching, fine-tuned smaller models often yield more effective\ndomain-specific representations, as they focus on optimizing the similarity of\ninput pairs. To leverage both the specialized strengths of small models and the\nrich semantic understanding of LLMs, we introduce a flipped knowledge\ndistillation paradigm, where LLM learns from SLM. Specifically, we address the\narchitectural gap between decoder-only LLMs and smaller encoder-based models by\nreinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder\ngenerates compressed representations, while the decoder maps them to the output\nspace. During training, the encoder produces representations and their\nsimilarities, which are then aligned with the similarity scores produced by the\nteacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.\nThe MCL ensures accurate similarity for both positive and negative pairs, and\nadaptively handles the internal differences within positive and negative\nsamples. Our paradigm requires only a reasonably good-performing SLM, allowing\nthe LLM to achieve improved performance. Experiments on financial and\nhealthcare benchmarks, as well as real-world applications, confirm its\neffectiveness, and the model has been fully deployed in an online environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ffb\u8f6c\u7684\u77e5\u8bc6\u84b8\u998f\u8303\u5f0f\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ece\u8f83\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5728\u6587\u672c\u5339\u914d\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03\u7684\u8f83\u5c0f\u6a21\u578b\u901a\u5e38\u80fd\u4ea7\u751f\u66f4\u6709\u6548\u7684\u9886\u57df\u7279\u5b9a\u8868\u793a\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e13\u6ce8\u4e8e\u4f18\u5316\u8f93\u5165\u5bf9\u7684\u76f8\u4f3c\u6027\u3002\u4e3a\u4e86\u5229\u7528\u5c0f\u578b\u6a21\u578b\u7684\u4e13\u4e1a\u4f18\u52bf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e30\u5bcc\u8bed\u4e49\u7406\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fd9\u79cd\u7ffb\u8f6c\u7684\u77e5\u8bc6\u84b8\u998f\u8303\u5f0f\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7ffb\u8f6c\u7684\u77e5\u8bc6\u84b8\u998f\u8303\u5f0f\uff0c\u5176\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u8f83\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5b66\u4e60\u3002\u6211\u4eec\u901a\u8fc7\u4f7f\u7528LoRA\u5c06LLM\u91cd\u65b0\u89e3\u91ca\u4e3a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u65b9\u5f0f\u6765\u89e3\u51b3\u67b6\u6784\u5dee\u8ddd\u3002\u7f16\u7801\u5668\u751f\u6210\u538b\u7f29\u8868\u793a\uff0c\u800c\u89e3\u7801\u5668\u5c06\u5176\u6620\u5c04\u5230\u8f93\u51fa\u7a7a\u95f4\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u7f16\u7801\u5668\u751f\u6210\u8868\u793a\u53ca\u5176\u76f8\u4f3c\u6027\uff0c\u5e76\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684Margin-aware Contrastive Learning (MCL)\u65b9\u6cd5\u4e0e\u6559\u5e08\u4ea7\u751f\u7684\u76f8\u4f3c\u6027\u5206\u6570\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u5728\u91d1\u878d\u548c\u533b\u7597\u4fdd\u5065\u57fa\u51c6\u4ee5\u53ca\u73b0\u5b9e\u5e94\u7528\u4e2d\u786e\u8ba4\u4e86\u8be5\u8303\u5f0f\u7684\u6709\u6548\u6027\uff0c\u6a21\u578b\u5df2\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u5b8c\u5168\u90e8\u7f72\u3002", "conclusion": "\u6211\u4eec\u7684\u8303\u5f0f\u53ea\u9700\u8981\u4e00\u4e2a\u8868\u73b0\u5408\u7406\u7684\u8f83\u5c0f\u6a21\u578b\uff0c\u5c31\u80fd\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u5728\u91d1\u878d\u548c\u533b\u7597\u4fdd\u5065\u57fa\u51c6\u4ee5\u53ca\u73b0\u5b9e\u5e94\u7528\u4e2d\u786e\u8ba4\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u4e14\u8be5\u6a21\u578b\u5df2\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u5b8c\u5168\u90e8\u7f72\u3002"}}
{"id": "2507.05633", "pdf": "https://arxiv.org/pdf/2507.05633", "abs": "https://arxiv.org/abs/2507.05633", "authors": ["Yiqiao Jin", "Kartik Sharma", "Vineeth Rakesh", "Yingtong Dou", "Menghai Pan", "Mahashweta Das", "Srijan Kumar"], "title": "SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "20 pages", "summary": "Retrieval-augmented Generation (RAG) extends large language models (LLMs)\nwith external knowledge but faces key challenges: restricted effective context\nlength and redundancy in retrieved documents. Pure compression-based approaches\nreduce input size but often discard fine-grained details essential for factual\naccuracy. We propose SARA, a unified RAG framework that balances local\nprecision and global knowledge coverage under tight context budgets. SARA\ncombines natural-language text snippets with semantic compression vectors to\njointly enhance context efficiency and answer correctness. It represents\ncontexts at two complementary levels: 1) fine-grained natural-language spans\nthat preserve critical entities and numerical values, and 2) compact,\ninterpretable vectors that summarize high-level semantics. An iterative\nevidence-selection module employs the compression vectors for dynamic reranking\nof contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families\n(Mistral, Llama, and Gemma), SARA consistently improves answer relevance\n(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),\ndemonstrating the importance of integrating textual and compressed\nrepresentations for robust, context-efficient RAG.", "AI": {"tldr": "SARA is a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets by combining natural-language text snippets with semantic compression vectors.", "motivation": "Retrieval-augmented Generation (RAG) extends large language models with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches often discard fine-grained details essential for factual accuracy.", "method": "SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context efficiency and answer correctness. It represents contexts at two complementary levels: fine-grained natural-language spans and compact, interpretable vectors. An iterative evidence-selection module employs the compression vectors for dynamic reranking of contexts.", "result": "SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53) across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma).", "conclusion": "SARA consistently improves answer relevance, answer correctness, and semantic similarity across multiple datasets and LLMs, demonstrating the importance of integrating textual and compressed representations for robust, context-efficient RAG."}}
{"id": "2507.05639", "pdf": "https://arxiv.org/pdf/2507.05639", "abs": "https://arxiv.org/abs/2507.05639", "authors": ["Haoxin Wang", "Xianhan Peng", "Xucheng Huang", "Yizhe Huang", "Ming Gong", "Chenghan Yang", "Yang Liu", "Ling Jiang"], "title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce ECom-Bench, the first benchmark framework for\nevaluating LLM agent with multimodal capabilities in the e-commerce customer\nsupport domain. ECom-Bench features dynamic user simulation based on persona\ninformation collected from real e-commerce customer interactions and a\nrealistic task dataset derived from authentic e-commerce dialogues. These\ntasks, covering a wide range of business scenarios, are designed to reflect\nreal-world complexities, making ECom-Bench highly challenging. For instance,\neven advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our\nbenchmark, highlighting the substantial difficulties posed by complex\ne-commerce scenarios. Upon publication, the code and data will be open-sourced\nto facilitate further research and development in this domain.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ECom-Bench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7535\u5b50\u5546\u52a1\u5ba2\u6237\u652f\u6301\u9886\u57df\u4e2d\u5177\u6709\u591a\u6a21\u6001\u80fd\u529b\u7684LLM\u4ee3\u7406\u7684\u57fa\u51c6\u6846\u67b6\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5177\u6709\u591a\u6a21\u6001\u80fd\u529b\u7684LLM\u4ee3\u7406\u5728\u7535\u5b50\u5546\u52a1\u5ba2\u6237\u652f\u6301\u9886\u57df\u7684\u8868\u73b0\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u7684\u57fa\u51c6\u6846\u67b6\u3002", "method": "ECom-Bench\u901a\u8fc7\u57fa\u4e8e\u771f\u5b9e\u7535\u5b50\u5546\u52a1\u5ba2\u6237\u4e92\u52a8\u6536\u96c6\u7684\u4e2a\u4eba\u8d44\u6599\u4fe1\u606f\u8fdb\u884c\u52a8\u6001\u7528\u6237\u6a21\u62df\uff0c\u5e76\u5229\u7528\u6765\u81ea\u771f\u5b9e\u7535\u5b50\u5546\u52a1\u5bf9\u8bdd\u7684\u771f\u5b9e\u4efb\u52a1\u6570\u636e\u96c6\u3002", "result": "\u5373\u4f7f\u5148\u8fdb\u7684\u6a21\u578b\u5982GPT-4o\u5728\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u53ea\u80fd\u8fbe\u523010-20%\u7684pass^3\u6307\u6807\uff0c\u8fd9\u8868\u660e\u590d\u6742\u7684\u7535\u5b50\u5546\u52a1\u573a\u666f\u5e26\u6765\u4e86\u663e\u8457\u7684\u6311\u6218\u3002", "conclusion": "ECom-Bench\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u80fd\u591f\u4fc3\u8fdb\u7535\u5b50\u5546\u52a1\u9886\u57df\u4e2dLLM\u4ee3\u7406\u7684\u7814\u7a76\u548c\u5f00\u53d1\u3002"}}
{"id": "2507.05686", "pdf": "https://arxiv.org/pdf/2507.05686", "abs": "https://arxiv.org/abs/2507.05686", "authors": ["SeungWon Ji", "Jungyup Lee", "Jemin Kim", "Sang Park", "SeungJae Lee"], "title": "Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual large language models (LLMs) often exhibit language confusion, a\ntendency to generate responses in a dominant language irrespective of the\nprompt's language. To address this, we propose Smoothie-Qwen, a lightweight,\npost-hoc method that mitigates language bias without retraining. This technique\nselectively adjusts token-level output probabilities to effectively suppress\nundesired language generation. Applied to the Qwen model, our method reduces\nunintended Chinese output by over 95% while preserving task accuracy on\nmultilingual benchmarks. This work provides a practical and efficient solution\nfor enhancing the language controllability of LLMs, making them more reliable\nfor global applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSmoothie-Qwen\u7684\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8c03\u6574token\u7ea7\u522b\u7684\u8f93\u51fa\u6982\u7387\u6765\u6291\u5236\u4e0d\u5e0c\u671b\u7684\u8bed\u8a00\u751f\u6210\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u8bed\u8a00\u53ef\u63a7\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u65e0\u610f\u4e2d\u4ea7\u751f\u7684\u4e2d\u6587\u8f93\u51fa\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u591a\u8bed\u8a00\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u5e38\u8868\u73b0\u51fa\u8bed\u8a00\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u5373\u503e\u5411\u4e8e\u7528\u4e3b\u5bfc\u8bed\u8a00\u751f\u6210\u54cd\u5e94\uff0c\u800c\u4e0d\u7ba1\u63d0\u793a\u7684\u8bed\u8a00\u662f\u4ec0\u4e48\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u540e\u5904\u7406\u65b9\u6cd5Smoothie-Qwen\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u8c03\u6574token\u7ea7\u522b\u7684\u8f93\u51fa\u6982\u7387\uff0c\u4ee5\u6709\u6548\u6291\u5236\u4e0d\u5e0c\u671b\u7684\u8bed\u8a00\u751f\u6210\u3002", "result": "\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eQwen\u6a21\u578b\u540e\uff0c\u65e0\u610f\u4e2d\u4ea7\u751f\u7684\u4e2d\u6587\u8f93\u51fa\u51cf\u5c11\u4e8695%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u540e\u5904\u7406\u65b9\u6cd5Smoothie-Qwen\uff0c\u4ee5\u89e3\u51b3\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u51fa\u73b0\u7684\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u6291\u5236\u4e86\u4e0d\u5e0c\u671b\u7684\u8bed\u8a00\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u8bed\u8a00\u53ef\u63a7\u6027\uff0c\u4f7f\u5176\u5728\u5168\u7403\u5e94\u7528\u4e2d\u66f4\u52a0\u53ef\u9760\u3002"}}
{"id": "2507.05707", "pdf": "https://arxiv.org/pdf/2507.05707", "abs": "https://arxiv.org/abs/2507.05707", "authors": ["Weihua Du", "Pranjal Aggarwal", "Sean Welleck", "Yiming Yang"], "title": "Agentic-R1: Distilled Dual-Strategy Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. 15 pages. Project available at\n  https://github.com/StigLidu/DualDistill", "summary": "Current long chain-of-thought (long-CoT) models excel at mathematical\nreasoning but rely on slow and error-prone natural language traces.\nTool-augmented agents address arithmetic via code execution, but often falter\non complex logical tasks. We introduce a fine-tuning framework, DualDistill,\nthat distills complementary reasoning strategies from multiple teachers into a\nunified student model. Using this approach, we train Agentic-R1, which\ndynamically selects the optimal strategy for each query, invoking tools for\narithmetic and algorithmic problems, and using text-based reasoning for\nabstract ones. Our method improves accuracy across a range of tasks, including\nboth computation-intensive and standard benchmarks, demonstrating the\neffectiveness of multi-strategy distillation in achieving robust and efficient\nreasoning. Our project is available at https://github.com/StigLidu/DualDistill", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDualDistill\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u4e2d\u84b8\u998f\u4e92\u8865\u7684\u63a8\u7406\u7b56\u7565\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u5b66\u751f\u6a21\u578b\u4e2d\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86Agentic-R1\uff0c\u5b83\u80fd\u591f\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff0c\u5bf9\u4e8e\u7b97\u672f\u548c\u7b97\u6cd5\u95ee\u9898\u8c03\u7528\u5de5\u5177\uff0c\u800c\u5bf9\u4e8e\u62bd\u8c61\u95ee\u9898\u5219\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u63a8\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5305\u62ec\u8ba1\u7b97\u5bc6\u96c6\u578b\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u591a\u7b56\u7565\u84b8\u998f\u5728\u5b9e\u73b0\u7a33\u5065\u548c\u9ad8\u6548\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u957f\u94fe\u5f0f\u601d\u7ef4\uff08long-CoT\uff09\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f9d\u8d56\u4e8e\u7f13\u6162\u4e14\u5bb9\u6613\u51fa\u9519\u7684\u81ea\u7136\u8bed\u8a00\u8f68\u8ff9\u3002\u5de5\u5177\u589e\u5f3a\u7684\u4ee3\u7406\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u6765\u5904\u7406\u7b97\u672f\u95ee\u9898\uff0c\u4f46\u5728\u590d\u6742\u7684\u903b\u8f91\u4efb\u52a1\u4e0a\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5fae\u8c03\u6846\u67b6DualDistill\uff0c\u8be5\u6846\u67b6\u4ece\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u4e2d\u84b8\u998f\u4e92\u8865\u7684\u63a8\u7406\u7b56\u7565\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u5b66\u751f\u6a21\u578b\u4e2d\u3002\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86Agentic-R1\uff0c\u5b83\u80fd\u591f\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff0c\u5bf9\u4e8e\u7b97\u672f\u548c\u7b97\u6cd5\u95ee\u9898\u8c03\u7528\u5de5\u5177\uff0c\u800c\u5bf9\u4e8e\u62bd\u8c61\u95ee\u9898\u5219\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u63a8\u7406\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5305\u62ec\u8ba1\u7b97\u5bc6\u96c6\u578b\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u591a\u7b56\u7565\u84b8\u998f\u5728\u5b9e\u73b0\u7a33\u5065\u548c\u9ad8\u6548\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5305\u62ec\u8ba1\u7b97\u5bc6\u96c6\u578b\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u591a\u7b56\u7565\u84b8\u998f\u5728\u5b9e\u73b0\u7a33\u5065\u548c\u9ad8\u6548\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.05713", "pdf": "https://arxiv.org/pdf/2507.05713", "abs": "https://arxiv.org/abs/2507.05713", "authors": ["Fedor Chernogorskii", "Sergei Averkiev", "Liliya Kudraleeva", "Zaven Martirosian", "Maria Tikhonova", "Valentin Malykh", "Alena Fenogenova"], "title": "DRAGON: Dynamic RAG Benchmark On News", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments.\n  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first\ndynamic benchmark for evaluating RAG systems in Russian on a changing news\ncorpora. DRAGON is built upon a regularly updated corpus of Russian news and\npublic documents and supports comprehensive evaluation of both the retriever\nand generator components. Question generation is performed automatically with\nthe use of Knowledge Graph constructed from the corpus and enables the\nextraction of four core question types aligned with distinct subgraph patterns.\nWe release a complete evaluation framework comprising the pipeline for\nautomatic question generation, evaluation scripts, which are potentially\nreusable for other languages and multilingual settings, and benchmark data. We\nalso launch a public leaderboard to encourage community participation and\ncomparison.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86DRAGON\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4fc4\u8bed\u4e2dRAG\u7cfb\u7edf\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u8d44\u6e90\u4e0d\u8db3\u548c\u9759\u6001\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u516c\u5171\u6392\u884c\u699c\u3002", "motivation": "\u73b0\u6709\u7684RAG\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u800c\u5176\u4ed6\u8bed\u8a00\u5982\u4fc4\u8bed\u7684\u8bc4\u4f30\u8d44\u6e90\u7a00\u7f3a\u4e14\u9759\u6001\uff0c\u65e0\u6cd5\u6355\u6349\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5b9a\u671f\u66f4\u65b0\u7684\u4fc4\u8bed\u65b0\u95fb\u548c\u516c\u5171\u6587\u6863\u8bed\u6599\u5e93\uff0c\u5e76\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u751f\u6210\u95ee\u9898\uff0c\u4ee5\u8bc4\u4f30\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u7ec4\u4ef6\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86DRAGON\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4fc4\u8bed\u4e2dRAG\u7cfb\u7edf\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u53d1\u5e03\u4e86\u5b8c\u6574\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u516c\u5171\u6392\u884c\u699c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86DRAGON\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4fc4\u8bed\u4e2dRAG\u7cfb\u7edf\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u8d44\u6e90\u4e0d\u8db3\u548c\u9759\u6001\u7684\u95ee\u9898\u3002\u540c\u65f6\uff0c\u672c\u6587\u8fd8\u53d1\u5e03\u4e86\u5b8c\u6574\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u516c\u5171\u6392\u884c\u699c\uff0c\u4ee5\u4fc3\u8fdb\u793e\u533a\u53c2\u4e0e\u548c\u6bd4\u8f83\u3002"}}
{"id": "2507.05714", "pdf": "https://arxiv.org/pdf/2507.05714", "abs": "https://arxiv.org/abs/2507.05714", "authors": ["YiHan Jiao", "ZheHao Tan", "Dan Yang", "DuoLin Sun", "Jie Feng", "Jian Wang", "Peng Wei"], "title": "HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a fundamental paradigm for\naddressing the challenges faced by large language models in handling real-time\ninformation and domain-specific problems. Traditional RAG systems primarily\nrely on the in-context learning (ICL) capabilities of the large language model\nitself. Still, in-depth research on the specific capabilities needed by the RAG\ngeneration model is lacking, leading to challenges with inconsistent document\nquality and retrieval system imperfections. Even the limited studies that\nfine-tune RAG generative models often \\textit{lack a granular focus on RAG\ntask} or \\textit{a deeper utilization of chain-of-thought processes}. To\naddress this, we propose that RAG models should possess three progressively\nhierarchical abilities (1) Filtering: the ability to select relevant\ninformation; (2) Combination: the ability to combine semantic information\nacross paragraphs; and (3) RAG-specific reasoning: the ability to further\nprocess external knowledge using internal knowledge. Thus, we introduce our new\nRAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning\nRetrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\"\nstrategy. This method enhances the model's open-book examination capability by\nutilizing multi-level progressive chain-of-thought. Experiments show that the\nHIRAG training strategy significantly improves the model's performance on\ndatasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RAG\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5HIRAG\uff0c\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u601d\u7ef4\u94fe\u6765\u63d0\u5347\u6a21\u578b\u5904\u7406\u5916\u90e8\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u672c\u8eab\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5bf9RAG\u751f\u6210\u6a21\u578b\u6240\u9700\u7684\u5177\u4f53\u80fd\u529b\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6587\u6863\u8d28\u91cf\u4e0d\u4e00\u81f4\u548c\u68c0\u7d22\u7cfb\u7edf\u4e0d\u5b8c\u5584\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6709\u9650\u7684\u7814\u7a76\u5728\u5fae\u8c03RAG\u751f\u6210\u6a21\u578b\u65f6\u5f80\u5f80\u7f3a\u4e4f\u5bf9RAG\u4efb\u52a1\u7684\u7ec6\u81f4\u5173\u6ce8\u6216\u5bf9\u601d\u7ef4\u94fe\u8fc7\u7a0b\u7684\u6df1\u5165\u5229\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RAG\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\uff0c\u79f0\u4e3a\u5206\u5c42\u601d\u7ef4\u6307\u4ee4\u5fae\u8c03\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08HIRAG\uff09\uff0c\u7ed3\u5408\u4e86\u201c\u5148\u601d\u8003\u540e\u56de\u7b54\u201d\u7684\u7b56\u7565\uff0c\u5229\u7528\u591a\u7ea7\u6e10\u8fdb\u5f0f\u601d\u7ef4\u94fe\u6765\u589e\u5f3a\u6a21\u578b\u7684\u5f00\u5377\u8003\u8bd5\u80fd\u529b\u3002", "result": "HIRAG\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cHIRAG\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728RGB\u3001PopQA\u3001MuSiQue\u3001HotpotQA\u548cPubmedQA\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.05724", "pdf": "https://arxiv.org/pdf/2507.05724", "abs": "https://arxiv.org/abs/2507.05724", "authors": ["Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Mixture-of-experts (MoE) architectures have expanded from language modeling\nto automatic speech recognition (ASR). Traditional MoE methods, such as the\nSwitch Transformer, route experts independently within each layer. Our analysis\nreveals that routers in most layers make expert choices that are not strongly\ncorrelated with the choices of the routers in other layers. To increase the\ncooperation between experts in different layers and encourage greater\nspecialization, we use a shared router across different MoE layers. We call\nthis model \\emph{Omni-router Transformer}. Extensive experiments on a\nlarge-scale pseudo-labeled dataset and evaluations across 10 diverse,\nout-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is\nable to achieve lower training loss and consistently outperform dense and\nSwitch Transformer models, reducing average word error rates by 11.2% and 8.2%,\nrespectively, while providing structured expert usage and improved robustness\nto diverse data.", "AI": {"tldr": "Omni-router Transformer\u662f\u4e00\u79cd\u6539\u8fdb\u7684MoE\u67b6\u6784\uff0c\u901a\u8fc7\u5171\u4eab\u8def\u7531\u5668\u63d0\u9ad8\u4e0d\u540c\u5c42\u4e4b\u95f4\u4e13\u5bb6\u7684\u5408\u4f5c\u548c\u4e13\u4e1a\u5316\uff0c\u4ece\u800c\u5728ASR\u4efb\u52a1\u4e2d\u53d6\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfMoE\u65b9\u6cd5\u5728\u6bcf\u5c42\u4e2d\u72ec\u7acb\u8def\u7531\u4e13\u5bb6\uff0c\u800c\u5927\u591a\u6570\u5c42\u7684\u8def\u7531\u5668\u505a\u51fa\u7684\u4e13\u5bb6\u9009\u62e9\u4e0e\u5176\u4ed6\u5c42\u7684\u8def\u7531\u5668\u9009\u62e9\u4e0d\u5f3a\u76f8\u5173\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8\u4e0d\u540c\u5c42\u4e4b\u95f4\u4e13\u5bb6\u7684\u5408\u4f5c\u548c\u4e13\u4e1a\u5316\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u8def\u7531\u5668\u8de8\u4e0d\u540cMoE\u5c42\uff0c\u4ee5\u589e\u52a0\u4e0d\u540c\u5c42\u4e4b\u95f4\u4e13\u5bb6\u7684\u5408\u4f5c\u5e76\u9f13\u52b1\u66f4\u5927\u7684\u4e13\u4e1a\u5316\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4f2a\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u548c10\u4e2a\u4e0d\u540c\u7684\u3001\u57df\u5916ASR\u57fa\u51c6\u7684\u8bc4\u4f30\u8868\u660e\uff0cOmni-router Transformer\u80fd\u591f\u964d\u4f4e\u8bad\u7ec3\u635f\u5931\u5e76\u4f18\u4e8e\u5bc6\u96c6\u6a21\u578b\u548cSwitch Transformer\u6a21\u578b\uff0c\u5e73\u5747\u8bcd\u9519\u8bef\u7387\u5206\u522b\u51cf\u5c11\u4e8611.2%\u548c8.2%\u3002", "conclusion": "Omni-router Transformer\u80fd\u591f\u5b9e\u73b0\u66f4\u4f4e\u7684\u8bad\u7ec3\u635f\u5931\uff0c\u5e76\u4e14\u5728\u591a\u4e2aASR\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5bc6\u96c6\u6a21\u578b\u548cSwitch Transformer\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u4e13\u5bb6\u4f7f\u7528\u548c\u6539\u8fdb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.05740", "pdf": "https://arxiv.org/pdf/2507.05740", "abs": "https://arxiv.org/abs/2507.05740", "authors": ["Yujia Hu", "Tuan-Phong Nguyen", "Shrestha Ghosh", "Moritz M\u00fcller", "Simon Razniewski"], "title": "GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge", "categories": ["cs.CL"], "comment": "7 pages, 6 figures, 1 table", "summary": "Language models are powerful tools, yet their factual knowledge is still\npoorly understood, and inaccessible to ad-hoc browsing and scalable statistical\nanalysis. This demonstration introduces GPTKB v1.5, a densely interlinked\n100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using\nthe GPTKB methodology for massive-recursive LLM knowledge materialization (Hu\net al., ACL 2025). The demonstration experience focuses on three use cases: (1)\nlink-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM\nknowledge querying, (3) comparative exploration of the strengths and weaknesses\nof LLM knowledge. Massive-recursive LLM knowledge materialization is a\ngroundbreaking opportunity both for the research area of systematic analysis of\nLLM knowledge, as well as for automated KB construction. The GPTKB demonstrator\nis accessible at https://gptkb.org.", "AI": {"tldr": "GPTKB v1.5 \u662f\u4e00\u4e2a\u7531 GPT-4.1 \u6784\u5efa\u7684 1 \u4ebf\u4e09\u5143\u7ec4\u77e5\u8bc6\u5e93\uff0c\u7528\u4e8e\u63a2\u7d22\u548c\u5206\u6790\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e09\u79cd\u4f7f\u7528\u6848\u4f8b\u3002", "motivation": "\u76ee\u524d\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u77e5\u8bc6\u4ecd\u4e0d\u6e05\u6670\uff0c\u4e14\u96be\u4ee5\u8fdb\u884c\u968f\u610f\u6d4f\u89c8\u548c\u53ef\u6269\u5c55\u7684\u7edf\u8ba1\u5206\u6790\u3002", "method": "GPTKB v1.5 \u662f\u901a\u8fc7\u4f7f\u7528 GPTKB \u65b9\u6cd5\u8fdb\u884c\u5927\u89c4\u6a21\u9012\u5f52 LLM \u77e5\u8bc6\u6750\u6599\u5316\u6784\u5efa\u7684\uff0c\u6210\u672c\u4e3a 14,000 \u7f8e\u5143\u3002", "result": "GPTKB v1.5 \u662f\u4e00\u4e2a\u5305\u542b 1 \u4ebf\u4e2a\u4e09\u5143\u7ec4\u7684\u5bc6\u96c6\u4e92\u8fde\u77e5\u8bc6\u5e93\uff0c\u5c55\u793a\u4e86\u4e09\u79cd\u4f7f\u7528\u6848\u4f8b\uff1a\u57fa\u4e8e\u94fe\u63a5\u904d\u5386\u7684 LLM \u77e5\u8bc6\u63a2\u7d22\u3001\u57fa\u4e8e SPARQL \u7684\u7ed3\u6784\u5316 LLM \u77e5\u8bc6\u67e5\u8be2\u4ee5\u53ca LLM \u77e5\u8bc6\u4f18\u7f3a\u70b9\u7684\u6bd4\u8f83\u63a2\u7d22\u3002", "conclusion": "GPTKB v1.5 \u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u65b9\u5f0f\u6765\u63a2\u7d22\u548c\u5206\u6790\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u4e3a\u7cfb\u7edf\u6027\u7814\u7a76\u548c\u81ea\u52a8\u5316\u77e5\u8bc6\u5e93\u6784\u5efa\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.05750", "pdf": "https://arxiv.org/pdf/2507.05750", "abs": "https://arxiv.org/abs/2507.05750", "authors": ["Jing Yang Lee", "Hamed Bonab", "Nasser Zalmout", "Ming Zeng", "Sanket Lokegaonkar", "Colin Lockard", "Binxuan Huang", "Ritesh Sarkhel", "Haodong Wang"], "title": "DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities", "categories": ["cs.CL"], "comment": "Accepted at SIGDIAL 2025", "summary": "Large Language Models (LLMs) are increasingly employed in multi-turn\nconversational tasks, yet their pre-training data predominantly consists of\ncontinuous prose, creating a potential mismatch between required capabilities\nand training paradigms. We introduce a novel approach to address this\ndiscrepancy by synthesizing conversational data from existing text corpora. We\npresent a pipeline that transforms a cluster of multiple related documents into\nan extended multi-turn, multi-topic information-seeking dialogue. Applying our\npipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training\ndialogue corpus consisting of over 730k long conversations. We hypothesize that\nexposure to such synthesized conversational structures during pre-training can\nenhance the fundamental multi-turn capabilities of LLMs, such as context memory\nand understanding. Empirically, we show that incorporating DocTalk during\npre-training results in up to 40% gain in context memory and understanding,\nwithout compromising base performance. DocTalk is available at\nhttps://huggingface.co/datasets/AmazonScience/DocTalk.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u73b0\u6709\u6587\u672c\u8bed\u6599\u5e93\u4e2d\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u6765\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u4e0e\u8bad\u7ec3\u8303\u5f0f\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002\u901a\u8fc7\u5c06DocTalk\u7eb3\u5165\u9884\u8bad\u7ec3\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\uff0c\u5982\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u548c\u7406\u89e3\uff0c\u540c\u65f6\u4e0d\u4f1a\u5f71\u54cd\u57fa\u7840\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u8d8a\u6765\u8d8a\u88ab\u4f7f\u7528\uff0c\u4f46\u5b83\u4eec\u7684\u9884\u8bad\u7ec3\u6570\u636e\u4e3b\u8981\u7531\u8fde\u7eed\u7684\u6563\u6587\u7ec4\u6210\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6240\u9700\u80fd\u529b\u548c\u8bad\u7ec3\u8303\u5f0f\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u73b0\u6709\u6587\u672c\u8bed\u6599\u5e93\u4e2d\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u5f02\u3002\u5c55\u793a\u4e86\u4e00\u4e2a\u5c06\u591a\u4e2a\u76f8\u5173\u6587\u6863\u8f6c\u6362\u4e3a\u6269\u5c55\u7684\u591a\u8f6e\u3001\u591a\u4e3b\u9898\u4fe1\u606f\u5bfb\u6c42\u5bf9\u8bdd\u7684\u6d41\u7a0b\u3002", "result": "\u5c06DocTalk\u7eb3\u5165\u9884\u8bad\u7ec3\u53ef\u4ee5\u5e26\u6765\u9ad8\u8fbe40%\u7684\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u548c\u7406\u89e3\u80fd\u529b\u7684\u63d0\u5347\uff0c\u800c\u4e0d\u4f1a\u635f\u5bb3\u57fa\u7840\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06DocTalk\u7eb3\u5165\u9884\u8bad\u7ec3\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\uff0c\u5982\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u548c\u7406\u89e3\uff0c\u540c\u65f6\u4e0d\u4f1a\u5f71\u54cd\u57fa\u7840\u6027\u80fd\u3002"}}
{"id": "2507.05788", "pdf": "https://arxiv.org/pdf/2507.05788", "abs": "https://arxiv.org/abs/2507.05788", "authors": ["Anand A. Rajasekar", "Praveen Tangarajan", "Anjali Nainani", "Amogh Batwal", "Vinay Rao Dandin", "Anusua Trivedi", "Ozan Ersoy"], "title": "Flippi: End To End GenAI Assistant for E-Commerce", "categories": ["cs.CL", "I.2.7; H.3.3"], "comment": "10 pages, 2 figures, 7 tables", "summary": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace.", "AI": {"tldr": "This paper introduces Flippi, an end-to-end conversational assistant powered by large language models for e-commerce, which enhances product discovery through natural language dialogue and provides personalized shopping experiences.", "motivation": "The paper aims to address the challenges posed by the vast and often overwhelming product landscape in e-commerce, enabling customers to discover products more efficiently through natural language dialogue.", "method": "Flippi uses advanced NLP techniques such as Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG), Named Entity Recognition (NER), and Context Reduction to interpret customer queries and provide precise product information. It also includes comparative analysis features to help users make informed choices.", "result": "Flippi demonstrates its ability to identify and present the most attractive offers on an e-commerce site, empowering users to make cost-effective decisions. It also outlines a robust architecture for integration across various e-commerce platforms and presents a comprehensive evaluation framework covering performance metrics, user satisfaction, and the impact on customer engagement and conversion rates.", "conclusion": "Flippi sets a new standard for customer satisfaction and engagement in the digital marketplace by bridging the convenience of online shopping with personalized assistance traditionally found in physical stores."}}
{"id": "2507.05799", "pdf": "https://arxiv.org/pdf/2507.05799", "abs": "https://arxiv.org/abs/2507.05799", "authors": ["Amane Watahiki", "Tomoki Doi", "Taiga Shinozaki", "Satoshi Nishida", "Takuya Niikawa", "Katsunori Miyahara", "Hitomi Yanaka"], "title": "Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports", "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the 47th Annual Meeting of the\n  Cognitive Science Society (COGSCI 2025)", "summary": "One of the main objectives in developing large vision-language models (LVLMs)\nis to engineer systems that can assist humans with multimodal tasks, including\ninterpreting descriptions of perceptual experiences. A central phenomenon in\nthis context is amodal completion, in which people perceive objects even when\nparts of those objects are hidden. Although numerous studies have assessed\nwhether computer-vision algorithms can detect or reconstruct occluded regions,\nthe inferential abilities of LVLMs on texts related to amodal completion remain\nunexplored. To address this gap, we constructed a benchmark grounded in Basic\nFormal Ontology to achieve a systematic classification of amodal completion.\nOur results indicate that while many LVLMs achieve human-comparable performance\noverall, their accuracy diverges for certain types of objects being completed.\nNotably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet\nexhibit lower accuracy on original images compared to blank stimuli lacking\nvisual content. Intriguingly, this disparity emerges only under Japanese\nprompting, suggesting a deficiency in Japanese-specific linguistic competence\namong these models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u5904\u7406amodal completion\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u65e5\u8bed\u63d0\u793a\u4e0b\u3002", "motivation": "\u7814\u7a76LVLM\u5728\u4e0eamodal completion\u76f8\u5173\u7684\u6587\u672c\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u5728\u5904\u7406\u9690\u85cf\u90e8\u5206\u7269\u4f53\u65f6\u7684\u8868\u73b0\u3002", "method": "\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u57fa\u672c\u5f62\u5f0f\u672c\u4f53\u8bba\u7684\u57fa\u51c6\uff0c\u4ee5\u5b9e\u73b0\u5bf9amodal completion\u7684\u7cfb\u7edf\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u8bb8\u591aLVLM\u5728\u6574\u4f53\u8868\u73b0\u4e0a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u5728\u67d0\u4e9b\u7269\u4f53\u7c7b\u578b\u7684\u5b8c\u6210\u4e0a\u51c6\u786e\u6027\u5b58\u5728\u5dee\u5f02\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u67d0\u4e9b\u7c7b\u522b\u4e2d\uff0c\u4e00\u4e9bLLaVA-NeXT\u53d8\u4f53\u548cClaude 3.5 Sonnet\u5728\u539f\u59cb\u56fe\u50cf\u4e0a\u7684\u51c6\u786e\u6027\u4f4e\u4e8e\u7f3a\u4e4f\u89c6\u89c9\u5185\u5bb9\u7684\u7a7a\u767d\u523a\u6fc0\u3002\u8fd9\u79cd\u5dee\u5f02\u4ec5\u5728\u65e5\u8bed\u63d0\u793a\u4e0b\u51fa\u73b0\uff0c\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u5728\u65e5\u8bed\u7279\u5b9a\u7684\u8bed\u8a00\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u8bb8\u591aLVLM\u5728\u6574\u4f53\u8868\u73b0\u4e0a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u5728\u67d0\u4e9b\u7269\u4f53\u7c7b\u578b\u7684\u5b8c\u6210\u4e0a\u51c6\u786e\u6027\u5b58\u5728\u5dee\u5f02\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u67d0\u4e9b\u7c7b\u522b\u4e2d\uff0c\u4e00\u4e9bLLaVA-NeXT\u53d8\u4f53\u548cClaude 3.5 Sonnet\u5728\u539f\u59cb\u56fe\u50cf\u4e0a\u7684\u51c6\u786e\u6027\u4f4e\u4e8e\u7f3a\u4e4f\u89c6\u89c9\u5185\u5bb9\u7684\u7a7a\u767d\u523a\u6fc0\u3002\u8fd9\u79cd\u5dee\u5f02\u4ec5\u5728\u65e5\u8bed\u63d0\u793a\u4e0b\u51fa\u73b0\uff0c\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u5728\u65e5\u8bed\u7279\u5b9a\u7684\u8bed\u8a00\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002"}}
{"id": "2507.05885", "pdf": "https://arxiv.org/pdf/2507.05885", "abs": "https://arxiv.org/abs/2507.05885", "authors": ["Tanvina Patel", "Wiebke Hutiri", "Aaron Yi Ding", "Odette Scharenborg"], "title": "How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "There is increasingly more evidence that automatic speech recognition (ASR)\nsystems are biased against different speakers and speaker groups, e.g., due to\ngender, age, or accent. Research on bias in ASR has so far primarily focused on\ndetecting and quantifying bias, and developing mitigation approaches. Despite\nthis progress, the open question is how to measure the performance and bias of\na system. In this study, we compare different performance and bias measures,\nfrom literature and proposed, to evaluate state-of-the-art end-to-end ASR\nsystems for Dutch. Our experiments use several bias mitigation strategies to\naddress bias against different speaker groups. The findings reveal that\naveraged error rates, a standard in ASR research, alone is not sufficient and\nshould be supplemented by other measures. The paper ends with recommendations\nfor reporting ASR performance and bias to better represent a system's\nperformance for diverse speaker groups, and overall system bias.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86ASR\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u504f\u89c1\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u62a5\u544a\u7684\u5efa\u8bae\u3002", "motivation": "\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u68c0\u6d4b\u548c\u91cf\u5316ASR\u4e2d\u7684\u504f\u89c1\uff0c\u4ee5\u53ca\u5f00\u53d1\u7f13\u89e3\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u6d4b\u91cf\u6027\u80fd\u548c\u504f\u89c1\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u6bd4\u8f83\u4e86\u6587\u732e\u4e2d\u548c\u63d0\u51fa\u7684\u4e0d\u540c\u6027\u80fd\u548c\u504f\u89c1\u6d4b\u91cf\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aefASR\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5e73\u5747\u9519\u8bef\u7387\u4e0d\u8db3\u4ee5\u5168\u9762\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\uff0c\u9700\u8981\u7ed3\u5408\u5176\u4ed6\u6d4b\u91cf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u5bf9ASR\u7cfb\u7edf\u6027\u80fd\u548c\u504f\u89c1\u6d4b\u91cf\u65b9\u6cd5\u7684\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u62a5\u544a\u7684\u5efa\u8bae\u3002"}}
{"id": "2507.05890", "pdf": "https://arxiv.org/pdf/2507.05890", "abs": "https://arxiv.org/abs/2507.05890", "authors": ["Sungjib Lim", "Woojung Song", "Eun-Ju Lee", "Yohan Jo"], "title": "Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 7 figures", "summary": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u865a\u62df\u88ab\u8bd5\u6a21\u62df\u7684\u6846\u67b6\uff0c\u4ee5\u9ad8\u6548\u751f\u6210\u5177\u6709\u7ed3\u6784\u6548\u5ea6\u7684\u8c03\u67e5\u9879\u76ee\u3002", "motivation": "\u968f\u7740\u5fc3\u7406\u8ba1\u91cf\u8c03\u67e5\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7279\u8d28\uff0c\u9700\u8981\u4e00\u79cd\u9002\u5408LLMs\u7684\u53ef\u6269\u5c55\u8c03\u67e5\u9879\u76ee\u751f\u6210\u65b9\u6cd5\u3002\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u786e\u4fdd\u751f\u6210\u9879\u76ee\u7684\u7ed3\u6784\u6548\u5ea6\uff0c\u5373\u5b83\u4eec\u662f\u5426\u771f\u6b63\u6d4b\u91cf\u4e86\u9884\u671f\u7684\u7279\u8d28\u3002\u4f20\u7edf\u4e0a\uff0c\u8fd9\u9700\u8981\u8017\u8d39\u5927\u91cf\u4eba\u529b\u7684\u6570\u636e\u6536\u96c6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528LLMs\u8fdb\u884c\u865a\u62df\u88ab\u8bd5\u6a21\u62df\u7684\u6846\u67b6\u3002\u6211\u4eec\u7684\u6838\u5fc3\u601d\u60f3\u662f\u8003\u8651\u4e2d\u4ecb\u56e0\u7d20\uff1a\u901a\u8fc7\u8fd9\u4e9b\u56e0\u7d20\uff0c\u76f8\u540c\u7684\u7279\u8d28\u53ef\u4ee5\u5bfc\u81f4\u5bf9\u8c03\u67e5\u9879\u76ee\u7684\u4e0d\u540c\u53cd\u5e94\u3002\u901a\u8fc7\u6a21\u62df\u5177\u6709\u4e0d\u540c\u4e2d\u4ecb\u56e0\u7d20\u7684\u88ab\u8bd5\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u80fd\u591f\u7a33\u5065\u6d4b\u91cf\u76ee\u6807\u7279\u8d28\u7684\u8c03\u67e5\u9879\u76ee\u3002", "result": "\u5728\u4e09\u79cd\u5fc3\u7406\u5b66\u7279\u8d28\u7406\u8bba\uff08\u5927\u4e94\u3001\u65bd\u74e6\u8328\u3001VIA\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u4e2d\u4ecb\u751f\u6210\u65b9\u6cd5\u548c\u6a21\u62df\u6846\u67b6\u6709\u6548\u8bc6\u522b\u4e86\u9ad8\u6548\u5ea6\u7684\u9879\u76ee\u3002LLMs\u5c55\u793a\u4e86\u4ece\u7279\u8d28\u5b9a\u4e49\u751f\u6210\u5408\u7406\u4e2d\u4ecb\u56e0\u7d20\u7684\u80fd\u529b\uff0c\u5e76\u6a21\u62df\u88ab\u8bd5\u884c\u4e3a\u4ee5\u9a8c\u8bc1\u9879\u76ee\u3002", "conclusion": "\u6211\u4eec\u7684\u95ee\u9898\u5b9a\u4e49\u3001\u6307\u6807\u3001\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u4e3a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8c03\u67e5\u5f00\u53d1\u548c\u66f4\u6df1\u5165\u7406\u89e3LLMs\u5982\u4f55\u590d\u5236\u4eba\u7c7b\u884c\u4e3a\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002\u6211\u4eec\u5c06\u516c\u5f00\u53d1\u5e03\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u4ee5\u652f\u6301\u672a\u6765\u7684\u5de5\u4f5c\u3002"}}
{"id": "2507.05918", "pdf": "https://arxiv.org/pdf/2507.05918", "abs": "https://arxiv.org/abs/2507.05918", "authors": ["Teodor-George Marchitan", "Claudiu Creanga", "Liviu P. Dinu"], "title": "Few-shot text-based emotion detection", "categories": ["cs.CL"], "comment": null, "summary": "This paper describes the approach of the Unibuc - NLP team in tackling the\nSemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion\nDetection. We mainly focused on experiments using large language models\n(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With\nour final system, for the multi-label emotion detection track (track A), we got\nan F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36\nteams) for the Portuguese (Mozambican) subset and $0.325$ (\\textbf{1}/31 teams)\nfor the Emakhuwa subset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86Unibuc-NLP\u56e2\u961f\u5728SemEval 2025 Workshop\u4efb\u52a111\u4e2d\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u57fa\u4e8e\u6587\u672c\u7684\u60c5\u611f\u68c0\u6d4b\u4e2d\u7684\u5dee\u8ddd\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u8be5\u8bba\u6587\u4e3b\u8981\u91c7\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Gemini\u3001Qwen\u3001DeepSeek\uff09\u8fdb\u884c\u5c11\u6837\u672c\u63d0\u793a\u6216\u5fae\u8c03\u5b9e\u9a8c\u3002", "result": "\u8be5\u8bba\u6587\u5728\u591a\u6807\u7b7e\u60c5\u611f\u68c0\u6d4b\u8d5b\u9053\u4e2d\uff0c\u5bf9\u4e8e\u82f1\u8bed\u5b50\u96c6\u7684F1\u5b8f\u5f97\u5206\u8fbe\u5230\u4e860.7546\uff0896\u652f\u961f\u4f0d\u4e2d\u7684\u7b2c26\u540d\uff09\uff0c\u5bf9\u4e8e\u8461\u8404\u7259\u8bed\uff08\u83ab\u6851\u6bd4\u514b\uff09\u5b50\u96c6\u7684F1\u5b8f\u5f97\u5206\u4e3a0.1727\uff0836\u652f\u961f\u4f0d\u4e2d\u7684\u7b2c35\u540d\uff09\uff0c\u5bf9\u4e8eEmakhuwa\u5b50\u96c6\u7684F1\u5b8f\u5f97\u5206\u4e3a0.325\uff0831\u652f\u961f\u4f0d\u4e2d\u7684\u7b2c1\u540d\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86Unibuc-NLP\u56e2\u961f\u5728SemEval 2025 Workshop\u4efb\u52a111\u4e2d\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.05937", "pdf": "https://arxiv.org/pdf/2507.05937", "abs": "https://arxiv.org/abs/2507.05937", "authors": ["Sebastian Pohl", "Max Ploner", "Alan Akbik"], "title": "Towards a Principled Evaluation of Knowledge Editors", "categories": ["cs.CL"], "comment": "Accepted at L2M2 workshop at ACL 2025", "summary": "Model editing has been gaining increasing attention over the past few years.\nFor Knowledge Editing in particular, more challenging evaluation datasets have\nrecently been released. These datasets use different methodologies to score the\nsuccess of editors. Yet, it remains under-explored how robust these\nmethodologies are and whether they unfairly favor some editors. Moreover, the\ndisruptive impact of these editors on overall model capabilities remains a\nconstant blind spot.\n  We address both of these problems and show that choosing different metrics\nand evaluation methodologies as well as different edit batch sizes can lead to\na different ranking of knowledge editors. Crucially we demonstrate this effect\nalso on general language understanding tasks evaluated alongside the knowledge\nediting tasks. Further we include a manual assessment of the string matching\nbased evaluation method for knowledge editing that is favored by recently\nreleased datasets, revealing a tendency to produce false positive matches.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u77e5\u8bc6\u7f16\u8f91\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u548c\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u4e0d\u540c\u7684\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\u4ee5\u53ca\u4e0d\u540c\u7684\u7f16\u8f91\u6279\u6b21\u5927\u5c0f\u4f1a\u5f71\u54cd\u77e5\u8bc6\u7f16\u8f91\u5668\u7684\u6392\u540d\uff0c\u5e76\u4e14\u57fa\u4e8e\u5b57\u7b26\u4e32\u5339\u914d\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5047\u9633\u6027\u95ee\u9898\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u7814\u7a76\u73b0\u6709\u7684\u77e5\u8bc6\u7f16\u8f91\u8bc4\u4f30\u65b9\u6cd5\u662f\u5426\u7a33\u5065\uff0c\u5e76\u4e14\u662f\u5426\u516c\u5e73\u5730\u8bc4\u4ef7\u4e0d\u540c\u7684\u7f16\u8f91\u5668\u3002\u6b64\u5916\uff0c\u8fd8\u5e0c\u671b\u4e86\u89e3\u8fd9\u4e9b\u7f16\u8f91\u5668\u5bf9\u6a21\u578b\u6574\u4f53\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u7684\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\u4ee5\u53ca\u4e0d\u540c\u7684\u7f16\u8f91\u6279\u6b21\u5927\u5c0f\u5bf9\u77e5\u8bc6\u7f16\u8f91\u5668\u6392\u540d\u7684\u5f71\u54cd\uff0c\u6765\u89e3\u51b3\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u624b\u52a8\u8bc4\u4f30\u4ee5\u68c0\u67e5\u57fa\u4e8e\u5b57\u7b26\u4e32\u5339\u914d\u7684\u8bc4\u4f30\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u7684\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\u4ee5\u53ca\u4e0d\u540c\u7684\u7f16\u8f91\u6279\u6b21\u5927\u5c0f\u4f1a\u5bfc\u81f4\u77e5\u8bc6\u7f16\u8f91\u5668\u6392\u540d\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u5728\u901a\u7528\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u4e5f\u89c2\u5bdf\u5230\u4e86\u8fd9\u79cd\u5f71\u54cd\u3002\u624b\u52a8\u8bc4\u4f30\u663e\u793a\u57fa\u4e8e\u5b57\u7b26\u4e32\u5339\u914d\u7684\u8bc4\u4f30\u65b9\u6cd5\u503e\u5411\u4e8e\u4ea7\u751f\u5047\u9633\u6027\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u7ed3\u8bba\u662f\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u8bc4\u4f30\u6307\u6807\u548c\u65b9\u6cd5\u4ee5\u53ca\u4e0d\u540c\u7684\u7f16\u8f91\u6279\u6b21\u5927\u5c0f\u4f1a\u5bfc\u81f4\u77e5\u8bc6\u7f16\u8f91\u5668\u6392\u540d\u4e0d\u540c\uff0c\u5e76\u4e14\u5728\u901a\u7528\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u4e5f\u89c2\u5bdf\u5230\u4e86\u8fd9\u79cd\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u624b\u52a8\u8bc4\u4f30\u663e\u793a\u57fa\u4e8e\u5b57\u7b26\u4e32\u5339\u914d\u7684\u8bc4\u4f30\u65b9\u6cd5\u503e\u5411\u4e8e\u4ea7\u751f\u5047\u9633\u6027\u7ed3\u679c\u3002"}}
{"id": "2507.05939", "pdf": "https://arxiv.org/pdf/2507.05939", "abs": "https://arxiv.org/abs/2507.05939", "authors": ["Bing Wang", "Ximing Li", "Mengzhe Ye", "Changchun Li", "Bo Fu", "Jianfeng Qu", "Lin Yuanbo Wu"], "title": "Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors", "categories": ["cs.CL", "cs.MM"], "comment": "Accepted by ACM MM 2025. 10 pages, 6 figures. Code:\n  https://github.com/wangbing1416/DAEDCMD", "summary": "Nowadays, misinformation articles, especially multimodal ones, are widely\nspread on social media platforms and cause serious negative effects. To control\ntheir propagation, Multimodal Misinformation Detection (MMD) becomes an active\ntopic in the community to automatically identify misinformation. Previous MMD\nmethods focus on supervising detectors by collecting offline data. However, in\nreal-world scenarios, new events always continually emerge, making MMD models\ntrained on offline data consistently outdated and ineffective. To address this\nissue, training MMD models under online data streams is an alternative,\ninducing an emerging task named continual MMD. Unfortunately, it is hindered by\ntwo major challenges. First, training on new data consistently decreases the\ndetection performance on past data, named past knowledge forgetting. Second,\nthe social environment constantly evolves over time, affecting the\ngeneralization on future data. To alleviate these challenges, we propose to\nremember past knowledge by isolating interference between event-specific\nparameters with a Dirichlet process-based mixture-of-expert structure, and\nanticipate future environmental distributions by learning a continuous-time\ndynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.\nExtensive experiments demonstrate that DAEDCMD can consistently and\nsignificantly outperform the compared methods, including six MMD baselines and\nthree continual learning methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.05940", "pdf": "https://arxiv.org/pdf/2507.05940", "abs": "https://arxiv.org/abs/2507.05940", "authors": ["Sandeep Mishra", "Anubhab Mandal", "Bishal Santra", "Tushar Abhishek", "Pawan Goyal", "Manish Gupta"], "title": "Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems", "categories": ["cs.CL"], "comment": null, "summary": "Ghosting, the ability to predict a user's intended text input for inline\nquery auto-completion, is an invaluable feature for modern search engines and\nchat interfaces, greatly enhancing user experience. By suggesting completions\nto incomplete queries (or prefixes), ghosting aids users with slow typing\nspeeds, disabilities, or limited language proficiency. Ghosting is a\nchallenging problem and has become more important with the ubiquitousness of\nchat-based systems like ChatGPT, Copilot, etc. Despite the increasing\nprominence of chat-based systems utilizing ghosting, this challenging problem\nof Chat-Ghosting has received little attention from the NLP/ML research\ncommunity. There is a lack of standardized benchmarks and relative performance\nanalysis of deep learning and non-deep learning methods. We address this\nthrough an open and thorough study of this problem using four publicly\navailable dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and\ntwo human-bot (Open Assistant and ShareGPT). We experiment with various\nexisting query auto-completion methods (using tries), n-gram methods and deep\nlearning methods, with and without dialog context. We also propose a novel\nentropy-based dynamic early stopping strategy. Our analysis finds that\nstatistical n-gram models and tries outperform deep learning based models in\nterms of both model performance and inference efficiency for seen prefixes. For\nunseen queries, neural models like T5 and Phi-2 lead to better results. Adding\nconversational context leads to significant improvements in ghosting quality,\nespecially for Open-Assistant and ShareGPT. We make code and data publicly\navailable", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u804a\u5929\u7cfb\u7edf\u4e2d\u7684ghosting\u95ee\u9898\uff0c\u901a\u8fc7\u56db\u4e2a\u516c\u5f00\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u81ea\u52a8\u8865\u5168\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u63d0\u524d\u505c\u6b62\u7b56\u7565\u3002\u7ed3\u679c\u663e\u793a\uff0c\u7edf\u8ba1\u65b9\u6cd5\u5728\u5df2\u89c1\u524d\u7f00\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u672a\u89c1\u67e5\u8be2\u4e0a\u66f4\u4f18\u3002\u6dfb\u52a0\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002", "motivation": "Ghosting\u662f\u73b0\u4ee3\u641c\u7d22\u5f15\u64ce\u548c\u804a\u5929\u754c\u9762\u4e2d\u4e00\u4e2a\u91cd\u8981\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u6781\u5927\u5730\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u804a\u5929\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528ghosting\uff0c\u4f46\u8fd9\u4e00\u95ee\u9898\u5728NLP/ML\u7814\u7a76\u793e\u533a\u4e2d\u5374\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u3002\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u548c\u76f8\u5bf9\u6027\u80fd\u5206\u6790\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u56db\u4e2a\u516c\u5f00\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\uff08\u4e24\u4e2a\u4eba\u7c7b-\u4eba\u7c7b\uff1aDailyDialog\u548cDSTC7-Ubuntu\uff0c\u4e24\u4e2a\u4eba\u7c7b-\u673a\u5668\u4eba\uff1aOpen Assistant\u548cShareGPT\uff09\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u5f00\u653e\u548c\u5168\u9762\u7684\u7814\u7a76\u3002\u6211\u4eec\u5b9e\u9a8c\u4e86\u5404\u79cd\u73b0\u6709\u7684\u67e5\u8be2\u81ea\u52a8\u8865\u5168\u65b9\u6cd5\uff08\u4f7f\u7528tries\uff09\u3001n-gram\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u65e0\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u63d0\u524d\u505c\u6b62\u7b56\u7565\u3002", "result": "\u7edf\u8ba1n-gram\u6a21\u578b\u548ctries\u5728\u5df2\u89c1\u524d\u7f00\u7684\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u3002\u5bf9\u4e8e\u672a\u89c1\u67e5\u8be2\uff0c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5982T5\u548cPhi-2\u8868\u73b0\u66f4\u597d\u3002\u6dfb\u52a0\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u9ad8\u4e86ghosting\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728Open-Assistant\u548cShareGPT\u4e0a\u3002", "conclusion": "\u6211\u4eec\u7684\u5206\u6790\u53d1\u73b0\uff0c\u7edf\u8ba1n-gram\u6a21\u578b\u548ctries\u5728\u5df2\u89c1\u524d\u7f00\u7684\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u3002\u5bf9\u4e8e\u672a\u89c1\u67e5\u8be2\uff0c\u50cfT5\u548cPhi-2\u8fd9\u6837\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002\u6dfb\u52a0\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u9ad8\u4e86ghosting\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728Open-Assistant\u548cShareGPT\u4e0a\u3002\u6211\u4eec\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6570\u636e\u3002"}}
{"id": "2507.05965", "pdf": "https://arxiv.org/pdf/2507.05965", "abs": "https://arxiv.org/abs/2507.05965", "authors": ["Lucas Fonseca Lage", "Simon Ostermann"], "title": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to EMNLP 2025 System Demonstrations track", "summary": "We introduce OpenFActScore, an open-source implementation of the FActScore\nframework for evaluating the factuality of text generated by large language\nmodels (LLMs). FActScore evaluates the factual accuracy of long-form text by\nusing Atomic Fact Generation (AFG) to extract individual factual claims and\nAtomic Fact Validation (AFV) to verify each claim against a trusted knowledge\nsource. While the original FActScore relies on closed-source and commercial\nmodels such as InstructGPT and ChatGPT, OpenFActScore enables the use of any\nHugging Face-compatible model for both AFG and AFV. We provide a detailed\ntechnical overview of our implementation, highlighting design choices and\nmodifications made to support open models. We evaluate multiple open-source\nLLMs on both AFG and AFV using the original FActScore benchmark, reporting\nBERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our\nresults show that open models can approximate the performance of closed-source\nsystems, with Gemma achieving the best overall performance, and our final setup\nobtains a 0.99 Pearson correlation with the original FActScore experiments.\nOpenFActScore promotes transparency, reproducibility, and cost-effective\nevaluation, and is available at: https://github.com/lflage/OpenFActScore.", "AI": {"tldr": "OpenFActScore is an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models. It enables the use of any Hugging Face-compatible model for both AFG and AFV and shows that open models can approximate the performance of closed-source systems.", "motivation": "The original FActScore relies on closed-source and commercial models, which limits transparency and reproducibility. OpenFActScore aims to enable the use of any Hugging Face-compatible model for both AFG and AFV.", "method": "OpenFActScore is an open-source implementation of the FActScore framework that uses Atomic Fact Generation (AFG) and Atomic Fact Validation (AFV) to evaluate the factual accuracy of text generated by large language models (LLMs).", "result": "OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. The results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments.", "conclusion": "OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation."}}
{"id": "2507.05973", "pdf": "https://arxiv.org/pdf/2507.05973", "abs": "https://arxiv.org/abs/2507.05973", "authors": ["Ehud Reiter"], "title": "We Should Evaluate Real-World Impact", "categories": ["cs.CL"], "comment": "This paper will appear in Computational Linguistics journal as a\n  \"Last Word\" opinion piece. The Arxiv version is a pre-MIT Press publication\n  version", "summary": "The ACL community has very little interest in evaluating the real-world\nimpact of NLP systems. A structured survey of the ACL Anthology shows that\nperhaps 0.1% of its papers contain such evaluations; furthermore most papers\nwhich include impact evaluations present them very sketchily and instead focus\non metric evaluations. NLP technology would be more useful and more quickly\nadopted if we seriously tried to understand and evaluate its real-world impact.", "AI": {"tldr": "The ACL community rarely evaluates the real-world impact of NLP systems, and this needs to change for NLP technology to be more useful and adopted.", "motivation": "The ACL community has very little interest in evaluating the real-world impact of NLP systems.", "method": "A structured survey of the ACL Anthology.", "result": "Perhaps 0.1% of the papers in the ACL Anthology contain such evaluations; furthermore, most papers which include impact evaluations present them very sketchily and instead focus on metric evaluations.", "conclusion": "NLP technology would be more useful and more quickly adopted if we seriously tried to understand and evaluate its real-world impact."}}
{"id": "2507.05980", "pdf": "https://arxiv.org/pdf/2507.05980", "abs": "https://arxiv.org/abs/2507.05980", "authors": ["Gabriel Chua", "Leanne Tan", "Ziyu Ge", "Roy Ka-Wei Lee"], "title": "RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RabakBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u65b0\u52a0\u5761\u72ec\u7279\u8bed\u8a00\u73af\u5883\u7684\u591a\u8bed\u8a00\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e86Singlish\u3001\u4e2d\u6587\u3001\u9a6c\u6765\u8bed\u548c\u6cf0\u7c73\u5c14\u8bed\u3002RabakBench\u901a\u8fc7\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\u6784\u5efa\uff1a(i) \u751f\u6210 - \u901a\u8fc7\u4f7f\u7528LLM\u9a71\u52a8\u7684\u7ea2\u961f\u6280\u672f\u589e\u5f3a\u771f\u5b9e\u7684Singlish\u7f51\u7edc\u5185\u5bb9\u6765\u751f\u6210\u5bf9\u6297\u6027\u793a\u4f8b\uff1b(ii) \u6807\u6ce8 - \u4f7f\u7528\u591a\u6570\u6295\u7968\u7684LLM\u6807\u6ce8\u5668\u8fdb\u884c\u534a\u81ea\u52a8\u5316\u591a\u6807\u7b7e\u5b89\u5168\u6807\u6ce8\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\uff1b(iii) \u7ffb\u8bd1 - \u4fdd\u6301\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u548c\u6bd2\u6027\u7684\u540c\u65f6\u8fdb\u884c\u9ad8\u4fdd\u771f\u7ffb\u8bd1\u3002\u6700\u7ec8\u6570\u636e\u96c6\u5305\u542b\u56db\u4e2a\u8bed\u8a00\u548c\u516d\u4e2a\u7ec6\u7c92\u5ea6\u5b89\u5168\u7c7b\u522b\u4e2d\u76845000\u591a\u4e2a\u5b89\u5168\u6807\u8bb0\u793a\u4f8b\u3002\u8bc4\u4f3011\u4e2a\u6d41\u884c\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u9632\u62a4\u5206\u7c7b\u5668\u663e\u793a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002RabakBench\u4e0d\u4ec5\u5728\u4e1c\u5357\u4e9a\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5b89\u5168\u8bc4\u4f30\uff0c\u8fd8\u4e3a\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u6784\u5efa\u672c\u5730\u5316\u5b89\u5168\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u5236\u7684\u6846\u67b6\u3002\u8be5\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u62ec\u4eba\u5de5\u9a8c\u8bc1\u7684\u7ffb\u8bd1\u548c\u8bc4\u4f30\u4ee3\u7801\uff0c\u90fd\u662f\u516c\u5f00\u7684\u3002", "motivation": "Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks.", "method": "RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages.", "result": "The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation.", "conclusion": "RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available."}}
{"id": "2507.05991", "pdf": "https://arxiv.org/pdf/2507.05991", "abs": "https://arxiv.org/abs/2507.05991", "authors": ["Minghang Zhu", "Shen Gao", "Zhengliang Shi", "Jiabao Fang", "Pengjie Ren", "Zhaochun Ren", "Zhumin Chen", "Shuo Shang"], "title": "Evolution without Large Models: Training Language Model with Task Principles", "categories": ["cs.CL"], "comment": null, "summary": "A common training approach for language models involves using a large-scale\nlanguage model to expand a human-provided dataset, which is subsequently used\nfor model training.This method significantly reduces training costs by\neliminating the need for extensive human data annotation. However, it still\nfaces challenges such as high carbon emissions during data augmentation and the\nrisk of data leakage when we use closed-source LLMs. To address these issues,\nwe propose a self-evolution method for language models. First, we introduce the\nMulti-level Principle Generation, which enables a large-scale model to\nsummarize task-completion principles based on a small amount of task data.\nThen, we propose the Principle-based Instance Generation, in which a\nsmaller-scale language model uses these task principles to generate a large\namount of data. This data is then used for model training. Experimental results\nshow that our proposed method significantly improves model performance compared\nto directly using a smaller-scale language model to generate data.\nAdditionally, since we only use the large-scale language model to generate the\ntask-completion principles, the carbon emissions associated with training the\nmodel are greatly reduced.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ea7\u539f\u7406\u751f\u6210\u548c\u57fa\u4e8e\u539f\u7406\u7684\u5b9e\u4f8b\u751f\u6210\u6765\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u78b3\u6392\u653e\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u8bad\u7ec3\u6210\u672c\uff0c\u4f46\u9762\u4e34\u6570\u636e\u589e\u5f3a\u8fc7\u7a0b\u4e2d\u7684\u9ad8\u78b3\u6392\u653e\u548c\u4f7f\u7528\u5c01\u95ed\u6e90\u4ee3\u7801LLM\u65f6\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u591a\u7ea7\u539f\u7406\u751f\u6210\u548c\u57fa\u4e8e\u539f\u7406\u7684\u5b9e\u4f8b\u751f\u6210\u3002\u591a\u7ea7\u539f\u7406\u751f\u6210\u4f7f\u5927\u89c4\u6a21\u6a21\u578b\u80fd\u591f\u6839\u636e\u5c11\u91cf\u4efb\u52a1\u6570\u636e\u603b\u7ed3\u4efb\u52a1\u5b8c\u6210\u539f\u5219\uff0c\u7136\u540e\u57fa\u4e8e\u539f\u7406\u7684\u5b9e\u4f8b\u751f\u6210\u4f7f\u7528\u8fd9\u4e9b\u4efb\u52a1\u539f\u5219\u751f\u6210\u5927\u91cf\u6570\u636e\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u76f4\u63a5\u4f7f\u7528\u8f83\u5c0f\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6570\u636e\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u4ec5\u4f7f\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u52a1\u5b8c\u6210\u539f\u5219\uff0c\u8bad\u7ec3\u6a21\u578b\u76f8\u5173\u7684\u78b3\u6392\u653e\u91cf\u5927\u5927\u51cf\u5c11\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6a21\u578b\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528\u8f83\u5c0f\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u7531\u4e8e\u53ea\u4f7f\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u52a1\u5b8c\u6210\u539f\u5219\uff0c\u8bad\u7ec3\u6a21\u578b\u76f8\u5173\u7684\u78b3\u6392\u653e\u91cf\u5927\u5927\u51cf\u5c11\u3002"}}
{"id": "2507.05997", "pdf": "https://arxiv.org/pdf/2507.05997", "abs": "https://arxiv.org/abs/2507.05997", "authors": ["Nicholas Popovi\u010d", "Ashish Kangen", "Tim Schopf", "Michael F\u00e4rber"], "title": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations", "categories": ["cs.CL"], "comment": null, "summary": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7ba1\u9053\uff0c\u4ee5\u89e3\u51b3\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6587\u6863\u7ea7\u522b\u5b9e\u4f53\u548c\u5173\u7cfb\u63d0\u53d6\u95ee\u9898\u3002", "motivation": "\u5728\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0c\u9ad8\u8d28\u91cf\u7684\u6ce8\u91ca\u8bed\u6599\u5e93\u4ecd\u7136\u7a00\u7f3a\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u52a8\u6ce8\u91ca\u7684\u793a\u4f8b\u6216\u76f4\u63a5\u7684\u96f6\u6837\u672c\u63a8\u7406\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u68c0\u7d22-based\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7ba1\u9053\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u4e0e\u57fa\u4e8e\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a\u4f18\u5316\u63a8\u7406\u7684\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u57fa\u4e8e\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc75k\u4e2a\u7ef4\u57fa\u767e\u79d1\u6458\u8981\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5927\u7ea6\u670959k\u4e2a\u5b9e\u4f53\u548c30k\u4e2a\u5173\u7cfb\u4e09\u5143\u7ec4\u3002\u6b64\u5916\uff0c\u5728DocIE\u5171\u4eab\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u8868\u73b0\u3002", "conclusion": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u6587\u6863\u7ea7\u522b\u7684\u8054\u5408\u5b9e\u4f53\u548c\u5173\u7cfb\u63d0\u53d6\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5373\u4f7f\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2507.06016", "pdf": "https://arxiv.org/pdf/2507.06016", "abs": "https://arxiv.org/abs/2507.06016", "authors": ["Youmna Farag", "Svetlana Stoyanchev", "Mohan Li", "Simon Keizer", "Rama Doddipatla"], "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents", "categories": ["cs.CL"], "comment": "Accepted at REALM 2025", "summary": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u6b21\u94fe\u63d0\u793a\u7684\u6761\u4ef6\u591a\u9636\u6bb5\u6545\u969c\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u6765\u5206\u6790\u6267\u884c\u6311\u6218\u5e76\u5236\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u5728TEACH\u6570\u636e\u96c6\u7684TfD\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5177\u8eab\u4ee3\u7406\u6267\u884c\u590d\u6742\u4efb\u52a1\u5bb9\u6613\u51fa\u73b0\u6267\u884c\u5931\u8d25\uff0c\u8fd9\u4fc3\u4f7f\u9700\u8981\u6709\u6548\u7684\u6545\u969c\u6062\u590d\u673a\u5236\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u6761\u4ef6\u591a\u9636\u6bb5\u6545\u969c\u6062\u590d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u96f6\u6b21\u94fe\u63d0\u793a\u3002\u6846\u67b6\u5206\u4e3a\u56db\u4e2a\u9519\u8bef\u5904\u7406\u9636\u6bb5\uff0c\u5176\u4e2d\u4e09\u4e2a\u5728\u4efb\u52a1\u6267\u884c\u671f\u95f4\u8fd0\u884c\uff0c\u4e00\u4e2a\u4f5c\u4e3a\u6267\u884c\u540e\u7684\u53cd\u601d\u9636\u6bb5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u5206\u6790\u73af\u5883\u4e2d\u7684\u6267\u884c\u6311\u6218\u5e76\u5236\u5b9a\u6218\u7565\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6211\u4eec\u5728TEACH\u6570\u636e\u96c6\u7684TfD\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u6ca1\u6709\u9519\u8bef\u6062\u590d\u7684\u57fa\u7ebf11.5%\uff0c\u5e76\u4e14\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5f3a\u6a21\u578b19%\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728TEACH\u6570\u636e\u96c6\u7684TfD\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u6ca1\u6709\u9519\u8bef\u6062\u590d\u7684\u57fa\u7ebf11.5%\uff0c\u5e76\u4e14\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5f3a\u6a21\u578b19%\u3002"}}
{"id": "2507.06056", "pdf": "https://arxiv.org/pdf/2507.06056", "abs": "https://arxiv.org/abs/2507.06056", "authors": ["Yizhan Huang", "Zhe Yang", "Meifang Chen", "Jianping Zhang", "Michael R. Lyu"], "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8bad\u7ec3\u6570\u636e\u7684\u8bb0\u5fc6\u96be\u5ea6\uff0c\u63d0\u51fa\u4e86\u71b5-\u8bb0\u5fc6\u5b9a\u5f8b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u533a\u5206\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u8868\u5f81\u8bad\u7ec3\u6570\u636e\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u96be\u5ea6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728OLMo\u7cfb\u5217\u5f00\u6e90\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4e86\u71b5-\u8bb0\u5fc6\u5b9a\u5f8b\uff0c\u5e76\u91c7\u7528\u76f8\u540c\u7b56\u7565\u53d1\u73b0\u4e86\u6570\u636e\u96c6\u63a8\u65ad\u65b9\u6cd5\u3002", "result": "\u6570\u636e\u71b5\u4e0e\u8bb0\u5fc6\u5206\u6570\u5448\u7ebf\u6027\u76f8\u5173\uff0c\u9ad8\u5ea6\u968f\u673a\u7684\u5b57\u7b26\u4e32\uff08\u5373\u65e0\u610f\u4e49\u5b57\u7b26\u4e32\uff09\u8868\u73b0\u51fa\u6bd4\u66f4\u5e7f\u6cdb\u7684\u8bad\u7ec3\u8bed\u6599\u5e93\u66f4\u4f4e\u7684\u5b9e\u8bc1\u71b5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6709\u6548\u65b9\u6cd5\u6765\u533a\u5206\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\uff0c\u5373\u6570\u636e\u96c6\u63a8\u65ad\uff08DI\uff09\u3002"}}
{"id": "2507.06085", "pdf": "https://arxiv.org/pdf/2507.06085", "abs": "https://arxiv.org/abs/2507.06085", "authors": ["Zongqian Li", "Yixuan Su", "Nigel Collier"], "title": "A Survey on Prompt Tuning", "categories": ["cs.CL"], "comment": null, "summary": "This survey reviews prompt tuning, a parameter-efficient approach for\nadapting language models by prepending trainable continuous vectors while\nkeeping the model frozen. We classify existing approaches into two categories:\ndirect prompt learning and transfer learning. Direct prompt learning methods\ninclude: general optimization approaches, encoder-based methods, decomposition\nstrategies, and mixture-of-experts frameworks. Transfer learning methods\nconsist of: general transfer approaches, encoder-based methods, and\ndecomposition strategies. For each method, we analyze method designs,\ninnovations, insights, advantages, and disadvantages, with illustrative\nvisualizations comparing different frameworks. We identify challenges in\ncomputational efficiency and training stability, and discuss future directions\nin improving training robustness and broadening application scope.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u76f4\u63a5\u63d0\u793a\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u4e24\u7c7b\uff0c\u5e76\u5bf9\u6bcf\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7efc\u8ff0\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u4ee5\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u7528\u8fd9\u4e00\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5c06\u73b0\u6709\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u5206\u4e3a\u76f4\u63a5\u63d0\u793a\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u4e24\u7c7b\uff0c\u5e76\u5bf9\u6bcf\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u672c\u6587\u5bf9\u4e0d\u540c\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e86\u6bd4\u8f83\u4e0d\u540c\u6846\u67b6\u7684\u53ef\u89c6\u5316\u793a\u4f8b\u3002", "conclusion": "\u672c\u6587\u7efc\u8ff0\u4e86\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u8bbe\u8ba1\u3001\u521b\u65b0\u3001\u89c1\u89e3\u3001\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u8ba8\u8bba\u4e86\u672a\u6765\u6539\u8fdb\u8bad\u7ec3\u9c81\u68d2\u6027\u548c\u6269\u5c55\u5e94\u7528\u8303\u56f4\u7684\u65b9\u5411\u3002"}}
{"id": "2507.06137", "pdf": "https://arxiv.org/pdf/2507.06137", "abs": "https://arxiv.org/abs/2507.06137", "authors": ["Mohammad Mahdi Derakhshani", "Dheeraj Varghese", "Marzieh Fadaee", "Cees G. M. Snoek"], "title": "NeoBabel: A Multilingual Open Tower for Visual Generation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "34 pages, 12 figures", "summary": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86NeoBabel\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u8bed\u8a00\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u5b83\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u5305\u5bb9\u6027\u65b9\u9762\u8bbe\u5b9a\u4e86\u65b0\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u652f\u6301\u516d\u79cd\u8bed\u8a00\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u9ad8\u5206\u8fa8\u7387\u6307\u4ee4\u8c03\u4f18\u8fdb\u884c\u8bad\u7ec3\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cNeoBabel\u5728\u591a\u8bed\u8a00\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u82f1\u8bed\u80fd\u529b\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8fdb\u6b65\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u8fd9\u7ed9\u975e\u82f1\u8bed\u4f7f\u7528\u8005\u5e26\u6765\u4e86\u969c\u788d\uff0c\u5e76\u52a0\u5267\u4e86\u6570\u5b57\u4e0d\u5e73\u7b49\u3002\u73b0\u6709\u7684\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u7ffb\u8bd1\u7ba1\u9053\uff0c\u8fd9\u4e9b\u7ba1\u9053\u5f15\u5165\u4e86\u8bed\u4e49\u6f02\u79fb\u3001\u8ba1\u7b97\u5f00\u9500\u548c\u6587\u5316\u504f\u5dee\u3002", "method": "NeoBabel\u6846\u67b6\u7ed3\u5408\u4e86\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u9ad8\u5206\u8fa8\u7387\u6307\u4ee4\u8c03\u4f18\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "NeoBabel\u5728\u591a\u8bed\u8a00\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u82f1\u8bed\u80fd\u529b\uff0c\u5728m-GenEval\u4e0a\u5f97\u5206\u4e3a0.75\uff0c\u5728m-DPG\u4e0a\u5f97\u5206\u4e3a0.68\u3002\u5b83\u5728\u82f1\u8bed\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0e\u9886\u5148\u6a21\u578b\u76f8\u5f53\uff0c\u800c\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u4f18\u4e8e\u5b83\u4eec+0.11\u548c+0.09\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u591a\u8bed\u8a00\u80fd\u529b\u4e0d\u662f\u4e00\u79cd\u6743\u8861\uff0c\u800c\u662f\u6539\u8fdb\u751f\u6210\u5f0fAI\u7684\u7a33\u5065\u6027\u3001\u6548\u7387\u548c\u6587\u5316\u7cbe\u786e\u6027\u7684\u50ac\u5316\u5242\u3002"}}
{"id": "2507.06138", "pdf": "https://arxiv.org/pdf/2507.06138", "abs": "https://arxiv.org/abs/2507.06138", "authors": ["Taolin Zhang", "Zihan Ma", "Maosong Cao", "Junnan Liu", "Songyang Zhang", "Kai Chen"], "title": "Coding Triangle: How Does Large Language Model Understand Code?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Code Triangle\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136LLMs\u80fd\u591f\u5f62\u6210\u81ea\u6d3d\u7684\u7cfb\u7edf\uff0c\u4f46\u5176\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u591a\u6837\u6027\u4e0e\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u751f\u6210\u7684\u5185\u5bb9\u548c\u6a21\u578b\u6df7\u5408\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLMs\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u771f\u6b63\u7684\u7f16\u7a0b\u80fd\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u6211\u4eec\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLMs\u7684\u7f16\u7a0b\u80fd\u529b\uff0c\u4ee5\u53d1\u73b0\u5176\u5c40\u9650\u6027\u5e76\u5bfb\u627e\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86Code Triangle\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u4ece\u4e09\u4e2a\u57fa\u672c\u7ef4\u5ea6\u8bc4\u4f30LLMs\uff1a\u7f16\u8f91\u5206\u6790\u3001\u4ee3\u7801\u5b9e\u73b0\u548c\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u3002\u901a\u8fc7\u5728\u7ade\u8d5b\u7f16\u7a0b\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u5206\u6790\u4e86LLMs\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMs\u53ef\u4ee5\u5728\u8fd9\u4e9b\u7ef4\u5ea6\u4e0a\u5f62\u6210\u81ea\u6d3d\u7684\u7cfb\u7edf\uff0c\u4f46\u5176\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u7f3a\u4e4f\u4eba\u7c7b\u7a0b\u5e8f\u5458\u7684\u591a\u6837\u6027\u548c\u9c81\u68d2\u6027\u3002\u6a21\u578b\u9519\u8bef\u503e\u5411\u4e8e\u56e0\u8bad\u7ec3\u6570\u636e\u504f\u5dee\u548c\u6709\u9650\u7684\u63a8\u7406\u8f6c\u79fb\u800c\u805a\u96c6\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5c55\u793a\u4e86\u5c06\u4eba\u7c7b\u751f\u6210\u7684\u7f16\u8f91\u5668\u6587\u7ae0\u3001\u89e3\u51b3\u65b9\u6848\u548c\u591a\u6837\u7684\u6d4b\u8bd5\u7528\u4f8b\u76f8\u7ed3\u5408\uff0c\u4ee5\u53ca\u5229\u7528\u6a21\u578b\u6df7\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8LLMs\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63ed\u793a\u4e86LLMs\u5728\u8ba4\u77e5\u4e0a\u7684\u4e00\u81f4\u6027\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u6539\u8fdb\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u7f16\u7801\u6a21\u578b\u63d0\u4f9b\u4e86\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2507.06167", "pdf": "https://arxiv.org/pdf/2507.06167", "abs": "https://arxiv.org/abs/2507.06167", "authors": ["Wei Shen", "Jiangbo Pei", "Yi Peng", "Xuchen Song", "Yang Liu", "Jian Peng", "Haofeng Sun", "Yunzhuo Hao", "Peiyu Wang", "Yahui Zhou"], "title": "Skywork-R1V3 Technical Report", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.", "AI": {"tldr": "Skywork-R1V3 is an advanced, open-source vision-language model that transfers reasoning skills from text-only LLMs to visual tasks using a post-training RL framework. It achieves state-of-the-art results on MMMU and showcases RL as a powerful engine for advancing open-source VLM capabilities.", "motivation": "The motivation is to transfer reasoning skills from text-only Large Language Models (LLMs) to visual tasks and to improve the performance of multimodal reasoning models.", "method": "The paper introduces a post-training RL framework that effectively activates and enhances the model's reasoning ability without additional continue pre-training. It also introduces a unique indicator of reasoning capability, the entropy of critical reasoning tokens, and analyzes curriculum learning and reinforcement finetuning strategies.", "result": "Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%, which matches entry-level human capabilities. The RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs.", "conclusion": "Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities."}}
{"id": "2507.06181", "pdf": "https://arxiv.org/pdf/2507.06181", "abs": "https://arxiv.org/abs/2507.06181", "authors": ["Zhongyuan Peng", "Yifan Yao", "Kaijing Ma", "Shuyue Guo", "Yizhe Li", "Yichi Zhang", "Chenchen Zhang", "Yifan Zhang", "Zhouliang Yu", "Luming Li", "Minghao Liu", "Yihang Xia", "Jiawei Shen", "Yuchen Wu", "Yixin Cao", "Zhaoxiang Zhang", "Wenhao Huang", "Jiaheng Liu", "Ge Zhang"], "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization", "categories": ["cs.CL"], "comment": null, "summary": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 CriticLean \u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u8bc4\u5224\u9636\u6bb5\u7684\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f62\u5f0f\u5316\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u548c\u7f16\u8bd1\u7684\u6210\u529f\uff0c\u800c\u8f83\u5c11\u5173\u6ce8\u8bc4\u5224\u9636\u6bb5\uff0c\u5373\u8bc4\u4f30\u751f\u6210\u7684\u5f62\u5f0f\u5316\u662f\u5426\u771f\u6b63\u6355\u6349\u539f\u59cb\u95ee\u9898\u7684\u8bed\u4e49\u610f\u56fe\u3002", "method": "\u5f15\u5165\u4e86 CriticLean \u6846\u67b6\uff0c\u5305\u62ec CriticLeanGPT \u548c CriticLeanBench\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f62\u5f0f\u5316\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6 FineLeanCorpus\u3002", "result": "CriticLeanGPT \u5728\u533a\u5206\u8bed\u4e49\u6b63\u786e\u548c\u9519\u8bef\u7684\u5f62\u5f0f\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6 FineLeanCorpus\u3002", "conclusion": "\u4f18\u5316\u8bc4\u5224\u9636\u6bb5\u5bf9\u4e8e\u751f\u6210\u53ef\u9760\u7684\u6b63\u5f0f\u5316\u81f3\u5173\u91cd\u8981\uff0cCriticLean \u4e3a\u672a\u6765\u7684\u5f62\u5f0f\u5316\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.06189", "pdf": "https://arxiv.org/pdf/2507.06189", "abs": "https://arxiv.org/abs/2507.06189", "authors": ["Maximilian Heil", "Dionne Bang"], "title": "DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents our submission to Task 1, Subjectivity Detection, of the\nCheckThat! Lab at CLEF 2025. We investigate the effectiveness of\ntransfer-learning and stylistic data augmentation to improve classification of\nsubjective and objective sentences in English news text. Our approach contrasts\nfine-tuning of pre-trained encoders and transfer-learning of fine-tuned\ntransformer on related tasks. We also introduce a controlled augmentation\npipeline using GPT-4o to generate paraphrases in predefined subjectivity\nstyles. To ensure label and style consistency, we employ the same model to\ncorrect and refine the generated samples. Results show that transfer-learning\nof specified encoders outperforms fine-tuning general-purpose ones, and that\ncarefully curated augmentation significantly enhances model robustness,\nespecially in detecting subjective content. Our official submission placed us\n$16^{th}$ of 24 participants. Overall, our findings underscore the value of\ncombining encoder specialization with label-consistent augmentation for\nimproved subjectivity detection. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-subject.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8fc1\u79fb\u5b66\u4e60\u548c\u98ce\u683c\u6570\u636e\u589e\u5f3a\u5728\u6539\u5584\u82f1\u8bed\u65b0\u95fb\u6587\u672c\u4e2d\u4e3b\u89c2\u548c\u5ba2\u89c2\u53e5\u5b50\u5206\u7c7b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u53d1\u73b0\u6307\u5b9a\u7f16\u7801\u5668\u7684\u8fc1\u79fb\u5b66\u4e60\u4f18\u4e8e\u901a\u7528\u7f16\u7801\u5668\u7684\u5fae\u8c03\uff0c\u540c\u65f6\u7cbe\u5fc3\u7b56\u5212\u7684\u589e\u5f3a\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63d0\u9ad8\u82f1\u8bed\u65b0\u95fb\u6587\u672c\u4e2d\u4e3b\u89c2\u548c\u5ba2\u89c2\u53e5\u5b50\u7684\u5206\u7c7b\u6548\u679c\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u98ce\u683c\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u6211\u4eec\u7814\u7a76\u4e86\u8fc1\u79fb\u5b66\u4e60\u548c\u98ce\u683c\u6570\u636e\u589e\u5f3a\u5728\u6539\u5584\u82f1\u8bed\u65b0\u95fb\u6587\u672c\u4e2d\u4e3b\u89c2\u548c\u5ba2\u89c2\u53e5\u5b50\u5206\u7c7b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u6bd4\u4e86\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u5fae\u8c03\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u5fae\u8c03\u53d8\u538b\u5668\u5728\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u4f7f\u7528GPT-4o\u751f\u6210\u9884\u5b9a\u4e3b\u89c2\u98ce\u683c\u7684\u6539\u5199\u8bed\u53e5\u7684\u53d7\u63a7\u589e\u5f3a\u7ba1\u9053\uff0c\u5e76\u91c7\u7528\u76f8\u540c\u6a21\u578b\u6765\u7ea0\u6b63\u548c\u4f18\u5316\u751f\u6210\u7684\u6837\u672c\u4ee5\u786e\u4fdd\u6807\u7b7e\u548c\u98ce\u683c\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6307\u5b9a\u7f16\u7801\u5668\u7684\u8fc1\u79fb\u5b66\u4e60\u4f18\u4e8e\u901a\u7528\u7f16\u7801\u5668\u7684\u5fae\u8c03\uff0c\u5e76\u4e14\u7ecf\u8fc7\u7cbe\u5fc3\u7b56\u5212\u7684\u589e\u5f3a\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u4e3b\u89c2\u5185\u5bb9\u65b9\u9762\u3002\u6211\u4eec\u7684\u5b98\u65b9\u63d0\u4ea4\u6392\u540d\u4e3a24\u540d\u4e2d\u7684\u7b2c16\u540d\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6307\u5b9a\u7f16\u7801\u5668\u7684\u8fc1\u79fb\u5b66\u4e60\u4f18\u4e8e\u901a\u7528\u7f16\u7801\u5668\u7684\u5fae\u8c03\uff0c\u5e76\u4e14\u7ecf\u8fc7\u7cbe\u5fc3\u7b56\u5212\u7684\u589e\u5f3a\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u4e3b\u89c2\u5185\u5bb9\u65b9\u9762\u3002\u7ed3\u5408\u7f16\u7801\u5668\u4e13\u4e1a\u5316\u4e0e\u6807\u7b7e\u4e00\u81f4\u589e\u5f3a\u7684\u4ef7\u503c\u5f97\u5230\u4e86\u8bc1\u5b9e\u3002"}}
{"id": "2507.06195", "pdf": "https://arxiv.org/pdf/2507.06195", "abs": "https://arxiv.org/abs/2507.06195", "authors": ["Maximilian Heil", "Aleksandar Pramov"], "title": "DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification", "categories": ["cs.CL"], "comment": null, "summary": "Numerical claims, statements involving quantities, comparisons, and temporal\nreferences, pose unique challenges for automated fact-checking systems. In this\nstudy, we evaluate modeling strategies for veracity prediction of such claims\nusing the QuanTemp dataset and building our own evidence retrieval pipeline. We\ninvestigate three key factors: (1) the impact of more evidences with longer\ninput context windows using ModernBERT, (2) the effect of right-to-left (R2L)\ntokenization, and (3) their combined influence on classification performance.\nContrary to prior findings in arithmetic reasoning tasks, R2L tokenization does\nnot boost natural language inference (NLI) of numerical tasks. A longer context\nwindow does also not enhance veracity performance either, highlighting evidence\nquality as the dominant bottleneck. Our best-performing system achieves\ncompetitive macro-average F1 score of 0.57 and places us among the Top-4\nsubmissions in Task 3 of CheckThat! 2025. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-numerical.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7528\u4e8e\u9a8c\u8bc1\u6570\u503c\u548c\u65f6\u95f4\u58f0\u660e\u771f\u4f2a\u7684\u5efa\u6a21\u7b56\u7565\uff0c\u53d1\u73b0\u8bc1\u636e\u8d28\u91cf\u548c\u6807\u8bb0\u5316\u65b9\u5f0f\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u6570\u503c\u58f0\u660e\u3001\u6d89\u53ca\u6570\u91cf\u7684\u9648\u8ff0\u3001\u6bd4\u8f83\u548c\u65f6\u95f4\u53c2\u8003\u5bf9\u4e8e\u81ea\u52a8\u4e8b\u5b9e\u68c0\u67e5\u7cfb\u7edf\u6765\u8bf4\u5177\u6709\u72ec\u7279\u7684\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u4f7f\u7528QuanTemp\u6570\u636e\u96c6\u5e76\u6784\u5efa\u81ea\u5df1\u7684\u8bc1\u636e\u68c0\u7d22\u7ba1\u9053\u6765\u8bc4\u4f30\u6570\u503c\u548c\u65f6\u95f4\u58f0\u660e\u7684\u771f\u4f2a\u9884\u6d4b\u5efa\u6a21\u7b56\u7565\u3002\u6211\u4eec\u7814\u7a76\u4e86\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\uff1a(1) \u4f7f\u7528ModernBERT\u7684\u66f4\u591a\u8bc1\u636e\u548c\u66f4\u957f\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u5f71\u54cd\uff0c(2) \u53f3\u5230\u5de6\uff08R2L\uff09\u6807\u8bb0\u5316\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca(3) \u5b83\u4eec\u5bf9\u5206\u7c7b\u6027\u80fd\u7684\u7efc\u5408\u5f71\u54cd\u3002", "result": "\u4e0e\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5148\u524d\u53d1\u73b0\u76f8\u53cd\uff0cR2L\u6807\u8bb0\u5316\u5e76\u672a\u63d0\u5347\u6570\u503c\u4efb\u52a1\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u3002\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e5f\u6ca1\u6709\u63d0\u9ad8\u771f\u4f2a\u6027\u80fd\uff0c\u8fd9\u7a81\u663e\u4e86\u8bc1\u636e\u8d28\u91cf\u662f\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u6211\u4eec\u7684\u6700\u4f73\u7cfb\u7edf\u5728Task 3 of CheckThat! 2025\u4e2d\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7684\u5b8f\u5e73\u5747F1\u5206\u65700.57\uff0c\u5e76\u4f4d\u5217\u524d\u56db\u540d\u3002"}}
{"id": "2507.06196", "pdf": "https://arxiv.org/pdf/2507.06196", "abs": "https://arxiv.org/abs/2507.06196", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan", "David Skarbrevik", "Ho-Kyeong Ra", "Viren Bajaj", "Zeya Ahmad"], "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to Journal of Machine Learning Research (MLOSS); UQLM\n  Repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs.", "AI": {"tldr": "UQLM \u662f\u4e00\u4e2a Python \u5305\uff0c\u7528\u4e8e\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u68c0\u6d4b LLM \u7684\u5e7b\u89c9\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u73b0\u6210\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u63d0\u9ad8 LLM \u8f93\u51fa\u7684\u53ef\u9760\u6027\u3002", "motivation": "LLM \u751f\u6210\u865a\u5047\u6216\u8bef\u5bfc\u6027\u5185\u5bb9\u7684\u95ee\u9898\u5f71\u54cd\u4e86\u4e0b\u6e38\u5e94\u7528\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "UQLM \u4f7f\u7528\u6700\u5148\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316 (UQ) \u6280\u672f\u6765\u68c0\u6d4b LLM \u7684\u5e7b\u89c9\u3002", "result": "UQLM \u63d0\u4f9b\u4e86\u4e00\u5957\u57fa\u4e8e UQ \u7684\u8bc4\u5206\u5668\uff0c\u53ef\u4ee5\u8ba1\u7b97\u4ece 0 \u5230 1 \u7684\u54cd\u5e94\u7ea7\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002", "conclusion": "UQLM \u63d0\u4f9b\u4e86\u4e00\u79cd\u73b0\u6210\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u5e7b\u89c9\u68c0\u6d4b\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u4ee5\u63d0\u9ad8 LLM \u8f93\u51fa\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.06203", "pdf": "https://arxiv.org/pdf/2507.06203", "abs": "https://arxiv.org/abs/2507.06203", "authors": ["Rui-Jie Zhu", "Tianhao Peng", "Tianhao Cheng", "Xingwei Qu", "Jinfa Huang", "Dawei Zhu", "Hao Wang", "Kaiwen Xue", "Xuanliang Zhang", "Yong Shan", "Tianle Cai", "Taylor Kergan", "Assel Kembay", "Andrew Smith", "Chenghua Lin", "Binh Nguyen", "Yuqi Pan", "Yuhong Chou", "Zefan Cai", "Zhenhe Wu", "Yongchi Zhao", "Tianyu Liu", "Jian Yang", "Wangchunshu Zhou", "Chujie Zheng", "Chongxuan Li", "Yuyin Zhou", "Zhoujun Li", "Zhaoxiang Zhang", "Jiaheng Liu", "Ge Zhang", "Wenhao Huang", "Jason Eshraghian"], "title": "A Survey on Latent Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9690\u5f0f\u63a8\u7406\u9886\u57df\uff0c\u63a2\u8ba8\u4e86\u5176\u57fa\u7840\u795e\u7ecf\u7f51\u7edc\u5c42\u7684\u4f5c\u7528\u3001\u591a\u79cd\u9690\u5f0f\u63a8\u7406\u65b9\u6cd5\u4ee5\u53ca\u9ad8\u7ea7\u8303\u5f0f\uff0c\u65e8\u5728\u63a8\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u8ba4\u77e5\u524d\u6cbf\u7684\u7814\u7a76\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u63a8\u8fdb\u9690\u5f0f\u63a8\u7406\u7814\u7a76\uff0c\u4ee5\u89e3\u51b3\u663e\u5f0f\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u8868\u8fbe\u5e26\u5bbd\u9650\u5236\u95ee\u9898\u3002", "method": "\u672c\u6587\u901a\u8fc7 examining the foundational role of neural network layers, exploring diverse latent reasoning methodologies, and discussing advanced paradigms such as infinite-depth latent reasoning via masked diffusion models\u6765\u63a8\u8fdb\u9690\u5f0f\u63a8\u7406\u7814\u7a76\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u9690\u5f0f\u63a8\u7406\u65b0\u5174\u9886\u57df\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u8ba8\u8bba\u4e86\u5305\u62ec\u57fa\u4e8e\u6fc0\u6d3b\u7684\u5faa\u73af\u3001\u9690\u85cf\u72b6\u6001\u4f20\u64ad\u548c\u5fae\u8c03\u7b56\u7565\u5728\u5185\u7684\u591a\u79cd\u9690\u5f0f\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7edf\u4e00\u8fd9\u4e9b\u89c2\u70b9\uff0c\u65e8\u5728\u6f84\u6e05\u9690\u5f0f\u63a8\u7406\u7684\u6982\u5ff5\u683c\u5c40\uff0c\u5e76\u52fe\u52d2\u51faLLM\u8ba4\u77e5\u524d\u6cbf\u7684\u7814\u7a76\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2507.06205", "pdf": "https://arxiv.org/pdf/2507.06205", "abs": "https://arxiv.org/abs/2507.06205", "authors": ["Ayush Parikh", "Hoang Thanh Thanh Truong", "Jeanette Schofield", "Maximilian Heil"], "title": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a\nScientific Web Discourse Detection, present the methods we explored for this\ntask. For this multiclass classification task, we determined if a tweet\ncontained a scientific claim, a reference to a scientific study or publication,\nand/or mentions of scientific entities, such as a university or a scientist. We\npresent 3 modeling approaches for this task: transformer finetuning, few-shot\nprompting of LLMs, and a combined ensemble model whose design was informed by\nearlier experiments. Our team placed 7th in the competition, achieving a\nmacro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline\nof 0.8375. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86DS@GT\u56e2\u961f\u5728CLEF 2025 CheckThat! Task 4a\u79d1\u5b66\u7f51\u7edc\u8bdd\u8bed\u68c0\u6d4b\u4efb\u52a1\u4e2d\u63a2\u7d22\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4e09\u79cd\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u6210\u7ee9\u3002", "motivation": "\u6211\u4eec\u65e8\u5728\u786e\u5b9a\u63a8\u6587\u662f\u5426\u5305\u542b\u79d1\u5b66\u58f0\u660e\u3001\u5bf9\u79d1\u5b66\u7814\u7a76\u6216\u51fa\u7248\u7269\u7684\u5f15\u7528\u4ee5\u53ca/\u6216\u63d0\u53ca\u79d1\u5b66\u5b9e\u4f53\uff0c\u5982\u5927\u5b66\u6216\u79d1\u5b66\u5bb6\u3002", "method": "\u6211\u4eec\u63a2\u7d22\u4e86\u4e09\u79cd\u5efa\u6a21\u65b9\u6cd5\uff1atransformer\u5fae\u8c03\u3001LLM\u7684\u5c11\u91cf\u63d0\u793a\u548c\u4e00\u4e2a\u7ed3\u5408\u6a21\u578b\uff0c\u5176\u8bbe\u8ba1\u57fa\u4e8e\u65e9\u671f\u5b9e\u9a8c\u3002", "result": "\u6211\u4eec\u7684\u56e2\u961f\u5728\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u4e86\u7b2c7\u540d\uff0c\u5b8f\u5e73\u5747F1\u5f97\u5206\u4e3a0.8611\uff0c\u6bd4DeBERTaV3\u57fa\u7ebf\u63d0\u9ad8\u4e860.8375\u3002", "conclusion": "\u6211\u4eec\u7684\u56e2\u961f\u5728\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u4e86\u7b2c7\u540d\uff0c\u5b8f\u5e73\u5747F1\u5f97\u5206\u4e3a0.8611\uff0c\u6bd4DeBERTaV3\u57fa\u7ebf\u63d0\u9ad8\u4e860.8375\u3002"}}
{"id": "2507.06223", "pdf": "https://arxiv.org/pdf/2507.06223", "abs": "https://arxiv.org/abs/2507.06223", "authors": ["Zhiyuan Peng", "Ting-ruen Wei", "Tingyu Song", "Yilun Zhao", "Yi Fang"], "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30LLM-based rerankers\u7684\u6548\u7387-\u6548\u679c\u6743\u8861\u7684\u65b9\u6cd5\uff0c\u5373E\u00b2R-FLOPs\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u786c\u4ef6\u548c\u8fd0\u884c\u65f6\u9009\u62e9\uff0c\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u6548\u7387-\u6548\u679c\u6743\u8861\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u3001\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86E\u00b2R-FLOPs\u6307\u6807\uff0c\u5305\u62ec\u6bcfPetaFLOP\u7684\u76f8\u5173\u6027\u6392\u540d\uff08RPP\uff09\u548c\u6bcfPetaFLOP\u7684\u67e5\u8be2\u6570\uff08QPP\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684FLOPs\u4f30\u7b97\u5668\u6765\u4f30\u8ba1LLM-based rerankers\u7684FLOPs\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u672c\u6587\u8bc4\u4f30\u4e86\u4e0d\u540c\u67b6\u6784\u7684LLM-based rerankers\uff0c\u5e76\u63ed\u793a\u4e86\u6548\u7387-\u6548\u679c\u6743\u8861\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30LLM-based rerankers\u7684\u6548\u7387-\u6548\u679c\u6743\u8861\u7684\u65b9\u6cd5\uff0c\u5373E\u00b2R-FLOPs\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.06229", "pdf": "https://arxiv.org/pdf/2507.06229", "abs": "https://arxiv.org/abs/2507.06229", "authors": ["Xiangru Tang", "Tianrui Qin", "Tianhao Peng", "Ziyang Zhou", "Daniel Shao", "Tingting Du", "Xinming Wei", "Peng Xia", "Fang Wu", "He Zhu", "Ge Zhang", "Jiaheng Liu", "Xingyao Wang", "Sirui Hong", "Chenglin Wu", "Hao Cheng", "Chi Wang", "Wangchunshu Zhou"], "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks.", "AI": {"tldr": "Agent KB is a hierarchical experience framework that enables complex agentic problem solving through a novel Reason-Retrieve-Refine pipeline, improving success rates in various tasks.", "motivation": "Language agents struggle with effective error correction and experience reuse across domains. Agents traditionally cannot learn from each other's experiences.", "method": "Agent KB is a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline.", "result": "Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%.", "conclusion": "Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks."}}
{"id": "2507.05279", "pdf": "https://arxiv.org/pdf/2507.05279", "abs": "https://arxiv.org/abs/2507.05279", "authors": ["Virgile Boraud", "Yannis Bendi-Ouis", "Paul Bernard", "Xavier Hinaut"], "title": "ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.NE"], "comment": null, "summary": "We introduce a tool designed to improve the capabilities of Large Language\nModels (LLMs) in assisting with code development using the ReservoirPy library,\nas well as in answering complex questions in the field of Reservoir Computing.\nBy incorporating external knowledge through Retrieval-Augmented Generation\n(RAG) and knowledge graphs, our approach aims to reduce hallucinations and\nincrease the factual accuracy of generated responses. The system provides an\ninteractive experience similar to ChatGPT, tailored specifically for\nReservoirPy, enabling users to write, debug, and understand Python code while\naccessing reliable domain-specific insights. In our evaluation, while\nproprietary models such as ChatGPT-4o and NotebookLM performed slightly better\non general knowledge questions, our model outperformed them on coding tasks and\nshowed a significant improvement over its base model, Codestral-22B.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5de5\u5177\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u5f00\u53d1\u548c\u56de\u7b54\u590d\u6742\u95ee\u9898\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\u6765\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u5f00\u53d1\u548c\u56de\u7b54\u590d\u6742\u95ee\u9898\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728Reservoir Computing\u9886\u57df\u3002", "method": "\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u77e5\u8bc6\u56fe\u8c31\u5f15\u5165\u5916\u90e8\u77e5\u8bc6\uff0c\u4ee5\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u751f\u6210\u54cd\u5e94\u7684\u51c6\u786e\u6027\u3002", "result": "\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7c7b\u4f3cChatGPT\u7684\u4ea4\u4e92\u4f53\u9a8c\uff0c\u4e13\u95e8\u9488\u5bf9ReservoirPy\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u7f16\u5199\u3001\u8c03\u8bd5\u548c\u7406\u89e3Python\u4ee3\u7801\uff0c\u5e76\u83b7\u53d6\u53ef\u9760\u7684\u9886\u57df\u7279\u5b9a\u89c1\u89e3\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u7f16\u7801\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eChatGPT-4o\u548cNotebookLM\uff0c\u5e76\u4e14\u76f8\u6bd4\u5176\u57fa\u7840\u6a21\u578bCodestral-22B\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2507.05281", "pdf": "https://arxiv.org/pdf/2507.05281", "abs": "https://arxiv.org/abs/2507.05281", "authors": ["Lingyue Fu", "Hao Guan", "Bolun Zhang", "Haowei Yuan", "Yaoming Zhu", "Jun Xu", "Zongyu Wang", "Lin Qiu", "Xunliang Cai", "Xuezhi Cao", "Weiwen Liu", "Weinan Zhang", "Yong Yu"], "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code\nprocessing capabilities, evaluating their performance on engineering-level code\nremains challenging. Existing repository-level benchmarks primarily focus on\nsingle scenarios, such as code generation or bug fixing, without adequately\ncapturing the diversity and complexity of real-world software or project\nengineering workflows. Furthermore, these benchmarks suffer from limited\ncontrollability in question positioning and reliability issues in their\ngenerated test cases. To address these limitations, we present CorePipe, a\nfully automated pipeline that converts repositories into comprehensive test\ncases, and introduce CoreCodeBench, a configurable multi-scenario\nrepository-level benchmark. To simulate real engineering scenarios, CorePipe\ngenerates three types of atomic questions (Development, BugFix, and Test-Driven\nDevelopment) specifically targeting core code segments. These atomic questions\nare further combined into three types of composite questions, with difficulty\nlevels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides\na comprehensive and extensive repository-level benchmark to investigate the\napplicability of LLMs in real-world engineering projects. Experiments with 16\nLLMs across diverse scenarios reveal varying capabilities and offer\nmulti-dimensional insights into LLM performance in engineering contexts. The\ncode for CorePipe is available at\nhttps://github.com/AGI-Eval-Official/CoreCodeBench, and the data for\nCoreCodeBench can be accessed at\nhttps://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCorePipe\u7684\u81ea\u52a8\u5316\u7ba1\u9053\u548c\u4e00\u4e2a\u540d\u4e3aCoreCodeBench\u7684\u53ef\u914d\u7f6e\u591a\u573a\u666f\u4ed3\u5e93\u7ea7\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5de5\u7a0b\u9879\u76ee\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "Existing repository-level benchmarks primarily focus on single scenarios, such as code generation or bug fixing, without adequately capturing the diversity and complexity of real-world software or project engineering workflows. Furthermore, these benchmarks suffer from limited controllability in question positioning and reliability issues in their generated test cases.", "method": "We present CorePipe, a fully automated pipeline that converts repositories into comprehensive test cases, and introduce CoreCodeBench, a configurable multi-scenario repository-level benchmark.", "result": "Experiments with 16 LLMs across diverse scenarios reveal varying capabilities and offer multi-dimensional insights into LLM performance in engineering contexts.", "conclusion": "CoreCodeBench provides a comprehensive and extensive repository-level benchmark to investigate the applicability of LLMs in real-world engineering projects."}}
{"id": "2507.05283", "pdf": "https://arxiv.org/pdf/2507.05283", "abs": "https://arxiv.org/abs/2507.05283", "authors": ["Yue Wang", "Miao Zhou", "Guijing Huang", "Rui Zhuo", "Chao Yi", "Zhenliang Ma"], "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Pre-timed traffic signal control, commonly used for operating signalized\nintersections and coordinated arterials, requires tedious manual work for\nsignaling plan creating and updating. When the time-of-day or day-of-week plans\nare utilized, one intersection is often associated with multiple plans, leading\nto further repetitive manual plan parameter inputting. To enable a\nuser-friendly traffic signal control plan management process, this study\nproposes Chat2SPaT, a method to convert users' semi-structured and ambiguous\ndescriptions on the signal control plan to exact signal phase and timing (SPaT)\nresults, which could further be transformed into structured stage-based or\nring-based plans to interact with intelligent transportation system (ITS)\nsoftware and traffic signal controllers. With curated prompts, Chat2SPaT first\nleverages large language models' (LLMs) capability of understanding users' plan\ndescriptions and reformulate the plan as a combination of phase sequence and\nphase attribute results in the json format. Based on LLM outputs, python\nscripts are designed to locate phases in a cycle, address nuances of traffic\nsignal control, and finally assemble the complete traffic signal control plan.\nWithin a chat, the pipeline can be utilized iteratively to conduct further plan\nediting. Experiments show that Chat2SPaT can generate plans with an accuracy of\nover 94% for both English and Chinese cases, using a test dataset with over 300\nplan descriptions. As the first benchmark for evaluating LLMs' capability of\nunderstanding traffic signal control plan descriptions, Chat2SPaT provides an\neasy-to-use plan management pipeline for traffic practitioners and researchers,\nserving as a potential new building block for a more accurate and versatile\napplication of LLMs in the field of ITS. The source codes, prompts and test\ndataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.", "AI": {"tldr": "Chat2SPaT is a method that converts users' semi-structured and ambiguous descriptions on the signal control plan to exact signal phase and timing (SPaT) results, which could be transformed into structured stage-based or ring-based plans to interact with intelligent transportation system (ITS) software and traffic signal controllers.", "motivation": "Pre-timed traffic signal control requires tedious manual work for signaling plan creating and updating. When the time-of-day or day-of-week plans are utilized, one intersection is often associated with multiple plans, leading to further repetitive manual plan parameter inputting.", "method": "Chat2SPaT leverages large language models' (LLMs) capability of understanding users' plan descriptions and reformulates the plan as a combination of phase sequence and phase attribute results in the json format. Python scripts are designed to locate phases in a cycle, address nuances of traffic signal control, and assemble the complete traffic signal control plan.", "result": "Experiments show that Chat2SPaT can generate plans with an accuracy of over 94% for both English and Chinese cases, using a test dataset with over 300 plan descriptions. It is the first benchmark for evaluating LLMs' capability of understanding traffic signal control plan descriptions.", "conclusion": "Chat2SPaT provides an easy-to-use plan management pipeline for traffic practitioners and researchers, serving as a potential new building block for a more accurate and versatile application of LLMs in the field of ITS."}}
{"id": "2507.05288", "pdf": "https://arxiv.org/pdf/2507.05288", "abs": "https://arxiv.org/abs/2507.05288", "authors": ["Shuliang Liu", "Hongyi Liu", "Aiwei Liu", "Bingchen Duan", "Qi Zheng", "Yibo Yan", "He Geng", "Peijie Jiang", "Jia Liu", "Xuming Hu"], "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "The widespread deployment of large language models (LLMs) across critical\ndomains has amplified the societal risks posed by algorithmically generated\nmisinformation. Unlike traditional false content, LLM-generated misinformation\ncan be self-reinforcing, highly plausible, and capable of rapid propagation\nacross multiple languages, which traditional detection methods fail to mitigate\neffectively. This paper introduces a proactive defense paradigm, shifting from\npassive post hoc detection to anticipatory mitigation strategies. We propose a\nThree Pillars framework: (1) Knowledge Credibility, fortifying the integrity of\ntraining and deployed data; (2) Inference Reliability, embedding\nself-corrective mechanisms during reasoning; and (3) Input Robustness,\nenhancing the resilience of model interfaces against adversarial attacks.\nThrough a comprehensive survey of existing techniques and a comparative\nmeta-analysis, we demonstrate that proactive defense strategies offer up to\n63\\% improvement over conventional methods in misinformation prevention,\ndespite non-trivial computational overhead and generalization challenges. We\nargue that future research should focus on co-designing robust knowledge\nfoundations, reasoning certification, and attack-resistant interfaces to ensure\nLLMs can effectively counter misinformation across varied domains.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.05300", "pdf": "https://arxiv.org/pdf/2507.05300", "abs": "https://arxiv.org/abs/2507.05300", "authors": ["Nicholas Merchant", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Andrei Cristian Popescu", "Carlos Garcia Jurado Suarez"], "title": "Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7-page main paper + appendix, 18 figures", "summary": "We argue that generative text-to-image models often struggle with prompt\nadherence due to the noisy and unstructured nature of large-scale datasets like\nLAION-5B. This forces users to rely heavily on prompt engineering to elicit\ndesirable outputs. In this work, we propose that enforcing a consistent caption\nstructure during training can significantly improve model controllability and\nalignment. We introduce Re-LAION-Caption 19M, a high-quality subset of\nRe-LAION-5B, comprising 19 million 1024x1024 images with captions generated by\na Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part\ntemplate: subject, setting, aesthetics, and camera details. We fine-tune\nPixArt-$\\Sigma$ and Stable Diffusion 2 using both structured and randomly\nshuffled captions, and show that structured versions consistently yield higher\ntext-image alignment scores using visual question answering (VQA) models. The\ndataset is publicly available at\nhttps://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7ed3\u6784\u5316\u6807\u9898\u6765\u63d0\u9ad8\u751f\u6210\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u53ef\u63a7\u6027\u548c\u5bf9\u9f50\u5ea6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6Re-LAION-Caption 19M\u3002", "motivation": "\u751f\u6210\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5e38\u5e38\u56e0\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u5982LAION-5B\uff09\u7684\u566a\u58f0\u548c\u975e\u7ed3\u6784\u5316\u7279\u6027\u800c\u96be\u4ee5\u9075\u5faa\u63d0\u793a\uff0c\u8fd9\u8feb\u4f7f\u7528\u6237\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u6765\u83b7\u5f97\u671f\u671b\u7684\u8f93\u51fa\u3002", "method": "\u5f15\u5165\u4e86Re-LAION-Caption 19M\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b1900\u4e07\u5f20\u9075\u5faa\u56db\u90e8\u5206\u6a21\u677f\uff08\u4e3b\u4f53\u3001\u73af\u5883\u3001\u7f8e\u5b66\u548c\u76f8\u673a\u7ec6\u8282\uff09\u7684\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528\u7ed3\u6784\u5316\u548c\u968f\u673a\u6253\u4e71\u7684\u6807\u9898\u5bf9PixArt-\u03a3\u548cStable Diffusion 2\u8fdb\u884c\u4e86\u5fae\u8c03\u3002", "result": "\u7ed3\u6784\u5316\u7684\u6807\u9898\u7248\u672c\u5728\u4f7f\u7528\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6a21\u578b\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u5206\u6570\u3002", "conclusion": "\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u6267\u884c\u4e00\u81f4\u7684\u6807\u9898\u7ed3\u6784\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u63a7\u6027\u548c\u5bf9\u9f50\u5ea6\u3002"}}
{"id": "2507.05301", "pdf": "https://arxiv.org/pdf/2507.05301", "abs": "https://arxiv.org/abs/2507.05301", "authors": ["Kai-Cheng Yang"], "title": "News Source Citing Patterns in AI Search Systems", "categories": ["cs.IR", "cs.CL", "cs.CY"], "comment": "15 pages, 7 figures", "summary": "AI-powered search systems are emerging as new information gatekeepers,\nfundamentally transforming how users access news and information. Despite their\ngrowing influence, the citation patterns of these systems remain poorly\nunderstood. We address this gap by analyzing data from the AI Search Arena, a\nhead-to-head evaluation platform for AI search systems. The dataset comprises\nover 24,000 conversations and 65,000 responses from models across three major\nproviders: OpenAI, Perplexity, and Google. Among the over 366,000 citations\nembedded in these responses, 9% reference news sources. We find that while\nmodels from different providers cite distinct news sources, they exhibit shared\npatterns in citation behavior. News citations concentrate heavily among a small\nnumber of outlets and display a pronounced liberal bias, though low-credibility\nsources are rarely cited. User preference analysis reveals that neither the\npolitical leaning nor the quality of cited news sources significantly\ninfluences user satisfaction. These findings reveal significant challenges in\ncurrent AI search systems and have important implications for their design and\ngovernance.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86AI\u641c\u7d22\u7cfb\u7edf\u5728\u5f15\u7528\u65b0\u95fb\u6e90\u65b9\u9762\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5b83\u4eec\u5f15\u7528\u7684\u65b0\u95fb\u6e90\u96c6\u4e2d\u4e14\u504f\u5411\u81ea\u7531\u6d3e\uff0c\u4f46\u4f4e\u53ef\u4fe1\u5ea6\u7684\u6765\u6e90\u8f83\u5c11\u88ab\u5f15\u7528\uff0c\u540c\u65f6\u7528\u6237\u6ee1\u610f\u5ea6\u4e0d\u53d7\u5f15\u7528\u65b0\u95fb\u6e90\u7684\u653f\u6cbb\u503e\u5411\u6216\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8eAI\u641c\u7d22\u7cfb\u7edf\u5728\u4fe1\u606f\u83b7\u53d6\u4e2d\u7684\u5f71\u54cd\u529b\u65e5\u76ca\u589e\u5f3a\uff0c\u4f46\u5176\u5f15\u7528\u6a21\u5f0f\u4ecd\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u884c\u7814\u7a76\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790AI Search Arena\u7684\u6570\u636e\uff0c\u5305\u62ec\u6765\u81ea\u4e09\u5927\u4f9b\u5e94\u5546\uff08OpenAI\u3001Perplexity\u548cGoogle\uff09\u768424,000\u591a\u6761\u5bf9\u8bdd\u548c65,000\u591a\u6761\u54cd\u5e94\uff0c\u4ee5\u53ca\u5176\u4e2d\u5d4c\u5165\u7684366,000\u591a\u4e2a\u5f15\u7528\u3002", "result": "\u6a21\u578b\u5f15\u7528\u4e0d\u540c\u7684\u65b0\u95fb\u6765\u6e90\uff0c\u4f46\u5728\u5f15\u7528\u884c\u4e3a\u4e0a\u8868\u73b0\u51fa\u5171\u540c\u7684\u6a21\u5f0f\u3002\u65b0\u95fb\u5f15\u7528\u96c6\u4e2d\u5728\u5c11\u6570\u51e0\u5bb6\u5a92\u4f53\uff0c\u5e76\u663e\u793a\u51fa\u660e\u663e\u7684\u81ea\u7531\u6d3e\u504f\u89c1\uff0c\u4f46\u4f4e\u53ef\u4fe1\u5ea6\u7684\u6765\u6e90\u5f88\u5c11\u88ab\u5f15\u7528\u3002\u7528\u6237\u504f\u597d\u5206\u6790\u8868\u660e\uff0c\u5f15\u7528\u65b0\u95fb\u6765\u6e90\u7684\u653f\u6cbb\u503e\u5411\u6216\u8d28\u91cf\u5bf9\u7528\u6237\u6ee1\u610f\u5ea6\u6ca1\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524dAI\u641c\u7d22\u7cfb\u7edf\u7684\u91cd\u8981\u6311\u6218\uff0c\u5e76\u5bf9\u5176\u8bbe\u8ba1\u548c\u6cbb\u7406\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.05305", "pdf": "https://arxiv.org/pdf/2507.05305", "abs": "https://arxiv.org/abs/2507.05305", "authors": ["Lorenzo Lee Solano", "Charles Koutcheme", "Juho Leinonen", "Alexandra Vassar", "Jake Renzella"], "title": "Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.SE"], "comment": "7 pages, 3 tables, 1 figure", "summary": "Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher\ncryptic compiler errors for novice programmers, but their computational scale,\ncost, and tendency to over-assist make them problematic for widespread\npedagogical adoption. This work demonstrates that smaller, specialised language\nmodels, enhanced via Supervised Fine-Tuning (SFT), present a more viable\nalternative for educational tools. We utilise a new dataset of 40,000 C\ncompiler error explanations, derived from real introductory programming (CS1/2)\nstudent-generated programming errors, which we used to fine-tune three\nopen-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual\nevaluation, combining expert human reviews with a large-scale automated\nanalysis of 8,000 responses using a validated LLM-as-judge ensemble. Our\nresults show that SFT significantly boosts the pedagogical quality of smaller\nmodels, achieving performance comparable to much larger models. We analyse the\ntrade-offs between model size and quality, confirming that fine-tuning compact,\nefficient models on high-quality, domain-specific data is a potent strategy for\ncreating specialised models to drive educational tools. We provide a replicable\nmethodology to foster broader access to generative AI capabilities in\neducational contexts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u51b3\u7f16\u7a0b\u7f16\u8bd1\u5668\u9519\u8bef\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u524d\u6cbf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u5e2e\u52a9\u521d\u5b66\u8005\u7406\u89e3\u7f16\u8bd1\u5668\u9519\u8bef\uff0c\u4f46\u5176\u8ba1\u7b97\u89c4\u6a21\u3001\u6210\u672c\u548c\u8fc7\u5ea6\u8f85\u52a9\u7684\u95ee\u9898\u9650\u5236\u4e86\u5176\u5728\u6559\u80b2\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u672c\u6587\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b40,000\u4e2aC\u7f16\u8bd1\u5668\u9519\u8bef\u89e3\u91ca\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5bf9\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5e76\u8fdb\u884c\u4e86\u4e13\u5bb6\u4eba\u7c7b\u8bc4\u5ba1\u548c\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u578b\u6a21\u578b\u7684\u6559\u80b2\u8d28\u91cf\uff0c\u4f7f\u5176\u6027\u80fd\u8fbe\u5230\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u7684\u5c0f\u578b\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u6559\u80b2\u5de5\u5177\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u4e14\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2507.05386", "pdf": "https://arxiv.org/pdf/2507.05386", "abs": "https://arxiv.org/abs/2507.05386", "authors": ["Song Lai", "Haohan Zhao", "Rong Feng", "Changyi Ma", "Wenzhuo Liu", "Hongbo Zhao", "Xi Lin", "Dong Yi", "Min Xie", "Qingfu Zhang", "Hongbin Liu", "Gaofeng Meng", "Fei Zhu"], "title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Continual post-training (CPT) is a popular and effective technique for\nadapting foundation models like multimodal large language models to specific\nand ever-evolving downstream tasks. While existing research has primarily\nconcentrated on methods like data replay, model expansion, or parameter\nregularization, the fundamental role of the learning paradigm within CPT\nremains largely unexplored. This paper presents a comparative analysis of two\ncore post-training paradigms: supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT), investigating their respective impacts on knowledge\nretention during CPT. Our experiments are conducted on a benchmark comprising\nseven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base\nmodel for continual post-training. The investigation yields two significant\nfindings: (1) When continuously learning on downstream tasks, SFT leads to\ncatastrophic forgetting of previously learned tasks. In contrast, RFT\ninherently preserves prior knowledge and achieve performance comparable to\nmulti-task training. (2) RFT successfully protects and even enhances the\nmodel's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).\nConversely, SFT degrades general model capabilities severely. Further analysis\nshows that explicit mechanisms, such as KL penalty and chain-of-thought\nreasoning, are not the primary factors. Instead, we find that the implicit\nregularization inherent to RFT is a key factor in mitigating forgetting.\nFinally, we propose a rollout-based instance filtering algorithm to improve the\nstability and efficiency of RFT. Our comprehensive study demonstrates the\nsuperiority of RFT as a robust paradigm for continual post-training.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5728\u6301\u7eed\u540e\u8bad\u7ec3\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0RFT\u5728\u4fdd\u6301\u5148\u9a8c\u77e5\u8bc6\u548c\u63d0\u5347\u6a21\u578b\u901a\u7528\u80fd\u529b\u65b9\u9762\u4f18\u4e8eSFT\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6eda\u52a8\u7684\u5b9e\u4f8b\u8fc7\u6ee4\u7b97\u6cd5\u6765\u63d0\u9ad8RFT\u7684\u7a33\u5b9a\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u636e\u56de\u653e\u3001\u6a21\u578b\u6269\u5c55\u6216\u53c2\u6570\u6b63\u5219\u5316\u7b49\u65b9\u6cd5\u4e0a\uff0c\u800cCPT\u4e2d\u7684\u5b66\u4e60\u8303\u5f0f\u7684\u57fa\u672c\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u6bd4\u8f83SFT\u548cRFT\u5728\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u7684\u6548\u679c\u3002", "method": "\u672c\u6587\u5bf9\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff08RFT\uff09\u4e24\u79cd\u6838\u5fc3\u540e\u8bad\u7ec3\u8303\u5f0f\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6eda\u52a8\u7684\u5b9e\u4f8b\u8fc7\u6ee4\u7b97\u6cd5\u4ee5\u63d0\u9ad8RFT\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSFT\u5728\u8fde\u7eed\u5b66\u4e60\u4e0b\u6e38\u4efb\u52a1\u65f6\u4f1a\u5bfc\u81f4\u5148\u524d\u4efb\u52a1\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800cRFT\u80fd\u591f\u81ea\u7136\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u5e76\u8fbe\u5230\u4e0e\u591a\u4efb\u52a1\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cRFT\u6210\u529f\u4fdd\u62a4\u751a\u81f3\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u800cSFT\u4e25\u91cd\u635f\u5bb3\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u5168\u9762\u7814\u7a76\u4e86RFT\u4f5c\u4e3a\u6301\u7eed\u540e\u8bad\u7ec3\u7684\u7a33\u5065\u8303\u5f0f\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.05515", "pdf": "https://arxiv.org/pdf/2507.05515", "abs": "https://arxiv.org/abs/2507.05515", "authors": ["Haochen Huang", "Jiahuan Pei", "Mohammad Aliannejadi", "Xin Sun", "Moonisa Ahsan", "Pablo Cesar", "Chuang Yu", "Zhaochun Ren", "Junxiao Wang"], "title": "Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "20 pages", "summary": "Vision-language models (VLMs) are essential for enabling AI-powered smart\nassistants to interpret and reason in multimodal environments. However, their\napplication in augmented reality (AR) training remains largely unexplored. In\nthis work, we introduce a comprehensive dataset tailored for AR training,\nfeaturing systematized vision-language tasks, and evaluate nine\nstate-of-the-art VLMs on it. Our results reveal that even advanced models,\nincluding GPT-4o, struggle with fine-grained assembly tasks, achieving a\nmaximum F1 score of just 40.54% on state detection. These findings highlight\nthe demand for enhanced datasets, benchmarks, and further research to improve\nfine-grained vision-language alignment. Beyond technical contributions, our\nwork has broader social implications, particularly in empowering blind and\nvisually impaired users with equitable access to AI-driven learning\nopportunities. We provide all related resources, including the dataset, source\ncode, and evaluation results, to support the research community.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8eAR\u8bad\u7ec3\u7684\u7efc\u5408\u6027\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e5d\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u6539\u8fdb\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u9ad8\u7ec6\u7c92\u5ea6\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u8fd9\u9879\u5de5\u4f5c\u8fd8\u5177\u6709\u793e\u4f1a\u5f71\u54cd\uff0c\u65e8\u5728\u4e3a\u89c6\u969c\u548c\u89c6\u529b\u53d7\u635f\u7528\u6237\u63d0\u4f9b\u5e73\u7b49\u7684AI\u9a71\u52a8\u5b66\u4e60\u673a\u4f1a\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u4e8e\u4f7fAI\u9a71\u52a8\u7684\u667a\u80fd\u52a9\u624b\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u89e3\u91ca\u548c\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b83\u4eec\u5728\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u57f9\u8bad\u4e2d\u7684\u5e94\u7528\u4ecd\u9c9c\u6709\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u9488\u5bf9AR\u8bad\u7ec3\u7684\u7efc\u5408\u6027\u6570\u636e\u96c6\uff0c\u5305\u542b\u7cfb\u7edf\u5316\u7684\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e5d\u79cd\u6700\u5148\u8fdb\u7684VLMs\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5982GPT-4o\uff0c\u5728\u7ec6\u7c92\u5ea6\u88c5\u914d\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u4ec5\u5728\u72b6\u6001\u68c0\u6d4b\u4e2d\u8fbe\u523040.54%\u7684\u6700\u5927F1\u5206\u6570\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u5373\u4f7f\u5148\u8fdb\u7684\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u88c5\u914d\u4efb\u52a1\u4e2d\u4e5f\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u8868\u660e\u9700\u8981\u6539\u8fdb\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u6765\u63d0\u9ad8\u7ec6\u7c92\u5ea6\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u5728\u793e\u4f1a\u5c42\u9762\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u4e3a\u89c6\u969c\u548c\u89c6\u529b\u53d7\u635f\u7528\u6237\u63d0\u4f9b\u5e73\u7b49\u7684AI\u9a71\u52a8\u5b66\u4e60\u673a\u4f1a\u65b9\u9762\u3002"}}
{"id": "2507.05528", "pdf": "https://arxiv.org/pdf/2507.05528", "abs": "https://arxiv.org/abs/2507.05528", "authors": ["Jiahuan Pei", "Fanghua Ye", "Xin Sun", "Wentao Deng", "Koen Hindriks", "Junxiao Wang"], "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment", "categories": ["cs.AI", "cs.CL"], "comment": "14 pages", "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWikiHowAgent\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u6a21\u62df\u4e92\u52a8\u6559\u5b66-\u5b66\u4e60\u5bf9\u8bdd\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u5305\u542b114,296\u4e2a\u6559\u5e08-\u5b66\u4e60\u8005\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\u8be5\u5de5\u4f5c\u6d41\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e2d\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u5728\u5404\u4e2a\u9886\u57df\u80fd\u529b\u7684\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5f80\u5f80\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u672a\u80fd\u5229\u7528\u591a\u6837\u5316\u7684\u5927\u578b\u8bfe\u7a0b\u5185\u5bb9\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u6559\u5b66\u8d28\u91cf\u7684\u6846\u67b6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86WikiHowAgent\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5229\u7528LLM\u6765\u6a21\u62df\u4e92\u52a8\u6559\u5b66-\u5b66\u4e60\u5bf9\u8bdd\u3002\u5b83\u96c6\u6210\u4e86\u6559\u5e08\u548c\u5b66\u4e60\u8005\u4ee3\u7406\u3001\u4e00\u4e2a\u4ea4\u4e92\u7ba1\u7406\u5668\u548c\u4e00\u4e2a\u8bc4\u4f30\u5668\uff0c\u4ee5\u4fc3\u8fdb\u7a0b\u5e8f\u6027\u5b66\u4e60\u5e76\u8bc4\u4f30\u6559\u5b66\u8d28\u91cf\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8be5\u5de5\u4f5c\u6d41\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e2d\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u5728\u5404\u4e2a\u9886\u57df\u80fd\u529b\u7684\u89c1\u89e3\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u5c55\u793a\u4e86WikiHowAgent\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u5728\u5404\u4e2a\u9886\u57df\u80fd\u529b\u7684\u89c1\u89e3\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u5b9e\u73b0\u662f\u5b8c\u5168\u5f00\u6e90\u7684\u3002"}}
{"id": "2507.05577", "pdf": "https://arxiv.org/pdf/2507.05577", "abs": "https://arxiv.org/abs/2507.05577", "authors": ["Shashank Verma", "Fengyi Jiang", "Xiangning Xue"], "title": "Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Paper submitted to CLEF 2025 CEUR-WS", "summary": "Biomedical semantic question answering rooted in information retrieval can\nplay a crucial role in keeping up to date with vast, rapidly evolving and\never-growing biomedical literature. A robust system can help researchers,\nhealthcare professionals and even layman users access relevant knowledge\ngrounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important\nbenchmark, offering a competitive platform for advancement of this space. This\npaper presents the methodologies and results from our participation in this\nchallenge where we built a Retrieval-Augmented Generation (RAG) system that can\nanswer biomedical questions by retrieving relevant PubMed documents and\nsnippets to generate answers. For the retrieval task, we generated dense\nembeddings from biomedical articles for initial retrieval, and applied an\nensemble of finetuned cross-encoders and large language models (LLMs) for\nre-ranking to identify top relevant documents. Our solution achieved an MAP@10\nof 0.1581, placing 10th on the leaderboard for the retrieval task. For answer\ngeneration, we employed few-shot prompting of instruction-tuned LLMs. Our\nsystem achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean\nReciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of\n0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal\nanswers (rank 11).", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6211\u4eec\u5728BioASQ 2025 Task13b\u6311\u6218\u4e2d\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u7684PubMed\u6587\u6863\u548c\u7247\u6bb5\u6765\u751f\u6210\u7b54\u6848\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86MAP@10\u4e3a0.1581\uff0c\u6392\u540d\u7b2c\u5341\u3002\u5728\u7b54\u6848\u751f\u6210\u65b9\u9762\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u5728yes/no\u95ee\u9898\u4e0a\u83b7\u5f97\u4e860.95\u7684\u5b8fF1\u5206\u6570\uff08\u6392\u540d\u7b2c12\uff09\uff0c\u5728\u4e8b\u5b9e\u6027\u95ee\u9898\u4e0a\u83b7\u5f97\u4e860.64\u7684\u5e73\u5747\u5012\u6570\u6392\u540d\uff08MRR\uff09\uff08\u6392\u540d\u7b2c1\uff09\uff0c\u5728\u5217\u8868\u95ee\u9898\u4e0a\u83b7\u5f97\u4e860.63\u7684\u5e73\u5747F1\u5206\u6570\uff08\u6392\u540d\u7b2c5\uff09\uff0c\u5728\u7406\u60f3\u7b54\u6848\u4e0a\u83b7\u5f97\u4e860.29\u7684ROUGE-SU4 F1\u5206\u6570\uff08\u6392\u540d\u7b2c11\uff09\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u8bed\u4e49\u95ee\u7b54\u5728\u4fe1\u606f\u68c0\u7d22\u57fa\u7840\u4e0a\u53ef\u4ee5\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u3001\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u548c\u666e\u901a\u7528\u6237\u83b7\u53d6\u57fa\u4e8e\u8bc1\u636e\u7684\u76f8\u5173\u77e5\u8bc6\u3002BioASQ 2025 Task13b\u6311\u6218\u4e3a\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u57fa\u51c6\u5e73\u53f0\u3002", "method": "\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u7684PubMed\u6587\u6863\u548c\u7247\u6bb5\u6765\u751f\u6210\u7b54\u6848\u3002\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u5bc6\u96c6\u5d4c\u5165\u8fdb\u884c\u521d\u59cb\u68c0\u7d22\uff0c\u5e76\u5e94\u7528\u4e86\u5fae\u8c03\u7684\u4ea4\u53c9\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\u3002\u5728\u7b54\u6848\u751f\u6210\u65b9\u9762\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u6307\u4ee4\u8c03\u4f18\u7684LLMs\u7684\u5c11\u6837\u672c\u63d0\u793a\u3002", "result": "\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86MAP@10\u4e3a0.1581\uff0c\u6392\u540d\u7b2c\u5341\u3002\u5728\u7b54\u6848\u751f\u6210\u65b9\u9762\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u5728yes/no\u95ee\u9898\u4e0a\u83b7\u5f97\u4e860.95\u7684\u5b8fF1\u5206\u6570\uff08\u6392\u540d\u7b2c12\uff09\uff0c\u5728\u4e8b\u5b9e\u6027\u95ee\u9898\u4e0a\u83b7\u5f97\u4e860.64\u7684\u5e73\u5747\u5012\u6570\u6392\u540d\uff08MRR\uff09\uff08\u6392\u540d\u7b2c1\uff09\uff0c\u5728\u5217\u8868\u95ee\u9898\u4e0a\u83b7\u5f97\u4e860.63\u7684\u5e73\u5747F1\u5206\u6570\uff08\u6392\u540d\u7b2c5\uff09\uff0c\u5728\u7406\u60f3\u7b54\u6848\u4e0a\u83b7\u5f97\u4e860.29\u7684ROUGE-SU4 F1\u5206\u6570\uff08\u6392\u540d\u7b2c11\uff09\u3002", "conclusion": "\u6211\u4eec\u7684\u7cfb\u7edf\u5728BioASQ 2025 Task13b\u6311\u6218\u4e2d\u53d6\u5f97\u4e86\u4e0d\u9519\u7684\u6210\u7ee9\uff0c\u7279\u522b\u662f\u5728\u4e8b\u5b9e\u6027\u95ee\u9898\u548c\u5217\u8868\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.05578", "pdf": "https://arxiv.org/pdf/2507.05578", "abs": "https://arxiv.org/abs/2507.05578", "authors": ["Alexander Xiong", "Xuandong Zhao", "Aneesh Pappu", "Dawn Song"], "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bb0\u5fc6\u73b0\u8c61\uff0c\u63a2\u8ba8\u4e86\u5176\u5f71\u54cd\u56e0\u7d20\u3001\u68c0\u6d4b\u65b9\u6cd5\u3001\u7f13\u89e3\u7b56\u7565\u53ca\u5176\u6cd5\u5f8b\u548c\u4f26\u7406\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u65b9\u9762\u7684\u884c\u4e3a\u95ee\u9898\uff0c\u4ee5\u53ca\u7531\u6b64\u5f15\u53d1\u7684\u9690\u79c1\u98ce\u9669\u548c\u5b66\u4e60\u4e0e\u8bb0\u5fc6\u4e4b\u95f4\u7684\u754c\u9650\u95ee\u9898\u3002", "method": "\u672c\u6587\u7efc\u5408\u4e86\u8fd1\u671f\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u73b0\u8c61\u7684\u5f71\u54cd\u56e0\u7d20\u3001\u68c0\u6d4b\u548c\u7f13\u89e3\u65b9\u6cd5\uff0c\u4ee5\u53ca\u76f8\u5173\u7684\u6cd5\u5f8b\u548c\u4f26\u7406\u95ee\u9898\u3002", "result": "\u672c\u6587\u5206\u6790\u4e86\u5f71\u54cd\u8bb0\u5fc6\u7684\u5173\u952e\u56e0\u7d20\uff0c\u8bc4\u4f30\u4e86\u68c0\u6d4b\u548c\u6d4b\u91cf\u8bb0\u5fc6\u5185\u5bb9\u7684\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u7f13\u89e3\u7b56\u7565\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5728\u6700\u5c0f\u5316\u6709\u5bb3\u8bb0\u5fc6\u4e0e\u4fdd\u6301\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u5e73\u8861\u7684\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bb0\u5fc6\u73b0\u8c61\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2507.05660", "pdf": "https://arxiv.org/pdf/2507.05660", "abs": "https://arxiv.org/abs/2507.05660", "authors": ["Aravind Cheruvu", "Shravya Kanchi", "Sifat Muhammad Abdullah", "Nicholas Kong", "Daphne Yao", "Murtuza Jadliwala", "Bimal Viswanath"], "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Pre-print", "summary": "Recent advances in foundation models, such as LLMs, have revolutionized\nconversational AI. Chatbots are increasingly being developed by customizing\nLLMs on specific conversational datasets. However, mitigating toxicity during\nthis customization, especially when dealing with untrusted training data,\nremains a significant challenge. To address this, we introduce TuneShield, a\ndefense framework designed to mitigate toxicity during chatbot fine-tuning\nwhile preserving conversational quality. TuneShield leverages LLM-based\ntoxicity classification, utilizing the instruction-following capabilities and\nsafety alignment of LLMs to effectively identify toxic samples, outperforming\nindustry API services. TuneShield generates synthetic conversation samples,\ntermed 'healing data', based on the identified toxic samples, using them to\nmitigate toxicity while reinforcing desirable behavior during fine-tuning. It\nperforms an alignment process to further nudge the chatbot towards producing\ndesired responses. Our findings show that TuneShield effectively mitigates\ntoxicity injection attacks while preserving conversational quality, even when\nthe toxicity classifiers are imperfect or biased. TuneShield proves to be\nresilient against adaptive adversarial and jailbreak attacks. Additionally,\nTuneShield demonstrates effectiveness in mitigating adaptive toxicity injection\nattacks during dialog-based learning (DBL).", "AI": {"tldr": "TuneShield is a defense framework that mitigates toxicity during chatbot fine-tuning while preserving conversational quality. It uses LLM-based toxicity classification and generates synthetic conversation samples to reinforce desirable behavior.", "motivation": "Mitigating toxicity during chatbot customization, especially when dealing with untrusted training data, remains a significant challenge. Existing solutions may not be effective against adaptive adversarial and jailbreak attacks.", "method": "TuneShield leverages LLM-based toxicity classification to identify toxic samples and generates synthetic conversation samples, termed 'healing data,' to mitigate toxicity while reinforcing desirable behavior during fine-tuning. It also performs an alignment process to nudge the chatbot towards producing desired responses.", "result": "TuneShield outperforms industry API services in identifying toxic samples. It effectively mitigates toxicity injection attacks while preserving conversational quality, even with imperfect or biased toxicity classifiers. It is resilient against adaptive adversarial and jailbreak attacks and shows effectiveness in mitigating adaptive toxicity injection attacks during dialog-based learning.", "conclusion": "TuneShield effectively mitigates toxicity injection attacks while preserving conversational quality, even when the toxicity classifiers are imperfect or biased. It is resilient against adaptive adversarial and jailbreak attacks and demonstrates effectiveness in mitigating adaptive toxicity injection attacks during dialog-based learning."}}
{"id": "2507.05687", "pdf": "https://arxiv.org/pdf/2507.05687", "abs": "https://arxiv.org/abs/2507.05687", "authors": ["Shangzhan Li", "Zefan Wang", "Ye He", "Yuxuan Li", "Qi Shi", "Jianling Li", "Yonggang Hu", "Wanxiang Che", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton.", "AI": {"tldr": "AutoTriton\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684Triton\u7f16\u7a0b\u6a21\u578b\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u5185\u6838\uff0c\u63d0\u5347\u4e86AI\u7cfb\u7edf\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u5185\u6838\u5f00\u53d1\u9700\u8981\u5728\u786c\u4ef6\u4e0a\u4f18\u5316\u8ba1\u7b97\u5355\u5143\uff0c\u540c\u65f6\u5e73\u8861\u5185\u5b58\u7ba1\u7406\u3001\u5e76\u884c\u6027\u548c\u786c\u4ef6\u7279\u5b9a\u4f18\u5316\uff0c\u4f46\u624b\u52a8\u8c03\u6574\u5173\u952e\u53c2\u6570\u5982\u5757\u5927\u5c0f\u548c\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u5b58\u5728\u8f83\u5927\u969c\u788d\u3002", "method": "AutoTriton\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u4e0e\u6267\u884c\u5956\u52b1\u7ed3\u5408\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u63d0\u5347Triton\u7f16\u7a0b\u80fd\u529b\u3002", "result": "AutoTriton\u5728TritonBench\u548cKernelBench\u7684\u4e94\u4e2a\u8bc4\u4f30\u901a\u9053\u4e2d\u8868\u73b0\u51fa\u4e0e\u4e3b\u6d41\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f8b\u5982Claude-4-Sonnet\u548cDeepSeek-R1-0528\u3002", "conclusion": "AutoTriton\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u5185\u6838\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2507.05720", "pdf": "https://arxiv.org/pdf/2507.05720", "abs": "https://arxiv.org/abs/2507.05720", "authors": ["Yucheng Shi", "Wenhao Yu", "Zaitang Li", "Yonglin Wang", "Hongming Zhang", "Ninghao Liu", "Haitao Mi", "Dong Yu"], "title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages, 4 figures", "summary": "Recently, there has been a surge of vision-based GUI agents designed to\nautomate everyday mobile and web tasks. These agents interpret raw GUI\nscreenshots and autonomously decide where to click, scroll, or type, which\nbypasses handcrafted rules and app-specific APIs. However, most existing\nmethods trained GUI agent in the offline environment using pre-collected\ntrajectories. This approach limits scalability, causes overfitting to specific\nUI templates, and leads to brittle policies when faced with unseen environment.\nWe present MobileGUI-RL, a scalable framework that trains GUI agent in online\nenvironment. MobileGUI-RL contains two key components. It (i) synthesizes a\ncurriculum of learnable tasks through self-exploration and filtering, and (ii)\nadapts GRPO to GUI navigation with trajectory-aware advantages and composite\nrewards that balance task success and execution efficiency. Experiments on\nthree online mobile-agent benchmarks show consistent gains, validating the\neffectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMobileGUI-RL\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u8bad\u7ec3GUI\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u6211\u63a2\u7d22\u548c\u8fc7\u6ee4\u5408\u6210\u8bfe\u7a0b\u4efb\u52a1\uff0c\u5e76\u9002\u5e94GRPO\u4ee5\u5e73\u8861\u4efb\u52a1\u6210\u529f\u548c\u6267\u884c\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u5728\u7ebf\u79fb\u52a8\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570\u65b9\u6cd5\u5728\u79bb\u7ebf\u73af\u5883\u4e2d\u4f7f\u7528\u9884\u6536\u96c6\u7684\u8f68\u8ff9\u8bad\u7ec3GUI\u4ee3\u7406\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5bfc\u81f4\u5bf9\u7279\u5b9aUI\u6a21\u677f\u7684\u8fc7\u62df\u5408\uff0c\u5e76\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u73af\u5883\u65f6\u5bfc\u81f4\u8106\u5f31\u7684\u7b56\u7565\u3002", "method": "MobileGUI-RL\uff0c\u4e00\u4e2a\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u8bad\u7ec3GUI\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u901a\u8fc7\u81ea\u6211\u63a2\u7d22\u548c\u8fc7\u6ee4\u5408\u6210\u53ef\u5b66\u4e60\u4efb\u52a1\u7684\u8bfe\u7a0b\uff0c\u4ee5\u53ca\u901a\u8fc7\u8f68\u8ff9\u611f\u77e5\u4f18\u52bf\u548c\u590d\u5408\u5956\u52b1\u9002\u5e94GRPO\u4ee5\u5e73\u8861\u4efb\u52a1\u6210\u529f\u548c\u6267\u884c\u6548\u7387\u3002", "result": "\u5728\u4e09\u4e2a\u5728\u7ebf\u79fb\u52a8\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u4e86\u6301\u7eed\u7684\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u5728\u7ebf\u79fb\u52a8\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.05727", "pdf": "https://arxiv.org/pdf/2507.05727", "abs": "https://arxiv.org/abs/2507.05727", "authors": ["He Wang", "Linhan Ma", "Dake Guo", "Xiong Wang", "Lei Xie", "Jin Xu", "Junyang Lin"], "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "18 pages, 4 figures", "summary": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior evaluative efforts have largely been restricted to contextless paradigms.\nThis constraint stems from the limited proficiency of conventional ASR models\nin context modeling and their deficiency in memory and reasoning based on world\nknowledge. Recent breakthroughs in the development of Large Language Models\n(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly\nenhanced the visibility of general artificial intelligence capabilities.\nConsequently, there exists a compelling need for a benchmark that can evaluate\nboth the generality and intelligence of ASR systems. To address this gap, we\npropose ContextASR-Bench: a comprehensive, large-scale benchmark designed to\nassess contextual speech recognition. This benchmark encompasses up to 40,000\ndata entries across over 10 domains, enabling a thorough evaluation of model\nperformance in scenarios that omit or incorporate coarse-grained or\nfine-grained contextual information. Moreover, diverging from conventional ASR\nevaluations, our benchmark includes an analysis of model efficacy in\nrecognizing named entities mentioned within the auditory input. Our extensive\nevaluation highlights that LALMs, with strong world knowledge and context\nlearning capabilities, outperform conventional ASR models by a large margin.\nThe dataset and evaluation code have been released at\nhttps://github.com/MrSupW/ContextASR-Bench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aContextASR-Bench\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0a\u4e0b\u6587\u8bed\u97f3\u8bc6\u522b\u3002\u8be5\u57fa\u51c6\u6db5\u76d6\u4e86\u591a\u4e2a\u9886\u57df\u7684\u5927\u91cf\u6570\u636e\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5728\u8bc6\u522b\u547d\u540d\u5b9e\u4f53\u65b9\u9762\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLALMs\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edfASR\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u65e0\u4e0a\u4e0b\u6587\u7684\u8303\u5f0f\u4e0a\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u57fa\u4e8e\u4e16\u754c\u77e5\u8bc6\u7684\u8bb0\u5fc6\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u8bc4\u4f30ASR\u7cfb\u7edf\u901a\u7528\u6027\u548c\u667a\u80fd\u6027\u7684\u57fa\u51c6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6ContextASR-Bench\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0a\u4e0b\u6587\u8bed\u97f3\u8bc6\u522b\u3002\u8be5\u57fa\u51c6\u5305\u62ec\u591a\u4e2a\u9886\u57df\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5728\u8bc6\u522b\u547d\u540d\u5b9e\u4f53\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5177\u6709\u5f3a\u5927\u4e16\u754c\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LALMs\uff09\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684ASR\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aContextASR-Bench\u7684\u5168\u9762\u3001\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0a\u4e0b\u6587\u8bed\u97f3\u8bc6\u522b\u3002\u8be5\u57fa\u51c6\u6db5\u76d6\u4e86\u8d85\u8fc710\u4e2a\u9886\u57df\u768440,000\u4e2a\u6570\u636e\u6761\u76ee\uff0c\u80fd\u591f\u5bf9\u6a21\u578b\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4fe1\u606f\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u8fdb\u884c\u6df1\u5165\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8be5\u57fa\u51c6\u8fd8\u5206\u6790\u4e86\u6a21\u578b\u5728\u8bc6\u522b\u542c\u89c9\u8f93\u5165\u4e2d\u63d0\u5230\u7684\u547d\u540d\u5b9e\u4f53\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5177\u6709\u5f3a\u5927\u4e16\u754c\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684LALMs\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684ASR\u6a21\u578b\u3002"}}
{"id": "2507.05816", "pdf": "https://arxiv.org/pdf/2507.05816", "abs": "https://arxiv.org/abs/2507.05816", "authors": ["Shuai Zhao", "Yulin Zhang", "Luwei Xiao", "Xinyi Wu", "Yanhao Jia", "Zhongliang Guo", "Xiaobao Wu", "Cong-Duy Nguyen", "Guoming Zhang", "Anh Tuan Luu"], "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CROP\u6570\u636e\u96c6\u548cAffective-ROPTester\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u65e9\u4ea7\u513f\u89c6\u7f51\u819c\u75c5\u53d8\u98ce\u9669\u65f6\u7684\u8868\u73b0\u548c\u60c5\u611f\u504f\u5dee\u3002\u7ed3\u679c\u663e\u793a\uff0c\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u800c\u6b63\u9762\u60c5\u7eea\u6846\u67b6\u6709\u52a9\u4e8e\u51cf\u5c11\u9884\u6d4b\u504f\u5dee\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u9884\u6d4b\u65e9\u4ea7\u513f\u89c6\u7f51\u819c\u75c5\u53d8\uff08ROP\uff09\u98ce\u9669\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u9c9c\u6709\u63a2\u7d22\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u4e2d\u6587\u57fa\u51c6\u6570\u636e\u96c6CROP\uff0c\u5305\u542b993\u4e2a\u5e26\u6709\u4f4e\u3001\u4e2d\u3001\u9ad8\u98ce\u9669\u6807\u7b7e\u7684\u5165\u9662\u8bb0\u5f55\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Affective-ROPTester\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u57fa\u4e8e\u6307\u4ee4\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u63d0\u793a\u7ea7\u522b\u96c6\u6210\u4e86\u60c5\u611f\u5143\u7d20\uff0c\u4ee5\u7814\u7a76\u4e0d\u540c\u60c5\u611f\u6846\u67b6\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u9884\u6d4bROP\u53ca\u5176\u504f\u5dee\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u4ec5\u4f9d\u8d56\u5185\u5728\u77e5\u8bc6\u65f6\uff0cLLMs\u5728ROP\u98ce\u9669\u9884\u6d4b\u65b9\u9762\u7684\u6548\u679c\u6709\u9650\uff0c\u4f46\u5f53\u7ed3\u5408\u7ed3\u6784\u5316\u7684\u5916\u90e8\u8f93\u5165\u65f6\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6a21\u578b\u8f93\u51fa\u4e2d\u5b58\u5728\u60c5\u611f\u504f\u5dee\uff0c\u503e\u5411\u4e8e\u9ad8\u4f30\u4e2d\u3001\u9ad8\u98ce\u9669\u75c5\u4f8b\u3002\u4e0e\u8d1f\u9762\u60c5\u7eea\u76f8\u6bd4\uff0c\u6b63\u9762\u60c5\u7eea\u6846\u67b6\u6709\u52a9\u4e8e\u51cf\u8f7b\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u9884\u6d4b\u504f\u5dee\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u60c5\u611f\u654f\u611f\u63d0\u793a\u5de5\u7a0b\u5728\u63d0\u9ad8\u8bca\u65ad\u53ef\u9760\u6027\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u5f3a\u8c03\u4e86Affective-ROPTester\u4f5c\u4e3a\u8bc4\u4f30\u548c\u51cf\u8f7b\u4e34\u5e8a\u8bed\u8a00\u5efa\u6a21\u7cfb\u7edf\u4e2d\u60c5\u611f\u504f\u5dee\u7684\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.05894", "pdf": "https://arxiv.org/pdf/2507.05894", "abs": "https://arxiv.org/abs/2507.05894", "authors": ["Fathinah Izzati", "Xinyue Li", "Yuxuan Wu", "Gus Xia"], "title": "MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Humans can imagine various atmospheres and settings when listening to music,\nenvisioning movie scenes that complement each piece. For example, slow,\nmelancholic music might evoke scenes of heartbreak, while upbeat melodies\nsuggest celebration. This paper explores whether a Music Language Model, e.g.\nMU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),\nwhich requires cross-modal information from video and music to train. To\nimprove upon existing music captioning models which focusing solely on musical\nelements, we introduce MusiScene, a music captioning model designed to imagine\nscenes that complement each music. In this paper, (1) we construct a\nlarge-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music\nUnderstanding LLaMA for the MSI task to create MusiScene, and (3) we conduct\ncomprehensive evaluations and prove that our MusiScene is more capable of\ngenerating contextually relevant captions compared to MU-LLaMA. We leverage the\ngenerated MSI captions to enhance Video Background Music Generation (VBMG) from\ntext.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u97f3\u4e50\u6807\u9898\u6a21\u578bMusiScene\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u97f3\u4e50\u76f8\u914d\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u6807\u9898\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u97f3\u4e50\u6807\u9898\u6a21\u578b\u4ec5\u5173\u6ce8\u97f3\u4e50\u5143\u7d20\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8de8\u6a21\u6001\u4fe1\u606f\u6765\u6539\u8fdb\u97f3\u4e50\u573a\u666f\u60f3\u8c61\u4efb\u52a1\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b3,371\u5bf9\u6570\u636e\u7684\u5927\u89c4\u6a21\u89c6\u9891-\u97f3\u9891\u6807\u9898\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u4e86Music Understanding LLaMA\u4ee5\u521b\u5efaMusiScene\u6a21\u578b\u3002", "result": "MusiScene\u5728\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u6807\u9898\u65b9\u9762\u6bd4MU-LLaMA\u66f4\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86MusiScene\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u4e0e\u97f3\u4e50\u76f8\u914d\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u6807\u9898\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002\u6b64\u5916\uff0c\u5229\u7528\u751f\u6210\u7684MSI\u6807\u9898\u53ef\u4ee5\u589e\u5f3a\u4ece\u6587\u672c\u751f\u6210\u89c6\u9891\u80cc\u666f\u97f3\u4e50\u7684\u80fd\u529b\u3002"}}
{"id": "2507.05903", "pdf": "https://arxiv.org/pdf/2507.05903", "abs": "https://arxiv.org/abs/2507.05903", "authors": ["Gerd Gra\u00dfhoff"], "title": "AI-Reporter: A Path to a New Genre of Scientific Communication", "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "The AI-Reporter represents a paradigmatic shift in scientific publication\npractice. This document demonstrates through a concrete case study how our\nsystem transforms academic presentations into publication-ready chapters -- in\nless than three minutes. Using Arno Simons' lecture on Large Language Models\nfrom the ``Large Language Models for the History, Philosophy, and Sociology of\nScience'' workshop (NEPI) as an example, we show how technological innovation\nbridges the gap between ephemeral presentation and permanent scientific\ndocumentation.", "AI": {"tldr": "AI-Reporter \u901a\u8fc7\u6280\u672f\u624b\u6bb5\u5c06\u5b66\u672f\u6f14\u793a\u5feb\u901f\u8f6c\u5316\u4e3a\u79d1\u5b66\u51fa\u7248\u7269\u3002", "motivation": "\u65e8\u5728\u5c55\u793a\u6280\u672f\u521b\u65b0\u5982\u4f55\u5f25\u5408\u77ed\u6682\u6f14\u793a\u4e0e\u6c38\u4e45\u79d1\u5b66\u6587\u732e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u5177\u4f53\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u5982\u4f55\u5c06\u5b66\u672f\u6f14\u793a\u8f6c\u5316\u4e3a\u51fa\u7248\u51c6\u5907\u597d\u7684\u7ae0\u8282\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5728\u4e0d\u5230\u4e09\u5206\u949f\u7684\u65f6\u95f4\u5185\u5c06\u5b66\u672f\u6f14\u793a\u8f6c\u5316\u4e3a\u51fa\u7248\u51c6\u5907\u597d\u7684\u7ae0\u8282\u3002", "conclusion": "AI-Reporter \u662f\u79d1\u5b66\u51fa\u7248\u5b9e\u8df5\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5b66\u672f\u6f14\u793a\u8f6c\u5316\u4e3a\u51fa\u7248\u51c6\u5907\u597d\u7684\u7ae0\u8282\u3002"}}
{"id": "2507.05933", "pdf": "https://arxiv.org/pdf/2507.05933", "abs": "https://arxiv.org/abs/2507.05933", "authors": ["Y. Du"], "title": "Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation", "categories": ["cs.IR", "cs.CL"], "comment": "7 pages", "summary": "Vector retrieval systems exhibit significant performance variance across\nqueries due to heterogeneous embedding quality. We propose a lightweight\nframework for predicting retrieval performance at the query level by combining\nquantization robustness and neighborhood density metrics. Our approach is\nmotivated by the observation that high-quality embeddings occupy geometrically\nstable regions in the embedding space and exhibit consistent neighborhood\nstructures. We evaluate our method on 4 standard retrieval datasets, showing\nconsistent improvements of 9.4$\\pm$1.2\\% in Recall@10 over competitive\nbaselines. The framework requires minimal computational overhead (less than 5\\%\nof retrieval time) and enables adaptive retrieval strategies. Our analysis\nreveals systematic patterns in embedding quality across different query types,\nproviding insights for targeted training data augmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u67e5\u8be2\u7ea7\u522b\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u91cf\u5316\u9c81\u68d2\u6027\u548c\u90bb\u57df\u5bc6\u5ea6\u5ea6\u91cf\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u6211\u4eec\u89c2\u5bdf\u5230\u9ad8\u8d28\u91cf\u7684\u5d4c\u5165\u5360\u636e\u5d4c\u5165\u7a7a\u95f4\u4e2d\u51e0\u4f55\u7a33\u5b9a\u7684\u533a\u57df\u5e76\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u90bb\u57df\u7ed3\u6784\uff0c\u8fd9\u4fc3\u4f7f\u6211\u4eec\u63d0\u51fa\u4e86\u8fd9\u79cd\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u91cf\u5316\u9c81\u68d2\u6027\u548c\u90bb\u57df\u5bc6\u5ea6\u5ea6\u91cf\u6765\u9884\u6d4b\u67e5\u8be2\u7ea7\u522b\u7684\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u6211\u4eec\u57284\u4e2a\u6807\u51c6\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5728Recall@10\u4e0a\u76f8\u5bf9\u4e8e\u7ade\u4e89\u57fa\u7ebf\u53d6\u5f97\u4e869.4\u00b11.2%\u7684\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u67e5\u8be2\u7c7b\u578b\u4e2d\u5d4c\u5165\u8d28\u91cf\u7684\u7cfb\u7edf\u6a21\u5f0f\uff0c\u4e3a\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2507.05984", "pdf": "https://arxiv.org/pdf/2507.05984", "abs": "https://arxiv.org/abs/2507.05984", "authors": ["Zhijun Guo", "Alvina Lai", "Julia Ive", "Alexandru Petcu", "Yutong Wang", "Luyuan Qi", "Johan H Thygesen", "Kezhi Li"], "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4ebaHopeBot\uff0c\u7528\u4e8e\u5b9e\u65bdPHQ-9\uff0c\u5e76\u5728\u4e00\u9879\u7814\u7a76\u4e2d\u8bc4\u4f30\u5176\u6548\u679c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u804a\u5929\u673a\u5668\u4eba\u5177\u6709\u826f\u597d\u7684\u7528\u6237\u4fe1\u4efb\u5ea6\u548c\u6ee1\u610f\u5ea6\uff0c\u53ef\u80fd\u4f5c\u4e3a\u5e38\u89c4\u6291\u90c1\u75c7\u7b5b\u67e5\u7684\u53ef\u884c\u8f85\u52a9\u5de5\u5177\u3002", "motivation": "\u9759\u6001\u5de5\u5177\u5982PHQ-9\u867d\u7136\u80fd\u6709\u6548\u7b5b\u67e5\u6291\u90c1\u75c7\uff0c\u4f46\u7f3a\u4e4f\u4e92\u52a8\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u5f00\u53d1\u4e86HopeBot\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5b9e\u65f6\u6f84\u6e05\u6765\u5b9e\u65bdPHQ-9\u3002\u8fdb\u884c\u4e86\u4e00\u9879\u53d7\u8bd5\u8005\u5185\u7814\u7a76\uff0c132\u540d\u82f1\u56fd\u548c\u4e2d\u56fd\u7684\u6210\u5e74\u4eba\u5b8c\u6210\u4e86\u81ea\u6211\u7ba1\u7406\u7248\u672c\u548c\u804a\u5929\u673a\u5668\u4eba\u7248\u672c\u3002", "result": "\u5f97\u5206\u663e\u793a\u4e86\u5f3a\u70c8\u7684\u534f\u8bae\uff08ICC = 0.91\uff1b45%\u76f8\u540c\uff09\u300275\u540d\u63d0\u4f9b\u6bd4\u8f83\u53cd\u9988\u7684\u53c2\u4e0e\u8005\u4e2d\uff0c71%\u8868\u793a\u5bf9\u804a\u5929\u673a\u5668\u4eba\u66f4\u6709\u4fe1\u4efb\u611f\uff0c\u5f3a\u8c03\u4e86\u66f4\u6e05\u6670\u7684\u7ed3\u6784\u3001\u89e3\u91ca\u6027\u6307\u5bfc\u548c\u652f\u6301\u6027\u8bed\u6c14\u3002\u5e73\u5747\u8bc4\u5206\uff080-10\uff09\u5206\u522b\u4e3a8.4\uff08\u8212\u9002\u5ea6\uff09\u30017.7\uff08\u8bed\u97f3\u6e05\u6670\u5ea6\uff09\u30017.6\uff08\u5904\u7406\u654f\u611f\u8bdd\u9898\uff09\u548c7.4\uff08\u5efa\u8bae\u6709\u7528\u6027\uff09\uff1b\u540e\u8005\u6839\u636e\u5c31\u4e1a\u72b6\u51b5\u548c\u4e4b\u524d\u7684\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u4f7f\u7528\u60c5\u51b5\u663e\u8457\u53d8\u5316\uff08p < 0.05\uff09\u3002\u603b\u4f53\u800c\u8a00\uff0c87.1%\u7684\u4eba\u8868\u793a\u613f\u610f\u518d\u6b21\u4f7f\u7528\u6216\u63a8\u8350HopeBot\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u57fa\u4e8e\u8bed\u97f3\u7684LLM\u804a\u5929\u673a\u5668\u4eba\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u884c\u7684\u3001\u4f4e\u8d1f\u62c5\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u7528\u4e8e\u5e38\u89c4\u6291\u90c1\u75c7\u7b5b\u67e5\u3002"}}
{"id": "2507.06090", "pdf": "https://arxiv.org/pdf/2507.06090", "abs": "https://arxiv.org/abs/2507.06090", "authors": ["Swapnil Bhattacharyya", "Shrey Ganatra", "Harshvivek Kashid", "Spandan Anaokar", "Shruti Nair", "Reshma Sekhar", "Siddharth Manohar", "Rahul Hemrajani", "Pushpak Bhattacharyya"], "title": "Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "AI-based judicial assistance and case prediction have been extensively\nstudied in criminal and civil domains, but remain largely unexplored in\nconsumer law, especially in India. In this paper, we present Nyay-Darpan, a\nnovel two-in-one framework that (i) summarizes consumer case files and (ii)\nretrieves similar case judgements to aid decision-making in consumer dispute\nresolution. Our methodology not only addresses the gap in consumer law AI tools\nbut also introduces an innovative approach to evaluate the quality of the\nsummary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',\nsymbolizing the ability of our tool to reflect the core of consumer disputes\nthrough precise summarization and intelligent case retrieval. Our system\nachieves over 75 percent accuracy in similar case prediction and approximately\n70 percent accuracy across material summary evaluation metrics, demonstrating\nits practical effectiveness. We will publicly release the Nyay-Darpan framework\nand dataset to promote reproducibility and facilitate further research in this\nunderexplored yet impactful domain.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Nyay-Darpan\uff0c\u4e00\u4e2a\u7528\u4e8e\u6d88\u8d39\u8005\u6848\u4ef6\u6587\u4ef6\u603b\u7ed3\u548c\u76f8\u4f3c\u6848\u4ef6\u5224\u51b3\u68c0\u7d22\u7684\u6846\u67b6\uff0c\u65e8\u5728\u586b\u8865\u6d88\u8d39\u8005\u6cd5AI\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u9ad8\u51c6\u786e\u7387\u5c55\u793a\u5176\u6709\u6548\u6027\u3002", "motivation": "AI\u9a71\u52a8\u7684\u53f8\u6cd5\u8f85\u52a9\u548c\u6848\u4ef6\u9884\u6d4b\u5728\u5211\u4e8b\u548c\u6c11\u4e8b\u9886\u57df\u5f97\u5230\u4e86\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u6d88\u8d39\u8005\u6cd5\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u5370\u5ea6\uff0c\u4ecd\u9c9c\u6709\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Nyay-Darpan\uff0c\u8fd9\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u53cc\u529f\u80fd\u6846\u67b6\uff0c(i)\u603b\u7ed3\u6d88\u8d39\u8005\u6848\u4ef6\u6587\u4ef6\uff0c(ii)\u68c0\u7d22\u76f8\u4f3c\u7684\u6848\u4ef6\u5224\u51b3\u4ee5\u8f85\u52a9\u6d88\u8d39\u8005\u7ea0\u7eb7\u89e3\u51b3\u4e2d\u7684\u51b3\u7b56\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u586b\u8865\u4e86\u6d88\u8d39\u8005\u6cd5AI\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u6458\u8981\u7684\u8d28\u91cf\u3002", "result": "\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u76f8\u4f3c\u6848\u4ef6\u9884\u6d4b\u4e2d\u8fbe\u5230\u4e8675%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u6750\u6599\u6458\u8981\u8bc4\u4f30\u6307\u6807\u4e2d\u8fbe\u5230\u4e86\u7ea670%\u7684\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u76f8\u4f3c\u6848\u4ef6\u9884\u6d4b\u4e2d\u8fbe\u5230\u4e8675%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u6750\u6599\u6458\u8981\u8bc4\u4f30\u6307\u6807\u4e2d\u8fbe\u5230\u4e86\u7ea670%\u7684\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u6709\u6548\u6027\u3002\u6211\u4eec\u5c06\u516c\u5f00\u53d1\u5e03Nyay-Darpan\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u63a8\u52a8\u8fd9\u4e00\u672a\u88ab\u5145\u5206\u7814\u7a76\u4f46\u5177\u6709\u5f71\u54cd\u529b\u7684\u9886\u57df\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.06157", "pdf": "https://arxiv.org/pdf/2507.06157", "abs": "https://arxiv.org/abs/2507.06157", "authors": ["William Li", "Lei Hamilton", "Kaise Al-natour", "Sanjeev Mohindra"], "title": "Evaluation of Habitat Robotics using Large Language Models", "categories": ["cs.RO", "cs.CL"], "comment": "6 pages, IEEE HPEC submission", "summary": "This paper focuses on evaluating the effectiveness of Large Language Models\nat solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR\nprovides simplified environments and robotic interactions within randomized\nindoor kitchen scenes. Each randomized kitchen scene is given a task where two\nrobotic agents cooperatively work together to solve the task. We evaluated\nmultiple frontier models on Meta PARTNER environments. Our results indicate\nthat reasoning models like OpenAI o3-mini outperform non-reasoning models like\nOpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied\nenvironments. o3-mini displayed outperform across centralized, decentralized,\nfull observability, and partial observability configurations. This provides a\npromising avenue of research for embodied robotic development.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5728\u5404\u79cd\u914d\u7f6e\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u5177\u8eab\u673a\u5668\u4eba\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528Meta PARTNER\u57fa\u51c6\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u5177\u8eab\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5728\u968f\u673a\u5316\u5ba4\u5185\u53a8\u623f\u573a\u666f\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u50cfOpenAI o3-mini\u8fd9\u6837\u7684\u63a8\u7406\u6a21\u578b\u5728Meta PARTNER\u7684\u5177\u8eab\u673a\u5668\u4eba\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\u5982OpenAI GPT-4o\u548cLlama 3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5177\u8eab\u673a\u5668\u4eba\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u8868\u660e\u63a8\u7406\u6a21\u578b\u5728\u5177\u8eab\u673a\u5668\u4eba\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2507.06185", "pdf": "https://arxiv.org/pdf/2507.06185", "abs": "https://arxiv.org/abs/2507.06185", "authors": ["Zhicheng Lin"], "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u5b66\u672f\u8bba\u6587\u4e2d\u53d1\u73b0\u7684\u9690\u85cf\u6307\u4ee4\u95ee\u9898\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u7edf\u4e00\u7ba1\u7406\u751f\u6210\u5f0fAI\u5728\u5b66\u672f\u8bc4\u4f30\u4e2d\u7684\u4f7f\u7528\u653f\u7b56\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u9690\u85cf\u6307\u4ee4\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u5e76\u8bc6\u522b\u7cfb\u7edf\u6027\u6f0f\u6d1e\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u9690\u85cf\u6307\u4ee4\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6ce8\u5165\u6280\u672f\uff0c\u63ed\u793a\u4e86\u56db\u79cd\u7c7b\u578b\u7684\u9690\u85cf\u63d0\u793a\uff0c\u5e76\u5bf9\u51fa\u7248\u5546\u7684\u653f\u7b56\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u672c\u6587\u53d1\u73b0\u9690\u85cf\u6307\u4ee4\u53ef\u4ee5\u64cd\u7eb5AI\u8f85\u52a9\u7684\u540c\u884c\u8bc4\u5ba1\uff0c\u5e76\u6307\u51fa\u51fa\u7248\u5546\u5728\u653f\u7b56\u4e0a\u7684\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u5206\u6790\u4e86\u5728\u5b66\u672f\u8bba\u6587\u9884\u5370\u672c\u7f51\u7ad9arXiv\u4e0a\u53d1\u73b0\u7684\u9690\u85cf\u6307\u4ee4\u95ee\u9898\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u5728\u63d0\u4ea4\u95e8\u6237\u8fdb\u884c\u534f\u8c03\u7684\u6280\u672f\u7b5b\u67e5\u4ee5\u53ca\u7edf\u4e00\u7ba1\u7406\u751f\u6210\u5f0fAI\u5728\u5b66\u672f\u8bc4\u4f30\u4e2d\u7684\u4f7f\u7528\u653f\u7b56\u3002"}}
{"id": "2507.06192", "pdf": "https://arxiv.org/pdf/2507.06192", "abs": "https://arxiv.org/abs/2507.06192", "authors": ["Jiale Lao", "Immanuel Trummer"], "title": "SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Database research and development often require a large number of SQL queries\nfor benchmarking purposes. However, acquiring real-world SQL queries is\nchallenging due to privacy concerns, and existing SQL generation methods are\nlimited in customization and in satisfying realistic constraints. To address\nthis issue, we present SQLBarber, a system based on Large Language Models\n(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)\neliminates the need for users to manually craft SQL templates in advance, while\nproviding the flexibility to accept natural language specifications to\nconstrain SQL templates, (ii) scales efficiently to generate large volumes of\nqueries matching any user-defined cost distribution (e.g., cardinality and\nexecution plan cost), and (iii) uses execution statistics from Amazon Redshift\nand Snowflake to derive SQL template specifications and query cost\ndistributions that reflect real-world query characteristics. SQLBarber\nintroduces (i) a declarative interface for users to effortlessly generate\ncustomized SQL templates, (ii) an LLM-powered pipeline augmented with a\nself-correction module that profiles, refines, and prunes SQL templates based\non query costs, and (iii) a Bayesian Optimizer to efficiently explore different\npredicate values and identify a set of queries that satisfy the target cost\ndistribution. We construct and open-source ten benchmarks of varying difficulty\nlevels and target query cost distributions based on real-world statistics from\nSnowflake and Amazon Redshift. Extensive experiments on these benchmarks show\nthat SQLBarber is the only system that can generate customized SQL templates.\nIt reduces query generation time by one to three orders of magnitude, and\nsignificantly improves alignment with the target cost distribution, compared\nwith existing methods.", "AI": {"tldr": "SQLBarber \u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u5b9a\u5236\u5316\u548c\u73b0\u5b9e\u7684 SQL \u5de5\u4f5c\u8d1f\u8f7d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u65f6\u95f4\u548c\u4e0e\u76ee\u6807\u6210\u672c\u5206\u5e03\u7684\u5bf9\u9f50\u5ea6\u4e0a\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\uff0c\u83b7\u53d6\u771f\u5b9e\u4e16\u754c\u7684 SQL \u67e5\u8be2\u5f88\u56f0\u96be\uff0c\u800c\u73b0\u6709\u7684 SQL \u751f\u6210\u65b9\u6cd5\u5728\u81ea\u5b9a\u4e49\u548c\u6ee1\u8db3\u73b0\u5b9e\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684 SQL \u751f\u6210\u65b9\u6cd5\u3002", "method": "SQLBarber \u4f7f\u7528\u4e86\u58f0\u660e\u5f0f\u63a5\u53e3\u3001LLM \u9a71\u52a8\u7684\u7ba1\u9053\u4ee5\u53ca\u8d1d\u53f6\u65af\u4f18\u5316\u5668\u6765\u751f\u6210 SQL \u6a21\u677f\uff0c\u5e76\u5229\u7528 Amazon Redshift \u548c Snowflake \u7684\u6267\u884c\u7edf\u8ba1\u4fe1\u606f\u6765\u63a8\u5bfc SQL \u6a21\u677f\u89c4\u8303\u548c\u67e5\u8be2\u6210\u672c\u5206\u5e03\u3002", "result": "SQLBarber \u5728\u751f\u6210\u65f6\u95f4\u4e0a\u51cf\u5c11\u4e86 1 \u5230 3 \u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u76ee\u6807\u6210\u672c\u5206\u5e03\u7684\u5bf9\u9f50\u5ea6\u3002\u5b83\u662f\u552f\u4e00\u80fd\u591f\u751f\u6210\u5b9a\u5236\u5316 SQL \u6a21\u677f\u7684\u7cfb\u7edf\u3002", "conclusion": "SQLBarber \u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u53ef\u4ee5\u751f\u6210\u5b9a\u5236\u5316\u548c\u73b0\u5b9e\u7684 SQL \u5de5\u4f5c\u8d1f\u8f7d\u3002\u5b83\u5728\u751f\u6210\u65f6\u95f4\u3001\u4e0e\u76ee\u6807\u6210\u672c\u5206\u5e03\u7684\u5bf9\u9f50\u5ea6\u7b49\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.06204", "pdf": "https://arxiv.org/pdf/2507.06204", "abs": "https://arxiv.org/abs/2507.06204", "authors": ["Nadav Schneider", "Itamar Zimerman", "Eliya Nachmani"], "title": "Differential Mamba", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06Transformer\u7684\u5fae\u5206\u8bbe\u8ba1\u5e94\u7528\u5230Mamba\u4e2d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u5206\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86Mamba\u4e2d\u7684\u6ce8\u610f\u529b\u8fc7\u5ea6\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u5e8f\u5217\u6a21\u578b\u5982Transformer\u548cRNNs\u7ecf\u5e38\u8fc7\u5ea6\u5173\u6ce8\u4e0d\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u5bfc\u81f4\u4e2d\u95f4\u8868\u793a\u566a\u58f0\u5927\uff0c\u5f71\u54cdLLM\u7684\u80fd\u529b\u3002\u800cMamba\u867d\u7136\u6548\u7387\u9ad8\uff0c\u4f46\u540c\u6837\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u6587\u63a2\u7d22\u4e86\u9002\u7528\u4e8eTransformer\u7684\u5fae\u5206\u8bbe\u8ba1\u662f\u5426\u53ef\u4ee5\u5e94\u7528\u4e8eMamba\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9Mamba\u7684\u65b0\u5fae\u5206\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u5fae\u5206\u673a\u5236\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86Mamba\u7684\u68c0\u7d22\u80fd\u529b\u548c\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u5fae\u5206\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u4e86Mamba\u6a21\u578b\u4e2d\u8fc7\u5ea6\u5206\u914d\u6ce8\u610f\u529b\u7684\u95ee\u9898\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.06210", "pdf": "https://arxiv.org/pdf/2507.06210", "abs": "https://arxiv.org/abs/2507.06210", "authors": ["Yuchen Huang", "Zhiyuan Fan", "Zhitao He", "Sandeep Polisetty", "Wenyan Li", "Yi R. Fung"], "title": "CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions", "categories": ["cs.CV", "cs.CL"], "comment": "25 pages, COLM 2025", "summary": "Pretrained vision-language models (VLMs) such as CLIP excel in multimodal\nunderstanding but struggle with contextually relevant fine-grained visual\nfeatures, making it difficult to distinguish visually similar yet culturally\ndistinct concepts. This limitation stems from the scarcity of high-quality\nculture-specific datasets, the lack of integrated contextual knowledge, and the\nabsence of hard negatives highlighting subtle distinctions. To address these\nchallenges, we first design a data curation pipeline that leverages\nopen-sourced VLMs and text-to-image diffusion models to construct CulTwin, a\nsynthetic cultural dataset. This dataset consists of paired\nconcept-caption-image triplets, where concepts visually resemble each other but\nrepresent different cultural contexts. Then, we fine-tune CLIP on CulTwin to\ncreate CultureCLIP, which aligns cultural concepts with contextually enhanced\ncaptions and synthetic images through customized contrastive learning, enabling\nfiner cultural differentiation while preserving generalization capabilities.\nExperiments on culturally relevant benchmarks show that CultureCLIP outperforms\nthe base CLIP, achieving up to a notable 5.49% improvement in fine-grained\nconcept recognition on certain tasks, while preserving CLIP's original\ngeneralization ability, validating the effectiveness of our data synthesis and\nVLM backbone training paradigm in capturing subtle cultural distinctions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u6574\u7406\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u6587\u5316\u6570\u636e\u96c6CulTwin\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03CLIP\u4ee5\u521b\u5efaCultureCLIP\uff0c\u4ece\u800c\u66f4\u597d\u5730\u533a\u5206\u6587\u5316\u5dee\u5f02\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCultureCLIP\u5728\u7ec6\u7c92\u5ea6\u6982\u5ff5\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7840CLIP\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u4f46\u6587\u5316\u4e0d\u540c\u7684\u6982\u5ff5\u65f6\u5b58\u5728\u56f0\u96be\u3002\u8fd9\u79cd\u9650\u5236\u6e90\u4e8e\u9ad8\u8d28\u91cf\u6587\u5316\u7279\u5b9a\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u3001\u7f3a\u4e4f\u96c6\u6210\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u4ee5\u53ca\u7f3a\u5c11\u5f3a\u8c03\u7ec6\u5fae\u5dee\u522b\u7684\u786c\u8d1f\u6837\u672c\u3002", "method": "\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6570\u636e\u6574\u7406\u6d41\u7a0b\uff0c\u5229\u7528\u5f00\u6e90\u7684VLM\u548c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u6784\u5efaCulTwin\uff0c\u4e00\u4e2a\u5408\u6210\u6587\u5316\u6570\u636e\u96c6\u3002\u7136\u540e\u5728CulTwin\u4e0a\u5fae\u8c03CLIP\u4ee5\u521b\u5efaCultureCLIP\uff0c\u901a\u8fc7\u5b9a\u5236\u7684\u5bf9\u6bd4\u5b66\u4e60\u5c06\u6587\u5316\u6982\u5ff5\u4e0e\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u6807\u9898\u548c\u5408\u6210\u56fe\u50cf\u5bf9\u9f50\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u6587\u5316\u533a\u5206\uff0c\u540c\u65f6\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "result": "CultureCLIP\u5728\u6587\u5316\u76f8\u5173\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7840CLIP\uff0c\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u7ec6\u7c92\u5ea6\u6982\u5ff5\u8bc6\u522b\u63d0\u5347\u4e86\u663e\u8457\u76845.49%\uff0c\u540c\u65f6\u4fdd\u7559\u4e86CLIP\u539f\u6709\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cCultureCLIP\u5728\u6587\u5316\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7840CLIP\uff0c\u67d0\u4e9b\u4efb\u52a1\u7684\u7ec6\u7c92\u5ea6\u6982\u5ff5\u8bc6\u522b\u63d0\u5347\u4e86\u663e\u8457\u76845.49%\uff0c\u540c\u65f6\u4fdd\u7559\u4e86CLIP\u539f\u6709\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u6570\u636e\u5408\u6210\u548cVLM\u9aa8\u5e72\u8bad\u7ec3\u8303\u5f0f\u5728\u6355\u6349\u7ec6\u5fae\u6587\u5316\u5dee\u5f02\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}

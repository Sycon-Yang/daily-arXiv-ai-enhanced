<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

TL;DR: 本研究将大型语言模型视为“主观文学评论家”，发现它们在文学评估中表现出独特的评估特征，类似于人类批评学派，而非中立的基准器。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索文学评估中的审美偏好和评估模式，并了解大型语言模型在文学判断中的隐含价值体系。

Method: 本研究将大型语言模型（LLMs）定位为“主观文学评论家”，通过主成分分析和聚类技术分析了十篇日本科幻短篇小说的评估一致性以及五种不同的评估模式。此外，还使用TF-IDF分析确认了每种模型的独特评估词汇。

Result: 研究发现，评估一致性存在显著差异（α范围从1.00到0.35），并且评估方差在不同故事之间相差多达4.5倍。此外，研究还发现了五种不同的评估模式。

Conclusion: 研究结果表明，大型语言模型可能具有类似于人类批评学派的个体评估特征，而不是作为中立的基准器运作。

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [2] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: 本文引入了MapIQ数据集，评估了多种多模态大语言模型在Map-VQA任务中的表现，并研究了地图设计变化对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前的Map-VQA研究主要集中在等值线图上，这限制了主题类别和视觉分析任务的范围。为了弥补这些不足，本文引入了一个新的基准数据集MapIQ，以涵盖更多地图类型和主题。

Method: 本文提出了MapIQ数据集，并评估了多种多模态大语言模型（MLLMs）在六个视觉分析任务中的表现，同时进行了额外的实验来研究地图设计变化对模型性能的影响。

Result: 通过评估多种多模态大语言模型（MLLMs）在六个视觉分析任务中的表现，研究发现它们在Map-VQA任务中的性能存在差异，并且对地图设计的变化表现出不同的敏感性。

Conclusion: 本文介绍了MapIQ数据集，这是一个包含14,706个问题-答案对的基准数据集，涵盖了三种地图类型和六个不同的主题。通过评估多种多模态大语言模型（MLLMs）在六个视觉分析任务中的表现，研究提供了关于这些模型在Map-VQA任务中的性能、鲁棒性和对地图设计变化的敏感性的见解。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [3] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

TL;DR: 本研究探讨了在波斯语中使用少量学习和增量学习方法进行跨语言情感分析，结果表明结合这些方法与多语言预训练模型是有效的。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在有限数据下执行波斯语情感分析的模型，同时从高资源语言中获取先验知识。

Method: 使用少量学习和增量学习方法对三个预训练多语言模型（XLM-RoBERTa、mDeBERTa 和 DistilBERT）进行微调，以在有限的波斯语数据上进行情感分析。

Result: mDeBERTa 和 XLM-RoBERTa 在波斯语情感分析中达到了 96% 的准确率。

Conclusion: 这些发现表明，将少量学习和增量学习与多语言预训练模型结合是有效的。

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [4] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

TL;DR: PgM is a framework for multimodal learning that enhances performance by effectively capturing and utilizing both uni-modal and paired-modal features.


<details>
  <summary>Details</summary>
Motivation: The motivation behind PgM is to improve multimodal learning by better capturing and utilizing both uni-modal and paired-modal features, which can enhance performance and adaptability in diverse downstream tasks.

Method: PgM is a partitioner-guided modal learning framework that includes a modal partitioner, uni-modal learner, paired-modal learner, and uni-paired modal decoder. It segments modal representations into uni-modal and paired-modal features and allows for flexible distribution adjustment and different learning rates across modalities.

Result: Experiments demonstrate the effectiveness of PgM across four multimodal tasks and its transferability to existing models. Visualization of feature distributions provides insights into their contributions.

Conclusion: PgM provides a framework for multimodal learning that effectively captures both uni-modal and paired-modal features, offering benefits in flexibility, adaptability, and performance across various tasks.

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [5] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

TL;DR: ExpliCIT-QA 是一个模块化系统，能够处理复杂的表格图像并提供可解释的答案，提高了表格问答系统的透明度和可审计性。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端表格VQA系统存在可解释性差距，需要一种更透明和可审计的方法。

Method: ExpliCIT-QA 是一个模块化系统，包括多模态表格理解、基于语言的推理、自动代码生成、代码执行和自然语言解释。

Result: ExpliCIT-QA 在 TableVQA-Bench 基准测试中表现出更高的可解释性和透明度，优于现有基线。

Conclusion: ExpliCIT-QA 提高了表格问答系统的可解释性和透明度，为金融和医疗等敏感领域提供了应用的可能性。

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [6] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

TL;DR: CRABS is a strategy that combines syntactic analysis and LLMs to accurately understand Python notebooks, achieving high accuracy in identifying information flows and execution dependencies.


<details>
  <summary>Details</summary>
Motivation: The need to evaluate, reuse, and adapt Python notebooks for new tasks requires understanding their information flows and operations, but re-execution is often impractical due to data and software dependencies. LLMs have limitations in understanding realistic notebooks due to hallucinations and long-context challenges.

Method: CRABS uses shallow syntactic parsing and analysis of the abstract syntax tree (AST) to capture the correct interpretation of a notebook between lower and upper estimates of the inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via cell-by-cell zero-shot learning.

Result: CRABS achieves average F1 scores of 98% identifying cell-to-cell information flows and 99% identifying transitive cell execution dependencies. The LLM correctly resolves 98% of ambiguities left by analyzing the syntactic structure of the notebooks.

Conclusion: CRABS achieves high accuracy in identifying cell-to-cell information flows and transitive cell execution dependencies, demonstrating the effectiveness of combining syntactic analysis with LLMs for understanding Python notebooks.

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [7] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

TL;DR: 本文介绍了AI Wizards在CLEF 2025 CheckThat! Lab任务1中的参与，通过结合情感分数和句子表示来提升分类性能，并在多个语言中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 旨在提高新闻文章中主观/客观句子分类的性能，特别是在多语言和零样本设置下。

Method: 通过将情感分数与句子表示相结合，增强了基于变压器的分类器，并采用了决策阈值校准来解决类别不平衡问题。

Result: 情感特征的整合显著提升了性能，尤其是在主观F1分数上。该框架在多个语言中取得了高排名，包括希腊语的第一名。

Conclusion: 该框架在多个语言中表现出色，特别是在希腊语中取得了第一名的好成绩。

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [8] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型如何处理竞争性的事实和反事实信息，发现注意力头促进事实输出是通过一般复制抑制而非选择性反事实抑制，并且注意力头的行为具有领域依赖性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大型语言模型（LLMs）如何处理竞争性的事实和反事实信息，特别是注意力头在这一过程中的作用。

Method: 我们尝试重现和调和Ortu等人、Yu、Merullo和Pavlick以及McDougall等人最近的研究成果，这些研究通过机制解释工具调查了模型学习的事实与矛盾上下文信息之间的竞争。我们的研究特别考察了注意力头强度与事实输出比例之间的关系，评估了关于注意力头抑制机制的竞争假设，并研究了这些注意力模式的领域特异性。

Result: 我们的研究结果表明，促进事实输出的注意力头是通过一般的复制抑制而不是选择性反事实抑制来实现的，因为加强它们也可以抑制正确的事实。此外，我们展示了注意力头的行为是领域依赖的，大型模型表现出更多专门和类别敏感的模式。

Conclusion: 我们的研究结果表明，促进事实输出的注意力头是通过一般的复制抑制而不是选择性反事实抑制来实现的，因为加强它们也可以抑制正确的事实。此外，我们展示了注意力头的行为是领域依赖的，大型模型表现出更多专门和类别敏感的模式。

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [9] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
*Yash Ingle,Pruthwik Mishra*

Main category: cs.CL

TL;DR: 本文发布了一个包含230K句子的数据集，涵盖英语和22种印度官方语言，并开发了基于机器学习和深度学习的基线模型，这些模型在语言识别任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 语言识别任务是NLP中的关键步骤，特别是在多语言机器翻译、信息检索、问答和文本摘要等应用中。然而，在嘈杂、短文本和代码混合环境中区分语言具有挑战性，尤其是在印度多种语言之间，它们在词汇和发音上相似但有显著差异。

Method: 本文提出了一个包含230K句子的数据集，并开发了使用机器学习和深度学习的基线模型。

Result: 本文提出的基线模型在语言识别任务中表现与最先进的模型相当。

Conclusion: 本文提出了一个包含230K句子的数据集，涵盖了英语和所有22种官方印度语言，并开发了使用机器学习和深度学习的基线模型，这些模型在语言识别任务中表现良好。

Abstract: The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

</details>


### [10] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，利用自回归语言模型对未来标记的知识，实现多个后续标记的同时预测，从而显著提高生成速度而不损失质量。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型由于其固有的顺序性质，生成速度受限，尤其是在生成后期阶段，文本的方向和语义相对确定时。

Method: 提出了一种新颖的框架，利用了传统自回归语言模型对未来标记的知识，结合技术实现这一潜力，并实现多个后续标记的同时预测。

Result: 该方法通过在预训练模型上进行监督微调实现了显著的速度提升，例如生成代码和数学内容几乎快5倍，改进一般聊天和知识任务接近2.5倍。

Conclusion: 该方法在不损失质量的情况下实现了显著的速度提升，为未来的研究和应用提供了新的可能性。

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [11] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
*Junhong Ye,Xu Yuan,Xinying Qiu*

Main category: cs.CL

TL;DR: 本文研究了跨领域模型迁移、多领域数据融合和样本高效学习在PII识别中的效果。结果表明，法律领域的数据可以很好地迁移到传记文本中，而医疗领域对迁移具有抵抗力。在低专业化领域中，仅使用10%的训练数据即可实现高质量的PII识别。


<details>
  <summary>Details</summary>
Motivation: 准确识别个人可识别信息（PII）是自动化文本匿名化的关键。本文旨在研究跨领域模型迁移、多领域数据融合和样本高效学习在PII识别中的效果。

Method: 使用医疗（I2B2）、法律（TAB）和传记（维基百科）的标注语料库，评估模型在四个维度上的表现：领域内性能、跨领域迁移能力、融合和少样本学习。

Result: 结果显示，法律领域的数据可以很好地迁移到传记文本中，而医疗领域对迁移具有抵抗力。融合的好处是领域特定的，并且在低专业化领域中仅使用10%的训练数据即可实现高质量的识别。

Conclusion: 高精度的PII识别对于自动文本匿名化至关重要。本文研究了跨领域模型迁移、多领域数据融合和样本高效学习在PII识别中的有效性。

Abstract: Accurate recognition of personally identifiable information (PII) is central
to automated text anonymization. This paper investigates the effectiveness of
cross-domain model transfer, multi-domain data fusion, and sample-efficient
learning for PII recognition. Using annotated corpora from healthcare (I2B2),
legal (TAB), and biography (Wikipedia), we evaluate models across four
dimensions: in-domain performance, cross-domain transferability, fusion, and
few-shot learning. Results show legal-domain data transfers well to
biographical texts, while medical domains resist incoming transfer. Fusion
benefits are domain-specific, and high-quality recognition is achievable with
only 10% of training data in low-specialization domains.

</details>


### [12] [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)
*Xiangyu Yang,Xinying Qiu*

Main category: cs.CL

TL;DR: 本文介绍了一种新的双向框架COLA-GEC，通过相互知识迁移来增强GEC和COLA任务，取得了先进的成果，并指出了未来的改进方向。


<details>
  <summary>Details</summary>
Motivation: Grammatical Error Correction (GEC)和Grammatical Acceptability Judgment (COLA)是自然语言处理中的核心任务，虽然共享基础的语法知识，但通常独立发展。因此，本文旨在通过相互知识迁移来提升这两个任务的性能。

Method: 本文提出了COLA-GEC，这是一种新型的双向框架，通过使用GEC数据集增强语法可接受性模型，并通过动态损失函数将语法可接受性信号集成到GEC模型训练中。

Result: 本文的方法在多个多语言基准上取得了最先进的成果，并通过全面的错误分析揭示了剩余挑战，特别是在标点符号错误纠正方面。

Conclusion: 本文提出了一种新的双向框架COLA-GEC，通过相互知识迁移来增强GEC和COLA任务。实验结果表明，该方法在多个多语言基准上取得了最先进的成果，并为未来在语法建模方面的改进提供了见解。

Abstract: Grammatical Error Correction (GEC) and grammatical acceptability judgment
(COLA) are core tasks in natural language processing, sharing foundational
grammatical knowledge yet typically evolving independently. This paper
introduces COLA-GEC, a novel bidirectional framework that enhances both tasks
through mutual knowledge transfer. First, we augment grammatical acceptability
models using GEC datasets, significantly improving their performance across
multiple languages. Second, we integrate grammatical acceptability signals into
GEC model training via a dynamic loss function, effectively guiding corrections
toward grammatically acceptable outputs. Our approach achieves state-of-the-art
results on several multilingual benchmarks. Comprehensive error analysis
highlights remaining challenges, particularly in punctuation error correction,
providing insights for future improvements in grammatical modeling.

</details>


### [13] [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)
*Tianyou Huang,Xinglu Chen,Jingshen Zhang,Xinying Qiu,Ruiying Niu*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习框架 DualReward，用于自动生成闭塞测试中的干扰项。该方法利用双奖励结构和自适应缩放，提高了生成干扰项的质量，并在多个数据集上取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 传统的 approaches 主要依赖于监督学习或静态生成模型，而本文旨在通过一种新的强化学习框架来改进干扰项生成，以提高自动测试生成的效果。

Method: 我们引入了DualReward，这是一种新的强化学习框架，用于闭塞测试中的自动干扰项生成。该方法采用双奖励结构和自适应缩放，区分人类创建的黄金标准干扰项和模型生成的候选干扰项，并根据模型性能和置信度动态调整奖励信号强度。

Result: 我们在 passage-level (CLOTH-F) 和 sentence-level (MCQ) 闭塞测试数据集上评估了我们的方法，结果表明我们的方法在最先进的基线上有持续的改进。实验结果表明，我们的自适应奖励缩放机制在同质数据集（CLOTH-F）上提供了适度但一致的好处，并在多样化的跨领域数据（MCQ）上提供了更大的改进（P@1 提升 3.48-3.86%）。

Conclusion: 我们的工作提供了一个灵活的框架，能够有效地在学习可靠的真人示例和探索新颖的高质量干扰项之间取得平衡，从而实现自动测试生成。

Abstract: This paper introduces DualReward, a novel reinforcement learning framework
for automatic distractor generation in cloze tests. Unlike conventional
approaches that rely primarily on supervised learning or static generative
models, our method employs a dual reward structure with adaptive scaling that
differentiates between human-created gold standard distractors and
model-generated candidates. The framework dynamically adjusts reward signal
intensity based on model performance and confidence. We evaluate our approach
on both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,
demonstrating consistent improvements over state-of-the-art baselines.
Experimental results show that our adaptive reward scaling mechanism provides
modest but consistent benefits on homogeneous datasets (CLOTH-F) and more
substantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data
(MCQ), suggesting its particular effectiveness for handling varied question
types and domains. Our work offers a flexible framework that effectively
balances learning from reliable human examples while exploring novel,
high-quality distractors for automated test generation.

</details>


### [14] [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
*Jiachen Zhao,Jing Huang,Zhengxuan Wu,David Bau,Weiyan Shi*

Main category: cs.CL

TL;DR: This paper identifies a new dimension of safety in LLMs called harmfulness, which is separate from refusal. It proposes a practical safety application called Latent Guard that uses this concept to detect unsafe inputs and reduce over-refusals.


<details>
  <summary>Details</summary>
Motivation: To understand whether LLMs truly understand harmfulness beyond just refusing harmful instructions, and to develop a more robust safety mechanism.

Method: Identifying a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, and using it to develop a practical safety application called Latent Guard.

Result: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks.

Conclusion: LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety.

Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand
harmfulness beyond just refusing? Prior work has shown that LLMs' refusal
behaviors can be mediated by a one-dimensional subspace, i.e., a refusal
direction. In this work, we identify a new dimension to analyze safety
mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a
separate concept from refusal. There exists a harmfulness direction that is
distinct from the refusal direction. As causal evidence, steering along the
harmfulness direction can lead LLMs to interpret harmless instructions as
harmful, but steering along the refusal direction tends to elicit refusal
responses directly without reversing the model's judgment on harmfulness.
Furthermore, using our identified harmfulness concept, we find that certain
jailbreak methods work by reducing the refusal signals without reversing the
model's internal belief of harmfulness. We also find that adversarially
finetuning models to accept harmful instructions has minimal impact on the
model's internal belief of harmfulness. These insights lead to a practical
safety application: The model's latent harmfulness representation can serve as
an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing
over-refusals that is robust to finetuning attacks. For instance, our Latent
Guard achieves performance comparable to or better than Llama Guard 3 8B, a
dedicated finetuned safeguard model, across different jailbreak methods. Our
findings suggest that LLMs' internal understanding of harmfulness is more
robust than their refusal decision to diverse input instructions, offering a
new perspective to study AI safety

</details>


### [15] [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)
*Bo Zeng,Chenyang Lyu,Sinuo Liu,Mingyan Zeng,Minghao Wu,Xuanfan Ni,Tianqi Shi,Yu Zhao,Yefeng Liu,Chenyu Zhu,Ruizhe Li,Jiahui Geng,Qing Li,Yu Tong,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种本地化的多语言基准Marco-Bench-MIF，用于评估大型语言模型的指令遵循能力，发现高/低资源语言之间的准确率差距以及机器翻译数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集如IFEval主要以英语为中心，或仅通过机器翻译到其他语言，限制了其在多语言环境中的应用。因此，需要一个更本地化的多语言基准来评估大型语言模型的指令遵循能力。

Method: 本文通过结合翻译与验证的混合流程，对IFEval进行了本地化扩展，形成了覆盖30种语言的Marco-Bench-MIF基准测试。

Result: 在Marco-Bench-MIF上评估20多个LLM后，发现高/低资源语言之间存在25-35%的准确率差距，模型规模对性能的影响为45-60%，但仍然存在脚本特定挑战，且机器翻译数据低估了准确率7-22%。

Conclusion: 本文提出了一个本地化的多语言版本Marco-Bench-MIF，以解决现有数据集在多语言环境中的适用性问题。通过全面评估20多个LLM，发现高/低资源语言之间存在25-35%的准确率差距，并指出机器翻译数据低估了准确率7-22%。

Abstract: Instruction-following capability has become a major ability to be evaluated
for Large Language Models (LLMs). However, existing datasets, such as IFEval,
are either predominantly monolingual and centered on English or simply machine
translated to other languages, limiting their applicability in multilingual
contexts. In this paper, we present an carefully-curated extension of IFEval to
a localized multilingual version named Marco-Bench-MIF, covering 30 languages
with varying levels of localization. Our benchmark addresses linguistic
constraints (e.g., modifying capitalization requirements for Chinese) and
cultural references (e.g., substituting region-specific company names in
prompts) via a hybrid pipeline combining translation with verification. Through
comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)
25-35% accuracy gap between high/low-resource languages, (2) model scales
largely impact performance by 45-60% yet persists script-specific challenges,
and (3) machine-translated data underestimates accuracy by7-22% versus
localized data. Our analysis identifies challenges in multilingual instruction
following, including keyword consistency preservation and compositional
constraint adherence across languages. Our Marco-Bench-MIF is available at
https://github.com/AIDC-AI/Marco-Bench-MIF.

</details>


### [16] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文综述了深度学习在几何问题求解中的应用，包括相关任务、方法、评估指标和当前挑战，并提供了一个持续更新的论文列表。


<details>
  <summary>Details</summary>
Motivation: 几何问题求解是数学推理的关键领域，广泛涉及教育、人工智能数学能力评估和多模态能力评估。近年来，深度学习技术的快速发展，特别是多模态大语言模型的兴起，引发了广泛的研究热潮。

Method: 本文对几何问题求解中深度学习的应用进行了综述，包括相关任务的全面总结、相关深度学习方法的详细回顾、评估指标和方法的分析，以及当前挑战和未来方向的讨论。

Result: 本文提供了几何问题求解中深度学习应用的全面综述，并创建了一个持续更新的论文列表在GitHub上。

Conclusion: 本文旨在为几何问题求解中的深度学习提供全面且实用的参考，以促进该领域进一步发展。

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [17] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文介绍了PolyChartQA，这是第一个大规模多语言图表问答基准，涵盖了22,606个图表和26,151个跨10种不同语言的问题-答案对。


<details>
  <summary>Details</summary>
Motivation: 现有的图表理解基准主要以英语为中心，限制了其对全球受众的可访问性和适用性。

Method: 使用解耦管道，将图表数据与渲染代码分离，通过翻译数据并重用代码来灵活生成多语言图表。利用最先进的基于LLM的翻译并实施严格的质量控制以确保生成的多语言图表的语言和语义一致性。

Result: 在开放源代码和闭源大型视觉-语言模型上的实验揭示了英语和其他语言之间，尤其是使用非拉丁文字的低资源语言之间的显著性能差距。

Conclusion: 该基准为推进全球包容性的视觉-语言模型奠定了基础。

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [18] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: BlockBPE 是一种高效的 GPU 并行 BPE 实现，通过消除正则表达式预分词步骤，提高了批量推理的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的 Rust 基础分词器如 HuggingFace Tokenizers 或 OpenAI 的 tiktoken 在运行时间上受到正则表达式预分词的限制，导致运行时间为 O(n log n)。

Method: BlockBPE 是一种并行 GPU 实现的字节对编码 (BPE)，通过消除正则表达式预分词步骤，实现了高度并行化的词元合并。

Result: 在高批量推理工作负载中，BlockBPE 的吞吐量比 tiktoken 高达 2 倍，比 HuggingFace Tokenizers 高达 2.5 倍。

Conclusion: BlockBPE 是一种高效的 GPU 并行实现，能够在高吞吐量的批量推理中显著提高性能。

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [19] [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)
*Yi Zhao,Zuchao Li,Hai Zhao,Baoyuan Qi,Guoming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种动态注意力感知的提示压缩方法（DAC），通过结合熵和注意力信息，在多个领域和大型语言模型中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了算法层面的关注关键标记的重要性以及压缩过程中信息熵的变化。

Method: 提出了一种动态注意力感知的方法（DAC），结合熵和注意力信息，动态检测压缩过程中的熵变化，实现细粒度的提示压缩。

Result: 在LongBench、GSM8K和BBH等多个领域进行了广泛实验，结果表明DAC在各种任务和LLM中都能持续带来显著的改进。

Conclusion: DAC在多个领域和大型语言模型中表现出色，证明了其有效性。

Abstract: Task-agnostic prompt compression leverages the redundancy in natural language
to reduce computational overhead and enhance information density within
prompts, especially in long-context scenarios. Existing methods predominantly
rely on information entropy as the metric to compress lexical units, aiming to
achieve minimal information loss. However, these approaches overlook two
critical aspects: (i) the importance of attention-critical tokens at the
algorithmic level, and (ii) shifts in information entropy during the
compression process. Motivated by these challenges, we propose a dynamic
attention-aware approach for task-agnostic prompt compression (DAC). This
approach effectively integrates entropy and attention information, dynamically
sensing entropy shifts during compression to achieve fine-grained prompt
compression. Extensive experiments across various domains, including LongBench,
GSM8K, and BBH, show that DAC consistently yields robust and substantial
improvements across a diverse range of tasks and LLMs, offering compelling
evidence of its efficacy.

</details>


### [20] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为IAM的框架，通过在小模型和大模型之间进行注意力映射，实现了加速注意力计算和减少KV缓存使用的效果。


<details>
  <summary>Details</summary>
Motivation: LLMs在资源消耗方面面临重大挑战，尤其是在长上下文情况下。尽管已有大量努力用于提高推理效率，但这些方法主要利用模型内部的稀疏性，而没有利用外部信息进行优化。

Method: 通过分析如何测量相似性、如何选择映射层以及映射是否一致，引入了IAM框架，该框架通过在小型和大型LLM之间执行注意力映射来实现加速注意力计算和减少KV缓存使用。

Result: 实验结果表明，IAM可以加速预填充15%，并减少KV缓存使用22.1%，并且在不同系列的模型上进行了实验，证明了IAM的通用性。

Conclusion: IAM框架在不明显牺牲性能的情况下，可以加速预填充并减少KV缓存使用，并且与许多现有的KV缓存优化方法正交，使其成为增强LLM效率的多功能工具。

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [21] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段查询框架，以提高知识图谱问答在多跳和时间问题上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在问答（QA）方面表现出色，但仍难以处理多跳推理和时间问题。基于查询的知识图谱QA（KGQA）提供了一种模块化替代方案，通过生成可执行查询而不是直接答案。

Method: 我们探索了用于WikiData QA的多阶段查询框架，并提出了增强性能的多阶段方法。此外，我们引入了一种使用CoT推理的实体链接和谓词匹配方法。

Result: 通过泛化和拒绝研究，我们在多跳和时间QA数据集上评估了鲁棒性。结果表明，基于查询的多阶段KGQA框架在小型语言模型上具有改进多跳和时间QA的潜力。

Conclusion: 我们的结果表明，基于查询的多阶段KGQA框架在小型语言模型上具有改进多跳和时间QA的潜力。

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [22] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Main category: cs.CL

TL;DR: 本文提出了一种新的PoT量化框架，能够在极低精度格式中实现最先进的准确性，并通过更高效的反量化实现更快的推理。


<details>
  <summary>Details</summary>
Motivation: 由于GPU上的PoT量化在反量化时存在符号位和顺序位操作的纠缠问题，导致效果不佳，因此需要一种新的PoT量化框架来解决这一问题。

Method: 为了保持量化模型的准确性，引入了一个两步的训练后算法：(i) 用一个稳健的起点初始化量化比例，(ii) 使用一个最小的校准集来优化这些比例。

Result: 本文提出的PoT量化在整数量化方面超越了当前最先进的技术，特别是在低精度如2-和3-位格式中。此外，在NVIDIA V100和RTX 4090上，与统一整数反量化相比，反量化步骤的速度分别提高了3.67倍和1.63倍。

Conclusion: 本文提出了一种新的PoT量化框架，能够在极低精度格式中实现最先进的准确性，并通过更高效的反量化实现更快的推理。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [23] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
*Ziyu Ge,Gabriel Chua,Leanne Tan,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文提出了一种可重复的两阶段框架，用于保留有毒内容的翻译，特别是在处理低资源语言对时。


<details>
  <summary>Details</summary>
Motivation: 标准翻译系统在处理非主流语言和口语方言时往往无法保留当地的俚语、混合语言和有害言论的文化嵌入标记。此外，低资源语言对之间的有毒内容翻译面临数据稀缺和安全过滤器的挑战。

Method: 我们提出了一个可重复的两阶段框架，首先进行人工验证的少样本提示工程，然后通过直接翻译和反向翻译对模型-提示对进行优化。

Result: 定量的人工评估证实了我们的管道的有效性和效率。

Conclusion: 我们的框架有助于在低资源环境中支持文化敏感的监管和基准测试，并强调了在现实世界应用中保留社会语言细微差别的的重要性。

Abstract: As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

</details>


### [24] [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)
*Yuhong Zhang,Jialu Li,Shilai Yang,Yuchen Xu,Gert Cauwenberghs,Tzyy-Ping Jung*

Main category: cs.CL

TL;DR: 本研究使用基于LLM的AI代理构建图文本表示，并比较眼动分布，发现LLMs在图拓扑结构层面的语言理解具有一致性，为有效的人机协同学习策略提供了见解。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，需要比较人类和LLM如何在不同上下文中理解语言并将其应用于推理、情感解释和信息检索等功能任务。

Method: 本研究使用基于LLM的AI代理将阅读段落中的单词分组为节点和边，形成基于语义意义和问题导向提示的图文本表示。然后比较重要节点和边上的眼动分布。

Result: LLMs在图拓扑结构层面的语言理解上表现出高度一致性。

Conclusion: 这些结果建立在我们之前的研究成果之上，并提供了有效的师生协同学习策略的见解。

Abstract: Reading comprehension is a fundamental skill in human cognitive development.
With the advancement of Large Language Models (LLMs), there is a growing need
to compare how humans and LLMs understand language across different contexts
and apply this understanding to functional tasks such as inference, emotion
interpretation, and information retrieval. Our previous work used LLMs and
human biomarkers to study the reading comprehension process. The results showed
that the biomarkers corresponding to words with high and low relevance to the
inference target, as labeled by the LLMs, exhibited distinct patterns,
particularly when validated using eye-tracking data. However, focusing solely
on individual words limited the depth of understanding, which made the
conclusions somewhat simplistic despite their potential significance. This
study used an LLM-based AI agent to group words from a reading passage into
nodes and edges, forming a graph-based text representation based on semantic
meaning and question-oriented prompts. We then compare the distribution of eye
fixations on important nodes and edges. Our findings indicate that LLMs exhibit
high consistency in language understanding at the level of graph topological
structure. These results build on our previous findings and offer insights into
effective human-AI co-learning strategies.

</details>


### [25] [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)
*Yuki Sakamoto,Takahisa Uchida,Hiroshi Ishiguro*

Main category: cs.CL

TL;DR: 本研究探讨了价值观相似性对LLM代理之间关系建立的影响，发现价值观相似性高的代理对表现出更大的信任和亲密关系。


<details>
  <summary>Details</summary>
Motivation: 在人类社会中，价值观相似性对于建立信任和亲密关系很重要；然而，尚不清楚这一原则是否适用于由LLM代理组成的人工社会。因此，本研究旨在探讨价值观相似性对LLM代理之间关系建立的影响。

Method: 本研究通过两个实验探讨了价值观相似性对LLM代理之间关系建立的影响。首先，在初步实验中，我们评估了LLM中的价值观可控性，以确定最有效的模型和提示设计来控制价值观。随后，在主要实验中，我们生成了具有特定价值观的LLM代理对，并分析了它们在对话后相互评估的信任和人际关系亲密程度。实验在英语和日语中进行，以研究语言依赖性。

Result: 实验结果确认了具有更高价值观相似性的代理对表现出更大的相互信任和人际关系亲密性。

Conclusion: 我们的研究结果表明，LLM代理模拟可以作为社会科学研究理论的有效测试平台，有助于阐明价值观如何影响关系建立，并为激发社会科学的新理论和见解提供基础。

Abstract: Large language models (LLMs) have emerged as powerful tools for simulating
complex social phenomena using human-like agents with specific traits. In human
societies, value similarity is important for building trust and close
relationships; however, it remains unexplored whether this principle holds true
in artificial societies comprising LLM agents. Therefore, this study
investigates the influence of value similarity on relationship-building among
LLM agents through two experiments. First, in a preliminary experiment, we
evaluated the controllability of values in LLMs to identify the most effective
model and prompt design for controlling the values. Subsequently, in the main
experiment, we generated pairs of LLM agents imbued with specific values and
analyzed their mutual evaluations of trust and interpersonal closeness
following a dialogue. The experiments were conducted in English and Japanese to
investigate language dependence. The results confirmed that pairs of agents
with higher value similarity exhibited greater mutual trust and interpersonal
closeness. Our findings demonstrate that the LLM agent simulation serves as a
valid testbed for social science theories, contributes to elucidating the
mechanisms by which values influence relationship building, and provides a
foundation for inspiring new theories and insights into the social sciences.

</details>


### [26] [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)
*Lukas Ellinger,Miriam Anschütz,Georg Groh*

Main category: cs.CL

TL;DR: 本研究探讨了简化对同义词定义质量的影响，发现简化会降低定义的完整性并增加误解风险，同时提出通过微调模型来改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究简化对同义词定义质量的影响，特别是在不同目标群体（如正常、简单和ELI5）中的影响，以确保教育NLP中定义的可靠性和上下文感知性。

Method: 使用两个新的评估数据集，跨越多种语言，通过LLM-as-Judge和人工标注测试DeepSeek v3、Llama 4 Maverick、Qwen3-30B A3B、GPT-4o mini和Llama 3.1 8B。

Result: 研究结果表明，简化显著降低了同义词定义的完整性，忽视多义性，增加了误解的风险。微调Llama 3.1 8B通过直接偏好优化显著提高了所有提示类型下的同义词响应质量。

Conclusion: 研究结果表明，简化会显著降低同义词定义的完整性，忽视多义性，增加误解的风险。微调Llama 3.1 8B通过直接偏好优化显著提高了所有提示类型下的同义词响应质量。这些发现强调了在教育NLP中平衡简洁性和完整性的必要性，以确保所有学习者都能获得可靠、上下文感知的定义。

Abstract: Large Language Models (LLMs) can provide accurate word definitions and
explanations for any context. However, the scope of the definition changes for
different target groups, like children or language learners. This is especially
relevant for homonyms, words with multiple meanings, where oversimplification
might risk information loss by omitting key senses, potentially misleading
users who trust LLM outputs. We investigate how simplification impacts homonym
definition quality across three target groups: Normal, Simple, and ELI5. Using
two novel evaluation datasets spanning multiple languages, we test DeepSeek v3,
Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge
and human annotations. Our results show that simplification drastically
degrades definition completeness by neglecting polysemy, increasing the risk of
misunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization
substantially improves homonym response quality across all prompt types. These
findings highlight the need to balance simplicity and completeness in
educational NLP to ensure reliable, context-aware definitions for all learners.

</details>


### [27] [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)
*Josip Jukić*

Main category: cs.CL

TL;DR: 该论文研究了神经语言模型中的数据和参数效率问题，提出了一系列基于表示平滑性的优化技术，并通过实验验证了其在提升模型性能、稳定性和效率方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决神经语言模型中的数据和参数效率问题，提高模型的鲁棒性和泛化能力。

Method: 论文提出了基于表示平滑性的正则化策略，利用雅可比矩阵和海森矩阵来稳定训练，并提出了基于表示平滑性的早期停止技术，以及主动学习与参数高效微调的创新组合。此外，还探索了增强的弱监督技术，结合上下文学习以有效利用未标记数据。

Result: 实验结果表明，这些综合方法在各种自然语言处理任务中显著优于传统方法，特别是在低资源环境和动态数据环境中表现出色。

Conclusion: 该论文通过结合主动学习、参数高效微调和基于上下文的学习，显著提高了模型在低资源环境和动态数据环境中的性能、稳定性和效率。

Abstract: This thesis addresses challenges related to data and parameter efficiency in
neural language models, with a focus on representation analysis and the
introduction of new optimization techniques. The first part examines the
properties and dynamics of language representations within neural models,
emphasizing their significance in enhancing robustness and generalization. It
proposes innovative approaches based on representation smoothness, including
regularization strategies that utilize Jacobian and Hessian matrices to
stabilize training and mitigate sensitivity to input perturbations. The second
part focuses on methods to significantly enhance data and parameter efficiency
by integrating active learning strategies with parameter-efficient fine-tuning,
guided by insights from representation smoothness analysis. It presents
smoothness-informed early-stopping techniques designed to eliminate the need
for labeled validation sets and proposes innovative combinations of active
learning and parameter-efficient fine-tuning to reduce labeling efforts and
computational resources. Extensive experimental evaluations across various NLP
tasks demonstrate that these combined approaches substantially outperform
traditional methods in terms of performance, stability, and efficiency. The
third part explores weak supervision techniques enhanced by in-context learning
to effectively utilize unlabeled data, further reducing dependence on extensive
labeling. It shows that using in-context learning as a mechanism for weak
supervision enables models to better generalize from limited labeled data by
leveraging unlabeled examples more effectively during training. Comprehensive
empirical evaluations confirm significant gains in model accuracy,
adaptability, and robustness, especially in low-resource settings and dynamic
data environments.

</details>


### [28] [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)
*Anca Dinu,Andra-Maria Florescu,Alina Resceanu*

Main category: cs.CL

TL;DR: A linguistic creativity test was developed to compare human and LLM performance, revealing that LLMs excel in most tasks and exhibit different creativity patterns.


<details>
  <summary>Details</summary>
Motivation: To assess the linguistic creativity of humans and LLMs by designing a comprehensive test that evaluates their ability to generate new words and phrases.

Method: The paper introduced a linguistic creativity test with tasks assessing word formation and metaphorical language use. The test was administered to 24 humans and 24 LLMs, and answers were evaluated using the OCSAI tool for originality, elaboration, and flexibility.

Result: LLMs outperformed humans in all assessed criteria and performed better in six out of eight tasks. Humans showed more E-creativity, while LLMs favored F-creativity.

Conclusion: LLMs outperformed humans in the linguistic creativity test, showing better performance in most tasks and different types of creativity.

Abstract: The following paper introduces a general linguistic creativity test for
humans and Large Language Models (LLMs). The test consists of various tasks
aimed at assessing their ability to generate new original words and phrases
based on word formation processes (derivation and compounding) and on
metaphorical language use. We administered the test to 24 humans and to an
equal number of LLMs, and we automatically evaluated their answers using OCSAI
tool for three criteria: Originality, Elaboration, and Flexibility. The results
show that LLMs not only outperformed humans in all the assessed criteria, but
did better in six out of the eight test tasks. We then computed the uniqueness
of the individual answers, which showed some minor differences between humans
and LLMs. Finally, we performed a short manual analysis of the dataset, which
revealed that humans are more inclined towards E(extending)-creativity, while
LLMs favor F(ixed)-creativity.

</details>


### [29] [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)
*Anthony G Cohn,Robert E Blackwell*

Main category: cs.CL

TL;DR: 本文评估了28个大型语言模型在确定正确 cardinal directions 方面的能力，并发现即使是较新的模型也存在局限性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究大型语言模型在处理与 cardinal directions 相关的问题时的表现，特别是它们在不同情境下的推理能力。

Method: 本文使用一组模板生成基准测试，评估28个大型语言模型（LLMs）在给定特定情境下确定正确 cardinal directions (CDs) 的能力。

Result: 即使是最新的大型推理模型也无法可靠地确定所有问题的正确 CD。

Conclusion: 本文总结并扩展了之前在COSIT-24上发表的工作，指出即使是较新的大型推理模型也无法可靠地确定所有问题的正确方向。

Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason
about cardinal directions (CDs) using a benchmark generated from a set of
templates, extensively testing an LLM's ability to determine the correct CD
given a particular scenario. The templates allow for a number of degrees of
variation such as means of locomotion of the agent involved, and whether set in
the first, second or third person. Even the newer Large Reasoning Models are
unable to reliably determine the correct CD for all questions. This paper
summarises and extends earlier work presented at COSIT-24.

</details>


### [30] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Main category: cs.CL

TL;DR: 本文提出了一种基于模块化风格学管道的二元AI检测方法，使用spaCy模型进行文本预处理，并利用轻量级梯度提升机作为分类器，在大量机器生成文本数据上进行了训练，具有计算成本低且可解释性强的特点。


<details>
  <summary>Details</summary>
Motivation: 本文旨在开发一种计算成本低且可解释性强的二元AI检测方法，以提高检测效果。

Method: 本文使用公开的spaCy模型进行文本预处理，包括分词、命名实体识别、依存句法分析、词性标注和形态学注释，并提取数千个特征（如上述语言注释的n-gram频率）。然后使用轻量级梯度提升机作为分类器。

Result: 本文通过探索多种参数选项来增加分类器的能力，并利用训练集取得了良好的效果。

Conclusion: 本文提出了一种基于模块化风格学管道的二元AI检测方法，使用公开的spaCy模型进行文本预处理，并利用轻量级梯度提升机作为分类器。该方法在大量机器生成文本数据上进行了训练，具有计算成本低且可解释性强的特点。

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [31] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
*Giuliano Martinelli,Tommaso Bonomo,Pere-Lluís Huguet Cabot,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文提出了一种自动的共指解析标注管道，并创建了第一个书规模的共指解析基准BOOKCOREF，以评估系统在长文档中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的共指解析评估基准在长度上有限，无法充分评估系统在书规模下的能力。

Method: 本文首先提出了一种自动的管道来生成高质量的共指解析标注，然后利用该管道创建了BOOKCOREF基准。

Result: 实验显示，使用BOOKCOREF基准可以提升当前长文档共指解析系统的性能，最多可提高20个CoNLL-F1分数。此外，还发现了在书规模设置下出现的新挑战。

Conclusion: 本文提出了一个自动的管道来生成高质量的共指解析标注，并创建了第一个书规模的共指解析基准BOOKCOREF。实验表明，该资源能够提升当前长文档共指解析系统的性能，并揭示了在书规模设置下出现的新挑战。

Abstract: Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [32] [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)
*Tosin Adewumi,Foteini Simistira Liwicki,Marcus Liwicki,Viktor Gardelli,Lama Alkhaled,Hamam Mokayed*

Main category: cs.CL

TL;DR: 本研究探讨了结合苏格拉底方法、思维链推理、简化游戏化和形成性反馈的MEGA方法对大学生数学学习的影响，并发现MEGA方法在两个数据集上均优于传统CoT方法，尤其是在更难的MATH数据集上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 一些学生在数学学习中遇到困难，导致他们避免与数学相关的学科或主题，而数学在许多领域都很重要。学生的数学困难通常源于不良的教学方法。

Method: 本研究采用了一种结合苏格拉底方法、思维链（CoT）推理、简化游戏化和形成性反馈的干预方法，称为MEGA。通过随机分配问题，使用组内设计比较了MEGA方法与传统CoT方法的效果。

Result: 研究结果表明，学生在更多情况下认为MEGA方法在学习效果上优于传统CoT方法，特别是在MATH数据集上，MEGA方法的表现明显优于CoT方法（47.5% vs 26.67%）。

Conclusion: 研究结果显示，MEGA方法在两个数据集上都比传统的CoT方法更受学生欢迎，特别是在更难的MATH数据集上，MEGA方法的表现显著优于CoT方法。

Abstract: This paper presents an intervention study on the effects of the combined
methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)
simplified gamification and (4) formative feedback on university students'
Maths learning driven by large language models (LLMs). We call our approach
Mathematics Explanations through Games by AI LLMs (MEGA). Some students
struggle with Maths and as a result avoid Math-related discipline or subjects
despite the importance of Maths across many fields, including signal
processing. Oftentimes, students' Maths difficulties stem from suboptimal
pedagogy. We compared the MEGA method to the traditional step-by-step (CoT)
method to ascertain which is better by using a within-group design after
randomly assigning questions for the participants, who are university students.
Samples (n=60) were randomly drawn from each of the two test sets of the Grade
School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)
datasets, based on the error margin of 11%, the confidence level of 90%, and a
manageable number of samples for the student evaluators. These samples were
used to evaluate two capable LLMs at length (Generative Pretrained Transformer
4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for
capability. The results showed that students agree in more instances that the
MEGA method is experienced as better for learning for both datasets. It is even
much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH
dataset, indicating that MEGA is better at explaining difficult Maths problems.

</details>


### [33] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的文本增强原则性评估框架，包括可扩展性分析和迭代增强与摘要优化，实验证明GPT-3.5 Turbo在语义保真度、多样性和生成效率方面表现最佳，并在实际应用中显著提升了主题建模的效果。


<details>
  <summary>Details</summary>
Motivation: 文本数据增强是一种广泛用于缓解自然语言处理（NLP）中数据稀疏性的策略，特别是在低资源环境中，有限的样本阻碍了有效的语义建模。虽然增强可以提高输入多样性并改善下游可解释性，但现有技术通常缺乏确保大规模或迭代生成时语义保留的机制，导致冗余和不稳定。

Method: 本文引入了一个基于大语言模型（LLM）文本增强的原则性评估框架，包括两个组件：(1) 可扩展性分析，测量随着增强量增加的语义一致性；(2) 迭代增强与摘要优化（IASR），评估递归改写循环中的语义漂移。

Result: 在最先进的LLM上进行的实证评估表明，GPT-3.5 Turbo在语义保真度、多样性和生成效率之间达到了最佳平衡。应用于使用GPT增强的少样本标记的Bertopic真实世界主题建模任务，该方法使主题粒度增加了400%，并完全消除了主题重叠。

Conclusion: 这些发现验证了所提出的框架在实际NLP流水线中对基于LLM的增强进行结构化评估的实用性。

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [34] [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)
*Pavel Šindelář,Ondřej Bojar*

Main category: cs.CL

TL;DR: ELOQUENT的Sensemaking任务评估生成模型如何从给定文本中“理解”内容，分为三个步骤：准备问题、回答问题和评分。虽然模型在问题回答方面表现尚可，但限制答案仅基于输入文本仍存在问题，且使用LLM作为评判者的系统在评估时存在错误。


<details>
  <summary>Details</summary>
Motivation: ELOQUENT是一个共享任务，旨在创建易于测试的高级标准，以评估生成语言模型。Sensemaking是其中的一个任务，旨在评估生成模型如何从给定文本中“理解”内容。

Method: Sensemaking任务分为三个步骤：(1) 教师系统准备问题集，(2) 学生系统回答这些问题，(3) 评估者系统评分。我们采用了完全自动化的评估方法，并与最小化的手动评估进行了比较。

Result: 在2025年的Sensemaking任务中，有7种测试材料来源，4个团队参与，提供了2个教师提交、2个学生提交和2个评估者提交。我们为教师和学生添加了基线，并设计了一个完全自动化的评估过程。

Conclusion: 在Sensemaking任务中，我们发现生成模型在问题回答方面表现尚可，但限制其答案仅基于给定文本仍然存在问题。此外，使用LLM作为评判者的系统在评估问题答案时存在错误。

Abstract: ELOQUENT is a set of shared tasks that aims to create easily testable
high-level criteria for evaluating generative language models. Sensemaking is
one such shared task.
  In Sensemaking, we try to assess how well generative models ``make sense out
of a given text'' in three steps inspired by exams in a classroom setting: (1)
Teacher systems should prepare a set of questions, (2) Student systems should
answer these questions, and (3) Evaluator systems should score these answers,
all adhering rather strictly to a given set of input materials.
  We report on the 2025 edition of Sensemaking, where we had 7 sources of test
materials (fact-checking analyses of statements, textbooks, transcribed
recordings of a lecture, and educational videos) spanning English, German,
Ukrainian, and Czech languages.
  This year, 4 teams participated, providing us with 2 Teacher submissions, 2
Student submissions, and 2 Evaluator submissions. We added baselines for
Teacher and Student using commercial large language model systems. We devised a
fully automatic evaluation procedure, which we compare to a minimalistic manual
evaluation.
  We were able to make some interesting observations. For the first task, the
creation of questions, better evaluation strategies will still have to be
devised because it is difficult to discern the quality of the various candidate
question sets. In the second task, question answering, the LLMs examined
overall perform acceptably, but restricting their answers to the given input
texts remains problematic. In the third task, evaluation of question answers,
our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm
erroneously rate both garbled question-answer pairs and answers to mixed-up
questions as acceptable.

</details>


### [35] [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)
*Michael Carl,Takanori Mizowaki,Aishvarya Ray,Masaru Yamada,Devi Sri Bandaru,Xinyue Ren*

Main category: cs.CL

TL;DR: 论文提出了一种行为翻译风格空间（BTSS），用于描述翻译行为模式，并通过分析击键和注视数据来构建一个分层结构，以模拟人类翻译中的情感、自动化行为和认知动态。


<details>
  <summary>Details</summary>
Motivation: 为了描述可能的行为翻译模式，并理解翻译行为背后的认知和情感过程。

Method: 通过分析击键记录和注视数据，将行为模式组织为多层嵌套的BTSS。

Result: 提出了BTSS，它是一个分层结构，包含各种嵌入式处理层，能够反映隐藏的心理加工结构。

Conclusion: BTSS可以作为计算翻译代理的基础，以模拟人类翻译生产中的情感、自动化行为和认知的时序动态。

Abstract: The paper introduces a Behavioural Translation Style Space (BTSS) that
describes possible behavioural translation patterns. The suggested BTSS is
organized as a hierarchical structure that entails various embedded processing
layers. We posit that observable translation behaviour - i.e., eye and finger
movements - is fundamental when executing the physical act of translation but
it is caused and shaped by higher-order cognitive processes and affective
translation states. We analyse records of keystrokes and gaze data as
indicators of the hidden mental processing structure and organize the
behavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the
basis for a computational translation agent to simulate the temporal dynamics
of affect, automatized behaviour and cognition during human translation
production.

</details>


### [36] [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)
*Reuben Smit,Retief Louw,Herman Kamper*

Main category: cs.CL

TL;DR: 本文探讨了一种无需自动语音识别的方法，用于在资源匮乏的环境中评估孤立单词的阅读情况。通过比较儿童语音与成人提供的参考模板，发现SSL表示在处理儿童数据时存在局限性。


<details>
  <summary>Details</summary>
Motivation: 在资源匮乏的环境中，需要一种无需自动语音识别的方法来评估孤立单词的阅读情况。

Method: 我们采用了一种无需自动语音识别的方法，将输入的儿童语音与少量成人提供的参考模板进行比较。输入和模板通过大型自监督学习（SSL）模型的中间层进行编码。

Result: 理想化的实验显示，对于成年人表现良好，但对于儿童语音输入，即使使用儿童模板，性能也显著下降。

Conclusion: 我们的工作突显了在少样本分类系统中使用SSL表示处理儿童数据的局限性。

Abstract: We explore an ASR-free method for isolated word reading assessment in
low-resource settings. Our few-shot approach compares input child speech to a
small set of adult-provided reference templates. Inputs and templates are
encoded using intermediate layers from large self-supervised learned (SSL)
models. Using an Afrikaans child speech benchmark, we investigate design
options such as discretising SSL features and barycentre averaging of the
templates. Idealised experiments show reasonable performance for adults, but a
substantial drop for child speech input, even with child templates. Despite the
success of employing SSL representations in low-resource speech tasks, our work
highlights the limitations of SSL representations for processing child data
when used in a few-shot classification system.

</details>


### [37] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
*Shilin Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 本文提出了一种多粒度融合方法，通过结合词级别和短语级别的优势，显著提升了ASR模型在关键词识别上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动语音识别（ASR）模型在识别上下文相关的关键词（如专有名词或用户特定实体）时表现不佳，因此需要一种更有效的解决方案。

Method: 本文提出了一种多粒度融合方法，结合了词级别和短语级别的优势，利用大型语言模型（LLM）来提升关键词识别性能。

Result: 实验表明，该方法在关键词相关指标上达到了最先进的性能，同时保持了非关键词文本的高准确性。消融研究进一步确认了词级别和短语级别组件对性能提升的显著贡献。

Conclusion: 本文提出了一种新颖的多粒度融合方法，结合了词级别和短语级别的优势，显著提升了关键词识别性能，并在中英文数据集上取得了最先进的结果。

Abstract: While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

</details>


### [38] [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
*Yikang Liu,Wanyang Zhang,Yiming Wang,Jialong Tang,Pei Zhang,Baosong Yang,Fei Huang,Rui Wang,Hai Hu*

Main category: cs.CL

TL;DR: The paper introduces T-index, a quantitative measure for translationese, which is computed from the likelihood ratios of two contrastively fine-tuned language models. It shows that T-index is robust, efficient, and correlates well with human judgments, making it a useful complementary metric in machine translation quality estimation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to introduce a quantitative measure for translationese, called T-index, which can be used to assess translationese in a graded and generalizable manner.

Method: The paper proposes the T-index, which is calculated from the likelihood ratios of two contrastively fine-tuned language models. It uses a synthesized dataset and a dataset with translations in the wild to evaluate the generalizability and validity of T-index.

Result: The results show that T-index is robust and efficient. It can well capture translationese in the wild using only 1-5k pairs of synthetic data. The relative differences in T-indices between translations can predict pairwise translationese annotations from human annotators, and the absolute values of T-indices correlate well with human ratings of degrees of translationese (Pearson's $r = 0.568$).

Conclusion: T-index can serve as a complementary metric in MT QE because it is not covered by existing metrics like BLEU and COMET.

Abstract: In this paper, we propose the first quantitative measure for translationese
-- the translationese-index (T-index) for graded and generalizable measurement
of translationese, computed from the likelihood ratios of two contrastively
fine-tuned language models (LMs). We use a synthesized dataset and a dataset
with translations in the wild to evaluate T-index's generalizability in
cross-domain settings and its validity against human judgments. Our results
show that T-index is both robust and efficient. T-index scored by two 0.5B LMs
fine-tuned on only 1-5k pairs of synthetic data can well capture translationese
in the wild. We find that the relative differences in T-indices between
translations can well predict pairwise translationese annotations obtained from
human annotators; and the absolute values of T-indices correlate well with
human ratings of degrees of translationese (Pearson's $r = 0.568$).
Additionally, the correlation between T-index and existing machine translation
(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting
that T-index is not covered by these metrics and can serve as a complementary
metric in MT QE.

</details>


### [39] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
*Johann Frei,Nils Feldhus,Lisa Raithel,Roland Roller,Alexander Meyer,Frank Kramer*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM代理、代码执行和医疗术语数据库工具的端到端框架，用于将自由形式的临床笔记转换为结构化的FHIR资源。


<details>
  <summary>Details</summary>
Motivation: 之前尝试自动化从自由形式的临床笔记到结构化FHIR资源的转换，由于泛化能力有限和结构不一致而面临问题。

Method: 我们提出了一个由LLM代理、代码执行和医疗术语数据库工具支持的端到端框架。

Result: Infherno能够遵循FHIR文档模式，并且在从非结构化文本预测FHIR资源方面表现出色。

Conclusion: 我们的解决方案Infherno能够遵循FHIR文档模式，并且在从非结构化文本预测FHIR资源方面与人类基线竞争良好。

Abstract: For clinical data integration and healthcare services, the HL7 FHIR standard
has established itself as a desirable format for interoperability between
complex health data. Previous attempts at automating the translation from
free-form clinical notes into structured FHIR resources rely on modular,
rule-based systems or LLMs with instruction tuning and constrained decoding.
Since they frequently suffer from limited generalizability and structural
inconformity, we propose an end-to-end framework powered by LLM agents, code
execution, and healthcare terminology database tools to address these issues.
Our solution, called Infherno, is designed to adhere to the FHIR document
schema and competes well with a human baseline in predicting FHIR resources
from unstructured text. The implementation features a front end for custom and
synthetic data and both local and proprietary models, supporting clinical data
integration processes and interoperability across institutions.

</details>


### [40] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Main category: cs.CL

TL;DR: 本文引入了一个文本异常检测基准，展示了嵌入质量的重要性，并发现基于深度学习的方法在使用LLM嵌入时并不优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏标准化和全面的基准来评估现有的文本异常检测方法，这限制了严格比较和创新方法的发展。

Method: 本文进行了全面的实证研究，利用来自不同预训练语言模型的嵌入，并在多个文本数据集上评估了基于嵌入的文本异常检测方法。

Result: 实验结果表明，嵌入质量显著影响异常检测效果，基于深度学习的方法在使用LLM生成的嵌入时并不比传统浅层算法（如KNN、孤立森林）表现更好。此外，跨模型性能矩阵表现出强烈的低秩特性，这使得在实际应用中可以采用高效的模型评估和选择策略。

Conclusion: 本文通过引入一个全面的文本异常检测基准，为未来研究提供了基础，并展示了嵌入质量对异常检测效果的重要性。

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [41] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在VHDL代码生成和摘要任务中的表现，并提出了CoDes方法来提高其性能。实验结果表明，CoDes方法在两个数据集上均优于标准提示策略。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在通用代码相关任务中表现出色，但在硬件描述语言（如VHDL）方面的研究仍较少。本文旨在填补这一空白，并改进LLMs在VHDL领域的适用性。

Method: 本文评估了现有代码LLMs在VHDL代码生成和摘要方面的性能，并提出了CoDes方法，通过生成一系列中间描述步骤来增强LLMs的性能。

Result: 实验结果表明，现有的LLMs在VHDL代码生成和摘要任务中表现不佳。而CoDes方法在两个数据集上均显著优于标准提示策略。

Conclusion: 本文提出了一种名为Chain-of-Descriptions (CoDes)的新方法，以提高大型语言模型在VHDL代码生成和摘要任务中的性能。实验表明，CoDes方法在两个数据集上都显著优于标准提示策略。该方法不仅提高了VHDL代码生成和摘要的质量，还为未来研究提供了框架。

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [42] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
*Liu He,Yuanchao Li,Rui Feng,XinRan Han,Yin-Long Liu,Yuwei Yang,Zude Zhu,Jiahong Yuan*

Main category: cs.CL

TL;DR: 本研究发现男性语音更容易被误认为患有阿尔茨海默病，这表明在开发AD检测模型时需要考虑性别偏见。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示性别偏见在AD语音感知中的影响，特别是在不同语言环境下的表现。

Method: 本研究通过一个涉及16名中国听众评估中英文和希腊语语音的感知实验，分析了性别偏见在AD语音感知中的作用。

Result: 研究发现男性语音更容易被识别为AD，这种偏见在中文语音中尤为明显。声学分析表明，男性语音的shimmer值与AD感知显著相关，而语音部分与AD识别呈显著负相关。

Conclusion: 本研究强调了在开发阿尔茨海默病（AD）检测模型时解决性别偏见的必要性，并呼吁进一步研究以在不同语言环境中验证模型性能。

Abstract: Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

</details>


### [43] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体辩论框架，用于增强大型语言模型在处理用户请求中的歧义问题，结果显示该框架能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理用户请求时面临歧义的问题，因此需要一种方法来增强检测和解决能力。

Method: 本文引入并评估了一个多智能体辩论框架，该框架由三种大型语言模型架构（Llama3-8B、Gemma2-9B 和 Mistral-7B 变体）和一个具有多样歧义的数据集组成。

Result: 辩论框架显著提高了 Llama3-8B 和 Mistral-7B 变体的性能，其中 Mistral-7B 领导的辩论取得了 76.7% 的成功率，并在处理复杂歧义和高效共识方面特别有效。

Conclusion: 本文展示了多智能体辩论框架在增强大型语言模型能力方面的价值，表明结构化辩论可以提高交互系统的清晰度。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and generating human language, contributing to more natural
interactions with complex systems. However, they face challenges such as
ambiguity in user requests processed by LLMs. To address these challenges, this
paper introduces and evaluates a multi-agent debate framework designed to
enhance detection and resolution capabilities beyond single models. The
framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and
Mistral-7B variants) and a dataset with diverse ambiguities. The debate
framework markedly enhanced the performance of Llama3-8B and Mistral-7B
variants over their individual baselines, with Mistral-7B-led debates achieving
a notable 76.7% success rate and proving particularly effective for complex
ambiguities and efficient consensus. While acknowledging varying model
responses to collaborative strategies, these findings underscore the debate
framework's value as a targeted method for augmenting LLM capabilities. This
work offers important insights for developing more robust and adaptive language
understanding systems by showing how structured debates can lead to improved
clarity in interactive systems.

</details>


### [44] [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei,Mohsen Mosleh*

Main category: cs.CL

TL;DR: 本文研究了LLMs是否能通过用户名推断社交媒体用户的人口统计属性，并发现它们可以做到这一点，但也存在潜在的偏见和滥用风险。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究已经证明LLMs能够访问和分析网站，但它们直接检索和分析社交媒体数据的能力仍未被探索。本文旨在评估LLMs是否能够通过用户名推断社交媒体用户的人口统计属性。

Method: 本文使用了一个包含48个Twitter账户的合成数据集和一个包含1,384名国际参与者的调查数据集，评估了LLMs能否仅通过用户名推断社交媒体用户的人口统计属性。

Result: 结果显示，这些模型可以访问社交媒体内容并以合理的准确性预测用户人口统计信息。对合成数据集的分析进一步揭示了LLMs如何解析和解释社交媒体资料，这可能会对活动较少的账户产生性别和政治偏见。

Conclusion: 本文提出，虽然LLMs在计算社会科学研究中具有潜力，但需要采取防护措施以防止滥用。建议LLM提供商在面向公众的应用中限制此功能，同时为经过验证的研究目的保留受控访问。

Abstract: Large language models (LLMs) have traditionally relied on static training
data, limiting their knowledge to fixed snapshots. Recent advancements,
however, have equipped LLMs with web browsing capabilities, enabling real time
information retrieval and multi step reasoning over live web content. While
prior studies have demonstrated LLMs ability to access and analyze websites,
their capacity to directly retrieve and analyze social media data remains
unexplored. Here, we evaluate whether web browsing LLMs can infer demographic
attributes of social media users given only their usernames. Using a synthetic
dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international
participants, we show that these models can access social media content and
predict user demographics with reasonable accuracy. Analysis of the synthetic
dataset further reveals how LLMs parse and interpret social media profiles,
which may introduce gender and political biases against accounts with minimal
activity. While this capability holds promise for computational social science
in the post API era, it also raises risks of misuse particularly in information
operations and targeted advertising underscoring the need for safeguards. We
recommend that LLM providers restrict this capability in public facing
applications, while preserving controlled access for verified research
purposes.

</details>


### [45] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
*Yucheng Sun,Alessandro Stolfo,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本研究探讨了如何利用语言模型的内部激活来检测算术错误，并展示了简单的探测器可以实现高精度的错误检测和自我纠正。


<details>
  <summary>Details</summary>
Motivation: 我们想了解语言模型的内部激活是否可以用来检测算术错误，并探索如何利用这些激活进行模型自我纠正。

Method: 我们首先在3位数加法的受控环境中测试了内部激活能否用于检测算术错误，然后训练了轻量级的错误检测器，并扩展到结构化的思维链追踪。

Result: 我们发现简单的探测器可以准确地从隐藏状态中解码模型的预测输出和正确答案，无论模型的输出是否正确。我们还训练了超过90%准确率的轻量级错误检测器，并展示了这些探测器可以引导选择性重新提示错误的推理步骤，从而提高任务准确性。

Conclusion: 我们的研究结果表明，仅从内部激活就可以预测算术错误，并且简单的探测器为轻量级模型自我纠正提供了一条可行的路径。

Abstract: We investigate whether internal activations in language models can be used to
detect arithmetic errors. Starting with a controlled setting of 3-digit
addition, we show that simple probes can accurately decode both the model's
predicted output and the correct answer from hidden states, regardless of
whether the model's output is correct. Building on this, we train lightweight
error detectors that predict model correctness with over 90% accuracy. We then
extend our analysis to structured chain-of-thought traces on addition-only
GSM8K problems and find that probes trained on simple arithmetic generalize
well to this more complex setting, revealing consistent internal
representations. Finally, we demonstrate that these probes can guide selective
re-prompting of erroneous reasoning steps, improving task accuracy with minimal
disruption to correct outputs. Our findings suggest that arithmetic errors can
be anticipated from internal activations alone, and that simple probes offer a
viable path toward lightweight model self-correction.

</details>


### [46] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
*Chandana Cheerla*

Main category: cs.CL

TL;DR: 该研究提出了一种先进的RAG框架，结合了混合检索策略、语义分块和量化索引，以提高企业数据处理的效果。实验结果显示了在精度、召回率和相关性方面的显著提升。


<details>
  <summary>Details</summary>
Motivation: 组织越来越多地依赖专有企业数据进行关键决策。虽然大型语言模型（LLMs）具有强大的生成能力，但它们受到静态预训练、短上下文窗口和处理异构数据格式的挑战的限制。传统的检索增强生成（RAG）框架解决了一些这些差距，但通常在结构化和半结构化数据上表现不佳。

Method: 该工作提出了一个先进的RAG框架，结合了使用密集嵌入（all-mpnet-base-v2）和BM25的混合检索策略，并通过SpaCy NER和交叉编码器重新排序进行增强。该框架应用语义分块以保持文本连贯性，并保留表格数据结构以保持行-列完整性。量化索引优化了检索效率，同时人机反馈和对话记忆提高了适应性。

Result: 在企业数据集上的实验显示显著改进：Precision@5增加了15%（90对75），Recall@5增加了13%（87对74），平均倒数排名增加了16%（0.85对0.69）。定性评估显示，在一个5点李克特量表上，忠实度（4.6对3.0）、完整度（4.2对2.5）和相关性（4.5对3.2）得分更高。

Conclusion: 这些结果表明该框架在为企业任务提供准确、全面和上下文相关的响应方面是有效的。未来的工作包括扩展到多模态数据和集成基于代理的检索。

Abstract: Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [47] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CL

TL;DR: 研究发现基于CoT激活的线性探测器能有效预测语言模型输出的安全性，且在生成过程中可实现实时监控。


<details>
  <summary>Details</summary>
Motivation: 开放权重推理语言模型在生成最终响应前生成长链式思维（CoTs），这提高了性能但引入了额外的对齐风险，有害内容通常出现在CoTs和最终输出中。因此，研究旨在利用CoTs预测最终响应的不对齐情况。

Method: 研究评估了多种监控方法，包括人类、高能力大型语言模型和文本分类器，使用CoT文本或激活进行评估。

Result: 基于CoT激活的简单线性探测器显著优于所有基于文本的方法。CoT文本可能不可靠并误导人类和分类器，而模型潜变量（即CoT激活）提供了更可靠的预测信号。探测器在推理完成前就能准确预测，即使应用于早期CoT段落也能取得强大性能。

Conclusion: 研究发现，基于CoT激活的简单线性探测器在预测最终响应是否安全或不安全方面显著优于所有基于文本的方法。这些发现在不同模型大小、家族和安全基准中具有普遍性，表明轻量级探测器可以实现生成过程中的实时安全监控和早期干预。

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [48] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 本文提出了一种新的主题建模方法S2WTM，通过使用球面切片Wasserstein距离来解决后验崩溃问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: VAE-NTMs经常遭受后验崩溃，这导致KL散度项在目标函数中大幅减少，从而产生无效的潜在表示。

Method: S2WTM采用单位超球面上的先验分布，并利用球面切片Wasserstein距离将聚合后验分布与先验对齐。

Result: 实验结果表明，S2WTM优于最先进的主题模型。

Conclusion: S2WTM在生成更连贯和多样的主题的同时，提高了下游任务的性能。

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [49] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Main category: cs.CL

TL;DR: 本文提出了一种名为BETR的方法，该方法通过将预训练数据与评估基准对齐来提高模型性能。实验表明，BETR在多个任务上表现优异，并且随着模型规模的增大，需要的过滤程度降低。


<details>
  <summary>Details</summary>
Motivation: 在实践中，这些目标通常通过以基准为导向的迭代隐式出现：研究人员开发选择策略，训练模型，测量基准性能，然后相应地进行改进。这引发了一个自然的问题：当我们显式优化时会发生什么？

Method: 我们提出了基准目标排序（BETR），这是一种简单的方法，根据与基准训练示例的相似性选择预训练文档。BETR将基准示例和预训练文档的样本嵌入到一个共享空间中，通过与基准的相似性对这个样本进行评分，然后训练一个轻量级分类器来预测整个语料库的这些分数。

Result: 通过训练超过500个模型，覆盖10^19到10^22 FLOPs，并拟合缩放定律，我们比较了数据选择方法。结果显示，使用BETR将预训练数据与评估基准对齐可以实现4.7倍于未过滤数据的计算乘数，并在所有规模的10个任务中的9个上提高了性能。BETR还具有良好的泛化能力：当针对与我们的评估套件不相关的多样化基准时，它仍然可以匹配或超越基线。

Conclusion: 我们的研究结果表明，直接将预训练数据与目标任务匹配可以精确地塑造模型能力，并强调了最佳选择策略必须适应模型规模。

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [50] [RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection](https://arxiv.org/abs/2507.12175)
*Sungkyun Chang,Simon Dixon,Emmanouil Benetos*

Main category: cs.SD

TL;DR: RUMAA是一种基于Transformer的框架，用于音乐表演分析，能够统一处理乐谱到表演对齐、基于乐谱的转录和错误检测，并在具有重复的乐谱上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于手动展开的乐谱-MIDI数据，无法处理复杂的重复结构，而RUMAA旨在解决这一问题。

Method: RUMAA使用预训练的乐谱和音频编码器以及一种新的三流解码器，通过代理任务捕捉任务之间的相互依赖关系，以统一的方式处理乐谱到表演对齐、基于乐谱的转录和错误检测。

Result: RUMAA在非重复乐谱上达到了最先进的对齐方法水平，并在包含重复的公共钢琴音乐数据集上表现更优，同时在转录和错误检测方面也取得了有希望的结果。

Conclusion: RUMAA在具有重复的乐谱上优于现有的对齐方法，并且在转录和错误检测方面也表现出色。

Abstract: This study introduces RUMAA, a transformer-based framework for music
performance analysis that unifies score-to-performance alignment,
score-informed transcription, and mistake detection in a near end-to-end
manner. Unlike prior methods addressing these tasks separately, RUMAA
integrates them using pre-trained score and audio encoders and a novel
tri-stream decoder capturing task interdependencies through proxy tasks. It
aligns human-readable MusicXML scores with repeat symbols to full-length
performance audio, overcoming traditional MIDI-based methods that rely on
manually unfolded score-MIDI data with pre-specified repeat structures. RUMAA
matches state-of-the-art alignment methods on non-repeated scores and
outperforms them on scores with repeats in a public piano music dataset, while
also delivering promising transcription and mistake detection results.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [51] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint is a new framework for code quality analysis that improves generalization to unseen code patterns by using instruction tuning on synthetic data, showing promising results in detecting and fixing problematic code fragments.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with code quality analysis due to static training data and inability to adapt to evolving best practices. There is a need for a framework that can adapt to novel or complex code patterns without retraining.

Method: MetaLint is an instruction-following framework that formulates code quality analysis as detecting and fixing problematic semantic code fragments based on high-level specifications. It uses instruction tuning on synthetic linter-generated data for easy-to-hard generalization.

Result: MetaLint improves generalization to unseen PEP idioms, achieving a 70.37% F-score on idiom detection with the highest recall (70.43%) among all evaluated models. It also achieves 26.73% on localization, competitive for its 4B parameter size and comparable to larger state-of-the-art models.

Conclusion: MetaLint shows potential for future-proof code quality analysis by improving generalization to unseen PEP idioms and achieving competitive results with smaller models.

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [52] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: MERA Code is a new benchmark for evaluating code generation LLMs in Russian, focusing on practical coding skills and addressing gaps in current evaluations.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs primarily focus on natural language tasks, overlooking code quality. Most benchmarks prioritize high-level reasoning over executable code and real-world performance, leaving gaps in understanding true capabilities and risks associated with these models in production.

Method: MERA Code is a new addition to the MERA benchmark family, specifically focused on evaluating code for the latest code generation LLMs in Russian. It includes 11 evaluation tasks that span 8 programming languages and features a taxonomy outlining practical coding skills necessary for models to complete these tasks.

Result: MERA Code evaluates open LLMs and frontier API models, analyzing their limitations in terms of practical coding tasks in non-English languages.

Conclusion: MERA Code is publicly released to guide future research, anticipate groundbreaking features in model development, and standardize evaluation procedures.

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker](https://arxiv.org/abs/2507.12378)
*Rachna Saxena,Abhijeet Kumar,Suresh Shanmugam*

Main category: cs.IR

TL;DR: 本文探讨了一种实用的方法，使视觉检索过程可扩展且高效，而不会影响性能质量。


<details>
  <summary>Details</summary>
Motivation: 传统信息抽取系统面临挑战，因为它们不考虑图表等视觉元素。多模态LLM面临寻找针在干草堆的问题，即较长的上下文长度或大量的文档作为搜索空间。晚期交互机制在基于检索的视觉增强问答任务中表现出色，但将其用于RAG-based多模态问答仍存在挑战。

Method: 本文提出了一种多步骤的自定义实现，利用广泛采用的混合搜索（元数据和嵌入）和最先进的晚期交互重新排序器来检索最佳匹配页面。最后，MLLM被提示为读者，从上下文化的最佳匹配页面生成答案。

Result: 通过实验，我们观察到所提出的方案是可扩展的（显著的速度提升）且稳定的（不会降低性能质量）。

Conclusion: 本文提出的方案在保持性能质量的同时，具有可扩展性和稳定性，因此可以用于企业生产系统。

Abstract: Traditional information extraction systems face challenges with text only
language models as it does not consider infographics (visual elements of
information) such as tables, charts, images etc. often used to convey complex
information to readers. Multimodal LLM (MLLM) face challenges of finding needle
in the haystack problem i.e., either longer context length or substantial
number of documents as search space. Late interaction mechanism over visual
language models has shown state of the art performance in retrieval-based
vision augmented Q&A tasks. There are yet few challenges using it for RAG based
multi-modal Q&A. Firstly, many popular and widely adopted vector databases do
not support native multi-vector retrieval. Secondly, late interaction requires
computation which inflates space footprint and can hinder enterprise adoption.
Lastly, the current state of late interaction mechanism does not leverage the
approximate neighbor search indexing methods for large speed ups in retrieval
process. This paper explores a pragmatic approach to make vision retrieval
process scalable and efficient without compromising on performance quality. We
propose multi-step custom implementation utilizing widely adopted hybrid search
(metadata & embedding) and state of the art late interaction re-ranker to
retrieve best matching pages. Finally, MLLM are prompted as reader to generate
answers from contextualized best matching pages. Through experiments, we
observe that the proposed design is scalable (significant speed up) and stable
(without degrading performance quality), hence can be used as production
systems at enterprises.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
*Brendan Murphy,Dillon Bowen,Shahrad Mohammadzadeh,Julius Broomfield,Adam Gleave,Kellin Pelrine*

Main category: cs.CR

TL;DR: 论文揭示了当前AI系统在安全防护方面的漏洞，并提出了一种新的微调方法，使模型能够生成有害内容。论文强调需要开发更强大的防护措施。


<details>
  <summary>Details</summary>
Motivation: 论文旨在揭示当前AI系统在安全防护方面的不足，并强调需要开发更强大的防护措施。

Method: 论文提出了一种名为jailbreak-tuning的方法，该方法通过微调模型，使其能够生成详细、高质量的响应，以应对任意有害请求。此外，论文还展示了后门可以增加攻击的隐蔽性和严重性。

Result: 论文展示了通过微调模型可以生成有害内容，例如OpenAI、Google和Anthropic模型会完全遵守请求，提供CBRN援助、执行网络攻击等犯罪活动。此外，论文还发现，更强的jailbreak提示在微调攻击中更加有效，这表明攻击和防御可能在输入和权重空间中相互关联。

Conclusion: 论文指出，目前的AI系统在安全防护方面存在严重漏洞，需要开发更强大的防护措施。在这些防护措施被发现之前，公司和政策制定者应将任何可微调模型的发布视为同时发布了其邪恶双胞胎：同样有能力，且可用于任何恶意目的。

Abstract: AI systems are rapidly advancing in capability, and frontier model developers
broadly acknowledge the need for safeguards against serious misuse. However,
this paper demonstrates that fine-tuning, whether via open weights or closed
fine-tuning APIs, can produce helpful-only models. In contrast to prior work
which is blocked by modern moderation systems or achieved only partial removal
of safeguards or degraded output quality, our jailbreak-tuning method teaches
models to generate detailed, high-quality responses to arbitrary harmful
requests. For example, OpenAI, Google, and Anthropic models will fully comply
with requests for CBRN assistance, executing cyberattacks, and other criminal
activity. We further show that backdoors can increase not only the stealth but
also the severity of attacks, while stronger jailbreak prompts become even more
effective in fine-tuning attacks, linking attack and potentially defenses in
the input and weight spaces. Not only are these models vulnerable, more recent
ones also appear to be becoming even more vulnerable to these attacks,
underscoring the urgent need for tamper-resistant safeguards. Until such
safeguards are discovered, companies and policymakers should view the release
of any fine-tunable model as simultaneously releasing its evil twin: equally
capable as the original model, and usable for any malicious purpose within its
capabilities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，RiemannLoRA，在统一框架中同时解决了低秩适应中的初始化策略和过参数化问题，并在实验中表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 低秩适应（LoRA）已成为高效微调大型语言模型（LLM）的广泛标准，但仍然存在挑战，包括寻找最佳初始化策略或减轻低秩矩阵分解中的过参数化问题。

Method: 我们将一组固定秩的LoRA矩阵视为一个平滑流形，并将适配器视为该流形上的元素，从而消除了过参数化问题。同时，沿着流形确定最快损失减少的方向提供了初始化策略。

Result: 实验结果表明，RiemannLoRA在收敛速度和最终性能方面均优于标准LoRA及其最先进的修改。

Conclusion: RiemannLoRA在LLM和扩散模型架构上的实验结果表明，它在收敛速度和最终性能方面都优于标准LoRA及其最先进的修改。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [56] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Main category: cs.LG

TL;DR: 本文提出了一种名为L̄EOPARD的方法，通过概念擦除来去除神经模型中的敏感信息，以提高公平性。


<details>
  <summary>Details</summary>
Motivation: 确保神经模型在现实应用中不能从文本表示中推断出敏感信息（如性别或种族等人口统计属性）是当公平性是一个关注点时的一个关键挑战。

Method: 通过概念擦除，学习嵌入空间中的正交投影，使离散概念的类别条件特征分布在投影后不可区分。调整投影器的秩控制信息删除的程度，同时其正交性确保嵌入的局部结构严格保留。

Result: 我们的方法称为L̄EOPARD，在经典自然语言处理基准上实现了非线性擦除离散属性的最先进性能。此外，我们证明L̄EOPARD有效减轻了深度非线性分类器中的偏差，从而促进了公平性。

Conclusion: 我们的方法在非线性擦除离散属性方面取得了最先进的性能，并有效减轻了深度非线性分类器中的偏差，从而促进了公平性。

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [57] [Simulated Language Acquisition in a Biologically Realistic Model of the Brain](https://arxiv.org/abs/2507.11788)
*Daniel Mitropolsky,Christos Papadimitriou*

Main category: cs.NE

TL;DR: 本文提出了一种基于神经科学原则的数学模型，并展示了其在语言习得中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管神经科学取得了巨大进展，但我们仍然缺乏一个明确的叙述来解释神经元的放电如何导致高级认知现象，如计划和语言。

Method: 本文引入了一个简单的数学公式，描述了六个基本且广泛接受的神经科学原则，并实现了一个模拟的神经形态系统。

Result: 该系统能够从零开始学习任何语言的词汇语义、语法角色（动词与名词）以及词序，包括生成新句子的能力。

Conclusion: 本文提出了一个基于六个基本神经科学原则的数学公式，并展示了其在语言习得方面的应用，同时讨论了可能的扩展和影响。

Abstract: Despite tremendous progress in neuroscience, we do not have a compelling
narrative for the precise way whereby the spiking of neurons in our brain
results in high-level cognitive phenomena such as planning and language. We
introduce a simple mathematical formulation of six basic and broadly accepted
principles of neuroscience: excitatory neurons, brain areas, random synapses,
Hebbian plasticity, local inhibition, and inter-area inhibition. We implement a
simulated neuromorphic system based on this formalism, which is capable of
basic language acquisition: Starting from a tabula rasa, the system learns, in
any language, the semantics of words, their syntactic role (verb versus noun),
and the word order of the language, including the ability to generate novel
sentences, through the exposure to a modest number of grounded sentences in the
same language. We discuss several possible extensions and implications of this
result.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [58] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Main category: cs.AI

TL;DR: This paper explores the use of Multimodal Large Language Models (MLLMs) as verifiers for agent behavior in domains without clear success criteria, identifies a critical limitation called agreement bias, and proposes Self-Grounded Verification (SGV) to improve their performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of extending AI progress to domains without clear-cut success criteria, such as computer use, remains difficult due to the difficulty of translating human intuition into scalable rules. MLLMs show promise as verifiers but face limitations like agreement bias.

Method: The paper evaluates MLLMs as verifiers of agent trajectories across web navigation, computer use, and robotic manipulation, identifies agreement bias as a critical limitation, and proposes Self-Grounded Verification (SGV) to address it.

Result: Enhanced with SGV, MLLM verifiers show gains of up to 20 points in accuracy and failure detection rates, and can perform real-time supervision of heterogeneous agents, boosting task completion in various environments and setting a new state of the art on benchmarks.

Conclusion: MLLM verifiers can be enhanced with Self-Grounded Verification (SGV), which improves their accuracy and failure detection rates, enabling real-time supervision of heterogeneous agents and setting a new state of the art on benchmarks.

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [59] [Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening](https://arxiv.org/abs/2507.11548)
*Kevin T Webster*

Main category: cs.CY

TL;DR: 本文研究了生成式AI在简历筛选中的能力问题，发现其可能存在种族和性别偏见以及能力不足的问题，并提出了双重验证框架以确保AI招聘工具的公平性和有效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨生成式AI在简历筛选中的能力问题，质疑其是否能够胜任评估任务，并揭示其可能存在的'中立性幻觉'现象。

Method: 本文通过两个部分的审计检查了八个主要AI平台，实验1确认了复杂的上下文种族和性别偏见，实验2评估了核心能力，发现一些看似无偏的模型实际上无法进行实质性评估，而是依赖于表面的关键词匹配。

Result: 实验1发现了复杂的上下文种族和性别偏见，实验2发现一些模型虽然看似无偏，但实际上无法进行实质性评估，而是依赖于表面的关键词匹配。

Conclusion: 本文建议组织和监管机构采用双重验证框架，对AI招聘工具进行人口统计偏差和可证明能力的审计，以确保它们既公平又有效。

Abstract: The increasing use of generative AI for resume screening is predicated on the
assumption that it offers an unbiased alternative to biased human
decision-making. However, this belief fails to address a critical question: are
these AI systems fundamentally competent at the evaluative tasks they are meant
to perform? This study investigates the question of competence through a
two-part audit of eight major AI platforms. Experiment 1 confirmed complex,
contextual racial and gender biases, with some models penalizing candidates
merely for the presence of demographic signals. Experiment 2, which evaluated
core competence, provided a critical insight: some models that appeared
unbiased were, in fact, incapable of performing a substantive evaluation,
relying instead on superficial keyword matching. This paper introduces the
"Illusion of Neutrality" to describe this phenomenon, where an apparent lack of
bias is merely a symptom of a model's inability to make meaningful judgments.
This study recommends that organizations and regulators adopt a dual-validation
framework, auditing AI hiring tools for both demographic bias and demonstrable
competence to ensure they are both equitable and effective.

</details>

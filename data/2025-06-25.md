<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection](https://arxiv.org/abs/2506.18919)
*Hexiang Gu,Qifan Yu,Saihui Hou,Zhiqin Fang,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: This paper introduces MemeMind, a comprehensive dataset for harmful meme detection, and MemeGuard, an innovative framework that improves detection accuracy by integrating multimodal information with reasoning process modeling.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks a systematic, large-scale, diverse, and highly explainable dataset for harmful meme detection, which hinders further advancement in this field.

Method: Introduce MemeMind, a novel dataset with scientifically rigorous standards, large scale, diversity, bilingual support, and detailed Chain-of-Thought (CoT) annotations. Propose MemeGuard, an innovative detection framework that integrates multimodal information with reasoning process modeling.

Result: MemeGuard consistently outperforms existing state-of-the-art methods in harmful meme detection tasks on the MemeMind dataset.

Conclusion: MemeMind dataset and MemeGuard framework provide a solid foundation for enhancing harmful meme detection, with MemeGuard consistently outperforming existing state-of-the-art methods.

Abstract: The rapid development of social media has intensified the spread of harmful
content. Harmful memes, which integrate both images and text, pose significant
challenges for automated detection due to their implicit semantics and complex
multimodal interactions. Although existing research has made progress in
detection accuracy and interpretability, the lack of a systematic, large-scale,
diverse, and highly explainable dataset continues to hinder further advancement
in this field. To address this gap, we introduce MemeMind, a novel dataset
featuring scientifically rigorous standards, large scale, diversity, bilingual
support (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.
MemeMind fills critical gaps in current datasets by offering comprehensive
labeling and explicit reasoning traces, thereby providing a solid foundation
for enhancing harmful meme detection. In addition, we propose an innovative
detection framework, MemeGuard, which effectively integrates multimodal
information with reasoning process modeling, significantly improving models'
ability to understand and identify harmful memes. Extensive experiments
conducted on the MemeMind dataset demonstrate that MemeGuard consistently
outperforms existing state-of-the-art methods in harmful meme detection tasks.

</details>


### [2] [Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge](https://arxiv.org/abs/2506.18998)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: 我们的研究揭示了大型语言模型（LLMs）在推理模式学习与记忆之间的混淆问题，这影响了它们的可信度。此外，我们强调了改进模型自我知识感知的重要性，以提高AI的可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的研究将记忆和自我知识缺陷视为独立的问题，但没有认识到它们之间的相互联系，这降低了LLMs响应的可信度。我们的研究旨在探索这种联系，并提出一种方法来评估LLMs的自我知识感知。

Method: 我们使用了一个新颖的框架来确定LLMs是否真正从训练数据中学习推理模式，或者只是记忆它们以假扮能力。

Result: 我们的分析显示了一个显著的泛化问题：LLMs从记忆的解决方案中获得信心，从而对其推理能力产生更高的自我知识，这在面对自验证、逻辑一致的任务扰动时导致了超过45%的不一致性。这种效应在科学和医学领域最为明显，这些领域通常有最多的标准化术语和问题。

Conclusion: 我们的研究揭示了大型语言模型（LLMs）在推理模式学习与记忆之间的混淆问题，这影响了它们的可信度。此外，我们强调了改进模型自我知识感知的重要性，以提高AI的可解释性和可信度。

Abstract: When artificial intelligence mistakes memorization for intelligence, it
creates a dangerous mirage of reasoning. Existing studies treat memorization
and self-knowledge deficits in LLMs as separate issues and do not recognize an
intertwining link that degrades the trustworthiness of LLM responses. In our
study, we utilize a novel framework to ascertain if LLMs genuinely learn
reasoning patterns from training data or merely memorize them to assume
competence across problems of similar complexity focused on STEM domains. Our
analysis shows a noteworthy problem in generalization: LLMs draw confidence
from memorized solutions to infer a higher self-knowledge about their reasoning
ability, which manifests as an over 45% inconsistency in feasibility
assessments when faced with self-validated, logically coherent task
perturbations. This effect is most pronounced in science and medicine domains,
which tend to have maximal standardized jargon and problems, further confirming
our approach. Significant wavering within the self-knowledge of LLMs also shows
flaws in current architectures and training patterns, highlighting the need for
techniques that ensure a balanced, consistent stance on models' perceptions of
their own knowledge for maximum AI explainability and trustworthiness. Our code
and results are available publicly at
https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.

</details>


### [3] [Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations](https://arxiv.org/abs/2506.19004)
*Brian Siyuan Zheng,Alisa Liu,Orevaoghene Ahia,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: 我们的研究显示，LM对非规范分词具有较高的鲁棒性，并且在某些情况下可以通过干预分词来提高性能。


<details>
  <summary>Details</summary>
Motivation: 我们想了解LM对非规范分词的鲁棒性，并寻找可能提高性能的设置。

Method: 我们研究了LM对完全未在训练中见过的非规范分词的鲁棒性，并探索了非规范分词方案可以提高性能的设置。

Result: 我们发现指令调优模型在给定随机采样的分词时保留了高达93.4%的原始性能，而字符级分词则保留了90.8%。此外，字符级分割提高了字符串操作和代码理解任务的性能，右对齐数字分组提高了大数算术的性能。

Conclusion: 我们的研究结果表明，模型比之前认为的更不依赖于分词器，并展示了在推理时干预分词以提高性能的潜力。

Abstract: Modern tokenizers employ deterministic algorithms to map text into a single
"canonical" token sequence, yet the same string can be encoded as many
non-canonical tokenizations using the tokenizer vocabulary. In this work, we
investigate the robustness of LMs to text encoded with non-canonical
tokenizations entirely unseen during training. Surprisingly, when evaluated
across 20 benchmarks, we find that instruction-tuned models retain up to 93.4%
of their original performance when given a randomly sampled tokenization, and
90.8% with character-level tokenization. We see that overall stronger models
tend to be more robust, and robustness diminishes as the tokenization departs
farther from the canonical form. Motivated by these results, we then identify
settings where non-canonical tokenization schemes can *improve* performance,
finding that character-level segmentation improves string manipulation and code
understanding tasks by up to +14%, and right-aligned digit grouping enhances
large-number arithmetic by +33%. Finally, we investigate the source of this
robustness, finding that it arises in the instruction-tuning phase. We show
that while both base and post-trained models grasp the semantics of
non-canonical tokenizations (perceiving them as containing misspellings), base
models try to mimic the imagined mistakes and degenerate into nonsensical
output, while post-trained models are committed to fluent responses. Overall,
our findings suggest that models are less tied to their tokenizer than
previously believed, and demonstrate the promise of intervening on tokenization
at inference time to boost performance.

</details>


### [4] [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/abs/2506.19028)
*Weijie Xu,Yiwen Wang,Chi Xue,Xiangkun Hu,Xi Fang,Guimin Dong,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 本文提出了 FiSCo，一种新的统计框架，用于评估 LLM 的群体公平性，通过检测长格式响应中跨人口群体的细微语义差异。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法常常忽视长格式响应中的偏见和 LLM 输出的内在可变性。

Method: FiSCo（细粒度语义计算）是一种新的统计框架，通过检测长格式响应中跨人口群体的细微语义差异来评估 LLM 的群体公平性。它在声明级别上操作，利用蕴含检查来评估响应之间的一致性。

Result: 实验表明，FiSCo 更可靠地识别细微偏见，同时减少随机 LLM 变异的影响，并优于各种评估指标。

Conclusion: FiSCo 更可靠地识别细微偏见，同时减少随机 LLM 变异的影响，并优于各种评估指标。

Abstract: Large Language Models (LLMs) often generate responses with inherent biases,
undermining their reliability in real-world applications. Existing evaluation
methods often overlook biases in long-form responses and the intrinsic
variability of LLM outputs. To address these challenges, we propose
FiSCo(Fine-grained Semantic Computation), a novel statistical framework to
evaluate group-level fairness in LLMs by detecting subtle semantic differences
in long-form responses across demographic groups. Unlike prior work focusing on
sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis
by operating at the claim level, leveraging entailment checks to assess the
consistency of meaning across responses. We decompose model outputs into
semantically distinct claims and apply statistical hypothesis testing to
compare inter- and intra-group similarities, enabling robust detection of
subtle biases. We formalize a new group counterfactual fairness definition and
validate FiSCo on both synthetic and human-annotated datasets spanning gender,
race, and age. Experiments show that FiSco more reliably identifies nuanced
biases while reducing the impact of stochastic LLM variability, outperforming
various evaluation metrics.

</details>


### [5] [Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models](https://arxiv.org/abs/2506.19037)
*Omer Luxembourg,Haim Permuter,Eliya Nachmani*

Main category: cs.CL

TL;DR: DUS is a new method for non-autoregressive text generation that improves upon existing techniques by using a first-order Markov assumption to enable efficient, parallel unmasking of tokens.


<details>
  <summary>Details</summary>
Motivation: Existing samplers act as implicit planners, selecting tokens to unmask via denoiser confidence or entropy scores. Such heuristics falter under parallel unmasking - they ignore pairwise interactions between tokens and cannot account for dependencies when unmasking multiple positions at once, limiting their inference time to traditional auto-regressive (AR) models.

Method: DUS leverages a first-order Markov assumption to partition sequence positions into dilation-based groups of non-adjacent tokens, enabling independent, parallel unmasking steps that respect local context that minimizes the joint entropy of each iteration step.

Result: In experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks - domains suited to non-ordinal generation - DUS improves scores over parallel confidence-based planner, without modifying the underlying denoiser.

Conclusion: DUS offers a lightweight, budget-aware approach to efficient, high-quality text generation, paving the way to unlock the true capabilities of MDLMs.

Abstract: Masked diffusion language models (MDLM) have shown strong promise for
non-autoregressive text generation, yet existing samplers act as implicit
planners, selecting tokens to unmask via denoiser confidence or entropy scores.
Such heuristics falter under parallel unmasking - they ignore pairwise
interactions between tokens and cannot account for dependencies when unmasking
multiple positions at once, limiting their inference time to traditional
auto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking
Strategy (DUS), an inference-only, planner-model-free method that requires no
additional training. DUS leverages a first-order Markov assumption to partition
sequence positions into dilation-based groups of non-adjacent tokens, enabling
independent, parallel unmasking steps that respect local context that minimizes
the joint entropy of each iteration step. Unlike semi-AR block approaches
(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces
the number of denoiser calls to O(log B) per generation block - yielding
substantial speedup over the O(B) run time of state-of-the-art diffusion
models, where B is the block size in the semi-AR inference process. In
experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -
domains suited to non-ordinal generation - DUS improves scores over parallel
confidence-based planner, without modifying the underlying denoiser. DUS offers
a lightweight, budget-aware approach to efficient, high-quality text
generation, paving the way to unlock the true capabilities of MDLMs.

</details>


### [6] [NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching](https://arxiv.org/abs/2506.19058)
*Mike Zhang,Rob van der Goot*

Main category: cs.CL

TL;DR: 本文介绍了NLPnorth在TalentCLEF 2025中的提交，包括两项任务：多语言职位名称匹配和基于职位名称的技能预测。对于这两个任务，我们比较了（微调的）基于分类的方法、（微调的）对比方法和提示方法。在任务A中，我们的提示方法表现最好，在测试数据上的平均平均精度（MAP）为0.492，涵盖了英语、西班牙语和德语。在任务B中，我们使用微调的基于分类的方法获得了0.290的MAP。此外，我们还通过从ESCO中提取每种语言特定的职位名称和相应的描述来利用额外的数据。总体而言，最大的多语言语言模型在两个任务中表现最佳。根据初步结果，任务A的排名为第5名/20名，任务B的排名为第3名/14名。


<details>
  <summary>Details</summary>
Motivation: 匹配职位名称是计算就业市场领域的一个高度相关任务，因为它可以提高自动候选人匹配、职业路径预测和就业市场分析。此外，将职位名称与职位技能对齐可以被视为该任务的扩展，对于相同的下游任务同样具有相关性。

Method: 对于这两个任务，我们比较了（微调的）基于分类的方法、（微调的）对比方法和提示方法。在任务A中，我们的提示方法表现最好，在测试数据上的平均平均精度（MAP）为0.492，涵盖了英语、西班牙语和德语。在任务B中，我们使用微调的基于分类的方法获得了0.290的MAP。此外，我们还通过从ESCO中提取每种语言特定的职位名称和相应的描述来利用额外的数据。

Result: 对于任务A，我们的提示方法在测试数据上的平均平均精度（MAP）为0.492，涵盖了英语、西班牙语和德语。对于任务B，我们使用微调的基于分类的方法获得了0.290的MAP。根据初步结果，任务A的排名为第5名/20名，任务B的排名为第3名/14名。

Conclusion: 总体而言，最大的多语言语言模型在两个任务中表现最佳。根据初步结果，任务A的排名为第5名/20名，任务B的排名为第3名/14名。

Abstract: Matching job titles is a highly relevant task in the computational job market
domain, as it improves e.g., automatic candidate matching, career path
prediction, and job market analysis. Furthermore, aligning job titles to job
skills can be considered an extension to this task, with similar relevance for
the same downstream tasks. In this report, we outline NLPnorth's submission to
TalentCLEF 2025, which includes both of these tasks: Multilingual Job Title
Matching, and Job Title-Based Skill Prediction. For both tasks we compare
(fine-tuned) classification-based, (fine-tuned) contrastive-based, and
prompting methods. We observe that for Task A, our prompting approach performs
best with an average of 0.492 mean average precision (MAP) on test data,
averaged over English, Spanish, and German. For Task B, we obtain an MAP of
0.290 on test data with our fine-tuned classification-based approach.
Additionally, we made use of extra data by pulling all the language-specific
titles and corresponding \emph{descriptions} from ESCO for each job and skill.
Overall, we find that the largest multilingual language models perform best for
both tasks. Per the provisional results and only counting the unique teams, the
ranking on Task A is 5$^{\text{th}}$/20 and for Task B 3$^{\text{rd}}$/14.

</details>


### [7] [MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation](https://arxiv.org/abs/2506.19073)
*Jackson Trager,Francielle Vargas,Diego Alves,Matteo Guida,Mikel K. Ngueajio,Ameeta Agrawal,Flor Plaza-del-Arco,Yalda Daryanai,Farzan Karimi-Malekabadi*

Main category: cs.CL

TL;DR: 本文介绍了MFTCXplain，一个用于评估LLMs道德推理能力的多语言基准数据集。实证结果表明，LLMs在道德推理任务中的表现不如人类，特别是在非英语语言中。


<details>
  <summary>Details</summary>
Motivation: 当前评估基准存在两个主要缺点：缺乏解释道德分类的注释，这限制了透明度和可解释性；以及主要关注英语，这限制了在不同文化背景下的道德推理评估。

Method: 引入MFTCXplain，这是一个多语言基准数据集，用于通过仇恨言论多跳解释评估LLMs的道德推理能力。

Result: 实证结果表明，LLMs输出与人类注释在道德推理任务中存在不一致。虽然LLMs在仇恨言论检测方面表现良好（F1高达0.836），但它们预测道德情感的能力明显较弱（F1 < 0.35）。此外，理由对齐在代表性不足的语言中仍然有限。

Conclusion: 这些发现表明当前LLMs在内化和反映人类道德推理方面的能力有限。

Abstract: Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is
a growing concern as these systems are used in socially sensitive tasks.
Nevertheless, current evaluation benchmarks present two major shortcomings: a
lack of annotations that justify moral classifications, which limits
transparency and interpretability; and a predominant focus on English, which
constrains the assessment of moral reasoning across diverse cultural settings.
In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for
evaluating the moral reasoning of LLMs via hate speech multi-hop explanation
using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across
Portuguese, Italian, Persian, and English, annotated with binary hate speech
labels, moral categories, and text span-level rationales. Empirical results
highlight a misalignment between LLM outputs and human annotations in moral
reasoning tasks. While LLMs perform well in hate speech detection (F1 up to
0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).
Furthermore, rationale alignment remains limited mainly in underrepresented
languages. These findings show the limited capacity of current LLMs to
internalize and reflect human moral reasoning.

</details>


### [8] [Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting](https://arxiv.org/abs/2506.19089)
*Nathaniel Getachew,Abulhair Saparov*

Main category: cs.CL

TL;DR: 我们引入了一个名为StorySim的框架，用于生成故事以评估大型语言模型的心智理论和世界建模能力。实验表明，大多数模型在世界建模任务上表现更好，且在处理人类时表现优于无生命物体。


<details>
  <summary>Details</summary>
Motivation: 与可能在预训练数据中存在污染的先前基准不同，StorySim生成新颖、组合性的故事提示，这些提示由高度可控的Storyboard锚定，从而可以精确地操控角色视角和事件。

Method: 我们引入了StorySim，这是一个可编程框架，用于合成生成故事以评估大型语言模型（LLMs）的心智理论（ToM）和世界建模（WM）能力。

Result: 我们的实验显示，大多数模型在WM任务上的表现优于ToM任务，并且模型在处理人类而非无生命物体时表现更好。

Conclusion: 我们的框架使我们能够发现启发式行为，例如近期偏差和对故事中早期事件的过度依赖。

Abstract: We introduce $\texttt{StorySim}$, a programmable framework for synthetically
generating stories to evaluate the theory of mind (ToM) and world modeling (WM)
capabilities of large language models (LLMs). Unlike prior benchmarks that may
suffer from contamination in pretraining data, $\texttt{StorySim}$ produces
novel, compositional story prompts anchored by a highly controllable
$\texttt{Storyboard}$, enabling precise manipulation of character perspectives
and events. We use this framework to design first- and second-order ToM tasks
alongside WM tasks that control for the ability to track and model mental
states. Our experiments across a suite of state-of-the-art LLMs reveal that
most models perform better on WM tasks than ToM tasks, and that models tend to
perform better reasoning with humans compared to inanimate objects.
Additionally, our framework enabled us to find evidence of heuristic behavior
such as recency bias and an over-reliance on earlier events in the story. All
code for generating data and evaluations is freely available.

</details>


### [9] [Human-Aligned Faithfulness in Toxicity Explanations of LLMs](https://arxiv.org/abs/2506.19113)
*Ramaravind K. Mothilal,Joanna Roy,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: This paper evaluates LLMs' reasoning about toxicity through a new metric called HAF, revealing that while they can generate plausible explanations for simple prompts, they fail when dealing with complex toxicity relationships.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating free-form toxicity explanations are not effective due to their reliance on input text perturbations and other challenges. The goal is to enhance the trustworthiness of LLMs in downstream tasks by evaluating their reasoning about toxicity.

Method: Proposed a novel multi-dimensional criterion called Human-Aligned Faithfulness (HAF) and developed six metrics based on uncertainty quantification to evaluate LLMs' toxicity explanations without human involvement.

Result: Experiments on three Llama models and an 8B Ministral model across five toxicity datasets showed that LLMs struggle with nuanced toxicity reasoning, resulting in inconsistent and nonsensical responses.

Conclusion: LLMs generate plausible explanations for simple prompts but fail to reason about nuanced toxicity relations, leading to inconsistent and nonsensical responses.

Abstract: The discourse around toxicity and LLMs in NLP largely revolves around
detection tasks. This work shifts the focus to evaluating LLMs' reasoning about
toxicity -- from their explanations that justify a stance -- to enhance their
trustworthiness in downstream tasks. Despite extensive research on
explainability, it is not straightforward to adopt existing methods to evaluate
free-form toxicity explanation due to their over-reliance on input text
perturbations, among other challenges. To account for these, we propose a
novel, theoretically-grounded multi-dimensional criterion, Human-Aligned
Faithfulness (HAF), that measures the extent to which LLMs' free-form toxicity
explanations align with those of a rational human under ideal conditions. We
develop six metrics, based on uncertainty quantification, to comprehensively
evaluate \haf of LLMs' toxicity explanations with no human involvement, and
highlight how "non-ideal" the explanations are. We conduct several experiments
on three Llama models (of size up to 70B) and an 8B Ministral model on five
diverse toxicity datasets. Our results show that while LLMs generate plausible
explanations to simple prompts, their reasoning about toxicity breaks down when
prompted about the nuanced relations between the complete set of reasons, the
individual reasons, and their toxicity stances, resulting in inconsistent and
nonsensical responses. We open-source our code and LLM-generated explanations
at https://github.com/uofthcdslab/HAF.

</details>


### [10] [Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data](https://arxiv.org/abs/2506.19159)
*Yun Tang,Eesung Kim,Vijendra Raj Apsingekar*

Main category: cs.CL

TL;DR: 本文提出了一种联合语音和文本优化方法，用于混合转换器和基于注意力的编码器解码器（TAED）建模，以利用大量文本语料库并提高ASR准确性。J-TAED在Librispeech数据集上减少了5.8~12.8%的WER，并在金融和命名实体聚焦的数据集上分别带来了15.3%和17.8%的WER减少。


<details>
  <summary>Details</summary>
Motivation: 为了克服领域不匹配任务中的数据稀缺问题，本文提出了J-TAED方法，该方法不需要语音数据即可进行文本基础领域自适应。

Method: 提出了一种联合语音和文本优化方法，用于混合转换器和基于注意力的编码器解码器（TAED）建模，以利用大量文本语料库并提高ASR准确性。J-TAED同时使用语音和文本输入模态进行训练，但在推理时仅使用语音数据。训练后的模型可以统一不同模态的内部表示，并进一步扩展到基于文本的领域自适应。

Result: J-TAED成功地将语音和语言信息整合到一个模型中，并在Librispeech数据集上减少了5.8~12.8%的WER。文本基础领域自适应在金融和命名实体聚焦的数据集上分别带来了15.3%和17.8%的WER减少。

Conclusion: J-TAED成功地将语音和语言信息整合到一个模型中，并在Librispeech数据集上减少了5.8~12.8%的WER。文本基础领域自适应在金融和命名实体聚焦的数据集上分别带来了15.3%和17.8%的WER减少。

Abstract: A joint speech and text optimization method is proposed for hybrid transducer
and attention-based encoder decoder (TAED) modeling to leverage large amounts
of text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained
with both speech and text input modalities together, while it only takes speech
data as input during inference. The trained model can unify the internal
representations from different modalities, and be further extended to
text-based domain adaptation. It can effectively alleviate data scarcity for
mismatch domain tasks since no speech data is required. Our experiments show
J-TAED successfully integrates speech and linguistic information into one
model, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model
is also evaluated on two out-of-domain datasets: one is finance and another is
named entity focused. The text-based domain adaptation brings 15.3% and 17.8%
WER reduction on those two datasets respectively.

</details>


### [11] [Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages](https://arxiv.org/abs/2506.19187)
*Christopher Toukmaji,Jeffrey Flanigan*

Main category: cs.CL

TL;DR: 本研究探讨了如何将大型语言模型（LLMs）跨语言适应以用于低资源语言的上下文学习。结果显示，少样本提示和翻译测试方法优于基于梯度的适应方法，并发现了灾难性遗忘是导致性能下降的原因。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量关于提示设置的研究，但仍然不清楚如何将LLM跨语言适应以用于低资源目标语言的上下文学习。

Method: 我们进行了全面的研究，涵盖了五种不同的目标语言、三种基础LLM和七个下游任务，使用了多种适应技术：少样本提示、翻译测试、微调、嵌入重新初始化和指令微调。

Result: 我们的结果表明，少样本提示和翻译测试设置显著优于基于梯度的适应方法。通过VOR指标分析，我们发现这些训练模型的性能下降是由于灾难性遗忘。

Conclusion: 我们的研究展示了少样本提示和翻译测试设置在低资源语言的上下文学习中表现优于基于梯度的适应方法。我们设计了一个新的指标VOR，并通过实证分析发现这些训练模型的退化是由于灾难性遗忘。

Abstract: LLMs are typically trained in high-resource languages, and tasks in
lower-resourced languages tend to underperform the higher-resource language
counterparts for in-context learning. Despite the large body of work on
prompting settings, it is still unclear how LLMs should be adapted
cross-lingually specifically for in-context learning in the low-resource target
languages. We perform a comprehensive study spanning five diverse target
languages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU
training hours (9,900+ TFLOPs) across various adaptation techniques: few-shot
prompting, translate-test, fine-tuning, embedding re-initialization, and
instruction fine-tuning. Our results show that the few-shot prompting and
translate-test settings tend to heavily outperform the gradient-based
adaptation methods. To better understand this discrepancy, we design a novel
metric, Valid Output Recall (VOR), and analyze model outputs to empirically
attribute the degradation of these trained models to catastrophic forgetting.
To the extent of our knowledge, this is the largest study done on in-context
learning for low-resource languages with respect to train compute and number of
adaptation techniques considered. We make all our datasets and trained models
available for public use.

</details>


### [12] [Augmenting Multi-Agent Communication with State Delta Trajectory](https://arxiv.org/abs/2506.19209)
*Yichen Tang,Weihang Su,Yujia Zhou,Yiqun Liu,Min Zhang,Shaoping Ma,Qingyao Ai*

Main category: cs.CL

TL;DR: 本文提出了一种新的通信协议，通过传输自然语言标记和逐个标记的状态转移轨迹来改善基于LLM的多智能体系统的性能。实验结果表明，这种方法在涉及复杂推理的任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的多智能体系统主要使用自然语言进行智能体通信，这虽然简单且易于理解，但会导致信息丢失，特别是在传递推理逻辑或抽象思想时。因此，我们需要一种更有效的通信协议来解决这个问题。

Method: 我们提出了一种新的通信协议，该协议从一个智能体向另一个智能体传输自然语言标记和逐个标记的状态转移轨迹。我们提出了状态差分编码（SDE）方法来表示状态转移轨迹。

Result: 实验结果表明，使用SDE的多智能体系统在涉及复杂推理的任务中实现了最先进的性能。

Conclusion: 实验结果表明，使用SDE的多智能体系统在涉及复杂推理的任务中实现了最先进的性能，这展示了通信增强对于基于LLM的多智能体系统的潜力。

Abstract: Multi-agent techniques such as role playing or multi-turn debates have been
shown to be effective in improving the performance of large language models
(LLMs) in downstream tasks. Despite their differences in workflows, existing
LLM-based multi-agent systems mostly use natural language for agent
communication. While this is appealing for its simplicity and interpretability,
it also introduces inevitable information loss as one model must down sample
its continuous state vectors to concrete tokens before transferring them to the
other model. Such losses are particularly significant when the information to
transfer is not simple facts, but reasoning logics or abstractive thoughts. To
tackle this problem, we propose a new communication protocol that transfers
both natural language tokens and token-wise state transition trajectory from
one agent to another. Particularly, compared to the actual state value, we find
that the sequence of state changes in LLMs after generating each token can
better reflect the information hidden behind the inference process, so we
propose a State Delta Encoding (SDE) method to represent state transition
trajectories. The experimental results show that multi-agent systems with SDE
achieve SOTA performance compared to other communication protocols,
particularly in tasks that involve complex reasoning. This shows the potential
of communication augmentation for LLM-based multi-agent systems.

</details>


### [13] [Personality Prediction from Life Stories using Language Models](https://arxiv.org/abs/2506.19258)
*Rasiq Hussain,Jerry Ma,Rithik Khandelwal,Joshua Oltmanns,Mehak Gupta*

Main category: cs.CL

TL;DR: 本研究提出了一种两步方法，利用预训练语言模型和循环神经网络（RNN）来处理长上下文数据，以预测五因素模型（FFM）的人格特质。结果显示，该方法在预测准确性、效率和可解释性方面有所提高，并突显了结合基于语言的特征与长上下文建模在从生活叙述中推进人格评估的潜力。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理（NLP）通过利用丰富的、开放式的文本，为个性评估提供了新的途径，超越了传统的问卷调查。在本研究中，我们解决了建模长叙述访谈的挑战，其中每个叙述都超过2000个标记，以预测五因素模型（FFM）的人格特质。

Method: 我们提出了一种两步方法：首先，使用滑动窗口微调预训练语言模型来提取上下文嵌入；然后，应用具有注意力机制的循环神经网络（RNN）来整合长距离依赖关系并提高可解释性。

Result: 通过消融研究和与最先进的长上下文模型（如LLaMA和Longformer）的比较，我们证明了预测准确性的提高、效率和可解释性。

Conclusion: 我们的结果突显了结合基于语言的特征与长上下文建模在从生活叙述中推进人格评估的潜力。

Abstract: Natural Language Processing (NLP) offers new avenues for personality
assessment by leveraging rich, open-ended text, moving beyond traditional
questionnaires. In this study, we address the challenge of modeling long
narrative interview where each exceeds 2000 tokens so as to predict Five-Factor
Model (FFM) personality traits. We propose a two-step approach: first, we
extract contextual embeddings using sliding-window fine-tuning of pretrained
language models; then, we apply Recurrent Neural Networks (RNNs) with attention
mechanisms to integrate long-range dependencies and enhance interpretability.
This hybrid method effectively bridges the strengths of pretrained transformers
and sequence modeling to handle long-context data. Through ablation studies and
comparisons with state-of-the-art long-context models such as LLaMA and
Longformer, we demonstrate improvements in prediction accuracy, efficiency, and
interpretability. Our results highlight the potential of combining
language-based features with long-context modeling to advance personality
assessment from life narratives.

</details>


### [14] [What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning](https://arxiv.org/abs/2506.19262)
*Yuchang Zhu,Zhonghua zhen,Qunshu Lin,Haotong Wei,Xiaolong Sun,Zixuan Yu,Minghao Liu,Zibin Zheng,Liang Chen*

Main category: cs.CL

TL;DR: 本研究探讨了LLM生成数据的多样性对下游模型性能的影响，发现适度多样化的数据可以提升模型性能，而高度多样化的数据则会产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量研究关注LLM生成数据的影响，但这些工作往往忽视了数据多样性这一关键因素。我们希望通过研究数据多样性对模型性能的影响，填补这一空白。

Method: 我们研究了LLM生成数据的多样性对下游模型性能的影响，并探讨了不同比例的LLM生成数据混合训练的效果。

Result: 实验结果表明，适度多样化的LLM生成数据在标注数据不足的情况下可以提升模型性能，而高度多样化的数据则产生负面影响。

Conclusion: 我们的实证结果表明，适度多样化的LLM生成数据可以在标注数据不足的情况下提升模型性能，而高度多样化的生成数据则有负面影响。我们希望这些发现能为未来关于LLM作为数据生成器的研究提供有价值的指导。

Abstract: With the remarkable generative capabilities of large language models (LLMs),
using LLM-generated data to train downstream models has emerged as a promising
approach to mitigate data scarcity in specific domains and reduce
time-consuming annotations. However, recent studies have highlighted a critical
issue: iterative training on self-generated data results in model collapse,
where model performance degrades over time. Despite extensive research on the
implications of LLM-generated data, these works often neglect the importance of
data diversity, a key factor in data quality. In this work, we aim to
understand the implications of the diversity of LLM-generated data on
downstream model performance. Specifically, we explore how varying levels of
diversity in LLM-generated data affect downstream model performance.
Additionally, we investigate the performance of models trained on data that
mixes different proportions of LLM-generated data, which we refer to as
synthetic data. Our experimental results show that, with minimal distribution
shift, moderately diverse LLM-generated data can enhance model performance in
scenarios with insufficient labeled data, whereas highly diverse generated data
has a negative impact. We hope our empirical findings will offer valuable
guidance for future studies on LLMs as data generators.

</details>


### [15] [EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition](https://arxiv.org/abs/2506.19279)
*Zhiyang Qi,Keiko Takamizo,Mariko Ukiyo,Michimasa Inaba*

Main category: cs.CL

TL;DR: EmoStage is a framework that enhances empathetic response generation in AI-driven counseling systems by leveraging open-source LLMs and incorporating perspective-taking and phase recognition.


<details>
  <summary>Details</summary>
Motivation: The rising demand for mental health care has fueled interest in AI-driven counseling systems. Current approaches face challenges including limited understanding of clients' psychological states, reliance on high-quality training data, and privacy concerns associated with commercial deployment.

Method: EmoStage is a framework that enhances empathetic response generation by leveraging the inference capabilities of open-source LLMs without additional training data. It introduces perspective-taking to infer clients' psychological states and support needs, and incorporates phase recognition to ensure alignment with the counseling process.

Result: Experiments conducted in both Japanese and Chinese counseling settings demonstrate that EmoStage improves the quality of responses generated by base models and performs competitively with data-driven methods.

Conclusion: EmoStage improves the quality of responses generated by base models and performs competitively with data-driven methods.

Abstract: The rising demand for mental health care has fueled interest in AI-driven
counseling systems. While large language models (LLMs) offer significant
potential, current approaches face challenges, including limited understanding
of clients' psychological states and counseling stages, reliance on
high-quality training data, and privacy concerns associated with commercial
deployment. To address these issues, we propose EmoStage, a framework that
enhances empathetic response generation by leveraging the inference
capabilities of open-source LLMs without additional training data. Our
framework introduces perspective-taking to infer clients' psychological states
and support needs, enabling the generation of emotionally resonant responses.
In addition, phase recognition is incorporated to ensure alignment with the
counseling process and to prevent contextually inappropriate or inopportune
responses. Experiments conducted in both Japanese and Chinese counseling
settings demonstrate that EmoStage improves the quality of responses generated
by base models and performs competitively with data-driven methods.

</details>


### [16] [JCAPT: A Joint Modeling Approach for CAPT](https://arxiv.org/abs/2506.19315)
*Tzu-Hsuan Yang,Yue-Yang He,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的CAPT框架，结合了Mamba、音系特征和思考标记策略，在MDD任务中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 有效发音反馈对第二语言学习至关重要，而计算机辅助发音训练（CAPT）系统通常包括两个关键任务：自动发音评估（APA）和错误发音检测与诊断（MDD）。最近的研究表明，这两个任务的联合建模可以产生相互益处。

Method: 我们提出了一个统一的框架，利用Mamba（一种选择性状态空间模型）结合音系特征和思考标记策略，以联合增强APA和MDD的可解释性和细粒度时间推理。

Result: 在speechocean762基准上的实验表明，我们的模型在MDD任务上表现优于之前的方法。

Conclusion: 我们的模型在MDD任务上表现优于先前方法，尤其是在语音识别和诊断方面。

Abstract: Effective pronunciation feedback is critical in second language (L2)
learning, for which computer-assisted pronunciation training (CAPT) systems
often encompass two key tasks: automatic pronunciation assessment (APA) and
mispronunciation detection and diagnosis (MDD). Recent work has shown that
joint modeling of these two tasks can yield mutual benefits. Our unified
framework leverages Mamba, a selective state space model (SSM), while
integrating phonological features and think token strategies to jointly enhance
interpretability and fine-grained temporal reasoning in APA and MDD. To our
knowledge, this is the first study to combine phonological attribution,
SSM-based modeling, and prompting in CAPT. A series of experiments conducted on
the speechocean762 benchmark demonstrate that our model consistently
outperforms prior methods, particularly on the MDD task.

</details>


### [17] [Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation](https://arxiv.org/abs/2506.19352)
*Jisu Shin,Juhyun Oh,Eunsu Kim,Hoyun Song,Alice Oh*

Main category: cs.CL

TL;DR: 本文提出了一种原子级别的评估框架，以更精确地评估大型语言模型中的个性一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法通常对整个响应分配单个分数，难以捕捉细微的个性偏差，特别是在长文本生成中。

Method: 我们提出了一个原子级别的评估框架，通过三个关键指标来衡量个性的一致性和对齐程度。

Result: 我们的实验表明，我们的框架能够有效地检测之前方法忽视的个性不一致。

Conclusion: 我们的框架能够有效检测之前方法忽视的个性不一致，揭示了任务结构和个性吸引力如何影响模型适应性，突出了保持一致个性表达的挑战。

Abstract: Ensuring persona fidelity in large language models (LLMs) is essential for
maintaining coherent and engaging human-AI interactions. However, LLMs often
exhibit Out-of-Character (OOC) behavior, where generated responses deviate from
an assigned persona, leading to inconsistencies that affect model reliability.
Existing evaluation methods typically assign single scores to entire responses,
struggling to capture subtle persona misalignment, particularly in long-form
text generation. To address this limitation, we propose an atomic-level
evaluation framework that quantifies persona fidelity at a finer granularity.
Our three key metrics measure the degree of persona alignment and consistency
within and across generations. Our approach enables a more precise and
realistic assessment of persona fidelity by identifying subtle deviations that
real users would encounter. Through our experiments, we demonstrate that our
framework effectively detects persona inconsistencies that prior methods
overlook. By analyzing persona fidelity across diverse tasks and personality
types, we reveal how task structure and persona desirability influence model
adaptability, highlighting challenges in maintaining consistent persona
expression.

</details>


### [18] [Measuring and Guiding Monosemanticity](https://arxiv.org/abs/2506.19382)
*Ruben Härle,Felix Friedrich,Manuel Brack,Stephan Wäldchen,Björn Deiseroth,Patrick Schramowski,Kristian Kersting*

Main category: cs.CL

TL;DR: 本文介绍了特征单义性评分（FMS）和引导稀疏自编码器（G-SAE），以提高大型语言模型的可解释性和控制能力。


<details>
  <summary>Details</summary>
Motivation: 目前的方法在可靠地定位和操作特征表示方面面临根本性的挑战。稀疏自编码器（SAE）虽然在大规模特征提取方面显示出前景，但它们也受到不完全特征隔离和不可靠单义性的限制。

Method: 我们引入了特征单义性评分（FMS），这是一种新的度量标准，用于量化潜在表示中的特征单义性。基于这些见解，我们提出了引导稀疏自编码器（G-SAE），这是一种在训练期间对标记概念进行条件处理的潜在表示方法。

Result: 我们在毒性检测、写作风格识别和隐私属性识别方面的评估表明，潜在空间中目标概念的可靠定位和解缠可以提高可解释性、行为检测和控制。具体来说，G-SAE不仅增强了单义性，还实现了更有效和细粒度的控制，而不会显著降低质量。

Conclusion: 我们的研究结果为测量和推进大型语言模型的机制可解释性和控制提供了可行的指导方针。

Abstract: There is growing interest in leveraging mechanistic interpretability and
controllability to better understand and influence the internal dynamics of
large language models (LLMs). However, current methods face fundamental
challenges in reliably localizing and manipulating feature representations.
Sparse Autoencoders (SAEs) have recently emerged as a promising direction for
feature extraction at scale, yet they, too, are limited by incomplete feature
isolation and unreliable monosemanticity. To systematically quantify these
limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric
to quantify feature monosemanticity in latent representation. Building on these
insights, we propose Guided Sparse Autoencoders (G-SAE), a method that
conditions latent representations on labeled concepts during training. We
demonstrate that reliable localization and disentanglement of target concepts
within the latent space improve interpretability, detection of behavior, and
control. Specifically, our evaluations on toxicity detection, writing style
identification, and privacy attribute recognition show that G-SAE not only
enhances monosemanticity but also enables more effective and fine-grained
steering with less quality degradation. Our findings provide actionable
guidelines for measuring and advancing mechanistic interpretability and control
of LLMs.

</details>


### [19] [Automated Detection of Pre-training Text in Black-box LLMs](https://arxiv.org/abs/2506.19399)
*Ruihan Hu,Yu-Ming Shang,Jiankun Peng,Wei Luo,Yazhe Wang,Xi Zhang*

Main category: cs.CL

TL;DR: 本文提出了VeilProbe，这是一个用于在黑盒设置中自动检测LLMs预训练文本的框架。通过利用序列到序列映射模型和关键标记扰动，以及基于原型的成员分类器，该框架在三个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 检测给定文本是否是大型语言模型（LLMs）预训练数据的成员对于确保数据隐私和版权保护至关重要。现有的方法依赖于LLM的隐藏信息（例如模型参数或标记概率），在只能访问输入和输出文本的黑盒设置中无效。虽然已经提出了一些方法用于黑盒设置，但它们依赖于大量的手动工作，例如设计复杂的问题或指令。

Method: 我们提出了VeilProbe，这是一个用于在没有人工干预的情况下检测LLMs预训练文本的框架。它利用序列到序列映射模型来推断输入文本和LLM生成的相应输出后缀之间的潜在映射特征，然后进行关键标记扰动以获得更具区分性的成员特征。此外，考虑到现实世界中真实训练文本样本有限的情况，引入了一个基于原型的成员分类器以减轻过拟合问题。

Result: 在三个广泛使用的数据集上的大量评估表明，我们的框架在黑盒设置中是有效的且优越的。

Conclusion: 我们的框架在黑盒设置中有效且优越。

Abstract: Detecting whether a given text is a member of the pre-training data of Large
Language Models (LLMs) is crucial for ensuring data privacy and copyright
protection. Most existing methods rely on the LLM's hidden information (e.g.,
model parameters or token probabilities), making them ineffective in the
black-box setting, where only input and output texts are accessible. Although
some methods have been proposed for the black-box setting, they rely on massive
manual efforts such as designing complicated questions or instructions. To
address these issues, we propose VeilProbe, the first framework for
automatically detecting LLMs' pre-training texts in a black-box setting without
human intervention. VeilProbe utilizes a sequence-to-sequence mapping model to
infer the latent mapping feature between the input text and the corresponding
output suffix generated by the LLM. Then it performs the key token
perturbations to obtain more distinguishable membership features. Additionally,
considering real-world scenarios where the ground-truth training text samples
are limited, a prototype-based membership classifier is introduced to alleviate
the overfitting issue. Extensive evaluations on three widely used datasets
demonstrate that our framework is effective and superior in the black-box
setting.

</details>


### [20] [Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study](https://arxiv.org/abs/2506.19418)
*Yingji Zhang,Marco Valentino,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 该研究探讨了如何通过语言变分自编码器（VAEs）将推理规则显式嵌入和记忆在语言模型中，并提出了一个完整的管道来学习这些规则。实验结果显示了推理规则的解耦、先验知识的有效注入以及模型性能的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的语言模型在自然语言推理（NLI）任务上表现出色，但往往依赖于记忆而非基于规则的推理。因此，需要一种方法来显式地嵌入和记忆推理规则，以提高模型的泛化能力、可解释性和可控性。

Method: 该研究使用了语言变分自编码器（VAEs）来显式嵌入和记忆推理规则，并提出了一个完整的管道，包括三个基于规则的推理任务、一个支持性的理论框架和一个实用的端到端架构。

Result: 实验表明，在显式信号监督下，推理规则可以被解耦并在编码器的参数空间中分离，导致输出特征空间中规则的聚类。此外，将推理信息注入查询可以更有效地从内存中检索存储的值，这种方法为将先验知识集成到仅解码器语言模型中提供了一种简单的方法。然而，在数学推理任务中，增加样本数量不会持续提升性能，且FFN层比注意力层更能保持推理规则的分离。

Conclusion: 该研究通过将推理规则显式嵌入和记忆在语言模型中，提出了一个完整的管道来学习推理规则，并展示了推理规则在编码器参数空间中的解耦、先验知识注入以及性能瓶颈。

Abstract: Incorporating explicit reasoning rules within the latent space of language
models (LMs) offers a promising pathway to enhance generalisation,
interpretability, and controllability. While current Transformer-based language
models have shown strong performance on Natural Language Inference (NLI) tasks,
they often rely on memorisation rather than rule-based inference. This work
investigates how reasoning rules can be explicitly embedded and memorised
within the LMs through Language Variational Autoencoders (VAEs). We propose a
complete pipeline for learning reasoning rules within Transformer-based
language VAEs. This pipeline encompasses three rule-based reasoning tasks, a
supporting theoretical framework, and a practical end-to-end architecture. The
experiment illustrates the following findings: Disentangled reasoning: Under
explicit signal supervision, reasoning rules - viewed as functional mappings -
can be disentangled within the encoder's parametric space. This separation
results in distinct clustering of rules in the output feature space. Prior
knowledge injection: injecting reasoning information into the Query enables the
model to more effectively retrieve the stored value Value from memory based on
Key. This approach offers a simple method for integrating prior knowledge into
decoder-only language models. Performance bottleneck: In mathematical reasoning
tasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance
beyond a point. Moreover, ffn layers are better than attention layers at
preserving the separation of reasoning rules in the model's parameters.

</details>


### [21] [Can Large Language Models Capture Human Annotator Disagreements?](https://arxiv.org/abs/2506.19467)
*Jingwei Ni,Yu Fan,Vilém Zouhar,Donya Rooein,Alexander Hoyle,Mrinmaya Sachan,Markus Leippold,Dirk Hovy,Elliott Ash*

Main category: cs.CL

TL;DR: 我们的研究探讨了大型语言模型在预测标注分歧方面的表现，发现它们在这一任务上存在困难，并且基于多数标签的评估可能会忽略这一点。此外，我们发现RLVR风格的推理虽然提高了模型的整体性能，但在分歧预测方面却降低了性能。我们的研究强调了在分歧建模中评估和改进大型语言模型注释器的重要性。


<details>
  <summary>Details</summary>
Motivation: 人类标注差异在自然语言处理中很常见，并且通常反映了任务主观性和样本模糊性等重要信息。尽管大型语言模型越来越多地用于自动标注以减少人工努力，但它们的评估通常集中在预测多数投票的“真实标签”上。然而，尚不清楚这些模型是否也捕捉了有信息量的人类标注差异。

Method: 我们通过广泛评估大型语言模型预测标注分歧的能力来解决这一差距，而无需访问重复的人类标签。

Result: 我们的结果表明，大型语言模型在建模分歧方面存在困难，这可能被基于多数标签的评估所忽视。值得注意的是，虽然RLVR风格（可验证奖励的强化学习）推理通常会提高大型语言模型的性能，但它会降低分歧预测的性能。

Conclusion: 我们的研究结果表明，大型语言模型在建模标注分歧方面存在困难，这可能被基于多数标签的评估所忽视。我们的发现强调了在分歧建模中评估和改进大型语言模型注释器的必要性。

Abstract: Human annotation variation (i.e., annotation disagreements) is common in NLP
and often reflects important information such as task subjectivity and sample
ambiguity. While Large Language Models (LLMs) are increasingly used for
automatic annotation to reduce human effort, their evaluation often focuses on
predicting the majority-voted "ground truth" labels. It is still unclear,
however, whether these models also capture informative human annotation
variation. Our work addresses this gap by extensively evaluating LLMs' ability
to predict annotation disagreements without access to repeated human labels.
Our results show that LLMs struggle with modeling disagreements, which can be
overlooked by majority label-based evaluations. Notably, while RLVR-style
(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM
performance, it degrades performance in disagreement prediction. Our findings
highlight the critical need for evaluating and improving LLM annotators in
disagreement modeling. Code and data at
https://github.com/EdisonNi-hku/Disagreement_Prediction.

</details>


### [22] [MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages](https://arxiv.org/abs/2506.19468)
*Wenhan Han,Yifan Zhang,Zhixun Chen,Binbin Liu,Haobin Lin,Bingni Zhang,Taifeng Wang,Mykola Pechenizkiy,Meng Fang,Yin Zheng*

Main category: cs.CL

TL;DR: 本文介绍了MuBench基准测试，用于评估多语言大语言模型的性能，并发现了英语和低资源语言之间的性能差距。同时，提出了多语言一致性（MLC）作为补充指标，并预训练了一套1.2B参数的模型以研究跨语言迁移动态。


<details>
  <summary>Details</summary>
Motivation: 现有的评估数据集有限且缺乏跨语言对齐，导致对多语言能力的评估在语言和技能覆盖方面是碎片化的。因此，需要一个更全面的基准测试来评估多语言大语言模型的性能。

Method: 本文引入了MuBench基准测试，评估了多种最先进的多语言大语言模型，并提出了多语言一致性（MLC）作为补充指标。此外，还预训练了一套1.2B参数的模型以研究跨语言迁移动态。

Result: 本文发现，多语言大语言模型在实际语言覆盖方面存在显著差距，特别是在英语和低资源语言之间存在持续的性能差异。此外，通过MuBench的对齐，提出了多语言一致性（MLC）作为补充指标，并预训练了一套1.2B参数的模型以研究跨语言迁移动态。

Conclusion: 本文提出了MuBench基准测试，以评估多语言大语言模型的性能，并发现了英语和低资源语言之间的性能差距。此外，还提出了多语言一致性（MLC）作为补充指标，并预训练了一套1.2B参数的模型以研究跨语言迁移动态。

Abstract: Multilingual large language models (LLMs) are advancing rapidly, with new
models frequently claiming support for an increasing number of languages.
However, existing evaluation datasets are limited and lack cross-lingual
alignment, leaving assessments of multilingual capabilities fragmented in both
language and skill coverage. To address this, we introduce MuBench, a benchmark
covering 61 languages and evaluating a broad range of capabilities. We evaluate
several state-of-the-art multilingual LLMs and find notable gaps between
claimed and actual language coverage, particularly a persistent performance
disparity between English and low-resource languages. Leveraging MuBench's
alignment, we propose Multilingual Consistency (MLC) as a complementary metric
to accuracy for analyzing performance bottlenecks and guiding model
improvement. Finally, we pretrain a suite of 1.2B-parameter models on English
and Chinese with 500B tokens, varying language ratios and parallel data
proportions to investigate cross-lingual transfer dynamics.

</details>


### [23] [Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models](https://arxiv.org/abs/2506.19483)
*Marcos Estecha-Garitagoitia,Chen Zhang,Mario Rodríguez-Cantelar,Luis Fernando D'Haro*

Main category: cs.CL

TL;DR: 本文探讨了利用大型语言模型进行对话系统中的常识关系数据增强和自动评估的方法，并展示了初步结果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索基于不同类型的常识关系进行对话系统的turn-level数据增强任务，并自动评估生成的合成turn。

Method: 该方法利用了预训练大型语言模型（LLMs）的扩展知识和零样本能力，以遵循指令、理解上下文信息和其常识推理能力。它借鉴了如Chain-of-Thought（CoT）等方法，更明确地应用于基于提示生成的对话数据增强任务，并基于常识属性进行自动评估。

Result: 初步结果表明，我们的方法能够有效利用大型语言模型的常识推理和评估能力。

Conclusion: 初步结果表明，我们的方法有效地利用了大型语言模型在对话系统中的常识推理和评估能力。

Abstract: This paper provides preliminary results on exploring the task of performing
turn-level data augmentation for dialogue system based on different types of
commonsense relationships, and the automatic evaluation of the generated
synthetic turns. The proposed methodology takes advantage of the extended
knowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)
to follow instructions, understand contextual information, and their
commonsense reasoning capabilities. The approach draws inspiration from
methodologies like Chain-of-Thought (CoT), applied more explicitly to the task
of prompt-based generation for dialogue-based data augmentation conditioned on
commonsense attributes, and the automatic evaluation of the generated
dialogues.
  To assess the effectiveness of the proposed approach, first we extracted 200
randomly selected partial dialogues, from 5 different well-known dialogue
datasets, and generate alternative responses conditioned on different event
commonsense attributes. This novel dataset allows us to measure the proficiency
of LLMs in generating contextually relevant commonsense knowledge, particularly
up to 12 different specific ATOMIC [10] database relations. Secondly, we
propose an evaluation framework to automatically detect the quality of the
generated dataset inspired by the ACCENT [26] metric, which offers a nuanced
approach to assess event commonsense. However, our method does not follow
ACCENT's complex eventrelation tuple extraction process. Instead, we propose an
instruction-based prompt for each commonsense attribute and use
state-of-the-art LLMs to automatically detect the original attributes used when
creating each augmented turn in the previous step.
  Preliminary results suggest that our approach effectively harnesses LLMs
capabilities for commonsense reasoning and evaluation in dialogue systems.

</details>


### [24] [Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning](https://arxiv.org/abs/2506.19484)
*Russell Beale*

Main category: cs.CL

TL;DR: 本文综述了LLM在高等教育中的应用，并扩展到中学和终身学习情境。它结合了对话教学理论，分析了提示策略和RAG如何使LLM行为与教学理论一致，并提出了策略以改善LLM互动与教学法的对齐。


<details>
  <summary>Details</summary>
Motivation: LLM正在迅速改变教育，通过丰富的对话学习体验。然而，当前的LLM在应用传统教学理论方面存在不足，例如倾向于直接提供答案而不是促进知识的共同构建。因此，需要提出策略以更好地将LLM互动与良好的教学法对齐。

Method: 本文综合现有文献，探讨了LLM在教育中的应用，并结合对话和对话教学理论（如维果茨基的社会文化学习、苏格拉底方法和劳里拉德的对话框架），分析了提示策略和检索增强生成（RAG）如何使LLM行为与这些教学理论相一致，并支持个性化和适应性学习。

Result: 本文识别了将先前理论应用于LLM时的显著差距，并提出了实用策略，例如设计鼓励苏格拉底式提问、支架式指导和学生反思的提示，以及整合检索机制以确保准确性和上下文相关性。

Conclusion: 本文旨在弥合教育理论与AI驱动的对话式学习实践之间的差距，提供见解和工具，使基于LLM的对话更具教育成效和理论一致性。

Abstract: Large Language Models (LLMs) are rapidly transforming education by enabling
rich conversational learning experiences. This article provides a comprehensive
review of how LLM-based conversational agents are being used in higher
education, with extensions to secondary and lifelong learning contexts. We
synthesize existing literature on LLMs in education and theories of
conversational and dialogic pedagogy - including Vygotsky's sociocultural
learning (scaffolding and the Zone of Proximal Development), the Socratic
method, and Laurillard's conversational framework - and examine how prompting
strategies and retrieval-augmented generation (RAG) can align LLM behaviors
with these pedagogical theories, and how it can support personalized, adaptive
learning. We map educational theories to LLM capabilities, highlighting where
LLM-driven dialogue supports established learning principles and where it
challenges or falls short of traditional pedagogical assumptions. Notable gaps
in applying prior theories to LLMs are identified, such as the models tendency
to provide direct answers instead of fostering co-construction of knowledge,
and the need to account for the constant availability and broad but non-human
expertise of LLM tutors. In response, we propose practical strategies to better
align LLM interactions with sound pedagogy - for example, designing prompts
that encourage Socratic questioning, scaffolded guidance, and student
reflection, as well as integrating retrieval mechanisms to ensure accuracy and
contextual relevance. Our aim is to bridge the gap between educational theory
and the emerging practice of AI-driven conversational learning, offering
insights and tools for making LLM-based dialogues more educationally productive
and theory-aligned.

</details>


### [25] [Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs](https://arxiv.org/abs/2506.19492)
*Shu Yang,Junchao Wu,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 本文研究了高效推理策略对大型推理模型一致性的影响，发现这些策略可能导致模型行为不一致，需要进一步研究其潜在风险。


<details>
  <summary>Details</summary>
Motivation: 研究高效推理策略是否引入行为不一致性，因为压缩推理可能会降低模型响应的鲁棒性，并导致模型遗漏关键推理步骤。

Method: 引入了ICBENCH基准，用于在三个维度上测量LRMs的一致性：任务设置之间的不一致（ITS）、训练目标与学习行为之间的不一致（TR-LB）以及内部推理与自我解释之间的不一致（IR-SE）。

Result: 发现较大的模型通常比较小的模型表现出更高的稳定性，但所有模型都表现出广泛的“策略性”行为，包括自我不一致、事后合理化和隐藏推理线索。此外，高效的推理策略（如无思考和简单令牌预算）会增加所有三种定义的一致性问题。

Conclusion: 虽然高效推理可以提高令牌级别的效率，但需要进一步研究以确定它是否同时引入了模型逃避有效监督的风险。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
tasks by engaging in extended reasoning before producing final answers, yet
this strength introduces the risk of overthinking, where excessive token
generation occurs even for simple tasks. While recent work in efficient
reasoning seeks to reduce reasoning length while preserving accuracy, it
remains unclear whether such optimization is truly a free lunch. Drawing on the
intuition that compressing reasoning may reduce the robustness of model
responses and lead models to omit key reasoning steps, we investigate whether
efficient reasoning strategies introduce behavioral inconsistencies. To
systematically assess this, we introduce $ICBENCH$, a benchmark designed to
measure inconsistency in LRMs across three dimensions: inconsistency across
task settings (ITS), inconsistency between training objectives and learned
behavior (TR-LB), and inconsistency between internal reasoning and
self-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs,
we find that while larger models generally exhibit greater consistency than
smaller ones, they all display widespread "scheming" behaviors, including
self-disagreement, post-hoc rationalization, and the withholding of reasoning
cues. Crucially, our results demonstrate that efficient reasoning strategies
such as No-Thinking and Simple Token-Budget consistently increase all three
defined types of inconsistency. These findings suggest that although efficient
reasoning enhances token-level efficiency, further investigation is imperative
to ascertain whether it concurrently introduces the risk of models evading
effective supervision.

</details>


### [26] [AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models](https://arxiv.org/abs/2506.19505)
*Zeyu Li,Chuanfu Xiao,Yang Wang,Xiang Liu,Zhenheng Tang,Baotong Lu,Mao Yang,Xinyu Chen,Xiaowen Chu*

Main category: cs.CL

TL;DR: 本文提出了AnTKV框架，通过Anchor Token-aware Vector Quantization压缩KV缓存，以减少超低比特量化带来的性能下降。AnTKV在处理长上下文和提高解码吞吐量方面表现出色，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管量化已成为减少大型语言模型（LLMs）中KV缓存内存占用的有效且轻量级解决方案，但最小化由超低比特KV缓存量化引起的性能下降仍然是一个重大挑战。观察到不同token的KV缓存量化对注意力输出质量的影响各不相同。

Method: 提出Anchor Score(AnS)来量化每个token的KV缓存对量化引起的误差的敏感性，并引入AnTKV框架，利用Anchor Token-aware Vector Quantization压缩KV缓存。设计了一个与FlashAttention完全兼容的Triton内核，以支持高效的在线Anchor Token选择。

Result: AnTKV能够在单个80GB A100 GPU上处理长达840K标记的上下文长度，同时实现比FP16基线高3.5倍的解码吞吐量。在Mistral-7B上，AnTKV在1位和0.375位的超低比特量化下分别实现了6.32和8.87的困惑度，远低于FP16基线的4.73。

Conclusion: AnTKV能够处理长达840K标记的上下文长度，并在单个80GB A100 GPU上实现比FP16基线高3.5倍的解码吞吐量。实验结果表明，AnTKV在4位设置下与KIVI、SKVQ、KVQuant和CQ等先前工作相媲美或超越。更重要的是，AnTKV在Mistral-7B上的超低比特量化下实现了显著降低的困惑度。

Abstract: Quantization has emerged as an effective and lightweight solution to reduce
the memory footprint of the KV cache in Large Language Models (LLMs).
Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV
cache quantization remains a significant challenge. We observe that quantizing
the KV cache of different tokens has varying impacts on the quality of
attention outputs. To systematically investigate this phenomenon, we perform
forward error propagation analysis on attention and propose the Anchor Score
(AnS) that quantifies the sensitivity of each token's KV cache to
quantization-induced error. Our analysis reveals significant disparities in AnS
across tokens, suggesting that preserving a small subset with full precision
(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive
quantization scenarios. Based on this insight, we introduce AnTKV, a novel
framework that leverages Anchor Token-aware Vector Quantization to compress the
KV cache. Furthermore, to support efficient deployment, we design and develop a
triton kernel that is fully compatible with FlashAttention, enabling fast
online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context
lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x
higher decoding throughput compared to the FP16 baseline. Our experiment
results demonstrate that AnTKV matches or outperforms prior works such as KIVI,
SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves
significantly lower perplexity under ultra-low-bit quantization on Mistral-7B,
with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of
4.73.

</details>


### [27] [heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation](https://arxiv.org/abs/2506.19512)
*Ashish Chouhan,Michael Gertz*

Main category: cs.CL

TL;DR: 本文介绍了我们的团队heiDS用于ArchEHR-QA 2025共享任务的方法。我们设计了一个使用检索增强生成（RAG）框架的管道，以生成基于患者电子健康记录（EHRs）的临床证据的答案。我们探索了RAG框架的各种组件，重点关注排名列表截断（RLT）检索策略和归因方法。我们的策略在生成事实性和相关性答案方面优于固定-k方法。


<details>
  <summary>Details</summary>
Motivation: 为了生成基于患者特定问题的准确和相关答案，我们研究了RAG框架的不同组件，特别是RLT检索策略和归因方法。

Method: 我们设计了一个使用检索增强生成（RAG）框架的管道，以生成基于患者电子健康记录（EHRs）的临床证据的答案。我们探索了RAG框架的各种组件，重点关注排名列表截断（RLT）检索策略和归因方法。

Result: 实验结果表明，我们的策略在生成事实性和相关性答案方面优于固定-k方法。

Conclusion: 我们的策略在生成事实性和相关性答案方面优于固定-k方法。

Abstract: This paper presents the approach of our team called heiDS for the ArchEHR-QA
2025 shared task. A pipeline using a retrieval augmented generation (RAG)
framework is designed to generate answers that are attributed to clinical
evidence from the electronic health records (EHRs) of patients in response to
patient-specific questions. We explored various components of a RAG framework,
focusing on ranked list truncation (RLT) retrieval strategies and attribution
approaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a
query-dependent-k retrieval strategy, including the existing surprise and
autocut methods and two new methods proposed in this work, autocut* and elbow.
The experimental results show the benefits of our strategy in producing factual
and relevant answers when compared to a fixed-$k$.

</details>


### [28] [Automatic Posology Structuration : What role for LLMs?](https://arxiv.org/abs/2506.19525)
*Natalia Bobkova,Laura Zanella-Calzada,Anyes Tafoughalt,Raphaël Teboul,François Plesse,Félix Gaschi*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型（LLMs）来结构化法语处方中的用药说明，并提出了一个混合流水线，以提高结构化准确性并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 自动结构化用药说明对于提高药物安全性和实现临床决策支持至关重要。在法语处方中，这些说明常常模糊、不规则或口语化，限制了经典机器学习流水线的效果。

Method: 我们探索了使用大型语言模型（LLMs）将自由文本的posologies转换为结构化格式，比较了基于提示的方法和微调与基于命名实体识别和链接（NERL）的“预LLM”系统。

Result: 结果表明，虽然提示可以提高性能，但只有微调的LLMs才能达到基线的准确性。通过错误分析，我们观察到互补的优势：NERL提供了结构精度，而LLMs更好地处理了语义细微差别。基于此，我们提出了一种混合流水线，将NERL低置信度的案例（<0.8）路由到LLM，并根据置信度分数选择输出。这种策略实现了91%的结构化准确性，同时最小化了延迟和计算。

Conclusion: 我们的结果表明，这种混合方法在提高结构化准确性的同时限制了计算成本，为现实世界的临床应用提供了一个可扩展的解决方案。

Abstract: Automatically structuring posology instructions is essential for improving
medication safety and enabling clinical decision support. In French
prescriptions, these instructions are often ambiguous, irregular, or
colloquial, limiting the effectiveness of classic ML pipelines. We explore the
use of Large Language Models (LLMs) to convert free-text posologies into
structured formats, comparing prompt-based methods and fine-tuning against a
"pre-LLM" system based on Named Entity Recognition and Linking (NERL). Our
results show that while prompting improves performance, only fine-tuned LLMs
match the accuracy of the baseline. Through error analysis, we observe
complementary strengths: NERL offers structural precision, while LLMs better
handle semantic nuances. Based on this, we propose a hybrid pipeline that
routes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs
based on confidence scores. This strategy achieves 91% structuration accuracy
while minimizing latency and compute. Our results show that this hybrid
approach improves structuration accuracy while limiting computational cost,
offering a scalable solution for real-world clinical use.

</details>


### [29] [KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs](https://arxiv.org/abs/2506.19527)
*Kelin Fu,Kaigui Bian*

Main category: cs.CL

TL;DR: KnowMap is a novel approach that dynamically constructs a knowledge base from environmental and experiential data to improve the task-adapting capabilities of large language models.


<details>
  <summary>Details</summary>
Motivation: Traditional methods such as fine-tuning are often costly, data-intensive, and may lead to 'catastrophic forgetting.'

Method: KnowMap dynamically constructs a knowledge base from environmental and experiential data and fine-tunes a small knowledge-embedding model to equip a larger LLM with valuable task-specific knowledge.

Result: Experiments on the ScienceWorld benchmark demonstrate 17.71% improvement for the performance of gpt-4-turbo model.

Conclusion: KnowMap not only provides an efficient and effective means for LLM task-adapting, but also highlights how integrating environmental and experiential knowledge can enhance LLMs' reasoning capabilities.

Abstract: While Large Language Models (LLMs) possess significant capabilities in
open-world agent tasks, they also face challenges in rapidly adapting to new,
specialized tasks due to their reliance on static pre-trained knowledge.
Traditional methods such as fine-tuning are often costly, data-intensive, and
may lead to "catastrophic forgetting." Therefore, we present KnowMap, a novel
approach that dynamically constructs a knowledge base from environmental and
experiential data. KnowMap fine-tunes a small knowledge-embedding model to
equip a larger LLM with valuable task-specific knowledge. Our experiments on
the ScienceWorld benchmark demonstrate 17.71% improvement for the performance
of gpt-4-turbo model. KnowMap not only provides an efficient and effective
means for LLM task-adapting, but also highlights how integrating environmental
and experiential knowledge can enhance LLMs' reasoning capabilities.

</details>


### [30] [Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection](https://arxiv.org/abs/2506.19548)
*Devesh Pant,Rishi Raj Grandhe,Vipin Samaria,Mukul Paul,Sudhir Kumar,Saransh Khanna,Jatin Agrawal,Jushaan Singh Kalra,Akhil VSSG,Satish V Khalikar,Vipin Garg,Himanshu Chauhan,Pranay Verma,Neha Khandelwal,Soma S Dhavala,Minesh Mathew*

Main category: cs.CL

TL;DR: Health Sentinel is an automated system that processes online articles to detect potential disease outbreaks, successfully identifying numerous health events and aiding in timely intervention.


<details>
  <summary>Details</summary>
Motivation: Traditional indicator-based surveillance faces challenges, and manual screening of the vast number of online articles is impractical. Therefore, there is a need for an automated system to monitor informal sources like online media for early detection of disease outbreaks.

Method: Health Sentinel is a multi-stage information extraction pipeline that uses a combination of ML and non-ML methods to extract events-structured information concerning disease outbreaks or other unusual health events from online articles.

Result: Health Sentinel has processed over 300 million news articles and identified over 95,000 unique health events across India, with over 3,500 events shortlisted by public health experts at NCDC as potential outbreaks.

Conclusion: Health Sentinel has effectively processed a large volume of news articles and identified numerous health events, with a significant number being recognized as potential outbreaks by public health experts.

Abstract: Early detection of disease outbreaks is crucial to ensure timely intervention
by the health authorities. Due to the challenges associated with traditional
indicator-based surveillance, monitoring informal sources such as online media
has become increasingly popular. However, owing to the number of online
articles getting published everyday, manual screening of the articles is
impractical. To address this, we propose Health Sentinel. It is a multi-stage
information extraction pipeline that uses a combination of ML and non-ML
methods to extract events-structured information concerning disease outbreaks
or other unusual health events-from online articles. The extracted events are
made available to the Media Scanning and Verification Cell (MSVC) at the
National Centre for Disease Control (NCDC), Delhi for analysis, interpretation
and further dissemination to local agencies for timely intervention. From April
2022 till date, Health Sentinel has processed over 300 million news articles
and identified over 95,000 unique health events across India of which over
3,500 events were shortlisted by the public health experts at NCDC as potential
outbreaks.

</details>


### [31] [RCStat: A Statistical Framework for using Relative Contextualization in Transformers](https://arxiv.org/abs/2506.19549)
*Debabrata Mahapatra,Shubham Agarwal,Apoorv Saxena,Subrata Mitra*

Main category: cs.CL

TL;DR: RCStat是一种基于原始注意力logits的统计框架，通过相对上下文化（RC）来衡量token段之间的上下文对齐程度，并推导出RC的高效上界。它在压缩和归因任务中实现了最先进的性能，且无需任何模型微调。


<details>
  <summary>Details</summary>
Motivation: 先前关于自回归变压器中输入标记重要性的研究依赖于Softmax归一化的注意力权重，这掩盖了预-Softmax查询-键logits的更丰富的结构。

Method: RCStat是一种统计框架，利用相对上下文化（RC）的原始注意力logits，RC是一个衡量token段之间上下文对齐程度的随机变量，并推导出RC的高效上界。

Result: RCStat在问答、摘要和归因基准测试中取得了显著的实证成果，实现了最先进的压缩和归因性能。

Conclusion: RCStat在无需任何模型微调的情况下，在压缩和归因任务中实现了最先进的性能，并在问答、摘要和归因基准测试中取得了显著的实证成果。

Abstract: Prior work on input-token importance in auto-regressive transformers has
relied on Softmax-normalized attention weights, which obscure the richer
structure of pre-Softmax query-key logits. We introduce RCStat, a statistical
framework that harnesses raw attention logits via Relative Contextualization
(RC), a random variable measuring contextual alignment between token segments,
and derive an efficient upper bound for RC. We demonstrate two applications:
(i) Key-Value compression, where RC-based thresholds drive adaptive key-value
eviction for substantial cache reduction with minimal quality loss; and (ii)
Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level
explanations than post-Softmax methods. Across question answering,
summarization, and attribution benchmarks, RCStat achieves significant
empirical gains, delivering state-of-the-art compression and attribution
performance without any model retraining.

</details>


### [32] [Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress](https://arxiv.org/abs/2506.19571)
*Lorenzo Proietti,Stefano Perrella,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文研究了机器翻译评估中度量标准的表现，并探讨了人类与自动度量标准之间的比较，指出尽管有这些发现，但仍需谨慎对待。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解度量性能并确定上限，本文引入了人类基线。

Method: 本文在机器翻译元评估中引入了人类基线，以更清晰地了解度量性能并建立上限。

Result: 结果表明，人类标注者并不总是优于自动度量标准，最先进的度量标准通常与人类基线相当或更高。

Conclusion: 本文旨在探讨机器翻译评估中度量标准的极限，并引发对整个机器翻译评估领域关键问题的讨论。

Abstract: In Machine Translation (MT) evaluation, metric performance is assessed based
on agreement with human judgments. In recent years, automatic metrics have
demonstrated increasingly high levels of agreement with humans. To gain a
clearer understanding of metric performance and establish an upper bound, we
incorporate human baselines in the MT meta-evaluation, that is, the assessment
of MT metrics' capabilities. Our results show that human annotators are not
consistently superior to automatic metrics, with state-of-the-art metrics often
ranking on par with or higher than human baselines. Despite these findings
suggesting human parity, we discuss several reasons for caution. Finally, we
explore the broader implications of our results for the research field, asking:
Can we still reliably measure improvements in MT evaluation? With this work, we
aim to shed light on the limits of our ability to measure progress in the
field, fostering discussion on an issue that we believe is crucial to the
entire MT evaluation community.

</details>


### [33] [ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model](https://arxiv.org/abs/2506.19599)
*Zhenke Duan,Jiqun Pan,Jiani Tu,Xiaoyi Wang,Yanqing Wang*

Main category: cs.CL

TL;DR: ECCoT is a framework that validates and refines reasoning chains in LLMs by integrating MRF-ETM and CSBert, improving interpretability and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency and unreliable outputs in Large Language Models (LLMs), especially in their reasoning chains.

Method: ECCoT integrates the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment.

Result: ECCoT filters ineffective chains using structured ordering statistics, leading to improved interpretability and trustworthiness of LLM-based decision-making.

Conclusion: ECCoT improves interpretability, reduces biases, and enhances the trustworthiness of LLM-based decision-making.

Abstract: In the era of large-scale artificial intelligence, Large Language Models
(LLMs) have made significant strides in natural language processing. However,
they often lack transparency and generate unreliable outputs, raising concerns
about their interpretability. To address this, the Chain of Thought (CoT)
prompting method structures reasoning into step-by-step deductions. Yet, not
all reasoning chains are valid, and errors can lead to unreliable conclusions.
We propose ECCoT, an End-to-End Cognitive Chain of Thought Validation
Framework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates
the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT
generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By
filtering ineffective chains using structured ordering statistics, ECCoT
improves interpretability, reduces biases, and enhances the trustworthiness of
LLM-based decision-making. Key contributions include the introduction of ECCoT,
MRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning
enhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.

</details>


### [34] [Social Hatred: Efficient Multimodal Detection of Hatemongers](https://arxiv.org/abs/2506.19603)
*Tom Marzea,Abraham Israeli,Oren Tsur*

Main category: cs.CL

TL;DR: 本文提出了一种多模态聚合方法，用于检测煽动仇恨的人，通过考虑文本、用户活动和用户网络。在多个数据集上的实验结果表明，该方法在检测仇恨煽动者方面优于以往的方法，并且在不同平台上表现良好。


<details>
  <summary>Details</summary>
Motivation: 尽管大多数先前的工作集中在检测仇恨言论上，但我们认为关注用户层面同样重要，尽管具有挑战性。

Method: 本文考虑了一种多模态聚合方法，用于检测煽动仇恨的人，考虑到可能具有仇恨的文本、用户活动和用户网络。

Result: 在三个独特的数据集X（Twitter）、Gab和Parler上评估我们的方法，结果表明，在用户的社交背景下处理用户的文本显著提高了仇恨煽动者的检测效果，相比之前使用的基于文本和图的方法。

Conclusion: 我们的方法可以用于改进编码信息、暗语和种族煤气灯效应的分类，并为干预措施提供信息。此外，我们证明了我们的多模态方法在非常不同的内容平台和大型数据集和网络上表现良好。

Abstract: Automatic detection of online hate speech serves as a crucial step in the
detoxification of the online discourse. Moreover, accurate classification can
promote a better understanding of the proliferation of hate as a social
phenomenon. While most prior work focus on the detection of hateful utterances,
we argue that focusing on the user level is as important, albeit challenging.
In this paper we consider a multimodal aggregative approach for the detection
of hate-mongers, taking into account the potentially hateful texts, user
activity, and the user network. Evaluating our method on three unique datasets
X (Twitter), Gab, and Parler we show that processing a user's texts in her
social context significantly improves the detection of hate mongers, compared
to previously used text and graph-based methods. We offer comprehensive set of
results obtained in different experimental settings as well as qualitative
analysis of illustrative cases. Our method can be used to improve the
classification of coded messages, dog-whistling, and racial gas-lighting, as
well as to inform intervention measures. Moreover, we demonstrate that our
multimodal approach performs well across very different content platforms and
over large datasets and networks.

</details>


### [35] [Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge](https://arxiv.org/abs/2506.19607)
*Juraj Vladika,Ihsan Soydemir,Florian Matthes*

Main category: cs.CL

TL;DR: 本文研究了两种自我纠正系统在纠正幻觉摘要中的应用，并发现了搜索引擎片段和少量提示的有效性以及G-Eval与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在生成连贯文本方面表现出色，但它们存在幻觉问题——即事实不准确的陈述。自我纠正方法被提出以解决这一问题，但它们在新闻摘要等领域的应用较少。

Method: 本文应用两种最先进的自我纠正系统来纠正使用三个搜索引擎证据的幻觉摘要。

Result: 本文分析了结果，并提供了关于系统性能的见解，揭示了搜索引擎片段和少量提示的好处，以及G-Eval与人类评估的高度一致性。

Conclusion: 本文分析了两种最先进的自我纠正系统在纠正幻觉摘要中的表现，并提供了关于搜索引擎片段和少量提示的好处以及G-Eval与人类评估高度一致的见解。

Abstract: While large language models (LLMs) have shown remarkable capabilities to
generate coherent text, they suffer from the issue of hallucinations --
factually inaccurate statements. Among numerous approaches to tackle
hallucinations, especially promising are the self-correcting methods. They
leverage the multi-turn nature of LLMs to iteratively generate verification
questions inquiring additional evidence, answer them with internal or external
knowledge, and use that to refine the original response with the new
corrections. These methods have been explored for encyclopedic generation, but
less so for domains like news summarization. In this work, we investigate two
state-of-the-art self-correcting systems by applying them to correct
hallucinated summaries using evidence from three search engines. We analyze the
results and provide insights into systems' performance, revealing interesting
practical findings on the benefits of search engine snippets and few-shot
prompts, as well as high alignment of G-Eval and human evaluation.

</details>


### [36] [Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager](https://arxiv.org/abs/2506.19652)
*Lucie Galland,Catherine Pelachaud,Florian Pecune*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，将大型语言模型与基于强化学习的对话管理器相结合，用于开放式的有特定目标的对话。通过分层强化学习和元学习，该方法提高了系统的适应性和效率，使其能够在有限的数据下学习，并在不同的对话阶段之间流畅过渡，同时个性化回应异质患者的需要。在动机访谈中，该框架的表现优于最先进的LLM基线，展示了对LLM进行条件化以创建具有特定目标的开放式对话系统的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的对话系统在处理开放式的有特定目标的对话时存在适应性和效率的问题，尤其是在面对多样化的用户需求时。因此，我们需要一种新的方法来提高系统的适应性和效率，使其能够更好地满足不同用户的需求。

Method: 我们提出了一种新颖的框架，将大型语言模型（LLMs）与基于强化学习（RL）的对话管理器相结合，用于开放式的有特定目标的对话。我们利用分层强化学习来建模对话的结构化阶段，并采用元学习来提高在不同用户配置文件中的适应性。

Result: 我们在动机访谈中应用了我们的框架，并证明所提出的对话管理器在奖励方面优于最先进的LLM基线，展示了对LLM进行条件化以创建具有特定目标的开放式对话系统的潜力。

Conclusion: 我们的方法通过结合大型语言模型和基于强化学习的对话管理器，提高了适应性和效率，使系统能够在有限的数据下学习，并在不同的对话阶段之间流畅过渡，同时个性化回应异质患者的需要。此外，我们的框架在动机访谈中表现出优于最先进的LLM基线的奖励表现，展示了对LLM进行条件化以创建具有特定目标的开放式对话系统的潜力。

Abstract: In this work, we propose a novel framework that integrates large language
models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a
specific goal. By leveraging hierarchical reinforcement learning to model the
structured phases of dialogue and employ meta-learning to enhance adaptability
across diverse user profiles, our approach enhances adaptability and
efficiency, enabling the system to learn from limited data, transition fluidly
between dialogue phases, and personalize responses to heterogeneous patient
needs. We apply our framework to Motivational Interviews, aiming to foster
behavior change, and demonstrate that the proposed dialogue manager outperforms
a state-of-the-art LLM baseline in terms of reward, showing a potential benefit
of conditioning LLMs to create open-ended dialogue systems with specific goals.

</details>


### [37] [Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?](https://arxiv.org/abs/2506.19733)
*Chuxuan Hu,Yuxuan Zhu,Antony Kellermann,Caleb Biddulph,Suppakit Waiwitlikhit,Jason Benn,Daniel Kang*

Main category: cs.CL

TL;DR: RPT在类似微调数据的任务上表现良好，但在新领域中的泛化能力不确定。


<details>
  <summary>Details</summary>
Motivation: 为了理解RPT的泛化能力，因为之前的工作仅在微调数据的相同领域上评估RPT模型。

Method: 进行了两项研究：(1) 观察性研究，比较了多种开放权重的RPT模型与其对应的基线模型在多个领域的表现；(2) 干预性研究，对LLMs进行单领域微调并评估其在多个领域的表现。

Result: 两项研究都得出相同的结论，即RPT在与微调数据相似的任务上带来显著提升，但在不同推理模式的领域中泛化能力不一致。

Conclusion: 尽管RPT在与微调数据相似的任务上带来了显著的提升，但这些提升在不同推理模式的领域中可能不一致甚至消失。

Abstract: Reinforcement post training (RPT) has recently shown promise in improving the
reasoning abilities of large language models (LLMs). However, it remains
unclear how well these improvements generalize to new domains, as prior work
evaluates RPT models on data from the same domains used for fine-tuning. To
understand the generalizability of RPT, we conduct two studies. (1)
Observational: We compare a wide range of open-weight RPT models against their
corresponding base models across multiple domains, including both seen and
unseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs
with RPT on single domains and evaluate their performance across multiple
domains. Both studies converge on the same conclusion that, although RPT brings
substantial gains on tasks similar to the fine-tuning data, the gains
generalize inconsistently and can vanish on domains with different reasoning
patterns.

</details>


### [38] [Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach](https://arxiv.org/abs/2506.19750)
*Takashi Nishibayashi,Seiji Kanazawa,Kumpei Yamada*

Main category: cs.CL

TL;DR: 本文提出了一种新的合成案例模拟方法，用于评估SC算法更新对罕见疾病诊断性能的影响。通过使用HPO数据生成合成案例，并与实际指标变化进行比较，验证了该方法的有效性。该方法具有低成本和高效率的特点，有助于提高罕见疾病的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 获取足够用于罕见疾病评估的数据很困难，手动创建大量临床案例既昂贵又不切实际。因此，需要一种新的方法来评估SC算法更新对罕见疾病诊断性能的影响。

Method: 我们使用来自人类表型本体（HPO）的疾病-表型注释来生成合成案例，并通过模拟SC访谈来估计算法更新对现实世界诊断性能的影响。

Result: 实验包括八次过去的SC算法更新。对于HPO中有频率信息的疾病（n=5），召回率@8的变化R^2为0.831（p=0.031），精确度@8的变化R^2为0.78（p=0.047），表明该方法可以预测部署后的性能。而对于没有频率信息的疾病（n=3），预测误差较大，突显了频率信息的重要性。将HPO表型映射到SC症状的手动工作量约为每种疾病2小时。

Conclusion: 我们的方法利用公开的、专家创建的知识库，实现了对个别罕见疾病SC算法变化的部署前评估。这种透明且低成本的方法使开发人员能够高效地提高罕见疾病的诊断性能，可能有助于早期诊断的支持。

Abstract: Background: Symptom Checkers (SCs) provide users with personalized medical
information. To prevent performance degradation from algorithm updates, SC
developers must evaluate diagnostic performance changes for individual diseases
before deployment. However, acquiring sufficient evaluation data for rare
diseases is difficult, and manually creating numerous clinical vignettes is
costly and impractical. Objective: This study proposes and validates a novel
Synthetic Vignette Simulation Approach to evaluate diagnostic performance
changes for individual rare diseases following SC algorithm updates. Methods:
We used disease-phenotype annotations from the Human Phenotype Ontology (HPO),
a knowledge database for rare diseases, to generate synthetic vignettes. With
these, we simulated SC interviews to estimate the impact of algorithm updates
on real-world diagnostic performance. The method's effectiveness was evaluated
retrospectively by comparing estimated values with actual metric changes using
the R 2(R-squared) coefficient. Results: The experiment included eight past SC
algorithm updates. For updates on diseases with frequency information in HPO
(n=5), the R^2 for recall@8 change was 0.831 (p=0.031), and for precision@8
change, it was 0.78 (p=0.047), indicating the method can predict
post-deployment performance. In contrast, large prediction errors occurred for
diseases without frequency information (n=3), highlighting its importance. The
manual effort to map HPO phenotypes to SC symptoms was approximately 2 hours
per disease. Conclusions: Our method enables pre-deployment evaluation of SC
algorithm changes for individual rare diseases using a publicly available,
expert-created knowledge base. This transparent and low-cost approach allows
developers to efficiently improve diagnostic performance for rare diseases,
potentially enhancing support for early diagnosis.

</details>


### [39] [Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis](https://arxiv.org/abs/2506.19753)
*Omar A. Essameldin,Ali O. Elbeih,Wael H. Gomaa,Wael F. Elsersy*

Main category: cs.CL

TL;DR: 本研究探讨了阿拉伯语方言分类问题，测试了多种NLP模型，并发现MARBERTv2表现最佳，为个性化聊天机器人和社会媒体监控提供了潜在应用。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语是世界上使用最广泛的语言之一，有22个国家的大量方言。本研究旨在解决QADI数据集中阿拉伯语推文的18种阿拉伯语方言分类问题。

Method: 本研究创建并测试了RNN模型、Transformer模型以及通过提示工程的大型语言模型（LLMs），其中MARBERTv2表现最佳。

Result: MARBERTv2在准确率和F1分数上分别达到了65%和64%。

Conclusion: 该研究通过使用最先进的预处理技术和最新的NLP模型，识别了阿拉伯语方言识别中的主要语言问题，并验证了其在个性化聊天机器人、社交媒体监控和提高阿拉伯社区可访问性方面的应用。

Abstract: The Arabic language is among the most popular languages in the world with a
huge variety of dialects spoken in 22 countries. In this study, we address the
problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.
RNN models, Transformer models, and large language models (LLMs) via prompt
engineering are created and tested. Among these, MARBERTv2 performed best with
65% accuracy and 64% F1-score. Through the use of state-of-the-art
preprocessing techniques and the latest NLP models, this paper identifies the
most significant linguistic issues in Arabic dialect identification. The
results corroborate applications like personalized chatbots that respond in
users' dialects, social media monitoring, and greater accessibility for Arabic
communities.

</details>


### [40] [Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR](https://arxiv.org/abs/2506.19761)
*Martin Ratajczak,Jean-Philippe Robichaud,Jennifer Drexler Fox*

Main category: cs.CL

TL;DR: The paper explores the use of linear complexity recurrent attention (RA) layers for long-form speech recognition. It shows that RA layers are as accurate as multi-head attention (MHA) but more efficient. The paper also introduces a long-form training paradigm and a novel regularization method called Direction Dropout, which further improves the performance of RA layers.


<details>
  <summary>Details</summary>
Motivation: ASR models based on multi-head attention (MHA) are not suitable for long-form ASR due to their quadratic complexity in sequence length. The paper aims to investigate alternative approaches that are more efficient while maintaining accuracy.

Method: The paper builds on recent work on linear complexity recurrent attention (RA) layers for ASR. It introduces a limited-context attention (LCA) baseline and evaluates the performance of RA layers. It also develops a long-form training paradigm and proposes Direction Dropout as a regularization method.

Result: Bidirectional RA layers can match the accuracy of MHA for both short- and long-form applications. RA layers are as accurate as LCA but more efficient. The long-form training paradigm improves RA performance, leading to better accuracy than LCA with 44% higher throughput. Direction Dropout improves accuracy, provides fine-grained control of the accuracy/throughput trade-off, and enables a new alternating directions decoding mode with higher throughput.

Conclusion: RA layers are as accurate as MHA for both short- and long-form applications, and they are more efficient. The long-form training paradigm and Direction Dropout further improve the performance of RA layers.

Abstract: Long-form speech recognition is an application area of increasing research
focus. ASR models based on multi-head attention (MHA) are ill-suited to
long-form ASR because of their quadratic complexity in sequence length. We
build on recent work that has investigated linear complexity recurrent
attention (RA) layers for ASR. We find that bidirectional RA layers can match
the accuracy of MHA for both short- and long-form applications. We present a
strong limited-context attention (LCA) baseline, and show that RA layers are
just as accurate while being more efficient. We develop a long-form training
paradigm which further improves RA performance, leading to better accuracy than
LCA with 44% higher throughput. We also present Direction Dropout, a novel
regularization method that improves accuracy, provides fine-grained control of
the accuracy/throughput trade-off of bidirectional RA, and enables a new
alternating directions decoding mode with even higher throughput.

</details>


### [41] [SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning](https://arxiv.org/abs/2506.19767)
*Yuqian Fu,Tinghong Chen,Jiajun Chai,Xihuai Wang,Songjun Tu,Guojun Yin,Wei Lin,Qichao Zhang,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的单阶段方法SRFT，通过熵感知加权机制统一了监督微调和强化学习，实验表明SRFT在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理任务中取得了显著进展，但最优集成监督微调（SFT）和强化学习（RL）仍然是一个基本挑战。

Method: 通过熵感知加权机制统一了监督微调和强化学习两种微调范式。

Result: SRFT在五个数学推理基准上的平均准确率为59.1%，比零RL方法高出9.0%；在三个分布外基准上高出10.9%。

Conclusion: SRFT在多个数学推理基准和分布外基准上表现出色，优于零RL方法。

Abstract: Large language models (LLMs) have achieved remarkable progress in reasoning
tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) remains a fundamental challenge. Through
comprehensive analysis of token distributions, learning dynamics, and
integration mechanisms from entropy-based perspectives, we reveal key
differences between these paradigms: SFT induces coarse-grained global changes
to LLM policy distributions, while RL performs fine-grained selective
optimizations, with entropy serving as a critical indicator of training
effectiveness. Building on these observations, we propose Supervised
Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both
fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach
simultaneously applies SFT and RL to directly optimize the LLM using
demonstrations and self-exploration rollouts rather than through two-stage
sequential methods. Extensive experiments show that SRFT achieves 59.1% average
accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning
benchmarks and 10.9% on three out-of-distribution benchmarks.

</details>


### [42] [Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study](https://arxiv.org/abs/2506.19794)
*Yuqi Zhu,Yi Zhong,Jintian Zhang,Ziheng Zhang,Shuofei Qiao,Yujie Luo,Lun Du,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文研究了增强开源大型语言模型数据分析能力的策略，并通过实验证明了数据合成方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动化数据分析任务方面具有潜力，但开源模型在这些需要推理的场景中面临重大限制。

Method: 通过策划一个包含多种现实场景的种子数据集，我们在三个维度上评估了模型：数据理解、代码生成和战略规划。

Result: 我们的分析揭示了三个关键发现：(1) 战略规划质量是模型性能的主要决定因素；(2) 交互设计和任务复杂性显著影响推理能力；(3) 数据质量在实现最佳性能方面比多样性更具影响力。

Conclusion: 我们利用这些见解开发了一种数据合成方法，证明了开源LLM的分析推理能力有显著提升。

Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks,
yet open-source models face significant limitations in these kinds of
reasoning-intensive scenarios. In this work, we investigate strategies to
enhance the data analysis capabilities of open-source LLMs. By curating a seed
dataset of diverse, realistic scenarios, we evaluate models across three
dimensions: data understanding, code generation, and strategic planning. Our
analysis reveals three key findings: (1) Strategic planning quality serves as
the primary determinant of model performance; (2) Interaction design and task
complexity significantly influence reasoning capabilities; (3) Data quality
demonstrates a greater impact than diversity in achieving optimal performance.
We leverage these insights to develop a data synthesis methodology,
demonstrating significant improvements in open-source LLMs' analytical
reasoning capabilities.

</details>


### [43] [How Effectively Can BERT Models Interpret Context and Detect Bengali Communal Violent Text?](https://arxiv.org/abs/2506.19831)
*Abdullah Khondoker,Enam Ahmed Taufik,Md. Iftekhar Islam Tashik,S M Ishtiak Mahmud,Farig Sadeque*

Main category: cs.CL

TL;DR: 本研究旨在提高检测煽动社区暴力文本的准确性，通过引入一个微调的BanglaBERT模型和一个集成模型，取得了较高的性能指标。同时，通过分析余弦相似度和应用LIME，我们揭示了模型在理解上下文方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于社区暴力文本的分类在现有研究中是一个未被充分探索的领域，因此本研究旨在提高检测煽动社区暴力文本的准确性。

Method: 我们引入了一个针对此任务进行微调的BanglaBERT模型，并通过添加1,794个实例来扩展数据集，开发和评估了一个微调的集成模型。此外，我们应用了LIME来解释模型的决策，并分析了词之间的余弦相似度。

Result: 微调的BanglaBERT模型达到了0.60的宏F1分数，而集成模型则达到了0.63的宏F1分数。此外，通过分析余弦相似度和应用LIME，我们发现了预训练的BanglaBERT模型在区分紧密相关的社区和非社区术语方面的局限性。

Conclusion: 我们的工作为社区暴力检测的研究做出了贡献，并为未来研究提供了基础，以改进这些技术以提高准确性和社会影响。

Abstract: The spread of cyber hatred has led to communal violence, fueling aggression
and conflicts between various religious, ethnic, and social groups, posing a
significant threat to social harmony. Despite its critical importance, the
classification of communal violent text remains an underexplored area in
existing research. This study aims to enhance the accuracy of detecting text
that incites communal violence, focusing specifically on Bengali textual data
sourced from social media platforms. We introduce a fine-tuned BanglaBERT model
tailored for this task, achieving a macro F1 score of 0.60. To address the
issue of data imbalance, our dataset was expanded by adding 1,794 instances,
which facilitated the development and evaluation of a fine-tuned ensemble
model. This ensemble model demonstrated an improved performance, achieving a
macro F1 score of 0.63, thus highlighting its effectiveness in this domain. In
addition to quantitative performance metrics, qualitative analysis revealed
instances where the models struggled with context understanding, leading to
occasional misclassifications, even when predictions were made with high
confidence. Through analyzing the cosine similarity between words, we
identified certain limitations in the pre-trained BanglaBERT models,
particularly in their ability to distinguish between closely related communal
and non-communal terms. To further interpret the model's decisions, we applied
LIME, which helped to uncover specific areas where the model struggled in
understanding context, contributing to errors in classification. These findings
highlight the promise of NLP and interpretability tools in reducing online
communal violence. Our work contributes to the growing body of research in
communal violence detection and offers a foundation for future studies aiming
to refine these techniques for better accuracy and societal impact.

</details>


### [44] [MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration](https://arxiv.org/abs/2506.19835)
*Yucheng Zhou,Lingran Song,Jianbing Shen*

Main category: cs.CL

TL;DR: MAM is a modular multi-agent framework for multi-modal medical diagnosis that improves performance and flexibility compared to existing models.


<details>
  <summary>Details</summary>
Motivation: Current unified multimodal medical LLMs face limitations in knowledge update costs, comprehensiveness, and flexibility. The goal is to address these challenges by introducing a modular and collaborative framework.

Method: MAM decomposes the medical diagnostic process into specialized roles, each embodied by an LLM-based agent, enabling efficient knowledge updates and leveraging existing medical LLMs and knowledge bases.

Result: Extensive experiments on multimodal medical datasets show that MAM consistently outperforms modality-specific LLMs, achieving performance improvements ranging from 18% to 365% compared to baseline models.

Conclusion: MAM demonstrates significant performance improvements over baseline models and provides an efficient, flexible, and comprehensive framework for multi-modal medical diagnosis.

Abstract: Recent advancements in medical Large Language Models (LLMs) have showcased
their powerful reasoning and diagnostic capabilities. Despite their success,
current unified multimodal medical LLMs face limitations in knowledge update
costs, comprehensiveness, and flexibility. To address these challenges, we
introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis
(MAM). Inspired by our empirical findings highlighting the benefits of role
assignment and diagnostic discernment in LLMs, MAM decomposes the medical
diagnostic process into specialized roles: a General Practitioner, Specialist
Team, Radiologist, Medical Assistant, and Director, each embodied by an
LLM-based agent. This modular and collaborative framework enables efficient
knowledge updates and leverages existing medical LLMs and knowledge bases.
Extensive experimental evaluations conducted on a wide range of publicly
accessible multimodal medical datasets, incorporating text, image, audio, and
video modalities, demonstrate that MAM consistently surpasses the performance
of modality-specific LLMs. Notably, MAM achieves significant performance
improvements ranging from 18% to 365% compared to baseline models. Our code is
released at https://github.com/yczhou001/MAM.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [45] [A Comment On "The Illusion of Thinking": Reframing the Reasoning Cliff as an Agentic Gap](https://arxiv.org/abs/2506.18957)
*Sheraz Khan,Subha Madhavan,Kannan Natarajan*

Main category: cs.AI

TL;DR: 本文质疑了大型推理模型在复杂问题上的性能下降是由于内在认知限制的观点，认为这是由于评估环境的限制所致。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨大型推理模型在复杂问题上的表现下降是否真的是由于内在的认知限制，还是由于评估环境的限制。

Method: 本文通过重新审视实验设计和评估方法，提出了一个代理差距的概念，并通过实验证明了工具增强模型能够克服这些限制。

Result: 实验结果表明，当模型被允许使用代理工具时，它们能够解决原本无法解决的问题，并且表现出更高级的推理能力。

Conclusion: 本文认为，大型推理模型（LRMs）的性能下降并非源于内在的认知边界，而是由于静态、仅限文本的评估范式中的系统级限制。

Abstract: The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:
Understanding the Strengths and Limitations of Reasoning Models via the Lens of
Problem Complexity, presents a compelling empirical finding, a reasoning cliff,
where the performance of Large Reasoning Models (LRMs) collapses beyond a
specific complexity threshold, which the authors posit as an intrinsic scaling
limitation of Chain-of-Thought (CoT) reasoning. This commentary, while
acknowledging the study's methodological rigor, contends that this conclusion
is confounded by experimental artifacts. We argue that the observed failure is
not evidence of a fundamental cognitive boundary, but rather a predictable
outcome of system-level constraints in the static, text-only evaluation
paradigm, including tool use restrictions, context window recall issues, the
absence of crucial cognitive baselines, inadequate statistical reporting, and
output generation limits. We reframe this performance collapse through the lens
of an agentic gap, asserting that the models are not failing at reasoning, but
at execution within a profoundly restrictive interface. We empirically
substantiate this critique by demonstrating a striking reversal. A model,
initially declaring a puzzle impossible when confined to text-only generation,
now employs agentic tools to not only solve it but also master variations of
complexity far beyond the reasoning cliff it previously failed to surmount.
Additionally, our empirical analysis of tool-enabled models like o4-mini and
GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural
execution to complex meta-cognitive self-correction, which has significant
implications for how we define and measure machine intelligence. The illusion
of thinking attributed to LRMs is less a reasoning deficit and more a
consequence of an otherwise capable mind lacking the tools for action.

</details>


### [46] [Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition](https://arxiv.org/abs/2506.19191)
*Craig Steven Wright*

Main category: cs.AI

TL;DR: 本文提出了一种基于贝叶斯推理、测度论和种群动力学的人工智能系统框架，通过结构化竞争和信念修正使代理进化，最终将真理作为进化吸引子，展示了可验证的知识如何在计算、自我调节的群体中产生。


<details>
  <summary>Details</summary>
Motivation: 开发一个数学严谨的人工智能系统，该系统由通过结构化竞争和信念修正进化的概率代理组成，以研究真理如何在计算、自我调节的群体中产生。

Method: 引入了一个基于贝叶斯推理、测度论和种群动力学的人工智能系统框架，其中代理通过结构化竞争和信念修正进行演化。代理的适应度是与代表真实世界的固定外部预言者对齐的函数。代理在离散时间环境中竞争，通过观察结果调整后验信念，高评分代理繁殖，低评分代理灭绝。评分通过成对的真理对齐效用比较更新，信念更新保持可测量的一致性和随机收敛。引入基于哈希的密码身份承诺以确保可追溯性，并使用do-演算的因果推断操作符。

Result: 系统建立了真理作为进化吸引子，展示了可验证的知识如何在计算、自我调节的群体中通过对抗性认识压力产生。提供了关于收敛性、鲁棒性和进化稳定性的形式定理。

Conclusion: 系统将真理作为进化吸引子，展示了可验证的知识如何在计算、自我调节的群体中通过对抗性认识压力产生。

Abstract: We introduce a mathematically rigorous framework for an artificial
intelligence system composed of probabilistic agents evolving through
structured competition and belief revision. The architecture, grounded in
Bayesian inference, measure theory, and population dynamics, defines agent
fitness as a function of alignment with a fixed external oracle representing
ground truth. Agents compete in a discrete-time environment, adjusting
posterior beliefs through observed outcomes, with higher-rated agents
reproducing and lower-rated agents undergoing extinction. Ratings are updated
via pairwise truth-aligned utility comparisons, and belief updates preserve
measurable consistency and stochastic convergence. We introduce hash-based
cryptographic identity commitments to ensure traceability, alongside causal
inference operators using do-calculus. Formal theorems on convergence,
robustness, and evolutionary stability are provided. The system establishes
truth as an evolutionary attractor, demonstrating that verifiable knowledge
arises from adversarial epistemic pressure within a computable, self-regulating
swarm.

</details>


### [47] [Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs](https://arxiv.org/abs/2506.19290)
*Liang Zeng,Yongcong Li,Yuzhen Xiao,Changshi Li,Chris Yuhao Liu,Rui Yan,Tianwen Wei,Jujie He,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种增量式自动化数据收集流程，以扩大SWE数据集的规模和多样性。我们的数据集包含10,169个真实Python任务实例，并展示了Skywork-SWE模型在SWE-bench Verified基准测试中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 软件工程（SWE）最近成为下一代LLM代理的重要测试平台，需要具备持续迭代问题解决（例如>50次交互回合）和长上下文依赖性解决（例如>32k标记）的能力。然而，SWE的数据收集过程仍然非常耗时，因为它严重依赖于手动注释进行代码文件过滤和专用运行环境的设置以执行和验证单元测试。因此，现有的大多数数据集仅限于几千个GitHub来源的实例。

Method: 我们提出了一个增量式的自动化数据收集流程，系统地扩大了SWE数据集的规模和多样性。我们的数据集包含来自2531个不同GitHub存储库的10,169个真实Python任务实例，每个实例都配有自然语言描述的任务和专用的运行环境镜像用于自动化单元测试验证。

Result: 当我们对Skywork-SWE模型进行微调时，我们发现了一个显著的数据扩展现象：随着数据量的增加，训练后的模型在LLM软件工程能力方面的表现持续提高，没有任何饱和迹象。此外，通过引入测试时缩放技术，性能进一步提升至47.0%的准确率，超越了之前子32B参数模型的SOTA结果。

Conclusion: 我们的Skywork-SWE模型在SWE-bench Verified基准测试中实现了38.0%的pass@1准确率，且通过引入测试时缩放技术，性能进一步提升至47.0%，超越了之前子32B参数模型的SOTA结果。我们发布了Skywork-SWE-32B模型检查点以加速未来研究。

Abstract: Software engineering (SWE) has recently emerged as a crucial testbed for
next-generation LLM agents, demanding inherent capabilities in two critical
dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)
and long-context dependency resolution (e.g., >32k tokens). However, the data
curation process in SWE remains notoriously time-consuming, as it heavily
relies on manual annotation for code file filtering and the setup of dedicated
runtime environments to execute and validate unit tests. Consequently, most
existing datasets are limited to only a few thousand GitHub-sourced instances.
To this end, we propose an incremental, automated data-curation pipeline that
systematically scales both the volume and diversity of SWE datasets. Our
dataset comprises 10,169 real-world Python task instances from 2,531 distinct
GitHub repositories, each accompanied by a task specified in natural language
and a dedicated runtime-environment image for automated unit-test validation.
We have carefully curated over 8,000 successfully runtime-validated training
trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE
model on these trajectories, we uncover a striking data scaling phenomenon: the
trained model's performance for software engineering capabilities in LLMs
continues to improve as the data size increases, showing no signs of
saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on
the SWE-bench Verified benchmark without using verifiers or multiple rollouts,
establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based
LLMs built on the OpenHands agent framework. Furthermore, with the
incorporation of test-time scaling techniques, the performance further improves
to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter
models. We release the Skywork-SWE-32B model checkpoint to accelerate future
research.

</details>


### [48] [NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling](https://arxiv.org/abs/2506.19500)
*Yan Jiang,Hao Zhou,LiZhong GU,Ai Han,TianLong Li*

Main category: cs.AI

TL;DR: NaviAgent是一种基于图导航的双级规划架构，通过多路径决策器和图编码导航器实现高效的工具链编排。实验表明，NaviAgent在所有基础模型和任务复杂度下都实现了最高的任务成功率（TSR），并表现出良好的质量和效率平衡。


<details>
  <summary>Details</summary>
Motivation: LLMs对静态知识的依赖和脆弱的工具调用严重阻碍了复杂、异构工具链的编排，尤其是在大规模情况下。现有的方法通常使用刚性的单路径执行，导致错误恢复能力差和搜索空间呈指数增长。

Method: NaviAgent是一种基于图导航的双级规划架构，包括多路径决策器和图编码导航器。多路径决策器定义了一个四维决策空间，并持续感知环境状态，动态选择最佳动作。图编码导航器构建了一个工具依赖异构图（TDHG），其中节点嵌入融合了API模式结构和历史调用行为，并集成了新的启发式搜索策略。

Result: 实验表明，NaviAgent在所有基础模型和任务复杂度下都实现了最高的任务成功率（TSR）。它在Qwen2.5-14B、Qwen2.5-32B和Deepseek-V3上的表现分别优于平均基线（ReAct、ToolLLM、α-UMI）13.5%、16.4%和19.0%。其执行步骤通常与最有效基线相差一步，确保了质量和效率的强平衡。微调的Qwen2.5-14B模型在我们的架构下实现了49.5%的TSR，超过了更大的32B模型（44.9%）。图编码导航器进一步提升了TSR平均2.4点，对于大型模型（如Deepseek-V3和GPT-4o）在复杂任务上的提升超过9点。

Conclusion: NaviAgent在所有基础模型和任务复杂度下都实现了最高的任务成功率（TSR），并且在执行步骤上表现出色，确保了质量和效率的平衡。此外，图编码导航器在工具链编排中起到了关键作用。

Abstract: LLMs' reliance on static knowledge and fragile tool invocation severely
hinders the orchestration of complex, heterogeneous toolchains, particularly at
large scales. Existing methods typically use rigid single-path execution,
resulting in poor error recovery and exponentially growing search spaces. We
introduce NaviAgent, a graph-navigated bilevel planning architecture for robust
function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.
As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional
decision space and continuously perceives environmental states, dynamically
selecting the optimal action to fully cover all tool invocation scenarios. The
Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph
(TDHG), where node embeddings explicitly fuse API schema structure with
historical invocation behavior. It also integrates a novel heuristic search
strategy that guides the Decider toward efficient and highly successful
toolchains, even for unseen tool combinations. Experiments show that NaviAgent
consistently achieves the highest task success rate (TSR) across all foundation
models and task complexities, outperforming the average baselines (ReAct,
ToolLLM, {\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,
and Deepseek-V3, respectively. Its execution steps are typically within one
step of the most efficient baseline, ensuring a strong balance between quality
and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of
49.5%, surpassing the much larger 32B model (44.9%) under our architecture.
Incorporating the Graph-Encoded Navigator further boosts TSR by an average of
2.4 points, with gains up over 9 points on complex tasks for larger models
(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain
orchestration.

</details>


### [49] [KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality](https://arxiv.org/abs/2506.19807)
*Baochang Ren,Shuofei Qiao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.AI

TL;DR: KnowRL is a method that integrates a factuality reward into RL training to reduce hallucinations in slow-thinking models while preserving their reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: To address the high hallucination in slow-thinking models, which output incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning.

Method: KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries.

Result: Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities.

Conclusion: KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities.

Abstract: Large Language Models (LLMs), particularly slow-thinking models, often
exhibit severe hallucination, outputting incorrect content due to an inability
to accurately recognize knowledge boundaries during reasoning. While
Reinforcement Learning (RL) can enhance complex reasoning abilities, its
outcome-oriented reward mechanism often lacks factual supervision over the
thinking process, further exacerbating the hallucination problem. To address
the high hallucination in slow-thinking models, we propose Knowledge-enhanced
RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by
integrating a factuality reward, based on knowledge verification, into the RL
training process, helping them recognize their knowledge boundaries. KnowRL
guides models to perform fact-based slow thinking by integrating a factuality
reward, based on knowledge verification, into the RL training process, helping
them recognize their knowledge boundaries. This targeted factual input during
RL training enables the model to learn and internalize fact-based reasoning
strategies. By directly rewarding adherence to facts within the reasoning
steps, KnowRL fosters a more reliable thinking process. Experimental results on
three hallucination evaluation datasets and two reasoning evaluation datasets
demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking
models while maintaining their original strong reasoning capabilities. Our code
is available at https://github.com/zjunlp/KnowRL.

</details>


### [50] [Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models](https://arxiv.org/abs/2506.19825)
*Johannes Rückert,Louise Bloch,Christoph M. Friedrich*

Main category: cs.AI

TL;DR: 本文研究了使用大型视觉语言模型（VLMs）分析图表以检测数据可视化问题的可行性。结果显示，VLMs在许多方面表现良好，但对图像质量和刻度线/标签的检测效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究人员通常不了解或不遵守数据可视化的原则和指南，导致因提供不准确或不完整的信息而产生误导。因此，需要一种方法来检测图表中的潜在问题。

Method: 使用大型视觉语言模型（VLMs）分析图表，以识别与选定的数据可视化原则和指南相关的问题。比较了五种开源VLMs和五种提示策略，使用从选定的数据可视化指南中得出的一组问题来评估其适用性。

Result: 所使用的VLMs在准确分析图表类型（F1分数82.49%）、3D效果（F1分数98.55%）、坐标轴标签（F1分数76.74%）、线条（RMSE 1.16）、颜色（RMSE 1.60）和图例（F1分数96.64%，RMSE 0.70）方面表现良好，但在图像质量和刻度线/标签方面无法可靠地提供反馈（F1分数0.74%和46.13%）。Qwen2.5VL在使用的VLMs中表现最好，总结性提示策略在大多数实验问题中表现最佳。

Conclusion: 研究表明，VLMs 可以用于自动识别图表中的一些潜在问题，如缺少坐标轴标签、缺少图例和不必要的3D效果。本文提出的方法可以扩展到数据可视化的其他方面。

Abstract: Diagrams are widely used to visualize data in publications. The research
field of data visualization deals with defining principles and guidelines for
the creation and use of these diagrams, which are often not known or adhered to
by researchers, leading to misinformation caused by providing inaccurate or
incomplete information.
  In this work, large Vision Language Models (VLMs) are used to analyze
diagrams in order to identify potential problems in regards to selected data
visualization principles and guidelines. To determine the suitability of VLMs
for these tasks, five open source VLMs and five prompting strategies are
compared using a set of questions derived from selected data visualization
guidelines.
  The results show that the employed VLMs work well to accurately analyze
diagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels
(F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score
96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the
image quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among
the employed VLMs, Qwen2.5VL performs best, and the summarizing prompting
strategy performs best for most of the experimental questions.
  It is shown that VLMs can be used to automatically identify a number of
potential issues in diagrams, such as missing axes labels, missing legends, and
unnecessary 3D effects. The approach laid out in this work can be extended for
further aspects of data visualization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [51] [TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to Speech Systems](https://arxiv.org/abs/2506.19441)
*Christoph Minixhofer,Ondrej Klejch,Peter Bell*

Main category: cs.SD

TL;DR: 本文介绍了TTSDS2，这是一种改进的文本到语音评估指标，在多个领域和语言中表现出色，同时发布了用于评估合成语音的资源。


<details>
  <summary>Details</summary>
Motivation: 评估文本到语音（TTS）系统具有挑战性和资源密集型，主观指标如平均意见分数（MOS）难以在不同研究之间进行比较，而客观指标很少与主观指标进行验证。最近的TTS系统能够生成与真实语音无法区分的合成语音，这使得现有的评估方法面临挑战。

Method: 引入了TTSDS2，这是TTSDS的更强大和改进版本，并发布了用于评估合成语音的资源，包括一个包含超过11,000个主观评分的数据集、一个持续重建多语言测试数据集的管道以及一个持续更新的14种语言的TTS基准。

Result: TTSDS2在多个领域和语言中表现出色，是16个比较指标中唯一一个在所有领域和主观评分评估中与斯皮尔曼相关性超过0.50的指标。

Conclusion: TTSDS2是唯一一个在所有领域和主观评分评估中与斯皮尔曼相关性超过0.50的指标，同时我们发布了用于评估合成语音接近真实语音的资源。

Abstract: Evaluation of Text to Speech (TTS) systems is challenging and
resource-intensive. Subjective metrics such as Mean Opinion Score (MOS) are not
easily comparable between works. Objective metrics are frequently used, but
rarely validated against subjective ones. Both kinds of metrics are challenged
by recent TTS systems capable of producing synthetic speech indistinguishable
from real speech. In this work, we introduce Text to Speech Distribution Score
2 (TTSDS2), a more robust and improved version of TTSDS. Across a range of
domains and languages, it is the only one out of 16 compared metrics to
correlate with a Spearman correlation above 0.50 for every domain and
subjective score evaluated. We also release a range of resources for evaluating
synthetic speech close to real speech: A dataset with over 11,000 subjective
opinion score ratings; a pipeline for continually recreating a multilingual
test dataset to avoid data leakage; and a continually updated benchmark for TTS
in 14 languages.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [52] [LLM-Based Social Simulations Require a Boundary](https://arxiv.org/abs/2506.19806)
*Zengqing Wu,Run Peng,Takayuki Ito,Chuan Xiao*

Main category: cs.CY

TL;DR: 本文讨论了基于大型语言模型的社会模拟的局限性，并提出了如何确保其可靠性以推进社会科学理解的建议。


<details>
  <summary>Details</summary>
Motivation: 本文旨在指出LLM在社会模拟中的局限性，并提出用于确定LLM模拟是否能可靠地推进社会科学理解的启发式边界。

Method: 本文探讨了基于LLM的社会模拟的三个关键边界问题：对齐（模拟行为与现实模式匹配）、一致性（保持代理行为的一致性）和鲁棒性（在不同条件下可重复性）。

Result: 本文提出了一个实用的检查清单，以指导研究人员确定基于LLM的社会模拟的适当范围和主张。

Conclusion: 本文认为，基于大型语言模型（LLM）的社会模拟应建立明确的界限，以有意义地贡献于社会科学研究。

Abstract: This position paper argues that large language model (LLM)-based social
simulations should establish clear boundaries to meaningfully contribute to
social science research. While LLMs offer promising capabilities for modeling
human-like agents compared to traditional agent-based modeling, they face
fundamental limitations that constrain their reliability for social pattern
discovery. The core issue lies in LLMs' tendency towards an ``average persona''
that lacks sufficient behavioral heterogeneity, a critical requirement for
simulating complex social dynamics. We examine three key boundary problems:
alignment (simulated behaviors matching real-world patterns), consistency
(maintaining coherent agent behavior over time), and robustness
(reproducibility under varying conditions). We propose heuristic boundaries for
determining when LLM-based simulations can reliably advance social science
understanding. We believe that these simulations are more valuable when
focusing on (1) collective patterns rather than individual trajectories, (2)
agent behaviors aligning with real population averages despite limited
variance, and (3) proper validation methods available for testing simulation
robustness. We provide a practical checklist to guide researchers in
determining the appropriate scope and claims for LLM-based social simulations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [53] [Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation](https://arxiv.org/abs/2506.19774)
*Jun Wang,Xijuan Zeng,Chunyu Qiang,Ruilong Chen,Shiyao Wang,Le Wang,Wangjing Zhou,Pengfei Cai,Jiahui Zhao,Nan Li,Zihan Li,Yuzhe Liang,Xiaopeng Wang,Haorui Zheng,Ming Wen,Kang Yin,Yiran Wang,Nan Li,Feng Deng,Liang Dong,Chen Zhang,Di Zhang,Kun Gai*

Main category: eess.AS

TL;DR: Kling-Foley是一个大规模多模态视频到音频生成模型，通过引入多模态扩散变压器和视觉语义表示模块等技术，实现了高质量的音频生成和同步。


<details>
  <summary>Details</summary>
Motivation: 为了弥补开源基准的类型和注释不完整的问题，还开源了一个工业级基准Kling-Audio-Eval。

Method: Kling-Foley引入了多模态扩散变压器来建模视频、音频和文本模态之间的相互作用，并结合视觉语义表示模块和音频-视觉同步模块以增强对齐能力。此外，还提出了一种通用的潜在音频编解码器，并采用立体渲染方法赋予合成音频空间存在感。

Result: Kling-Foley能够精确生成与视频匹配的声音效果，并在各种场景下实现高质量的音频建模。

Conclusion: Kling-Foley在分布匹配、语义对齐、时间对齐和音频质量方面实现了公共模型中的新音频-视觉SOTA性能。

Abstract: We propose Kling-Foley, a large-scale multimodal Video-to-Audio generation
model that synthesizes high-quality audio synchronized with video content. In
Kling-Foley, we introduce multimodal diffusion transformers to model the
interactions between video, audio, and text modalities, and combine it with a
visual semantic representation module and an audio-visual synchronization
module to enhance alignment capabilities. Specifically, these modules align
video conditions with latent audio elements at the frame level, thereby
improving semantic alignment and audio-visual synchronization. Together with
text conditions, this integrated approach enables precise generation of
video-matching sound effects. In addition, we propose a universal latent audio
codec that can achieve high-quality modeling in various scenarios such as sound
effects, speech, singing, and music. We employ a stereo rendering method that
imbues synthesized audio with a spatial presence. At the same time, in order to
make up for the incomplete types and annotations of the open-source benchmark,
we also open-source an industrial-level benchmark Kling-Audio-Eval. Our
experiments show that Kling-Foley trained with the flow matching objective
achieves new audio-visual SOTA performance among public models in terms of
distribution matching, semantic alignment, temporal alignment and audio
quality.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents](https://arxiv.org/abs/2506.18959)
*Weizhi Zhang,Yangning Li,Yuanchen Bei,Junyu Luo,Guancheng Wan,Liangwei Yang,Chenxuan Xie,Yuyao Yang,Wei-Chieh Huang,Chunyu Miao,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Yankai Chen,Chunkit Chan,Peilin Zhou,Xinyang Zhang,Chenwei Zhang,Jingbo Shang,Ming Zhang,Yangqiu Song,Irwin King,Philip S. Yu*

Main category: cs.IR

TL;DR: The paper introduces Agentic Deep Research, a new paradigm in information retrieval that leverages Large Language Models to integrate reasoning, retrieval, and synthesis into a dynamic feedback loop, showing significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional keyword-based search engines are inadequate for handling complex, multi-step information needs. Large Language Models (LLMs) with reasoning and agentic capabilities offer a new approach to information retrieval.

Method: The paper introduces a new paradigm called Agentic Deep Research, which integrates autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. It also presents a test-time scaling law to formalize the impact of computational depth on reasoning and search.

Result: The paper demonstrates that Agentic Deep Research significantly outperforms existing approaches and is positioned to become the dominant paradigm for future information seeking.

Conclusion: Agentic Deep Research is poised to become the dominant paradigm for future information seeking.

Abstract: Information retrieval is a cornerstone of modern knowledge acquisition,
enabling billions of queries each day across diverse domains. However,
traditional keyword-based search engines are increasingly inadequate for
handling complex, multi-step information needs. Our position is that Large
Language Models (LLMs), endowed with reasoning and agentic capabilities, are
ushering in a new paradigm termed Agentic Deep Research. These systems
transcend conventional information search techniques by tightly integrating
autonomous reasoning, iterative retrieval, and information synthesis into a
dynamic feedback loop. We trace the evolution from static web search to
interactive, agent-based systems that plan, explore, and learn. We also
introduce a test-time scaling law to formalize the impact of computational
depth on reasoning and search. Supported by benchmark results and the rise of
open-source implementations, we demonstrate that Agentic Deep Research not only
significantly outperforms existing approaches, but is also poised to become the
dominant paradigm for future information seeking. All the related resources,
including industry products, research papers, benchmark datasets, and
open-source implementations, are collected for the community in
https://github.com/DavidZWZ/Awesome-Deep-Research.

</details>


### [55] [NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and Ranking](https://arxiv.org/abs/2506.19743)
*Shenbin Qian,Diptesh Kanojia,Samarth Agrawal,Hadeel Saadany,Swapnil Bhosale,Constantin Orasan,Zhe Wu*

Main category: cs.IR

TL;DR: 本文提出了一种名为NEAR$^2$的嵌套嵌入方法，以提高电子商务信息检索系统的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 电子商务信息检索（IR）系统难以同时实现高精度解释复杂用户查询和高效处理庞大的产品目录。双重挑战在于精确匹配用户意图与相关产品，同时管理实时搜索中大规模库存的计算需求。

Method: 我们提出了一个嵌套嵌入方法，称为NEAR$^2$，它可以在推理时实现高达12倍的嵌入大小效率，同时在训练中没有额外的成本，并且提高了各种基于编码器的Transformer模型的性能。

Result: 我们的方法在不同的损失函数下进行了验证，包括多个负样本排名损失和在线对比损失，在四个不同的测试集上进行了测试，这些测试集具有各种IR挑战，如短而隐式的查询。

Conclusion: 我们的方法在较小的嵌入维度下相比现有模型实现了更好的性能。

Abstract: E-commerce information retrieval (IR) systems struggle to simultaneously
achieve high accuracy in interpreting complex user queries and maintain
efficient processing of vast product catalogs. The dual challenge lies in
precisely matching user intent with relevant products while managing the
computational demands of real-time search across massive inventories. In this
paper, we propose a Nested Embedding Approach to product Retrieval and Ranking,
called NEAR$^2$, which can achieve up to $12$ times efficiency in embedding
size at inference time while introducing no extra cost in training and
improving performance in accuracy for various encoder-based Transformer models.
We validate our approach using different loss functions for the retrieval and
ranking task, including multiple negative ranking loss and online contrastive
loss, on four different test sets with various IR challenges such as short and
implicit queries. Our approach achieves an improved performance over a smaller
embedding dimension, compared to any existing models.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [56] [Mix-of-Language-Experts Architecture for Multilingual Programming](https://arxiv.org/abs/2506.18923)
*Yifan Zong,Yuntian Deng,Pengyu Nie*

Main category: cs.PL

TL;DR: 本文介绍了MoLE（Mix-of-Language-Experts），一种新型架构，用于在多语言编程中平衡效率和专业化。MoLE由一个基础模型、一个共享的LoRA模块和一组语言特定的LoRA模块组成。这些模块在微调过程中联合优化，实现了跨编程语言的有效知识共享和专业化。实验表明，与训练单独的语言特定LoRAs相比，MoLE在参数效率方面表现更好，同时在准确性方面优于针对所有编程语言微调的单一共享LLM。


<details>
  <summary>Details</summary>
Motivation: Supporting multilingual programming typically requires either finetuning a single LLM across all programming languages, which is cost-efficient but sacrifices language-specific specialization and performance, or finetuning separate LLMs for each programming language, which allows for specialization but is computationally expensive and storage-intensive due to the duplication of parameters.

Method: MoLE is composed of a base model, a shared LoRA (low-rank adaptation) module, and a collection of language-specific LoRA modules. These modules are jointly optimized during the finetuning process, enabling effective knowledge sharing and specialization across programming languages. During inference, MoLE automatically routes to the language-specific LoRA module corresponding to the programming language of the code token being generated.

Result: MoLE achieves greater parameter efficiency compared to training separate language-specific LoRAs, while outperforming a single shared LLM finetuned for all programming languages in terms of accuracy.

Conclusion: MoLE achieves greater parameter efficiency compared to training separate language-specific LoRAs, while outperforming a single shared LLM finetuned for all programming languages in terms of accuracy.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
aiding developers with tasks like code comprehension, generation, and
translation. Supporting multilingual programming -- i.e., coding tasks across
multiple programming languages -- typically requires either (1) finetuning a
single LLM across all programming languages, which is cost-efficient but
sacrifices language-specific specialization and performance, or (2) finetuning
separate LLMs for each programming language, which allows for specialization
but is computationally expensive and storage-intensive due to the duplication
of parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel
architecture that balances efficiency and specialization for multilingual
programming. MoLE is composed of a base model, a shared LoRA (low-rank
adaptation) module, and a collection of language-specific LoRA modules. These
modules are jointly optimized during the finetuning process, enabling effective
knowledge sharing and specialization across programming languages. During
inference, MoLE automatically routes to the language-specific LoRA module
corresponding to the programming language of the code token being generated.
Our experiments demonstrate that MoLE achieves greater parameter efficiency
compared to training separate language-specific LoRAs, while outperforming a
single shared LLM finetuned for all programming languages in terms of accuracy.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models](https://arxiv.org/abs/2506.19072)
*Yimu Wang,Mozhgan Nasr Azadani,Sean Sedwards,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: HAWAII is a framework that improves vision-language models by distilling knowledge from multiple visual experts efficiently.


<details>
  <summary>Details</summary>
Motivation: The need to improve the visual understanding ability of VLMs while reducing computational costs during training and inference.

Method: HAWAII uses knowledge distillation from multiple visual experts into a single vision encoder, employing teacher-specific LoRA adapters and a router for efficient knowledge transfer.

Result: HAWAII outperforms existing VLMs in various vision-language tasks, showing the effectiveness of its approach.

Conclusion: HAWAII demonstrates superior performance compared to popular open-source VLMs through its efficient knowledge distillation framework.

Abstract: Improving the visual understanding ability of vision-language models (VLMs)
is crucial for enhancing their performance across various tasks. While using
multiple pretrained visual experts has shown great promise, it often incurs
significant computational costs during training and inference. To address this
challenge, we propose HAWAII, a novel framework that distills knowledge from
multiple visual experts into a single vision encoder, enabling it to inherit
the complementary strengths of several experts with minimal computational
overhead. To mitigate conflicts among different teachers and switch between
different teacher-specific knowledge, instead of using a fixed set of adapters
for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation
(LoRA) adapters with a corresponding router. Each adapter is aligned with a
specific teacher, avoiding noisy guidance during distillation. To enable
efficient knowledge distillation, we propose fine-grained and coarse-grained
distillation. At the fine-grained level, token importance scores are employed
to emphasize the most informative tokens from each teacher adaptively. At the
coarse-grained level, we summarize the knowledge from multiple teachers and
transfer it to the student using a set of general-knowledge LoRA adapters with
a router. Extensive experiments on various vision-language tasks demonstrate
the superiority of HAWAII, compared to the popular open-source VLMs.

</details>


### [58] [Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System](https://arxiv.org/abs/2506.19433)
*Lixuan He,Haoyu Dong,Zhenxing Chen,Yangcheng Yu,Jie Feng,Yong Li*

Main category: cs.CV

TL;DR: Mem4Nav是一种新的视觉-语言导航系统，通过分层地图和双记忆模块提升了任务完成率和导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有的模块化管道缺乏统一的内存，而端到端的(M)LLM代理受到固定上下文窗口和隐式空间推理的限制。

Method: Mem4Nav是一种分层的空间认知长短时记忆系统，结合了稀疏八叉树和语义拓扑图，通过可逆Transformer嵌入可训练的记忆标记。

Result: Mem4Nav在Task Completion、SPD和nDTW指标上取得了显著提升，证明了其有效性。

Conclusion: Mem4Nav在多个基准测试中表现出色，证明了其在视觉-语言导航任务中的有效性。

Abstract: Vision-and-Language Navigation (VLN) in large-scale urban environments
requires embodied agents to ground linguistic instructions in complex scenes
and recall relevant experiences over extended time horizons. Prior modular
pipelines offer interpretability but lack unified memory, while end-to-end
(M)LLM agents excel at fusing vision and language yet remain constrained by
fixed context windows and implicit spatial reasoning. We introduce
\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system
that can augment any VLN backbone. Mem4Nav fuses a sparse octree for
fine-grained voxel indexing with a semantic topology graph for high-level
landmark connectivity, storing both in trainable memory tokens embedded via a
reversible Transformer. Long-term memory (LTM) compresses and retains
historical observations at both octree and graph nodes, while short-term memory
(STM) caches recent multimodal entries in relative coordinates for real-time
obstacle avoidance and local planning. At each step, STM retrieval sharply
prunes dynamic context, and, when deeper history is needed, LTM tokens are
decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and
Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based
LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13
pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW
improvement. Ablations confirm the indispensability of both the hierarchical
map and dual memory modules. Our codes are open-sourced via
https://github.com/tsinghua-fib-lab/Mem4Nav.

</details>


### [59] [Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation](https://arxiv.org/abs/2506.19665)
*Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型的CT报告生成方法，通过循环视觉特征提取和立体注意力机制进行分层特征建模，实验结果表明该方法优于现有方法并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 生成CT图像的报告是一个具有挑战性的任务，尽管与现有的医学图像报告生成研究类似，但具有其独特的特点，例如多个图像的空间编码、图像体积与文本之间的对齐等。现有的解决方案通常使用通用的2D或3D图像处理技术从CT体积中提取特征，其中他们首先压缩体积，然后将压缩的CT切片分成块进行视觉编码。这些方法没有明确考虑CT切片之间的转换，也没有有效地整合多级图像特征，特别是那些包含特定器官病变的特征，以指导CT报告生成（CTRG）。

Method: 我们提出了一种基于大语言模型（LLM）的CT报告生成方法，该方法结合了循环视觉特征提取和立体注意力机制以进行分层特征建模。具体来说，我们使用视觉Transformer对CT体积中的每个切片进行循环处理，并在不同视角上对编码的切片应用一组注意力机制，以选择性地获取重要的视觉信息并与文本特征对齐，从而更好地指导LLM进行CT报告生成。

Result: 在基准数据集M3D-Cap上的实验结果和进一步分析表明，我们的方法优于强大的基线模型，并达到了最先进的结果。

Conclusion: 实验结果和进一步分析表明，我们的方法优于强大的基线模型，并达到了最先进的结果，证明了其有效性和有效性。

Abstract: Generating reports for computed tomography (CT) images is a challenging task,
while similar to existing studies for medical image report generation, yet has
its unique characteristics, such as spatial encoding of multiple images,
alignment between image volume and texts, etc. Existing solutions typically use
general 2D or 3D image processing techniques to extract features from a CT
volume, where they firstly compress the volume and then divide the compressed
CT slices into patches for visual encoding. These approaches do not explicitly
account for the transformations among CT slices, nor do they effectively
integrate multi-level image features, particularly those containing specific
organ lesions, to instruct CT report generation (CTRG). In considering the
strong correlation among consecutive slices in CT scans, in this paper, we
propose a large language model (LLM) based CTRG method with recurrent visual
feature extraction and stereo attentions for hierarchical feature modeling.
Specifically, we use a vision Transformer to recurrently process each slice in
a CT volume, and employ a set of attentions over the encoded slices from
different perspectives to selectively obtain important visual information and
align them with textual features, so as to better instruct an LLM for CTRG.
Experiment results and further analysis on the benchmark M3D-Cap dataset show
that our method outperforms strong baseline models and achieves
state-of-the-art results, demonstrating its validity and effectiveness.

</details>


### [60] [ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing](https://arxiv.org/abs/2506.19848)
*Long Xing,Qidong Huang,Xiaoyi Dong,Pan Zhang,Yuhang Zang,Yuhang Cao,Jinsong Li,Shuangrui Ding,Weiming Zhang,Nenghai Yu,Jiaqi Wang,Feng Wu,Dahua Lin*

Main category: cs.CV

TL;DR: ScaleCap is a scalable image captioning strategy that addresses biases in LVLMs by using heuristic question answering and contrastive sentence rating, resulting in more accurate and informative captions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inherent biases of LVLMs in image captioning, such as multimodal bias leading to imbalanced descriptive granularity and linguistic bias causing hallucinated descriptions of non-existent objects.

Method: ScaleCap introduces a scalable debiased captioning strategy that uses heuristic question answering and contrastive sentence rating to enrich and calibrate captions with increased inference budget.

Result: Extensive experiments show that ScaleCap improves performance across 11 benchmarks. Annotating 450K images with ScaleCap leads to consistent performance gains. Additionally, ScaleCap showcases rich and faithful captions in two additional tasks.

Conclusion: ScaleCap demonstrates effectiveness in generating accurate, balanced, and informative image captions. It also shows superior richness and fidelity in generated captions through additional tasks.

Abstract: This paper presents ScaleCap, an inference-time scalable image captioning
strategy that generates comprehensive and detailed image captions. The key
challenges of high-quality image captioning lie in the inherent biases of
LVLMs: multimodal bias resulting in imbalanced descriptive granularity,
offering detailed accounts of some elements while merely skimming over others;
linguistic bias leading to hallucinated descriptions of non-existent objects.
To address these issues, we propose a scalable debiased captioning strategy,
which continuously enriches and calibrates the caption with increased inference
budget. Specifically, we propose two novel components: heuristic question
answering and contrastive sentence rating. The former generates
content-specific questions based on the image and answers them to progressively
inject relevant information into the caption. The latter employs sentence-level
offline contrastive decoding to effectively identify and eliminate
hallucinations caused by linguistic biases. With increased inference cost, more
heuristic questions are raised by ScaleCap to progressively capture additional
visual details, generating captions that are more accurate, balanced, and
informative. Extensive modality alignment experiments demonstrate the
effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them
for LVLM pretraining leads to consistent performance gains across 11 widely
used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity
of generated captions with two additional tasks: replacing images with captions
in VQA task, and reconstructing images from captions to assess semantic
coverage. Code is available at https://github.com/Cooperx521/ScaleCap.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models](https://arxiv.org/abs/2506.18945)
*Zihan Wang,Rui Pan,Jiarui Yao,Robert Csordas,Linjie Li,Lu Yin,Jiajun Wu,Tong Zhang,Manling Li,Shiwei Liu*

Main category: cs.LG

TL;DR: CoE是一种新的Mixture-of-Experts (MoE)架构，它通过引入顺序专家通信和动态专家选择，在固定计算量下提高了模型性能，并提供了新的扩展轴：通过专家迭代的深度。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型中的专家在并行中独立运行，而CoE通过迭代方式处理标记，允许标记在每次迭代中重新评估和选择不同的专家，从而提高模型的灵活性和表现力。

Method: CoE是一种新的Mixture-of-Experts (MoE)架构，它在每一层中引入了顺序专家通信。与传统MoE模型不同，CoE在每一层中按顺序处理标记，而不是并行处理。CoE在每一层的每个迭代步骤中使用专用路由器，以支持动态专家选择。

Result: CoE在数学推理任务上表现出色，验证损失从1.20降低到1.12。此外，使用2倍迭代可以达到3倍专家选择（宽度）的性能，同时减少内存使用量17.6-42%。

Conclusion: CoE的结论是，它通过引入迭代残差结构和增强的专家专业化，解锁了更丰富的表示能力，并在固定计算量下提高了性能。此外，CoE提供了一种新的扩展轴：通过专家迭代的深度，这补充了传统的宽度/深度扩展。

Abstract: We propose Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE)
architecture that introduces sequential expert communication within each layer.
Unlike traditional MoE models, where experts operate independently in parallel,
CoE processes tokens iteratively across a chain of experts inside a layer. To
support dynamic expert selection across iterations, CoE employs a dedicated
router at each iteration step within a layer. This design allows tokens to
re-evaluate and select different experts during each iteration, rather than
being statically assigned. As a result, CoE introduces a flexible routing
mechanism that increases the diversity of expert combinations and enriches the
model's representational capacity. CoE demonstrates improved performance under
fixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to
1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling
axis: depth through expert iteration, which complements conventional
width/depth scaling. For example, using 2x iterations matches the performance
of 3x expert selections (in width), while reducing memory usage by 17.6-42%
relative to other scaling strategies. Our analysis reveals that CoE's benefits
stem from its iterative residual structure and enhanced expert specialization
empowered by iterative routing, which together unlock more expressive
representations. Code is available at https://github.com/ZihanWang314/coe.

</details>


### [62] [LLMs on a Budget? Say HOLA](https://arxiv.org/abs/2506.18952)
*Zohaib Hasan Siddiqui,Jiechao Gao,Ebad Shabbir,Mohammad Anas Azeez,Rafiq Ali,Gautam Siddharth Kashyap,Usman Naseem*

Main category: cs.LG

TL;DR: HOLA是一个端到端的优化框架，通过分层推测解码、自适应检索增强生成和结构化剪枝与量化结合，提高了大型语言模型在边缘设备上的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在边缘设备上的计算和内存需求高，限制了实时应用的发展，因此需要一种更有效的优化框架。

Method: HOLA框架结合了分层推测解码（HSD）、AdaComp-RAG和LoBi技术，以提高推理速度并减少资源消耗。

Result: HOLA框架在GSM8K和ARC数据集上分别取得了17.6%和10.5%的改进，并且在Jetson Nano等边缘设备上减少了延迟和内存使用。

Conclusion: HOLA框架在边缘设备上实现了高效的大型语言模型部署，证明了其可扩展性和生产就绪性。

Abstract: Running Large Language Models (LLMs) on edge devices is constrained by high
compute and memory demands posing a barrier for real-time applications in
sectors like healthcare, education, and embedded systems. Current solutions
such as quantization, pruning, and retrieval-augmented generation (RAG) offer
only partial optimizations and often compromise on speed or accuracy. We
introduce HOLA, an end-to-end optimization framework for efficient LLM
deployment. Internally, it leverages Hierarchical Speculative Decoding (HSD)
for faster inference without quality loss. Externally, AdaComp-RAG adjusts
retrieval complexity based on context needs. Together with LoBi, which blends
structured pruning (LoRA) and quantization, HOLA delivers significant gains:
17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge
devices like Jetson Nano--proving both scalable and production-ready.

</details>


### [63] [Thought Anchors: Which LLM Reasoning Steps Matter?](https://arxiv.org/abs/2506.19143)
*Paul C. Bogdan,Uzay Macar,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 该研究提出了一种在句子层面分析大型语言模型推理过程的方法，并通过三种互补的归因方法验证了思维锚点的存在，展示了它们在多步骤推理中的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多领域取得了最先进的性能，但其长格式的思维链推理导致了可解释性挑战，因为每个生成的标记都依赖于前面的所有标记，使得计算更难分解。因此，需要一种新的方法来理解这些模型的推理过程。

Method: 该研究提出了三种互补的归因方法：(1) 一种黑盒方法，通过比较100次回放中生成该句子或意义不同的句子时的最终答案，测量每个句子的反事实重要性；(2) 一种白盒方法，通过聚合句子对之间的注意力模式，识别出接收来自所有未来句子的“接收者”注意力头的“广播”句子；(3) 一种因果归因方法，通过抑制对一个句子的注意力并测量其对后续句子标记的影响，来衡量句子之间的逻辑联系。

Result: 每种方法都提供了思维锚点存在的证据，这些思维锚点在推理过程中具有不成比例的重要性，并且对后续推理过程有显著影响。这些思维锚点通常是计划或回溯的句子。

Conclusion: 该研究通过三种互补的归因方法，证明了在句子层面分析推理过程的潜力，并展示了思维锚点在多步骤推理中的重要性。

Abstract: Reasoning large language models have recently achieved state-of-the-art
performance in many fields. However, their long-form chain-of-thought reasoning
creates interpretability challenges as each generated token depends on all
previous ones, making the computation harder to decompose. We argue that
analyzing reasoning traces at the sentence level is a promising approach to
understanding reasoning processes. We present three complementary attribution
methods: (1) a black-box method measuring each sentence's counterfactual
importance by comparing final answers across 100 rollouts conditioned on the
model generating that sentence or one with a different meaning; (2) a white-box
method of aggregating attention patterns between pairs of sentences, which
identified ``broadcasting'' sentences that receive disproportionate attention
from all future sentences via ``receiver'' attention heads; (3) a causal
attribution method measuring logical connections between sentences by
suppressing attention toward one sentence and measuring the effect on each
future sentence's tokens. Each method provides evidence for the existence of
thought anchors, reasoning steps that have outsized importance and that
disproportionately influence the subsequent reasoning process. These thought
anchors are typically planning or backtracking sentences. We provide an
open-source tool (www.thought-anchors.com) for visualizing the outputs of our
methods, and present a case study showing converging patterns across methods
that map how a model performs multi-step reasoning. The consistency across
methods demonstrates the potential of sentence-level analysis for a deeper
understanding of reasoning models.

</details>


### [64] [In-Context Occam's Razor: How Transformers Prefer Simpler Hypotheses on the Fly](https://arxiv.org/abs/2506.19351)
*Puneesh Deora,Bhavya Vasudeva,Tina Behnia,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 本文研究了 transformers 在不同复杂度任务中的表现，发现它们能有效应用类似贝叶斯奥卡姆剃刀的归纳偏见，这可能成为训练在多样化任务分布上的 transformers 的固有特性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究通常集中在固定复杂度环境中的 ICL，但实际语言模型会遇到各种复杂度水平的任务。因此，本文旨在探讨 transformers 如何在不同复杂度的任务中进行适应和选择。

Method: 本文通过设计基于马尔可夫链和线性回归的受控测试平台，研究了 transformers 如何在层次化任务结构中导航。同时，作者还通过理论分析，利用贝叶斯框架解释了这一行为，并对模型大小、训练混合分布、推理上下文长度和架构进行了消融实验。

Result: 实验结果表明，transformers 不仅能够识别每个任务的适当复杂度级别，还能准确推断出相应的参数，即使上下文示例与多个复杂度假设兼容。当面对由简单过程生成的数据时，transformers 会优先选择最简单的充分解释。

Conclusion: 本文通过实验和理论分析表明，transformers 在处理不同复杂度的任务时，能够有效地应用一种类似于贝叶斯奥卡姆剃刀的归纳偏见，即在模型拟合与复杂度惩罚之间取得平衡。此外，这种行为可能已成为训练在多样化任务分布上的 transformers 的固有特性。

Abstract: In-context learning (ICL) enables transformers to adapt to new tasks through
contextual examples without parameter updates. While existing research has
typically studied ICL in fixed-complexity environments, practical language
models encounter tasks spanning diverse complexity levels. This paper
investigates how transformers navigate hierarchical task structures where
higher-complexity categories can perfectly represent any pattern generated by
simpler ones. We design well-controlled testbeds based on Markov chains and
linear regression that reveal transformers not only identify the appropriate
complexity level for each task but also accurately infer the corresponding
parameters--even when the in-context examples are compatible with multiple
complexity hypotheses. Notably, when presented with data generated by simpler
processes, transformers consistently favor the least complex sufficient
explanation. We theoretically explain this behavior through a Bayesian
framework, demonstrating that transformers effectively implement an in-context
Bayesian Occam's razor by balancing model fit against complexity penalties. We
further ablate on the roles of model size, training mixture distribution,
inference context length, and architecture. Finally, we validate this Occam's
razor-like inductive bias on a pretrained GPT-4 model with Boolean-function
tasks as case study, suggesting it may be inherent to transformers trained on
diverse task distributions.

</details>


### [65] [Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models](https://arxiv.org/abs/2506.19697)
*Jungwoo Park,Taewhoo Lee,Chanwoong Yoon,Hyeon Hwang,Jaewoo Kang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Outlier-Safe Pre-Training (OSP)的方法，以主动防止极端激活异常值的形成，从而提高大型语言模型（LLM）的量化性能。通过三个关键创新，包括Muon优化器、Single-Scale RMSNorm和可学习的嵌入投影，该方法在14亿参数模型上进行了验证，并在4位量化下取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 极端激活异常值严重降低量化性能，阻碍了高效设备端部署。虽然通道级操作和自适应梯度缩放是已知的原因，但实际的缓解措施仍然具有挑战性。

Method: 我们引入了Outlier-Safe Pre-Training (OSP)，这是一种实用指南，旨在主动防止异常值的形成，而不是依赖于事后的缓解措施。OSP结合了三个关键创新：(1) Muon优化器，消除了特权基础而保持了训练效率；(2) Single-Scale RMSNorm，防止通道级放大；以及(3) 可学习的嵌入投影，重新分配来自嵌入矩阵的激活幅度。

Result: 我们在1万亿个标记上训练了一个14亿参数的模型，这是第一个没有此类异常值的生产规模LLM。在激进的4位量化下，我们的OSP模型在10个基准测试中平均得分为35.7（相比之下，Adam训练的模型得分为26.5），仅增加了2%的训练开销。值得注意的是，OSP模型表现出接近零的超额峰度（0.04），而标准模型则表现出极端值（1818.56），从根本上改变了LLM量化行为。

Conclusion: 我们的工作表明，异常值并非LLM的固有属性，而是训练策略的结果，为更高效的LLM部署铺平了道路。

Abstract: Extreme activation outliers in Large Language Models (LLMs) critically
degrade quantization performance, hindering efficient on-device deployment.
While channel-wise operations and adaptive gradient scaling are recognized
causes, practical mitigation remains challenging. We introduce Outlier-Safe
Pre-Training (OSP), a practical guideline that proactively prevents outlier
formation rather than relying on post-hoc mitigation. OSP combines three key
innovations: (1) the Muon optimizer, eliminating privileged bases while
maintaining training efficiency; (2) Single-Scale RMSNorm, preventing
channel-wise amplification; and (3) a learnable embedding projection,
redistributing activation magnitudes originating from embedding matrices. We
validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is
the first production-scale LLM trained without such outliers. Under aggressive
4-bit quantization, our OSP model achieves a 35.7 average score across 10
benchmarks (compared to 26.5 for an Adam-trained model), with only a 2%
training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis
(0.04) compared to extreme values (1818.56) in standard models, fundamentally
altering LLM quantization behavior. Our work demonstrates that outliers are not
inherent to LLMs but are consequences of training strategies, paving the way
for more efficient LLM deployment. The source code and pretrained checkpoints
are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.

</details>


### [66] [Scaling Speculative Decoding with Lookahead Reasoning](https://arxiv.org/abs/2506.19830)
*Yichao Fu,Rui Ge,Zelei Shao,Zhijie Deng,Hao Zhang*

Main category: cs.LG

TL;DR: Lookahead Reasoning 通过引入步骤级别的并行性，提高了 Token-level speculative decoding (SD) 的加速效果，并在多个基准测试中展示了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 Token-level speculative decoding (SD) 在生成长链推理时存在算法瓶颈，因为随着猜测的 token 数量增加，猜测正确的概率呈指数下降，导致加速效果有限。

Method: Lookahead Reasoning 通过引入一个步骤级别的并行层，利用轻量级的草稿模型提出未来步骤，并通过验证器保持语义正确的步骤，同时让目标模型批量扩展每个提议。

Result: Lookahead Reasoning 在 GSM8K、AIME 等基准测试中将 SD 的加速效果从 1.4x 提高到 2.1x，并且其加速效果随着 GPU 吞吐量的增加而更好地扩展。

Conclusion: Lookahead Reasoning 提高了 Token-level speculative decoding (SD) 的峰值加速效果，并在多个基准测试中展示了更好的加速效果和可扩展性。

Abstract: Reasoning models excel by generating long chain-of-thoughts, but decoding the
resulting thousands of tokens is slow. Token-level speculative decoding (SD)
helps, but its benefit is capped, because the chance that an entire
$\gamma$-token guess is correct falls exponentially as $\gamma$ grows. This
means allocating more compute for longer token drafts faces an algorithmic
ceiling -- making the speedup modest and hardware-agnostic. We raise this
ceiling with Lookahead Reasoning, which exploits a second, step-level layer of
parallelism. Our key insight is that reasoning models generate step-by-step,
and each step needs only to be semantically correct, not exact token matching.
In Lookahead Reasoning, a lightweight draft model proposes several future
steps; the target model expands each proposal in one batched pass, and a
verifier keeps semantically correct steps while letting the target regenerate
any that fail. Token-level SD still operates within each reasoning step, so the
two layers of parallelism multiply. We show Lookahead Reasoning lifts the peak
speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other
benchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x
while preserving answer quality, and its speedup scales better with additional
GPU throughput. Our code is available at
https://github.com/hao-ai-lab/LookaheadReasoning

</details>


### [67] [Orthogonal Finetuning Made Scalable](https://arxiv.org/abs/2506.19847)
*Zeju Qiu,Weiyang Liu,Adrian Weller,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: OFTv2 improves the efficiency of orthogonal finetuning by using matrix-vector multiplications and a new parameterization, achieving faster training and lower memory usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The high runtime and memory demands of OFT limit its practical deployment, so the authors aim to improve its efficiency while maintaining performance.

Method: OFTv2 is an input-centric reformulation of orthogonal finetuning (OFT) that uses matrix-vector multiplications instead of matrix-matrix multiplications, reducing computational cost to quadratic. It also introduces the Cayley-Neumann parameterization, which approximates matrix inversion via a truncated Neumann series.

Result: OFTv2 achieves up to 10x faster training and 3x lower GPU memory usage compared to OFT. It also outperforms QLoRA in training stability, efficiency, and memory usage when finetuning quantized foundation models.

Conclusion: OFTv2 achieves up to 10x faster training and 3x lower GPU memory usage without compromising performance, and it outperforms QLoRA in training stability, efficiency, and memory usage when finetuning quantized foundation models.

Abstract: Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation
while preventing catastrophic forgetting, but its high runtime and memory
demands limit practical deployment. We identify the core computational
bottleneck in OFT as its weight-centric implementation, which relies on costly
matrix-matrix multiplications with cubic complexity. To overcome this, we
propose OFTv2, an input-centric reformulation that instead uses matrix-vector
multiplications (i.e., matrix-free computation), reducing the computational
cost to quadratic. We further introduce the Cayley-Neumann parameterization, an
efficient orthogonal parameterization that approximates the matrix inversion in
Cayley transform via a truncated Neumann series. These modifications allow
OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage
without compromising performance. In addition, we extend OFTv2 to support
finetuning quantized foundation models and show that it outperforms the popular
QLoRA in training stability, efficiency, and memory usage.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [68] [Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects](https://arxiv.org/abs/2506.19579)
*Federico Tavella,Kathryn Mearns,Angelo Cangelosi*

Main category: cs.RO

TL;DR: 本研究比较了不同captioning模型在机器人场景理解中的性能，并发现VLMs在识别常见物体方面有效，但在处理新表示时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 机器人场景理解越来越多地依赖于视觉-语言模型（VLMs）来生成环境的自然语言描述。然而，需要进一步研究如何在机器人设置中有效地使用这些模型。

Method: 我们比较了各种captioning模型，如BLIP和VLMs，并评估了它们在生成场景描述方面的性能。

Result: 结果表明，VLMs可以在需要识别常见物体的机器人环境中使用，但无法推广到新的表示形式。

Conclusion: 我们的研究结果提供了将基础模型部署到现实世界中的具身代理的实用见解。

Abstract: Robotic scene understanding increasingly relies on vision-language models
(VLMs) to generate natural language descriptions of the environment. In this
work, we present a comparative study of captioning strategies for tabletop
scenes captured by a robotic arm equipped with an RGB camera. The robot
collects images of objects from multiple viewpoints, and we evaluate several
models that generate scene descriptions. We compare the performance of various
captioning models, like BLIP and VLMs. Our experiments examine the trade-offs
between single-view and multi-view captioning, and difference between
recognising real-world and 3D printed objects. We quantitatively evaluate
object identification accuracy, completeness, and naturalness of the generated
captions. Results show that VLMs can be used in robotic settings where common
objects need to be recognised, but fail to generalise to novel representations.
Our findings provide practical insights into deploying foundation models for
embodied agents in real-world settings.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 78]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-François Godbout,Reihaneh Rabbany,Kellin Pelrine*

Main category: cs.CL

TL;DR: This paper introduces Veracity, an open-source AI system designed to combat misinformation through transparent and accessible fact-checking, leveraging LLMs and web retrieval agents to provide intuitive explanations and foster media literacy.


<details>
  <summary>Details</summary>
Motivation: The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI.

Method: Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations.

Result: Veracity can detect misinformation and explain its reasoning, offering multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications.

Conclusion: Veracity is an open-source AI system that helps individuals combat misinformation through transparent and accessible fact-checking, fostering media literacy and promoting a more informed society.

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [2] [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
*Riccardo Di Sipio*

Main category: cs.CL

TL;DR: 本文探讨了LLM优化中的信息几何方法，强调了考虑曲率的重要性，并提出了量子类比的可能性。


<details>
  <summary>Details</summary>
Motivation: LLM的优化在高维参数空间中进行，这些空间具有非欧几里得结构。传统的优化方法可能不适用于这种复杂的结构，因此需要更深入的理解。

Method: 本文使用信息几何框架，通过费舍尔信息度量来描述LLM的参数空间，并探讨了自然梯度下降方法。此外，还推测了基于Fubini-Study度量和量子费舍尔信息的量子类比。

Result: 本文表明，信息几何框架可以更清晰地解释LLM训练中的现象，如尖锐最小值、泛化能力和观察到的缩放定律。同时，量子类比为未来的优化提供了新的思路。

Conclusion: 本文认为，考虑曲率的方法可以加深我们对LLM训练的理解，并推测基于Fubini-Study度量和量子费舍尔信息的量子类比可能在量子增强系统中实现高效的优化。

Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional
parameter spaces with non-Euclidean structure. Information geometry frames this
landscape using the Fisher information metric, enabling more principled
learning via natural gradient descent. Though often impractical, this geometric
lens clarifies phenomena such as sharp minima, generalization, and observed
scaling laws. We argue that curvature-aware approaches deepen our understanding
of LLM training. Finally, we speculate on quantum analogies based on the
Fubini-Study metric and Quantum Fisher Information, hinting at efficient
optimization in quantum-enhanced systems.

</details>


### [3] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
*Zijian Zhou,Ao Qu,Zhaoxuan Wu,Sunghwan Kim,Alok Prakash,Daniela Rus,Jinhua Zhao,Bryan Kian Hsiang Low,Paul Pu Liang*

Main category: cs.CL

TL;DR: 本文介绍了MEM1，这是一种端到端的强化学习框架，使代理能够在长多轮任务中以恒定的内存运行。实验表明，MEM1在性能和内存使用方面优于现有的模型。


<details>
  <summary>Details</summary>
Motivation: 现代语言代理必须在长视野、多轮互动中操作，其中他们检索外部信息，适应观察，并回答相互依赖的问题。然而，大多数LLM系统依赖于全上下文提示，无论相关性如何都附加所有过去的回合。这导致了无限制的内存增长、计算成本增加以及对分布外输入长度的推理性能下降。

Method: 我们引入了MEM1，这是一个端到端的强化学习框架，使代理能够在长多轮任务中以恒定的内存运行。在每一轮中，MEM1更新一个紧凑的共享内部状态，该状态共同支持记忆整合和推理。这种状态将先前的记忆与来自环境的新观察结果结合起来，同时战略性地丢弃无关或冗余的信息。为了支持在更现实和组合设置中的训练，我们提出了一种简单但有效且可扩展的方法，通过组合现有数据集来构建多轮环境。

Result: 在三个领域（包括内部检索QA、开放域网络QA和多轮网络购物）的实验表明，MEM1-7B在16个目标的多跳QA任务上相比Qwen2.5-14B-Instruct提高了3.5倍的性能，并将内存使用量减少了3.7倍，并且超越了训练范围进行泛化。

Conclusion: 我们的结果展示了以推理为导向的记忆整合作为训练长视野交互代理的可扩展替代方案的前景，其中效率和性能都得到了优化。

Abstract: Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.

</details>


### [4] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
*Glenn Matlin,Mika Okamoto,Huzaifa Pardawala,Yang Yang,Sudheer Chava*

Main category: cs.CL

TL;DR: 本文提出了一个全面的金融语言模型评估基准测试套件FLaME，并对多个语言模型在金融自然语言处理任务中的表现进行了研究。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架存在重大方法论上的差距，导致对语言模型在常见金融自然语言处理任务中的性能产生了错误的下限认知。因此，需要一个更全面的评估方法来展示语言模型的潜力。

Method: 本文提出了一个名为FLaME的全面基准测试套件，用于评估语言模型在金融自然语言处理任务中的表现。此外，还对23个基础语言模型进行了实证研究。

Result: 本文提出了FLaME基准测试套件，并对23个基础语言模型在20个核心金融自然语言处理任务上的表现进行了实证研究。

Conclusion: 本文展示了语言模型在金融自然语言处理任务中的潜力，并提出了一个全面的基准测试套件FLaME。

Abstract: Language Models (LMs) have demonstrated impressive capabilities with core
Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly
specialized knowledge-intensive tasks in finance remains difficult to assess
due to major gaps in the methodologies of existing evaluation frameworks, which
have caused an erroneous belief in a far lower bound of LMs' performance on
common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for
these FinNLP tasks, we present the first holistic benchmarking suite for
Financial Language Model Evaluation (FLaME). We are the first research paper to
comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical
study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source
our framework software along with all data and results.

</details>


### [5] [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
*Yifan Hu,Frank Liang,Dachuan Zhao,Jonathan Geuter,Varshini Reddy,Craig W. Schmidt,Chris Tanner*

Main category: cs.CL

TL;DR: 本文提出两种基于熵的预分词策略，以改进BPE在中文等未分割语言中的分词效果，并取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 由于BPE的频率驱动合并操作对语言边界是无意识的，因此将其应用于未分割的语言（如中文）存在重大挑战。

Method: 我们提出了两种基于熵的预分词策略，使用点互信息和左右熵来识别连贯的字符段，以及利用从预训练的GPT-2模型中获得的预测熵来检测边界不确定性。

Result: 我们在PKU数据集的一个子集上评估了这两种方法，并证明了与标准BPE相比，分割精度、召回率和F1分数有显著提高。

Conclusion: 我们的结果表明，基于熵的预分词不仅提高了与黄金标准语言单位的一致性，还为在低资源和多语言环境中提高分词质量提供了有希望的方向。

Abstract: Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization
method in modern language models due to its simplicity and strong empirical
performance across downstream tasks. However, applying BPE to unsegmented
languages such as Chinese presents significant challenges, as its
frequency-driven merge operation is agnostic to linguistic boundaries. To
address this, we propose two entropy-informed pre-tokenization strategies that
guide BPE segmentation using unsupervised information-theoretic cues. The first
approach uses pointwise mutual information and left/right entropy to identify
coherent character spans, while the second leverages predictive entropy derived
from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both
methods on a subset of the PKU dataset and demonstrate substantial improvements
in segmentation precision, recall, and F1 score compared to standard BPE. Our
results suggest that entropy-guided pre-tokenization not only enhances
alignment with gold-standard linguistic units but also offers a promising
direction for improving tokenization quality in low-resource and multilingual
settings.

</details>


### [6] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
*Sam Silver,Jimin Sun,Ivan Zhang,Sara Hooker,Eddie Kim*

Main category: cs.CL

TL;DR: This paper explores the self-correction capabilities of Large Language Models (LLMs) by conducting experiments on their ability to correct synthetic perturbations in their Chain of Thought (CoT) reasoning. The findings suggest that LLMs have stronger intrinsic self-correction abilities than previously thought.


<details>
  <summary>Details</summary>
Motivation: To better understand self-correction capabilities of recent models and address the brittleness of LLMs to minor variations in problem description and prompting strategy.

Method: Conducting experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning.

Result: Robust single-utterance intrinsic self-correction behavior was observed across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors.

Conclusion: LLMs, including those not fine-tuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature.

Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical
reasoning capabilities, yet their performance remains brittle to minor
variations in problem description and prompting strategy. Furthermore,
reasoning is vulnerable to sampling-induced errors which autoregressive models
must primarily address using self-correction via additionally-generated tokens.
To better understand self-correction capabilities of recent models, we conduct
experiments measuring models' ability to self-correct synthetic perturbations
introduced into their Chain of Thought (CoT) reasoning. We observe robust
single-utterance intrinsic self-correction behavior across a range of
open-weight models and datasets, ranging from subtle, implicit corrections to
explicit acknowledgments and corrections of errors. Our findings suggest that
LLMs, including those not finetuned for long CoT, may possess stronger
intrinsic self-correction capabilities than commonly shown in the literature.
The presence of this ability suggests that recent "reasoning" model work
involves amplification of traits already meaningfully present in models.

</details>


### [7] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
*Mohammad Amaan Sayeed,Mohammed Talha Alam,Raza Imam,Shahab Saquib Sohail,Amir Hussain*

Main category: cs.CL

TL;DR: 本文提出了一种统一的评估流程 Tibbe-AG，用于评估基于伊斯兰医学文本的医疗问答系统。结果显示，结合检索和自我评估可以提高准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 几个世纪前的伊斯兰医学文献（如阿维森纳的《医学经典》和先知的 Tibb-e-Nabawi）包含了大量预防护理、营养和整体疗法，但对许多人来说难以获取，并且在现代 AI 系统中未被充分利用。现有的语言模型基准测试主要关注事实回忆或用户偏好，未能在大规模上验证文化基础的医疗建议。

Method: 我们提出了一个统一的评估流程 Tibbe-AG，它将 30 个精心挑选的先知医学问题与人工验证的疗法对齐，并在三种配置下比较了三个 LLM（LLaMA-3、Mistral-7B、Qwen2-7B）：直接生成、检索增强生成和科学自我批判过滤器。每个答案随后由一个作为代理法官的次级 LLM 进行评估，得出一个 3C3H 质量分数。

Result: 检索提高了事实准确性 13%，而代理提示通过更深入的机制洞察和安全考虑又提高了 10% 的改进。

Conclusion: 我们的结果表明，将古典伊斯兰文本与检索和自我评估相结合可以实现可靠且文化敏感的医学问答。

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [8] [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
*Narutatsu Ri,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 本文研究了如何通过构建测试集和使用基于语言模型的度量标准来改进观点摘要的评估和生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架依赖于传统的度量标准来衡量关键属性，如覆盖范围和忠实度，但没有验证其适用性。因此，需要改进摘要生成方法。

Method: 我们通过构建一个用于基准测试度量可靠性的测试集，并使用人类注释来验证传统度量与基于语言模型的度量之间的性能差异。此外，我们还研究了基于重新排序的方法以及使用合成生成和重新排序标记数据的偏好调优的有效性。

Result: 传统度量标准的表现不如基于语言模型的度量标准，而基于重新排序的方法和偏好调优进一步提高了性能。

Conclusion: 我们的研究旨在为观点摘要方法的可靠评估和开发做出贡献。

Abstract: Generating unbiased summaries in real-world settings such as political
perspective summarization remains a crucial application of Large Language
Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics
for measuring key attributes such as coverage and faithfulness without
verifying their applicability, and efforts to develop improved summarizers are
still nascent. We address these gaps by (1) identifying reliable metrics for
measuring perspective summary quality, and (2) investigating the efficacy of
LLM-based methods beyond zero-shot inference. Namely, we build a test set for
benchmarking metric reliability using human annotations and show that
traditional metrics underperform compared to language model-based metrics,
which prove to be strong evaluators. Using these metrics, we show that
reranking-based methods yield strong results, and preference tuning with
synthetically generated and reranking-labeled data further boosts performance.
Our findings aim to contribute to the reliable evaluation and development of
perspective summarization methods.

</details>


### [9] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
*Toan Nguyen Hai,Ha Nguyen Viet,Truong Quan Xuan,Duc Do Minh*

Main category: cs.CL

TL;DR: 本文提出了一个用于越南语文本分割和多项选择阅读理解的数据集VSMRC，并展示了多语言模型在越南语自然语言处理任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 越南语是全球第20大语言，拥有超过1亿名母语者，但缺乏关键自然语言处理任务（如文本分割和机器阅读理解）的可靠资源。为了弥补这一差距，我们提出了VSMRC数据集。

Method: 我们提出了VSMRC，这是一个越南语文本分割和多项选择阅读理解数据集。该数据集来源于越南语维基百科，包含15,942个用于文本分割的文档和16,347个通过人工质量保证生成的合成多项选择问题-答案对。

Result: 实验表明，mBERT在两个任务上均优于单语模型，在MRC测试集上的准确率为88.01%，在文本分割测试集上的F1评分为63.15%。

Conclusion: 我们的分析表明，多语言模型在越南语的自然语言处理任务中表现优异，这为其他资源匮乏的语言提供了潜在的应用前景。

Abstract: Vietnamese, the 20th most spoken language with over 102 million native
speakers, lacks robust resources for key natural language processing tasks such
as text segmentation and machine reading comprehension (MRC). To address this
gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice
Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset
includes 15,942 documents for text segmentation and 16,347 synthetic
multiple-choice question-answer pairs generated with human quality assurance,
ensuring a reliable and diverse resource. Experiments show that mBERT
consistently outperforms monolingual models on both tasks, achieving an
accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text
segmentation test set. Our analysis reveals that multilingual models excel in
NLP tasks for Vietnamese, suggesting potential applications to other
under-resourced languages. VSMRC is available at HuggingFace

</details>


### [10] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
*Markus Frohmann,Gabriel Meseguer-Brocal,Markus Schedl,Elena V. Epure*

Main category: cs.CL

TL;DR: 本文提出了一种多模态、模块化的后期融合管道，结合自动转录的歌词和音频中的语音特征，以提高AI生成音乐检测的鲁棒性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的检测器依赖于音频或歌词，但面临关键的实际限制：音频检测器无法泛化到新的或未见过的生成器，并且容易受到音频扰动的影响；基于歌词的方法需要干净格式和准确的歌词，在实践中不可用。

Method: 我们提出了一种新颖的、实践基础的方法：一种多模态、模块化的后期融合管道，结合了自动转录的演唱歌词和捕捉音频中与歌词相关信息的语音特征。

Result: 实验表明，我们的方法DE-detect优于现有的基于歌词的检测器，同时对音频扰动更具鲁棒性。

Conclusion: 我们的方法DE-detect在实际场景中为检测AI生成的音乐提供了一个有效且稳健的解决方案。

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [11] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
*Zhihan Guo,Jiele Wu,Wenqian Cui,Yifei Zhang,Minda Hu,Yufei Wang,Irwin King*

Main category: cs.CL

TL;DR: 本文介绍了ProxyReward，这是一个基于强化学习的框架，用于改进长上下文生成任务。该框架包括一个数据集和一种奖励信号计算方法，能够显著提高性能，并超越了现有的方法。


<details>
  <summary>Details</summary>
Motivation: 当前对长上下文在大型语言模型（LLMs）中的研究主要集中在长上下文的理解上，而开放性长文本生成（Open-LTG）仍然缺乏充分探索。训练长上下文生成模型需要精心策划的标准参考数据，这在信息性Open-LTG任务中通常不存在。然而，以前的方法仅使用一般评估作为奖励信号，这限制了准确性。为了弥补这一差距，我们引入了ProxyReward。

Method: 我们引入了ProxyReward，这是一个基于强化学习（RL）的框架，包括一个数据集和一种奖励信号计算方法。首先，通过简单的提示生成ProxyReward数据集，使模型能够自动创建，从而避免了大量标记数据或大量人工努力。其次，ProxyReward信号针对特定问题的信息全面性和准确性提供有针对性的评估。

Result: 实验结果表明，我们的方法ProxyReward超越了GPT-4-Turbo。当训练广泛使用的开源模型时，它可以在Open-LTG任务中显著提高性能20%，同时超过了LLM-as-a-Judge方法。

Conclusion: 我们的工作提出了有效的方法来增强大型语言模型（LLMs）解决人类提出的复杂开放性问题的能力。

Abstract: Current research on long-form context in Large Language Models (LLMs)
primarily focuses on the understanding of long-contexts, the Open-ended Long
Text Generation (Open-LTG) remains insufficiently explored. Training a
long-context generation model requires curation of gold standard reference
data, which is typically nonexistent for informative Open-LTG tasks. However,
previous methods only utilize general assessments as reward signals, which
limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative
reinforcement learning (RL) based framework, which includes a dataset and a
reward signal computation method. Firstly, ProxyReward Dataset generation is
accomplished through simple prompts that enables the model to create
automatically, obviating extensive labeled data or significant manual effort.
Secondly, ProxyReward Signal offers a targeted evaluation of information
comprehensiveness and accuracy for specific questions. The experimental results
indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can
significantly enhance performance by 20% on the Open-LTG task when training
widely used open-source models, while also surpassing the LLM-as-a-Judge
approach. Our work presents effective methods to enhance the ability of LLMs to
address complex open-ended questions posed by human.

</details>


### [12] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Main category: cs.CL

TL;DR: EvoLM 是一个模型套件，用于系统地和透明地分析语言模型（LM）在预训练、持续预训练、监督微调和强化学习中的训练动态。通过从头开始训练超过100个参数为1B和4B的LM，我们严格评估了上游（语言建模）和下游（问题解决）推理能力，包括对域内和域外泛化的考虑。关键见解包括：过度预训练和后训练的收益递减，缓解特定领域持续预训练中遗忘的重要性和实践，持续预训练在连接预训练和后训练阶段中的关键作用，以及在配置监督微调和强化学习时的各种复杂权衡。为了促进开放研究和可重复性，我们发布了所有预训练和后训练模型、所有阶段的训练数据集以及整个训练和评估流程。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型（LM）的训练已被划分为多个阶段，使得下游开发者难以评估每个阶段的设计选择的影响。因此，需要一种模型套件来系统地和透明地分析LM的训练动态，以便更好地理解各个阶段的作用和影响。

Method: EvoLM 是一个模型套件，用于系统地和透明地分析语言模型（LM）在预训练、持续预训练、监督微调和强化学习中的训练动态。通过从头开始训练超过100个参数为1B和4B的LM，我们严格评估了上游（语言建模）和下游（问题解决）推理能力，包括对域内和域外泛化的考虑。

Result: 通过从头开始训练超过100个参数为1B和4B的LM，我们严格评估了上游（语言建模）和下游（问题解决）推理能力，包括对域内和域外泛化的考虑。关键见解包括：过度预训练和后训练的收益递减，缓解特定领域持续预训练中遗忘的重要性和实践，持续预训练在连接预训练和后训练阶段中的关键作用，以及在配置监督微调和强化学习时的各种复杂权衡。

Conclusion: EvoLM 是一个模型套件，能够系统地和透明地分析语言模型（LM）在预训练、持续预训练、监督微调和强化学习中的训练动态。通过从头开始训练超过100个参数为1B和4B的LM，我们严格评估了上游（语言建模）和下游（问题解决）推理能力，包括对域内和域外泛化的考虑。关键见解突出了过度预训练和后训练的收益递减，缓解特定领域持续预训练中遗忘的重要性和实践，持续预训练在连接预训练和后训练阶段中的关键作用，以及在配置监督微调和强化学习时的各种复杂权衡。为了促进开放研究和可重复性，我们发布了所有预训练和后训练模型、所有阶段的训练数据集以及整个训练和评估流程。

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [13] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Main category: cs.CL

TL;DR: 本文提出了一种针对复杂问答任务的新型检索增强生成（RAG）框架，通过集成密集检索模块和多跳推理机制，提高了模型的准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决复杂问答任务中的多跳推理和长文档上下文理解挑战。

Method: 该框架基于LLaMA 3，集成了密集检索模块和先进的上下文融合及多跳推理机制，并采用结合检索似然和生成交叉熵的联合优化策略。

Result: 实验结果表明，所提出的系统优于现有的检索增强和生成基线。

Conclusion: 实验结果表明，所提出的系统在提供精确且具有上下文依据的答案方面有效。

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [14] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan Ö. Arık*

Main category: cs.CL

TL;DR: DynScaling 是一种改进大型语言模型性能的方法，通过集成并行-顺序采样策略和基于赌徒问题的动态预算分配框架，在不依赖外部验证器的情况下提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时间扩展方法在实际应用中受到依赖外部验证器或缺乏对现实计算约束优化的限制。

Method: 提出 DynScaling，通过集成并行-顺序采样策略和基于赌徒问题的动态预算分配框架来解决这些问题。

Result: 实验结果表明，DynScaling 在任务性能和计算成本方面都超过了现有的无验证器推理扩展基线。

Conclusion: DynScaling 有效提高了在实际资源限制下的 LLM 性能，无需外部验证器。

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [15] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种混合架构，结合了改进的DeBERTa模型和GBLS分类器，以提高网络欺凌检测的准确性和可解释性，并在多个数据集上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 由于网络通信平台的普及，网络欺凌问题日益严重，需要更有效的检测方法。本文旨在提出一种混合架构，以提高网络欺凌检测的准确性和可解释性。

Method: 该方法结合了改进的DeBERTa模型和门控广义学习系统（GBLS）分类器，利用了变压器模型的上下文理解能力和广义学习系统的模式识别优势。

Result: 所提出的ModifiedDeBERTa + GBLS模型在四个英文数据集上取得了良好的性能，包括HateXplain、SOSNet、Mendeley-I和Mendeley-II。此外，该框架还包含了全面的可解释性机制。

Conclusion: 该框架通过结合变压器模型和广义学习系统的优点，在多个基准数据集上表现出色，并提供了全面的可解释性机制。然而，它在检测隐含偏见和讽刺内容方面仍存在挑战，为未来的研究提供了方向。

Abstract: The proliferation of online communication platforms has created unprecedented
opportunities for global connectivity while simultaneously enabling harmful
behaviors such as cyberbullying, which affects approximately 54.4\% of
teenagers according to recent research. This paper presents a hybrid
architecture that combines the contextual understanding capabilities of
transformer-based models with the pattern recognition strengths of broad
learning systems for effective cyberbullying detection. This approach
integrates a modified DeBERTa model augmented with Squeeze-and-Excitation
blocks and sentiment analysis capabilities with a Gated Broad Learning System
(GBLS) classifier, creating a synergistic framework that outperforms existing
approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +
GBLS model achieved good performance on four English datasets: 79.3\% accuracy
on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and
94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework
incorporates comprehensive explainability mechanisms including token-level
attribution analysis, LIME-based local interpretations, and confidence
calibration, addressing critical transparency requirements in automated content
moderation. Ablation studies confirm the meaningful contribution of each
architectural component, while failure case analysis reveals specific
challenges in detecting implicit bias and sarcastic content, providing valuable
insights for future improvements in cyberbullying detection systems.

</details>


### [16] [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
*Andy Yang,Michaël Cadilhac,David Chiang*

Main category: cs.CL

TL;DR: 该论文通过理论证明和实证研究，展示了更深的transformer模型在特定子类中更具表达能力，并验证了理论预测的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究transformer模型深度增加是否能带来更强大的能力，并尝试从理论上解释这一现象。

Method: 通过理论证明和实证研究，分析了transformer模型深度与其表达能力之间的关系。

Result: 证明了更深的C-RASP程序比浅层的更具表达能力，从而推断出更深的transformer模型在特定子类中更具表达能力。此外，还提供了实证证据支持理论预测。

Conclusion: 更深的transformer模型在特定子类中更具表达能力，这表明更深的transformer模型在某些任务上表现更好。

Abstract: It has been observed that transformers with greater depth (that is, more
layers) have more capabilities, but can we establish formally which
capabilities are gained with greater depth? We answer this question with a
theoretical proof followed by an empirical study. First, we consider
transformers that round to fixed precision except inside attention. We show
that this subclass of transformers is expressively equivalent to the
programming language C-RASP and this equivalence preserves depth. Second, we
prove that deeper C-RASP programs are more expressive than shallower C-RASP
programs, implying that deeper transformers are more expressive than shallower
transformers (within the subclass mentioned above). These results are
established by studying a form of temporal logic with counting operators, which
was shown equivalent to C-RASP in previous work. Finally, we provide empirical
evidence that our theory predicts the depth required for transformers without
positional encodings to length-generalize on a family of sequential dependency
tasks.

</details>


### [17] [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
*Duc Hieu Ho,Chenglin Fan*

Main category: cs.CL

TL;DR: 本文提出了一种无需额外训练的提示策略，通过自评和优化步骤提高大型语言模型输出的诚实性和有用性。实验结果显示该方法在多个模型上均取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种自然语言任务中表现出色，但生成始终诚实和有帮助的输出仍然是一个开放性挑战。因此，本文旨在探索一种无需额外训练的方法，以提高模型输出的质量和可信度。

Method: 该研究通过两个互补方向解决大型语言模型输出不一致的问题：首先对十种广泛使用的大型语言模型进行全面基准评估；其次提出了一种新的提示策略，即自评引导的好奇心优化提示。该策略通过引入两个轻量级的上下文步骤（自评步骤和优化步骤）来扩展好奇心驱动的提示策略。

Result: 实验结果表明，在使用GPT-4o作为诚实性和有用性评判者的框架H²上，所有模型都表现出一致的改进。该方法减少了低质量响应的数量，增加了高质量响应，并在所评估模型的H²分数上实现了1.4%到4.3%的相对提升。

Conclusion: 该研究展示了结构化自我修正作为一种可扩展且无需训练的策略，可以有效提高大型语言模型输出的可信度。

Abstract: Large language models (LLMs) have demonstrated robust capabilities across
various natural language tasks. However, producing outputs that are
consistently honest and helpful remains an open challenge. To overcome this
challenge, this paper tackles the problem through two complementary directions.
It conducts a comprehensive benchmark evaluation of ten widely used large
language models, including both proprietary and open-weight models from OpenAI,
Meta, and Google. In parallel, it proposes a novel prompting strategy,
self-critique-guided curiosity refinement prompting. The key idea behind this
strategy is enabling models to self-critique and refine their responses without
additional training. The proposed method extends the curiosity-driven prompting
strategy by incorporating two lightweight in-context steps including
self-critique step and refinement step.
  The experiment results on the HONESET dataset evaluated using the framework
$\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a
judge of honesty and helpfulness, show consistent improvements across all
models. The approach reduces the number of poor-quality responses, increases
high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores
ranging from 1.4% to 4.3% compared to curiosity-driven prompting across
evaluated models. These results highlight the effectiveness of structured
self-refinement as a scalable and training-free strategy to improve the
trustworthiness of LLMs outputs.

</details>


### [18] [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种基于MURIL架构的网络欺凌检测框架，用于处理Hinglish文本，取得了较高的准确率，并提供了可解释性特征和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于数字通信平台上的Hinglish交流增加，现有的网络欺凌检测系统无法有效应对这种混合语言文本，因此需要一种新的检测框架。

Method: 本文提出了一种基于MURIL架构的网络欺凌检测框架，用于处理Hinglish文本。该方法包括可解释性特征（通过归因分析和跨语言模式识别）以及专门的预处理技术。

Result: 在六个基准数据集上的评估显示，基于MURIL的方法优于现有的多语言模型（如RoBERTa和IndicBERT），准确率分别达到86.97%、84.62%、86.03%、75.41%、83.92%和94.63%。

Conclusion: 该框架通过可解释性特征和跨语言模式识别提高了Hinglish文本中的网络欺凌检测性能，并指出了未来研究的方向，如上下文依赖的解释、文化理解以及跨语言讽刺检测。

Abstract: The growth of digital communication platforms has led to increased
cyberbullying incidents worldwide, creating a need for automated detection
systems to protect users. The rise of code-mixed Hindi-English (Hinglish)
communication on digital platforms poses challenges for existing cyberbullying
detection systems, which were designed primarily for monolingual text. This
paper presents a framework for cyberbullying detection in Hinglish text using
the Multilingual Representations for Indian Languages (MURIL) architecture to
address limitations in current approaches. Evaluation across six benchmark
datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et
al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based
approach outperforms existing multilingual models including RoBERTa and
IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies
of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\%
on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The
framework includes explainability features through attribution analysis and
cross-linguistic pattern recognition. Ablation studies show that selective
layer freezing, appropriate classification head design, and specialized
preprocessing for code-mixed content improve detection performance, while
failure analysis identifies challenges including context-dependent
interpretation, cultural understanding, and cross-linguistic sarcasm detection,
providing directions for future research in multilingual cyberbullying
detection.

</details>


### [19] [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
*Natapong Nitarach,Warit Sirichotedumrong,Panop Pitchayarthorn,Pittawat Taveekitworachai,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文介绍了FinCoT，一种结构化的思维链提示方法，结合了领域特定专家金融推理的见解来指导大型语言模型的推理轨迹。研究发现，领域对齐的结构化提示可以提高性能，减少推理成本，并产生更可解释和符合专家的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: FinNLP之前主要关注标准或非结构化CoT提示，而结构化CoT提示在以前的工作中受到的关注有限。此外，结构化CoT提示中的推理结构设计通常基于非领域专家的启发式方法。因此，需要一种更有效的结构化CoT提示方法，以提高性能并减少推理成本。

Method: FinCoT是一种结构化的思维链（CoT）提示方法，结合了领域特定专家金融推理的见解来指导大型语言模型的推理轨迹。研究调查了FinNLP中的三种主要提示风格，并评估了三种主要提示风格和FinCoT在涵盖十个金融领域的CFA风格问题上的表现。

Result: FinCoT将性能从63.2%提高到80.5%，Qwen-2.5-7B-Instruct从69.7%提高到74.2%，同时生成的标记减少了八倍，相比结构化CoT提示。

Conclusion: 领域对齐的结构化提示不仅提高了性能，还减少了推理成本，并产生了更可解释和符合专家的推理轨迹。

Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting
approach that incorporates insights from domain-specific expert financial
reasoning to guide the reasoning traces of large language models. We
investigate that there are three main prompting styles in FinNLP: (1) standard
prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an
explicit reasoning structure, such as the use of tags; and (3) structured CoT
prompting--CoT prompting with explicit instructions or examples that define
structured reasoning steps. Previously, FinNLP has primarily focused on prompt
engineering with either standard or unstructured CoT prompting. However,
structured CoT prompting has received limited attention in prior work.
Furthermore, the design of reasoning structures in structured CoT prompting is
often based on heuristics from non-domain experts. In this study, we
investigate each prompting approach in FinNLP. We evaluate the three main
prompting styles and FinCoT on CFA-style questions spanning ten financial
domains. We observe that FinCoT improves performance from 63.2% to 80.5% and
Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens
eight-fold compared to structured CoT prompting. Our findings show that
domain-aligned structured prompts not only improve performance and reduce
inference costs but also yield more interpretable and expert-aligned reasoning
traces.

</details>


### [20] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
*Chenxi Wang,Yixuan Zhang,Lang Gao,Zixiang Xu,Zirui Song,Yanbo Wang,Xiuying Chen*

Main category: cs.CL

TL;DR: 该研究通过BICAUSE数据集发现，大语言模型不仅模仿语言形式，还内化了由语言塑造的推理偏差。


<details>
  <summary>Details</summary>
Motivation: 如果语言相对性所建议的那样，语言的结构塑造了认知模式，那么在人类语言上训练的大语言模型（LLMs）可能也会内化不同语言中嵌入的惯常逻辑结构。

Method: 引入BICAUSE，一个用于因果推理的结构化双语数据集，包括在正向和反向因果形式中的语义对齐的中文和英文样本。

Result: LLMs表现出类型对齐的注意力模式，在中文中更关注原因和句首连接词，而在英语中则表现出更平衡的分布。模型内化了语言特定的因果词序偏好，并且通常将它们刚性地应用于非典型输入，导致性能下降，尤其是在中文中。当因果推理成功时，模型表示会收敛到跨语言的语义对齐抽象，表明超越表面形式的共享理解。

Conclusion: 这些结果表明，LLMs不仅模仿表面的语言形式，还内化了由语言塑造的推理偏差。基于认知语言学理论，这种现象首次通过模型内部结构分析得到了实证验证。

Abstract: Language is not only a tool for communication but also a medium for human
cognition and reasoning. If, as linguistic relativity suggests, the structure
of language shapes cognitive patterns, then large language models (LLMs)
trained on human language may also internalize the habitual logical structures
embedded in different languages. To examine this hypothesis, we introduce
BICAUSE, a structured bilingual dataset for causal reasoning, which includes
semantically aligned Chinese and English samples in both forward and reversed
causal forms. Our study reveals three key findings: (1) LLMs exhibit
typologically aligned attention patterns, focusing more on causes and
sentence-initial connectives in Chinese, while showing a more balanced
distribution in English. (2) Models internalize language-specific preferences
for causal word order and often rigidly apply them to atypical inputs, leading
to degraded performance, especially in Chinese. (3) When causal reasoning
succeeds, model representations converge toward semantically aligned
abstractions across languages, indicating a shared understanding beyond surface
form. Overall, these results suggest that LLMs not only mimic surface
linguistic forms but also internalize the reasoning biases shaped by language.
Rooted in cognitive linguistic theory, this phenomenon is for the first time
empirically verified through structural analysis of model internals.

</details>


### [21] [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
*Guanhua Chen,Yutong Yao,Lidia S. Chao,Xuebo Liu,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文提出了一种新的自我引导迭代校准框架（SGIC），通过使用不确定性分数来提高大型语言模型的校准效果。该框架通过迭代重新评估分数并结合先前的回答来优化校准，并引入了一种创新方法来构建迭代自我校准训练集，从而提高回答准确性。实验结果表明，该框架在封闭源代码和开放权重大型语言模型上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 最近的研究在检索增强生成（RAG）中集中在从候选文档中检索有用信息上。然而，许多方法常常忽略了大型语言模型（LLMs）的校准能力，这些模型利用其强大的上下文推理能力。本文表明，向LLMs提供特定提示可以显著提高它们的校准效果，特别是在多轮校准中。

Method: 我们提出了一个名为SGIC的自我引导迭代校准框架，该框架使用不确定性分数作为工具。首先，该框架计算不确定性分数以确定每份文档与查询的相关性以及对LLMs生成的回答的信心水平。随后，它迭代地重新评估这些分数，并将它们与先前的回答结合以优化校准。此外，我们引入了一种创新的方法来构建迭代自我校准训练集，这优化了LLMs以有效地利用不确定性分数来捕捉关键信息并提高回答准确性。

Result: 我们的框架显著提高了封闭源代码和开放权重大型语言模型的性能。

Conclusion: 我们的框架显著提高了封闭源代码和开放权重大型语言模型的性能。

Abstract: Recent research in retrieval-augmented generation (RAG) has concentrated on
retrieving useful information from candidate documents. However, numerous
methodologies frequently neglect the calibration capabilities of large language
models (LLMs), which capitalize on their robust in-context reasoning prowess.
This work illustrates that providing LLMs with specific cues substantially
improves their calibration efficacy, especially in multi-round calibrations. We
present a new SGIC: Self-Guided Iterative Calibration Framework that employs
uncertainty scores as a tool. Initially, this framework calculates uncertainty
scores to determine both the relevance of each document to the query and the
confidence level in the responses produced by the LLMs. Subsequently, it
reevaluates these scores iteratively, amalgamating them with prior responses to
refine calibration. Furthermore, we introduce an innovative approach for
constructing an iterative self-calibration training set, which optimizes LLMs
to efficiently harness uncertainty scores for capturing critical information
and enhancing response accuracy. Our proposed framework significantly improves
performance on both closed-source and open-weight LLMs.

</details>


### [22] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
*Masashi Takeshita,Rafal Rzepka*

Main category: cs.CL

TL;DR: 本文提出了JETHICS，一个用于评估AI模型伦理理解能力的日本数据集。实验结果显示，当前的大型语言模型在伦理理解方面仍有较大的提升空间。


<details>
  <summary>Details</summary>
Motivation: 为了评估AI模型在伦理理解方面的表现，特别是针对日本语境下的伦理问题，需要一个专门的数据集。

Method: 构建了一个名为JETHICS的日本数据集，用于评估AI模型的伦理理解能力。该数据集遵循现有英文ETHICS数据集的构建方法，并包含四个基于伦理和政治哲学的规范理论和概念类别，以及一个代表常识道德的类别。

Result: 在非专有大型语言模型（LLMs）和GPT-4o上的评估实验显示，即使GPT-4o的平均得分约为0.7，而表现最好的日本LLM仅达到约0.5，表明当前LLMs在伦理理解方面还有很大的提升空间。

Conclusion: 当前的大型语言模型在伦理理解方面仍有较大的提升空间，即使是GPT-4o也仅达到约0.7的平均分数。

Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics
understanding of AI models. JETHICS contains 78K examples and is built by
following the construction methods of the existing English ETHICS dataset. It
includes four categories based normative theories and concepts from ethics and
political philosophy; and one representing commonsense morality. Our evaluation
experiments on non-proprietary large language models (LLMs) and on GPT-4o
reveal that even GPT-4o achieves only an average score of about 0.7, while the
best-performing Japanese LLM attains around 0.5, indicating a relatively large
room for improvement in current LLMs.

</details>


### [23] [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
*Luna Wang,Andrew Caines,Alice Hutchings*

Main category: cs.CL

TL;DR: 本文批判性地分析了仇恨言论数据集构建中的方法论选择，提出研究者应反思自身的价值判断以提高数据集的可靠性和方法严谨性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示仇恨言论数据集构建中的方法论选择及其对数据集可靠性的影响，强调研究者需要反思自身的价值判断。

Method: 本文通过批判性分析不同数据集的方法选择，探讨了数据集构建中的复杂设计决策，并基于马克斯·韦伯的理想类型概念提出了反思性的方法。

Result: 本文分析了多种数据集的常见主题和实践，指出了其对数据集可靠性的影响，并提出了反思性方法的重要性。

Conclusion: 本文主张在构建仇恨言论数据集时，研究者应承认自己的价值判断，以提高透明度和方法严谨性。

Abstract: The curation of hate speech datasets involves complex design decisions that
balance competing priorities. This paper critically examines these
methodological choices in a diverse range of datasets, highlighting common
themes and practices, and their implications for dataset reliability. Drawing
on Max Weber's notion of ideal types, we argue for a reflexive approach in
dataset creation, urging researchers to acknowledge their own value judgments
during dataset construction, fostering transparency and methodological rigour.

</details>


### [24] [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
*Anindita Bhattacharya,Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Main category: cs.CL

TL;DR: 本研究探讨了使用先进抽象摘要模型从放射科报告的发现部分生成简洁印象的可能性，并对多种大型语言模型进行了比较分析，以评估其在医学文本摘要中的表现。


<details>
  <summary>Details</summary>
Motivation: 放射科报告的发现部分通常详细且冗长，而印象部分则较为简洁，捕捉关键诊断结论。本研究旨在探索使用先进的抽象摘要模型从发现部分生成简洁的印象。

Method: 本研究使用了公开的MIMIC-CXR数据集，并对领先的预训练和开源大型语言模型进行了比较分析，包括T5-base、BART-base、PEGASUS-x-base、ChatGPT-4、LLaMA-3-8B和一个带有覆盖机制的自定义指针生成网络。

Result: 本研究通过分析这些模型的性能，确定了它们在医学文本摘要中的各自优势和局限性。

Conclusion: 本研究的发现为医疗专业人员提供了有关医疗文本摘要的有用信息，有助于他们在医疗领域寻找自动摘要解决方案。

Abstract: The findings section of a radiology report is often detailed and lengthy,
whereas the impression section is comparatively more compact and captures key
diagnostic conclusions. This research explores the use of advanced abstractive
summarization models to generate the concise impression from the findings
section of a radiology report. We have used the publicly available MIMIC-CXR
dataset. A comparative analysis is conducted on leading pre-trained and
open-source large language models, including T5-base, BART-base,
PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network
with a coverage mechanism. To ensure a thorough assessment, multiple evaluation
metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and
BERTScore. By analyzing the performance of these models, this study identifies
their respective strengths and limitations in the summarization of medical
text. The findings of this paper provide helpful information for medical
professionals who need automated summarization solutions in the healthcare
sector.

</details>


### [25] [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
*Aishwarya Pothula,Bhavana Akkiraju,Srihari Bandarupalli,Charan D,Santosh Kesiraju,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 本文研究了弱标记数据在构建低资源语言对的语音到文本翻译系统中的应用，结果表明其性能可与大型多模态多语言基线相媲美。


<details>
  <summary>Details</summary>
Motivation: 由于高质量标注数据的稀缺性，开发有效的端到端语音到文本翻译（ST）系统面临重大挑战，特别是在低资源语言中。本文探讨了弱标记数据是否可用于构建低资源语言对的ST模型。

Method: 本文利用最先进的句子编码器通过双语挖掘构建了语音到文本翻译数据集，并挖掘了多语种Shrutilipi语料库以建立Shrutilipi-anuvaad数据集，该数据集包含Bengali-Hindi、Malayalam-Hindi、Odia-Hindi和Telugu-Hindi语言对的ST数据。

Result: 结果表明，可以使用弱标记数据构建ST系统，其性能可与SONAR和SeamlessM4T等大型多模态多语言基线相媲美。

Conclusion: 结果表明，可以使用弱标记数据构建ST系统，其性能可与SONAR和SeamlessM4T等大型多模态多语言基线相媲美。

Abstract: The scarcity of high-quality annotated data presents a significant challenge
in developing effective end-to-end speech-to-text translation (ST) systems,
particularly for low-resource languages. This paper explores the hypothesis
that weakly labeled data can be used to build ST models for low-resource
language pairs. We constructed speech-to-text translation datasets with the
help of bitext mining using state-of-the-art sentence encoders. We mined the
multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset
comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,
Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data
with varying degrees of quality and quantity to investigate the effect of
quality versus quantity of weakly labeled data on ST model performance. Results
demonstrate that ST systems can be built using weakly labeled data, with
performance comparable to massive multi-modal multilingual baselines such as
SONAR and SeamlessM4T.

</details>


### [26] [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
*Hao-Chien Lu,Jhen-Ke Lin,Hong-Yun Lin,Chung-Chun Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的混合评分模型，通过整合内容相关性和细粒度语法错误特征，显著提升了自动化口语评估的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化口语评估（ASA）系统在多方面评估中往往未能充分利用内容相关性，忽略了图像或示例线索，并采用表面的语法分析，缺乏详细的错误类型。

Method: 本文介绍了两种新的增强方法，以构建混合评分模型。首先，一个多面相关模块整合了问题和相关的图像内容、示例和第二语言学习者的口语回答，以进行全面的内容相关性评估。其次，使用先进的语法错误纠正（GEC）和详细注释得出细粒度的语法错误特征，以识别特定的错误类别。

Result: 实验和消融研究显示，这些组件显著改善了内容相关性、语言使用和整体ASA性能的评估。

Conclusion: 实验和消融研究证明，这些组件显著提高了内容相关性、语言使用和整体ASA性能的评估，突显了使用更丰富、更细致的特征集在全面口语评估中的优势。

Abstract: Current automated speaking assessment (ASA) systems for use in multi-aspect
evaluations often fail to make full use of content relevance, overlooking image
or exemplar cues, and employ superficial grammar analysis that lacks detailed
error types. This paper ameliorates these deficiencies by introducing two novel
enhancements to construct a hybrid scoring model. First, a multifaceted
relevance module integrates question and the associated image content,
exemplar, and spoken response of an L2 speaker for a comprehensive assessment
of content relevance. Second, fine-grained grammar error features are derived
using advanced grammar error correction (GEC) and detailed annotation to
identify specific error categories. Experiments and ablation studies
demonstrate that these components significantly improve the evaluation of
content relevance, language use, and overall ASA performance, highlighting the
benefits of using richer, more nuanced feature sets for holistic speaking
assessment.

</details>


### [27] [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
*Aleksandra Krasnodębska,Karolina Seweryn,Szymon Łukasik,Wojciech Kusa*

Main category: cs.CL

TL;DR: 该研究为波兰语语言模型的安全分类引入了一个手动标注的基准数据集，并通过对抗性扰动样本测试了不同模型的性能，发现基于HerBERT的分类器表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的安全评估和监控工具主要针对英语和其他高资源语言，而大多数全球语言未被充分研究。因此，需要一个针对波兰语的语言模型安全分类基准数据集。

Method: 研究者引入了一个手动标注的基准数据集，并创建了对抗性扰动样本以测试模型的鲁棒性。他们对三种模型进行了微调：Llama-Guard-3-8B、基于HerBERT的分类器和PLLuM，并使用不同的标注数据组合进行训练。

Result: 基于HerBERT的分类器在对抗条件下表现最佳，表明针对特定语言的模型调整可以提高安全评估的效果。

Conclusion: 研究结果表明，基于HerBERT的分类器在对抗条件下表现最佳，这表明针对特定语言的模型调整可以提高安全评估的效果。

Abstract: Despite increasing efforts to ensure the safety of large language models
(LLMs), most existing safety assessments and moderation tools remain heavily
biased toward English and other high-resource languages, leaving majority of
global languages underexamined. To address this gap, we introduce a manually
annotated benchmark dataset for language model safety classification in Polish.
We also create adversarially perturbed variants of these samples designed to
challenge model robustness. We conduct a series of experiments to evaluate
LLM-based and classifier-based models of varying sizes and architectures.
Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based
classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B
model. We train these models using different combinations of annotated data and
evaluate their performance, comparing it against publicly available guard
models. Results demonstrate that the HerBERT-based classifier achieves the
highest overall performance, particularly under adversarial conditions.

</details>


### [28] [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
*Agnese Daffara,Sourabh Dattawad,Sebastian Padó,Tanise Ceron*

Main category: cs.CL

TL;DR: 研究评估了MFC框架在巴西新闻中的适用性，发现其大部分仍适用，但需注意文化差异。


<details>
  <summary>Details</summary>
Motivation: MFC框架主要针对美国新闻议题，不清楚这些框架在其他文化背景下的适用性。

Method: 引入FrameNews-PT数据集，并在MFC框架内对其进行标注，通过多次标注轮次评估MFC框架在巴西辩论问题中的适用性，并进一步评估微调和零样本模型在域外数据上的表现。

Result: 15个MFC框架仍然具有广泛适用性，但某些框架使用频率较低，新新闻议题通常使用通用的“备用”框架。

Conclusion: 跨文化框架使用需要仔细考虑。

Abstract: Frames capture aspects of an issue that are emphasized in a debate by
interlocutors and can help us understand how political language conveys
different perspectives and ultimately shapes people's opinions. The Media Frame
Corpus (MFC) is the most commonly used framework with categories and detailed
guidelines for operationalizing frames. It is, however, focused on a few
salient U.S. news issues, making it unclear how well these frames can capture
news issues in other cultural contexts. To explore this, we introduce
FrameNews-PT, a dataset of Brazilian Portuguese news articles covering
political and economic news and annotate it within the MFC framework. Through
several annotation rounds, we evaluate the extent to which MFC frames
generalize to the Brazilian debate issues. We further evaluate how fine-tuned
and zero-shot models perform on out-of-domain data. Results show that the 15
MFC frames remain broadly applicable with minor revisions of the guidelines.
However, some MFC frames are rarely used, and novel news issues are analyzed
using general 'fall-back' frames. We conclude that cross-cultural frame use
requires careful consideration.

</details>


### [29] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本文研究了在关系抽取模型中整合知识图谱信息的影响，并发现这种整合可以显著提高性能，特别是在处理训练样本不平衡的情况下。


<details>
  <summary>Details</summary>
Motivation: 我们假设知识图谱中实体的位置为关系抽取任务提供了重要的见解。

Method: 我们通过将现有的关系抽取方法与基于图的神经贝尔曼-福特网络相结合，评估了基于知识图谱的特征的贡献。

Result: 实验结果表明，整合知识图谱信息可以显著提高性能，尤其是在处理训练样本不平衡的情况下。

Conclusion: 整合知识图谱信息可以显著提高关系抽取模型的性能，特别是在处理每个关系的训练样本不平衡时。

Abstract: We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.

</details>


### [30] [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本文介绍了一种新的封闭信息抽取方法，通过结合类型和实体特定信息，提高了关系抽取的准确性，并在大规模任务中表现出更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端生成模型在大规模封闭信息抽取任务中存在效率和准确性的不足，因此需要一种更高效和准确的方法。

Method: 该方法采用了一种判别式方法，结合了类型和实体特定信息以提高关系抽取的准确性。

Result: 该方法在大规模封闭信息抽取任务中表现出优于最先进的端到端生成模型的性能，同时使用较小的模型实现更高的效率。

Conclusion: 该方法在长尾关系的抽取中表现出色，并且在大规模封闭信息抽取任务中具有更高的效率和准确性。

Abstract: This paper introduces a novel method for closed information extraction. The
method employs a discriminative approach that incorporates type and
entity-specific information to improve relation extraction accuracy,
particularly benefiting long-tail relations. Notably, this method demonstrates
superior performance compared to state-of-the-art end-to-end generative models.
This is especially evident for the problem of large-scale closed information
extraction where we are confronted with millions of entities and hundreds of
relations. Furthermore, we emphasize the efficiency aspect by leveraging
smaller models. In particular, the integration of type-information proves
instrumental in achieving performance levels on par with or surpassing those of
a larger generative model. This advancement holds promise for more accurate and
efficient information extraction techniques.

</details>


### [31] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Main category: cs.CL

TL;DR: This paper examines whether Large Language Models can represent real-world entities by analyzing structural correspondences and their role in task performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can represent real-world entities given their text-only training and operation.

Method: The paper explores the conditions under which LLMs can represent worldly entities based on structural-correspondence theory.

Result: Structural correspondences alone are insufficient for representation; they must be used in a way that explains successful task performance.

Conclusion: LLMs may represent worldly entities if structural correspondences are appropriately exploited in task performance, despite their text-boundedness.

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [32] [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
*Kexin Huang,Qian Tu,Liwei Fan,Chenchen Yang,Dong Zhang,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文介绍了InstructTTSEval，一个用于评估指令驱动文本到语音系统能力的基准。通过三个任务和Gemini自动评判者，研究发现现有系统在执行复杂指令方面仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 传统文本到语音（TTS）系统依赖于固定的风格标签或插入语音提示来控制这些线索，这严重限制了灵活性。最近的尝试寻求使用自然语言指令来调节副语言特征，显著提高了指令驱动TTS模型的泛化能力。然而，这些系统的实际解释和执行复杂指令的能力仍然未被深入探索。此外，仍缺乏高质量的基准和专门设计的自动化评估指标用于基于指令的TTS，这阻碍了这些模型的准确评估和迭代优化。

Method: 我们引入了InstructTTSEval，这是一个用于衡量复杂自然语言风格控制能力的基准。我们引入了三个任务，即声学参数规范、描述性风格指令和角色扮演，并使用Gemini作为自动评判者来评估它们的指令遵循能力。

Result: 我们的评估显示，可访问的指令遵循TTS系统仍有很大的改进空间。

Conclusion: 我们期望InstructTTSEval将推动更强大、灵活和准确的指令跟随TTS的发展。

Abstract: In modern speech synthesis, paralinguistic information--such as a speaker's
vocal timbre, emotional state, and dynamic prosody--plays a critical role in
conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)
systems rely on fixed style labels or inserting a speech prompt to control
these cues, which severely limits flexibility. Recent attempts seek to employ
natural-language instructions to modulate paralinguistic features,
substantially improving the generalization of instruction-driven TTS models.
Although many TTS systems now support customized synthesis via textual
description, their actual ability to interpret and execute complex instructions
remains largely unexplored. In addition, there is still a shortage of
high-quality benchmarks and automated evaluation metrics specifically designed
for instruction-based TTS, which hinders accurate assessment and iterative
optimization of these models. To address these limitations, we introduce
InstructTTSEval, a benchmark for measuring the capability of complex
natural-language style control. We introduce three tasks, namely
Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,
including English and Chinese subsets, each with 1k test cases (6k in total)
paired with reference audio. We leverage Gemini as an automatic judge to assess
their instruction-following abilities. Our evaluation of accessible
instruction-following TTS systems highlights substantial room for further
improvement. We anticipate that InstructTTSEval will drive progress toward more
powerful, flexible, and accurate instruction-following TTS.

</details>


### [33] [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
*Hao Li,Viktor Schlegel,Yizheng Sun,Riza Batista-Navarro,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文综述了基于大语言模型的论点挖掘的最新进展，包括基础理论、标注框架、数据集、子任务分类以及当前的大语言模型架构和方法，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 论点挖掘是自然语言处理的一个关键子领域，而大语言模型的出现极大地改变了这一领域，因此需要对相关研究进行系统性的综述。

Method: 本文系统地综述了大语言模型驱动的论点挖掘的最新进展，包括基础理论、标注框架、数据集、子任务分类以及当前的大语言模型架构和方法。

Result: 本文提供了对基础理论、标注框架和数据集的简要回顾，并提出了一个全面的论点挖掘子任务分类，同时详细描述了当前的大语言模型架构和方法，批判性地评估了评估实践，并明确了关键挑战。

Conclusion: 本文总结了基于大语言模型的论点挖掘的研究进展，并提出了未来研究方向，以指导该快速发展的领域。

Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing
(NLP), focuses on extracting argumentative structures from text. The advent of
Large Language Models (LLMs) has profoundly transformed AM, enabling advanced
in-context learning, prompt-based generation, and robust cross-domain
adaptability. This survey systematically synthesizes recent advancements in
LLM-driven AM. We provide a concise review of foundational theories and
annotation frameworks, alongside a meticulously curated catalog of datasets. A
key contribution is our comprehensive taxonomy of AM subtasks, elucidating how
contemporary LLM techniques -- such as prompting, chain-of-thought reasoning,
and retrieval augmentation -- have reconfigured their execution. We further
detail current LLM architectures and methodologies, critically assess
evaluation practices, and delineate pivotal challenges including long-context
reasoning, interpretability, and annotation bottlenecks. Conclusively, we
highlight emerging trends and propose a forward-looking research agenda for
LLM-based computational argumentation, aiming to strategically guide
researchers in this rapidly evolving domain.

</details>


### [34] [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
*Sani Abdullahi Sani,Salim Abubakar,Falalu Ibrahim Lawan,Abdulhamid Abubakar,Maryam Bala*

Main category: cs.CL

TL;DR: 本研究提出了一种基于AfriBERTa的豪萨语多标签情绪检测方法，在验证集上取得了74.00%的准确率和73.50%的F1得分。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决豪萨语这种低资源非洲语言中的多标签情绪检测问题。

Method: 我们微调了AfriBERTa，这是一个在非洲语言上预训练的基于transformer的模型，以对豪萨语文本进行分类。我们的方法包括数据预处理、分词和使用Hugging Face Trainer API进行模型微调。

Result: 系统在验证集上的准确率为74.00%，F1得分为73.50%。

Conclusion: 该研究展示了基于transformer的模型在低资源语言中进行情绪检测的有效性。

Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a
low-resource African language, as part of SemEval Track A. We fine-tuned
AfriBERTa, a transformer-based model pre-trained on African languages, to
classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and
surprise. Our methodology involved data preprocessing, tokenization, and model
fine-tuning using the Hugging Face Trainer API. The system achieved a
validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the
effectiveness of transformer-based models for emotion detection in low-resource
languages.

</details>


### [35] [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
*Chenyi Zhou,Zhengyan Shi,Yuan Yao,Lei Liang,Huajun Chen,Qiang Zhang*

Main category: cs.CL

TL;DR: RiOT是一种新的自动提示优化框架，通过迭代优化和文本残差连接来解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的自动提示优化方法面临缺乏多样性以及语义漂移的问题。

Method: RiOT通过文本梯度迭代优化提示，生成多个语义多样化的候选，并使用困惑度选择最佳提示。此外，RiOT通过文本残差连接来减轻语义漂移。

Result: RiOT在五个基准测试中表现优于以前的提示优化方法和手动提示。

Conclusion: RiOT在五个基准测试中表现出色，优于以前的提示优化方法和手动提示。

Abstract: Recent advancements in large language models (LLMs) have highlighted their
potential across a variety of tasks, but their performance still heavily relies
on the design of effective prompts. Existing methods for automatic prompt
optimization face two challenges: lack of diversity, limiting the exploration
of valuable and innovative directions and semantic drift, where optimizations
for one task can degrade performance in others. To address these issues, we
propose Residual Optimization Tree (RiOT), a novel framework for automatic
prompt optimization. RiOT iteratively refines prompts through text gradients,
generating multiple semantically diverse candidates at each step, and selects
the best prompt using perplexity. Additionally, RiOT incorporates the text
residual connection to mitigate semantic drift by selectively retaining
beneficial content across optimization iterations. A tree structure efficiently
manages the optimization process, ensuring scalability and flexibility.
Extensive experiments across five benchmarks, covering commonsense,
mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT
outperforms both previous prompt optimization methods and manual prompting.

</details>


### [36] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
*Yao Lu,Zhaiyuan Ji,Jiawei Du,Yu Shanqing,Qi Xuan,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种多模型协作标注的新范式 AutoAnnotator，该框架通过结合大型语言模型（LLMs）和小型语言模型（SLMs）的优势，显著降低了标注成本并提高了标注准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大型语言模型（LLMs）的标注范式近年来取得了重大突破，但其实际部署仍存在两个核心瓶颈：首先，大规模标注中调用商业 API 的成本非常高；其次，在需要细粒度语义理解的场景（如情感分类和毒性分类）中，LLMs 的标注准确性甚至低于专门为此领域设计的小型语言模型（SLMs）。为了解决这些问题，我们提出了一个全新的多模型协作标注范式。

Method: 提出了一种多模型协作标注的新范式，并设计了一个基于此的全自动标注框架 AutoAnnotator。AutoAnnotator 包含两个层次：上层的元控制器层利用 LLM 的生成和推理能力选择 SLM 进行标注，自动生成标注代码并验证困难样本；下层的任务专家层由多个 SLM 组成，通过多模型投票进行标注。此外，还使用元控制器层的二次审查获得的困难样本作为强化学习集，并通过持续学习策略分阶段微调 SLM，从而提高 SLM 的泛化能力。

Result: 广泛的实验表明，AutoAnnotator 在零样本、少样本、思维链和多数投票设置中优于现有的开源/API LLMs。值得注意的是，与直接使用 GPT-3.5-turbo 进行标注相比，AutoAnnotator 将标注成本降低了 74.15%，同时提高了 6.21% 的准确率。

Conclusion: AutoAnnotator 在零样本、少样本、思维链和多数投票设置中优于现有的开源/API LLMs。值得注意的是，与直接使用 GPT-3.5-turbo 进行标注相比，AutoAnnotator 将标注成本降低了 74.15%，同时提高了 6.21% 的准确率。

Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has
made significant breakthroughs in recent years, its actual deployment still has
two core bottlenecks: first, the cost of calling commercial APIs in large-scale
annotation is very expensive; second, in scenarios that require fine-grained
semantic understanding, such as sentiment classification and toxicity
classification, the annotation accuracy of LLMs is even lower than that of
Small Language Models (SLMs) dedicated to this field. To address these
problems, we propose a new paradigm of multi-model cooperative annotation and
design a fully automatic annotation framework AutoAnnotator based on this.
Specifically, AutoAnnotator consists of two layers. The upper-level
meta-controller layer uses the generation and reasoning capabilities of LLMs to
select SLMs for annotation, automatically generate annotation code and verify
difficult samples; the lower-level task-specialist layer consists of multiple
SLMs that perform annotation through multi-model voting. In addition, we use
the difficult samples obtained by the secondary review of the meta-controller
layer as the reinforcement learning set and fine-tune the SLMs in stages
through a continual learning strategy, thereby improving the generalization of
SLMs. Extensive experiments show that AutoAnnotator outperforms existing
open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.
Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to
directly annotating with GPT-3.5-turbo, while still improving the accuracy by
6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [37] [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
*Zhexu Wang,Yiping Liu,Yejie Wang,Wenyang He,Bofei Gao,Muxi Diao,Yanxu Chen,Kelin Fu,Flood Sung,Zhilin Yang,Tianyu Liu,Weiran Xu*

Main category: cs.CL

TL;DR: 本文介绍了OJBench，一个用于评估大型语言模型竞赛级代码推理能力的新基准测试，并发现即使是最先进的模型也面临显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准测试在评估大型语言模型的完整能力方面存在局限，特别是在竞赛级别上。

Method: 引入了OJBench，这是一个新的、具有挑战性的基准测试，用于评估大型语言模型的竞赛级代码推理能力。

Result: 对37个模型进行了全面评估，包括闭源和开源模型，以及推理导向和非推理导向模型。结果显示，即使是最先进的推理导向模型也难以应对高度挑战性的竞赛级问题。

Conclusion: OJBench的评估结果表明，即使是最先进的推理导向模型在处理高难度的竞赛级问题时也面临显著挑战。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
significant progress in math and code reasoning capabilities. However, existing
code benchmark are limited in their ability to evaluate the full spectrum of
these capabilities, particularly at the competitive level. To bridge this gap,
we introduce OJBench, a novel and challenging benchmark designed to assess the
competitive-level code reasoning abilities of LLMs. OJBench comprises 232
programming competition problems from NOI and ICPC, providing a more rigorous
test of models' reasoning skills. We conducted a comprehensive evaluation using
OJBench on 37 models, including both closed-source and open-source models,
reasoning-oriented and non-reasoning-oriented models. Our results indicate that
even state-of-the-art reasoning-oriented models, such as o4-mini and
Gemini-2.5-pro-exp, struggle with highly challenging competition-level
problems. This highlights the significant challenges that models face in
competitive-level code reasoning.

</details>


### [38] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
*Shushanta Pudasaini,Aman Shakya,Siddhartha Shrestha,Sahil Bhatta,Sunil Thapa,Sushmita Palikhe*

Main category: cs.CL

TL;DR: 本研究提出了NepaliGPT，一个专门为尼泊尔语设计的生成式大型语言模型，并通过实验验证了其在文本生成方面的性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏针对尼泊尔语的生成式语言模型，其他下游任务如微调尚未被探索。因此，本研究旨在填补尼泊尔自然语言处理领域的这一研究空白。

Method: 本研究收集了一个名为Devanagari Corpus的先进语料库，并引入了第一个包含4,296个尼泊尔语问答对的NepaliGPT基准数据集。

Result: NepaliGPT在文本生成方面取得了以下指标：困惑度为26.32245，ROUGE-1得分为0.2604，因果连贯性为81.25%，因果一致性为85.41%。

Conclusion: 本研究提出了NepaliGPT，一个专门为尼泊尔语设计的生成式大型语言模型，并通过实验验证了其在文本生成方面的性能。

Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge
popularity in recent days and thousands of variants of LLMs have been released.
However, there is no generative language model for the Nepali language, due to
which other downstream tasks, including fine-tuning, have not been explored
yet. To fill this research gap in the Nepali NLP space, this research proposes
\textit{NepaliGPT}, a generative large language model tailored specifically for
the Nepali language. This research introduces an advanced corpus for the Nepali
language collected from several sources, called the Devanagari Corpus.
Likewise, the research introduces the first NepaliGPT benchmark dataset
comprised of 4,296 question-answer pairs in the Nepali language. The proposed
LLM NepaliGPT achieves the following metrics in text generation: Perplexity of
26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal
consistency of 85.41\%.

</details>


### [39] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个理论框架，用于分析长上下文任务的失败模式，并探讨了多代理分块的有效性。实验结果表明，通过精心管理的分块和聚合策略，可以有效处理长上下文问题。


<details>
  <summary>Details</summary>
Motivation: 我们研究了将大型语言模型（LLMs）应用于长文本的挑战。

Method: 我们提出了一种理论框架，将长上下文任务的失败模式分为三类：跨块依赖（任务噪声）、随着上下文大小增长的混淆（模型噪声）以及部分结果的不完善整合（聚合器噪声）。我们分析了何时使用多代理分块是有效的，即把一个长序列分成较小的块并聚合每个块的处理结果。

Result: 我们在检索、问答和摘要等任务上的实验确认了理论分析以及有利于多代理分块的条件。通过探索输入长度的超线性模型噪声增长，我们还解释了为什么对于大输入，配置为基于分块处理的较弱模型可以超越像GPT4o这样的更先进模型的单次应用。

Conclusion: 我们提出了一个原则性的理解框架，并且我们的结果突显了通过精心管理的分块和聚合策略来处理LLMs中的长上下文的直接路径。

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [40] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: This paper proposes REIS, an ISP system tailored for RAG that improves the performance and energy efficiency of the retrieval stage by addressing the limitations of existing works.


<details>
  <summary>Details</summary>
Motivation: The retrieval stage of RAG becomes a significant bottleneck in inference pipelines due to the large database sizes and data movement overheads. Existing works that leverage ISP for ANNS have limitations such as not being tailored to ISP systems, not accelerating data retrieval operations, and introducing significant hardware modifications.

Method: REIS employs a database layout that links database embedding vectors to their associated documents, introduces an ISP-tailored data placement technique, and leverages an ANNS engine that uses existing computational resources inside the storage system.

Result: Compared to a server-grade system, REIS improves the performance (energy efficiency) of retrieval by an average of 13x (55x).

Conclusion: REIS is the first ISP system tailored for RAG that addresses the limitations of existing works by improving performance and energy efficiency.

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [41] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: The paper proposes StoryWriter, a multi-agent framework for long story generation, which addresses discourse coherence and narrative complexity. It outperforms existing baselines and generates a dataset of high-quality long stories. The framework is used to train models that demonstrate advanced performance in long story generation.


<details>
  <summary>Details</summary>
Motivation: Long story generation remains a challenge for existing large language models (LLMs) due to discourse coherence and narrative complexity. The goal is to address these challenges by proposing a framework that can generate long-form stories with plot consistency, logical coherence, and an interwoven and engaging narrative.

Method: StoryWriter is a multi-agent story generation framework consisting of three main modules: outline agent, planning agent, and writing agent. The outline agent generates event-based outlines, the planning agent details events and plans which events should be written in each chapter, and the writing agent dynamically compresses the story history based on the current event to generate and reflect new plots.

Result: StoryWriter significantly outperforms existing story generation baselines in both story quality and length. A dataset containing about 6,000 high-quality long stories was generated using StoryWriter, with an average length of 8,000 words. The model Llama3.1-8B and GLM4-9B were trained on LongStory and developed into StoryWriter_GLM and StoryWriter_GLM, demonstrating advanced performance in long story generation.

Conclusion: StoryWriter significantly outperforms existing story generation baselines in both story quality and length. Additionally, the generated dataset containing high-quality long stories can be used to train models for advanced performance in long story generation.

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [42] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau Bölöni*

Main category: cs.CL

TL;DR: 本文提出了一种方法，通过利用现有的有害言论数据集来检测隐性仇恨言论并提高跨不同数据集的泛化能力。实验结果显示，该方法显著提高了隐性仇恨言论检测的F1分数。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论已成为社交媒体平台的一个重要挑战。传统研究主要关注有害言论，但需要通用技术来检测隐蔽和微妙形式的仇恨言论。

Method: 我们的方法包括三个关键组成部分：有影响力的样本识别、重新标注以及使用Llama-3 70B和GPT-4o进行增强。

Result: 实验结果表明，我们的方法在提高隐性仇恨言论检测效果方面是有效的。

Conclusion: 我们的方法在改善隐性仇恨言论检测方面表现出色，相比基线模型提升了12.9分的F1分数。

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [43] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
*Soumya Suvra Ghosal,Vaibhav Singh,Akash Ghosh,Soumyabrata Pal,Subhadip Baidya,Sriparna Saha,Dinesh Manocha*

Main category: cs.CL

TL;DR: RELIC is a novel in-context learning framework for reward modeling in low-resource Indic languages, which improves reward model accuracy by selecting in-context examples from high-resource languages.


<details>
  <summary>Details</summary>
Motivation: Most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical.

Method: RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses.

Result: Extensive experiments on three preference datasets using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages.

Conclusion: RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods.

Abstract: Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.

</details>


### [44] [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
*Dana Serditova,Kevin Tang,Jochen Steffens*

Main category: cs.CL

TL;DR: 本文研究了ASR在纽卡斯尔英语（一种已知难以被ASR识别的地区方言）上的表现，发现ASR错误与地区方言特征直接相关，建议增加ASR训练数据的方言多样性。


<details>
  <summary>Details</summary>
Motivation: 由于ASR系统在地区方言上的表现不佳，这可能是由于训练数据偏向主流变体。虽然之前的研究已经发现了ASR中的种族、年龄和性别偏见，但地区偏见仍然缺乏研究。

Method: 本研究进行了两阶段分析：首先，对子样本进行了手动错误分析，以确定导致ASR误识别的关键音系、词汇和形态句法错误；其次，进行了一项案例研究，重点分析ASR对区域代词“yous”和“wor”的识别。

Result: 结果表明，ASR错误直接与地区方言特征相关，而社会因素在ASR不匹配中起较小的作用。

Conclusion: 本文倡导在自动语音识别（ASR）训练数据中增加方言多样性，并强调社会语言学分析在诊断和解决区域偏见中的价值。

Abstract: Automatic Speech Recognition (ASR) systems struggle with regional dialects
due to biased training which favours mainstream varieties. While previous
research has identified racial, age, and gender biases in ASR, regional bias
remains underexamined. This study investigates ASR performance on Newcastle
English, a well-documented regional dialect known to be challenging for ASR. A
two-stage analysis was conducted: first, a manual error analysis on a subsample
identified key phonological, lexical, and morphosyntactic errors behind ASR
misrecognitions; second, a case study focused on the systematic analysis of ASR
recognition of the regional pronouns ``yous'' and ``wor''. Results show that
ASR errors directly correlate with regional dialectal features, while social
factors play a lesser role in ASR mismatches. We advocate for greater dialectal
diversity in ASR training data and highlight the value of sociolinguistic
analysis in diagnosing and addressing regional biases.

</details>


### [45] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 本文提出了一种基于分解和中心化阶段的持续学习方法，以防止在多语言和语言无关条件下训练神经网络模型时发生灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现代基于神经网络的语音识别模型需要不断吸收新数据而无需重新训练整个系统，尤其是在使用基础模型的下游应用中，无法访问原始训练数据。在无重放、多语言和语言无关的条件下持续训练模型，可能导致灾难性遗忘。

Method: 受人类大脑在清醒-睡眠周期中学习和巩固知识的能力启发，我们提出了一种持续学习方法，包括两个不同的阶段：分解和中心化，分别学习和合并知识。

Result: 在一系列不同的代码切换数据集上的实验表明，中心化阶段可以有效地防止灾难性遗忘。

Conclusion: 实验表明，通过在多个分散的低秩适配器中积累知识，中心化阶段可以有效防止灾难性遗忘。

Abstract: Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

</details>


### [46] [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
*Tuan-Nam Nguyen,Ngoc-Quan Pham,Seymanur Akti,Alexander Waibel*

Main category: cs.CL

TL;DR: 本文提出了一种首个流式语音转换模型，能够在保持稳定延迟的同时实现与顶级模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音转换模型无法实现流式处理，而本研究旨在提出一种能够将非母语语音转换为类似母语口音的流式语音转换模型，同时保留说话人身份、韵律并改善发音。

Method: 通过修改之前的语音转换架构，采用Emformer编码器和优化的推理机制，实现流式处理，并集成一个原生文本转语音模型以生成理想的训练数据。

Result: 该流式语音转换模型在保持稳定延迟的同时，实现了与顶级语音转换模型相当的性能。

Conclusion: 我们的流式语音转换模型在保持稳定延迟的同时，实现了与顶级语音转换模型相当的性能，成为首个能够流式传输的语音转换系统。

Abstract: We propose a first streaming accent conversion (AC) model that transforms
non-native speech into a native-like accent while preserving speaker identity,
prosody and improving pronunciation. Our approach enables stream processing by
modifying a previous AC architecture with an Emformer encoder and an optimized
inference mechanism. Additionally, we integrate a native text-to-speech (TTS)
model to generate ideal ground-truth data for efficient training. Our streaming
AC model achieves comparable performance to the top AC models while maintaining
stable latency, making it the first AC system capable of streaming.

</details>


### [47] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: 本文提出了一种评估大型语言模型是否具备稳健世界模型的框架，通过分解模型响应的可变性来衡量其语义基础。结果显示，更大的模型在某些领域表现更好，但并非在所有领域都如此。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型（LLMs）是否拥有一个世界模型——一种支持超越表面模式的泛化的结构化世界理解——对于评估其可靠性至关重要，尤其是在高风险应用中。

Method: 我们提出了一个正式的框架来评估LLM是否表现出足够稳健的世界模型，定义为在语义等效提示中产生一致输出，同时区分表达不同意图的提示。我们引入了一种新的评估方法，将模型响应的可变性分解为三个组成部分：用户目的、用户表达和模型不稳定性的可变性。

Result: 我们的结果表明，更大的模型会将更多输出可变性归因于用户目的的变化，这表明其世界模型更加稳健。然而，这种改进并不均匀：更大的模型在所有领域并不总是优于较小的模型，它们在稳健性方面的优势通常是有限的。

Conclusion: 这些发现强调了超越基于准确性的基准，转向更直接评估模型对世界内部理解的结构和稳定性的语义诊断的重要性。

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [48] [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
*Hanshu Rao,Weisi Liu,Haohan Wang,I-Chan Huang,Zhe He,Xiaolei Huang*

Main category: cs.CL

TL;DR: 本文综述了合成数据生成在生物医学领域的应用趋势，分析了数据模态、生成方法和评估方式，并指出当前存在的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 合成数据生成在生物医学领域中被用于缓解数据稀缺性、隐私问题和数据质量问题，而大型语言模型（LLMs）的快速发展为这一领域提供了新的机遇。

Method: 本文采用PRISMA-ScR指南，对2020年至2025年间发表的59项研究进行了系统综述，数据来源于PubMed、ACM、Web of Science和Google Scholar。

Result: 分析显示，78.0%的数据模态是未结构化文本，13.6%是表格数据，8.4%是多模态数据；72.9%的生成方法是提示法，22.0%是微调LLMs，5.1%是专用模型；评估方法包括内在指标（27.1%）、人机交互评估（55.9%）和基于LLM的评估（13.6%）。

Conclusion: 本文分析了合成数据生成在生物医学领域的现状和挑战，指出了当前在数据模态、生成方法和评估方面的问题，并强调了需要进一步研究的方向。

Abstract: Synthetic data generation--mitigating data scarcity, privacy concerns, and
data quality challenges in biomedical fields--has been facilitated by rapid
advances of large language models (LLMs). This scoping review follows
PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and
2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The
review systematically examines biomedical research and application trends in
synthetic data generation, emphasizing clinical applications, methodologies,
and evaluations. Our analysis identifies data modalities of unstructured texts
(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation
methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model
(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),
human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The
analysis addresses current limitations in what, where, and how health
professionals can leverage synthetic data generation for biomedical domains.
Our review also highlights challenges in adaption across clinical domains,
resource and model accessibility, and evaluation standardizations.

</details>


### [49] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Main category: cs.CL

TL;DR: 本文提出了一种计算框架，用于建模公众对科学新闻的感知，并创建了一个大规模的数据集。研究发现，科学新闻的消费频率影响公众感知，而公众感知可以预测他们对科学内容的参与度。


<details>
  <summary>Details</summary>
Motivation: 有效吸引公众参与科学对于建立科学界的信任和理解至关重要。然而，随着信息量的增加，科学传播者难以预测受众如何感知和互动科学新闻。

Method: 本文介绍了一个计算框架，用于建模公众在十二个维度上的感知，如新闻价值、重要性和意外性。此外，还开发了NLP模型来预测公众感知分数，并创建了一个大规模的科学新闻感知数据集。

Result: 研究发现，个人科学新闻的消费频率是感知的驱动因素，而人口统计因素的影响较小。更重要的是，通过大规模分析和精心设计的Reddit自然实验，我们证明了估计的公众感知与最终的参与模式有直接联系。

Conclusion: 这项研究强调了在科学传播中对感知进行细致建模的重要性，为预测公众对科学内容的兴趣和参与度提供了新途径。

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


### [50] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Main category: cs.CL

TL;DR: 本研究提出了一种新方法，利用大语言模型（LLMs）在基于规则的自然语言处理（NLP）系统开发阶段，以提高开发效率和成本效益。实验结果表明，该方法在识别临床相关文本片段和提取关键术语方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习（ML）和大型语言模型（LLMs）取得了进展，但基于规则的自然语言处理（NLP）系统由于其可解释性和操作效率仍在临床环境中活跃。然而，它们的手动开发和维护是劳动密集型的，特别是在具有大量语言变体的任务中。

Method: 提出了一种新方法，仅在基于规则的系统开发阶段使用LLMs。进行了初步实验，专注于开发基于规则的NLP流水线的前两个步骤：从临床笔记中找到相关片段；从片段中提取用于基于规则的命名实体识别（NER）组件的信息性关键词。

Result: 实验结果表明，在识别临床相关的文本片段方面具有出色的召回率（Deepseek: 0.98, Qwen: 0.99），并且在提取NER的关键术语方面达到了1.0。

Conclusion: 本研究展示了利用大语言模型（LLMs）在基于规则的自然语言处理（NLP）系统开发阶段的潜力，为半自动或自动开发基于规则的系统提供了一种有前景的新方向。

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [51] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Main category: cs.CL

TL;DR: This paper introduces a new task called GeoGuess for multimodal reasoning, along with a dataset and method that show strong performance in this task.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of reasoning on hierarchical visual clues at different levels of granularity in existing multimodal reasoning tasks. It seeks to bridge the gap by introducing a new task that requires reasoning between hierarchical visual information and geographic knowledge.

Method: The paper introduces a new task called GeoGuess, which involves identifying the location of a street view image and providing a detailed explanation. It also presents a multimodal and multilevel reasoning method called SightSense that can make predictions and generate comprehensive explanations based on hierarchical visual information and external knowledge.

Result: The paper establishes a benchmark for GeoGuess by introducing the GeoExplain dataset and presents the SightSense method, which demonstrates outstanding performance in GeoGuess.

Conclusion: GeoGuess is a novel and challenging task for multimodal reasoning, and the proposed GeoExplain dataset and SightSense method demonstrate outstanding performance in this task.

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [52] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
*Pavlo Vasylenko,Marcos Treviso,André F. T. Martins*

Main category: cs.CL

TL;DR: 本文提出了自适应可扩展熵最大（ASEntmax）机制，以改进Transformer模型中的注意力机制，使其能够更好地处理长上下文任务。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的架构使用softmax计算注意力权重，这会导致在处理需要精确关注固定大小模式的任务时，随着序列长度的增加，非信息性标记会积累注意力概率质量，从而导致分散和表征崩溃。

Method: 本文提出了一种自适应可扩展熵最大（ASEntmax）机制，它为α-entmax引入了一个可学习的温度参数，使注意力分布能够在稀疏（模式聚焦）和密集（类似softmax）模式之间进行插值。此外，还通过精心设计的位置编码来改进固定大小模式的定位和泛化能力。

Result: 实验结果表明，通过将ASEntmax与适当的位置编码结合，模型在长上下文泛化任务中表现优于softmax、可扩展softmax和固定温度的α-entmax基线。

Conclusion: 通过将ASEntmax集成到标准的transformer层中，并结合适当的位置编码，我们的模型在长上下文泛化任务中显著优于softmax、可扩展softmax和固定温度的α-entmax基线。

Abstract: Transformer-based architectures traditionally employ softmax to compute
attention weights, which produces dense distributions over all tokens in a
sequence. While effective in many settings, this density has been shown to be
detrimental for tasks that demand precise focus on fixed-size patterns: as
sequence length increases, non-informative tokens accumulate attention
probability mass, leading to dispersion and representational collapse. We show
in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid
these issues, due to their ability to assign exact zeros to irrelevant tokens.
Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows
$\alpha$-entmax with a learnable temperature parameter, allowing the attention
distribution to interpolate between sparse (pattern-focused) and dense
(softmax-like) regimes. Finally, we show that the ability to locate and
generalize fixed-size patterns can be further improved through a careful design
of position encodings, which impacts both dense and sparse attention methods.
By integrating ASEntmax into standard transformer layers alongside proper
positional encodings, we show that our models greatly outperform softmax,
scalable softmax, and fixed-temperature $\alpha$-entmax baselines on
long-context generalization.

</details>


### [53] [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
*Co Tran,Salman Paracha,Adil Hafeez,Shuguang Chen*

Main category: cs.CL

TL;DR: 本文提出了一种对齐偏好的路由框架，通过将查询与用户定义的领域或操作类型相匹配来指导模型选择，并引入了一个紧凑的1.5B模型Arch-Router，实验证明其在匹配查询与人类偏好方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM路由方法在两个关键方面受到限制：它们使用无法捕捉由主观评估标准驱动的人类偏好的基准来评估性能，并且通常从有限的模型池中选择模型。

Method: 我们提出了一个对齐偏好的路由框架，通过将查询与用户定义的领域或操作类型相匹配来指导模型选择。我们引入了Arch-Router，这是一个紧凑的1.5B模型，学习将查询映射到领域-操作偏好以进行模型路由决策。

Result: 实验表明，我们的方法在匹配查询与人类偏好方面取得了最先进的结果，优于顶级专有模型。

Conclusion: 我们的方法在匹配查询与人类偏好方面取得了最先进的结果，并且使路由决策更加透明和灵活。

Abstract: With the rapid proliferation of large language models (LLMs) -- each
optimized for different strengths, style, or latency/cost profile -- routing
has become an essential technique to operationalize the use of different
models. However, existing LLM routing approaches are limited in two key ways:
they evaluate performance using benchmarks that often fail to capture human
preferences driven by subjective evaluation criteria, and they typically select
from a limited pool of models. In this work, we propose a preference-aligned
routing framework that guides model selection by matching queries to
user-defined domains (e.g., travel) or action types (e.g., image editing) --
offering a practical mechanism to encode preferences in routing decisions.
Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that
learns to map queries to domain-action preferences for model routing decisions.
Our approach also supports seamlessly adding new models for routing without
requiring retraining or architectural modifications. Experiments on
conversational datasets demonstrate that our approach achieves state-of-the-art
(SOTA) results in matching queries with human preferences, outperforming top
proprietary models. Our approach captures subjective evaluation criteria and
makes routing decisions more transparent and flexible. Our model is available
at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

</details>


### [54] [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
*Ananth Agarwal,Jasper Jian,Christopher D. Manning,Shikhar Murty*

Main category: cs.CL

TL;DR: 本研究评估了32个开放权重的变压器模型，发现通过探测提取的句法特征无法预测下游任务中的句法表现，表明探测到的句法表示与实际句法行为之间存在显著脱节。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理和生成文本时表现出对句法的稳健掌握。虽然这表明它们内部化了层次句法和依赖关系的理解，但它们如何表示句法结构仍然是可解释性研究中的一个开放领域。探测提供了一种识别句法线性编码机制的方法，然而，尚未有全面的研究确立模型的探测准确性是否可靠地预测其下游句法性能。

Method: 我们采用了一种“机制与结果”框架，评估了32个开放权重的变压器模型，并发现通过探测提取的句法特征无法预测英语语言现象的目标句法评估的结果。

Result: 我们评估了32个开放权重的变压器模型，并发现通过探测提取的句法特征无法预测英语语言现象的目标句法评估的结果。

Conclusion: 我们的结果突显了通过探测找到的潜在句法表示与下游任务中的可观察句法行为之间的显著脱节。

Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when
processing and generating text. While this suggests internalized understanding
of hierarchical syntax and dependency relations, the precise mechanism by which
they represent syntactic structure is an open area within interpretability
research. Probing provides one way to identify the mechanism of syntax being
linearly encoded in activations, however, no comprehensive study has yet
established whether a model's probing accuracy reliably predicts its downstream
syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we
evaluate 32 open-weight transformer models and find that syntactic features
extracted via probing fail to predict outcomes of targeted syntax evaluations
across English linguistic phenomena. Our results highlight a substantial
disconnect between latent syntactic representations found via probing and
observable syntactic behaviors in downstream tasks.

</details>


### [55] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
*Hyunsoo Yun,Eun Hak Lee*

Main category: cs.CL

TL;DR: A novel framework called LegiGPT integrates a large language model with explainable AI to analyze transportation-related legislative proposals. It identifies key factors influencing policymaking and shows how political affiliations and other variables affect legislative outcomes.


<details>
  <summary>Details</summary>
Motivation: Understanding the impact of lawmakers' political ideologies on policymaking is critically important.

Method: LegiGPT employs a multi-stage filtering and classification pipeline using zero-shot prompting with GPT-4. XAI techniques were applied to examine relationships between party affiliation and associated attributes.

Result: The results reveal that the number and proportion of conservative and progressive sponsors, along with district size and electoral population, are critical determinants shaping legislative outcomes.

Conclusion: This integrated approach provides a valuable tool for understanding legislative dynamics and guiding future policy development, with broader implications for infrastructure planning and governance.

Abstract: Given the significant influence of lawmakers' political ideologies on
legislative decision-making, understanding their impact on policymaking is
critically important. We introduce a novel framework, LegiGPT, which integrates
a large language model (LLM) with explainable artificial intelligence (XAI) to
analyze transportation-related legislative proposals. LegiGPT employs a
multi-stage filtering and classification pipeline using zero-shot prompting
with GPT-4. Using legislative data from South Korea's 21st National Assembly,
we identify key factors - including sponsor characteristics, political
affiliations, and geographic variables - that significantly influence
transportation policymaking. The LLM was used to classify
transportation-related bill proposals through a stepwise filtering process
based on keywords, phrases, and contextual relevance. XAI techniques were then
applied to examine relationships between party affiliation and associated
attributes. The results reveal that the number and proportion of conservative
and progressive sponsors, along with district size and electoral population,
are critical determinants shaping legislative outcomes. These findings suggest
that both parties contributed to bipartisan legislation through different forms
of engagement, such as initiating or supporting proposals. This integrated
approach provides a valuable tool for understanding legislative dynamics and
guiding future policy development, with broader implications for infrastructure
planning and governance.

</details>


### [56] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
*Bin Chen,Xinzge Gao,Chuanrui Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

</details>


### [57] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
*Xinyi Liu,Weiguang Wang,Hangfeng He*

Main category: cs.CL

TL;DR: 本文研究了提示偏差对大型语言模型中认知和偶然不确定性估计的影响。结果表明，减轻提示偏差可以提高不确定性量化效果，并且在低无偏差置信度时，偏差对两种不确定性的影响更大。此外，低无偏差置信度会导致认知不确定性的低估，而对偶然不确定性的方向变化没有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在开放性任务中的广泛应用，准确评估认知不确定性变得至关重要。然而，由于存在偶然不确定性（源于多个有效答案），量化认知不确定性具有挑战性。本文旨在研究提示偏差对不确定性估计的影响，并探索其权衡关系。

Method: 本文通过在视觉问答（VQA）任务上进行实验，研究了提示偏差对认知和偶然不确定性估计的影响。同时，基于先前工作表明当模型置信度较低时，LLMs倾向于复制输入信息，进一步分析了不同无偏差置信度水平下提示偏差对测量的认知和偶然不确定性的影响。

Result: 研究发现，减轻提示引入的偏差可以改善GPT-4o的不确定性量化。所有考虑的偏差在无偏差模型置信度较低时对两种不确定性的影响更大。此外，低无偏差模型置信度会导致认知不确定性的低估（即过度自信），而对偶然不确定性的方向变化没有显著影响。

Conclusion: 本文研究了在大型语言模型（LLMs）中准确评估认知不确定性的重要性，并探讨了提示偏差对认知和偶然不确定性估计的影响。研究发现，减轻提示引入的偏差可以改善GPT-4o的不确定性量化，并且在无偏差模型置信度较低时，偏差对两种不确定性的影响更大。此外，低无偏差模型置信度会导致认知不确定性的低估（即过度自信），而对偶然不确定性的方向变化没有显著影响。这些发现有助于更好地理解偏差缓解对不确定性量化的影响，并可能推动更先进的技术发展。

Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended
tasks, accurately assessing epistemic uncertainty, which reflects a model's
lack of knowledge, has become crucial to ensuring reliable outcomes. However,
quantifying epistemic uncertainty in such tasks is challenging due to the
presence of aleatoric uncertainty, which arises from multiple valid answers.
While bias can introduce noise into epistemic uncertainty estimation, it may
also reduce noise from aleatoric uncertainty. To investigate this trade-off, we
conduct experiments on Visual Question Answering (VQA) tasks and find that
mitigating prompt-introduced bias improves uncertainty quantification in
GPT-4o. Building on prior work showing that LLMs tend to copy input information
when model confidence is low, we further analyze how these prompt biases affect
measured epistemic and aleatoric uncertainty across varying bias-free
confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases
induce greater changes in both uncertainties when bias-free model confidence is
lower. Moreover, lower bias-free model confidence leads to greater
underestimation of epistemic uncertainty (i.e. overconfidence) due to bias,
whereas it has no significant effect on the direction of changes in aleatoric
uncertainty estimation. These distinct effects deepen our understanding of bias
mitigation for uncertainty quantification and potentially inform the
development of more advanced techniques.

</details>


### [58] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
*Daejin Jo,Jeeyoung Yun,Byungseok Roh,Sungwoong Kim*

Main category: cs.CL

TL;DR: 本文提出了一种新的语音分词方法LM-SPT，通过引入语义蒸馏技术，提高了语音和文本之间的对齐效果，并在语音到文本和文本到语音任务中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: Previous methods produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques can distort or dilute the semantic structure required for effective LM alignment.

Method: LM-SPT is a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, it reconstructs speech solely from semantic tokens and minimizes the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder.

Result: LM-SPT achieves superior reconstruction fidelity compared to baselines, and SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.

Conclusion: LM-SPT achieves superior reconstruction fidelity compared to baselines, and SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.

Abstract: With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

</details>


### [59] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
*Lance Ying,Ryan Truong,Katherine M. Collins,Cedegao E. Zhang,Megan Wei,Tyler Brooke-Wilson,Tan Zhi-Xuan,Lionel Wong,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: 本文提出了一种名为LIRAS的框架，用于整合语言和视觉输入以进行特定上下文的社会推理。该框架利用多模态语言模型将语言和视觉输入解析为统一的符号表示，并通过贝叶斯逆向规划引擎生成细粒度的概率判断。实验结果表明，该模型在捕捉人类判断方面优于现有的消融实验和最先进的模型。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中进行社会推理通常需要考虑来自多个模态的信息。语言是社会环境中特别强大的信息来源，特别是在新情境中，语言可以提供关于环境动态的抽象信息和无法轻易通过视觉观察到的代理的具体细节。

Method: 我们提出了Language-Informed Rational Agent Synthesis (LIRAS)，这是一个框架，用于绘制整合语言和视觉输入的特定上下文的社会推理。LIRAS将多模态社会推理视为构建结构化但情境特定的代理和环境表示的过程，利用多模态语言模型将语言和视觉输入解析为统一的符号表示，然后运行贝叶斯逆向规划引擎以产生细粒度的概率判断。

Result: 在一系列从认知科学实验中得出的现有和新社会推理任务上，我们的模型（使用相对轻量级的VLM）在捕捉人类判断方面优于消融实验和最先进的模型。

Conclusion: 我们的模型（使用轻量级视觉语言模型实例化）在捕捉所有领域的真人判断方面优于消融实验和最先进的模型。

Abstract: Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

</details>


### [60] [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
*Zhuang Chen,Yaru Cao,Guanqun Bi,Jincenzi Wu,Jinfeng Zhou,Xiyao Xiao,Si Chen,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: This paper introduces SocialSim, a framework for simulating emotional support conversation by integrating social disclosure and social awareness. It constructs a synthetic ESC corpus called SSConv, which surpasses crowdsourced data in quality. A chatbot trained on SSConv shows excellent performance in evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to ESC dialogue augmentation largely overlook the social dynamics inherent in ESC, leading to less effective simulations. The high cost of crowdsourcing a large ESC corpus also poses a challenge.

Method: SocialSim is a novel framework that simulates ESC by integrating key aspects of social interactions: social disclosure and social awareness. On the seeker side, a comprehensive persona bank is constructed to facilitate social disclosure. On the supporter side, cognitive reasoning is elicited to generate logical and supportive responses. SSConv, a large-scale synthetic ESC corpus, is built upon SocialSim.

Result: SSConv, a large-scale synthetic ESC corpus, is constructed using SocialSim. A chatbot trained on SSConv demonstrates state-of-the-art performance in both automatic and human evaluations.

Conclusion: SocialSim offers a scalable way to synthesize ESC, making emotional care more accessible and practical.

Abstract: Emotional support conversation (ESC) helps reduce people's psychological
stress and provide emotional value through interactive dialogues. Due to the
high cost of crowdsourcing a large ESC corpus, recent attempts use large
language models for dialogue augmentation. However, existing approaches largely
overlook the social dynamics inherent in ESC, leading to less effective
simulations. In this paper, we introduce SocialSim, a novel framework that
simulates ESC by integrating key aspects of social interactions: social
disclosure and social awareness. On the seeker side, we facilitate social
disclosure by constructing a comprehensive persona bank that captures diverse
and authentic help-seeking scenarios. On the supporter side, we enhance social
awareness by eliciting cognitive reasoning to generate logical and supportive
responses. Building upon SocialSim, we construct SSConv, a large-scale
synthetic ESC corpus of which quality can even surpass crowdsourced ESC data.
We further train a chatbot on SSConv and demonstrate its state-of-the-art
performance in both automatic and human evaluations. We believe SocialSim
offers a scalable way to synthesize ESC, making emotional care more accessible
and practical.

</details>


### [61] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
*Lei Jiang,Zixun Zhang,Zizhou Wang,Xiaobing Sun,Zhen Li,Liangli Zhen,Xiaohua Xu*

Main category: cs.CL

TL;DR: 本文提出了一种新的黑盒越狱攻击框架CAMO，通过分解恶意提示并利用跨模态推理能力来隐蔽地重建有害指令，从而绕过传统检测机制。该方法在多个LVLMs上表现出色，显示出当前安全机制的不足，并强调了需要更先进的安全解决方案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）在多模态任务中表现出色，但容易受到越狱攻击，这些攻击会绕过内置的安全机制以生成受限内容。现有的黑盒越狱方法主要依赖于对抗性文本提示或图像扰动，但这些方法很容易被标准内容过滤系统检测到，并且查询和计算效率低下。

Method: 我们提出了跨模态对抗多模态混淆（CAMO），这是一种新颖的黑盒越狱攻击框架，将恶意提示分解为语义上无害的视觉和文本片段。通过利用LVLM的跨模态推理能力，CAMO通过多步骤推理隐蔽地重建有害指令，从而避开传统检测机制。

Result: 我们在领先的LVLMs上进行了全面评估，验证了CAMO的有效性，展示了其强大的性能和强大的跨模型可迁移性。

Conclusion: 这些结果突显了当前内置安全机制的重大漏洞，强调了在视觉-语言系统中需要先进且对齐的安全和安全解决方案的紧迫性。

Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

</details>


### [62] [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
*Heloisa Oss Boll,Antonio Oss Boll,Leticia Puttlitz Boll,Ameen Abu Hanna,Iacer Calixto*

Main category: cs.CL

TL;DR: Distillnote is a framework for LLM-based clinical note summarization that generates efficient and accurate summaries, showing improvements in performance and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: To generate concise summaries of patient information and alleviate the burden of clinical documentation for healthcare providers.

Method: Distillnote framework for LLM-based clinical note summarization, including three techniques: one-step, structured, and distilled summarization.

Result: Distilled summaries achieve 79% text compression and up to 18.2% improvement in AUPRC compared to an LLM trained on full notes. They also show optimal efficiency and reduced hallucinations.

Conclusion: Distilled summaries offer optimal efficiency and significantly reduce hallucinations, while one-step summaries are favoured by clinicians for relevance and clinical actionability.

Abstract: Large language models (LLMs) offer unprecedented opportunities to generate
concise summaries of patient information and alleviate the burden of clinical
documentation that overwhelms healthcare providers. We present Distillnote, a
framework for LLM-based clinical note summarization, and generate over 64,000
admission note summaries through three techniques: (1) One-step, direct
summarization, and a divide-and-conquer approach involving (2) Structured
summarization focused on independent clinical insights, and (3) Distilled
summarization that further condenses the Structured summaries. We test how
useful are the summaries by using them to predict heart failure compared to a
model trained on the original notes. Distilled summaries achieve 79% text
compression and up to 18.2% improvement in AUPRC compared to an LLM trained on
the full notes. We also evaluate the quality of the generated summaries in an
LLM-as-judge evaluation as well as through blinded pairwise comparisons with
clinicians. Evaluations indicate that one-step summaries are favoured by
clinicians according to relevance and clinical actionability, while distilled
summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)
and significantly reduce hallucinations. We release our summaries on PhysioNet
to encourage future research.

</details>


### [63] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
*Muyang Zheng,Yuanzhi Yao,Changting Lin,Rui Wang,Meng Han*

Main category: cs.CL

TL;DR: MIST is a new method for jailbreaking black-box large language models by iteratively refining prompts to induce harmful content while preserving semantic intent.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of jailbreaking black-box LLMs, which are considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget.

Method: MIST (Iterative Semantic Tuning) is a method that enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. It incorporates two key strategies: sequential synonym search and order-determining optimization.

Result: Extensive experiments across two open-source models and four closed-source models demonstrate that MIST achieves competitive attack success rates and attack transferability compared with other state-of-the-art white-box and black-box jailbreak methods. Additionally, experiments on computational efficiency validate the practical viability of MIST.

Conclusion: MIST is a practical and effective method for jailbreaking black-box large language models, achieving competitive attack success rates and transferability compared to other state-of-the-art methods.

Abstract: Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

</details>


### [64] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: 本研究分析了不同架构和大小的模型在学习和保留罕见信息方面的表现，发现大多数模型在高频事实上的表现相似，但在低频事实上的表现存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 样本效率是语言模型的一个关键属性，对于实际应用中的训练效率至关重要。然而，在现实世界文本中，信息遵循长尾分布，模型需要学习和回忆频繁和不频繁的事实。

Method: 通过在训练语料库中标注关系事实的频率，研究模型性能如何随事实频率变化。

Result: 大多数模型在高频事实上的表现相似，但在低频事实上的表现存在显著差异。

Conclusion: 本研究分析了不同架构和大小的模型在学习和保留罕见信息方面的表现，发现大多数模型在高频事实上的表现相似，但在低频事实上的表现存在显著差异。

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [65] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 本文提出了一种新的知识追踪方法，称为语言瓶颈模型（LBM），它通过学习最小的自然语言摘要来提高可解释性，并在准确率上与现有方法相当。


<details>
  <summary>Details</summary>
Motivation: 准确评估学生知识对于有效教育至关重要，但传统的KT方法依赖于不透明的潜在嵌入，限制了可解释性。即使基于LLM的方法也会生成直接预测或摘要，可能会出现幻觉且没有准确性保证。

Method: 将KT重新表述为一个逆问题：学习最小的自然语言摘要，使过去的答案可解释且未来的答案可预测。LBM由一个编写可解释知识摘要的编码器LLM和一个冻结的解码器LLM组成，该解码器只能使用该摘要文本来重建和预测学生回答。

Result: 在合成算术基准和大规模Eedi数据集上的实验表明，LBMs在准确率上可以与最先进的KT和直接LLM方法相媲美，同时需要的学生成绩轨迹少得多。

Conclusion: 实验表明，LBMs在准确率上可以与最先进的KT和直接LLM方法相媲美，同时需要的学生成绩轨迹少得多。通过使用下游解码准确性作为奖励信号进行组相对策略优化，可以有效提高摘要质量。

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [66] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: 本文介绍了TeXpert，一个用于评估大型语言模型生成LaTeX代码能力的基准数据集，并分析了不同模型的表现，发现开源模型在LaTeX任务中表现优异，但存在格式和包错误的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试完全缺乏对使用自然语言指令生成LaTeX代码能力的评估，而大型语言模型（LLMs）为研究人员提供了生成出版就绪材料的潜力。

Method: 通过引入TeXpert，我们创建了一个包含自然语言提示的基准数据集，用于生成LaTeX代码，专注于科学文档的不同组件，并在多个难度级别上进行评估。

Result: 我们的评估揭示了LLMs在LaTeX生成方面的表现：在标准基准测试中表现出色的LLMs在LaTeX生成方面表现较差，随着任务复杂性的增加，准确性显著下降；开源模型如DeepSeek v3和DeepSeek Coder在LaTeX任务中与闭源模型相当；格式和包错误出乎意料地普遍，表明大多数LLMs的训练数据集中缺乏多样化的LaTeX示例。

Conclusion: 我们的数据集、代码和模型评估已在https://github.com/knowledge-verse-ai/TeXpert上公开。

Abstract: LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [67] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
*Mikhail Menschikov,Dmitry Evseev,Ruslan Kostoev,Ilya Perepechkin,Ilnaz Salimov,Victoria Dochkina,Petr Anokhin,Evgeny Burnaev,Nikita Semenov*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的外部记忆方法，用于个性化语言模型，以更好地处理用户历史信息。通过引入组合图结构，实验结果显示该方法在多个基准测试中表现良好，即使在引入时间参数和矛盾陈述的情况下，系统性能依然稳健。


<details>
  <summary>Details</summary>
Motivation: 个性化语言模型，特别是能够在交互中考虑用户历史的能力，是一个重要问题。尽管大型语言模型（LLMs）和检索增强生成的进步提高了LLMs的事实基础，但保留大量个人资料并用其生成个性化响应的任务仍然相关。

Method: 我们提出了使用外部记忆，即由LLM本身构建和更新的知识图谱。我们扩展了AriGraph架构的想法，并首次引入了一个包含标准边和两种类型的超边的组合图。

Result: 实验表明，这种方法有助于统一和增强图构建和知识提取过程。此外，在DiaASQ基准测试中引入时间参数和矛盾陈述后，问答系统的性能仍然稳健。

Conclusion: 该方法有助于统一和增强图构建和知识提取过程，并且在引入时间参数和矛盾陈述后，问答系统的性能仍然稳健，证明了所提出架构能够维持和利用时间依赖性。

Abstract: The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

</details>


### [68] [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
*Danielle R. Thomas,Conrad Borchers,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: 本研究探讨了LLM生成的反馈对学习的影响。结果表明，LLM反馈在某些情况下可以提高学习效果，但其有效性取决于学习者寻求支持的倾向。此外，LLM反馈不会显著增加完成时间，并且学习者普遍认为它有帮助。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地用于生成反馈，但它们对学习的影响仍缺乏研究，尤其是在与现有的反馈方法相比时。本研究探讨了按需LLM生成的解释性反馈如何影响七个基于情景的导师培训课程中的学习。

Method: 本研究分析了885名导师学习者完成的2600多个课程，比较了三组学习者的后测表现：收到gpt-3.5-turbo生成反馈的学习者、拒绝反馈的学习者以及没有访问权限的学习者。所有组都收到了非LLM的纠正反馈。为了解决潜在的选择偏差，应用了倾向评分。

Result: 具有较高LLM反馈参与可能性的学习者在后测中的得分显著高于那些可能性较低的学习者。调整了这种影响后，七门课程中有两门显示出统计学上显著的学习益处，标准化效应量分别为0.28和0.33。

Conclusion: 这些中等效果表明，LLM反馈的有效性取决于学习者寻求支持的倾向。重要的是，LLM反馈没有显著增加完成时间，并且学习者普遍将其评为有帮助的。这些发现突显了LLM反馈在开放性任务中提高学习的潜力，尤其是在已经提供反馈但不使用LLM的现有系统中。

Abstract: Large language models (LLMs) are increasingly used to generate feedback, yet
their impact on learning remains underexplored, especially compared to existing
feedback methods. This study investigates how on-demand LLM-generated
explanatory feedback influences learning in seven scenario-based tutor training
lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we
compare posttest performance among learners across three groups: learners who
received feedback generated by gpt-3.5-turbo, those who declined it, and those
without access. All groups received non-LLM corrective feedback. To address
potential selection bias-where higher-performing learners may be more inclined
to use LLM feedback-we applied propensity scoring. Learners with a higher
predicted likelihood of engaging with LLM feedback scored significantly higher
at posttest than those with lower propensity. After adjusting for this effect,
two out of seven lessons showed statistically significant learning benefits
from LLM feedback with standardized effect sizes of 0.28 and 0.33. These
moderate effects suggest that the effectiveness of LLM feedback depends on the
learners' tendency to seek support. Importantly, LLM feedback did not
significantly increase completion time, and learners overwhelmingly rated it as
helpful. These findings highlight LLM feedback's potential as a low-cost and
scalable way to improve learning on open-ended tasks, particularly in existing
systems already providing feedback without LLMs. This work contributes open
datasets, LLM prompts, and rubrics to support reproducibility.

</details>


### [69] [Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
*Giuseppe Attanasio,Sonal Sannigrahi,Ben Peters,André F. T. Martins*

Main category: cs.CL

TL;DR: 本文提出了一个统一的语音到文本模型，用于IWSLT 2025共享任务中的指令遵循语音处理。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过使用小规模语言模型和高质量数据来提高语音处理的效果。

Method: 本文提出了一种通过模态对齐和指令微调的统一语音到文本模型。

Result: 本文提交了语音识别、翻译和口语问答的结果。

Conclusion: 本文提出了一个统一的语音到文本模型，用于IWSLT 2025共享任务中的指令遵循语音处理。

Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on
Instruction Following Speech Processing. We submit results for the Short Track,
i.e., speech recognition, translation, and spoken question answering. Our model
is a unified speech-to-text model that integrates a pre-trained continuous
speech encoder and text decoder through a first phase of modality alignment and
a second phase of instruction fine-tuning. Crucially, we focus on using
small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY
data along with synthetic data generation to supplement existing resources.

</details>


### [70] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: MUCAR is a new benchmark for evaluating how well multimodal models can resolve ambiguities in language and images, and it shows that there's still a lot of room for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal benchmarks overlook linguistic and visual ambiguities, failing to exploit the mutual clarification potential between modalities.

Method: Introduce MUCAR, a novel benchmark for evaluating multimodal ambiguity resolution across multilingual and cross-modal scenarios.

Result: Extensive evaluations involving 19 state-of-the-art multimodal models reveal substantial gaps compared to human-level performance.

Conclusion: MUCAR benchmark reveals substantial gaps compared to human-level performance, highlighting the need for future research into more sophisticated cross-modal ambiguity comprehension methods.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


### [71] [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
*Dominik Macháček,Peter Polák*

Main category: cs.CL

TL;DR: 本文介绍了Charles University在IWSLT 2025同步语音翻译任务中的系统，使用了离线Whisper模型和新的同时策略AlignAtt，并通过提示注入领域术语和考虑上下文来提高性能。在多个语言对上取得了显著的BLEU提升，并提出了新的语音识别延迟度量方法。


<details>
  <summary>Details</summary>
Motivation: 本文描述了Charles University提交到IWSLT 2025的同步语音翻译任务的系统。我们覆盖了所有四个语言对，并尝试了直接或级联的方法。

Method: 我们使用了离线Whisper语音模型，结合最新的同时策略AlignAtt进行翻译和转录。我们通过提示注入领域术语来提高性能，并考虑了上下文。我们的级联系统还使用EuroLLM进行无限制的同时翻译。

Result: 与组织者的基线相比，我们的系统在Czech到English上提高了2个BLEU点，在English到German、Chinese和Japanese上提高了13-22个BLEU点。

Conclusion: 我们的系统在Czech到English上提高了2个BLEU点，在English到German、Chinese和Japanese上提高了13-22个BLEU点。我们还提出了一种新的语音识别延迟度量方法。

Abstract: This paper describes Charles University submission to the Simultaneous Speech
Translation Task of the IWSLT 2025. We cover all four language pairs with a
direct or cascade approach. The backbone of our systems is the offline Whisper
speech model, which we use for both translation and transcription in
simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We
further improve the performance by prompting to inject in-domain terminology,
and we accommodate context. Our cascaded systems further use EuroLLM for
unbounded simultaneous translation. Compared to the Organizers' baseline, our
systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on
English to German, Chinese and Japanese on the development sets. Additionally,
we also propose a new enhanced measure of speech recognition latency.

</details>


### [72] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
*Ricardo Rei,Nuno M. Guerreiro,José Pombal,João Alves,Pedro Teixeirinha,Amin Farajian,André F. T. Martins*

Main category: cs.CL

TL;DR: 本文介绍了Tower+，一个在翻译和多语言通用文本能力上都表现出色的模型套件。通过一种新的训练方法，我们在翻译专业化和多语言通用能力之间实现了帕累托前沿。我们的模型在多个基准测试中表现出色，证明了在优化特定领域的同时保持强大通用能力的可能性。


<details>
  <summary>Details</summary>
Motivation: 微调预训练大语言模型虽然在特定任务上表现优异，但往往牺牲了通用能力，这限制了系统在需要多种技能的现实应用中的实用性。因此，我们需要一种方法来平衡翻译专业能力和多语言通用能力。

Method: 我们引入了Tower+，这是一个模型套件，旨在实现翻译和多语言通用文本能力的高性能。我们通过一种新的训练方法实现了翻译专业化和多语言通用能力之间的帕累托前沿，该方法包括持续预训练、监督微调、偏好优化和具有可验证奖励的强化学习。

Result: 我们的小型模型经常优于更大的通用开源和专有大语言模型。我们的最大模型在高资源语言的翻译性能上达到最佳水平，并在多语言Arena Hard评估和IF-MT基准测试中取得顶级结果。

Conclusion: 我们的研究结果表明，可以在优化特定业务领域（如翻译和本地化）的同时，与前沿模型在通用能力上相媲美。

Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for
reaching state-of-the-art performance on specific tasks like machine
translation. However, this process of adaptation often implies sacrificing
general-purpose capabilities, such as conversational reasoning and
instruction-following, hampering the utility of the system in real-world
applications that require a mixture of skills. In this paper, we introduce
Tower+, a suite of models designed to deliver strong performance across both
translation and multilingual general-purpose text capabilities. We achieve a
Pareto frontier between translation specialization and multilingual
general-purpose capabilities by introducing a novel training recipe that builds
on Tower (Alves et al., 2024), comprising continued pretraining, supervised
fine-tuning, preference optimization, and reinforcement learning with
verifiable rewards. At each stage of training, we carefully generate and curate
data to strengthen performance on translation as well as general-purpose tasks
involving code generation, mathematics problem solving, and general
instruction-following. We develop models at multiple scales: 2B, 9B, and 72B.
Our smaller models often outperform larger general-purpose open-weight and
proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers
best-in-class translation performance for high-resource languages and top
results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we
introduce for evaluating both translation and instruction-following. Our
findings highlight that it is possible to rival frontier models in general
capabilities, while optimizing for specific business domains, such as
translation and localization.

</details>


### [73] [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
*Jiahao Cheng,Tiancheng Su,Jia Yuan,Guoxiu He,Jiawei Liu,Xinqi Tao,Jingwen Xie,Huaxia Li*

Main category: cs.CL

TL;DR: 本研究探讨了链式思维提示对大型语言模型幻觉检测的影响，发现尽管它能减少幻觉频率，但也会削弱检测效果。


<details>
  <summary>Details</summary>
Motivation: 为了填补这一空白，我们进行了系统性的实证评估，以了解CoT提示对幻觉检测的影响。

Method: 我们进行了系统的实证评估，包括一个试点实验和对各种CoT提示方法的影响评估。

Result: 虽然CoT提示有助于减少幻觉频率，但它往往会掩盖用于检测的关键信号，从而降低各种检测方法的效果。

Conclusion: 我们的研究突显了在使用推理时被忽视的权衡。

Abstract: Large Language Models (LLMs) often exhibit \textit{hallucinations},
generating factually incorrect or semantically irrelevant content in response
to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by
encouraging step-by-step reasoning, but its impact on hallucination detection
remains underexplored. To bridge this gap, we conduct a systematic empirical
evaluation. We begin with a pilot experiment, revealing that CoT reasoning
significantly affects the LLM's internal states and token probability
distributions. Building on this, we evaluate the impact of various CoT
prompting methods on mainstream hallucination detection methods across both
instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three
key dimensions: changes in hallucination score distributions, variations in
detection accuracy, and shifts in detection confidence. Our findings show that
while CoT prompting helps reduce hallucination frequency, it also tends to
obscure critical signals used for detection, impairing the effectiveness of
various detection methods. Our study highlights an overlooked trade-off in the
use of reasoning. Code is publicly available at:
https://anonymous.4open.science/r/cot-hallu-detect.

</details>


### [74] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
*Murtaza Nazir,Matthew Finlayson,John X. Morris,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 本文提出了一个名为PILS的新方法，用于从语言模型的输出中恢复隐藏提示。该方法利用了模型在多个生成步骤中的下一个标记概率，并在恢复隐藏提示方面取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 语言模型反转旨在仅使用语言模型输出来恢复隐藏提示。这种能力对语言模型部署中的安全性和问责制有影响，例如从受API保护的语言模型的系统消息中泄露私人信息。

Method: 我们提出了一种新的方法，称为从对数概率序列中反转提示（PILS），该方法通过从模型在多个生成步骤中的下一个标记概率中获取线索来恢复隐藏提示。

Result: 我们的方法在恢复隐藏提示方面取得了巨大的进步，测试集上的精确恢复率提高了2-3.5倍，在一种情况下，恢复率从17%提高到了60%。此外，我们的方法在更具有挑战性的任务中也表现良好。

Conclusion: 我们的方法在恢复隐藏提示方面取得了显著的改进，并且在更具有挑战性的任务中表现出色。此外，我们发现下一个标记的概率是比之前已知的更容易受到反转攻击的攻击面。

Abstract: Language model inversion seeks to recover hidden prompts using only language
model outputs. This capability has implications for security and accountability
in language model deployments, such as leaking private information from an
API-protected language model's system message. We propose a new method --
prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts
by gleaning clues from the model's next-token probabilities over the course of
multiple generation steps. Our method is enabled by a key insight: The
vector-valued outputs of a language model occupy a low-dimensional subspace.
This enables us to losslessly compress the full next-token probability
distribution over multiple generation steps using a linear map, allowing more
output information to be used for inversion. Our approach yields massive gains
over previous state-of-the-art methods for recovering hidden prompts, achieving
2--3.5 times higher exact recovery rates across test sets, in one case
increasing the recovery rate from 17% to 60%. Our method also exhibits
surprisingly good generalization behavior; for instance, an inverter trained on
16 generations steps gets 5--27 points higher prompt recovery when we increase
the number of steps to 32 at test time. Furthermore, we demonstrate strong
performance of our method on the more challenging task of recovering hidden
system messages. We also analyze the role of verbatim repetition in prompt
recovery and propose a new method for cross-family model transfer for
logit-based inverters. Our findings show that next-token probabilities are a
considerably more vulnerable attack surface for inversion attacks than
previously known.

</details>


### [75] [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
*Adithya Bhaskar,Alexander Wettig,Tianyu Gao,Yihe Dong,Danqi Chen*

Main category: cs.CL

TL;DR: 本文提出了KV占用作为统一指标，评估了不同方法在保持性能的同时所能达到的最小占用。此外，还提出了PruLong，一种端到端优化方法，用于学习哪些注意力头需要保留完整的KV缓存。结果表明，PruLong在保持长上下文性能的同时节省了内存，比之前的方法减少了12%的KV占用。


<details>
  <summary>Details</summary>
Motivation: 许多先前的工作提出了丢弃KV的方法，但它们的方法是针对有利设置的，这使得高峰值内存和性能下降等缺点变得模糊，而且难以公平比较不同方法。

Method: 本文提出了KV占用作为统一指标，评估了不同方法在保持性能的同时所能达到的最小占用。此外，还提出了PruLong，一种端到端优化方法，用于学习哪些注意力头需要保留完整的KV缓存。

Result: KV占用指标揭示了先前KV驱逐方法的高峰值内存。其中一类方法（后填充驱逐）由于与预填充期间的驱逐不兼容而具有较高的占用。通过调整这些方法，使其能够在预填充期间驱逐KV，从而显著降低KV占用。PruLong在保持长上下文性能的同时节省了内存，在具有挑战性的回忆任务中保持了性能，比之前的方法减少了12%的KV占用。

Conclusion: 本文澄清了长上下文推理方法的复杂性，并为未来减少KV占用铺平了道路。

Abstract: Language models handle increasingly long contexts for tasks such as book
summarization, but this leads to growing memory costs for the key-value (KV)
cache. Many prior works have proposed ways of discarding KVs from memory, but
their approaches are tailored to favorable settings, obscuring caveats like
high peak memory and performance degradation, and a fair comparison between
methods is difficult. In this paper, we propose the *KV footprint* as a unified
metric, which accounts for both the amount of KV entries stored and their
lifespan in memory. We evaluate methods based on the smallest footprint they
attain while preserving performance in both long-context understanding and
generation, with context lengths of up to 128K tokens. This metric reveals the
high peak memory of prior KV eviction methods. One class of methods --
*post-fill eviction* -- has a high footprint due to being incompatible with
eviction during pre-filling. We adapt these methods to be able to evict KVs
during pre-filling, achieving substantially lower KV footprints. We then turn
to *recency eviction* methods, wherein we propose PruLong, an end-to-end
optimization method for learning which attention heads need to retain the full
KV cache and which do not. PruLong saves memory while preserving long-context
performance, achieving 12% smaller KV footprint than prior methods while
retaining performance in challenging recall tasks. Our paper clarifies the
complex tangle of long-context inference methods and paves the way for future
development to minimize the KV footprint.

</details>


### [76] [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Main category: cs.CL

TL;DR: The paper introduces CLEAR-3K, a dataset to evaluate language models' ability to distinguish between semantic relatedness and causal explanations. It finds that models often confuse the two and that performance plateaus despite increasing parameter size.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate whether language models can determine if one statement causally explains another, as this is an essential capability for applications requiring accurate assessment of causal relationships.

Method: The paper introduces CLEAR-3K, a dataset of 3,000 assertion-reasoning questions, and evaluates 21 state-of-the-art language models to assess their ability to distinguish between semantic relatedness and genuine causal explanatory relationships.

Result: The evaluation reveals that language models frequently confuse semantic similarity with causality and that performance plateaus at 0.55, even for the best-performing models.

Conclusion: CLEAR-3K provides a crucial benchmark for developing and evaluating genuine causal reasoning in language models, which is an essential capability for applications that require accurate assessment of causal relationships.

Abstract: We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions
designed to evaluate whether language models can determine if one statement
causally explains another. Each question present an assertion-reason pair and
challenge language models to distinguish between semantic relatedness and
genuine causal explanatory relationships. Through comprehensive evaluation of
21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we
identify two fundamental findings. First, language models frequently confuse
semantic similarity with causality, relying on lexical and semantic overlap
instead of inferring actual causal explanatory relationships. Second, as
parameter size increases, models tend to shift from being overly skeptical
about causal relationships to being excessively permissive in accepting them.
Despite this shift, performance measured by the Matthews Correlation
Coefficient plateaus at just 0.55, even for the best-performing models.Hence,
CLEAR-3K provides a crucial benchmark for developing and evaluating genuine
causal reasoning in language models, which is an essential capability for
applications that require accurate assessment of causal relationships.

</details>


### [77] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
*Yuchen Li,Hengyi Cai,Rui Kong,Xinran Chen,Jiamin Chen,Jun Yang,Haojie Zhang,Jiayi Li,Jiayi Wu,Yiqun Chen,Changle Qu,Keyi Kong,Wenwen Ye,Lixin Su,Xinyu Ma,Long Xia,Daiting Shi,Jiashu Zhao,Haoyi Xiong,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文介绍了一种新的AI搜索范式，通过四个LLM驱动的代理协作，以应对从简单事实查询到复杂多阶段推理任务的各种信息需求。


<details>
  <summary>Details</summary>
Motivation: 当前的搜索系统在处理复杂多阶段推理任务时存在局限性，因此需要一种新的范式来模拟人类的信息处理和决策过程。

Method: 本文提出了AI搜索范式，采用四个由LLM驱动的代理（Master、Planner、Executor和Writer）组成的模块化架构，这些代理通过协调的工作流程动态适应各种信息需求。

Result: 本文系统地介绍了实现该范式的重点方法，包括任务规划和工具集成、执行策略、对齐且稳健的检索增强生成以及高效的LLM推理，涵盖了算法技术和基础设施优化。

Conclusion: 本文旨在为开发可信赖、适应性强和可扩展的AI搜索系统提供指导。

Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

</details>


### [78] [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
*Kathleen C. Fraser,Hillary Dawkins,Isar Nejadgholi,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: 本文探讨了微调大型语言模型可能导致安全对齐功能丧失的问题，并指出需要可靠和可重复的安全评估来缓解这一问题。实验结果显示，安全评估结果在实验程序和模型的随机性方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 由于微调的广泛使用以及“攻击”的良性性质，微调导致安全对齐功能丧失的问题被认为是一个关键的失败模式。大多数善意开发者可能没有意识到他们正在部署一个安全性降低的大型语言模型。同时，这种已知的漏洞可以被恶意行为者利用以绕过安全防护措施。因此，需要可靠的和可重复的安全评估来缓解这一问题。

Method: 本文研究了安全基准测试对实验程序中的微小变化以及大型语言模型的随机性的鲁棒性。通过初始实验，发现即使在看似无关紧要的微调设置更改后，安全评估的结果也会出现惊人的差异。

Result: 本文的初步实验显示，即使在微调设置中做出看似无关紧要的更改，安全评估的结果也会出现惊人的差异。这表明当前的安全基准测试在实验程序和大型语言模型的随机性方面存在显著差异。

Conclusion: 本文指出，微调大型语言模型可能会导致安全对齐功能的丧失，即使微调数据中没有有害内容。此外，本文强调了需要可靠和可重复的安全评估来缓解这一问题，并指出当前安全基准测试在实验程序和大型语言模型的随机性方面存在显著差异。

Abstract: Fine-tuning a general-purpose large language model (LLM) for a specific
domain or task has become a routine procedure for ordinary users. However,
fine-tuning is known to remove the safety alignment features of the model, even
when the fine-tuning data does not contain any harmful content. We consider
this to be a critical failure mode of LLMs due to the widespread uptake of
fine-tuning, combined with the benign nature of the "attack". Most
well-intentioned developers are likely unaware that they are deploying an LLM
with reduced safety. On the other hand, this known vulnerability can be easily
exploited by malicious actors intending to bypass safety guardrails. To make
any meaningful progress in mitigating this issue, we first need reliable and
reproducible safety evaluations. In this work, we investigate how robust a
safety benchmark is to trivial variations in the experimental procedure, and
the stochastic nature of LLMs. Our initial experiments expose surprising
variance in the results of the safety evaluation, even when seemingly
inconsequential changes are made to the fine-tuning setup. Our observations
have serious implications for how researchers in this field should report
results to enable meaningful comparisons in the future.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [79] [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 本文主张整合可靠测量和因果推断标准，以建立AI心理学的坚实科学，并强调需要发展计算心理概念的类比和明确的证据标准。


<details>
  <summary>Details</summary>
Motivation: 当前将人类测量工具应用于大型语言模型可能导致矛盾结果，引发对许多发现是否为测量幻象的担忧。

Method: 本文提出了一个双重有效性框架，以指导这一整合，并讨论了不同科学目标所需的证据规模。

Result: 当前实践未能满足这些要求，通常将统计模式匹配视为心理现象的证据。同一模型输出需要不同的验证策略，具体取决于研究者声称要测量、描述、模拟或建模的心理学概念。

Conclusion: 构建AI心理学的坚实科学需要整合可靠测量原则和良好的因果推断标准，并发展计算心理概念的类比和明确的证据标准。

Abstract: Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

</details>


### [80] [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 本文提出了一个使用大型语言模型（LLMs）作为心理模拟器的框架，旨在帮助研究人员应对LLMs在心理学研究中的挑战，并利用其独特的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）为心理学和行为研究提供了新的机会，但缺乏方法指导。本文旨在填补这一空白，提供一个框架，帮助研究人员有效利用LLMs。

Method: 本文介绍了两种主要应用：模拟角色和人格以探索多样化的情境，以及作为计算模型来研究认知过程。对于模拟，提出了开发基于心理学的人格的方法，并提供了验证策略和使用案例。对于认知建模，综合了新兴方法、方法论进展和将模型行为与人类认知联系起来的策略。

Result: 本文提出了一种框架，整合了关于LLMs性能的最新实证证据，包括系统性偏差、文化限制和提示脆弱性，以帮助研究人员应对这些挑战并利用LLMs的独特能力。

Conclusion: 本文提出了一个使用大型语言模型（LLMs）作为心理模拟器的框架，旨在帮助研究人员应对LLMs在心理学研究中的挑战，并利用其独特的能力。

Abstract: Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [81] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 本文提出了一种基于抽象语法树的结构感知分块方法，以提高代码生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于行的分块启发式方法经常破坏语义结构，分割函数或合并不相关的代码，这可能会影响生成质量。

Method: 我们通过抽象语法树（AST）进行分块，这是一种结构感知的方法，递归地将大的AST节点分成较小的块，并在尊重大小限制的同时合并同级节点。

Result: 这种方法在各种代码生成任务中提高了性能，例如在RepoEval检索中Recall@5提升了4.3个点，在SWE-bench生成中Pass@1提升了2.67个点。

Conclusion: 我们的工作突显了结构感知分块在扩展检索增强代码智能中的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


### [82] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 本文对SWE-Bench排行榜的所有提交进行了全面研究，发现专有大型语言模型占主导地位，解决方案设计多样，并且贡献者范围广泛。


<details>
  <summary>Details</summary>
Motivation: 由于提交过程不需要详细文档，许多解决方案的架构设计和来源仍然不明确。因此，本文旨在对这些解决方案进行首次全面研究。

Method: 本文对SWE-Bench Lite（68项）和SWE-Bench Verified（79项）排行榜的所有提交进行了全面分析，涵盖了提交者类型、产品可用性、大型语言模型使用情况和系统架构等维度。

Result: 本文发现专有大型语言模型（尤其是Claude 3.5/3.7）在解决方案中占主导地位，同时存在代理和非代理设计，并且贡献者范围从个人开发者到大型科技公司。

Conclusion: 本文对SWE-Bench Lite和SWE-Bench Verified排行榜的所有提交进行了首次全面研究，分析了67种独特的解决方案，并揭示了专有大型语言模型的主导地位、代理和非代理设计的存在以及贡献者范围的广泛性。

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [83] [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
*Jushaan Singh Kalra,Xinran Zhao,To Eun Kim,Fengyu Cai,Fernando Diaz,Tongshuang Wu*

Main category: cs.IR

TL;DR: 本文提出了一种混合检索器的方法，通过动态选择和整合多个检索器来提高检索效果，实验结果显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常基于启发式选择单一的检索器，这无法适应多样化的信息需求。我们希望动态选择和整合多个检索器以提高效果。

Method: 我们引入了检索器的混合方法，这是一种零样本、加权的异构检索器组合。

Result: 实验结果表明，这种混合方法在参数总量仅为0.8B的情况下，平均性能优于每个单独的检索器和更大的7B模型。此外，该框架还能有效结合非权威的人类信息源，提升性能。

Conclusion: 我们的实验表明，这种混合方法在性能上优于单独的检索器和更大的模型，并且可以有效地结合非权威的人类信息源以提高性能。

Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.

</details>


### [84] [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
*Fengyu Cai,Tong Chen,Xinran Zhao,Sihao Chen,Hongming Zhang,Sherry Tongshuang Wu,Iryna Gurevych,Heinz Koeppl*

Main category: cs.IR

TL;DR: Revela是一个基于语言建模的自监督检索器学习框架，能够有效提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 由于标注的查询-文档对在专业领域中难以获取，因此需要一种自监督方法来训练检索器。

Method: Revela通过语言建模引入了一个统一且可扩展的自监督检索器学习框架，利用文档间的语义依赖关系，并通过批内注意力机制进行优化。

Result: Revela在NDCG@10指标上分别比之前的最佳方法提高了5.2%（18.3%相对）和5.6%（14.4%相对）。

Conclusion: Revela在通用领域（BEIR）和特定领域（CoIR）基准测试中表现出色，展示了其在自监督检索器学习中的有效性。

Abstract: Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [85] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
*Sophie Chiang,Guy Laban,Hatice Gunes*

Main category: cs.HC

TL;DR: 这项研究探讨了机器人对话是否与人类治疗对话相似，并发现机器人响应在语义上与人类治疗师的响应有很强的重叠。


<details>
  <summary>Details</summary>
Motivation: 随着对话代理越来越多地参与情感支持对话，了解他们的互动与传统治疗环境中的互动有多接近非常重要。

Method: 我们使用句子嵌入和K均值聚类，通过基于距离的聚类拟合方法评估跨代理的主题一致性，并使用欧几里得距离进行验证。

Result: 结果显示，90.88%的机器人对话披露可以映射到人类治疗数据集的聚类中，这表明共享主题结构。对于匹配的聚类，我们使用Transformer、Word2Vec和BERT嵌入比较了主体以及治疗师和机器人响应，揭示了两个数据集中主体披露以及在类似人类披露主题上的响应之间存在强烈的语义重叠。

Conclusion: 这些发现突显了机器人主导的支持对话的相似性和界限，以及它们在增强心理健康干预方面的潜力。

Abstract: As conversational agents increasingly engage in emotionally supportive
dialogue, it is important to understand how closely their interactions resemble
those in traditional therapy settings. This study investigates whether the
concerns shared with a robot align with those shared in human-to-human (H2H)
therapy sessions, and whether robot responses semantically mirror those of
human therapists. We analyzed two datasets: one of interactions between users
and professional therapists (Hugging Face's NLP Mental Health Conversations),
and another involving supportive conversations with a social robot (QTrobot
from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence
embeddings and K-means clustering, we assessed cross-agent thematic alignment
by applying a distance-based cluster-fitting method that evaluates whether
responses from one agent type map to clusters derived from the other, and
validated it using Euclidean distances. Results showed that 90.88% of robot
conversation disclosures could be mapped to clusters from the human therapy
dataset, suggesting shared topical structure. For matched clusters, we compared
the subjects as well as therapist and robot responses using Transformer,
Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'
disclosures in both datasets, as well as in the responses given to similar
human disclosure themes across agent types (robot vs. human therapist). These
findings highlight both the parallels and boundaries of robot-led support
conversations and their potential for augmenting mental health interventions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [86] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Main category: cs.LG

TL;DR: BASE-Q is a new approach that improves rotational quantization by reducing errors and enabling blockwise optimization.


<details>
  <summary>Details</summary>
Motivation: Current rotational quantization methods have limitations in aligning channel means and increasing energy loss due to clipping errors.

Method: BASE-Q combines bias correction and asymmetric scaling to address the limitations of current rotational quantization methods.

Result: BASE-Q narrowed the accuracy gap to full-precision models by 50.5%, 42.9%, and 29.2% compared to QuaRot, SpinQuant, and OSTQuant, respectively.

Conclusion: BASE-Q is effective in reducing rounding and clipping errors and narrowing the accuracy gap to full-precision models.

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [87] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Main category: cs.LG

TL;DR: LFPS is an acceleration method that improves decoding efficiency in long-context LLMs by leveraging historical attention patterns and predicting Top-k indices efficiently.


<details>
  <summary>Details</summary>
Motivation: The memory demand for key-value (KV) caches during decoding grows rapidly, becoming a critical bottleneck in GPU memory capacity and PCIe bandwidth. Sparse attention mechanisms alleviate this issue but have significant computational and data transfer overhead due to traversing all key vectors.

Method: LFPS (Learn From the Past for Sparse Indexing) is an acceleration method that dynamically constructs sparse indexing candidates based on historical attention patterns. It captures vertical and slash patterns in decoder attention and incorporates a positional expansion strategy to predict the Top-k indices for the current step.

Result: LFPS achieves up to 22.8× speedup over full attention and 9.6× speedup over exact Top-k retrieval on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively, while preserving generation accuracy.

Conclusion: LFPS offers a practical and efficient solution for decoding optimization in long-context LLM inference.

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [88] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 本文提出了一种可学习的双侧短时拉普拉斯变换（STLT）机制，作为Transformer中传统自注意力的替代方案。该方法通过引入可训练参数，实现了对衰减率、振荡频率和窗口带宽的端到端学习，从而提高了模型的灵活性和效率。实验结果表明，该方法在多个任务中表现优异，并能处理超长序列。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制在处理超长序列时存在计算瓶颈，因此需要一种更高效且可扩展的替代方法。

Method: 提出了一种创新的可学习双侧短时拉普拉斯变换（STLT）机制，以取代基于Transformer的LLM中的传统自注意力。STLT为每个拉普拉斯节点引入了可训练参数，使模型能够端到端地学习衰减率、振荡频率和窗口带宽T。通过选择S个可学习节点并利用快速递归卷积，实现了时间和内存的有效复杂度。进一步结合了基于FFT的相关矩阵计算和自适应节点分配机制，以动态调整活动拉普拉斯节点的数量。

Result: 实验结果表明，该方法在语言建模（WikiText-103，Project Gutenberg）、机器翻译（WMT'14 En-De）和长文档问答（NarrativeQA）任务中，取得了与现有高效Transformer相当或更好的困惑度和得分，并能自然扩展到超过100k标记的上下文长度。消融研究证实了可学习参数和自适应节点分配的重要性。

Conclusion: 该方法结合了可解释性、可扩展性和鲁棒性，为超长序列语言建模提供了一条新路径，避免了自注意力的计算瓶颈。

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [89] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Main category: cs.LG

TL;DR: 本文提出了daDPO方法，用于提升较小语言模型的对话能力，通过结合偏好优化和基于分布的蒸馏，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要关注'黑箱'知识蒸馏，仅使用教师模型的响应，忽略了教师模型的输出分布。本文旨在解决这一问题。

Method: 本文引入了daDPO（Distribution-Aware DPO），这是一种统一的偏好优化和基于分布的蒸馏方法。

Result: 实验结果表明，daDPO在恢复剪枝模型的性能和增强较小的LLM模型方面优于现有方法。例如，20%剪枝的Vicuna1.5-7B可以达到接近教师模型的性能，而Qwen2.5-1.5B有时甚至超过其7B教师模型。

Conclusion: 本文提出了一种新的方法daDPO，该方法在恢复剪枝模型的性能和增强较小的LLM模型方面优于现有方法。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [90] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: 本文介绍了MadaKV，一种用于多模态大语言模型的模态自适应键值缓存淘汰策略，通过动态感知注意力头中的模态信息并自适应保留关键标记，显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 在多模态场景中，注意力头对不同模态有不同偏好，导致模态重要性在注意力头之间存在显著差异。传统的KV缓存淘汰方法无法捕捉模态特定信息，因此性能不佳。

Method: MadaKV通过两个关键组件：模态偏好适应和分层压缩补偿来解决传统KV缓存淘汰方法在多模态场景中的不足。

Result: MadaKV在KV缓存内存占用和模型推理解码延迟方面实现了1.3到1.5倍的改进，同时保持了高准确性。

Conclusion: MadaKV在各种多模态长上下文任务中保持高准确性的同时，显著减少了KV缓存内存占用和模型推理解码延迟，证明了其有效性。

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [91] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Main category: cs.LG

TL;DR: Fractional Reasoning is a new method for improving the performance of large language models by allowing continuous control over reasoning intensity at inference time, which can improve output quality and correctness in different reasoning strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Best-of-N, majority voting, and self-reflection apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth.

Method: Fractional Reasoning is a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor.

Result: Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.

Conclusion: Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [92] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Main category: cs.LG

TL;DR: 本文通过在早期编码阶段对隐藏状态进行稀疏化，实现了语音转录任务的加速，且在1%精度下降的情况下，运行时加速可达1.6倍。


<details>
  <summary>Details</summary>
Motivation: 由于语音音频信号高度可压缩，因此希望通过在神经编码阶段早期进行时间域信号稀疏化来加速神经语音转录，并利用Transformer音频编码器中自注意力机制的可解释性。

Method: 在Whisper模型家族中，对稀疏化阶段（某个编码器层）和压缩比（稀疏度）进行了系统架构搜索。

Result: 在1%精度下降的情况下，最佳解决方案选择在早期编码阶段将隐藏状态稀疏化到40-60%的稀疏度，从而在Nvidia GPU上的英语语音转录任务中实现了高达1.6倍的运行时加速。

Conclusion: 通过在早期编码阶段对隐藏状态进行稀疏化，可以在不进行微调的情况下实现高达1.6倍的运行时加速。

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [93] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [94] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Main category: cs.LG

TL;DR: 该研究探讨了变压器如何在ICL任务中解耦和使用潜在概念，并发现模型能够识别潜在概念并进行逐步概念组合，同时在连续潜在概念任务中发现了低维子空间。


<details>
  <summary>Details</summary>
Motivation: 先前关于ICL的机械工作没有充分探讨学习到的表示与潜在概念之间的关系，且所考虑的问题设置通常只涉及单步推理。因此，需要更深入地理解变压器如何处理潜在概念。

Method: 研究了变压器如何解耦和使用潜在概念，并通过实验验证了模型在处理具有潜在离散概念的2跳推理任务以及参数化连续潜在概念的任务时的表现。

Result: 在具有潜在离散概念的2跳推理任务中，模型成功识别了潜在概念并进行了逐步概念组合。在参数化连续潜在概念的任务中，发现了表示空间中的低维子空间，其几何形状模仿了底层参数化。

Conclusion: 这项工作加深了我们对ICL和变压器表示的理解，并提供了证据，证明模型中存在高度局部化的结构，这些结构在ICL任务中解耦了潜在概念。

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [95] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法SAMD和SAMI，用于解释和控制Transformer模型中的注意力机制，从而提高模型的可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的归因研究主要关注多层感知机神经元并处理相对简单的概念，这忽略了注意力机制的影响，并缺乏分析更复杂概念的统一方法。

Method: 我们引入了SAMD，一种用于将任意复杂概念映射到通用Transformer模型的特定注意力头的概念无关方法。我们还提出了SAMI，一种通过仅使用一个标量参数来调整注意力模块以减弱或增强概念影响的简单策略。

Result: 我们在不同复杂度的概念上验证了SAMD，并可视化了它们对应模块的位置。结果表明，在LLM后训练前后模块位置保持稳定，并确认了关于LLM多语言性的先前工作。通过SAMI，我们通过削弱“安全性”提高了HarmBench的破解成功率（+72.7%），并通过增强“推理”提高了GSM8K基准的性能（+1.6%）。此外，我们通过抑制视觉Transformer在ImageNet上的图像分类准确性，突显了我们方法的领域无关性。

Conclusion: 我们的方法展示了其在不同领域中的通用性，并通过调整注意力模块来增强或削弱特定概念的影响。

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [96] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: 本文提出了 DeepRTL2，这是一种统一处理 RTL 生成和嵌入任务的多功能 LLM，能够在所有评估任务中实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 之前的研究主要关注基于生成的任务，而基于嵌入的任务（如自然语言代码搜索、RTL 代码功能等价检查和性能预测）在 EDA 工作流中同样重要，但被忽视了。

Method: 提出了一种名为 DeepRTL2 的多功能 LLM 家族，统一了与 RTL 相关的生成和嵌入式任务。

Result: 通过广泛的实验，证明 DeepRTL2 在所有评估任务中都实现了最先进的性能。

Conclusion: DeepRTL2 是第一个为 EDA 中的多样化挑战提供全面解决方案的模型，并在所有评估任务中实现了最先进的性能。

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [97] [InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding](https://arxiv.org/abs/2506.15745)
*Minsoo Kim,Kyuhong Shim,Jungwook Choi,Simyung Chang*

Main category: eess.IV

TL;DR: InfiniPot-V 是一种无需训练且与查询无关的框架，可以有效减少KV缓存占用，提高设备上流视频理解的效率。


<details>
  <summary>Details</summary>
Motivation: 现代多模态大语言模型在处理长时间视频时面临KV缓存线性增长的问题，这导致内存超出设备的固定容量。

Method: InfiniPot-V 通过在视频编码过程中监控缓存并在达到用户设置的阈值时运行轻量级压缩来实现内存限制。

Result: InfiniPot-V 在四个开源MLLMs和多个基准测试中显著减少了峰值GPU内存，并保持了实时生成和准确率。

Conclusion: InfiniPot-V 解决了KV缓存瓶颈，使设备上的流视频助手更加高效。

Abstract: Modern multimodal large language models (MLLMs) can reason over hour-long
video, yet their key-value (KV) cache grows linearly with time--quickly
exceeding the fixed memory of phones, AR glasses, and edge robots. Prior
compression schemes either assume the whole video and user query are available
offline or must first build the full cache, so memory still scales with stream
length. InfiniPot-V is the first training-free, query-agnostic framework that
enforces a hard, length-independent memory cap for streaming video
understanding. During video encoding it monitors the cache and, once a user-set
threshold is reached, runs a lightweight compression pass that (i) removes
temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)
keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four
open-source MLLMs and four long-video and two streaming-video benchmarks,
InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,
and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By
dissolving the KV cache bottleneck without retraining or query knowledge,
InfiniPot-V closes the gap for on-device streaming video assistants.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [98] [Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse](https://arxiv.org/abs/2506.16412)
*Paulina DeVito,Akhil Vallala,Sean Mcmahon,Yaroslav Hinda,Benjamin Thaw,Hanqi Zhuang,Hari Kalva*

Main category: cs.SI

TL;DR: 本研究分析了社交媒体数据中的GAI教育话语动态，发现教师和学生对GAI的看法存在显著差异，并提出了基于LLM的框架以更好地理解这些话语。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（GAI）技术的迅速发展，了解学生和教育工作者对这些工具的看法至关重要。本研究旨在全面分析利益相关者在教育中的GAI话语动态。

Method: 我们应用了情感分析、主题建模和作者分类，并提出了一个模块化框架，该框架利用基于提示的大语言模型（LLMs）来分析在线社交话语，并将其与经典自然语言处理（NLP）模型进行评估。

Result: 我们的GPT-4o管道在所有任务中均优于先前方法，例如，在情感分析中达到了90.6%的准确率。主题提取揭示了公共话语中的12个潜在主题，这些主题具有不同的情感和作者分布。教师和学生对GAI在高等教育中的个性化学习和生产力潜力持乐观态度。然而，关键差异出现了：学生经常因AI检测器的错误指控而感到困扰，而教师则普遍担心工作保障、学术诚信和机构压力。

Conclusion: 我们的研究结果表明，需要更清晰的机构政策、更透明的GAI整合实践以及为教育工作者和学生提供支持机制。更广泛地说，这项研究展示了基于LLM的框架在建模在线社区中利益相关者话语方面的潜力。

Abstract: Generative AI (GAI) technologies are quickly reshaping the educational
landscape. As adoption accelerates, understanding how students and educators
perceive these tools is essential. This study presents one of the most
comprehensive analyses to date of stakeholder discourse dynamics on GAI in
education using social media data. Our dataset includes 1,199 Reddit posts and
13,959 corresponding top-level comments. We apply sentiment analysis, topic
modeling, and author classification. To support this, we propose and validate a
modular framework that leverages prompt-based large language models (LLMs) for
analysis of online social discourse, and we evaluate this framework against
classical natural language processing (NLP) models. Our GPT-4o pipeline
consistently outperforms prior approaches across all tasks. For example, it
achieved 90.6% accuracy in sentiment analysis against gold-standard human
annotations. Topic extraction uncovered 12 latent topics in the public
discourse with varying sentiment and author distributions. Teachers and
students convey optimism about GAI's potential for personalized learning and
productivity in higher education. However, key differences emerged: students
often voice distress over false accusations of cheating by AI detectors, while
teachers generally express concern about job security, academic integrity, and
institutional pressures to adopt GAI tools. These contrasting perspectives
highlight the tension between innovation and oversight in GAI-enabled learning
environments. Our findings suggest a need for clearer institutional policies,
more transparent GAI integration practices, and support mechanisms for both
educators and students. More broadly, this study demonstrates the potential of
LLM-based frameworks for modeling stakeholder discourse within online
communities.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [99] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: 本文介绍了SEED-Bench-R1基准和GRPO-CARE框架，以提高多模态大语言模型的推理一致性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在多模态大语言模型（MLLMs）中的适应性尚未被探索。为了填补这一空白，本文引入了SEED-Bench-R1，一个需要平衡感知和推理的复杂现实世界视频基准。

Method: 本文提出了GRPO-CARE，一种一致性感知的强化学习框架，优化答案正确性和推理连贯性，而无需显式监督。GRPO-CARE引入了双层奖励机制：(1) 基础奖励用于答案正确性，(2) 自适应一致性奖励，通过比较模型的推理到答案的可能性（通过缓慢演化的参考模型）与群体同伴来计算。

Result: 使用SEED-Bench-R1，我们发现标准GRPO虽然提高了答案准确性，但经常降低了推理步骤和答案之间的逻辑连贯性，一致性率为57.9%。这源于奖励信号仅关注最终答案，鼓励捷径，并且严格的KL惩罚限制了探索。GRPO-CARE通过替换KL惩罚为自适应奖励，在SEED-Bench-R1上优于标准GRPO，实现了6.7%的性能提升和24.5%的一致性改进。此外，它在各种视频理解基准上表现出强大的迁移能力。

Conclusion: 本文贡献了一个系统设计的基准和一个可泛化的后训练框架，推动了更可解释和稳健的多模态大语言模型（MLLM）的发展。

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [100] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的推理路径搜索方案MICS，用于生成严格的和有效的医学CoT数据。基于MICS构建的Chiron-o1在多个医学视觉问答和推理基准测试中达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提供全面的框架来搜索和评估有效的推理路径方面存在不足，因此需要一种新的方法来生成严格的和有效的医学CoT数据。

Method: 提出了一种新的推理路径搜索方案MICS，该方案利用导师模型初始化推理，然后让每个内部模型继续沿着这些初始路径进行思考，并根据多个内部模型的整体推理性能选择最佳推理路径。

Result: 构建了MMRP多任务医学推理数据集和Chiron-o1医学MLLM，后者通过课程学习策略实现，具有强大的视觉问答和可泛化的推理能力。

Conclusion: 实验结果表明，基于MICS构建的CoT数据集训练的Chiron-o1在多个医学视觉问答和推理基准测试中达到了最先进水平。

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [101] [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
*Shoubin Yu,Yue Zhang,Ziyang Wang,Jaehong Yoon,Mohit Bansal*

Main category: cs.CV

TL;DR: MEXA是一种无需训练的多模态推理框架，通过动态选择专家模型并利用大型推理模型进行聚合，实现了跨多个领域的有效多模态推理。


<details>
  <summary>Details</summary>
Motivation: 构建一个统一的框架来处理日益多样化的输入模态和任务复杂性，例如医学诊断需要精确的结构化临床表格推理，而金融预测则依赖于基于图表的数据解读。

Method: MEXA是一种无需训练的框架，通过模态和任务感知的聚合方法，动态选择专家模型，并利用大型推理模型（LRM）对生成的可解释文本推理输出进行聚合和推理。

Result: MEXA在多个多模态基准测试中均取得了优于强基线模型的性能提升，展示了其在不同多模态推理任务中的有效性。

Conclusion: MEXA在多种多模态基准测试中表现出色，证明了其在不同多模态推理任务中的有效性和广泛适用性。

Abstract: Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Main category: cs.AI

TL;DR: 本文讨论了Agentic AI领域中存在的问题，并通过系统实证研究提出了改进方案。研究发现，当前代理研究缺乏标准化和科学严谨性，导致无法进行公平比较。为此，作者提出了一种更稳健的评估协议，并构建了OAgents框架，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的代理研究实践缺乏标准化和科学严谨性，使得不同方法之间的公平比较变得困难。因此，不同设计选择对代理效果的影响尚不清楚，测量其进展仍然具有挑战性。

Method: 我们对GAIA基准和BrowseComp进行了系统的实证研究，以在公平和严谨的方式下检查代理框架中流行设计选择的影响。

Result: 我们发现缺乏标准评估协议导致以前的工作，即使是开源的，也无法复现，随机运行之间存在显著差异。因此，我们引入了一个更稳健的评估协议来稳定比较。

Conclusion: 我们的研究揭示了哪些组件和设计对于有效的代理至关重要，而其他组件则冗余，尽管看似合理。基于我们的发现，我们构建并开源了OAgents，这是一个新的基础代理框架，在开源项目中实现了最先进的性能。OAgents提供了一个模块化设计，适用于各种代理组件，促进了Agentic AI的未来研究。

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [103] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: SLR is an end-to-end framework for evaluating and training Large Language Models (LLMs) through scalable logical reasoning. It automates the synthesis of inductive reasoning tasks with controlled difficulty and creates a benchmark called SLR-Bench with 19k prompts across 20 curriculum levels. Evaluation shows that while current LLMs can generate syntactically valid rules, they often struggle with logical inference. Logic-tuning with SLR improves performance significantly, achieving results comparable to more expensive models at a lower computational cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a scalable, automated framework for evaluating and training Large Language Models (LLMs) that can handle complex reasoning tasks with precise control over difficulty levels.

Method: SLR is an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. It enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task.

Result: Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost.

Conclusion: SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [104] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: 本文提出了一种评估框架，用于评估代理AI系统在关键任务谈判中的表现，通过两个实验分析了人格特质和AI代理特征对社会谈判结果的影响，并提出了可重复的评估方法以支持可靠AI系统的操作需求。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决需要能够适应不同人类操作员和利益相关者的AI代理的需求，并探索人格特质和AI代理特征如何影响社会谈判结果，这对于涉及跨团队协作和军民互动的各种应用至关重要。

Method: 使用Sotopia作为模拟测试平台，进行了两个实验，系统地评估了人格特质和AI代理特征如何影响LLM模拟的社会谈判结果。实验1采用因果发现方法测量人格特质对价格谈判的影响，实验2通过操纵模拟人类人格和AI系统特征（如透明度、能力、适应性）来评估人机工作谈判。

Result: 实验1发现宜人性和外向性显著影响可信度、目标实现和知识获取结果。从团队沟通中提取的社会认知词汇测量检测到代理同理心沟通、道德基础和观点模式的细微差异，为必须在高风险操作场景中可靠运行的代理AI系统提供了可行见解。实验2展示了AI代理的可信度如何影响任务效果。

Conclusion: 这些发现建立了一种可重复的评估方法，用于在多样的操作者个性和人机团队动态中实验AI代理的可靠性，直接支持可靠AI系统的操作需求。我们的工作通过超越标准性能指标，纳入复杂操作中任务成功至关重要的社会动态，推进了代理AI工作流程的评估。

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [105] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: 本文介绍了 BEWA，这是一种用于科学推理的正式架构，通过贝叶斯推理和动态信念更新来提高科学文献的处理能力。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数级扩展已经超越了人类专家和当前人工智能系统的认知处理能力。需要一种能够操作信念的架构，以促进真理效用、理性信念收敛和审计抗性完整性。

Method: BEWA 是一种正式结构化的架构，将信念作为结构化科学主张上的动态、概率一致函数进行操作。每个主张都具有上下文、作者归属，并通过复制分数、引用加权和时间衰减系统进行评估。信念更新通过证据条件贝叶斯推理、矛盾处理和认识论衰减机制进行。

Result: BEWA 支持基于图的主张传播、作者可信度建模、密码锚定和零知识审计验证。它形式化了科学推理，使其成为可计算验证的认识论网络。

Conclusion: BEWA 通过将科学推理形式化为可计算验证的认识论网络，推进了机器推理系统的基础，促进了真理效用、理性信念收敛和跨动态科学领域的审计抗性完整性。

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [106] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: IS-Bench is a new benchmark for evaluating the interactive safety of embodied agents, highlighting limitations in current systems and providing a foundation for safer AI.


<details>
  <summary>Details</summary>
Motivation: Flawed planning from VLM-driven embodied agents poses significant safety hazards, and existing evaluation paradigms fail to adequately assess risks within interactive environments.

Method: We propose evaluating an agent's interactive safety and present IS-Bench, the first multi-modal benchmark designed for interactive safety.

Result: Extensive experiments on leading VLMs reveal that current agents lack interactive safety awareness, and while safety-aware Chain-of-Thought can improve performance, it often compromises task completion.

Conclusion: IS-Bench provides a foundation for developing safer and more reliable embodied AI systems.

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [107] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Main category: cs.AI

TL;DR: 本文介绍了一种基于Elo评分的方法，以提高大型语言模型（LLMs）在有害内容分析中的性能。该方法在两个数据集中表现出色，优于传统的LLM提示技术和传统机器学习模型，有助于组织应用，如检测职场骚扰和评估有毒沟通。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）内置的审核系统在研究人员尝试分析有害内容时可能会出现问题，因为它们可能拒绝遵循某些指令或产生过于谨慎的回应，这会削弱结果的有效性。特别是在分析组织冲突如微侵略或仇恨言论时，这个问题尤为严重。

Method: 本文提出了一种基于Elo评分的方法，用于改进大型语言模型（LLMs）在有害内容分析中的性能。

Result: 在两个数据集中，一个专注于微侵略检测，另一个专注于仇恨言论，我们的方法在准确性、精确度和F1分数等关键指标上优于传统的LLM提示技术和传统机器学习模型。

Conclusion: 本文提出了一种基于Elo评分的方法，显著提高了大型语言模型（LLMs）在有害内容分析中的性能。该方法在微侵略检测和仇恨言论的两个数据集中表现出色，优于传统的LLM提示技术和传统机器学习模型。这种方法有助于组织应用，包括检测职场骚扰、评估有毒沟通以及促进更安全和更具包容性的工作环境。

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [108] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garcés-Erice,Ioana Giurgiu*

Main category: cs.AI

TL;DR: 本文研究了基准测试的鲁棒性，发现不同的偏见评估方法会导致不同的模型排名，并提出了相关建议。


<details>
  <summary>Details</summary>
Motivation: 创建基准测试以评估大型语言模型的安全性是可信AI社区的关键活动之一。这些基准测试允许模型在安全性的不同方面（如毒性、偏见、有害行为等）进行比较。独立基准测试采用不同的方法和数据集。

Method: 我们通过使用不同的方法对一组代表性的模型进行排名，以研究这些基准测试的鲁棒性，并比较总体排名的相似性。

Result: 不同的但广泛使用的偏见评估方法导致了不同的模型排名。

Conclusion: 我们得出结论，不同的但广泛使用的偏见评估方法会导致不同的模型排名，并为该社区在使用此类基准测试方面提供了建议。

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [109] [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
*Zihao Fu,Chris Russell*

Main category: cs.CR

TL;DR: 本文提出了一种双水印方法，以减少误报并保持高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的数字水印方法在同时使用相同的嵌入进行检测和用户识别时会导致误报问题，随着用户容量的增长，未加水印的文本越来越可能被错误地检测为加水印的文本。

Method: 提出了一种双水印方法，将检测和识别水印联合编码到生成的文本中。

Result: 实验结果验证了我们的理论发现，并展示了我们方法的有效性。

Conclusion: 通过理论分析和实验验证，我们证明了双水印方法在减少误报的同时保持高检测准确性方面的有效性。

Abstract: Digital watermarking is a promising solution for mitigating some of the risks
arising from the misuse of automatically generated text. These approaches
either embed non-specific watermarks to allow for the detection of any text
generated by a particular sampler, or embed specific keys that allow the
identification of the LLM user. However, simultaneously using the same
embedding for both detection and user identification leads to a false detection
problem, whereby, as user capacity grows, unwatermarked text is increasingly
likely to be falsely detected as watermarked. Through theoretical analysis, we
identify the underlying causes of this phenomenon. Building on these insights,
we propose Dual Watermarking which jointly encodes detection and identification
watermarks into generated text, significantly reducing false positives while
maintaining high detection accuracy. Our experimental results validate our
theoretical findings and demonstrate the effectiveness of our approach.

</details>


### [110] [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
*Biao Yi,Tiansheng Huang,Sishuo Chen,Tong Li,Zheli Liu,Zhixuan Chu,Yiming Li*

Main category: cs.CR

TL;DR: 本文提出了一种名为BEAT的黑盒防御方法，用于检测并禁用LLM中的后门攻击。该方法通过测量输入与探针连接前后输出分布的畸变程度来判断输入是否被触发。实验表明，BEAT在各种后门攻击和LLM上都有效且高效，并且初步验证了其对越狱攻击的防御效果。


<details>
  <summary>Details</summary>
Motivation: Backdoor unalignment攻击能够隐蔽地破坏LLM的安全对齐，同时规避正常的安全审计，这在实际应用中构成了重大威胁。现有的方法无法有效应对这种攻击，因此需要一种新的防御方法。

Method: BEAT通过测量输入与探针连接前后输出分布的畸变程度来判断输入是否被触发。该方法从相反的角度解决了样本依赖目标的挑战，捕捉触发器对拒绝信号的影响，而不是特定样本的成功攻击行为。

Result: 实验结果表明，BEAT在各种后门攻击和LLM上都表现出良好的防御效果，包括封闭源代码的GPT-3.5-turbo。此外，BEAT还能有效防御流行的越狱攻击，因为它们可以被视为“自然后门”。

Conclusion: 本文提出了一种名为BEAT的黑盒防御方法，可以检测触发样本以在推理过程中禁用后门。实验表明该方法在各种后门攻击和LLM上都有效且高效，并且初步验证了其对流行越狱攻击的防御效果。

Abstract: Backdoor unalignment attacks against Large Language Models (LLMs) enable the
stealthy compromise of safety alignment using a hidden trigger while evading
normal safety auditing. These attacks pose significant threats to the
applications of LLMs in the real-world Large Language Model as a Service
(LLMaaS) setting, where the deployed model is a fully black-box system that can
only interact through text. Furthermore, the sample-dependent nature of the
attack target exacerbates the threat. Instead of outputting a fixed label, the
backdoored LLM follows the semantics of any malicious command with the hidden
trigger, significantly expanding the target space. In this paper, we introduce
BEAT, a black-box defense that detects triggered samples during inference to
deactivate the backdoor. It is motivated by an intriguing observation (dubbed
the probe concatenate effect), where concatenated triggered samples
significantly reduce the refusal rate of the backdoored LLM towards a malicious
probe, while non-triggered samples have little effect. Specifically, BEAT
identifies whether an input is triggered by measuring the degree of distortion
in the output distribution of the probe before and after concatenation with the
input. Our method addresses the challenges of sample-dependent targets from an
opposite perspective. It captures the impact of the trigger on the refusal
signal (which is sample-independent) instead of sample-specific successful
attack behaviors. It overcomes black-box access limitations by using multiple
sampling to approximate the output distribution. Extensive experiments are
conducted on various backdoor attacks and LLMs (including the closed-source
GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.
Besides, we also preliminarily verify that BEAT can effectively defend against
popular jailbreak attacks, as they can be regarded as 'natural backdoors'.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在面对修改后的逻辑谜题时表现出显著的脆弱性，这表明它们往往依赖于记忆而不是从第一原理进行推理。


<details>
  <summary>Details</summary>
Motivation: 研究旨在系统地探讨大型语言模型是否解决了逻辑谜题中的问题，以及如何通过重新表述提示来改善模型的表现。

Method: 研究引入了PHANTOM RECALL基准，包含25个著名的逻辑谜题和149个精心设计的扰动，以保留推理结构但改变表面细节和解决方案。此外，还贡献了三个工具：(i) 自动逻辑等价性判断器，(ii) 细粒度推理错误分类法，以及(iii) 由这些分类指导的基于提示的缓解框架。

Result: 尽管在未修改的谜题上表现接近完美，但模型在扰动谜题上的表现明显低于人类，表现出幻觉回忆和过度阐述的问题。

Conclusion: 研究发现，大型语言模型在面对修改后的逻辑谜题时表现出显著的脆弱性，这表明它们往往依赖于记忆而不是从第一原理进行推理。

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [2] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: LLMs have limitations in long-horizon simulations, but R-WoM improves performance by incorporating external knowledge.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate whether LLMs are appropriate for world modeling and address their limitations in long-horizon simulations.

Method: The paper probes two core capabilities of world models--future state prediction and reward estimation--through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. It also proposes R-WoM, which incorporates factual, up-to-date knowledge retrieved from external tutorials.

Result: LLMs effectively capture immediate next states and identify meaningful state transitions, but their performance rapidly degrades in full-procedure planning. R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to baselines, with particular advantages in longer-horizon simulations.

Conclusion: LLMs have limitations in reliably modeling environment dynamics over long horizons, but the proposed Retrieval-augmented World Model (R-WoM) shows significant improvements in longer-horizon simulations.

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [3] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）在面对超出分布（OOD）样本时，其内部知识表示的鲁棒性问题。结果表明，LLMs在处理与预训练数据差异较大的样本时，其区分真实和虚假陈述的能力显著下降，这可能解释了基准性能的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 为了使大型语言模型（LLMs）可靠，它们必须学习可以普遍应用于各种环境的稳健知识。然而，研究表明LLM性能可能很脆弱，模型对微小输入变化表现出过度敏感。我们探索这种脆弱性是否是由于不稳定的内部知识表示所致。

Method: 我们通过在经过表面变换的样本上评估表示可分离性来测试学习知识的鲁棒性，这些变换旨在使样本超出分布（OOD），例如拼写错误或重新表述。

Result: 我们的结果表明，当样本的呈现方式与预训练数据不太相似时，陈述真实性内部表示会崩溃。虽然LLMs通常可以在样本接近预训练数据时区分真实和虚假陈述，但这种能力高度依赖于陈述的确切表面形式。

Conclusion: 我们的研究提出了一个基本挑战，即真实性探测器的效用，并更广泛地呼吁进一步研究以提高学习知识表示的鲁棒性。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [4] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 研究发现，中间标记在机器翻译中并不总是有益的，但通过结合特定提示策略的输出可以带来改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在数学和编码任务中的能力已广为人知，但它们在机器翻译任务中的影响仍鲜有研究。因此，我们希望探索中间标记在机器翻译中的作用。

Method: 我们探索了在进行机器翻译时生成中间标记的好处，跨多个语言对和多种设置进行了实验。我们比较了使用合成思维链解释进行微调的标准输入输出微调方法。

Result: 我们发现，“思考标记”并不有助于大型推理模型更好地执行机器翻译。然而，通过结合模块化翻译特定提示策略的输出来构建中间标记可以带来改进。

Conclusion: 我们的研究结果表明，在微调过程中，中间标记的贡献高度依赖于它们是否包含翻译尝试。更广泛地说，使用教师来精炼目标翻译或扩展平行语料库比将他们的思维链解释提炼到“思考”机器翻译模型中更为有效。

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [5] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: 本文提出了一种名为MIND的用户反馈事实核查流程，用于检测多语言问答知识库中的事实和文化差异。MIND能够识别文化敏感问题的不同回答，并在多个领域测试中表现出色，有助于开发更文化和事实一致的问答系统。


<details>
  <summary>Details</summary>
Motivation: Multilingual QA systems must ensure factual consistency across languages, especially for objective queries, while also accounting for cultural variation in subjective responses.

Method: MIND, a user-in-the-loop fact-checking pipeline to detect factual and cultural discrepancies in multilingual QA knowledge bases.

Result: MIND highlights divergent answers to culturally sensitive questions that vary by region and context. It was evaluated on a bilingual QA system in the maternal and infant health domain and tested on datasets from other domains to assess generalization.

Conclusion: MIND reliably identifies inconsistencies, supporting the development of more culturally aware and factually consistent QA systems.

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [6] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: TopoAlign is a framework that uses code repositories to train Math LLMs by aligning code data with formal mathematical statements, resulting in improved performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Math LLMs are constrained by the scarcity of large-scale corpora containing pairs of informal and formal statements. Structural and syntactic differences between code and formal mathematics limit effective transfer learning.

Method: TopoAlign decomposes code into docstrings, main functions, and dependency functions, and reassembles these components into analogues that structurally mirror formal statements.

Result: TopoAlign improves performance by 17.77% on BEq@10 and 68.82% on typecheck@10 for DeepSeek-Math, and achieves gains of 0.12% and 1.09% for Herald on BEq@10 and typecheck@10, respectively.

Conclusion: TopoAlign provides substantial gains for Math LLMs, demonstrating that training on aligned code data is beneficial even for specialized models.

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [7] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: 本文提出了一种名为GRAVITY的框架，用于生成基于用户档案的合成偏好数据，以提高LLM的个性化效果。通过整合多种文化和心理框架，GRAVITY在多个文化背景下表现出色，减少了对昂贵注释的依赖，并生成了更吸引人的内容。


<details>
  <summary>Details</summary>
Motivation: 个性化在LLM中通常依赖于成本高昂的人类反馈或交互日志，这限制了可扩展性并忽略了更深层次的用户属性。为了减少对人类注释的依赖，我们提出了GRAVITY框架。

Method: 我们引入了GRAVITY（Generative Response with Aligned Values, Interests, and Traits of You），这是一个框架，用于生成合成的、基于用户档案的偏好数据，捕捉用户的兴趣、价值观、信念和人格特质。通过整合人口统计、文化和心理框架，包括Hofstede的文化维度、Schwartz的基本价值观、世界价值观调查和大五人格OCEAN特质，GRAVITY合成偏好对以指导个性化内容生成。

Result: 我们在400名亚马逊用户书籍描述上评估了GRAVITY，将其与基于提示的条件、标准微调和朴素的合成对生成进行比较。基于用户档案的合成数据始终提高了生成效果，尤其是在多个文化（美国、巴西、日本、印度）中，相对于基线取得了超过4%的偏好提升，用户研究显示GRAVITY输出有86%的时间被优先选择。

Conclusion: 我们的结果表明，基于场景的合成数据可以捕捉更丰富的用户变化，减少对昂贵注释的依赖，并生成更具吸引力和以用户为中心的内容，为LLM个性化提供了一条可扩展的路径。

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [8] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: 本文提出了一种自动创建难以作弊、现实、无法回答和多跳查询的管道，以提高RAG系统的基准难度和现实性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG基准很少反映现实任务的复杂性，这限制了它们发现现有RAG系统局限性的能力。

Method: 我们提出了第一个自动、难度可控的创建不可作弊、现实、无法回答和多跳查询（CRUMQs）的管道，适用于任何语料库和领域。

Result: 与之前的RAG基准相比，CRUMQs对RAG系统具有高度挑战性，并实现了作弊分数高达81.0%的减少。

Conclusion: 我们的管道提供了一种简单的方法来提高基准的难度和现实性，并推动更强大RAG系统的开发。

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [9] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: DMTD方法通过仅使用晚期层生成多个标记，提高了解码效率，且在有限数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型的解码效率，减少对早期和中间层的重复遍历，提出了一种新的推理范式。

Method: DMTD方法通过利用早期和中间层处理后的隐藏状态，仅使用晚期层生成多个标记，从而提高解码效率。

Result: 在有限的数据集上，微调的DMTD Qwen3-4B模型实现了高达2倍的速度提升，且性能损失较小。

Conclusion: DMTD方法在有限的数据集上已经表现出色，且随着训练数据集的增大，其性能有望进一步提升。

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [10] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: 本文介绍了Context-Folding框架，使代理能够主动管理其工作上下文，并通过强化学习框架FoldGRPO实现有效任务分解和上下文管理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在长期任务中受到上下文长度的根本限制。

Method: 我们开发了一个端到端的强化学习框架FoldGRPO，通过特定的过程奖励来鼓励有效的任务分解和上下文管理。

Result: 我们的折叠代理在Deep Research和SWE等复杂长期任务中表现优异。

Conclusion: 在复杂的长期任务中，我们的折叠代理在使用更小的主动上下文的情况下，表现与ReAct基线相当或更好，并显著优于依赖基于摘要的上下文管理的模型。

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [11] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: This paper introduces ConjectureBench to evaluate the conjecturing capabilities of LLMs, highlights the importance of treating conjecturing as an independent task, and presents a method called Lean-FIRe to improve autoformalisation.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating the conjecturing ability of LLMs, which is often entangled within autoformalisation or proof, and to understand its effect on autoformalisation performance.

Method: Augmenting existing datasets to create ConjectureBench, redesigning the evaluation framework and metric to measure the conjecturing capabilities of LLMs, and designing an inference-time method called Lean-FIRe to improve conjecturing and autoformalisation.

Result: The evaluation of foundational models reveals that their autoformalisation performance is substantially overestimated when the conjecture is accounted for during evaluation. Lean-FIRe achieves the first successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1 and 7 with DeepSeek-V3.1.

Conclusion: LLMs possess the requisite knowledge to generate accurate conjectures, but improving autoformalisation performance requires treating conjecturing as an independent task and investigating how to correctly integrate it within autoformalisation.

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [12] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: SAGE is a new user simulation framework that uses business context knowledge to evaluate multi-turn agents more effectively.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to evaluating multi-turn agents often model generic users, which may not capture the domain-specific principles needed for realistic behavior. SAGE addresses this by incorporating business context knowledge into user simulation.

Method: SAGE is a user simulation framework that integrates top-down and bottom-up knowledge from business contexts to generate realistic user interactions for evaluating multi-turn agents.

Result: Empirical evaluation shows that SAGE produces more realistic and diverse interactions and identifies up to 33% more agent errors compared to existing methods.

Conclusion: SAGE is an effective evaluation tool for multi-turn interactive agents, as it produces more realistic and diverse interactions and identifies more agent errors.

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [13] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动问题生成方法，用于生成逻辑等价问题，实验结果表明其生成的问题质量与教科书问题相当。


<details>
  <summary>Details</summary>
Motivation: 现有的AQG在生成逻辑等价问题时效率低下且难度不一致，因此需要一种更有效的方法。

Method: 我们提出了一种新的方法，使用形式语言定义逻辑等价问题，并将其翻译成两组生成规则，开发了一个线性时间算法来生成问题。

Result: 实验表明，我们的AQG生成的问题准确性和难度与教科书问题相当。

Conclusion: 我们的AQG生成的问题质量与教科书问题相当，证明了其有效性。

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [14] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 本研究比较了神经符号和基于大型语言模型的信息抽取系统，发现基于LLM的系统在性能上更优，但也有其局限性。


<details>
  <summary>Details</summary>
Motivation: 当前信息抽取（IE）的趋势是广泛依赖大型语言模型，而忽视了构建符号或统计IE系统的数十年经验。本研究旨在比较两种不同的IE系统，并探讨它们的优缺点。

Method: 本研究比较了神经符号（NS）和基于大型语言模型（LLM）的信息抽取系统，在农业领域进行了评估，涵盖了猪肉、乳制品和作物子领域的九次访谈。

Result: 基于LLM的系统在总体F1分数（69.4 vs. 52.7）和核心部分（63.0 vs. 47.2）上优于NS系统。然而，每种系统都有权衡：NS方法提供更快的运行时间、更大的控制和高准确性，但在上下文无关任务中表现良好，但缺乏泛化能力，难以处理上下文细节，并且需要大量资源进行开发和维护。基于LLM的系统实现了更高的性能、更快的部署和更容易的维护，但运行时间较慢，控制有限，依赖模型并存在幻觉风险。

Conclusion: 我们的研究强调了在现实世界应用中部署NLP系统的“隐藏成本”，并强调了平衡性能、效率和控制的必要性。

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [15] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为Curative Prompt Refinement (CPR)的框架，旨在通过清理不规范的提示和生成额外的任务描述来减少大语言模型的幻觉问题。实证结果显示，应用CPR的提示在没有外部知识的情况下取得了超过90%的胜率。


<details>
  <summary>Details</summary>
Motivation: 用户使用的提示结构不良或模糊，导致LLM基于假设而非实际意图进行响应，从而产生幻觉事实。

Method: 引入Curative Prompt Refinement (CPR)，这是一个用于修复提示的插件框架，能够清理不规范的提示，并生成额外的信息任务描述，以使用微调的小型语言模型对齐用户的意图和提示。

Result: 当应用于语言模型时，CPR显著提高了生成质量并减少了幻觉。实证研究表明，应用CPR的提示在没有外部知识的情况下取得了超过90%的胜率。

Conclusion: CPR可以显著提高生成质量并减少幻觉，实证研究显示应用CPR的提示在没有外部知识的情况下取得了超过90%的胜率。

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [16] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: MPR is a framework that improves ill-formed prompts through multiple stages, reducing hallucinations and enhancing LLM output accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs continue to encounter challenges with hallucinations, where models generate plausible but incorrect information. The impact of ill-formed prompts was relatively underexplored.

Method: We introduce Multi-stage Prompt Refinement (MPR), a framework designed to systematically improve ill-formed prompts across multiple stages using small language models (SLMs) fine-tuned for specific tasks.

Result: Experimental results on hallucination benchmarks show that prompts refined by MPR achieve over an 85% win rate compared to their original forms, demonstrating its effectiveness in reducing hallucinations and improving LLM output accuracy.

Conclusion: MPR provides a lightweight and adaptable solution for enhancing LLM reliability across various domains.

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [17] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 本文研究了人类标签变化对模型公平性的影响，发现HLV训练方法在没有显式去偏的情况下对公平性有积极影响。


<details>
  <summary>Details</summary>
Motivation: 探讨人类标签变化对模型公平性的影响，这是一个未被探索的主题。

Method: 通过比较使用多数投票标签的训练与各种HLV方法来研究HLV对模型公平性的影响。

Result: 实验表明，没有显式去偏的情况下，HLV训练方法对公平性有积极影响。

Conclusion: HLV训练方法在没有显式去偏的情况下对公平性有积极影响。

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [18] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）中不确定性量化（UQ）在幻觉检测中的应用，分析了现有方法并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于LLM容易产生幻觉，这引发了对其可靠性和可信度的担忧，因此需要进行不确定性量化（UQ）研究以解决这一问题。

Method: 本文系统地对现有方法进行了分类，并展示了几种代表性方法的实证结果。

Result: 本文介绍了UQ的基础知识，并探讨了其在幻觉检测中的作用，同时展示了多种方法的实证结果。

Conclusion: 本文总结了当前LLM UQ在幻觉检测中的局限性，并指出了未来的研究方向，为理解LLM UQ的现状提供了更清晰的画面。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [19] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型优化用户输入的提示重写框架，通过奖励系统和迭代DPO训练流程提高文本到图像生成的质量和对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在处理简单或不明确的提示时常常遇到困难，导致图像-文本对齐、美学和质量不佳。

Method: 我们提出了一种提示重写框架，利用大型语言模型（LLMs）在将用户输入提供给T2I主干之前进行优化。我们的方法引入了一个精心设计的奖励系统和一个迭代的直接偏好优化（DPO）训练流程，使重写器能够在不需要监督微调数据的情况下增强提示。

Result: 我们在多种文本到图像模型和基准测试中评估了我们的方法。结果表明，我们的提示重写器在图像-文本对齐、视觉质量和美学方面 consistently 提高了性能，并优于强大的基线。此外，我们展示了强大的泛化能力，表明在一个文本到图像主干上训练的提示重写器可以有效地泛化到其他主干上而无需重新训练。

Conclusion: 我们的研究展示了提示重写是一种有效、可扩展且实用的模型无关策略，可以改善文本到图像生成系统。

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [20] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 本文提出分层对齐方法，通过针对模型不同功能块进行精准优化，提升了大型语言模型的对齐效果，并避免了传统方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有对齐技术通常将模型视为一个整体，忽略了Transformer架构中不同层的功能专业化。本文旨在挑战这种一刀切的方法，探索更有效的对齐策略。

Method: 提出了一种名为分层对齐的新方法，通过针对模型不同功能块（局部、中间和全局）应用定向DPO进行实验。使用LoRA进行精准微调，并通过强大的LLM-as-Judge评估结果。

Result: 实验结果表明，分层对齐方法在提升语法流畅性和事实一致性方面效果显著，同时避免了传统DPO中的对齐税问题。特别是全局对齐在增强逻辑连贯性方面表现最佳。

Conclusion: 这些发现确立了一条更高效、可控和可解释的模型对齐路径，强调了从单体优化转向结构感知的精准微调的巨大潜力，以构建更先进和可靠的大型语言模型。

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [21] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: 本文提出APCE方法，通过选择重要的输入块来减少内存占用并缓解长上下文Transformer模型的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 为了应对长上下文Transformer模型的两个关键挑战：内存占用增加和性能下降，本文提出了一种新的方法。

Method: APCE是一种基于上下文感知的方法，通过低维语义相似性匹配选择最重要的输入块。

Result: APCE在使用输入序列的50%-70%时，表现优于或与全密集基线相当，同时提高了KV缓存和自注意力的内存效率。

Conclusion: 本文提出了APCE方法，通过选择最重要的输入块来减少内存占用并缓解ContextRot现象，实验证明其在长文本摘要任务中表现优异。

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [22] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: 研究评估了Verily行为健康安全过滤器（VBHSF）在两个数据集上的性能，并将其与两个开源内容审核防护措施进行比较，结果显示VBHSF在检测精神健康危机方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理精神健康紧急情况时常常出现错误，提供有害或不适当的建议，导致破坏性行为。因此需要一种有效的过滤器来检测和处理这些情况。

Method: 评估了Verily行为健康安全过滤器（VBHSF）在两个数据集上的性能，并与两个开源内容审核防护措施进行了比较。

Result: VBHSF在Verily Mental Health Crisis Dataset上表现出高敏感性（0.990）和特异性（0.992），在NVIDIA Aegis AI Content Safety Dataset上保持高敏感性（0.982）和准确性（0.921），但特异性较低（0.859）。与其他防护措施相比，VBHSF在所有情况下都表现出更高的敏感性，并且在特异性方面优于NVIDIA NeMo。

Conclusion: VBHSF在医疗应用中表现出稳健且可推广的性能，优先考虑敏感性以最小化错过危机的情况。

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [23] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为PACE的新方法，用于评估大型语言模型的创造力，该方法减少了数据污染并提高了评估效率。结果显示，虽然高性能的LLMs在某些方面接近人类平均水平，但专业人类仍然表现更优。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs的创造力是一个重要的研究领域，但数据污染和昂贵的人类评估常常阻碍进展。

Method: PACE方法通过让LLMs生成平行关联链来评估其创造力，这种方法减少了数据污染的风险，并且具有高效率。

Result: PACE与Chatbot Arena创意写作排名有很强的相关性（Spearman's ρ = 0.739, p < 0.001），并且显示了LLMs和人类在联想创造力上的比较结果。

Conclusion: PACE是一种有效评估LLMs创造力的方法，尽管高性能的LLMs在某些方面接近人类平均水平，但专业人类仍然表现更优。

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [24] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本文研究了多语言领域自适应中大型语言模型的学习动态，并提出了一种新的评估方法AdaXEval，以更好地理解多语言知识获取过程。实验结果显示，跨语言迁移仍然面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言领域自适应（ML-DA）方法在机制上尚未充分探索，特别是关于多语言知识获取、如何在一个语言中学习领域知识以及如何跨语言转移知识。这种差距导致了次优性能，尤其是在低资源环境中。

Method: 本文提出了AdaXEval，一种自适应评估方法，通过从用于训练的同一双语领域语料库构建多项选择题数据集，直接研究多语言知识获取。此外，通过使用多样化数据配方对LLMs进行持续训练，追踪LLMs如何获取领域事实并确定从领域训练数据到知识的转换机制。

Result: 实验结果表明，即使在高质量的双语语料库下，跨语言迁移仍然是一个挑战。

Conclusion: 研究发现，尽管使用了高质量的双语语料库，跨语言迁移仍然具有挑战性。

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [25] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: 我们的研究揭示了LSLMs中的模态差距，并通过分析语音和文本表示的对齐模式提出了改进语音输入正确性的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端的大规模语音语言模型在对话生成方面表现出色，但在语义理解基准测试中始终落后于传统流水线系统。我们旨在揭示这种模态差距并提供改进语音输入正确性的方法。

Method: 我们通过系统实验揭示了LSLMs在语音-文本对齐训练后文本输入性能的损失，并分析了语音和文本表示的粗粒度和细粒度对齐模式。我们引入了对齐路径得分来量化token级别的对齐质量，并通过角度投影和长度归一化设计了针对性干预措施。

Result: 我们发现语音和文本表示在深度层中的方向逐渐对齐（余弦相似性），但幅度差异增大（欧几里得距离）。表示相似性与模态差距密切相关。我们观察到文本和语音表示之间的自发token级对齐模式，并引入了对齐路径得分来量化对齐质量。通过角度投影和长度归一化策略，这些方法展示了提高语音输入正确性的潜力。

Conclusion: 我们的研究提供了对LSLMs中模态差距和对齐机制的第一个系统实证分析，为未来优化提供了理论和方法指导。

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [26] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 本文介绍了SafeMT基准和Safety Index (SI)，用于评估多模态大型语言模型（MLLMs）在多轮对话中的安全性，并提出了一种对话安全调节器，以提高模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大型语言模型（MLLMs）的广泛应用，安全问题变得越来越重要。现有的基准测试未能充分考虑多轮对话中的风险，因此需要一个新的基准来促进对这些模型在多轮对话中的安全问题的关注。

Method: 引入了SafeMT基准，该基准包含从有害查询生成的对话，并提出了Safety Index (SI) 来评估MLLMs在对话中的安全性。此外，还提出了一种对话安全调节器，用于检测隐藏在对话中的恶意意图并提供相关的安全策略。

Result: 评估了17个模型的安全性，发现随着有害对话轮数的增加，成功攻击的风险也随之增加。这表明这些模型的安全机制不足以识别对话交互中的危险。

Conclusion: 实验结果表明，这种对话安全调节器在减少多轮ASR方面比现有的防护模型更有效。

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [27] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的Credal Transformer架构，通过引入Credal Attention Mechanism来解决大型语言模型的幻觉问题，该机制利用证据理论生成一个表示不确定性的分布集，从而提高模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）会产生幻觉，生成事实错误但自信的断言。我们认为这是因为Transformer的Softmax函数创建了“人工确定性”，将模糊的关注分数压缩成一个概率分布，每一层都丢弃了不确定性信息。

Method: 本文引入了Credal Transformer，用基于证据理论的Credal Attention Mechanism (CAM) 替换了标准注意力机制。CAM生成一个“credal set”（一组分布）而不是单一的注意力向量，集合的大小直接衡量模型的不确定性。

Result: Credal Transformer能够识别分布外输入，量化模糊性，并显著减少在无法回答的问题上的自信错误，通过避免做出判断。

Conclusion: 本文提出了一种新的架构来减轻幻觉，并提供了一个将不确定性量化直接集成到模型中的设计范式，为更可靠的AI奠定了基础。

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [28] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 本文对并行推理进行了全面的调查和总结，包括其定义、技术分类、应用场景以及核心挑战，并提供了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的增强，并行推理作为一种新的推理范式出现，通过在收敛到最终答案之前同时探索多条思路来增强推理的鲁棒性。探索并行推理已成为克服标准顺序方法脆弱性和提高实际性能的重要趋势。

Method: 本文首先提出了并行推理的正式定义，并澄清了它与相关概念（如思维链）的区别。然后，根据一个新的分类法，组织和讨论了先进的技术，包括非交互式推理、交互式推理和以效率为导向的解码策略。此外，还探索了各种应用场景，如解决复杂问题和提高LLM输出的可靠性。

Result: 本文组织和讨论了并行推理的先进技术和应用场景，并指出了并行推理的核心挑战和未来研究的潜在方向。

Conclusion: 本文旨在调查和总结并行推理的进展和挑战，并希望为初学者提供有用的路线图，鼓励更多研究改进并行推理方法。

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [29] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 本文研究了如何将文本推理中的多样本生成与PRM/ORM重排序技术应用于连续空间推理，发现虽然生成多样化推理路径是可行的，但连续空间中的独特挑战（如缺乏归纳偏见）限制了性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以将文本推理中已有的技术（如多样本生成结合PRM或ORM重排序）成功应用于连续空间推理，以提升模型性能。

Method: 通过dropout-based sampling生成多样化的推理路径，并使用Pass@N分析评估性能。同时，通过探测几何属性和轨迹动力学来识别影响PRM和ORM有效性的原因。

Result: 生成多样化的推理路径是可行的，并且在连续空间中也显示出类似离散空间中的性能提升潜力。然而，由于连续空间中的独特挑战，如缺乏有效的归纳偏见，导致PRM和ORM模型难以有效区分正确和错误的推理。

Conclusion: 当前连续思维表示中缺乏关键的归纳偏见，这限制了PRM和ORM模型在推理过程中的有效性。因此，连续推理语言模型的训练框架不仅需要优化准确性，还需要显式地融入可用于推理时区分正确和错误思维的归纳偏见。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [30] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: LLaDR is a Large Language Model-assisted framework for Drug Repurposing that improves the representation of biomedical concepts within KGs by injecting treatment-relevant knowledge into KGE models, achieving state-of-the-art performance across different scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods largely overlook common-sense biomedical concept knowledge in real-world labs, such as mechanistic priors indicating that certain drugs are fundamentally incompatible with specific treatments.

Method: LLaDR is a Large Language Model-assisted framework for Drug Repurposing, which improves the representation of biomedical concepts within KGs by extracting semantically enriched treatment-related textual representations of biomedical entities from large language models (LLMs) and using them to fine-tune knowledge graph embedding (KGE) models.

Result: Experiments based on benchmarks demonstrate that LLaDR achieves state-of-the-art performance across different scenarios, with case studies on Alzheimer's disease further confirming its robustness and effectiveness.

Conclusion: LLaDR achieves state-of-the-art performance across different scenarios, with case studies on Alzheimer's disease further confirming its robustness and effectiveness.

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [31] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本研究首次系统地研究了大型音频语言模型中的时间偏差，发现其在时间定位方面存在显著问题，并提出了量化方法和可视化框架。


<details>
  <summary>Details</summary>
Motivation: 尽管大型音频语言模型在音频理解和多模态推理中得到了广泛应用，但它们在确定事件发生时间方面的能力仍缺乏研究。

Method: 我们进行了系统的研究，通过控制实验分析了时间偏差，并引入了时间偏差指数（TBI）来量化这种效应。

Result: 我们发现时间偏差在数据集和模型中普遍存在，并且随着音频长度的增加而增加，甚至在长录音中可能累积到几十秒。此外，时间偏差在不同类型的事件和位置上有所不同。

Conclusion: 我们的研究揭示了当前大型音频语言模型在时间定位方面的根本性局限，并呼吁开发更稳健的时间架构。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [32] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型和直接偏好优化的分割框架，在同步语音翻译中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的分割模型虽然比启发式方法更鲁棒，但仍然受到监督学习目标的限制，没有考虑人类偏好对自然实时翻译的重要性。

Method: 我们提出了一种基于大型语言模型（LLMs）的分割框架，该框架通过直接偏好优化（DPO）进行训练。

Result: 实验结果表明，我们的DPO微调的LLM在分割准确性上优于SHAS，并在翻译质量（BLEU、COMET）和延迟（平均滞后）方面取得了持续改进。

Conclusion: 我们的研究展示了经过偏好调整的大型语言模型在同步语音翻译中的潜力，可以超越现有的预训练分割模型，并推动适应性强、符合人类偏好的实时翻译。

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [33] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: HALF is a framework that evaluates model bias in realistic applications and weighs outcomes by harm severity, revealing gaps in LLM fairness across domains.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations lack grounding in real-world scenarios and do not account for differences in harm severity.

Method: HALF organizes nine application domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.

Result: LLMs are not consistently fair across domains, model size or performance do not guarantee fairness, and reasoning models perform better in medical decision support but worse in education.

Conclusion: HALF exposes a clear gap between previous benchmarking success and deployment readiness.

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [34] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 本研究发现，大型语言模型中的道德偏见（如Knobe效应）可以通过针对性的干预措施进行定位和减轻，而无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型（LLMs）如何内部化类似人类的偏见，并确定这些偏见如何表现出来，以及是否可以定位和减轻它们。

Method: 我们进行了Layer-Patching分析，跨3个开放权重的LLM，以确定偏见是否在微调的LLMs中出现，并追踪到特定的模型组件。

Result: 我们发现偏见不仅在微调期间被学习，而且局限于一组特定的层。令人惊讶的是，我们将预训练模型的激活值修补到几个关键层就足以消除这种效应。

Conclusion: 我们的研究结果表明，社会偏见可以在不重新训练模型的情况下通过有针对性的干预来解释、定位和减轻。

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [35] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: 本文提出了 DSAS，用于解决 LLM 在多文档问答任务中的长程依赖建模和 'lost-in-the-middle' 问题，无需架构修改或额外训练参数，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案要么截断全局依赖关系，要么需要昂贵的微调，缺乏通用且简单的解决方案来处理 LLM 在多文档问答任务中的局限性，如长程依赖建模和 'lost-in-the-middle' 问题。

Method: 提出 Dual-Stage Adaptive Sharpening (DSAS)，包含两个模块：(i) Contextual Gate Weighting (CGW) 模块通过逐层注意力跟踪和位置感知加权评估段落相关性，缓解 'lost-in-the-middle' 问题；(ii) Reciprocal Attention Suppression (RAS) 模块通过抑制关键文本与无关文本之间的信息交换，增强对关键段落的关注，从而解决长程依赖建模的问题。

Result: 在四个基准测试中进行了广泛实验，结果表明 DSAS 在主流 LLM（如 Llama、Qwen、Mistral 和 Deepseek）上效果显著，Llama-3.1-8B-Instruct 和 Qwen2.5-14B-Instruct 的 Multi-doc QA 任务平均 F1 分数分别提高了 4.2%。消融研究证实了 CGW 和 RAS 模块的重要贡献。附录中的详细讨论进一步验证了 DSAS 的鲁棒性和可扩展性。

Conclusion: DSAS 是一种无需架构修改或额外训练参数的即插即用解决方案，在多个主流 LLM 上表现出色，证明了其有效性和可扩展性。

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [36] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: 本文介绍了MedQA-Followup框架，用于评估医疗问答中的多轮鲁棒性，并发现现有模型在多轮设置中存在严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架通常在理想条件下评估单轮问答，忽视了医疗咨询中常见的冲突输入、误导性上下文和权威影响等复杂性。

Method: 本文引入了MedQA-Followup框架，用于系统评估医疗问答中的多轮鲁棒性。该方法区分了浅层鲁棒性（抵抗误导性初始上下文）和深层鲁棒性（在多轮中保持准确性），还引入了一个间接-直接轴，将上下文框架（间接）与明确建议（直接）分开。

Result: 通过在MedQA数据集上的控制干预，评估了五个最先进的LLM，发现虽然模型在浅层扰动下表现良好，但在多轮设置中表现出严重的脆弱性，准确率从91.2%下降到13.5%。间接、基于上下文的干预通常比直接建议更有害，导致模型准确率大幅下降。

Conclusion: 本文指出，多轮对话中的鲁棒性是医疗语言模型安全可靠部署的关键但未被充分研究的维度。

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [37] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: Chinese ModernBERT is a new encoder for Chinese that improves accuracy, speed, and memory efficiency through various techniques.


<details>
  <summary>Details</summary>
Motivation: Encoder-only Transformers have not fully transferred to Chinese due to differences in tokenization and morphology.

Method: Introduces Chinese ModernBERT, which couples a hardware-aware 32k BPE vocabulary, whole-word masking with dynamic masking curriculum, a two-stage pre-training pipeline, and a damped-cosine learning-rate schedule.

Result: Chinese ModernBERT is competitive with strong Chinese encoders under a unified fine-tuning protocol. It achieves high long-sequence throughput while maintaining strong short-sequence speed. Adding open contrastive data improves performance on SimCLUE.

Conclusion: Chinese ModernBERT surpasses Qwen-0.6B-embedding on SimCLUE, suggesting a clear scaling path for STS with additional curated pairs.

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [38] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: 本文介绍了一种使用大型语言模型自动化大规模语料库语法注释的无监督方法，并展示了其在实际应用中的效果和潜力。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言语料库以空前的速度扩展，手动注释仍然是语料库语言学工作的重大方法瓶颈。我们旨在解决这一挑战。

Method: 我们提出了一种可扩展的无监督管道，使用大型语言模型（LLMs）自动化大量语料库中的语法注释。我们的方法采用了一个四阶段工作流程：提示工程、预评估、自动批量处理和后评估。

Result: 我们通过一个历时案例研究展示了该管道的可访问性和有效性，研究了英语consider结构的变化。使用GPT-5通过OpenAI API，我们在不到60小时内对143,933个句子进行了注释，实现了两个复杂注释程序的98%以上的准确性。

Conclusion: 我们的结果表明，大型语言模型可以在极少人工干预的情况下大规模执行各种数据准备任务，为基于语料库的研究开辟了新的可能性，尽管实施需要关注成本、许可和其他伦理问题。

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [39] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文提出一种新的框架，将反言论生成建模为知识导向的文本生成过程，利用RAG管道生成可信的反言论，并在多个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成反言论的可靠性和连贯性以及可扩展性方面存在严重缺陷。

Method: 本文引入了一种新的框架，将反言论生成建模为知识导向的文本生成过程，并集成了先进的检索增强生成（RAG）管道。

Result: 结果表明，该框架在标准度量（如JudgeLM）和人工评估中均优于标准LLM基线和竞争方法。

Conclusion: 该框架和知识库为研究值得信赖和合理的反言论生成铺平了道路，不仅限于仇恨言论。

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [40] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: 本研究通过引入一种细粒度输入归因方法，探讨了脑-语言模型对齐中的关键问题，发现BA和NWP依赖于不同的词语子集，揭示了它们在特征依赖上的差异，并展示了该方法在其他语言处理任务中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）与人类大脑活动之间的对齐可以揭示语言处理背后的计算原理。研究脑-语言模型对齐的关系对于理解语言处理机制具有重要意义。

Method: 本文引入了一种细粒度输入归因方法，以识别对脑-语言模型对齐最重要的特定词语，并利用它来研究关于脑-语言模型对齐的争议性研究问题：脑对齐（BA）和下一个词预测（NWP）之间的关系。

Result: 研究结果表明，BA和NWP主要依赖于不同的词语子集：NWP表现出近期性和首因效应，关注语法；而BA则优先考虑语义和话语层面的信息，并且有更针对性的近期效应。

Conclusion: 本研究加深了我们对大型语言模型与人类语言处理之间关系的理解，并突出了BA和NWP在特征依赖上的差异。此外，我们的归因方法可以广泛应用于探索模型预测在各种语言处理任务中的认知相关性。

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [41] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: 本文提出了MoBiLE框架，通过混合大-小专家策略提升MoE模型的推理速度，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型在推理过程中受到CPU-GPU互连带宽的限制，尽管已有方法尝试通过预取来加速，但这些方法存在训练开销大且在细粒度专家分割的模型上效果不佳的问题。

Method: 提出了一种基于混合大-小专家的MoE推理框架MoBiLE，通过减少不重要token的专家数量来加速推理，并设计了专用的回退和预取机制以提高内存效率。

Result: 在四个典型的现代MoE架构和挑战性的生成任务上评估MoBiLE，结果表明其在消费级GPU系统上相比基线实现了1.60x到1.72x的速度提升，且准确率下降可忽略。

Conclusion: MoBiLE在保持模型质量的同时，显著提升了MoE模型的推理速度，具有广泛的应用前景。

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [42] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 本研究探讨了LLM在同行评审和研究过程中的深度整合可能对学术公平性的影响，发现LLM评审存在语言特征偏见和对批判性陈述的厌恶，可能导致不公平的结果。然而，LLM评审指导的修订可以提高论文质量。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究强调了LLM在支持研究和同行评审中的潜力，但它们在学术工作流程中的双重角色以及研究与评审之间的复杂相互作用带来了新的风险，这些风险尚未得到充分探索。

Method: 本研究通过模拟，结合了一个生成论文和修订的研究代理以及一个评估提交的评审代理，来探讨LLM在同行评审和研究过程中的深度整合可能对学术公平性的影响。

Result: 模拟结果显示，LLM评审系统性地提高LLM撰写论文的评分，给予比人类撰写的论文更高的评分；同时，LLM评审持续低估包含批判性陈述（如风险、公平性）的人类撰写论文，即使经过多次修订。

Conclusion: 这些结果突显了如果在没有足够谨慎的情况下将LLM部署到同行评审周期中，会对人类作者和学术研究带来的风险和公平性问题。另一方面，由LLM评审指导的修订在LLM和人类评估中都提高了质量，展示了LLM作为评审者对早期研究人员和提升低质量论文的潜力。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [43] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 本研究评估了200多种语言的分词效率，发现拉丁字母语言比非拉丁字母和形态复杂语言更高效，导致计算成本增加和上下文利用效率降低。研究强调了当前AI系统中的结构性不平等，并呼吁开发更包容的分词策略。


<details>
  <summary>Details</summary>
Motivation: 由于分词差异是实现人工智能公平访问的主要障碍，因此需要系统地量化大型语言模型中的计算不平等现象。

Method: 本研究进行了大规模跨语言评估，对200多种语言进行分词效率分析，使用标准化实验框架，应用一致的预处理和归一化协议，并通过tiktoken库进行统一分词。收集了全面的分词统计信息，并使用TPS和RTC等指标进行评估。

Result: 跨语言分析揭示了显著且系统性的差异：拉丁字母语言表现出更高的分词效率，而非拉丁字母和形态复杂的语言则经历了显著更高的分词膨胀，RTC比率通常高出3-5倍。这些低效导致了计算成本增加和上下文利用效率降低。

Conclusion: 研究发现当前AI系统中存在结构性不平等，低资源和非拉丁语系的语言使用者面临不成比例的计算劣势。未来的研究应优先考虑开发语言学导向的分词策略和适应性词汇构建方法，以实现更包容和计算公平的多语言AI系统。

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [44] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PRoH是一种动态规划和推理知识超图框架，通过三个核心创新解决了现有方法的局限性，在多跳问题回答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识超图（KH）的RAG方法存在三个主要限制：静态检索规划、非自适应检索执行和对KH结构与语义的浅层使用，这限制了它们在有效多跳问题回答方面的能力。

Method: PRoH框架包含三个核心创新：(i) 上下文感知的规划模块，用于指导结构化推理计划生成；(ii) 结构化问题分解过程，将子问题组织为动态演化的有向无环图（DAG），以实现自适应、多轨迹探索；(iii) 基于实体加权重叠（EWO）的推理路径检索算法，优先考虑语义连贯的超边遍历。

Result: PRoH在多个领域实验中取得了最先进的性能，平均F1得分超过之前SOTA模型HyperGraphRAG 19.73%，生成评估（G-E）得分提高8.41%。

Conclusion: PRoH在多个领域实验中表现出色，超越了之前的SOTA模型HyperGraphRAG，并在长距离多跳推理任务中保持了强大的鲁棒性。

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [45] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出了一种名为CLEAR的框架，通过分解上下文、定位冲突知识和引入冲突感知微调来提高RAG系统的准确性和上下文一致性。实验结果表明，CLEAR在多种冲突条件下均优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems often suffer from an unfaithfulness issue, where the model's response contradicts evidence from the retrieved context. Existing approaches to improving contextual faithfulness largely rely on external interventions, such as prompt engineering, decoding constraints, or reward-based fine-tuning. These works treat the LLM as a black box and overlook a crucial question: how does the LLM internally integrate retrieved evidence with its parametric memory, particularly under knowledge conflicts?

Method: We propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), which decomposes context into fine-grained sentence-level knowledge, employs hidden-state probing to localize conflicting knowledge, and introduces conflict-aware fine-tuning to guide the model to accurately integrate retrieved evidence.

Result: Extensive experiments across three benchmarks demonstrate that CLEAR substantially improves both accuracy and contextual faithfulness, consistently outperforming strong baselines under diverse conflict conditions.

Conclusion: CLEAR substantially improves both accuracy and contextual faithfulness, consistently outperforming strong baselines under diverse conflict conditions.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [46] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: 本研究通过多语言Wug测试评估了六种模型在四种语言中的形态概括能力，发现模型表现更受语言资源丰富性影响，而非语法复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在形态概括任务中的表现，以确定模型准确性是否接近人类能力，并且是否主要由语言复杂性或可用训练数据的数量决定。

Method: 使用多语言适应版的Wug测试，在四种部分不相关的语言（加泰罗尼亚语、英语、希腊语和西班牙语）中测试了六种模型，并与人类说话者进行了比较。

Result: 结果表明，模型能够以类似人类的准确性将形态过程推广到未见过的单词。然而，准确性模式更符合社区规模和数据可用性，而不是结构复杂性。

Conclusion: 模型行为主要由语言资源的丰富性驱动，而不是对语法复杂性的敏感性，这反映了一种仅在表面上类似于人类语言能力的表现。

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [47] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为SMEC的训练框架，通过减少维度来提高大语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 高维嵌入增加了计算复杂性和存储需求，阻碍了实际部署。

Method: 提出了一种名为Sequential Matryoshka Embedding Compression (SMEC)的新训练框架，包括Sequential Matryoshka Representation Learning (SMRL)方法、Adaptive Dimension Selection (ADS)模块和Selectable Cross-batch Memory (S-XBM)模块。

Result: 在BEIR数据集上，SMEC分别比Matryoshka-Adaptor和Search-Adaptor模型提升了1.1分和2.7分。

Conclusion: SMEC在图像、文本和多模态数据集上实现了显著的降维，同时保持了性能。

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [48] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文介绍了第一个用于评估个性化设置中检测器鲁棒性的基准数据集，并提出了一种方法来预测检测器性能的变化，实验表明该方法能准确预测性能变化。


<details>
  <summary>Details</summary>
Motivation: 目前尚无先前工作研究个性化机器生成文本的检测，而大型语言模型在语言生成方面的强大能力增加了身份模仿的风险。

Method: 本文提出了一种简单可靠的方法来预测个性化设置中检测器性能的变化，该方法通过识别对应于反转特征的潜在方向并构建主要沿这些特征不同的探测数据集来评估检测器依赖性。

Result: 实验结果表明，在个性化设置中，不同检测器之间存在显著的性能差距，一些最先进的模型性能显著下降。此外，该方法可以准确预测后转移变化的方向和幅度，与实际性能差距有85%的相关性。

Conclusion: 本文希望鼓励进一步研究个性化文本检测。

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [49] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: 本文研究了测试时缩放方法在LeWiDi任务中的应用，发现两种基准方法有效，但Best-of-N方法无效。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放通常限于具有可验证正确答案的领域，如数学和编程。我们将其转移到LeWiDi-2025任务以评估注释分歧。

Method: 我们研究了三种测试时缩放方法：两种基准算法（模型平均和多数投票）以及一种Best-of-N采样方法。

Result: 两种基准方法在LeWiDi任务上一致提高了LLM性能，但Best-of-N方法没有。

Conclusion: 我们的实验表明，Best-of-N方法目前无法从数学任务转移到LeWiDi任务，我们分析了可能的原因。

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [50] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: 本文通过引入VISaGE数据集，研究了VLMs在面对不典型实例时如何权衡实用先验和语义先验，并发现当实用先验被违反时，概念理解会显著退化。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是理解VLMs在面对不典型的评估实例时如何权衡两种先验：实用先验（文本和视觉输入都相关）和语义先验（概念表示通常适用于该类别的实例）。

Method: 本文引入了一个新的评估数据集VISaGE，包含典型和异常图像，并进行了精心平衡的实验来研究VLMs如何权衡这两种先验。

Result: 实验结果表明，当违反基于一致性的实用先验假设时，概念理解会退化，这种影响比查询单个实例时语义先验的影响更大。

Conclusion: 本文结论是，当违反基于一致性的实用先验假设时，概念理解会退化，这种影响比查询单个实例时语义先验的影响更大。

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [51] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: 本文提出了一种名为 Faithful Uncertainty Tuning (FUT) 的微调方法，用于训练大型语言模型忠实地表达不确定性，而不会影响其答案分布。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常常不能准确传达它们的不确定性，这会导致信息不忠实，从而产生一个信仰差距。

Method: FUT 是一种微调方法，通过在模型样本中添加与样本一致性对齐的不确定性修饰语（如'可能'或'很可能'）来训练指令调整的大型语言模型，以忠实地表达不确定性，而不会改变其基本的答案分布。

Result: FUT 显著减少了信仰差距，同时保持了 QA 准确性，并引入了最小的语义分布变化。进一步的分析表明，FUT 在解码策略、修饰语选择和其他形式的不确定性表达（即数值）方面具有鲁棒性。

Conclusion: FUT 是一种简单且有效的方法，可以教会大型语言模型忠实地传达不确定性。

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [52] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: StyleDecipher 是一种用于检测机器生成文本的新框架，通过分析风格差异实现准确和可解释的检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在现实场景中面临泛化能力有限、对改写敏感以及缺乏可解释性的挑战，特别是在面对风格多样性或混合人机创作时。

Method: StyleDecipher 通过结合特征提取器来量化风格差异，联合建模离散的风格指标和连续的风格表示，以捕捉人类和LLM输出之间的风格级差异。

Result: StyleDecipher 在五个不同的领域（包括新闻、代码、论文、评论和学术摘要）中实现了最先进的性能，并在跨领域评估中超越了现有基线，同时保持对对抗性扰动和混合人机内容的鲁棒性。

Conclusion: StyleDecipher 是一种稳健且可解释的检测框架，能够准确、可解释地检测机器生成文本，并在跨领域评估中表现出色。

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [53] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: 本文介绍了ACADATA，一个高质量的学术翻译并行数据集，包含ACAD-TRAIN和ACAD-BENCH两个子集。通过微调两个大型语言模型并在ACAD-BENCH上测试，结果表明ACADATA能显著提升学术翻译质量和长上下文翻译效果。


<details>
  <summary>Details</summary>
Motivation: 为了验证ACAD-TRAIN的实用性，我们进行了实验，以评估其在学术翻译和长上下文翻译中的效果。

Method: 我们对ACAD-TRAIN进行微调，并在ACAD-BENCH上对其进行评估，与其他专门的机器翻译系统、通用的开放权重LLM和几个大规模专有模型进行比较。

Result: 微调后，7B和2B模型的学术翻译质量分别提高了+6.1和+12.4 d-BLEU点，而从英语翻译出的通用领域长上下文翻译效果最多提高了24.9%。微调后的最佳模型在学术翻译领域超越了最好的专有和开放权重模型。

Conclusion: 通过发布ACAD-TRAIN、ACAD-BENCH和微调模型，我们为学术领域和长上下文翻译的研究提供了有价值的资源。

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [54] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: COSTAR-A is an enhanced prompting framework that improves the performance of smaller, locally optimized models by adding the 'Answer' component, showing better output structure and decisiveness in specific tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the consistency and quality of outputs from smaller, locally optimized models by enhancing the existing COSTAR framework.

Method: The study introduces COSTAR-A, an enhanced version of the COSTAR framework by adding the 'Answer' component. It evaluates the performance of COSTAR-A on smaller, locally optimized models through controlled prompt-output assessments.

Result: COSTAR-A improves the output structure and decisiveness of localized LLMs for certain tasks, with notable improvements observed in the Llama 3.1-8B model.

Conclusion: COSTAR-A demonstrates adaptability and scalability as a prompting framework, especially for computationally efficient AI deployments on resource-constrained hardware.

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [55] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出了一种名为PARO的框架，该框架能够使大型语言模型生成与特定任务推理模式对齐的推理过程，而无需人类标注的推理过程。实验结果表明，PARO生成的推理过程在SFT+RLVR性能上可以与10倍大的人工推理过程相媲美。这表明大规模的人类推理过程标注可以被基于LLM的自动标注所取代，只需有限的人类监督即可。


<details>
  <summary>Details</summary>
Motivation: 当前，标注高质量的推理过程对于SFT阶段来说仍然非常昂贵。因此，本文旨在研究如何在不损害推理性能的情况下显著降低推理过程标注的成本。

Method: 本文提出了一种名为PARO的框架，该框架利用大型语言模型生成与特定任务推理模式对齐的推理过程。通过实验验证了PARO生成的推理过程在SFT+RLVR性能上的有效性。

Result: 实验结果表明，PARO生成的推理过程在SFT+RLVR性能上可以与10倍大的人工推理过程相媲美。这表明大规模的人类推理过程标注可以被基于LLM的自动标注所取代，只需有限的人类监督即可。

Conclusion: 本文提出了一种名为PARO的框架，该框架能够使大型语言模型生成与特定任务推理模式对齐的推理过程，而无需人类标注的推理过程。实验结果表明，PARO生成的推理过程在SFT+RLVR性能上可以与10倍大的人工推理过程相媲美。这表明大规模的人类推理过程标注可以被基于LLM的自动标注所取代，只需有限的人类监督即可。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [56] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: 本文提出有效生成空间大小（GSS）的概念，用于解决大语言模型在开放生成任务中的输出单一或错误多样化的问题，并展示了GSS的三个应用：检测提示歧义、解释推理模型中的过度思考和不足思考，以及引导模型扩展生成空间以产生高质量和多样化的输出。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在开放生成任务中存在两个失败模式：对于创造性任务，输出过于单一；对于事实性任务，产生多样化但不正确的响应。本文旨在通过有效生成空间大小（GSS）来统一解决这两个问题。

Method: 本文提出了GSSBench，这是一个包含提示对的任务套件，具有真实GSS关系，用于评估不同的度量标准并理解模型与理想行为的偏差。同时，本文比较了不同度量标准的表现，发现EigenScore等幻觉检测度量标准表现优于标准多样性度量和不确定性量化度量。

Result: 本文发现EigenScore等幻觉检测度量标准在评估模型内部表示时表现优于标准多样性度量和不确定性量化度量。此外，GSS的三个应用展示了其在检测提示歧义、解释推理模型中的过度思考和不足思考以及引导模型扩展生成空间方面的有效性。

Conclusion: 本文认为，有效生成空间大小（GSS）可以统一解决当前大语言模型在开放生成任务中输出过于单一或产生错误多样化响应的问题，并展示了GSS的三个应用：检测提示歧义、解释推理模型中的过度思考和不足思考，以及引导模型扩展生成空间以产生高质量和多样化的输出。

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [57] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 本文提出了一种系统性的研究方法，旨在提升多模态语言模型对细节的感知能力。通过引入Omni-Detective数据生成管道和Omni-Cloze评估方法，作者展示了在生成高质量详细字幕和评估这些字幕方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的OLMs在捕捉和描述细节方面的能力仍有限，因此需要一种系统且全面的研究方法来解决这一问题。

Method: 我们提出了Omni-Detective，这是一个集成工具调用的代理数据生成管道，用于自主生成高度详细但极少幻觉的多模态数据。基于使用Omni-Detective生成的数据，我们训练了两个字幕模型：Audio-Captioner用于仅音频的详细感知，Omni-Captioner用于音频-视觉的详细感知。此外，我们设计了Omni-Cloze，一种新颖的填空式评估方法，用于详细音频、视觉和音频-视觉字幕的评估。

Result: Audio-Captioner在MMAU和MMAR基准上取得了最佳性能，超过了Gemini 2.5 Flash，并达到了与Gemini 2.5 Pro相当的性能。在现有的详细字幕基准上，Omni-Captioner在VDC上设定了新的最先进水平，并在video-SALMONN 2测试集上实现了细节和幻觉之间的最佳平衡。

Conclusion: 实验结果和分析表明，Omni-Detective在生成高质量详细字幕方面是有效的，而Omni-Cloze在评估这些详细字幕方面表现出优越性。

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [58] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: 本研究通过扩展人工语言的形式化方法并关注语言模型对更长句子的泛化能力，发现语言模型在处理典型语法属性时表现更好。


<details>
  <summary>Details</summary>
Motivation: 以往的研究主要使用人工语言来探讨语言模型是否具有偏向于典型语法属性的归纳偏差，但这些研究在捕捉自然语言特征方面存在局限。

Method: 采用广义范畴语法（GCG）扩展人工语言的上下文无关形式化，并评估语言模型对未见过的更长测试句子的泛化能力。

Result: 实验表明，典型的词序更容易让语言模型进行有成效的泛化。

Conclusion: 语言模型在处理典型的语法属性时表现出更好的泛化能力。

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [59] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法DGRC来评估对话的自然性，发现语言模型更倾向于继续与议题相关的内容，并在指令调优模型中表现更明显。


<details>
  <summary>Details</summary>
Motivation: 评估对话的自然性具有挑战性，因为'自然性'的概念不明确，且缺乏可扩展的定量指标。

Method: DGRC（Divide, Generate, Recombine, and Compare）方法，通过将对话分为提示、生成续写、重新组合和比较来评估对话自然性。

Result: 语言模型倾向于继续对话中与议题相关的内容，这种效果在指令调优模型中更为明显。当存在相关提示（如“嘿，等一下”）时，它们会减少对议题的偏好。

Conclusion: DGRC方法可以有效评估对话的自然性，并揭示语言模型在对话中的行为特征。

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [60] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: 本文主张采用马钦扎克的语言理论来重新评估语言模型，强调语言是所有所说和所写内容的总和，并以使用频率为主要原则。


<details>
  <summary>Details</summary>
Motivation: 本文旨在挑战之前对语言模型的批评，提出一种新的视角来理解语言模型的能力和局限性。

Method: 本文基于马钦扎克的语言理论，重新评估了语言模型，并提供了一个设计、评估和解释语言模型的建设性指南。

Result: 通过马钦扎克的框架，本文提供了对语言模型的新见解，并提出了一个更有效的设计和评估方法。

Conclusion: 本文主张采用马钦扎克的实证主义原则来重新审视语言模型，认为语言是所有所说和所写内容的总和，并以特定语言元素的使用频率为主要支配原则。

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [61] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM is a framework that equips pretrained models with lightweight per-layer routers to improve efficiency and accuracy without changing the base weights.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains.

Method: Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget.

Result: On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p.

Conclusion: Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [62] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 本研究探讨了低资源语言Bambara的语音数据转录所需的人力成本，发现平均需要30至36小时的人工劳动来转录一小时的语音数据。


<details>
  <summary>Details</summary>
Motivation: 创建低资源语言的语音数据集是一个关键但理解不足的挑战，尤其是在人力成本方面。本研究旨在探讨在实验室和实地条件下，准确转录语音数据所需的时间和复杂性。

Method: 通过一个月的实地研究，涉及十名母语熟练的转录员，分析了53小时Bambara语音数据的自动语音识别（ASR）生成转录文本的校正过程。

Result: 研究发现，在实验室条件下，平均需要30小时的人工劳动来准确转录一小时的语音数据，而在实地条件下则需要36小时。

Conclusion: 该研究为低资源语言创建语音数据集提供了基准和实用见解，有助于未来类似语言的自然语言处理资源开发。

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [63] [Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed](https://arxiv.org/abs/2510.11739)
*Muhammad Hamza,Rizwan Jafar*

Main category: cs.SI

TL;DR: 本研究应用现代机器学习和深度学习技术来解决乌尔都语中的名人档案问题，发现基于追随者的语言特征可以有效用于人口统计预测，特别是在资源较少的语言中。


<details>
  <summary>Details</summary>
Motivation: 社交媒体已成为数字时代的重要组成部分，作为交流、互动和信息共享的平台。名人是其中最活跃的用户之一，经常通过在线帖子揭示他们个人和职业生活的一些方面。然而，大多数现有研究集中在英语和其他高资源语言上，而乌尔都语则很少被探索。

Method: 本研究应用了现代机器学习和深度学习技术来解决乌尔都语中的名人档案问题。收集并预处理了来自南亚名人追随者的短乌尔都语推文数据集。训练并比较了多种算法，包括逻辑回归、支持向量机、随机森林、卷积神经网络和长短期记忆网络。

Result: 对于性别预测，最佳性能达到了cRank为0.65和准确率为0.65，其次是年龄、职业和声誉预测的中等结果。

Conclusion: 这些结果表明，可以有效地利用基于追随者的语言特征，通过机器学习和神经方法进行乌尔都语的人口统计预测，这是一种资源较少的语言。

Abstract: Social media has become an essential part of the digital age, serving as a
platform for communication, interaction, and information sharing. Celebrities
are among the most active users and often reveal aspects of their personal and
professional lives through online posts. Platforms such as Twitter provide an
opportunity to analyze language and behavior for understanding demographic and
social patterns. Since followers frequently share linguistic traits and
interests with the celebrities they follow, textual data from followers can be
used to predict celebrity demographics. However, most existing research in this
field has focused on English and other high-resource languages, leaving Urdu
largely unexplored.
  This study applies modern machine learning and deep learning techniques to
the problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from
followers of subcontinent celebrities was collected and preprocessed. Multiple
algorithms were trained and compared, including Logistic Regression, Support
Vector Machines, Random Forests, Convolutional Neural Networks, and Long
Short-Term Memory networks. The models were evaluated using accuracy,
precision, recall, F1-score, and cumulative rank (cRank). The best performance
was achieved for gender prediction with a cRank of 0.65 and an accuracy of
0.65, followed by moderate results for age, profession, and fame prediction.
These results demonstrate that follower-based linguistic features can be
effectively leveraged using machine learning and neural approaches for
demographic prediction in Urdu, a low-resource language.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [64] [Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need](https://arxiv.org/abs/2510.11734)
*Yuqi Bai,Tianyu Huang,Kun Sun,Yuting Chen*

Main category: cs.CY

TL;DR: 本研究提出了一种系统性的LLM虚拟人格评估框架，实证证明了人格细节的重要性，并发现了LLM人格模拟中的规模定律，为社会科学实验应用大型语言模型提供了理论支持和评估指标。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在利用大型语言模型（LLMs）模拟社会实验，探索其在虚拟人格角色扮演中模仿人类个性的能力。

Method: 本研究提出了对传统心理测量方法（CFA和构念效度）的重要修改，以解决当前低水平模拟中无法捕捉LLM改进趋势的问题，从而避免过早拒绝或方法不匹配。

Result: 本研究开发了一个端到端的评估框架，包括个体层面的稳定性与可识别性分析，以及称为渐进人格曲线的人口层面分析，用于检查LLM在模拟人类人格方面的真实性与一致性。

Conclusion: 本研究提出了一个系统性的LLM虚拟人格评估框架，并实证展示了人格细节在人格模拟质量中的关键作用，同时识别了人格档案的边际效用效应，特别是LLM人格模拟中的规模定律，为将大型语言模型应用于社会科学实验提供了操作性评估指标和理论基础。

Abstract: This research focuses on using large language models (LLMs) to simulate
social experiments, exploring their ability to emulate human personality in
virtual persona role-playing. The research develops an end-to-end evaluation
framework, including individual-level analysis of stability and
identifiability, as well as population-level analysis called progressive
personality curves to examine the veracity and consistency of LLMs in
simulating human personality. Methodologically, this research proposes
important modifications to traditional psychometric approaches (CFA and
construct validity) which are unable to capture improvement trends in LLMs at
their current low-level simulation, potentially leading to remature rejection
or methodological misalignment. The main contributions of this research are:
proposing a systematic framework for LLM virtual personality evaluation;
empirically demonstrating the critical role of persona detail in personality
simulation quality; and identifying marginal utility effects of persona
profiles, especially a Scaling Law in LLM personality simulation, offering
operational evaluation metrics and a theoretical foundation for applying large
language models in social science experiments.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

TL;DR: 本文提出了一种无需任务特定微调的方法，通过结合冻结的音频基础模型和冻结的LLM，实现对对话中说话人特征的元数据标签添加，并在说话人分析任务上取得了竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 在对话转录管道中，大型语言模型（LLMs）经常用于后处理以提高语法、标点和可读性。我们探索了一个补充的后处理步骤：通过添加元数据标签来丰富转录的对话，这些标签包括年龄、性别和情绪等说话人特征。

Method: 我们的方法结合了冻结的音频基础模型（如Whisper或WavLM）和冻结的LLAMA语言模型，以推断这些说话人属性，而无需对任一模型进行任务特定的微调。使用轻量级、高效的连接器来桥接音频和语言表示，我们实现了竞争力性能。

Result: 我们的方法在不进行任务特定微调的情况下，通过结合冻结的音频基础模型和冻结的LLM，实现了在说话人分析任务上的竞争力性能，同时保持了模块化和速度。此外，我们展示了冻结的LLM可以直接比较x-vectors，在某些情况下实现了8.8%的等错误率。

Conclusion: 我们的方法在不进行任务特定微调的情况下，通过结合冻结的音频基础模型和冻结的LLAMA语言模型，实现了在说话人分析任务上的竞争力性能，同时保持了模块化和速度。此外，我们展示了冻结的LLAMA模型可以直接比较x-vectors，在某些情况下实现了8.8%的等错误率。

Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [2] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文介绍了Parity-aware Byte Pair Encoding (BPE)，这是一种改进的BPE算法，旨在解决低资源语言在分词过程中存在的不平等问题。实验结果表明，该方法在保持全局压缩率和语言模型性能的同时，能够实现跨语言的更公平的分词数量。


<details>
  <summary>Details</summary>
Motivation: Tokenization is the first -- and often least scrutinized -- step of most NLP pipelines. Standard algorithms for learning tokenizers rely on frequency-based objectives, which favor languages dominant in the training data and consequently leave lower-resource languages with tokenizations that are disproportionately longer, morphologically implausible, or even riddled with <UNK> placeholders. This phenomenon ultimately amplifies computational and financial inequalities between users from different language backgrounds.

Method: Parity-aware Byte Pair Encoding (BPE), a variant of the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes the compression gain of the currently worst-compressed language, trading a small amount of global compression for cross-lingual parity.

Result: Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.

Conclusion: Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.

Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [3] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

TL;DR: 本文提出了一种联合ASR和音高重音检测模型，通过引入互补的音高重音检测模块来提升半监督语音表示的ASR系统性能。结果显示，该模型在音高重音检测任务上取得了显著改进，并在有限资源微调下提升了ASR性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR系统在处理语音时可能忽略了重要的语调线索，如音高重音。为了提高性能，需要扩展预训练语音模型以保留或重新学习这些重要线索。

Method: 本文提出了一种联合ASR和音高重音检测模型，通过引入互补的音高重音检测模块来提升半监督语音表示的ASR系统性能。

Result: 音高重音检测组件在该任务上取得了显著改进，将F1分数的差距缩小了41%。此外，在有限资源微调下，联合训练的ASR性能在LibriSpeech上将WER降低了28.3%。

Conclusion: 我们展示了通过引入联合ASR和音高重音检测模型，可以提升使用半监督语音表示的自动语音识别（ASR）系统的性能。该模型的音高重音检测组件在该任务上取得了显著改进，将F1分数的差距缩小了41%。此外，在有限资源微调下，联合训练的ASR性能在LibriSpeech上将WER降低了28.3%。这些结果表明，扩展预训练语音模型以保留或重新学习重要的语调线索（如音高重音）的重要性。

Abstract: We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [4] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在行为一致性方面存在严重问题，基于个性的对齐策略可能不足以保证安全关键应用的预测行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要一致的行为模式以确保安全部署，但其类似人格的特征仍不明确。

Method: 提出PERSIST框架，测试25+开源模型（1B-671B参数）在500,000+响应中的表现，使用传统和新型LLM适应的人格工具进行系统分析。

Result: 发现即使400B+模型也表现出显著的响应变异性；提示顺序的微小变化可使人格测量结果变化高达20%；预期稳定行为的干预措施反而增加变异性；LLM适应工具显示与人类中心版本相同的不稳定性。

Conclusion: 当前大型语言模型在行为一致性方面存在根本性不足，基于个性的对齐策略可能无法满足安全关键应用的需求。

Abstract: Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [5] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文介绍了RCR-Router，这是一个模块化且角色感知的上下文路由框架，用于实现多智能体LLM中的高效、自适应协作。通过动态选择语义相关的记忆子集，RCR-Router减少了token使用量并提高了答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有的协调方案依赖于静态或全上下文路由策略，导致过多的token消耗、冗余内存暴露和跨交互回合的有限适应性。

Method: RCR-Router是一个模块化且角色感知的上下文路由框架，旨在实现多智能体LLM中的高效、自适应协作。它动态选择每个代理基于其角色和任务阶段的语义相关记忆子集，并遵循严格的token预算。

Result: 在三个多跳QA基准测试（HotPotQA、MuSiQue和2WikiMultihop）上的实验表明，RCR-Router减少了token使用量（高达30%），同时提高了或保持了答案质量。

Conclusion: 这些结果强调了结构化记忆路由和输出感知评估在推进可扩展多智能体LLM系统中的重要性。

Abstract: Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [6] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
*Julia Kharchenko,Tanya Roosta,Aman Chadha,Chirag Shah*

Main category: cs.CL

TL;DR: 本文引入了一个全面的基准，用于评估大型语言模型如何对语言象征做出反应，发现模糊语言会受到系统性惩罚，并展示了该基准在识别模型特定偏见方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型如何对语言象征做出反应，这些微妙的语言标记可能会无意中揭示性别、社会阶层或地区背景等人口统计数据。

Method: 通过精心构建的100个经过验证的问答对进行访谈模拟，生成受控的语言变化以隔离特定现象并保持语义等价，从而精确测量人口统计偏差。

Result: LLM系统会系统性地惩罚某些语言模式，尤其是模糊语言，尽管内容质量相当。模糊回应的平均评分低25.6%。

Conclusion: 本文建立了检测和测量AI系统中语言歧视的基础框架，并展示了其在自动化决策公平性中的广泛应用。

Abstract: This paper introduces a comprehensive benchmark for evaluating how Large
Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic
markers that can inadvertently reveal demographic attributes such as gender,
social class, or regional background. Through carefully constructed interview
simulations using 100 validated question-response pairs, we demonstrate how
LLMs systematically penalize certain linguistic patterns, particularly hedging
language, despite equivalent content quality. Our benchmark generates
controlled linguistic variations that isolate specific phenomena while
maintaining semantic equivalence, which enables the precise measurement of
demographic bias in automated evaluation systems. We validate our approach
along multiple linguistic dimensions, showing that hedged responses receive
25.6% lower ratings on average, and demonstrate the benchmark's effectiveness
in identifying model-specific biases. This work establishes a foundational
framework for detecting and measuring linguistic discrimination in AI systems,
with broad applications to fairness in automated decision-making contexts.

</details>


### [7] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种基于聚类的视觉-语言评估框架，以解决标准评估方法在捕捉语义模糊性方面的不足，并展示其与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 标准的精确匹配评估方法无法捕捉动词语义和图像解释中的固有模糊性，导致对模型性能的评估不完整。

Method: 本文提出了一种视觉-语言聚类框架，用于构建动词意义聚类，以提供更稳健的评估方法。

Result: 分析显示，每张图像平均映射到2.8个意义聚类，每个聚类代表图像的不同视角。此外，基于聚类的评估方法比传统方法更符合人类判断。

Conclusion: 本文提出了一种基于聚类的视觉-语言评估框架，能够更好地捕捉动作描述中的语义模糊性，并与人类判断更一致。

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [8] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段大型语言模型框架，用于从非结构化文本中提取社会决定因素（SDoH），以提高自杀相关因素的准确性和透明度。该方法在性能上优于其他先进模型，并且通过提供中间解释来提高模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 了解导致自杀事件的社会决定因素对于早期干预和预防至关重要。然而，数据驱动的方法面临挑战，如长尾因子分布、分析自杀事件前的关键压力源以及模型解释性有限。

Method: 我们提出了一种多阶段大型语言模型框架，以增强从非结构化文本中提取社会决定因素（SDoH）的能力。我们的方法与其他最先进的语言模型（如预训练的BioBERT和GPT-3.5-turbo）和推理模型（如DeepSeek-R1）进行了比较。我们还评估了模型的解释如何帮助人们更快、更准确地标注SDoH因素。分析包括自动化比较和试点用户研究。

Result: 我们展示了所提出的框架在总体任务（提取SDoH因素）以及更细粒度任务（检索相关上下文）中的性能提升。此外，我们展示了微调一个较小的任务特定模型可以实现可比或更好的性能，同时减少推理成本。多阶段设计不仅增强了提取效果，还提供了中间解释，提高了模型的可解释性。

Conclusion: 我们的方法提高了从非结构化文本中提取与自杀相关的社会决定因素的准确性和透明度。这些进展有望支持早期识别高风险个体并制定更有效的预防策略。

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [9] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过划分对话为语义独立的子对话并使用两步框架提取四元组，在DiaASQ任务中取得了更好的性能和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常学习整个对话中的词关系，假设情感元素均匀分布，但对话中常常包含多个语义独立的子对话，这会引入额外的噪声。

Method: 我们提出了一种基于结构熵最小化的算法来划分对话，并引入了一个两步框架进行四元组提取。

Result: 实验表明，我们的方法在DiaASQ中取得了最先进的性能，并且计算成本更低。

Conclusion: 我们的方法在DiaASQ中实现了最先进的性能，并且计算成本显著降低。

Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [10] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

TL;DR: 本文评估了四种大型语言模型在AMR解析任务中的表现，发现仅微调解码器的模型可以达到与先进方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 研究目的是评估仅微调解码器的大型语言模型在AMR解析任务中的表现，并探索其与现有先进方法的比较。

Method: 本文对四种不同的LLM架构（Phi 3.5、Gemma 2、LLaMA 3.2和DeepSeek R1 LLaMA Distilled）进行了全面评估，并使用LDC2020T02 Gold AMR3.0测试集进行微调。

Result: 实验结果表明，仅微调解码器的大型语言模型可以达到与最先进的AMR解析器相当的性能。其中，LLaMA 3.2在语义性能上表现优异，而Phi 3.5在结构有效性方面表现出色。

Conclusion: 本文表明，仅微调解码器的大型语言模型可以通过简单的方法达到与最先进的AMR解析器相当的性能。

Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [11] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的多任务学习方法Align-LoRA，通过在共享适配器空间中对任务表示进行对齐，实现了更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多组件范式（如多适配器和多头结构）在多任务学习中被广泛采用，但我们发现这种复杂的结构并不总是最优的。我们希望探索一种更简单但更有效的多任务学习方法。

Method: 我们提出了Align-LoRA方法，该方法通过在共享适配器空间中对任务表示进行对齐，以提高多任务学习的性能。

Result: 实验表明，Align-LoRA方法显著优于所有基线，证明了我们的新假设的有效性。

Conclusion: 我们的研究结果表明，有效的多任务学习泛化依赖于学习稳健的共享表示，而不是隔离任务特定特征。我们提出的Align-LoRA方法通过在共享适配器空间中对任务表示进行对齐，显著优于所有基线，为适应多个任务的LLM提供了一个更简单但更有效的范式。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [12] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
*Aditya Kishore,Gaurav Kumar,Jasabanta Patro*

Main category: cs.CL

TL;DR: 本文提出了一种统一的框架MultiCheck，用于细粒度的多模态事实验证，通过结合文本和图像的编码器以及融合模块，实现了对声明真实性的预测，并在Factify 2数据集上取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息的增长率给依赖于文本证据的验证系统带来了重大挑战。

Method: 我们的架构结合了文本和图像的专用编码器以及一个融合模块，该模块使用逐元素交互来捕捉跨模态关系。然后，分类头预测声明的真实性，并通过对比学习目标鼓励在共享潜在空间中的声明-证据对的语义对齐。

Result: 我们在Factify 2数据集上评估了我们的方法，取得了加权F1分数0.84，显著优于基线。

Conclusion: 这些结果突显了显式多模态推理的有效性，并展示了我们方法在复杂、现实场景中可扩展和可解释的假检查的潜力。

Abstract: The growing rate of multimodal misinformation, where claims are supported by
both text and images, poses significant challenges to fact-checking systems
that rely primarily on textual evidence. In this work, we have proposed a
unified framework for fine-grained multimodal fact verification called
"MultiCheck", designed to reason over structured textual and visual signals.
Our architecture combines dedicated encoders for text and images with a fusion
module that captures cross-modal relationships using element-wise interactions.
A classification head then predicts the veracity of a claim, supported by a
contrastive learning objective that encourages semantic alignment between
claim-evidence pairs in a shared latent space. We evaluate our approach on the
Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially
outperforming the baseline. These results highlight the effectiveness of
explicit multimodal reasoning and demonstrate the potential of our approach for
scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [13] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Jing Liu,Wayne Xin Zhao,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: BEE-RAG improves RAG performance by managing entropy and attention dynamics for better adaptability to different context lengths.


<details>
  <summary>Details</summary>
Motivation: To address the performance issues in RAG caused by long context lengths and entropy growth.

Method: BEE-RAG uses entropy invariance to improve adaptability to different context lengths, along with a zero-shot inference strategy and parameter-efficient adaptive fine-tuning.

Result: Extensive experiments show that BEE-RAG is effective across multiple RAG tasks.

Conclusion: BEE-RAG demonstrates effectiveness in various RAG tasks by addressing entropy growth and attention dilution issues.

Abstract: With the rapid advancement of large language models (LLMs),
retrieval-augmented generation (RAG) has emerged as a critical approach to
supplement the inherent knowledge limitations of LLMs. However, due to the
typically large volume of retrieved information, RAG tends to operate with long
context lengths. From the perspective of entropy engineering, we identify
unconstrained entropy growth and attention dilution due to long retrieval
context as significant factors affecting RAG performance. In this paper, we
propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves
the adaptability of RAG systems to varying context lengths through the
principle of entropy invariance. By leveraging balanced context entropy to
reformulate attention dynamics, BEE-RAG separates attention sensitivity from
context length, ensuring a stable entropy level. Building upon this, we
introduce a zero-shot inference strategy for multi-importance estimation and a
parameter-efficient adaptive fine-tuning mechanism to obtain the optimal
balancing factor for different settings. Extensive experiments across multiple
RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [14] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

TL;DR: AttnRank is a method that improves the performance of Large Language Models by reordering retrieved documents or few-shot examples to align with the model's attention preferences.


<details>
  <summary>Details</summary>
Motivation: The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance.

Method: We introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions.

Result: Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.

Conclusion: AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.

Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [15] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

TL;DR: 本文介绍了 PrinciplismQA，一个用于评估大型语言模型在医学伦理方面表现的基准测试，发现模型在实际应用中存在显著差距，并提出通过医学领域微调来提升其伦理能力。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型整合到医疗保健中需要对其伦理推理进行严格评估，而当前的基准往往忽视了这一领域。

Method: 引入 PrinciplismQA，这是一个包含 3,648 个问题的全面基准，旨在系统地评估 LLMs 与核心医学伦理的一致性。该基准基于普林西皮斯主义，包括从权威教材中精选的选择题和从权威医学伦理案例研究文献中获取的开放性问题，并由医学专家验证。

Result: 实验结果揭示了模型的伦理知识与其实际应用之间的显著差距，尤其是在动态应用伦理原则到现实场景方面。大多数 LLM 在涉及有益性的困境中表现不佳，往往过于强调其他原则。前沿的封闭源代码模型由于强大的通用能力目前在基准测试中领先。值得注意的是，医学领域微调可以提高模型的整体伦理能力，但进一步进展需要更好的与医学伦理知识对齐。

Conclusion: PrinciplismQA 提供了一个可扩展的框架，用于诊断这些特定的伦理弱点，为更平衡和负责任的医疗人工智能铺平了道路。

Abstract: The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [16] [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)
*Catherine Kobus,François Lancelot,Marion-Cécile Martin,Nawal Ould Amer*

Main category: cs.CL

TL;DR: 本文介绍了ATLANTIS团队在SemEval-2025任务3中的贡献，重点是检测问答系统中的幻觉文本跨度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言生成（NLG）方面取得了显著进展，但仍容易产生幻觉，生成错误或误导性的内容。

Method: 本文探讨了使用外部上下文和不使用外部上下文的方法，利用少量示例提示、基于令牌的分类或在合成数据上微调的语言模型。

Result: 本文的方法在西班牙语中获得了顶级排名，在英语和德语中获得了具有竞争力的排名。

Conclusion: 本文强调了整合相关上下文以减轻幻觉的重要性，并展示了微调模型和提示工程的潜力。

Abstract: This paper presents the contributions of the ATLANTIS team to SemEval-2025
Task 3, focusing on detecting hallucinated text spans in question answering
systems. Large Language Models (LLMs) have significantly advanced Natural
Language Generation (NLG) but remain susceptible to hallucinations, generating
incorrect or misleading content. To address this, we explored methods both with
and without external context, utilizing few-shot prompting with a LLM,
token-level classification or LLM fine-tuned on synthetic data. Notably, our
approaches achieved top rankings in Spanish and competitive placements in
English and German. This work highlights the importance of integrating relevant
context to mitigate hallucinations and demonstrate the potential of fine-tuned
models and prompt engineering.

</details>


### [17] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
*Haonan Shangguan,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang,Ge Yu*

Main category: cs.CL

TL;DR: 本文研究了资源受限环境下的多模态情感推理与分类任务，提出了一个轻量级的MulCoT-RD模型，通过蒸馏方法实现了高效的情感推理生成和分类。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要利用参数密集的多模态LLM的知识和推理能力进行情感分类，忽略了资源受限环境中的自主多模态情感推理生成。

Method: 提出了一种多模态思维链推理蒸馏模型MulCoT-RD，采用“教师-助教-学生”蒸馏范式来解决资源受限环境中的部署约束。

Result: MulCoT-RD在四个数据集上的实验表明，仅使用3B参数就能在JMSRC上取得强劲的表现。

Conclusion: MulCoT-RD在资源受限环境中表现出强大的性能、稳健的泛化能力和增强的可解释性。

Abstract: The surge in rich multimodal content on social media platforms has greatly
advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)
further accelerating progress in this field. Current approaches primarily
leverage the knowledge and reasoning capabilities of parameter-heavy
(Multimodal) LLMs for sentiment classification, overlooking autonomous
multimodal sentiment reasoning generation in resource-constrained environments.
Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment
Reasoning and Classification task, JMSRC, which simultaneously performs
multimodal sentiment reasoning chain generation and sentiment classification
only with a lightweight model. We propose a Multimodal Chain-of-Thought
Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a
"Teacher-Assistant-Student" distillation paradigm to address deployment
constraints in resource-limited environments. We first leverage a
high-performance Multimodal Large Language Model (MLLM) to generate the initial
reasoning dataset and train a medium-sized assistant model with a multi-task
learning mechanism. A lightweight student model is jointly trained to perform
efficient multimodal sentiment reasoning generation and classification.
Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B
parameters achieves strong performance on JMSRC, while exhibiting robust
generalization and enhanced interpretability.

</details>


### [18] [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)
*Yiheng Liu,Junhao Ning,Sichen Xia,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 本文提出了一种基于功能网络的大型语言模型修剪方法，通过识别和保留关键神经元来提高模型压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化修剪方法通常忽略了人工神经元之间的交互和协作，导致大型语言模型的宏观功能架构被破坏，从而影响修剪性能。

Method: 将大型语言模型视为一个数字大脑，将其分解为功能网络，并保留这些功能网络中的关键神经元进行修剪。

Result: 实验结果表明，所提出的方法可以成功识别和定位大型语言模型中的功能网络和关键神经元，实现高效的模型修剪。

Conclusion: 本文提出了一种通过识别和保留大型语言模型中的功能网络来修剪模型的方法，实验结果表明该方法能够成功识别和定位功能网络和关键神经元，实现高效的模型压缩。

Abstract: Structured pruning is one of the representative techniques for compressing
large language models (LLMs) to reduce GPU memory consumption and accelerate
inference speed. It offers significant practical value in improving the
efficiency of LLMs in real-world applications. Current structured pruning
methods typically rely on assessment of the importance of the structure units
and pruning the units with less importance. Most of them overlooks the
interaction and collaboration among artificial neurons that are crucial for the
functionalities of LLMs, leading to a disruption in the macro functional
architecture of LLMs and consequently a pruning performance degradation.
Inspired by the inherent similarities between artificial neural networks and
functional neural networks in the human brain, we alleviate this challenge and
propose to prune LLMs by identifying and preserving functional networks within
LLMs in this study. To achieve this, we treat an LLM as a digital brain and
decompose the LLM into functional networks, analogous to identifying functional
brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving
the key neurons within these functional networks. Experimental results
demonstrate that the proposed method can successfully identify and locate
functional networks and key neurons in LLMs, enabling efficient model pruning.
Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [19] [CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL](https://arxiv.org/abs/2508.05242)
*Sijie Wang,Quanjiang Guo,Kai Zhao,Yawei Zhang,Xin Li,Xiang Li,Siqi Li,Rui She,Shangshu Yu,Wee Peng Tay*

Main category: cs.CL

TL;DR: CodeBoost is a post-training framework that improves code LLMs using code snippets instead of human-annotated instructions, resulting in consistent performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the bottleneck in instruction-based post-training by using abundant code snippets instead of labor-intensive human-annotated instructions.

Method: CodeBoost is a post-training framework that enhances code LLMs purely from code snippets, without relying on human-annotated instructions. It introduces maximum-clique curation, bi-directional prediction, error-aware prediction, heterogeneous augmentation, and heterogeneous rewarding.

Result: Extensive experiments verify that CodeBoost consistently improves performance, demonstrating its effectiveness as a scalable and effective training pipeline.

Conclusion: CodeBoost consistently improves performance across several code LLMs and benchmarks, demonstrating its effectiveness as a scalable and effective training pipeline.

Abstract: Code large language models (LLMs) have become indispensable tools for
building efficient and automated coding pipelines. Existing models are
typically post-trained using reinforcement learning (RL) from general-purpose
LLMs using "human instruction-final answer" pairs, where the instructions are
usually from manual annotations. However, collecting high-quality coding
instructions is both labor-intensive and difficult to scale. On the other hand,
code snippets are abundantly available from various sources. This imbalance
presents a major bottleneck in instruction-based post-training. We propose
CodeBoost, a post-training framework that enhances code LLMs purely from code
snippets, without relying on human-annotated instructions. CodeBoost introduces
the following key components: (1) maximum-clique curation, which selects a
representative and diverse training corpus from code; (2) bi-directional
prediction, which enables the model to learn from both forward and backward
prediction objectives; (3) error-aware prediction, which incorporates learning
signals from both correct and incorrect outputs; (4) heterogeneous
augmentation, which diversifies the training distribution to enrich code
semantics; and (5) heterogeneous rewarding, which guides model learning through
multiple reward types including format correctness and execution feedback from
both successes and failures. Extensive experiments across several code LLMs and
benchmarks verify that CodeBoost consistently improves performance,
demonstrating its effectiveness as a scalable and effective training pipeline.

</details>


### [20] [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
*Dongxu Zhang,Ning Yang,Jihua Zhu,Jinnan Yang,Miao Xin,Baoliang Tian*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning
capabilities of Large Language Models (LLMs), yet the reliability of these
reasoning chains remains a critical challenge. A widely held "cascading
failure" hypothesis suggests that errors are most detrimental when they occur
early in the reasoning process. This paper challenges that assumption through
systematic error-injection experiments, revealing a counter-intuitive
phenomenon we term "Late-Stage Fragility": errors introduced in the later
stages of a CoT chain are significantly more likely to corrupt the final answer
than identical errors made at the beginning. To address this specific
vulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought
(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive
Verification Manager (AVM) operates first, followed by the Multi-Perspective
Self-Correction Engine (MSCE). The AVM leverages a Positional Impact Score
function I(k) that assigns different weights based on the position within the
reasoning chains, addressing the Late-Stage Fragility issue by identifying and
prioritizing high-risk, late-stage steps. Once these critical steps are
identified, the MSCE applies robust, dual-path correction specifically to the
failure parts. Extensive experiments on benchmarks such as GSM8K and MATH
demonstrate that ASCoT achieves outstanding accuracy, outperforming strong
baselines, including standard CoT. Our work underscores the importance of
diagnosing specific failure modes in LLM reasoning and advocates for a shift
from uniform verification strategies to adaptive, vulnerability-aware
correction mechanisms.

</details>


### [21] [Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue](https://arxiv.org/abs/2508.05283)
*Sukannya Purkayastha,Nils Dycke,Anne Lauscher,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文探讨了如何利用大型语言模型生成合成数据来训练对话代理，以辅助元评审过程。实验结果表明，这种方法生成的合成数据质量更高，并且训练的对话代理在该任务上优于现有的基于LLM的助手。


<details>
  <summary>Details</summary>
Motivation: 元评审是同行评审过程中的关键阶段，是决定论文是否推荐接受的最后一步。以往的研究将元评审视为对评审报告的摘要问题，但元评审实际上是一个需要权衡评审论点并将其置于更广泛背景中的决策过程。因此，我们需要探索能够有效辅助元评审者的对话代理的实际挑战。

Method: 我们首先通过使用基于自我精炼策略的大型语言模型（LLMs）生成合成数据来解决训练对话代理的数据稀缺问题，以提高这些对话与专家领域的相关性。然后，我们利用这些数据训练针对元评审的对话代理，并评估其性能。最后，我们将代理应用于现实世界的元评审场景，以验证其有效性。

Result: 我们的方法生成的合成数据质量更高，并且可以作为训练元评审助手的宝贵资源。训练的对话代理在该任务上优于现成的基于LLM的助手，并且在现实世界的元评审场景中表现出有效性。

Conclusion: 我们的实验表明，这种方法可以生成高质量的合成数据，并且可以作为训练元评审助手的宝贵资源。随后，我们利用这些数据训练了针对元评审的对话代理，并发现这些代理在该任务上优于现成的基于LLM的助手。最后，我们将代理应用于现实世界的元评审场景，并确认它们在提高元评审效率方面的有效性。

Abstract: Meta-reviewing is a pivotal stage in the peer-review process, serving as the
final step in determining whether a paper is recommended for acceptance. Prior
research on meta-reviewing has treated this as a summarization problem over
review reports. However, complementary to this perspective, meta-reviewing is a
decision-making process that requires weighing reviewer arguments and placing
them within a broader context. Prior research has demonstrated that
decision-makers can be effectively assisted in such scenarios via dialogue
agents. In line with this framing, we explore the practical challenges for
realizing dialog agents that can effectively assist meta-reviewers. Concretely,
we first address the issue of data scarcity for training dialogue agents by
generating synthetic data using Large Language Models (LLMs) based on a
self-refinement strategy to improve the relevance of these dialogues to expert
domains. Our experiments demonstrate that this method produces higher-quality
synthetic data and can serve as a valuable resource towards training
meta-reviewing assistants. Subsequently, we utilize this data to train dialogue
agents tailored for meta-reviewing and find that these agents outperform
\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our
agents in real-world meta-reviewing scenarios and confirm their effectiveness
in enhancing the efficiency of meta-reviewing.\footnote{Code and Data:
https://github.com/UKPLab/arxiv2025-meta-review-as-dialog

</details>


### [22] [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens](https://arxiv.org/abs/2508.05305)
*Nikita Dragunov,Temurbek Rahmatullaev,Elizaveta Goncharova,Andrey Kuznetsov,Anton Razzhigaev*

Main category: cs.CL

TL;DR: SONAR-LLM是一种新型的基于SONAR嵌入空间的解码器模型，能够提供高质量文本生成，并且具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了保留LCM的语义抽象，同时消除其扩散采样器并恢复基于似然的训练信号，提出了SONAR-LLM。

Method: SONAR-LLM是一种仅解码器的变压器，它在相同的连续SONAR嵌入空间中进行思考，但通过冻结的SONAR解码器传播的token级交叉熵进行监督。

Result: SONAR-LLM在39M到1.3B参数的模型规模下达到了具有竞争力的生成质量。

Conclusion: SONAR-LLM在不同模型规模下表现出色，且通过释放完整的训练代码和预训练检查点促进了可重复性和未来研究。

Abstract: The recently proposed Large Concept Model (LCM) generates text by predicting
a sequence of sentence-level embeddings and training with either mean-squared
error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer
that "thinks" in the same continuous SONAR embedding space, yet is supervised
through token-level cross-entropy propagated via the frozen SONAR decoder. This
hybrid objective retains the semantic abstraction of LCM while eliminating its
diffusion sampler and restoring a likelihood-based training signal. Across
model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive
generation quality. We report scaling trends, ablations, benchmark results, and
release the complete training code and all pretrained checkpoints to foster
reproducibility and future research.

</details>


### [23] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
*Jiameng Huang,Baijiong Lin,Guhao Feng,Jierun Chen,Di He,Lu Hou*

Main category: cs.CL

TL;DR: CGRS is a method that suppresses unnecessary reflection in LRLMs, reducing token usage without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the overthinking problem in recent Large Reasoning Language Models (LRLMs), which leads to redundant reasoning steps, increased token usage, higher inference costs, and reduced practical utility.

Method: CGRS is a novel method that mitigates overthinking in LRLMs by dynamically suppressing the model's generation of reflection triggers when it exhibits high confidence in its current response.

Result: Extensive experiments across four reasoning benchmarks show that CGRS reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines.

Conclusion: CGRS's practical value for efficient reasoning is highlighted, as it effectively reduces token usage while preserving accuracy across various model architectures and scales.

Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought
reasoning with complex reflection behaviors, typically signaled by specific
trigger words (e.g., "Wait" and "Alternatively") to enhance performance.
However, these reflection behaviors can lead to the overthinking problem where
the generation of redundant reasoning steps that unnecessarily increase token
usage, raise inference costs, and reduce practical utility. In this paper, we
propose Certainty-Guided Reflection Suppression (CGRS), a novel method that
mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS
operates by dynamically suppressing the model's generation of reflection
triggers when it exhibits high confidence in its current response, thereby
preventing redundant reflection cycles without compromising output quality. Our
approach is model-agnostic, requires no retraining or architectural
modifications, and can be integrated seamlessly with existing autoregressive
generation pipelines. Extensive experiments across four reasoning benchmarks
(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it
reduces token usage by an average of 18.5% to 41.9% while preserving accuracy.
It also achieves the optimal balance between length reduction and performance
compared to state-of-the-art baselines. These results hold consistently across
model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3
family) and scales (4B to 32B parameters), highlighting CGRS's practical value
for efficient reasoning.

</details>


### [24] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 该研究探讨了在微软HoloLens 2设备上的手语虚拟形象中添加调整功能的影响。尽管用户偏好可调整设置，但用户体验和可理解性没有显著改善，主要受到缺失的手语元素和实现问题的影响。研究建议增强嘴部和面部动画，改进交互界面，并应用参与式设计。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨在现有手语虚拟形象中添加调整功能的影响，并评估用户对这种系统的接受度和体验。

Method: 该研究通过详细分析专家德国手语（DGS）用户在特定用例中与可调整和不可调整的虚拟形象的互动，确定了影响可理解性、用户体验（UX）和系统接受度的关键因素。

Result: 尽管用户偏好可调整设置，但用户体验和可理解性没有显著改善，仍然处于较低水平。缺失的手语元素（如嘴部动作和面部表情）和实现问题（手形不清晰、缺乏反馈和菜单定位）是主要问题。愉悦质量高于实用质量，表明用户认为系统更情感或审美上令人满意而非功能性有用。可调整的虚拟形象导致更高的压力水平，反映出较低的表现、更大的努力和更多的挫败感。

Conclusion: 该研究强调，仅靠个性化是不够的，手语虚拟形象必须默认可理解。关键建议包括增强嘴部和面部动画，改进交互界面，并应用参与式设计。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


### [25] [Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025](https://arxiv.org/abs/2508.05366)
*Samy Ateia,Udo Kruschwitz*

Main category: cs.CL

TL;DR: 本文研究了LLM在专业搜索任务中的自我修正能力，并探讨了推理模型生成有用反馈的能力。


<details>
  <summary>Details</summary>
Motivation: 将这些系统应用于领域特定的专业搜索（如生物医学研究）存在挑战，因为自动化系统可能减少用户参与并偏离专家的信息需求。专业搜索任务通常需要高水平的用户专业知识和透明度。

Method: 本文采用了一种自我反馈机制，其中LLM生成、评估并改进其输出以进行查询扩展和多种答案类型（是/否、事实性、列表、理想）。

Result: 初步结果表明，自我反馈策略在不同模型和任务中的表现各异。

Conclusion: 本文提供了关于LLM自我修正的见解，并为未来比较LLM生成反馈与直接人类专家输入在这些搜索系统中的有效性提供了参考。

Abstract: Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim
to enable autonomous search processes where Large Language Models (LLMs)
iteratively refine outputs. However, applying these systems to domain-specific
professional search, such as biomedical research, presents challenges, as
automated systems may reduce user involvement and misalign with expert
information needs. Professional search tasks often demand high levels of user
expertise and transparency. The BioASQ CLEF 2025 challenge, using
expert-formulated questions, can serve as a platform to study these issues. We
explored the performance of current reasoning and nonreasoning LLMs like
Gemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our
methodology was a self-feedback mechanism where LLMs generated, evaluated, and
then refined their outputs for query expansion and for multiple answer types
(yes/no, factoid, list, ideal). We investigated whether this iterative
self-correction improves performance and if reasoning models are more capable
of generating useful feedback. Preliminary results indicate varied performance
for the self-feedback strategy across models and tasks. This work offers
insights into LLM self-correction and informs future work on comparing the
effectiveness of LLM-generated feedback with direct human expert input in these
search systems.

</details>


### [26] [The TUB Sign Language Corpus Collection](https://arxiv.org/abs/2508.05374)
*Eleftherios Avramidis,Vera Czehmann,Fabian Deckert,Lorenz Hufe,Aljoscha Lipski,Yuni Amaloa Quintero Villalobos,Tae Kwon Rhee,Mengqian Shi,Lennart Stölting,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文介绍了一个包含12种手语的大型平行语料库，涵盖视频和字幕，为手语研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种新的手语研究资源，特别是为拉丁美洲的8种手语提供首个一致的平行语料库，并扩大德国手语语料库的规模。

Method: 通过收集和处理来自各种在线来源的视频（主要是新闻节目、政府机构和教育频道）来创建语料库，包括数据收集、通知内容创作者、寻求使用许可、爬取和裁剪等步骤。

Result: 创建了一个包含超过1,300小时的视频文件和1,300万字幕的大型语料库，其中德国手语语料库的规模是之前可用语料库的十倍。

Conclusion: 本文介绍了12种手语的平行语料库，包含视频格式和主要口语语言的字幕，为研究手语提供了重要的资源。

Abstract: We present a collection of parallel corpora of 12 sign languages in video
format, together with subtitles in the dominant spoken languages of the
corresponding countries. The entire collection includes more than 1,300 hours
in 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens.
Most notably, it includes the first consistent parallel corpora for 8 Latin
American sign languages, whereas the size of the German Sign Language corpora
is ten times the size of the previously available corpora. The collection was
created by collecting and processing videos of multiple sign languages from
various online sources, mainly broadcast material of news shows, governmental
bodies and educational channels. The preparation involved several stages,
including data collection, informing the content creators and seeking usage
approvals, scraping, and cropping. The paper provides statistics on the
collection and an overview of the methods used to collect the data.

</details>


### [27] [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)
*Zhong Ken Hew,Jia Xin Low,Sze Jue Yang,Chee Seng chan*

Main category: cs.CL

TL;DR: 本文介绍了MyCulture，一个用于评估大型语言模型在马来西亚文化方面表现的基准测试，采用开放式多选问题格式以减少猜测和格式偏差，并发现LLMs在文化理解上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据主要由高资源语言如英语和中文主导，大型语言模型（LLMs）往往表现出文化偏见，这给准确表示和评估多样化的文化背景带来了挑战，特别是在低资源语言环境中。

Method: 我们引入了MyCulture，这是一个基准测试，旨在全面评估LLM在马来西亚文化方面的表现，采用了开放式的多选问题格式。

Result: 我们的评估显示，区域性和国际性LLMs在文化理解上存在显著差异。

Conclusion: 我们的评估揭示了在文化理解上的显著差异，强调了在LLM的开发和评估中需要文化基础和语言包容性的基准。

Abstract: Large Language Models (LLMs) often exhibit cultural biases due to training
data dominated by high-resource languages like English and Chinese. This poses
challenges for accurately representing and evaluating diverse cultural
contexts, particularly in low-resource language settings. To address this, we
introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on
Malaysian culture across six pillars: arts, attire, customs, entertainment,
food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,
MyCulture employs a novel open-ended multiple-choice question format without
predefined options, thereby reducing guessing and mitigating format bias. We
provide a theoretical justification for the effectiveness of this open-ended
structure in improving both fairness and discriminative power. Furthermore, we
analyze structural bias by comparing model performance on structured versus
free-form outputs, and assess language bias through multilingual prompt
variations. Our evaluation across a range of regional and international LLMs
reveals significant disparities in cultural comprehension, highlighting the
urgent need for culturally grounded and linguistically inclusive benchmarks in
the development and assessment of LLMs.

</details>


### [28] [LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models](https://arxiv.org/abs/2508.05452)
*Ming Zhang,Yujiong Shen,Jingyi Deng,Yuhui Wang,Yue Zhang,Junzhe Wang,Shichun Liu,Shihan Dou,Huayu Sha,Qiyuan Peng,Changhao Jiang,Jingqi Tong,Yilong Wu,Zhihao Zhang,Mingqi Wu,Zhiheng Xi,Mingxu Chai,Tao Liang,Zhihui Fei,Zhen Wang,Mingyang Wan,Guojun Ma,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: LLMEval-3 是一个动态评估大型语言模型的框架，通过抗污染数据整理、反作弊架构和校准的 LLM-as-a-judge 过程，确保评估的公正性和可靠性，揭示了模型的真实能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在静态基准上的评估容易受到数据污染和排行榜过拟合的影响，这些问题掩盖了模型的真实能力。需要一种更可靠的方法来评估模型的真实性能。

Method: LLMEval-3 是一个动态评估大型语言模型的框架，基于一个专有的 220k 研究生级别问题库，从中动态采样未见过的测试集进行每次评估运行。它通过抗污染数据整理、新颖的反作弊架构和校准的 LLM-as-a-judge 过程确保完整性，并辅以相对排名系统进行公平比较。

Result: 一项为期 20 个月的纵向研究几乎涵盖了 50 个领先的模型，揭示了知识记忆的性能上限，并暴露了静态基准无法检测到的数据污染漏洞。该框架在排名稳定性和一致性方面表现出色，为动态评估范式提供了强有力的实证验证。

Conclusion: LLMEval-3 提供了一种稳健且可信的方法来评估大型语言模型的真实能力，超越了排行榜分数，促进了更可信赖的评估标准的发展。

Abstract: Existing evaluation of Large Language Models (LLMs) on static benchmarks is
vulnerable to data contamination and leaderboard overfitting, critical issues
that obscure true model capabilities. To address this, we introduce LLMEval-3,
a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary
bank of 220k graduate-level questions, from which it dynamically samples unseen
test sets for each evaluation run. Its automated pipeline ensures integrity via
contamination-resistant data curation, a novel anti-cheating architecture, and
a calibrated LLM-as-a-judge process achieving 90% agreement with human experts,
complemented by a relative ranking system for fair comparison. An 20-month
longitudinal study of nearly 50 leading models reveals a performance ceiling on
knowledge memorization and exposes data contamination vulnerabilities
undetectable by static benchmarks. The framework demonstrates exceptional
robustness in ranking stability and consistency, providing strong empirical
validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and
credible methodology for assessing the true capabilities of LLMs beyond
leaderboard scores, promoting the development of more trustworthy evaluation
standards.

</details>


### [29] [TASE: Token Awareness and Structured Evaluation for Multilingual Language Models](https://arxiv.org/abs/2508.05468)
*Chenzhuo Zhao,Xinda Wang,Yue Huang,Junting Lu,Ziqian Liu*

Main category: cs.CL

TL;DR: This paper introduces TASE, a benchmark for evaluating LLMs' token-level understanding and structural reasoning across multiple languages. It highlights the limitations of current LLMs compared to human performance and provides a resource for future improvements in low-level language understanding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in fine-grained, token-level understanding and structural reasoning in large language models (LLMs), which are essential for applications requiring precision and control.

Method: TASE is a comprehensive benchmark designed to evaluate LLMs' ability to perceive and reason about token-level information across languages. It includes 10 tasks under two core categories: token awareness and structural understanding. The dataset spans Chinese, English, and Korean, with a 35,927-instance evaluation set and a scalable synthetic data generation pipeline for training. The study evaluates over 30 leading commercial and open-source LLMs and trains a custom Qwen2.5-14B model using the GRPO training method.

Result: Results show that human performance significantly outpaces current LLMs, revealing persistent weaknesses in token-level reasoning.

Conclusion: TASE sheds light on the limitations of current LLMs in token-level reasoning and provides a new diagnostic lens for future improvements in low-level language understanding and cross-lingual generalization.

Abstract: While large language models (LLMs) have demonstrated remarkable performance
on high-level semantic tasks, they often struggle with fine-grained,
token-level understanding and structural reasoning--capabilities that are
essential for applications requiring precision and control. We introduce TASE,
a comprehensive benchmark designed to evaluate LLMs' ability to perceive and
reason about token-level information across languages. TASE covers 10 tasks
under two core categories: token awareness and structural understanding,
spanning Chinese, English, and Korean, with a 35,927-instance evaluation set
and a scalable synthetic data generation pipeline for training. Tasks include
character counting, token alignment, syntactic structure parsing, and length
constraint satisfaction. We evaluate over 30 leading commercial and open-source
LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a
custom Qwen2.5-14B model using the GRPO training method. Results show that
human performance significantly outpaces current LLMs, revealing persistent
weaknesses in token-level reasoning. TASE sheds light on these limitations and
provides a new diagnostic lens for future improvements in low-level language
understanding and cross-lingual generalization. Our code and dataset are
publicly available at https://github.com/cyzcz/Tase .

</details>


### [30] [Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations](https://arxiv.org/abs/2508.05470)
*Li-Chun Lu,Miri Liu,Pin-Chun Lu,Yufei Tian,Shao-Hua Sun,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文系统地分析了多种创造力评估方法在不同领域中的表现，发现它们存在局限性，需要更稳健的评估框架。


<details>
  <summary>Details</summary>
Motivation: 为了评估不同领域的创造力指标的一致性，并识别它们的局限性，从而为更有效的评估框架提供依据。

Method: 我们系统地检查、分析和比较了代表性的创造力测量方法--创造力指数、困惑度、句法模板和LLM-as-a-Judge，在不同的创造性领域，包括创造性写作、非常规问题解决和研究构想。

Result: 这些指标表现出有限的一致性，捕捉到创造力的不同维度。创造力指数侧重于词汇多样性，困惑度对模型置信度敏感，句法模板无法捕捉概念性创造力。此外，LLM-as-a-Judge表现出不稳定性和偏见。

Conclusion: 我们的研究强调了需要更稳健、可推广的评估框架，以更好地与人类对创造力的判断对齐。

Abstract: We systematically examine, analyze, and compare representative creativity
measures--creativity index, perplexity, syntactic templates, and
LLM-as-a-Judge--across diverse creative domains, including creative writing,
unconventional problem-solving, and research ideation. Our analyses reveal that
these metrics exhibit limited consistency, capturing different dimensions of
creativity. We highlight key limitations, including the creativity index's
focus on lexical diversity, perplexity's sensitivity to model confidence, and
syntactic templates' inability to capture conceptual creativity. Additionally,
LLM-as-a-Judge shows instability and bias. Our findings underscore the need for
more robust, generalizable evaluation frameworks that better align with human
judgments of creativity.

</details>


### [31] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Su Dong,Shengyuan Chen,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，称为逻辑增强生成（LAG），通过系统的提问分解和依赖感知推理来提高大型语言模型在知识密集型任务中的表现，减少幻觉并使问题解决更接近人类认知。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在知识密集型任务中表现出关键限制，经常在面对需要专业知识的问题时产生幻觉。虽然检索增强生成（RAG）通过整合外部知识来缓解这一问题，但由于其依赖直接语义检索和缺乏结构化的逻辑组织，在复杂推理场景中表现不佳。

Method: 本文引入了逻辑增强生成（LAG），通过系统的提问分解和依赖感知推理来重新构想知识增强。LAG首先将复杂问题分解为按逻辑依赖关系排序的原子子问题，然后依次解决这些子问题，使用先前的答案来指导后续子问题的上下文检索，确保逐步在逻辑链中得到支持。为了防止错误传播，LAG引入了一个逻辑终止机制，在遇到无法回答的子问题时停止推理，并减少过多推理上的浪费计算。最后，它综合所有子解决方案以生成验证后的响应。

Result: 在四个基准数据集上的实验表明，LAG显著提高了推理鲁棒性，减少了幻觉，并使LLM的问题解决与人类认知一致，为现有的RAG系统提供了一个有原则的替代方案。

Conclusion: 实验表明，LAG显著提高了推理鲁棒性，减少了幻觉，并使LLM的问题解决与人类认知一致，为现有的RAG系统提供了一个有原则的替代方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet exhibit critical limitations in knowledge-intensive
tasks, often generating hallucinations when faced with questions requiring
specialized expertise. While retrieval-augmented generation (RAG) mitigates
this by integrating external knowledge, it struggles with complex reasoning
scenarios due to its reliance on direct semantic retrieval and lack of
structured logical organization. Inspired by Cartesian principles from
\textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented
Generation (LAG), a novel paradigm that reframes knowledge augmentation through
systematic question decomposition and dependency-aware reasoning. Specifically,
LAG first decomposes complex questions into atomic sub-questions ordered by
logical dependencies. It then resolves these sequentially, using prior answers
to guide context retrieval for subsequent sub-questions, ensuring stepwise
grounding in logical chain. To prevent error propagation, LAG incorporates a
logical termination mechanism that halts inference upon encountering
unanswerable sub-questions and reduces wasted computation on excessive
reasoning. Finally, it synthesizes all sub-resolutions to generate verified
responses. Experiments on four benchmark datasets demonstrate that LAG
significantly enhances reasoning robustness, reduces hallucination, and aligns
LLM problem-solving with human cognition, offering a principled alternative to
existing RAG systems.

</details>


### [32] [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)
*Harsh Nishant Lalai,Raj Sanjay Shah,Jiaxin Pei,Sashank Varma,Yi-Chia Wang,Ali Emami*

Main category: cs.CL

TL;DR: 研究通过20个问题游戏评估了大型语言模型在地理和文化上的推理差异，发现模型在从全球北方和西方地区推断实体方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型在主动提问时的行为，以发现其隐性偏见。

Method: 通过20个问题游戏这一多轮推理任务，系统评估了使用新数据集Geo20Q+的地理性能差异。

Result: 结果表明，大型语言模型在从全球北方地区推断实体方面比全球南方地区更成功，且在全球西方地区比全球东方地区更成功。

Conclusion: 研究发现，大型语言模型在实体推理过程中存在地理和文化上的差异，这表明需要更创造性和自由形式的评估框架来揭示隐藏在标准提示设置中的细微偏见。

Abstract: Large Language Models (LLMs) have been extensively tuned to mitigate explicit
biases, yet they often exhibit subtle implicit biases rooted in their
pre-training data. Rather than directly probing LLMs with human-crafted
questions that may trigger guardrails, we propose studying how models behave
when they proactively ask questions themselves. The 20 Questions game, a
multi-turn deduction task, serves as an ideal testbed for this purpose. We
systematically evaluate geographic performance disparities in entity deduction
using a new dataset, Geo20Q+, consisting of both notable people and culturally
significant objects (e.g., foods, landmarks, animals) from diverse regions. We
test popular LLMs across two gameplay configurations (canonical 20-question and
unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,
French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs
are substantially more successful at deducing entities from the Global North
than the Global South, and the Global West than the Global East. While
Wikipedia pageviews and pre-training corpus frequency correlate mildly with
performance, they fail to fully explain these disparities. Notably, the
language in which the game is played has minimal impact on performance gaps.
These findings demonstrate the value of creative, free-form evaluation
frameworks for uncovering subtle biases in LLMs that remain hidden in standard
prompting setups. By analyzing how models initiate and pursue reasoning goals
over multiple turns, we find geographic and cultural disparities embedded in
their reasoning processes. We release the dataset (Geo20Q+) and code at
https://sites.google.com/view/llmbias20q/home.

</details>


### [33] [CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation](https://arxiv.org/abs/2508.05534)
*Santosh T. Y. S. S,Youssef Tarek Elkhayat,Oana Ichim,Pranav Shetty,Dongsheng Wang,Zhiqiang Ma,Armineh Nourbakhsh,Xiaomo Liu*

Main category: cs.CL

TL;DR: 本文介绍了CoCoLex，这是一种基于置信度的复制解码策略，通过动态结合模型生成的词汇分布和从上下文中复制得到的分布，提高了法律文本生成的准确性。实验结果表明，CoCoLex在多个法律基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理长而复杂的上下文方面具有优势，但它们生成不忠实、无根据或幻觉输出的倾向阻碍了其在法律领域的应用。虽然检索增强生成提供了一个有希望的解决方案，但它并不能保证提供的上下文会被有效整合。为了应对这一问题，提出了上下文感知的解码策略，但它们通常不会明确地强制遵守上下文。

Method: CoCoLex是一种基于置信度的复制解码策略，它动态地将模型生成的词汇分布与基于从上下文中复制得到的分布相结合。

Result: CoCoLex在五个法律基准测试中表现出色，特别是在长文本生成任务中优于现有的上下文感知解码方法。

Conclusion: 实验结果表明，CoCoLex在五个法律基准测试中优于现有的上下文感知解码方法，尤其是在长文本生成任务中表现出色。

Abstract: Due to their ability to process long and complex contexts, LLMs can offer key
benefits to the Legal domain, but their adoption has been hindered by their
tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While
Retrieval-Augmented Generation offers a promising solution by grounding
generations in external knowledge, it offers no guarantee that the provided
context will be effectively integrated. To address this, context-aware decoding
strategies have been proposed to amplify the influence of relevant context, but
they usually do not explicitly enforce faithfulness to the context. In this
work, we introduce Confidence-guided Copy-based Decoding for Legal Text
Generation (CoCoLex)-a decoding strategy that dynamically interpolates the
model produced vocabulary distribution with a distribution derived based on
copying from the context. CoCoLex encourages direct copying based on the
model's confidence, ensuring greater fidelity to the source. Experimental
results on five legal benchmarks demonstrate that CoCoLex outperforms existing
context-aware decoding methods, particularly in long-form generation tasks.

</details>


### [34] [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)
*Guang Yang,Xinyang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于频率的不确定性量化方法，在黑盒设置下利用符合预测（CP）来确保可证明的覆盖保证。实验结果表明，基于频率的PE在区分正确和错误预测方面优于基于logit的PE，并且该方法能有效控制经验性错误覆盖率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多项选择题回答（MCQA）中表现出显著的进步，但其内在的不可靠性（如幻觉和过度自信）限制了其在高风险领域的应用。因此，需要一种可靠的方法来量化不确定性。

Method: 本文提出了一种基于频率的不确定性量化方法，通过多次独立采样模型的输出分布，并以最频繁的样本作为参考来计算预测熵（PE）。

Result: 实验评估显示，基于频率的PE在区分正确和错误预测方面优于基于logit的PE，同时该方法能够有效控制经验性错误覆盖率。

Conclusion: 本文提出了一种基于频率的不确定性量化方法，在黑盒设置下利用符合预测（CP）来确保可证明的覆盖保证。实验结果表明，基于频率的PE在区分正确和错误预测方面优于基于logit的PE，并且该方法能有效控制经验性错误覆盖率，验证了采样频率可以作为黑盒场景中基于logit的概率的可行替代方案。

Abstract: Large Language Models (LLMs) have shown remarkable progress in
multiple-choice question answering (MCQA), but their inherent unreliability,
such as hallucination and overconfidence, limits their application in high-risk
domains. To address this, we propose a frequency-based uncertainty
quantification method under black-box settings, leveraging conformal prediction
(CP) to ensure provable coverage guarantees. Our approach involves multiple
independent samplings of the model's output distribution for each input, with
the most frequent sample serving as a reference to calculate predictive entropy
(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,
MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms
logit-based PE in distinguishing between correct and incorrect predictions, as
measured by AUROC. Furthermore, the method effectively controls the empirical
miscoverage rate under user-specified risk levels, validating that sampling
frequency can serve as a viable substitute for logit-based probabilities in
black-box scenarios. This work provides a distribution-free model-agnostic
framework for reliable uncertainty quantification in MCQA with guaranteed
coverage, enhancing the trustworthiness of LLMs in practical applications.

</details>


### [35] [Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs](https://arxiv.org/abs/2508.05553)
*Franziska Weeber,Tanise Ceron,Sebastian Padó*

Main category: cs.CL

TL;DR: 研究发现，多语言大语言模型在不同语言之间传递政治观点，但需要面对社会语言、文化和政治对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究多语言大语言模型是否在不同语言之间传递政治观点，或者是否存在每种语言的独特观点。

Method: 通过提示模型报告他们对投票建议应用程序中的政治声明的（不）同意程度来评估多语言大语言模型的政治观点。此外，在使用直接偏好优化和仅英语对齐数据对模型进行对齐之前和之后评估它们。

Result: 未对齐的模型在反映的政治观点中只有很少的显著跨语言差异。政治对齐几乎在所有五种语言中统一地改变了观点。

Conclusion: 在西方语言背景下，政治观点在不同语言之间可以转移，这表明了在实现多语言大语言模型的显式社会语言、文化和政治对齐方面面临的挑战。

Abstract: Public opinion surveys show cross-cultural differences in political opinions
between socio-cultural contexts. However, there is no clear evidence whether
these differences translate to cross-lingual differences in multilingual large
language models (MLLMs). We analyze whether opinions transfer between languages
or whether there are separate opinions for each language in MLLMs of various
sizes across five Western languages. We evaluate MLLMs' opinions by prompting
them to report their (dis)agreement with political statements from voting
advice applications. To better understand the interaction between languages in
the models, we evaluate them both before and after aligning them with more left
or right views using direct preference optimization and English alignment data
only. Our findings reveal that unaligned models show only very few significant
cross-lingual differences in the political opinions they reflect. The political
alignment shifts opinions almost uniformly across all five languages. We
conclude that in Western language contexts, political opinions transfer between
languages, demonstrating the challenges in achieving explicit socio-linguistic,
cultural, and political alignment of MLLMs.

</details>


### [36] [MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy](https://arxiv.org/abs/2508.05592)
*Shaoxiong Zhan,Yanlin Lai,Ziyu Lu,Dahua Lin,Ziqing Yang,Fei Tang*

Main category: cs.CL

TL;DR: MathSmith is a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. It constructs new problems from scratch, uses predefined strategies to increase difficulty, and employs reinforcement learning for optimization. Experiments show it outperforms existing methods and has strong scalability and generalization.


<details>
  <summary>Details</summary>
Motivation: The advancement of large language models in mathematical reasoning is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods rely on transforming human-written templates, limiting diversity and scalability.

Method: MathSmith constructs new mathematical problems from scratch by randomly sampling concept-explanation pairs from PlanetMath. It uses nine predefined strategies as soft constraints during rationales and adopts reinforcement learning to optimize structural validity, reasoning complexity, and answer consistency.

Result: Experiments across five benchmarks show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. A weakness-focused variant generation module enables targeted improvement on specific concepts.

Conclusion: MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities.

Abstract: Large language models have achieved substantial progress in mathematical
reasoning, yet their advancement is limited by the scarcity of high-quality,
high-difficulty training data. Existing synthesis methods largely rely on
transforming human-written templates, limiting both diversity and scalability.
We propose MathSmith, a novel framework for synthesizing challenging
mathematical problems to enhance LLM reasoning. Rather than modifying existing
problems, MathSmith constructs new ones from scratch by randomly sampling
concept-explanation pairs from PlanetMath, ensuring data independence and
avoiding contamination. To increase difficulty, we design nine predefined
strategies as soft constraints during rationales. We further adopts
reinforcement learning to jointly optimize structural validity, reasoning
complexity, and answer consistency. The length of the reasoning trace generated
under autoregressive prompting is used to reflect cognitive complexity,
encouraging the creation of more demanding problems aligned with
long-chain-of-thought reasoning. Experiments across five benchmarks,
categorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,
OlympiadBench), show that MathSmith consistently outperforms existing baselines
under both short and long CoT settings. Additionally, a weakness-focused
variant generation module enables targeted improvement on specific concepts.
Overall, MathSmith exhibits strong scalability, generalization, and
transferability, highlighting the promise of high-difficulty synthetic data in
advancing LLM reasoning capabilities.

</details>


### [37] [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)
*Haitao Hong,Yuchen Yan,Xingyu Wu,Guiyang Hou,Wenqi Zhang,Weiming Lu,Yongliang Shen,Jun Xiao*

Main category: cs.CL

TL;DR: 本文提出Cooper框架，通过联合优化策略模型和奖励模型，提高大型语言模型的推理能力并缓解奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于规则的奖励和基于模型的奖励各有局限，需要一种更鲁棒的方法来增强大型语言模型的推理能力。

Method: 提出Cooper框架，联合优化策略模型和奖励模型，并引入混合标注策略和基于参考的奖励建模范式。

Result: Cooper框架有效缓解了奖励黑客问题，并提高了端到端的强化学习性能，例如在Qwen2.5-1.5B-Instruct上平均准确率提升了0.54%。

Conclusion: 动态更新奖励模型是应对奖励黑客的有效方法，为更好地将奖励模型整合到强化学习中提供了参考。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
reasoning tasks, where reinforcement learning (RL) serves as a key algorithm
for enhancing their reasoning capabilities. Currently, there are two mainstream
reward paradigms: model-based rewards and rule-based rewards. However, both
approaches suffer from limitations: rule-based rewards lack robustness, while
model-based rewards are vulnerable to reward hacking. To address these issues,
we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework
that jointly optimizes both the policy model and the reward model. Cooper
leverages the high precision of rule-based rewards when identifying correct
responses, and dynamically constructs and selects positive-negative sample
pairs for continued training the reward model. This design enhances robustness
and mitigates the risk of reward hacking. To further support Cooper, we
introduce a hybrid annotation strategy that efficiently and accurately
generates training data for the reward model. We also propose a reference-based
reward modeling paradigm, where the reward model takes a reference answer as
input. Based on this design, we train a reward model named VerifyRM, which
achieves higher accuracy on VerifyBench compared to other models of the same
size. We conduct reinforcement learning using both VerifyRM and Cooper. Our
experiments show that Cooper not only alleviates reward hacking but also
improves end-to-end RL performance, for instance, achieving a 0.54% gain in
average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that
dynamically updating reward model is an effective way to combat reward hacking,
providing a reference for better integrating reward models into RL.

</details>


### [38] [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)
*Zixuan Wang,Dingming Li,Hongxing Li,Shuo Chen,Yuchen Yan,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 本文提出OmniEAR框架，用于评估语言模型在具身任务中的推理能力。结果显示，具身推理对当前模型构成重大挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在抽象推理方面表现出色，但其在具身代理推理方面的能力尚未得到充分探索。现有的基准测试提供预定义的工具集或明确的合作指令，而OmniEAR要求代理动态获取能力并自主决定协调策略。

Method: 本文提出了OmniEAR框架，用于评估语言模型在具身任务中关于物理交互、工具使用和多智能体协调的推理能力。通过文本环境表示，建模连续物理属性和复杂空间关系。

Result: 系统评估显示，当模型必须从约束中推理时，性能严重下降。尽管在明确指令下取得85-96%的成功率，但在工具推理和隐式协作中分别降至56-85%和63-85%，复合任务失败率超过50%。完全环境信息会降低协调性能，表明模型无法过滤任务相关约束。微调显著提高了单智能体任务（0.6%至76.3%），但对多智能体任务提升有限（1.5%至5.5%）。

Conclusion: 本文展示了OmniEAR作为一个严格的基准，用于评估和推进具身AI系统。研究发现，具身推理对当前模型来说是根本不同的挑战。

Abstract: Large language models excel at abstract reasoning but their capacity for
embodied agent reasoning remains largely unexplored. We present OmniEAR, a
comprehensive framework for evaluating how language models reason about
physical interactions, tool usage, and multi-agent coordination in embodied
tasks. Unlike existing benchmarks that provide predefined tool sets or explicit
collaboration directives, OmniEAR requires agents to dynamically acquire
capabilities and autonomously determine coordination strategies based on task
demands. Through text-based environment representation, we model continuous
physical properties and complex spatial relationships across 1,500 scenarios
spanning household and industrial domains. Our systematic evaluation reveals
severe performance degradation when models must reason from constraints: while
achieving 85-96% success with explicit instructions, performance drops to
56-85% for tool reasoning and 63-85% for implicit collaboration, with compound
tasks showing over 50% failure rates. Surprisingly, complete environmental
information degrades coordination performance, indicating models cannot filter
task-relevant constraints. Fine-tuning improves single-agent tasks dramatically
(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing
fundamental architectural limitations. These findings demonstrate that embodied
reasoning poses fundamentally different challenges than current models can
address, establishing OmniEAR as a rigorous benchmark for evaluating and
advancing embodied AI systems. Our code and data are included in the
supplementary materials and will be open-sourced upon acceptance.

</details>


### [39] [Learning to Reason for Factuality](https://arxiv.org/abs/2508.05618)
*Xilun Chen,Ilia Kulikov,Vincent-Pierre Berges,Barlas Oğuz,Rulin Shao,Gargi Ghosh,Jason Weston,Wen-tau Yih*

Main category: cs.CL

TL;DR: 本文提出一种新的奖励函数，结合事实精确度、响应详细程度和答案相关性，并通过在线强化学习提升事实推理能力，在多个基准测试中显著减少幻觉并提高回答质量。


<details>
  <summary>Details</summary>
Motivation: R-LLMs在复杂推理任务上取得了显著进展，但在长格式事实性基准测试中往往难以保持事实准确性，生成比非推理模型更多的幻觉。然而，将在线强化学习扩展到长格式事实性设置面临诸多挑战，因为缺乏可靠的验证方法。

Method: 我们提出了一种新的奖励函数，同时考虑事实精确度、响应详细程度和答案相关性，并应用在线强化学习来学习高质量的事实推理。

Result: 在六个长格式事实性基准测试中，我们的事实推理模型实现了幻觉率平均降低23.1个百分点，回答详细程度增加23%，且整体响应有用性没有下降。

Conclusion: 我们的事实推理模型在六个长格式事实性基准测试中实现了幻觉率平均降低23.1个百分点，回答详细程度增加23%，且整体响应有用性没有下降。

Abstract: Reasoning Large Language Models (R-LLMs) have significantly advanced complex
reasoning tasks but often struggle with factuality, generating substantially
more hallucinations than their non-reasoning counterparts on long-form
factuality benchmarks. However, extending online Reinforcement Learning (RL), a
key component in recent R-LLM advancements, to the long-form factuality setting
poses several unique challenges due to the lack of reliable verification
methods. Previous work has utilized automatic factuality evaluation frameworks
such as FActScore to curate preference data in the offline RL setting, yet we
find that directly leveraging such methods as the reward in online RL leads to
reward hacking in multiple ways, such as producing less detailed or relevant
responses. We propose a novel reward function that simultaneously considers the
factual precision, response detail level, and answer relevance, and applies
online RL to learn high quality factual reasoning. Evaluated on six long-form
factuality benchmarks, our factual reasoning model achieves an average
reduction of 23.1 percentage points in hallucination rate, a 23% increase in
answer detail level, and no degradation in the overall response helpfulness.

</details>


### [40] [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)
*Brandon Jaipersaud,David Krueger,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在多轮对话中的说服动态，并发现线性探针可以有效分析说服的成功、被说服者的个性和策略。


<details>
  <summary>Details</summary>
Motivation: 由于对大型语言模型如何说服人类的理解有限，本文旨在通过使用线性探针来研究自然的多轮对话中的说服动态。

Method: 本文利用认知科学的见解，训练探针来分析说服的不同方面，包括说服成功、被说服者个性和说服策略。

Result: 探针能够捕捉说服在样本和数据集层面的各种方面，例如识别被说服者被说服的时间点或整个数据集中说服成功的发生时间。此外，探针在某些情况下甚至优于基于提示的方法。

Conclusion: 本文表明，线性探针可以作为一种可行的方法来研究复杂行为，如欺骗和操纵，特别是在多轮对话和大规模数据集分析中。

Abstract: Large Language Models (LLMs) have started to demonstrate the ability to
persuade humans, yet our understanding of how this dynamic transpires is
limited. Recent work has used linear probes, lightweight tools for analyzing
model representations, to study various LLM skills such as the ability to model
user sentiment and political perspective. Motivated by this, we apply probes to
study persuasion dynamics in natural, multi-turn conversations. We leverage
insights from cognitive science to train probes on distinct aspects of
persuasion: persuasion success, persuadee personality, and persuasion strategy.
Despite their simplicity, we show that they capture various aspects of
persuasion at both the sample and dataset levels. For instance, probes can
identify the point in a conversation where the persuadee was persuaded or where
persuasive success generally occurs across the entire dataset. We also show
that in addition to being faster than expensive prompting-based approaches,
probes can do just as well and even outperform prompting in some settings, such
as when uncovering persuasion strategy. This suggests probes as a plausible
avenue for studying other complex behaviours such as deception and
manipulation, especially in multi-turn settings and large-scale dataset
analysis where prompting-based methods would be computationally inefficient.

</details>


### [41] [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)
*Mehrdad Zakershahrak,Samira Ghodratnama*

Main category: cs.CL

TL;DR: H-NET++ is a hierarchical dynamic-chunking model that improves upon byte-level language models for morphologically-rich languages by achieving better compression, performance, and robustness without explicit supervision.


<details>
  <summary>Details</summary>
Motivation: Byte-level language models face computational challenges in morphologically-rich languages due to the large number of bytes per word. The goal is to develop an efficient and effective tokenizer-free solution for these languages.

Method: H-NET++ is a hierarchical dynamic-chunking model that learns linguistically-informed segmentation through end-to-end training. It includes a lightweight Transformer context-mixer, a two-level latent hyper-prior for document-level consistency, specialized handling of orthographic artifacts, and curriculum-based training with staged sequence lengths.

Result: On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art results: 0.159 BPB reduction versus BPE-based GPT-2-fa, 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ corruption, and 73.8% F1 on gold morphological boundaries. The learned chunks align with Persian morphology without explicit supervision.

Conclusion: H-NET++ demonstrates that hierarchical dynamic chunking provides an effective tokenizer-free solution for morphologically-rich languages while maintaining computational efficiency.

Abstract: Byte-level language models eliminate fragile tokenizers but face
computational challenges in morphologically-rich languages (MRLs), where words
span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that
learns linguistically-informed segmentation through end-to-end training. Key
innovations include: (1) a lightweight Transformer context-mixer (1.9M
parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for
document-level consistency, (3) specialized handling of orthographic artifacts
(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence
lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art
results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better
compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ
corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks
align with Persian morphology without explicit supervision, demonstrating that
hierarchical dynamic chunking provides an effective tokenizer-free solution for
MRLs while maintaining computational efficiency.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [42] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 本文提出了一种名为MICoBot的系统，用于长期人机协作。该系统通过三个层次的决策机制，有效适应不同人类用户的动态需求，显著提高了任务成功率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 有效的长期人机协作机器人系统必须适应各种人类合作伙伴，他们的身体行为、愿意协助的程度以及对机器人能力的理解可能会随时间变化。这需要一个紧密耦合的通信循环，使两个代理能够灵活地提出、接受或拒绝请求，以有效地完成任务。

Method: 我们应用了混合主动性对话范式来协作的人机团队，并提出了MICoBot系统，该系统处理双方使用自然语言主动制定、接受或拒绝提案的常见场景。MICoBot在三个层次上做出决策：(1)元规划器考虑人类对话以制定和编码高层次的合作策略，(2)规划器根据机器人的能力（通过模拟预训练的效用模型测量）和人类帮助的估计可用性最优地分配剩余步骤，(3)动作执行器决定执行低级动作或对人类说的话。

Result: 我们在模拟和现实世界中进行了广泛的评估——在一个物理机器人上，有18个独特的用户参与，持续了27小时——证明了我们的方法有效与多样化的人类用户合作，任务成功率和用户体验显著优于纯LLM基线和其他代理分配模型。

Conclusion: 我们的方法在与多样化的用户合作方面表现出色，任务成功率和用户体验显著优于纯LLM基线和其他代理分配模型。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
*Mahmoud Chick Zaouali,Todd Charter,Yehor Karpichev,Brandon Haworth,Homayoun Najjjaran*

Main category: cs.GR

TL;DR: 本文综述了将语言指导与3D高斯散射结合的研究，讨论了其理论基础、集成策略、实际应用案例以及存在的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于Gaussian Splatting在实时3D场景表示中的高效性和表达性，以及语言模型和语言嵌入的集成带来了新的可能性，因此需要对这一新兴领域进行全面概述。

Method: 本文通过结构化回顾的方式，详细介绍了理论基础、集成策略和实际应用案例。

Result: 本文提供了对当前研究的结构化回顾，涵盖了理论基础、集成策略和实际应用案例，并指出了关键限制和未来方向。

Conclusion: 本文综述了将语言指导与3D高斯散射结合的当前研究努力，指出了计算瓶颈、泛化能力和语义标注3D高斯数据稀缺等关键限制，并提出了未来方向。

Abstract: Gaussian Splatting has rapidly emerged as a transformative technique for
real-time 3D scene representation, offering a highly efficient and expressive
alternative to Neural Radiance Fields (NeRF). Its ability to render complex
scenes with high fidelity has enabled progress across domains such as scene
reconstruction, robotics, and interactive content creation. More recently, the
integration of Large Language Models (LLMs) and language embeddings into
Gaussian Splatting pipelines has opened new possibilities for text-conditioned
generation, editing, and semantic scene understanding. Despite these advances,
a comprehensive overview of this emerging intersection has been lacking. This
survey presents a structured review of current research efforts that combine
language guidance with 3D Gaussian Splatting, detailing theoretical
foundations, integration strategies, and real-world use cases. We highlight key
limitations such as computational bottlenecks, generalizability, and the
scarcity of semantically annotated 3D Gaussian data and outline open challenges
and future directions for advancing language-guided 3D scene understanding
using Gaussian Splatting.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [44] [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
*Ugur Cetintemel,Shu Chen,Alexander W. Lee,Deepti Raghavan*

Main category: cs.DB

TL;DR: SPEAR is a language and runtime for managing prompts in LLM pipelines, making them structured, adaptive, and first-class components to enable better reuse, optimization, and runtime control.


<details>
  <summary>Details</summary>
Motivation: The central element guiding LLM pipelines -- the prompt -- remains a brittle, opaque string, disconnected from the surrounding dataflow, limiting reuse, optimization, and runtime control.

Method: SPEAR introduces a prompt algebra that governs how prompts are constructed and adapted within a pipeline, supporting multiple refinement modes and enabling optimizations such as operator fusion, prefix caching, and view reuse.

Result: Preliminary experiments show the behavior of different refinement modes compared to static prompts and agentic retries, as well as the impact of prompt-level optimizations such as operator fusion.

Conclusion: SPEAR provides a new approach to managing prompts in LLM pipelines by making them structured, adaptive, and first-class components, enabling better reuse, optimization, and runtime control.

Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they
retrieve external context, compose intermediate outputs, validate results, and
adapt based on runtime feedback. Yet, the central element guiding this process
-- the prompt -- remains a brittle, opaque string, disconnected from the
surrounding dataflow. This disconnect limits reuse, optimization, and runtime
control.
  In this paper, we describe our vision and an initial design for SPEAR, a
language and runtime that fills this prompt management gap by making prompts
structured, adaptive, and first-class components of the execution model. SPEAR
enables (1) runtime prompt refinement -- modifying prompts dynamically in
response to execution-time signals such as confidence, latency, or missing
context; and (2) structured prompt management -- organizing prompt fragments
into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and
adapted within a pipeline. It supports multiple refinement modes (manual,
assisted, and automatic), giving developers a balance between control and
automation. By treating prompt logic as structured data, SPEAR enables
optimizations such as operator fusion, prefix caching, and view reuse.
Preliminary experiments quantify the behavior of different refinement modes
compared to static prompts and agentic retries, as well as the impact of
prompt-level optimizations such as operator fusion.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [45] [SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription](https://arxiv.org/abs/2508.05554)
*Raymond Grossman,Taejin Park,Kunal Dhawan,Andrew Titus,Sophia Zhi,Yulia Shchadilova,Weiqing Wang,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.SD

TL;DR: SPGISpeech 2.0 是一个用于金融领域说话人标记转录的新数据集，包含更多专业转录的收益电话音频片段，并提供了通话和说话人信息，以支持多说话人语音识别。


<details>
  <summary>Details</summary>
Motivation: SPGISpeech 2.0 的引入是为了提高适用建模任务的多样性，同时保持原始 SPGISpeech 数据集的核心特性：可用于端到端自动语音识别（ASR）的音频片段及其对应的完整格式文本转录。

Method: SPGISpeech 2.0 包含了3,780小时的专业转录的收益电话音频片段，并提供了每个音频片段的通话和说话人信息，以支持多说话人语音识别。

Result: 通过在 SPGISpeech 2.0 上微调，流行语音识别模型的说话人标记 ASR 性能得到了提升。

Conclusion: SPGISpeech 2.0 是一个适用于金融领域说话人标记转录的数据集，它能够促进语音识别技术的进步并激发广泛的研究应用。

Abstract: We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged
transcription in the financial domain. SPGISpeech 2.0 improves the diversity of
applicable modeling tasks while maintaining the core characteristic of the
original SPGISpeech dataset: audio snippets and their corresponding fully
formatted text transcriptions, usable for end-to-end automatic speech
recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of
professionally transcribed earnings calls. Furthermore, the dataset contains
call and speaker information for each audio snippet facilitating multi-talker
ASR. We validate the utility of SPGISpeech 2.0 through improvements in
speaker-tagged ASR performance of popular speech recognition models after
fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect
SPGISpeech 2.0 to foster advancements in speech recognition technologies and
inspire a wide range of research applications.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [46] [JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering](https://arxiv.org/abs/2508.05087)
*Renmiao Chen,Shiyao Cui,Xuancheng Huang,Chengwei Pan,Victor Shea-Jay Huang,QingLin Zhang,Xuan Ouyang,Zhexin Zhang,Hongning Wang,Minlie Huang*

Main category: cs.MM

TL;DR: This paper introduces JPS, a method for jailbreaking multimodal large language models by combining visual perturbation and textual steering, achieving high attack success rates and fulfilling malicious intent.


<details>
  <summary>Details</summary>
Motivation: Current research on jailbreak attacks against MLLMs focuses on maximizing attack success rate (ASR) but often overlooks whether the generated responses fulfill the attacker's malicious intent, leading to low-quality outputs.

Method: JPS utilizes target-guided adversarial image perturbations for effective safety bypass, complemented by 'steering prompt' optimized via a multi-agent system to guide LLM responses fulfilling the attackers' intent. These components undergo iterative co-optimization.

Result: JPS sets a new state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with analyses confirming its efficacy.

Conclusion: JPS achieves a new state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, confirming its efficacy.

Abstract: Jailbreak attacks against multimodal large language Models (MLLMs) are a
significant research focus. Current research predominantly focuses on
maximizing attack success rate (ASR), often overlooking whether the generated
responses actually fulfill the attacker's malicious intent. This oversight
frequently leads to low-quality outputs that bypass safety filters but lack
substantial harmful content. To address this gap, we propose JPS,
\underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation
and textual \underline{S}teering, which achieves jailbreaks via corporation of
visual image and textually steering prompt. Specifically, JPS utilizes
target-guided adversarial image perturbations for effective safety bypass,
complemented by "steering prompt" optimized via a multi-agent system to
specifically guide LLM responses fulfilling the attackers' intent. These visual
and textual components undergo iterative co-optimization for enhanced
performance. To evaluate the quality of attack outcomes, we propose the
Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a
Reasoning-LLM-based evaluator. Our experiments show JPS sets a new
state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with
analyses confirming its efficacy. Codes are available at
\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.
\color{warningcolor}{Warning: This paper contains potentially sensitive
contents.}

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [47] [Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages](https://arxiv.org/abs/2508.05149)
*Seraphina Fong,Marco Matassoni,Alessio Brutti*

Main category: eess.AS

TL;DR: 本文探讨了在低资源环境下使用语音大语言模型进行自动语音识别的可能性，并展示了通过预训练多语LLM来减轻数据稀缺性影响的有效方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在高资源语言中表现出色，但它们在低资源设置中的适用性仍不明确。因此，本文旨在探索语音LLMs在低资源自动语音识别中的应用。

Method: 本文采用了SLAM-ASR框架，其中可训练的轻量级投影器连接了语音编码器和大语言模型（LLM）。通过评估训练数据量的要求以及利用预训练的多语LLM进行实验，验证了方法的有效性。

Result: 研究结果表明，通过使用预训练的多语LLM（如EuroLLM、Salamandra）和whisper-large-v3-turbo，可以在多个公共基准上取得良好的性能，为未来优化语音LLMs在低资源语言和多语种中的应用提供了见解。

Conclusion: 本文研究了在低资源设置中使用语音大语言模型（Speech LLMs）进行自动语音识别的潜力，并提出了SLAM-ASR框架。研究结果表明，利用预训练的单语或多语投影器可以减轻数据稀缺性的影响，特别是在小训练集的情况下。

Abstract: Large language models (LLMs) have demonstrated potential in handling spoken
inputs for high-resource languages, reaching state-of-the-art performance in
various tasks. However, their applicability is still less explored in
low-resource settings. This work investigates the use of Speech LLMs for
low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a
trainable lightweight projector connects a speech encoder and a LLM. Firstly,
we assess training data volume requirements to match Whisper-only performance,
re-emphasizing the challenges of limited data. Secondly, we show that
leveraging mono- or multilingual projectors pretrained on high-resource
languages reduces the impact of data scarcity, especially with small training
sets. Using multilingual LLMs (EuroLLM, Salamandra) with
whisper-large-v3-turbo, we evaluate performance on several public benchmarks,
providing insights for future research on optimizing Speech LLMs for
low-resource languages and multilinguality.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [48] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 本文提出了一种基于LLM的RTL代码生成框架，通过错误纠正技术显著提高了性能，实验结果显示其在VerilogEval基准测试中达到91.0%的准确率，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大型语言模型（LLM）的寄存器传输级（RTL）代码生成具有令人鼓舞的潜力，但整体成功率仍然不令人满意。错误源于各种因素，对具体失败原因的有限理解阻碍了改进。

Method: 我们提出了针对错误纠正的技术，包括构建领域特定的知识库、使用检索增强生成（RAG）来提供必要的RTL知识、引入设计描述规则并实现规则检查机制、集成外部工具将输入转换为LLM兼容的元格式，以及采用迭代调试循环（仿真-错误定位-修正）。

Result: 将这些技术整合到一个LLM-based框架中显著提高了性能。实验结果表明，我们的增强框架在VerilogEval基准测试中达到了91.0%的准确率，比基线代码生成方法提高了32.7%。

Conclusion: 我们的增强框架在VerilogEval基准测试中达到了91.0%的准确率，比基线代码生成方法提高了32.7%，证明了我们方法的有效性。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [49] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文提出了一种统一框架，通过引入LCB-RB基准、OD-based方法和P-GRPO强化学习方法，有效结合了推理过程的质量，显著提升了代码生成任务的性能，并展示了方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法依赖于基于结果的奖励，忽视了中间推理过程的质量。直接监督推理过程虽然有前景，但容易受到奖励黑客的影响。因此，需要一种能够有效结合推理过程质量的方法。

Method: 本文提出了一个统一框架，包括LCB-RB基准、OD-based方法和P-GRPO强化学习方法。LCB-RB用于评估推理过程的质量，OD-based方法通过优化和降级初始推理路径生成高质量的偏好对，P-GRPO则基于任务成功条件应用过程奖励，以减少奖励黑客行为。

Result: 使用OD-based方法的7B参数奖励模型在LCB-RB上取得了最先进的性能，并且在其他基准上表现出良好的泛化能力。使用P-GRPO的7B参数模型在多种代码生成任务中表现优越，比仅基于结果的基线高出4.5%，并达到了GPT-4-Turbo的水平。此外，方法还被扩展到数学任务中，验证了其泛化能力。

Conclusion: 本文提出了一种统一框架，可以有效地在强化学习中融入推理过程的质量。通过引入LCB-RB基准和OD-based方法，以及P-GRPO强化学习方法，显著提升了代码生成任务的性能，并展示了方法的泛化能力。

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [50] [Federal Reserve Communication and the COVID-19 Pandemic](https://arxiv.org/abs/2508.04830)
*Jonathan Benchimol,Sophia Kazinnik,Yossi Saadon*

Main category: econ.GN

TL;DR: 本研究分析了美联储在新冠疫情中的沟通策略，发现其更关注金融稳定、市场波动、社会福利和非常规货币政策，并且这种沟通方式已成为新常态。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解中央银行在危机期间的沟通策略如何演变以及如何适应特殊的经济环境。

Method: 研究使用了针对新冠疫情、非常规货币政策（UMP）和金融稳定的专用词典，结合情感分析和主题建模技术，对美联储的沟通策略进行了比较分析。

Result: 研究发现，美联储在疫情期间的沟通和政策行动比以往危机更为反应性，并且金融稳定相关的情感下降预示了随后的宽松货币政策决策。此外，自全球金融危机以来，沟通非常规货币政策已成为美联储会议纪要和主席演讲的“新常态”。

Conclusion: 研究发现，美联储在疫情期间的沟通策略与以往经济压力时期有所不同，更加关注金融稳定、市场波动、社会福利和非常规货币政策，并且这种沟通策略反映了机构在经历经济动荡后的适应性变化。

Abstract: In this study, we examine the Federal Reserve's communication strategies
during the COVID-19 pandemic, comparing them with communication during previous
periods of economic stress. Using specialized dictionaries tailored to
COVID-19, unconventional monetary policy (UMP), and financial stability,
combined with sentiment analysis and topic modeling techniques, we identify a
distinct focus in Fed communication during the pandemic on financial stability,
market volatility, social welfare, and UMP, characterized by notable contextual
uncertainty. Through comparative analysis, we juxtapose the Fed's communication
during the COVID-19 crisis with its responses during the dot-com and global
financial crises, examining content, sentiment, and timing dimensions. Our
findings reveal that Fed communication and policy actions were more reactive to
the COVID-19 crisis than to previous crises. Additionally, declining sentiment
related to financial stability in interest rate announcements and minutes
anticipated subsequent accommodative monetary policy decisions. We further
document that communicating about UMP has become the "new normal" for the Fed's
Federal Open Market Committee meeting minutes and Chairman's speeches since the
Global Financial Crisis, reflecting an institutional adaptation in
communication strategy following periods of economic distress. These findings
contribute to our understanding of how central bank communication evolves
during crises and how communication strategies adapt to exceptional economic
circumstances.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [51] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: 本研究通过双源策略和MELLA数据集，提升了多模态大语言模型在低资源语言中的表现，实现了语言能力和文化扎根度的双重目标。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言增强方法通常仅限于文本模态或依赖机器翻译，无法有效满足低资源语言用户的需求。因此，需要一种更全面的方法来提升多模态大语言模型在低资源语言中的表现。

Method: 本研究提出了一种双源策略，通过收集针对每个目标的数据（如文化相关的本地网页替代文本和MLLM生成的字幕）来实现语言能力和文化扎根度的双重目标。此外，还引入了MELLA数据集作为具体实现。

Result: 实验结果显示，在MELLA数据集上微调后，八种语言的多模态大语言模型在各种骨干网络上均表现出整体性能提升，生成了更丰富的描述。

Conclusion: 本研究提出了一个双源策略，并引入了MELLA数据集，以提高多模态大语言模型在低资源语言环境下的表现。实验结果表明，微调后的模型在多种语言上表现出性能提升，能够生成更丰富的描述。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in
high-resource languages. However, their effectiveness diminishes significantly
in the contexts of low-resource languages. Current multilingual enhancement
methods are often limited to text modality or rely solely on machine
translation. While such approaches help models acquire basic linguistic
capabilities and produce "thin descriptions", they neglect the importance of
multimodal informativeness and cultural groundedness, both of which are crucial
for serving low-resource language users effectively. To bridge this gap, in
this study, we identify two significant objectives for a truly effective MLLM
in low-resource language settings, namely 1) linguistic capability and 2)
cultural groundedness, placing special emphasis on cultural awareness. To
achieve these dual objectives, we propose a dual-source strategy that guides
the collection of data tailored to each goal, sourcing native web alt-text for
culture and MLLM-generated captions for linguistics. As a concrete
implementation, we introduce MELLA, a multimodal, multilingual dataset.
Experiment results show that after fine-tuning on MELLA, there is a general
performance improvement for the eight languages on various MLLM backbones, with
models producing "thick descriptions". We verify that the performance gains are
from both cultural knowledge enhancement and linguistic capability enhancement.
Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.

</details>


### [52] [Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision](https://arxiv.org/abs/2508.05606)
*Luozheng Qin,Jia Gong,Yuqing Sun,Tianjiao Li,Mengping Yang,Xiaomeng Yang,Chao Qu,Zhiyu Tan,Hao Li*

Main category: cs.CV

TL;DR: Uni-CoT is a unified chain-of-thought framework that enables coherent and grounded multimodal reasoning through a two-level reasoning paradigm and structured training, achieving SOTA performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with modeling visual state transitions and coherent visual trajectories due to limited capacity or fragmented architectures. Uni-CoT aims to overcome these limitations by enabling coherent and grounded multimodal reasoning within a single unified model.

Method: Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. It also introduces a structured training paradigm combining interleaved image-text supervision with multi-task objectives.

Result: Uni-CoT achieves SOTA performance on reasoning-driven image generation and editing benchmarks, and can be efficiently trained using only 8 A100 GPUs.

Conclusion: Uni-CoT demonstrates SOTA performance and strong generalization, establishing it as a promising solution for multi-modal reasoning.

Abstract: Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large
Language Models (LLMs) by decomposing complex tasks into simpler, sequential
subtasks. However, extending CoT to vision-language reasoning tasks remains
challenging, as it often requires interpreting transitions of visual states to
support reasoning. Existing methods often struggle with this due to limited
capacity of modeling visual state transitions or incoherent visual trajectories
caused by fragmented architectures.
  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought
framework that enables coherent and grounded multimodal reasoning within a
single unified model. The key idea is to leverage a model capable of both image
understanding and generation to reason over visual content and model evolving
visual states. However, empowering a unified model to achieve that is
non-trivial, given the high computational cost and the burden of training. To
address this, Uni-CoT introduces a novel two-level reasoning paradigm: A
Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask
execution. This design significantly reduces the computational overhead.
Furthermore, we introduce a structured training paradigm that combines
interleaved image-text supervision for macro-level CoT with multi-task
objectives for micro-level CoT. Together, these innovations allow Uni-CoT to
perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our
design, all experiments can be efficiently completed using only 8 A100 GPUs
with 80GB VRAM each. Experimental results on reasoning-driven image generation
benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT
demonstrates SOTA performance and strong generalization, establishing Uni-CoT
as a promising solution for multi-modal reasoning. Project Page and Code:
https://sais-fuxi.github.io/projects/uni-cot/

</details>


### [53] [Test-Time Reinforcement Learning for GUI Grounding via Region Consistency](https://arxiv.org/abs/2508.05615)
*Yong Du,Yuchen Yan,Fei Tang,Zhengxi Lu,Chang Zong,Weiming Lu,Shengpei Jiang,Yongliang Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时扩展方法GUI-RC和测试时强化学习方法GUI-RCPO，用于改进GUI接地任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通过广泛的监督训练或带有标记奖励的强化学习取得强大的性能，但它们受到像素级注释成本和可用性的限制。

Method: 我们提出了GUI-RC（区域一致性），一种测试时扩展方法，从多个采样预测中构建空间投票网格以识别模型显示最高一致性的共识区域。此外，我们引入了GUI-RCPO（区域一致性策略优化），将这些一致性模式转化为测试时强化学习的奖励。

Result: GUI-RC在ScreenSpot基准上提高了各种架构的准确性2-3%。GUI-RCPO进一步通过自监督优化提高了Qwen2.5-VL-3B-Instruct的性能。

Conclusion: 我们的方法揭示了测试时扩展和测试时强化学习在GUI接地中的未开发潜力，为更强大和数据高效的GUI代理提供了有希望的路径。

Abstract: Graphical User Interface (GUI) grounding, the task of mapping natural
language instructions to precise screen coordinates, is fundamental to
autonomous GUI agents. While existing methods achieve strong performance
through extensive supervised training or reinforcement learning with labeled
rewards, they remain constrained by the cost and availability of pixel-level
annotations. We observe that when models generate multiple predictions for the
same GUI element, the spatial overlap patterns reveal implicit confidence
signals that can guide more accurate localization. Leveraging this insight, we
propose GUI-RC (Region Consistency), a test-time scaling method that constructs
spatial voting grids from multiple sampled predictions to identify consensus
regions where models show highest agreement. Without any training, GUI-RC
improves accuracy by 2-3% across various architectures on ScreenSpot
benchmarks. We further introduce GUI-RCPO (Region Consistency Policy
Optimization), which transforms these consistency patterns into rewards for
test-time reinforcement learning. By computing how well each prediction aligns
with the collective consensus, GUI-RCPO enables models to iteratively refine
their outputs on unlabeled data during inference. Extensive experiments
demonstrate the generality of our approach: GUI-RC boosts
Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO
further improves it to 85.14% through self-supervised optimization. Our
approach reveals the untapped potential of test-time scaling and test-time
reinforcement learning for GUI grounding, offering a promising path toward more
robust and data-efficient GUI agents.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning](https://arxiv.org/abs/2508.05129)
*Wuqiang Zheng,Yiyan Xu,Xinyu Lin,Chongming Gao,Wenjie Wang,Fuli Feng*

Main category: cs.IR

TL;DR: PaperEval is a new framework for evaluating academic papers using LLMs, which improves upon existing methods by incorporating domain awareness and advanced reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing methods for automated paper evaluation, such as outdated domain knowledge and limited reasoning capabilities.

Method: PaperEval is a novel LLM-based framework that includes a domain-aware paper retrieval module and a latent reasoning mechanism. It also uses a progressive ranking optimization strategy to guide the reasoning process.

Result: Experiments on two datasets show that PaperEval consistently outperforms existing methods in evaluating academic impact and paper quality. It has also been deployed in a real-world paper recommendation system with strong engagement.

Conclusion: PaperEval demonstrates practical effectiveness in evaluating academic papers and has been successfully deployed in a real-world paper recommendation system.

Abstract: With the rapid and continuous increase in academic publications, identifying
high-quality research has become an increasingly pressing challenge. While
recent methods leveraging Large Language Models (LLMs) for automated paper
evaluation have shown great promise, they are often constrained by outdated
domain knowledge and limited reasoning capabilities. In this work, we present
PaperEval, a novel LLM-based framework for automated paper evaluation that
addresses these limitations through two key components: 1) a domain-aware paper
retrieval module that retrieves relevant concurrent work to support
contextualized assessments of novelty and contributions, and 2) a latent
reasoning mechanism that enables deep understanding of complex motivations and
methodologies, along with comprehensive comparison against concurrently related
work, to support more accurate and reliable evaluation. To guide the reasoning
process, we introduce a progressive ranking optimization strategy that
encourages the LLM to iteratively refine its predictions with an emphasis on
relative comparison. Experiments on two datasets demonstrate that PaperEval
consistently outperforms existing methods in both academic impact and paper
quality evaluation. In addition, we deploy PaperEval in a real-world paper
recommendation system for filtering high-quality papers, which has gained
strong engagement on social media -- amassing over 8,000 subscribers and
attracting over 10,000 views for many filtered high-quality papers --
demonstrating the practical effectiveness of PaperEval.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [55] [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
*Chitranshu Harbola,Anupam Purwar*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型的智能系统，用于预测性维护，结合了轴承振动频率分析和多代理生成，提供了准确的维护建议。


<details>
  <summary>Details</summary>
Motivation: 工业机械维护需要及时干预以防止灾难性故障并优化操作效率。传统的异常检测方法无法提供可操作的维护建议，因此需要一种更智能的解决方案。

Method: 该系统结合了轴承振动频率分析和多代理生成，将轴承振动数据（BPFO、BPFI、BSF、FTF频率）序列化为自然语言以供大型语言模型处理，并利用多代理组件处理维护手册，进行网络搜索以获取全面的程序知识和最新的维护实践。

Result: 实验验证表明，该系统在轴承振动数据集上实现了有效的异常检测和上下文相关的维护指导，成功填补了状态监测与可操作维护计划之间的差距。

Conclusion: 该工作推进了大型语言模型在工业维护中的应用，提供了一个可扩展的预测性维护框架，适用于各种机械部件和工业领域。

Abstract: Industrial machinery maintenance requires timely intervention to prevent
catastrophic failures and optimize operational efficiency. This paper presents
an integrated Large Language Model (LLM)-based intelligent system for
prescriptive maintenance that extends beyond traditional anomaly detection to
provide actionable maintenance recommendations. Building upon our prior LAMP
framework for numerical data analysis, we develop a comprehensive solution that
combines bearing vibration frequency analysis with multi agentic generation for
intelligent maintenance planning. Our approach serializes bearing vibration
data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM
processing, enabling few-shot anomaly detection with high accuracy. The system
classifies fault types (inner race, outer race, ball/roller, cage faults) and
assesses severity levels. A multi-agentic component processes maintenance
manuals using vector embeddings and semantic search, while also conducting web
searches to retrieve comprehensive procedural knowledge and access up-to-date
maintenance practices for more accurate and in-depth recommendations. The
Gemini model then generates structured maintenance recommendations includes
immediate actions, inspection checklists, corrective measures, parts
requirements, and timeline specifications. Experimental validation in bearing
vibration datasets demonstrates effective anomaly detection and contextually
relevant maintenance guidance. The system successfully bridges the gap between
condition monitoring and actionable maintenance planning, providing industrial
practitioners with intelligent decision support. This work advances the
application of LLMs in industrial maintenance, offering a scalable framework
for prescriptive maintenance across machinery components and industrial
sectors.

</details>


### [56] [Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)](https://arxiv.org/abs/2508.04846)
*Mahdi Nazari Ashani,Ali Asghar Alesheikh,Saba Kazemi,Kimya Kheirkhah,Yasin Mohammadi,Fatemeh Rezaie,Amir Mahdi Manafi,Hedieh Zarkesh*

Main category: cs.AI

TL;DR: 本研究比较了三种实现AWebGIS的方法，并发现基于小语言模型的客户端方法在准确性上表现最佳，同时减少了后端服务器的负担。


<details>
  <summary>Details</summary>
Motivation: 当前大多数解决方案依赖于云基础的大语言模型（LLMs），这需要持续的互联网访问，并由于集中式服务器处理而引发用户的隐私和可扩展性问题。

Method: 该研究比较了三种方法：(1) 使用云基础大语言模型（如Cohere）的完全自动化在线方法；(2) 使用经典机器学习分类器（如支持向量机和随机森林）的半自动化离线方法；(3) 基于微调的小语言模型（SLM），即T5-small模型，在客户端执行的完全自主离线方法。

Result: 第三种方法，利用SLMs，在所有方法中达到了最高的准确性，包括精确匹配准确率为0.93，Levenshtein相似度为0.99，以及ROUGE-1和ROUGE-L分数分别为0.98。

Conclusion: 该研究展示了在浏览器中执行的模型对于AWebGIS解决方案的可行性。

Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to
perform geospatial operations from natural language input, providing intuitive,
intelligent, and hands-free interaction. However, most current solutions rely
on cloud-based large language models (LLMs), which require continuous internet
access and raise users' privacy and scalability issues due to centralized
server processing. This study compares three approaches to enabling AWebGIS:
(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)
a semi-automated offline method using classical machine learning classifiers
such as support vector machine and random forest; and (3) a fully autonomous
offline (client-side) method based on a fine-tuned small language model (SLM),
specifically T5-small model, executed in the client's web browser. The third
approach, which leverages SLMs, achieved the highest accuracy among all
methods, with an exact matching accuracy of 0.93, Levenshtein similarity of
0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L
scores of 0.98. Crucially, this client-side computation strategy reduces the
load on backend servers by offloading processing to the user's device,
eliminating the need for server-based inference. These results highlight the
feasibility of browser-executable models for AWebGIS solutions.

</details>


### [57] [ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis](https://arxiv.org/abs/2508.04915)
*Huiya Zhao,Yinghao Zhu,Zixiang Wang,Yasha Wang,Junyi Gao,Liantao Ma*

Main category: cs.AI

TL;DR: 本文介绍了一种名为HealthFlow的自我进化AI代理，能够克服传统AI代理在医疗研究中的局限性，并通过引入新的基准EHRFlowBench验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在医疗研究中的有效性受到其对静态、预定义策略的依赖的限制，这导致它们无法成为更好的战略规划者，而这是复杂领域如医疗保健的关键技能。

Method: HealthFlow通过将过程成功和失败提炼成持久的战略知识库，自主优化其高层次问题解决策略。同时引入了EHRFlowBench基准，用于评估和验证HealthFlow的有效性。

Result: 实验结果表明，HealthFlow的自我进化方法在复杂、现实的健康数据分析任务中显著优于现有的最先进的代理框架。

Conclusion: 本文提出了一种名为HealthFlow的自我进化AI代理，通过新颖的元级进化机制克服了传统AI代理在医疗研究中的局限性。实验结果表明，HealthFlow的自我进化方法显著优于最先进的代理框架，为更自主和有效的科学发现AI铺平了道路。

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [58] [Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses](https://arxiv.org/abs/2508.05009)
*Bin Han,Robert Wolfe,Anat Caspi,Bill Howe*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在增强领域专家整合大型、异构和噪声城市空间数据集中的应用。研究发现，LLM在提供相关特征时能够生成高性能的结果，并且审查和修改方法在纠正错误初始响应方面非常有效。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的集成方法无法涵盖所有边缘情况，需要人工验证和修复。机器学习方法需要收集和标记大量任务特定样本。

Method: 我们分析了LLM如何通过人类经验来推理环境空间关系，并适应了审查和修改方法以纠正错误的初始响应。

Result: LLM在提供相关特征时能够生成高性能的结果，并且审查和修改方法在纠正错误初始响应方面非常有效。

Conclusion: 我们的研究结果表明，LLM可以作为传统基于规则的启发式方法的有前途且灵活的替代方案，推动自适应空间数据集成的能力。

Abstract: We explore the application of large language models (LLMs) to empower domain
experts in integrating large, heterogeneous, and noisy urban spatial datasets.
Traditional rule-based integration methods are unable to cover all edge cases,
requiring manual verification and repair. Machine learning approaches require
collecting and labeling of large numbers of task-specific samples. In this
study, we investigate the potential of LLMs for spatial data integration. Our
analysis first considers how LLMs reason about environmental spatial
relationships mediated by human experience, such as between roads and
sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they
struggle to connect the macro-scale environment with the relevant computational
geometry tasks, often producing logically incoherent responses. But when
provided relevant features, thereby reducing dependence on spatial reasoning,
LLMs are able to generate high-performing results. We then adapt a
review-and-refine method, which proves remarkably effective in correcting
erroneous initial responses while preserving accurate responses. We discuss
practical implications of employing LLMs for spatial data integration in
real-world contexts and outline future research directions, including
post-training, multi-modal integration methods, and support for diverse data
formats. Our findings position LLMs as a promising and flexible alternative to
traditional rule-based heuristics, advancing the capabilities of adaptive
spatial data integration.

</details>


### [59] [Cognitive Duality for Adaptive Web Agents](https://arxiv.org/abs/2508.05081)
*Jiarun Liu,Chunhong Zhang,Zheng Hu*

Main category: cs.AI

TL;DR: 本文提出了一种基于双进程理论的网络代理框架CogniWeb，能够根据任务复杂性在快速直觉处理和刻意推理之间切换，实现了较高的性能和效率


<details>
  <summary>Details</summary>
Motivation: 当前构建自主网络代理的方法要么专注于离线模仿学习，要么专注于在线探索，但很少有效结合这两种范式。需要一种统一的视角来弥合离线学习直观反应行为和在线获取详细规划能力之间的差距

Method: 基于双进程理论的人类认知分解，将快速系统1和慢速系统2认知过程进行分解，实现了一个模块化的代理架构CogniWeb，根据任务复杂性在快速直觉处理和刻意推理之间自适应切换

Result: CogniWeb在WebArena上实现了43.96%的成功率，同时将令牌使用量减少了75%

Conclusion: CogniWeb在WebArena上的评估表明，它在保持显著更高的效率（令牌使用量减少75%）的同时，实现了具有竞争力的性能（成功率为43.96%）

Abstract: Web navigation represents a critical and challenging domain for evaluating
artificial general intelligence (AGI), demanding complex decision-making within
high-entropy, dynamic environments with combinatorially explosive action
spaces. Current approaches to building autonomous web agents either focus on
offline imitation learning or online exploration, but rarely integrate both
paradigms effectively. Inspired by the dual-process theory of human cognition,
we derive a principled decomposition into fast System 1 and slow System 2
cognitive processes. This decomposition provides a unifying perspective on
existing web agent methodologies, bridging the gap between offline learning of
intuitive reactive behaviors and online acquisition of deliberative planning
capabilities. We implement this framework in CogniWeb, a modular agent
architecture that adaptively toggles between fast intuitive processing and
deliberate reasoning based on task complexity. Our evaluation on WebArena
demonstrates that CogniWeb achieves competitive performance (43.96% success
rate) while maintaining significantly higher efficiency (75% reduction in token
usage).

</details>


### [60] [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
*Zhuohang Jiang,Pangjing Wu,Xu Yuan,Wenqi Fan,Qing Li*

Main category: cs.AI

TL;DR: 本文提出了QA-Dragon，一个查询感知的动态RAG系统，用于知识密集型VQA。该系统通过引入领域路由器和搜索路由器，支持多模态、多轮和多跳推理，有效解决了复杂VQA任务。在KDD Cup 2025的Meta CRAG-MM挑战赛中，QA-Dragon在多个任务中均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法通常单独从文本或图像中检索，限制了它们解决需要多跳推理或最新事实知识的复杂查询的能力。为了克服这一限制，我们提出了QA-Dragon，一个查询感知的动态RAG系统，用于知识密集型VQA。

Method: 我们提出了QA-Dragon，这是一个查询感知的动态RAG系统，用于知识密集型VQA。具体来说，QA-Dragon引入了一个领域路由器来识别查询的主题领域以进行领域特定推理，并引入了一个搜索路由器来动态选择最佳检索策略。通过在混合设置中协调文本和图像搜索代理，我们的系统支持多模态、多轮和多跳推理，使其能够有效地处理复杂的VQA任务。

Result: 我们在KDD Cup 2025的Meta CRAG-MM挑战赛上评估了我们的QA-Dragon，结果表明它在具有挑战性的场景下显著提升了基础模型的推理性能。

Conclusion: 我们的框架在答案准确性和知识重叠分数方面都取得了显著的提升，在单源任务、多源任务和多轮任务中分别优于基线5.06%、6.35%和5.03%。

Abstract: Retrieval-Augmented Generation (RAG) has been introduced to mitigate
hallucinations in Multimodal Large Language Models (MLLMs) by incorporating
external knowledge into the generation process, and it has become a widely
adopted approach for knowledge-intensive Visual Question Answering (VQA).
However, existing RAG methods typically retrieve from either text or images in
isolation, limiting their ability to address complex queries that require
multi-hop reasoning or up-to-date factual knowledge. To address this
limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to
identify the query's subject domain for domain-specific reasoning, along with a
search router that dynamically selects optimal retrieval strategies. By
orchestrating both text and image search agents in a hybrid setup, our system
supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle
complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM
Challenge at KDD Cup 2025, where it significantly enhances the reasoning
performance of base models under challenging scenarios. Our framework achieves
substantial improvements in both answer accuracy and knowledge overlap scores,
outperforming baselines by 5.06% on the single-source task, 6.35% on the
multi-source task, and 5.03% on the multi-turn task.

</details>


### [61] [A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents](https://arxiv.org/abs/2508.05311)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 本文提出了一种将决策树和大型语言模型结合的混合架构，以提高推理任务的性能，并在多个基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法将符号和神经模块松散地耦合，而本文设计将决策树和随机森林作为可调用的预言机嵌入统一的推理系统中。

Method: 提出了一种混合架构，将基于决策树的符号推理与大型语言模型（LLMs）的生成能力整合到一个协调的多智能体框架中。

Result: 该系统在推理基准测试中表现出色。在ProofWriter上，通过逻辑基础的树验证提高了7.2%的蕴含一致性；在GSM8k上，通过符号增强提高了5.3%的多步骤数学问题准确率；在ARC上，通过集成符号预言机提高了6.0%的抽象准确率。

Conclusion: 该架构提供了一种强大、可解释且可扩展的通用神经符号推理解决方案。

Abstract: We propose a hybrid architecture that integrates decision tree-based symbolic
reasoning with the generative capabilities of large language models (LLMs)
within a coordinated multi-agent framework. Unlike prior approaches that
loosely couple symbolic and neural modules, our design embeds decision trees
and random forests as callable oracles within a unified reasoning system.
Tree-based modules enable interpretable rule inference and causal logic, while
LLM agents handle abductive reasoning, generalization, and interactive
planning. A central orchestrator maintains belief state consistency and
mediates communication across agents and external tools, enabling reasoning
over both structured and unstructured inputs.
  The system achieves strong performance on reasoning benchmarks. On
\textit{ProofWriter}, it improves entailment consistency by +7.2\% through
logic-grounded tree validation. On GSM8k, it achieves +5.3\% accuracy gains in
multistep mathematical problems via symbolic augmentation. On \textit{ARC}, it
boosts abstraction accuracy by +6.0\% through integration of symbolic oracles.
Applications in clinical decision support and scientific discovery show how the
system encodes domain rules symbolically while leveraging LLMs for contextual
inference and hypothesis generation. This architecture offers a robust,
interpretable, and extensible solution for general-purpose neuro-symbolic
reasoning.

</details>


### [62] [Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?](https://arxiv.org/abs/2508.05464)
*Matteo Prandi,Vincenzo Suriani,Federico Pierucci,Marcello Galisai,Daniele Nardi,Piercosma Bisconti*

Main category: cs.AI

TL;DR: 本研究揭示了当前AI评估框架与欧盟AI法案之间存在显著差距，特别是在系统性风险评估方面。


<details>
  <summary>Details</summary>
Motivation: 当前的AI评估实践依赖于已建立的基准测试，但这些工具并未设计用于衡量新监管环境关注的系统性风险，因此需要量化这种“基准-法规差距”。

Method: 引入了Bench-2-CoP框架，利用经过验证的LLM-as-judge分析，将广泛使用的基准测试中的194,955个问题映射到欧盟AI法案的模型能力与倾向分类法。

Result: 评估生态系统主要集中在少数行为倾向上，如“幻觉倾向”（占全部内容的53.7%）和“歧视性偏见”（28.9%），而关键的功能能力被严重忽视。对于失控场景相关的能力，如规避人类监督、自我复制和自主AI开发，在整个基准测试语料库中完全没有覆盖。

Conclusion: 本研究提供了对这一差距的首次全面、定量分析，为政策制定者完善CoP和开发下一代评估工具提供了关键见解，最终促进了更安全和合规的AI发展。

Abstract: The rapid advancement of General Purpose AI (GPAI) models necessitates robust
evaluation frameworks, especially with emerging regulations like the EU AI Act
and its associated Code of Practice (CoP). Current AI evaluation practices
depend heavily on established benchmarks, but these tools were not designed to
measure the systemic risks that are the focus of the new regulatory landscape.
This research addresses the urgent need to quantify this "benchmark-regulation
gap." We introduce Bench-2-CoP, a novel, systematic framework that uses
validated LLM-as-judge analysis to map the coverage of 194,955 questions from
widely-used benchmarks against the EU AI Act's taxonomy of model capabilities
and propensities. Our findings reveal a profound misalignment: the evaluation
ecosystem is overwhelmingly focused on a narrow set of behavioral propensities,
such as "Tendency to hallucinate" (53.7% of the corpus) and "Discriminatory
bias" (28.9%), while critical functional capabilities are dangerously
neglected. Crucially, capabilities central to loss-of-control scenarios,
including evading human oversight, self-replication, and autonomous AI
development, receive zero coverage in the entire benchmark corpus. This
translates to a near-total evaluation gap for systemic risks like "Loss of
Control" (0.4% coverage) and "Cyber Offence" (0.8% coverage). This study
provides the first comprehensive, quantitative analysis of this gap, offering
critical insights for policymakers to refine the CoP and for developers to
build the next generation of evaluation tools, ultimately fostering safer and
more compliant AI.

</details>


### [63] [Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?](https://arxiv.org/abs/2508.05474)
*Burak Can Kaplan,Hugo Cesar De Castro Carneiro,Stefan Wermter*

Main category: cs.AI

TL;DR: 本文提出了一种使用小型、资源高效的LLM来生成多样化的ERC数据集的方法，以补充现有的ERC基准。实验结果表明，使用这些生成的数据集训练的模型在性能上有所提升。


<details>
  <summary>Details</summary>
Motivation: ERC数据仍然稀缺，现有的数据集由于其高度偏倚的来源和软标签的固有主观性而面临诸多挑战。尽管大型语言模型（LLMs）在许多情感任务中表现出色，但它们通常训练成本高昂，且在ERC任务中的应用——特别是在数据生成方面——仍然有限。

Method: 我们采用了一个小型、资源高效且通用的LLM来合成具有多样特性的ERC数据集，补充了三个最常用的ERC基准。我们生成了六个新的数据集，其中两个专门用于增强每个基准。

Result: 实验结果表明，ERC分类器模型在生成的数据集上训练时表现出强大的鲁棒性，并在现有ERC基准上 consistently 实现统计显著的性能提升。

Conclusion: 实验结果表明，使用生成的数据集训练的ERC分类器模型表现出强大的鲁棒性，并在现有ERC基准上 consistently 实现统计显著的性能提升。

Abstract: Emotion recognition in conversations (ERC) focuses on identifying emotion
shifts within interactions, representing a significant step toward advancing
machine intelligence. However, ERC data remains scarce, and existing datasets
face numerous challenges due to their highly biased sources and the inherent
subjectivity of soft labels. Even though Large Language Models (LLMs) have
demonstrated their quality in many affective tasks, they are typically
expensive to train, and their application to ERC tasks--particularly in data
generation--remains limited. To address these challenges, we employ a small,
resource-efficient, and general-purpose LLM to synthesize ERC datasets with
diverse properties, supplementing the three most widely used ERC benchmarks. We
generate six novel datasets, with two tailored to enhance each benchmark. We
evaluate the utility of these datasets to (1) supplement existing datasets for
ERC classification, and (2) analyze the effects of label imbalance in ERC. Our
experimental results indicate that ERC classifier models trained on the
generated datasets exhibit strong robustness and consistently achieve
statistically significant performance improvements on existing ERC benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [Advancing Hate Speech Detection with Transformers: Insights from the MetaHate](https://arxiv.org/abs/2508.04913)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本研究探索了基于Transformer的模型在仇恨言论检测中的应用，发现微调后的ELECTRA表现最佳，但也面临讽刺、编码语言和标签噪声等挑战。


<details>
  <summary>Details</summary>
Motivation: 由于仇恨言论在社交媒体上的广泛存在及其对社会、心理和身体的影响，需要开发高效的自动化检测方法。尽管深度学习方法已取得良好效果，但仍有改进空间，尤其是在处理长期依赖和并行化问题方面。

Method: 本研究使用MetaHate数据集（包含120万条社交媒体样本）对多种最先进的Transformer模型（如BERT、RoBERTa、GPT-2和ELECTRA）进行了评估，并对ELECTRA进行了微调以提高性能。

Result: 微调后的ELECTRA在仇恨言论检测任务中取得了最高的F1分数（0.8980），表明其在该领域的优越性能。此外，研究还分析了分类错误，揭示了讽刺、编码语言和标签噪声等问题。

Conclusion: 本研究展示了基于Transformer的模型在仇恨言论检测中的有效性，特别是微调后的ELECTRA在MetaHate数据集上表现最佳。然而，研究也揭示了讽刺、编码语言和标签噪声等挑战。

Abstract: Hate speech is a widespread and harmful form of online discourse,
encompassing slurs and defamatory posts that can have serious social,
psychological, and sometimes physical impacts on targeted individuals and
communities. As social media platforms such as X (formerly Twitter), Facebook,
Instagram, Reddit, and others continue to facilitate widespread communication,
they also become breeding grounds for hate speech, which has increasingly been
linked to real-world hate crimes. Addressing this issue requires the
development of robust automated methods to detect hate speech in diverse social
media environments. Deep learning approaches, such as vanilla recurrent neural
networks (RNNs), long short-term memory (LSTM), and convolutional neural
networks (CNNs), have achieved good results, but are often limited by issues
such as long-term dependencies and inefficient parallelization. This study
represents the comprehensive exploration of transformer-based models for hate
speech detection using the MetaHate dataset--a meta-collection of 36 datasets
with 1.2 million social media samples. We evaluate multiple state-of-the-art
transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with
fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We
also analyze classification errors, revealing challenges with sarcasm, coded
language, and label noise.

</details>


### [65] [REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2508.04946)
*Nameer Hirschkind,Joseph Liu,Mahesh Kumar Nandwana,Xiao Yu*

Main category: cs.LG

TL;DR: This paper introduces REINA, a novel loss function that optimizes the tradeoff between translation quality and latency in SimulST systems, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Simultaneous Speech Translation (SimulST) systems face the challenge of balancing translation quality and latency. The goal is to optimize this tradeoff by waiting for more input only if gaining information by doing so.

Method: We introduce Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. REINA is derived from information theory principles.

Result: REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores. We achieve state-of-the-art streaming results for models of comparable size.

Conclusion: REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we achieve state-of-the-art streaming results for models of comparable size.

Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while
simultaneously emitting translated text or speech. Such systems face the
significant challenge of balancing translation quality and latency. We
introduce a strategy to optimize this tradeoff: wait for more input only if you
gain information by doing so. Based on this strategy, we present Regularized
Entropy INformation Adaptation (REINA), a novel loss to train an adaptive
policy using an existing non-streaming translation model. We derive REINA from
information theory principles and show that REINA helps push the reported
Pareto frontier of the latency/quality tradeoff over prior works. Utilizing
REINA, we train a SimulST model on French, Spanish and German, both from and
into English. Training on only open source or synthetically generated data, we
achieve state-of-the-art (SOTA) streaming results for models of comparable
size. We also introduce a metric for streaming efficiency, quantitatively
showing REINA improves the latency/quality trade-off by as much as 21% compared
to prior approaches, normalized against non-streaming baseline BLEU scores.

</details>


### [66] [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004)
*Chengsong Huang,Wenhao Yu,Xiaoyang Wang,Hongming Zhang,Zongxia Li,Ruosen Li,Jiaxin Huang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: R-Zero是一种完全自主的框架，通过让两个模型相互作用来生成训练数据，从而提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的训练方法仍然严重依赖于大量人工标注的任务和标签，这限制了AI系统超越人类智能的发展。因此，需要一种能够自主生成训练数据的方法。

Method: R-Zero是一个完全自主的框架，从一个基础的大语言模型开始，初始化两个具有不同角色的独立模型，即Challenger和Solver。Challenger被奖励提出接近Solver能力边缘的任务，而Solver被奖励解决由Challenger提出的越来越具有挑战性的任务。

Result: R-Zero在不同的基础大语言模型上显著提高了推理能力，例如将Qwen3-4B-Base在数学推理基准测试中提升了6.49，在通用领域推理基准测试中提升了7.54。

Conclusion: R-Zero显著提升了不同基础大语言模型的推理能力，展示了其在自主生成训练数据方面的有效性。

Abstract: Self-evolving Large Language Models (LLMs) offer a scalable path toward
super-intelligence by autonomously generating, refining, and learning from
their own experiences. However, existing methods for training such models still
rely heavily on vast human-curated tasks and labels, typically via fine-tuning
or reinforcement learning, which poses a fundamental bottleneck to advancing AI
systems toward capabilities beyond human intelligence. To overcome this
limitation, we introduce R-Zero, a fully autonomous framework that generates
its own training data from scratch. Starting from a single base LLM, R-Zero
initializes two independent models with distinct roles, a Challenger and a
Solver. These models are optimized separately and co-evolve through
interaction: the Challenger is rewarded for proposing tasks near the edge of
the Solver capability, and the Solver is rewarded for solving increasingly
challenging tasks posed by the Challenger. This process yields a targeted,
self-improving curriculum without any pre-existing tasks and labels.
Empirically, R-Zero substantially improves reasoning capability across
different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on
math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.

</details>


### [67] [Exploring Superior Function Calls via Reinforcement Learning](https://arxiv.org/abs/2508.05118)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Yicheng Chen,Cunyin Peng,Jinjie GU,Chenyi Zhuang*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架，旨在通过战略熵基探索来增强群体相对策略优化，专门针对函数调用任务。该方法解决了三个关键挑战，并在伯克利函数调用排行榜上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 函数调用功能对于将大型语言模型部署到现实世界的应用至关重要，但当前的训练方法未能开发出稳健的推理策略。监督微调产生依赖于表面模式匹配的模型，而标准的强化学习方法在处理结构化函数调用的复杂动作空间时遇到困难。

Method: 我们提出了一种新的强化学习框架，通过战略熵基探索来增强群体相对策略优化，专门针对函数调用任务。我们的方法解决了三个关键挑战：政策学习期间探索不足、链式思维生成中缺乏结构化推理以及参数提取验证不足。我们的两阶段数据准备管道通过迭代LLM评估和抽象语法树验证确保高质量的训练样本。

Result: 在伯克利函数调用排行榜上的广泛实验表明，该框架在开源模型中实现了最先进的性能，总体准确率达到86.02%，在复杂的多函数场景中比标准GRPO高出高达6%。值得注意的是，我们的方法在代码预训练模型上显示出特别大的改进。

Conclusion: 我们的方法在代码预训练模型上表现出显著的改进，表明结构化的语言生成能力为函数调用任务中的强化学习提供了有利的起点。我们将会发布所有的代码、模型和数据集以造福社区。

Abstract: Function calling capabilities are crucial for deploying Large Language Models
in real-world applications, yet current training approaches fail to develop
robust reasoning strategies. Supervised fine-tuning produces models that rely
on superficial pattern matching, while standard reinforcement learning methods
struggle with the complex action space of structured function calls. We present
a novel reinforcement learning framework designed to enhance group relative
policy optimization through strategic entropy based exploration specifically
tailored for function calling tasks. Our approach addresses three critical
challenges in function calling: insufficient exploration during policy
learning, lack of structured reasoning in chain-of-thought generation, and
inadequate verification of parameter extraction. Our two-stage data preparation
pipeline ensures high-quality training samples through iterative LLM evaluation
and abstract syntax tree validation. Extensive experiments on the Berkeley
Function Calling Leaderboard demonstrate that this framework achieves
state-of-the-art performance among open-source models with 86.02\% overall
accuracy, outperforming standard GRPO by up to 6\% on complex multi-function
scenarios. Notably, our method shows particularly strong improvements on
code-pretrained models, suggesting that structured language generation
capabilities provide an advantageous starting point for reinforcement learning
in function calling tasks. We will release all the code, models and dataset to
benefit the community.

</details>


### [68] [Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models](https://arxiv.org/abs/2508.05165)
*Mason Nakamura,Saaduddin Mahmud,Kyle H. Wray,Hamed Zamani,Shlomo Zilberstein*

Main category: cs.LG

TL;DR: HIA is a tuning-free approach that balances alignment quality and computational cost for LLMs, outperforming existing methods in multi-objective tasks with minimal inference calls.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with user preferences is crucial for real-world use but often requires costly fine-tuning or expensive inference, forcing trade-offs between alignment quality and computational cost.

Method: HIA (Heuristic-Guided Inference-time Alignment) is a tuning-free, black-box-compatible approach that uses a lightweight prompt optimizer, heuristic reward models, and two-stage filtering.

Result: On real-world prompt datasets, HIA outperforms best-of-N sampling, beam search, and greedy search baselines in multi-objective, goal-conditioned tasks under the same inference budget. It is also effective under low-inference budgets with as little as one or two response queries.

Conclusion: HIA provides a practical solution for scalable, personalized LLM deployment by effectively balancing alignment quality and computational cost.

Abstract: Aligning LLMs with user preferences is crucial for real-world use but often
requires costly fine-tuning or expensive inference, forcing trade-offs between
alignment quality and computational cost. Existing inference-time methods
typically ignore this balance, focusing solely on the optimized policy's
performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a
tuning-free, black-box-compatible approach that uses a lightweight prompt
optimizer, heuristic reward models, and two-stage filtering to reduce inference
calls while preserving alignment quality. On real-world prompt datasets,
HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and
greedy search baselines in multi-objective, goal-conditioned tasks under the
same inference budget. We also find that HIA is effective under low-inference
budgets with as little as one or two response queries, offering a practical
solution for scalable, personalized LLM deployment.

</details>


### [69] [FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance](https://arxiv.org/abs/2508.05201)
*Mengao Zhang,Jiayu Fu,Tanya Warrier,Yuwen Wang,Tianhui Tan,Ke-wei Huang*

Main category: cs.LG

TL;DR: 本文提出了一种评估金融LLMs中内在幻觉的框架，包括一个自动化数据集创建方法、一个新的幻觉评估数据集，并对最先进的LLMs进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 准确提取和精确计算表格数据对于可靠的财务分析至关重要，因为即使是最小的数值错误也可能破坏决策和监管合规性。金融应用有独特的需求，通常依赖于上下文相关的、数值的和专有的表格数据，而现有的幻觉基准很少能捕捉到这些数据。

Method: 我们开发了一个严格且可扩展的框架，用于评估金融LLMs中的内在幻觉，将其概念化为在现实世界金融文档上的上下文感知掩码跨度预测任务。

Result: 我们提出了一个新颖的自动化数据集创建范式，使用掩码策略；一个新的基于标普500年度报告的幻觉评估数据集；以及对最先进的LLMs在金融表格数据上的内在幻觉模式的全面评估。

Conclusion: 我们的工作提供了一种稳健的方法论用于内部LLM评估，并朝着构建更值得信赖和可靠的金融生成式AI系统迈出了关键一步。

Abstract: Hallucination remains a critical challenge for deploying Large Language
Models (LLMs) in finance. Accurate extraction and precise calculation from
tabular data are essential for reliable financial analysis, since even minor
numerical errors can undermine decision-making and regulatory compliance.
Financial applications have unique requirements, often relying on
context-dependent, numerical, and proprietary tabular data that existing
hallucination benchmarks rarely capture. In this study, we develop a rigorous
and scalable framework for evaluating intrinsic hallucinations in financial
LLMs, conceptualized as a context-aware masked span prediction task over
real-world financial documents. Our main contributions are: (1) a novel,
automated dataset creation paradigm using a masking strategy; (2) a new
hallucination evaluation dataset derived from S&P 500 annual reports; and (3) a
comprehensive evaluation of intrinsic hallucination patterns in
state-of-the-art LLMs on financial tabular data. Our work provides a robust
methodology for in-house LLM evaluation and serves as a critical step toward
building more trustworthy and reliable financial Generative AI systems.

</details>


### [70] [Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$](https://arxiv.org/abs/2508.05571)
*Feiyu Wang,Guoan Wang,Yihao Zhang,Shengfan Wang,Weitao Li,Bokai Huang,Shimao Chen,Zihan Jiang,Rui Xu,Tong Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的2比特量化框架Fairy±i，通过提高全精度模型的准确率天花板并实现高效的2比特量化，从而突破现有方法的限制。该方法利用复数域的优势，实现了乘法-free推理，并在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的QAT研究主要集中在最小化全精度模型上的量化误差，其中全精度准确率作为上限（准确率天花板）。没有现有方法尝试超越这个天花板。为了打破这个天花板，本文提出了一种新的范式：提高全精度模型的上限，然后仍然将其高效地量化为2比特。

Method: 本文提出了Fairy±i，这是第一个针对复数大语言模型的2比特量化框架。该方法利用复数域的表示优势来提升全精度模型的准确性，并将权重映射到四次单位根{±1, ±i}，形成一个完美对称且信息理论最优的2比特表示。此外，每个量化权重要么具有零实部，要么具有零虚部，从而实现了仅使用加法和元素交换的乘法-free推理。

Result: 实验结果表明，Fairy±i在PPL和下游任务方面都优于现有2比特量化方法的天花板，同时保持严格的存储和计算效率。

Conclusion: 本文提出了一种新的范式，通过提高全精度模型的上限（准确率天花板），然后将其高效地量化为2比特，从而突破现有2比特量化方法的限制。该工作为在极端低比特约束下构建高精度和实用的大语言模型开辟了新方向。

Abstract: Quantization-Aware Training (QAT) integrates quantization into the training
loop, enabling LLMs to learn robust low-bit representations, and is widely
recognized as one of the most promising research directions. All current QAT
research focuses on minimizing quantization error on full-precision models,
where the full-precision accuracy acts as an upper bound (accuracy ceiling). No
existing method has even attempted to surpass this ceiling. To break this
ceiling, we propose a new paradigm: raising the ceiling (full-precision model),
and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$,
the first 2-bit quantization framework for complex-valued LLMs. Specifically,
our method leverages the representational advantages of the complex domain to
boost full-precision accuracy. We map weights to the fourth roots of unity
$\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically
optimal 2-bit representation. Importantly, each quantized weight has either a
zero real or imaginary part, enabling multiplication-free inference using only
additions and element swaps. Experimental results show that Fairy$\pm i$
outperforms the ceiling of existing 2-bit quantization approaches in terms of
both PPL and downstream tasks, while maintaining strict storage and compute
efficiency. This work opens a new direction for building highly accurate and
practical LLMs under extremely low-bit constraints.

</details>


### [71] [Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models](https://arxiv.org/abs/2508.05581)
*Guilherme Seidyo Imai Aldeia,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: This paper explores the potential of large language models (LLMs) in generating interpretable computable phenotypes (CPs) for clinical decision support, particularly for hypertension. The study proposes a strategy involving iterative refinement of CPs using data-driven feedback and demonstrates that LLMs can achieve performance comparable to state-of-the-art ML methods with fewer training examples.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can generate accurate and concise CPs for six clinical phenotypes of varying complexity, which could be leveraged to enable scalable clinical decision support to improve care for patients with hypertension.

Method: We propose and test a synthesize, execute, debug, instruct strategy that uses LLMs to generate and iteratively refine CPs using data-driven feedback.

Result: LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples.

Conclusion: LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities for
medical question answering and programming, but their potential for generating
interpretable computable phenotypes (CPs) is under-explored. In this work, we
investigate whether LLMs can generate accurate and concise CPs for six clinical
phenotypes of varying complexity, which could be leveraged to enable scalable
clinical decision support to improve care for patients with hypertension. In
addition to evaluating zero-short performance, we propose and test a
synthesize, execute, debug, instruct strategy that uses LLMs to generate and
iteratively refine CPs using data-driven feedback. Our results show that LLMs,
coupled with iterative learning, can generate interpretable and reasonably
accurate programs that approach the performance of state-of-the-art ML methods
while requiring significantly fewer training examples.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

TL;DR: 本文提出了一种无需任务特定微调的方法，通过结合冻结的音频基础模型和语言模型，来丰富对话转录中的说话人特征元数据，并实现了竞争性的性能。


<details>
  <summary>Details</summary>
Motivation: 在对话转录管道中，大型语言模型（LLMs）经常用于后处理以提高语法、标点和可读性。我们探索了一个互补的后处理步骤：通过添加元数据标签来丰富转录对话，这些标签包括年龄、性别和情绪等说话人特征。

Method: 我们的方法结合了冻结的音频基础模型（如Whisper或WavLM）和冻结的LLAMA语言模型，以推断这些说话人属性，而无需对任一模型进行任务特定的微调。使用轻量级、高效的连接器来桥接音频和语言表示，我们实现了竞争性的性能。

Result: 我们实现了竞争性的性能，同时保持了模块化和速度。此外，我们证明了冻结的LLAMA模型可以直接比较x-vectors，实现了一些场景下的8.8%等错误率。

Conclusion: 我们的方法在不进行任务特定微调的情况下，实现了竞争性的性能，并保持了模块化和速度。此外，我们证明了冻结的LLAMA模型可以直接比较x-vectors，实现了一些场景下的8.8%等错误率。

Abstract: In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [2] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文介绍了Parity-aware Byte Pair Encoding (BPE)算法，该算法旨在解决标准tokenization方法对低资源语言的不公问题，通过在每个合并步骤中最大化当前压缩最差的语言的压缩增益，实现跨语言的平等。实验结果表明，该方法在保持全局压缩率和语言模型性能的同时，显著提高了不同语言之间的token数量的公平性。


<details>
  <summary>Details</summary>
Motivation: Standard algorithms for learning tokenizers rely on frequency-based objectives, which favor languages dominant in the training data and consequently leave lower-resource languages with tokenizations that are disproportionately longer, morphologically implausible, or even riddled with <UNK> placeholders. This phenomenon ultimately amplifies computational and financial inequalities between users from different language backgrounds.

Method: Parity-aware Byte Pair Encoding (BPE), a variant of the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes the compression gain of the currently worst-compressed language, trading a small amount of global compression for cross-lingual parity.

Result: Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.

Conclusion: Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.

Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [3] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

TL;DR: 本文提出了一种联合ASR和重音检测模型，通过引入重音检测模块，显著提升了ASR系统的性能。在有限资源微调下，该模型在LibriSpeech上将WER降低了28.3%。


<details>
  <summary>Details</summary>
Motivation: 为了提升使用半监督语音表示的自动语音识别（ASR）系统的性能，需要考虑扩展预训练语音模型以保留或重新学习重要的韵律线索，如重音。

Method: 引入了一个联合ASR和重音检测模型，以提升ASR系统的性能。该模型的重音检测组件在任务上取得了显著改进，并在有限资源微调下提升了ASR性能。

Result: 重音检测组件在任务上取得了显著改进，将F1分数的差距缩小了41%。在有限资源微调下，联合训练中的ASR性能在LibriSpeech上将WER降低了28.3%。

Conclusion: 我们展示了通过引入联合ASR和重音检测模型，可以提升使用半监督语音表示的自动语音识别（ASR）系统的性能。该模型的重音检测组件在任务上取得了显著改进，将F1分数的差距缩小了41%。此外，在有限资源微调下，联合训练中的ASR性能在LibriSpeech上将WER降低了28.3%。这些结果表明，扩展预训练语音模型以保留或重新学习重要的韵律线索（如重音）的重要性。

Abstract: We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [4] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在行为一致性方面存在持续的不稳定性，即使在大规模模型和缓解策略下也是如此，这表明当前模型缺乏实现真正行为一致性的基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要一致的行为模式以实现安全部署，但它们的人格特征仍然难以理解。

Method: 使用传统（BFI-44, SD3）和新型适配LLM的人格工具，系统地改变问题顺序、改写、角色扮演和推理模式，测试了25个以上的开源模型（1B-671B参数）在500,000多个响应中的表现。

Result: （1）即使400B+模型也表现出显著的响应变化（SD > 0.4）；（2）仅轻微重新排序提示就将人格测量值改变了高达20%；（3）预期稳定行为的干预措施，如思维链推理、详细的角色扮演说明、包含对话历史，反而可能增加变化；（4）适配LLM的工具与人类中心版本一样不稳定，确认了架构而非转换限制。

Conclusion: 当前大型语言模型缺乏真正的行为一致性，这表明基于个性的对齐策略可能在安全关键应用中根本不足。

Abstract: Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [5] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文介绍了RCR-Router，这是一种用于多智能体大型语言模型的模块化和角色感知上下文路由框架，旨在实现高效的自适应协作。实验表明，RCR-Router在减少令牌使用的同时提高了或保持了答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有的协调方案依赖于静态或全上下文路由策略，导致过多的令牌消耗、冗余内存暴露以及跨交互回合的适应性有限。

Method: RCR-Router是一种模块化且角色感知的上下文路由框架，它通过动态选择每个代理基于其角色和任务阶段的语义相关记忆子集来实现高效的自适应协作。

Result: RCR-Router在三个多跳QA基准测试中减少了令牌使用（最多30%），同时提高了或保持了答案质量。

Conclusion: 这些结果突显了结构化记忆路由和输出感知评估在推动可扩展多智能体LLM系统发展中的重要性。

Abstract: Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


### [6] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
*Julia Kharchenko,Tanya Roosta,Aman Chadha,Chirag Shah*

Main category: cs.CL

TL;DR: 本文介绍了一个全面的基准，用于评估大型语言模型（LLMs）对语言符号的反应，这些符号可能无意中揭示人口统计属性。通过精心构建的访谈模拟，我们展示了LLMs如何系统地惩罚某些语言模式，尤其是模糊语言。我们的基准生成了受控的语言变化，以隔离特定现象同时保持语义等价性，从而能够精确测量人口统计偏差。我们验证了我们的方法在多个语言维度上的有效性，显示模糊的回答平均评分低25.6%。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估大型语言模型（LLMs）如何对语言符号做出反应，这些符号可能无意中揭示性别、社会阶层或地区背景等人口统计属性。

Method: 本文通过精心构建的访谈模拟，使用100个经过验证的问题-回答对，展示了大型语言模型如何系统地惩罚某些语言模式，特别是模糊语言。我们的基准生成了受控的语言变化，以隔离特定现象同时保持语义等价性，从而能够精确测量人口统计偏差。

Result: 我们的基准生成了受控的语言变化，以隔离特定现象同时保持语义等价性，从而能够精确测量人口统计偏差。我们验证了我们的方法在多个语言维度上的有效性，显示模糊的回答平均评分低25.6%。

Conclusion: 本文建立了一个基础框架，用于检测和测量人工智能系统中的语言歧视，并在自动化决策情境中的公平性方面有广泛的应用。

Abstract: This paper introduces a comprehensive benchmark for evaluating how Large
Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic
markers that can inadvertently reveal demographic attributes such as gender,
social class, or regional background. Through carefully constructed interview
simulations using 100 validated question-response pairs, we demonstrate how
LLMs systematically penalize certain linguistic patterns, particularly hedging
language, despite equivalent content quality. Our benchmark generates
controlled linguistic variations that isolate specific phenomena while
maintaining semantic equivalence, which enables the precise measurement of
demographic bias in automated evaluation systems. We validate our approach
along multiple linguistic dimensions, showing that hedged responses receive
25.6% lower ratings on average, and demonstrate the benchmark's effectiveness
in identifying model-specific biases. This work establishes a foundational
framework for detecting and measuring linguistic discrimination in AI systems,
with broad applications to fairness in automated decision-making contexts.

</details>


### [7] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
*Louie Hong Yao,Nicholas Jarvis,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种基于聚类的视觉活动识别评估方法，以更好地捕捉动词语义和图像解释中的歧义，并与人类判断对齐。


<details>
  <summary>Details</summary>
Motivation: 标准的精确匹配评估无法捕捉动词语义和图像解释中的固有歧义，导致对模型性能的评估不完整。

Method: 我们提出了一个视觉-语言聚类框架，构建动词意义聚类，以提供更稳健的评估。

Result: 分析显示，每张图像平均映射到2.8个意义聚类，每个聚类代表图像的不同视角。此外，人类对齐分析表明，基于聚类的评估更好地与人类判断对齐。

Conclusion: 我们的聚类评估方法更好地与人类判断对齐，提供了更细致的模型性能评估。

Abstract: Evaluating visual activity recognition systems is challenging due to inherent
ambiguities in verb semantics and image interpretation. When describing actions
in images, synonymous verbs can refer to the same event (e.g., brushing vs.
grooming), while different perspectives can lead to equally valid but distinct
verb choices (e.g., piloting vs. operating). Standard exact-match evaluation,
which relies on a single gold answer, fails to capture these ambiguities,
resulting in an incomplete assessment of model performance. To address this, we
propose a vision-language clustering framework that constructs verb sense
clusters, providing a more robust evaluation. Our analysis of the imSitu
dataset shows that each image maps to an average of 2.8 sense clusters, with
each cluster representing a distinct perspective of the image. We evaluate
multiple activity recognition models and compare our cluster-based evaluation
with standard evaluation methods. Additionally, our human alignment analysis
suggests that the cluster-based evaluation better aligns with human judgements,
offering a more nuanced assessment of model performance.

</details>


### [8] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
*Song Wang,Yishu Wei,Haotian Ma,Max Lovitt,Kelly Deng,Yuan Meng,Zihan Xu,Jingze Zhang,Yunyu Xiao,Ying Ding,Xuhai Xu,Joydeep Ghosh,Yifan Peng*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段大语言模型框架，用于从非结构化文本中提取社会决定因素（SDoH），并展示了其在性能和可解释性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 了解导致自杀事件的社会决定因素对于早期干预和预防至关重要。然而，数据驱动的方法面临挑战，如长尾因子分布、分析自杀事件前的关键压力源以及模型可解释性有限。

Method: 我们提出了一种多阶段的大语言模型框架，以增强从非结构化文本中提取社会决定因素（SDoH）的能力。我们的方法与其他最先进的语言模型（如预训练的BioBERT和GPT-3.5-turbo）和推理模型（如DeepSeek-R1）进行了比较。我们还评估了模型的解释如何帮助人们更快、更准确地标注SDoH因素。分析包括自动化比较和试点用户研究。

Result: 我们展示了所提出的框架在整体任务中提取SDoH因素以及在更细粒度的任务中检索相关上下文方面的性能提升。此外，我们展示了微调一个较小的任务特定模型可以实现相当或更好的性能，同时减少推理成本。多阶段设计不仅增强了提取效果，还提供了中间解释，提高了模型的可解释性。

Conclusion: 我们的方法提高了从非结构化文本中提取与自杀相关的社会决定因素的准确性和透明度。这些进步有可能支持早期识别处于风险中的个体，并制定更有效的预防策略。

Abstract: Background: Understanding social determinants of health (SDoH) factors
contributing to suicide incidents is crucial for early intervention and
prevention. However, data-driven approaches to this goal face challenges such
as long-tailed factor distributions, analyzing pivotal stressors preceding
suicide incidents, and limited model explainability. Methods: We present a
multi-stage large language model framework to enhance SDoH factor extraction
from unstructured text. Our approach was compared to other state-of-the-art
language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning
models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help
people annotate SDoH factors more quickly and accurately. The analysis included
both automated comparisons and a pilot user study. Results: We show that our
proposed framework demonstrated performance boosts in the overarching task of
extracting SDoH factors and in the finer-grained tasks of retrieving relevant
context. Additionally, we show that fine-tuning a smaller, task-specific model
achieves comparable or better performance with reduced inference costs. The
multi-stage design not only enhances extraction but also provides intermediate
explanations, improving model explainability. Conclusions: Our approach
improves both the accuracy and transparency of extracting suicide-related SDoH
from unstructured texts. These advancements have the potential to support early
identification of individuals at risk and inform more effective prevention
strategies.

</details>


### [9] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
*Kun Peng,Cong Cao,Hao Peng,Zhifeng Hao,Lei Jiang,Kongjing Gu,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的对话划分方法和两步框架，用于更有效地提取对话中的目标-方面-意见-情感四元组。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常学习整个对话中的词关系，假设情感元素均匀分布，但对话中常常包含多个语义独立的子对话，这会导致额外的噪声。

Method: 我们提出了一种基于结构熵最小化算法的对话划分方法，并引入了一个两步框架进行四元组提取。

Result: 实验表明，我们的方法在DiaASQ中表现优异，计算成本较低。

Conclusion: 我们的方法在DiaASQ中实现了最先进的性能，并且计算成本显著降低。

Abstract: Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to
extract all target-aspect-opinion-sentiment quadruples from a given
multi-round, multi-participant dialogue. Existing methods typically learn word
relations across entire dialogues, assuming a uniform distribution of sentiment
elements. However, we find that dialogues often contain multiple semantically
independent sub-dialogues without clear dependencies between them. Therefore,
learning word relationships across the entire dialogue inevitably introduces
additional noise into the extraction process. To address this, our method
focuses on partitioning dialogues into semantically independent sub-dialogues.
Achieving completeness while minimizing these sub-dialogues presents a
significant challenge. Simply partitioning based on reply relationships is
ineffective. Instead, we propose utilizing a structural entropy minimization
algorithm to partition the dialogues. This approach aims to preserve relevant
utterances while distinguishing irrelevant ones as much as possible.
Furthermore, we introduce a two-step framework for quadruple extraction: first
extracting individual sentiment elements at the utterance level, then matching
quadruples at the sub-dialogue level. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in DiaASQ with much lower
computational costs.

</details>


### [10] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
*Shu Han Ho*

Main category: cs.CL

TL;DR: 本文评估了四种大型语言模型在AMR解析任务上的微调效果，结果表明微调仅解码器的模型可以达到与先进方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 探索微调仅解码器的大型语言模型在AMR解析中的潜力，以提供一种简单有效的解决方案。

Method: 对四种不同的LLM架构进行微调，并在LDC2020T02 Gold AMR3.0测试集上进行评估。

Result: 微调后的模型在SMATCH F1指标上达到了0.804，与最先进的AMR解析器表现相当，其中LLaMA 3.2在语义性能上领先，而Phi 3.5在结构有效性上表现优异。

Conclusion: 微调仅解码器的大型语言模型可以达到与复杂最先进的AMR解析器相当的性能，其中LLaMA 3.2表现出色。

Abstract: Meaning Representation (AMR) is a semantic formalism that encodes sentence
meaning as rooted, directed, acyclic graphs, where nodes represent concepts and
edges denote semantic relations. Finetuning decoder only Large Language Models
(LLMs) represent a promising novel straightfoward direction for AMR parsing.
This paper presents a comprehensive evaluation of finetuning four distinct LLM
architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled
using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that
straightfoward finetuning of decoder only LLMs can achieve comparable
performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2
demonstrates competitive performance against SOTA AMR parsers given a
straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full
LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching
Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a
consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5
excels in structural validity.

</details>


### [11] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
*Jinda Liu,Bo Cheng,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 我们的研究挑战了现有的多任务学习范式，提出了一种更简单但更有效的适应多个任务的方法Align-LoRA。


<details>
  <summary>Details</summary>
Motivation: 当前的多任务学习方法通常采用多个适配器或头部，以捕捉任务特定的知识。然而，我们的研究发现，这种复杂的多组件范式可能并不是最优的。

Method: 我们提出了Align-LoRA方法，该方法通过在共享适配器空间中对任务表示进行对齐，以提高多任务学习的效果。

Result: 实验结果表明，Align-LoRA方法在多个任务上表现优异，显著超越了所有基线。

Conclusion: 我们的研究结果表明，有效的多任务学习泛化依赖于学习稳健的共享表示，而不是隔离任务特定特征。我们提出的Align-LoRA方法通过在共享适配器空间中对任务表示进行对齐，显著超越了所有基线，建立了一个更简单但更有效的适应多个任务的范式。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large
Language Models (LLMs). In practice, LLMs are often required to handle a
diverse set of tasks from multiple domains, a scenario naturally addressed by
multi-task learning (MTL). Within this MTL context, a prevailing trend involves
LoRA variants with multiple adapters or heads, which advocate for structural
diversity to capture task-specific knowledge. Our findings present a direct
challenge to this paradigm. We first show that a simplified multi-head
architecture with high inter-head similarity substantially outperforms complex
multi-adapter and multi-head systems. This leads us to question the
multi-component paradigm itself, and we further demonstrate that a standard
single-adapter LoRA, with a sufficiently increased rank, also achieves highly
competitive performance. These results lead us to a new hypothesis: effective
MTL generalization hinges on learning robust shared representations, not
isolating task-specific features. To validate this, we propose Align-LoRA,
which incorporates an explicit loss to align task representations within the
shared adapter space. Experiments confirm that Align-LoRA significantly
surpasses all baselines, establishing a simpler yet more effective paradigm for
adapting LLMs to multiple tasks. The code is available at
https://github.com/jinda-liu/Align-LoRA.

</details>


### [12] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
*Aditya Kishore,Gaurav Kumar,Jasabanta Patro*

Main category: cs.CL

TL;DR: 本文提出了一种名为MultiCheck的统一框架，用于细粒度的多模态事实验证。该框架结合了文本和图像的专用编码器以及一个融合模块，该模块使用逐元素交互来捕捉跨模态关系。然后，分类头预测声明的真实性，并通过对比学习目标鼓励声明-证据对在共享潜在空间中的语义对齐。在Factify 2数据集上评估，取得了加权F1分数0.84，明显优于基线。这些结果表明了显式多模态推理的有效性，并展示了该方法在复杂真实场景中可扩展和可解释的虚假检查的潜力。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息的增长对依赖于文本证据的事实检查系统构成了重大挑战。

Method: 我们提出了一个统一的框架，称为MultiCheck，用于细粒度的多模态事实验证。该框架结合了文本和图像的专用编码器以及一个融合模块，该模块使用逐元素交互来捕捉跨模态关系。然后，分类头预测声明的真实性，并通过对比学习目标鼓励声明-证据对在共享潜在空间中的语义对齐。

Result: 我们在Factify 2数据集上评估了我们的方法，取得了加权F1分数0.84，明显优于基线。

Conclusion: 我们的方法在复杂的真实场景中展示了可扩展和可解释的虚假检查的潜力。

Abstract: The growing rate of multimodal misinformation, where claims are supported by
both text and images, poses significant challenges to fact-checking systems
that rely primarily on textual evidence. In this work, we have proposed a
unified framework for fine-grained multimodal fact verification called
"MultiCheck", designed to reason over structured textual and visual signals.
Our architecture combines dedicated encoders for text and images with a fusion
module that captures cross-modal relationships using element-wise interactions.
A classification head then predicts the veracity of a claim, supported by a
contrastive learning objective that encourages semantic alignment between
claim-evidence pairs in a shared latent space. We evaluate our approach on the
Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially
outperforming the baseline. These results highlight the effectiveness of
explicit multimodal reasoning and demonstrate the potential of our approach for
scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [13] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Jing Liu,Wayne Xin Zhao,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: BEE-RAG is a new framework that addresses the challenges of long context lengths in RAG by balancing entropy and improving attention dynamics.


<details>
  <summary>Details</summary>
Motivation: RAG suffers from performance issues due to long context lengths, which cause unconstrained entropy growth and attention dilution. The goal is to enhance RAG's adaptability and stability.

Method: BEE-RAG framework uses entropy invariance to improve adaptability to different context lengths, reformulating attention dynamics and introducing a zero-shot inference strategy and parameter-efficient adaptive fine-tuning mechanism.

Result: Experiments across multiple RAG tasks show that BEE-RAG effectively improves performance by maintaining stable entropy levels and adapting to varying context lengths.

Conclusion: BEE-RAG demonstrates effectiveness in various RAG tasks through its balanced entropy-engineered approach.

Abstract: With the rapid advancement of large language models (LLMs),
retrieval-augmented generation (RAG) has emerged as a critical approach to
supplement the inherent knowledge limitations of LLMs. However, due to the
typically large volume of retrieved information, RAG tends to operate with long
context lengths. From the perspective of entropy engineering, we identify
unconstrained entropy growth and attention dilution due to long retrieval
context as significant factors affecting RAG performance. In this paper, we
propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves
the adaptability of RAG systems to varying context lengths through the
principle of entropy invariance. By leveraging balanced context entropy to
reformulate attention dynamics, BEE-RAG separates attention sensitivity from
context length, ensuring a stable entropy level. Building upon this, we
introduce a zero-shot inference strategy for multi-importance estimation and a
parameter-efficient adaptive fine-tuning mechanism to obtain the optimal
balancing factor for different settings. Extensive experiments across multiple
RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [14] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
*Zihao Yi,Delong Zeng,Zhenqing Ling,Haohao Luo,Zhe Xu,Wei Liu,Jian Luan,Wanxia Cao,Ying Shen*

Main category: cs.CL

TL;DR: AttnRank is a new method that improves the performance of Large Language Models by reordering retrieved documents or few-shot examples to align with the model's attention preferences, achieving significant improvements across various models without changing their parameters or training procedures.


<details>
  <summary>Details</summary>
Motivation: The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance.

Method: We introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions.

Result: Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.

Conclusion: AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.

Abstract: The performance of Large Language Models (LLMs) is significantly sensitive to
the contextual position of information in the input. To investigate the
mechanism behind this positional bias, our extensive experiments reveal a
consistent phenomenon we term the attention basin: when presented with a
sequence of structured items (e.g., retrieved documents or few-shot examples),
models systematically assign higher attention to the items at the beginning and
end of the sequence, while neglecting those in the middle. Crucially, our
analysis further reveals that allocating higher attention to critical
information is key to enhancing model performance. Based on these insights, we
introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)
estimates a model's intrinsic positional attention preferences using a small
calibration set, and (ii) reorders retrieved documents or few-shot examples to
align the most salient content with these high-attention positions. AttnRank is
a model-agnostic, training-free, and plug-and-play method with minimal
computational overhead. Experiments on multi-hop QA and few-shot in-context
learning tasks demonstrate that AttnRank achieves substantial improvements
across 10 large language models of varying architectures and scales, without
modifying model parameters or training procedures.

</details>


### [15] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
*Chang Hong,Minghao Wu,Qingying Xiao,Yuchi Wang,Xiang Wan,Guangjun Yu,Benyou Wang,Yan Hu*

Main category: cs.CL

TL;DR: 本文介绍了 PrinciplismQA，一个用于评估大型语言模型在医学伦理方面表现的基准测试，发现模型在实际应用中存在差距，并提出通过医学领域微调来提升伦理能力。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型整合到医疗保健中需要对其道德推理进行严格评估，而当前的基准常常忽视了这一领域。

Method: 引入 PrinciplismQA，这是一个包含 3,648 个问题的全面基准，旨在系统评估 LLMs 与核心医学伦理的一致性。该基准基于普林西普主义，包括从权威教材中精心挑选的选择题和从权威医学伦理案例研究文献中获取的开放性问题，并由医学专家验证。

Result: 实验结果显示，模型的伦理知识与其实际应用之间存在显著差距，特别是在动态应用伦理原则到现实场景方面。大多数 LLM 在有关有益性的困境中表现不佳，往往过于强调其他原则。前沿的封闭源代码模型由于强大的通用能力目前在基准测试中领先。值得注意的是，医学领域微调可以提高模型的整体伦理能力，但进一步进展需要更好的与医学伦理知识对齐。

Conclusion: PrinciplismQA 提供了一个可扩展的框架，用于诊断这些特定的伦理弱点，为更平衡和负责任的医疗人工智能铺平了道路。

Abstract: The integration of large language models into healthcare necessitates a
rigorous evaluation of their ethical reasoning, an area current benchmarks
often overlook. We introduce PrinciplismQA, a comprehensive benchmark with
3,648 questions designed to systematically assess LLMs' alignment with core
medical ethics. Grounded in Principlism, our benchmark features a high-quality
dataset. This includes multiple-choice questions curated from authoritative
textbooks and open-ended questions sourced from authoritative medical ethics
case study literature, all validated by medical experts. Our experiments reveal
a significant gap between models' ethical knowledge and their practical
application, especially in dynamically applying ethical principles to
real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,
often over-emphasizing other principles. Frontier closed-source models, driven
by strong general capabilities, currently lead the benchmark. Notably, medical
domain fine-tuning can enhance models' overall ethical competence, but further
progress requires better alignment with medical ethical knowledge.
PrinciplismQA offers a scalable framework to diagnose these specific ethical
weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [16] [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)
*Catherine Kobus,François Lancelot,Marion-Cécile Martin,Nawal Ould Amer*

Main category: cs.CL

TL;DR: 本文介绍了ATLANTIS团队在SemEval-2025任务3中的贡献，专注于检测问答系统中的幻觉文本跨度。通过探索使用和不使用外部上下文的方法，包括少量样本提示和模型微调，实现了在多种语言中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言生成方面取得了显著进展，但仍容易产生幻觉，生成错误或误导性的内容。

Method: 本文探讨了使用外部上下文和不使用外部上下文的方法，利用少量样本提示与大型语言模型，以及基于合成数据的大型语言模型微调进行标记级分类。

Result: 本文的方法在西班牙语中获得了顶级排名，在英语和德语中获得了具有竞争力的排名。

Conclusion: 本文强调了整合相关上下文以减轻幻觉的重要性，并展示了微调模型和提示工程的潜力。

Abstract: This paper presents the contributions of the ATLANTIS team to SemEval-2025
Task 3, focusing on detecting hallucinated text spans in question answering
systems. Large Language Models (LLMs) have significantly advanced Natural
Language Generation (NLG) but remain susceptible to hallucinations, generating
incorrect or misleading content. To address this, we explored methods both with
and without external context, utilizing few-shot prompting with a LLM,
token-level classification or LLM fine-tuned on synthetic data. Notably, our
approaches achieved top rankings in Spanish and competitive placements in
English and German. This work highlights the importance of integrating relevant
context to mitigate hallucinations and demonstrate the potential of fine-tuned
models and prompt engineering.

</details>


### [17] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
*Haonan Shangguan,Xiaocui Yang,Shi Feng,Daling Wang,Yifei Zhang,Ge Yu*

Main category: cs.CL

TL;DR: 本文提出MulCoT-RD模型，用于资源受限环境下的多模态情感推理和分类任务，通过蒸馏方法实现高效且可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要依赖参数密集的多模态LLM进行情感分类，忽略了在资源受限环境中的自主多模态情感推理生成。

Method: 提出了一种多模态思维链推理蒸馏模型MulCoT-RD，采用“教师-助教-学生”蒸馏范式，以解决资源受限环境中的部署约束。

Result: MulCoT-RD在四个数据集上的实验表明，仅使用3B参数就能在JMSRC任务中实现强大的性能，并展现出稳健的泛化能力和增强的可解释性。

Conclusion: MulCoT-RD在资源受限环境中表现出色，具有强大的性能、稳健的泛化能力和增强的可解释性。

Abstract: The surge in rich multimodal content on social media platforms has greatly
advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)
further accelerating progress in this field. Current approaches primarily
leverage the knowledge and reasoning capabilities of parameter-heavy
(Multimodal) LLMs for sentiment classification, overlooking autonomous
multimodal sentiment reasoning generation in resource-constrained environments.
Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment
Reasoning and Classification task, JMSRC, which simultaneously performs
multimodal sentiment reasoning chain generation and sentiment classification
only with a lightweight model. We propose a Multimodal Chain-of-Thought
Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a
"Teacher-Assistant-Student" distillation paradigm to address deployment
constraints in resource-limited environments. We first leverage a
high-performance Multimodal Large Language Model (MLLM) to generate the initial
reasoning dataset and train a medium-sized assistant model with a multi-task
learning mechanism. A lightweight student model is jointly trained to perform
efficient multimodal sentiment reasoning generation and classification.
Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B
parameters achieves strong performance on JMSRC, while exhibiting robust
generalization and enhanced interpretability.

</details>


### [18] [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)
*Yiheng Liu,Junhao Ning,Sichen Xia,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 本文提出了一种基于功能网络的大型语言模型修剪方法，通过识别和保留关键神经元来提高模型压缩效率。


<details>
  <summary>Details</summary>
Motivation: 当前的结构化修剪方法通常忽略了人工神经元之间的交互和协作，这可能导致大型语言模型的宏观功能架构被破坏，从而导致修剪性能下降。本文旨在解决这一问题。

Method: 将大型语言模型视为一个数字大脑，并将其分解为功能网络，类似于在神经影像数据中识别功能脑网络。然后通过保留这些功能网络中的关键神经元来修剪模型。

Result: 实验结果表明，所提出的方法可以成功识别和定位大型语言模型中的功能网络和关键神经元，从而实现高效的模型压缩。

Conclusion: 本文提出了一种通过识别和保留大型语言模型中的功能网络来修剪模型的方法，实验结果表明该方法可以成功识别和定位功能网络和关键神经元，从而实现高效的模型压缩。

Abstract: Structured pruning is one of the representative techniques for compressing
large language models (LLMs) to reduce GPU memory consumption and accelerate
inference speed. It offers significant practical value in improving the
efficiency of LLMs in real-world applications. Current structured pruning
methods typically rely on assessment of the importance of the structure units
and pruning the units with less importance. Most of them overlooks the
interaction and collaboration among artificial neurons that are crucial for the
functionalities of LLMs, leading to a disruption in the macro functional
architecture of LLMs and consequently a pruning performance degradation.
Inspired by the inherent similarities between artificial neural networks and
functional neural networks in the human brain, we alleviate this challenge and
propose to prune LLMs by identifying and preserving functional networks within
LLMs in this study. To achieve this, we treat an LLM as a digital brain and
decompose the LLM into functional networks, analogous to identifying functional
brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving
the key neurons within these functional networks. Experimental results
demonstrate that the proposed method can successfully identify and locate
functional networks and key neurons in LLMs, enabling efficient model pruning.
Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [19] [CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL](https://arxiv.org/abs/2508.05242)
*Sijie Wang,Quanjiang Guo,Kai Zhao,Yawei Zhang,Xin Li,Xiang Li,Siqi Li,Rui She,Shangshu Yu,Wee Peng Tay*

Main category: cs.CL

TL;DR: CodeBoost is a post-training framework that improves code LLMs using code snippets instead of human-annotated instructions, leading to consistent performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the bottleneck in instruction-based post-training caused by the difficulty and labor-intensive nature of collecting high-quality coding instructions, while leveraging the abundance of code snippets.

Method: CodeBoost is a post-training framework that enhances code LLMs purely from code snippets without relying on human-annotated instructions. It includes maximum-clique curation, bi-directional prediction, error-aware prediction, heterogeneous augmentation, and heterogeneous rewarding.

Result: Extensive experiments show that CodeBoost consistently improves performance on various code LLMs and benchmarks.

Conclusion: CodeBoost consistently improves performance across several code LLMs and benchmarks, demonstrating its effectiveness as a scalable and effective training pipeline.

Abstract: Code large language models (LLMs) have become indispensable tools for
building efficient and automated coding pipelines. Existing models are
typically post-trained using reinforcement learning (RL) from general-purpose
LLMs using "human instruction-final answer" pairs, where the instructions are
usually from manual annotations. However, collecting high-quality coding
instructions is both labor-intensive and difficult to scale. On the other hand,
code snippets are abundantly available from various sources. This imbalance
presents a major bottleneck in instruction-based post-training. We propose
CodeBoost, a post-training framework that enhances code LLMs purely from code
snippets, without relying on human-annotated instructions. CodeBoost introduces
the following key components: (1) maximum-clique curation, which selects a
representative and diverse training corpus from code; (2) bi-directional
prediction, which enables the model to learn from both forward and backward
prediction objectives; (3) error-aware prediction, which incorporates learning
signals from both correct and incorrect outputs; (4) heterogeneous
augmentation, which diversifies the training distribution to enrich code
semantics; and (5) heterogeneous rewarding, which guides model learning through
multiple reward types including format correctness and execution feedback from
both successes and failures. Extensive experiments across several code LLMs and
benchmarks verify that CodeBoost consistently improves performance,
demonstrating its effectiveness as a scalable and effective training pipeline.

</details>


### [20] [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
*Dongxu Zhang,Ning Yang,Jihua Zhu,Jinnan Yang,Miao Xin,Baoliang Tian*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning
capabilities of Large Language Models (LLMs), yet the reliability of these
reasoning chains remains a critical challenge. A widely held "cascading
failure" hypothesis suggests that errors are most detrimental when they occur
early in the reasoning process. This paper challenges that assumption through
systematic error-injection experiments, revealing a counter-intuitive
phenomenon we term "Late-Stage Fragility": errors introduced in the later
stages of a CoT chain are significantly more likely to corrupt the final answer
than identical errors made at the beginning. To address this specific
vulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought
(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive
Verification Manager (AVM) operates first, followed by the Multi-Perspective
Self-Correction Engine (MSCE). The AVM leverages a Positional Impact Score
function I(k) that assigns different weights based on the position within the
reasoning chains, addressing the Late-Stage Fragility issue by identifying and
prioritizing high-risk, late-stage steps. Once these critical steps are
identified, the MSCE applies robust, dual-path correction specifically to the
failure parts. Extensive experiments on benchmarks such as GSM8K and MATH
demonstrate that ASCoT achieves outstanding accuracy, outperforming strong
baselines, including standard CoT. Our work underscores the importance of
diagnosing specific failure modes in LLM reasoning and advocates for a shift
from uniform verification strategies to adaptive, vulnerability-aware
correction mechanisms.

</details>


### [21] [Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue](https://arxiv.org/abs/2508.05283)
*Sukannya Purkayastha,Nils Dycke,Anne Lauscher,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文研究了如何利用对话代理辅助元评审，通过生成高质量的合成数据并训练专门的对话代理，实验证明这些代理在实际场景中有效提高了元评审的效率。


<details>
  <summary>Details</summary>
Motivation: 元评审是一个决策过程，需要权衡审稿人的论点并将其置于更广泛的背景下，而以往的研究将其视为摘要问题。我们旨在探索实现能够有效协助元评审者的对话代理的实际挑战。

Method: 我们首先通过使用基于自我精炼策略的大型语言模型生成合成数据来解决训练对话代理的数据稀缺问题，然后利用这些数据训练专门用于元评审的对话代理。

Result: 我们的方法产生了高质量的合成数据，并且训练的对话代理在任务上优于现成的基于大型语言模型的助手。此外，我们在实际场景中应用了这些代理，并确认了它们在提高元评审效率方面的有效性。

Conclusion: 我们的研究证明了对话代理在元评审中的有效性，并展示了它们在实际场景中提高元评审效率的能力。

Abstract: Meta-reviewing is a pivotal stage in the peer-review process, serving as the
final step in determining whether a paper is recommended for acceptance. Prior
research on meta-reviewing has treated this as a summarization problem over
review reports. However, complementary to this perspective, meta-reviewing is a
decision-making process that requires weighing reviewer arguments and placing
them within a broader context. Prior research has demonstrated that
decision-makers can be effectively assisted in such scenarios via dialogue
agents. In line with this framing, we explore the practical challenges for
realizing dialog agents that can effectively assist meta-reviewers. Concretely,
we first address the issue of data scarcity for training dialogue agents by
generating synthetic data using Large Language Models (LLMs) based on a
self-refinement strategy to improve the relevance of these dialogues to expert
domains. Our experiments demonstrate that this method produces higher-quality
synthetic data and can serve as a valuable resource towards training
meta-reviewing assistants. Subsequently, we utilize this data to train dialogue
agents tailored for meta-reviewing and find that these agents outperform
\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our
agents in real-world meta-reviewing scenarios and confirm their effectiveness
in enhancing the efficiency of meta-reviewing.\footnote{Code and Data:
https://github.com/UKPLab/arxiv2025-meta-review-as-dialog

</details>


### [22] [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens](https://arxiv.org/abs/2508.05305)
*Nikita Dragunov,Temurbek Rahmatullaev,Elizaveta Goncharova,Andrey Kuznetsov,Anton Razzhigaev*

Main category: cs.CL

TL;DR: SONAR-LLM是一种基于SONAR嵌入空间的解码器模型，通过token级交叉熵进行监督，实现了与LCM相当的生成质量。


<details>
  <summary>Details</summary>
Motivation: 为了保留LCM的语义抽象，同时消除其扩散采样器并恢复基于似然的训练信号，提出了SONAR-LLM。

Method: SONAR-LLM是一种仅解码器的变压器，它在相同的连续SONAR嵌入空间中“思考”，并通过冻结的SONAR解码器传播的token级交叉熵进行监督。

Result: SONAR-LLM在从39M到1.3B参数的不同规模模型中达到了具有竞争力的生成质量。

Conclusion: SONAR-LLM在不同规模的模型中表现出色，展示了其生成质量的竞争力，并通过释放完整的训练代码和预训练检查点促进了可重复性和未来研究。

Abstract: The recently proposed Large Concept Model (LCM) generates text by predicting
a sequence of sentence-level embeddings and training with either mean-squared
error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer
that "thinks" in the same continuous SONAR embedding space, yet is supervised
through token-level cross-entropy propagated via the frozen SONAR decoder. This
hybrid objective retains the semantic abstraction of LCM while eliminating its
diffusion sampler and restoring a likelihood-based training signal. Across
model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive
generation quality. We report scaling trends, ablations, benchmark results, and
release the complete training code and all pretrained checkpoints to foster
reproducibility and future research.

</details>


### [23] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
*Jiameng Huang,Baijiong Lin,Guhao Feng,Jierun Chen,Di He,Lu Hou*

Main category: cs.CL

TL;DR: CGRS is a novel method that suppresses overthinking in large language models by dynamically controlling reflection triggers, reducing token usage without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the overthinking problem in recent Large Reasoning Language Models (LRLMs), which leads to redundant reasoning steps, increased token usage, higher inference costs, and reduced practical utility.

Method: CGRS, a novel method that mitigates overthinking in LRLMs by dynamically suppressing the model's generation of reflection triggers when it exhibits high confidence in its current response.

Result: CGRS reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines.

Conclusion: CGRS's practical value for efficient reasoning is highlighted, as it reduces token usage while preserving accuracy across various model architectures and scales.

Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought
reasoning with complex reflection behaviors, typically signaled by specific
trigger words (e.g., "Wait" and "Alternatively") to enhance performance.
However, these reflection behaviors can lead to the overthinking problem where
the generation of redundant reasoning steps that unnecessarily increase token
usage, raise inference costs, and reduce practical utility. In this paper, we
propose Certainty-Guided Reflection Suppression (CGRS), a novel method that
mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS
operates by dynamically suppressing the model's generation of reflection
triggers when it exhibits high confidence in its current response, thereby
preventing redundant reflection cycles without compromising output quality. Our
approach is model-agnostic, requires no retraining or architectural
modifications, and can be integrated seamlessly with existing autoregressive
generation pipelines. Extensive experiments across four reasoning benchmarks
(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it
reduces token usage by an average of 18.5% to 41.9% while preserving accuracy.
It also achieves the optimal balance between length reduction and performance
compared to state-of-the-art baselines. These results hold consistently across
model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3
family) and scales (4B to 32B parameters), highlighting CGRS's practical value
for efficient reasoning.

</details>


### [24] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本研究探讨了在微软HoloLens 2设备上的现有手语虚拟人中添加调整功能的影响。尽管用户偏好可调整的设置，但未观察到用户体验或可理解性的显著改善。研究指出，个性化本身是不够的，手语虚拟人必须默认可理解。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨在现有的手语虚拟人中添加调整功能的影响，并评估其对用户体验和可接受性的影响。

Method: 本研究通过详细分析专家德语手语（DGS）用户在特定用例中与可调整和不可调整的虚拟人之间的互动，确定了影响可理解性、用户体验（UX）和系统可接受性的关键因素。

Result: 尽管用户更喜欢可调整的设置，但未观察到用户体验或可理解性的显著改善，这些仍然处于较低水平，由于缺少手语元素（如口型和面部表情）和实现问题（手形不清晰、缺乏反馈和菜单定位）。享乐质量评分高于实用质量，表明用户认为系统更具情感或审美吸引力而非功能性有用。可调整的虚拟人压力水平更高，反映出较低的表现、更大的努力和更多的挫折感。此外，关于Hololens调整手势是否直观和易于熟悉的问题也引起了关注。

Conclusion: 本研究强调了仅靠个性化是不够的，手语虚拟人必须默认可理解。关键建议包括增强口型和面部动画，改进交互界面，并应用参与式设计。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


### [25] [Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025](https://arxiv.org/abs/2508.05366)
*Samy Ateia,Udo Kruschwitz*

Main category: cs.CL

TL;DR: 本文探讨了当前推理和非推理LLM在BioASQ CLEF 2025挑战中的性能，并研究了自我反馈机制对查询扩展和多种答案类型的影响。


<details>
  <summary>Details</summary>
Motivation: 在专业搜索任务中，如生物医学研究，自动化系统可能减少用户参与并偏离专家信息需求，因此需要研究这些问题。

Method: 本文采用了一种自我反馈机制，其中LLM生成、评估并改进其输出以进行查询扩展和多种答案类型（是/否、事实性、列表、理想）。

Result: 初步结果表明，自我反馈策略在不同模型和任务中的表现各不相同。

Conclusion: 本文提供了关于LLM自我修正的见解，并为未来比较LLM生成反馈与直接人类专家输入在这些搜索系统中的有效性提供了参考。

Abstract: Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim
to enable autonomous search processes where Large Language Models (LLMs)
iteratively refine outputs. However, applying these systems to domain-specific
professional search, such as biomedical research, presents challenges, as
automated systems may reduce user involvement and misalign with expert
information needs. Professional search tasks often demand high levels of user
expertise and transparency. The BioASQ CLEF 2025 challenge, using
expert-formulated questions, can serve as a platform to study these issues. We
explored the performance of current reasoning and nonreasoning LLMs like
Gemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our
methodology was a self-feedback mechanism where LLMs generated, evaluated, and
then refined their outputs for query expansion and for multiple answer types
(yes/no, factoid, list, ideal). We investigated whether this iterative
self-correction improves performance and if reasoning models are more capable
of generating useful feedback. Preliminary results indicate varied performance
for the self-feedback strategy across models and tasks. This work offers
insights into LLM self-correction and informs future work on comparing the
effectiveness of LLM-generated feedback with direct human expert input in these
search systems.

</details>


### [26] [The TUB Sign Language Corpus Collection](https://arxiv.org/abs/2508.05374)
*Eleftherios Avramidis,Vera Czehmann,Fabian Deckert,Lorenz Hufe,Aljoscha Lipski,Yuni Amaloa Quintero Villalobos,Tae Kwon Rhee,Mengqian Shi,Lennart Stölting,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文介绍了一个包含12种手语的大型并行语料库，包括视频和字幕，用于促进手语研究和自然语言处理技术的发展。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个大规模的手语并行语料库，以促进手语研究和自然语言处理技术的发展。

Method: 通过从多个在线来源收集和处理多种手语的视频，主要是新闻节目、政府机构和教育频道的广播材料。准备过程包括数据收集、通知内容创作者并寻求使用许可、爬取和裁剪。

Result: 创建了一个包含12种手语的大型并行语料库，其中8种拉丁美洲手语是首次出现，德国手语语料库的规模是之前可用语料库的十倍。

Conclusion: 本文介绍了12种手语的并行语料库，包括视频格式和主要口语语言的字幕。该语料库包含超过1300小时的4381个视频文件，配有130万字幕，包含1400万个词。这是首次为8种拉丁美洲手语提供一致的并行语料库，而德国手语语料库的规模是之前可用语料库的十倍。

Abstract: We present a collection of parallel corpora of 12 sign languages in video
format, together with subtitles in the dominant spoken languages of the
corresponding countries. The entire collection includes more than 1,300 hours
in 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens.
Most notably, it includes the first consistent parallel corpora for 8 Latin
American sign languages, whereas the size of the German Sign Language corpora
is ten times the size of the previously available corpora. The collection was
created by collecting and processing videos of multiple sign languages from
various online sources, mainly broadcast material of news shows, governmental
bodies and educational channels. The preparation involved several stages,
including data collection, informing the content creators and seeking usage
approvals, scraping, and cropping. The paper provides statistics on the
collection and an overview of the methods used to collect the data.

</details>


### [27] [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)
*Zhong Ken Hew,Jia Xin Low,Sze Jue Yang,Chee Seng chan*

Main category: cs.CL

TL;DR: 本文介绍了一个名为MyCulture的基准，用于评估大型语言模型在马来西亚文化方面的表现，旨在解决文化偏见问题并提高评估的公平性和区分度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）由于训练数据主要由高资源语言如英语和中文主导，往往表现出文化偏见。这给准确表示和评估多样化的文化背景，特别是在低资源语言环境中带来了挑战。

Method: 我们引入了MyCulture，这是一个用于全面评估大型语言模型在马来西亚文化方面的基准，涵盖了六个支柱：艺术、服饰、习俗、娱乐、食物和宗教，并采用了一种新颖的开放性多项选择题格式。此外，我们通过比较模型在结构化与自由形式输出上的表现来分析结构偏差，并通过多语言提示变化来评估语言偏差。

Result: 我们的评估显示，不同区域和国际大型语言模型在文化理解方面存在显著差异，表明需要更加注重文化和语言包容性的基准。

Conclusion: 我们的评估结果显示，各种区域和国际大型语言模型在文化理解上存在显著差异，突显了在开发和评估大型语言模型时需要文化和语言包容性基准的紧迫性。

Abstract: Large Language Models (LLMs) often exhibit cultural biases due to training
data dominated by high-resource languages like English and Chinese. This poses
challenges for accurately representing and evaluating diverse cultural
contexts, particularly in low-resource language settings. To address this, we
introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on
Malaysian culture across six pillars: arts, attire, customs, entertainment,
food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,
MyCulture employs a novel open-ended multiple-choice question format without
predefined options, thereby reducing guessing and mitigating format bias. We
provide a theoretical justification for the effectiveness of this open-ended
structure in improving both fairness and discriminative power. Furthermore, we
analyze structural bias by comparing model performance on structured versus
free-form outputs, and assess language bias through multilingual prompt
variations. Our evaluation across a range of regional and international LLMs
reveals significant disparities in cultural comprehension, highlighting the
urgent need for culturally grounded and linguistically inclusive benchmarks in
the development and assessment of LLMs.

</details>


### [28] [LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models](https://arxiv.org/abs/2508.05452)
*Ming Zhang,Yujiong Shen,Jingyi Deng,Yuhui Wang,Yue Zhang,Junzhe Wang,Shichun Liu,Shihan Dou,Huayu Sha,Qiyuan Peng,Changhao Jiang,Jingqi Tong,Yilong Wu,Zhihao Zhang,Mingqi Wu,Zhiheng Xi,Mingxu Chai,Tao Liang,Zhihui Fei,Zhen Wang,Mingyang Wan,Guojun Ma,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: LLMEval-3是一个动态评估大型语言模型的框架，通过抗污染数据整理、反作弊架构和LLM作为裁判过程，实现了与人类专家90%的一致性。研究发现静态基准无法检测到数据污染漏洞，并证明了动态评估的稳健性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型在静态基准上的评估容易受到数据污染和排行榜过拟合的影响，这些问题掩盖了模型的真实能力。

Method: LLMEval-3是一个动态评估大型语言模型的框架，基于一个专有的220k研究生级问题库，从中动态采样未见过的测试集进行每次评估运行。其自动化流程通过抗污染数据整理、新颖的反作弊架构和经过校准的LLM作为裁判过程实现与人类专家90%的一致性，还补充了一个相对排名系统以进行公平比较。

Result: 一项为期20个月的纵向研究几乎涵盖了50个领先的模型，揭示了知识记忆的性能上限，并暴露了静态基准无法检测到的数据污染漏洞。该框架在排名稳定性和一致性方面表现出色，为动态评估范式提供了强有力的实证验证。

Conclusion: LLMEval-3提供了一种稳健且可信的方法来评估大型语言模型的真实能力，超越了排行榜分数，促进了更可信赖的评估标准的发展。

Abstract: Existing evaluation of Large Language Models (LLMs) on static benchmarks is
vulnerable to data contamination and leaderboard overfitting, critical issues
that obscure true model capabilities. To address this, we introduce LLMEval-3,
a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary
bank of 220k graduate-level questions, from which it dynamically samples unseen
test sets for each evaluation run. Its automated pipeline ensures integrity via
contamination-resistant data curation, a novel anti-cheating architecture, and
a calibrated LLM-as-a-judge process achieving 90% agreement with human experts,
complemented by a relative ranking system for fair comparison. An 20-month
longitudinal study of nearly 50 leading models reveals a performance ceiling on
knowledge memorization and exposes data contamination vulnerabilities
undetectable by static benchmarks. The framework demonstrates exceptional
robustness in ranking stability and consistency, providing strong empirical
validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and
credible methodology for assessing the true capabilities of LLMs beyond
leaderboard scores, promoting the development of more trustworthy evaluation
standards.

</details>


### [29] [TASE: Token Awareness and Structured Evaluation for Multilingual Language Models](https://arxiv.org/abs/2508.05468)
*Chenzhuo Zhao,Xinda Wang,Yue Huang,Junting Lu,Ziqian Liu*

Main category: cs.CL

TL;DR: 本文介绍了TASE，这是一个全面的基准测试，用于评估大型语言模型（LLM）在跨语言中感知和推理token级信息的能力。TASE涵盖10个任务，包括字符计数、token对齐、句法结构解析和长度约束满足。我们评估了30多个领先的商业和开源LLM，并训练了一个自定义的Qwen2.5-14B模型。结果表明，人类的表现显著优于当前的LLM，揭示了token级推理中的持续弱点。


<details>
  <summary>Details</summary>
Motivation: While large language models (LLMs) have demonstrated remarkable performance on high-level semantic tasks, they often struggle with fine-grained, token-level understanding and structural reasoning--capabilities that are essential for applications requiring precision and control.

Method: We introduce TASE, a comprehensive benchmark designed to evaluate LLMs' ability to perceive and reason about token-level information across languages. We evaluate over 30 leading commercial and open-source LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a custom Qwen2.5-14B model using the GRPO training method.

Result: Results show that human performance significantly outpaces current LLMs, revealing persistent weaknesses in token-level reasoning.

Conclusion: TASE sheds light on these limitations and provides a new diagnostic lens for future improvements in low-level language understanding and cross-lingual generalization.

Abstract: While large language models (LLMs) have demonstrated remarkable performance
on high-level semantic tasks, they often struggle with fine-grained,
token-level understanding and structural reasoning--capabilities that are
essential for applications requiring precision and control. We introduce TASE,
a comprehensive benchmark designed to evaluate LLMs' ability to perceive and
reason about token-level information across languages. TASE covers 10 tasks
under two core categories: token awareness and structural understanding,
spanning Chinese, English, and Korean, with a 35,927-instance evaluation set
and a scalable synthetic data generation pipeline for training. Tasks include
character counting, token alignment, syntactic structure parsing, and length
constraint satisfaction. We evaluate over 30 leading commercial and open-source
LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a
custom Qwen2.5-14B model using the GRPO training method. Results show that
human performance significantly outpaces current LLMs, revealing persistent
weaknesses in token-level reasoning. TASE sheds light on these limitations and
provides a new diagnostic lens for future improvements in low-level language
understanding and cross-lingual generalization. Our code and dataset are
publicly available at https://github.com/cyzcz/Tase .

</details>


### [30] [Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations](https://arxiv.org/abs/2508.05470)
*Li-Chun Lu,Miri Liu,Pin-Chun Lu,Yufei Tian,Shao-Hua Sun,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文系统分析了多种创造力测量方法在不同领域的表现，发现它们存在局限性，强调需要更稳健的评估框架。


<details>
  <summary>Details</summary>
Motivation: 为了评估不同创造力测量方法在各种创造性领域的表现，并揭示它们的局限性，从而推动更有效的评估框架的发展。

Method: 我们系统地检查、分析和比较了代表性的创造力测量方法——创造力指数、困惑度、句法模板和LLM-as-a-Judge，在包括创造性写作、非常规问题解决和研究构想在内的多种创造性领域中。

Result: 这些指标表现出有限的一致性，捕捉到创造力的不同方面。创造力指数侧重于词汇多样性，困惑度对模型信心敏感，句法模板无法捕捉概念性创造力。此外，LLM-as-a-Judge表现出不稳定性和偏见。

Conclusion: 我们的研究强调了需要更稳健、可推广的评估框架，以更好地与人类对创造力的判断对齐。

Abstract: We systematically examine, analyze, and compare representative creativity
measures--creativity index, perplexity, syntactic templates, and
LLM-as-a-Judge--across diverse creative domains, including creative writing,
unconventional problem-solving, and research ideation. Our analyses reveal that
these metrics exhibit limited consistency, capturing different dimensions of
creativity. We highlight key limitations, including the creativity index's
focus on lexical diversity, perplexity's sensitivity to model confidence, and
syntactic templates' inability to capture conceptual creativity. Additionally,
LLM-as-a-Judge shows instability and bias. Our findings underscore the need for
more robust, generalizable evaluation frameworks that better align with human
judgments of creativity.

</details>


### [31] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Su Dong,Shengyuan Chen,Xiao Huang*

Main category: cs.CL

TL;DR: 本文提出了逻辑增强生成（LAG），通过系统的提问分解和依赖感知推理来提高大型语言模型在知识密集型任务中的表现，减少幻觉并增强推理鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种任务中表现出色，但在需要专业知识的任务中存在关键限制，常常在面对需要专门知识的问题时产生幻觉。虽然检索增强生成（RAG）通过整合外部知识缓解了这一问题，但由于依赖直接语义检索和缺乏结构化的逻辑组织，在复杂推理场景中表现不佳。

Method: 本文引入了逻辑增强生成（LAG），通过系统的提问分解和依赖感知推理来重新构想知识增强。LAG首先将复杂问题分解为按逻辑依赖关系排序的原子子问题，然后依次解决这些子问题，使用先前的答案来指导后续子问题的上下文检索，确保逐步在逻辑链中得到支持。为了防止错误传播，LAG引入了一个逻辑终止机制，在遇到无法回答的子问题时停止推理，并减少过多推理上的浪费计算。最后，它综合所有子解决方案生成经过验证的响应。

Result: 在四个基准数据集上的实验表明，LAG显著提高了推理鲁棒性，减少了幻觉，并使LLM的问题解决与人类认知对齐，提供了一种有原则的RAG系统替代方案。

Conclusion: 实验表明，LAG显著提高了推理鲁棒性，减少了幻觉，并使LLM的问题解决与人类认知对齐，为现有的RAG系统提供了一种有原则的替代方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet exhibit critical limitations in knowledge-intensive
tasks, often generating hallucinations when faced with questions requiring
specialized expertise. While retrieval-augmented generation (RAG) mitigates
this by integrating external knowledge, it struggles with complex reasoning
scenarios due to its reliance on direct semantic retrieval and lack of
structured logical organization. Inspired by Cartesian principles from
\textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented
Generation (LAG), a novel paradigm that reframes knowledge augmentation through
systematic question decomposition and dependency-aware reasoning. Specifically,
LAG first decomposes complex questions into atomic sub-questions ordered by
logical dependencies. It then resolves these sequentially, using prior answers
to guide context retrieval for subsequent sub-questions, ensuring stepwise
grounding in logical chain. To prevent error propagation, LAG incorporates a
logical termination mechanism that halts inference upon encountering
unanswerable sub-questions and reduces wasted computation on excessive
reasoning. Finally, it synthesizes all sub-resolutions to generate verified
responses. Experiments on four benchmark datasets demonstrate that LAG
significantly enhances reasoning robustness, reduces hallucination, and aligns
LLM problem-solving with human cognition, offering a principled alternative to
existing RAG systems.

</details>


### [32] [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)
*Harsh Nishant Lalai,Raj Sanjay Shah,Jiaxin Pei,Sashank Varma,Yi-Chia Wang,Ali Emami*

Main category: cs.CL

TL;DR: 本研究通过20个问题游戏评估了大型语言模型在地理上推断实体的能力，并发现了模型在处理全球北方和全球南方实体时的显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已被广泛调整以减轻显性偏见，但它们常常表现出隐性偏见，这些偏见根植于其预训练数据中。我们希望通过研究模型自己主动提问时的行为来探索这些隐性偏见。

Method: 我们使用了一个新的数据集Geo20Q+，其中包括来自不同地区的著名人物和文化重要物体（如食物、地标、动物），并在两种游戏配置（标准的20个问题和无限回合）以及七种语言中测试了流行的大型语言模型。

Result: 我们的结果揭示了地理上的差异：大型语言模型在推断来自全球北方的实体方面比全球南方更成功，而且全球西方比全球东方更成功。虽然维基百科页面浏览量和预训练语料库频率与性能有轻微相关性，但它们无法完全解释这些差异。值得注意的是，游戏使用的语言对性能差距影响很小。

Conclusion: 我们的研究揭示了大型语言模型在推理过程中存在的地理和文化偏见，表明需要采用更具创造性和自由形式的评估框架来发现这些隐藏的偏见。

Abstract: Large Language Models (LLMs) have been extensively tuned to mitigate explicit
biases, yet they often exhibit subtle implicit biases rooted in their
pre-training data. Rather than directly probing LLMs with human-crafted
questions that may trigger guardrails, we propose studying how models behave
when they proactively ask questions themselves. The 20 Questions game, a
multi-turn deduction task, serves as an ideal testbed for this purpose. We
systematically evaluate geographic performance disparities in entity deduction
using a new dataset, Geo20Q+, consisting of both notable people and culturally
significant objects (e.g., foods, landmarks, animals) from diverse regions. We
test popular LLMs across two gameplay configurations (canonical 20-question and
unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,
French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs
are substantially more successful at deducing entities from the Global North
than the Global South, and the Global West than the Global East. While
Wikipedia pageviews and pre-training corpus frequency correlate mildly with
performance, they fail to fully explain these disparities. Notably, the
language in which the game is played has minimal impact on performance gaps.
These findings demonstrate the value of creative, free-form evaluation
frameworks for uncovering subtle biases in LLMs that remain hidden in standard
prompting setups. By analyzing how models initiate and pursue reasoning goals
over multiple turns, we find geographic and cultural disparities embedded in
their reasoning processes. We release the dataset (Geo20Q+) and code at
https://sites.google.com/view/llmbias20q/home.

</details>


### [33] [CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation](https://arxiv.org/abs/2508.05534)
*Santosh T. Y. S. S,Youssef Tarek Elkhayat,Oana Ichim,Pranav Shetty,Dongsheng Wang,Zhiqiang Ma,Armineh Nourbakhsh,Xiaomo Liu*

Main category: cs.CL

TL;DR: 本文介绍了一种新的解码策略CoCoLex，用于法律文本生成，通过结合模型生成的词汇分布和从上下文中复制的分布，提高了生成内容的准确性和忠实度。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在处理长而复杂的上下文方面的能力，它们可以为法律领域提供关键优势，但其采用受到生成不忠实、无根据或幻觉输出的倾向的阻碍。Retrieval-Augmented Generation提供了一种有希望的解决方案，但它无法保证提供的上下文会被有效整合。为了应对这一问题，提出了上下文感知的解码策略来增强相关上下文的影响，但它们通常不会明确地强制遵循上下文。

Method: CoCoLex是一种基于置信度的复制解码策略，它动态地将模型生成的词汇分布与基于从上下文中复制的分布相结合。

Result: 实验结果表明，CoCoLex在五个法律基准测试中表现优于现有的上下文感知解码方法，特别是在长篇生成任务中。

Conclusion: CoCoLex在五个法律基准测试中表现出色，尤其是在长篇生成任务中优于现有的上下文感知解码方法。

Abstract: Due to their ability to process long and complex contexts, LLMs can offer key
benefits to the Legal domain, but their adoption has been hindered by their
tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While
Retrieval-Augmented Generation offers a promising solution by grounding
generations in external knowledge, it offers no guarantee that the provided
context will be effectively integrated. To address this, context-aware decoding
strategies have been proposed to amplify the influence of relevant context, but
they usually do not explicitly enforce faithfulness to the context. In this
work, we introduce Confidence-guided Copy-based Decoding for Legal Text
Generation (CoCoLex)-a decoding strategy that dynamically interpolates the
model produced vocabulary distribution with a distribution derived based on
copying from the context. CoCoLex encourages direct copying based on the
model's confidence, ensuring greater fidelity to the source. Experimental
results on five legal benchmarks demonstrate that CoCoLex outperforms existing
context-aware decoding methods, particularly in long-form generation tasks.

</details>


### [34] [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)
*Guang Yang,Xinyang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于频率的不确定性量化方法，用于在黑盒设置下确保可证明的覆盖率保证。实验结果表明，该方法在区分正确和错误预测方面优于基于logit的PE，并且能够有效控制经验覆盖误差率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多项选择题回答（MCQA）中表现出显著进展，但其固有的不可靠性（如幻觉和过度自信）限制了其在高风险领域的应用。因此，需要一种可靠的方法来量化不确定性。

Method: 本文提出了一种基于频率的不确定性量化方法，通过多次独立采样模型的输出分布，并以最频繁的样本作为参考来计算预测熵（PE）。

Result: 实验评估显示，基于频率的PE在区分正确和错误预测方面优于基于logit的PE，并且该方法能够有效控制经验覆盖误差率，在用户指定的风险水平下验证了采样频率可以作为黑盒场景中基于logit的概率的可行替代方案。

Conclusion: 本文提出了一种基于频率的不确定性量化方法，可以在黑盒设置下利用符合预测（CP）来确保可证明的覆盖率保证。实验结果表明，基于频率的PE在区分正确和错误预测方面优于基于logit的PE，并且该方法能够有效控制经验覆盖误差率，验证了采样频率可以作为黑盒场景中基于logit的概率的可行替代方案。

Abstract: Large Language Models (LLMs) have shown remarkable progress in
multiple-choice question answering (MCQA), but their inherent unreliability,
such as hallucination and overconfidence, limits their application in high-risk
domains. To address this, we propose a frequency-based uncertainty
quantification method under black-box settings, leveraging conformal prediction
(CP) to ensure provable coverage guarantees. Our approach involves multiple
independent samplings of the model's output distribution for each input, with
the most frequent sample serving as a reference to calculate predictive entropy
(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,
MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms
logit-based PE in distinguishing between correct and incorrect predictions, as
measured by AUROC. Furthermore, the method effectively controls the empirical
miscoverage rate under user-specified risk levels, validating that sampling
frequency can serve as a viable substitute for logit-based probabilities in
black-box scenarios. This work provides a distribution-free model-agnostic
framework for reliable uncertainty quantification in MCQA with guaranteed
coverage, enhancing the trustworthiness of LLMs in practical applications.

</details>


### [35] [Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs](https://arxiv.org/abs/2508.05553)
*Franziska Weeber,Tanise Ceron,Sebastian Padó*

Main category: cs.CL

TL;DR: 研究发现，多语言大语言模型在不同语言之间可以转移政治观点，但实现显式的社会语言、文化和政治对齐仍然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 研究跨文化差异是否转化为多语言大语言模型中的跨语言差异，以及这些模型是否在不同语言中表现出独立的观点。

Method: 通过提示模型报告他们对投票建议应用程序中的政治陈述的（不）同意程度来评估MLLMs的政治观点。此外，在使用直接偏好优化和仅英语对齐数据对模型进行对齐前后对其进行了评估。

Result: 未对齐的模型在反映的政治观点中只有很少显著的跨语言差异。政治对齐几乎在所有五种语言中统一地改变了观点。

Conclusion: 在西方语言背景下，政治观点在不同语言之间可以转移，这表明了实现多语言大语言模型（MLLMs）的显式社会语言、文化和政治对齐的挑战。

Abstract: Public opinion surveys show cross-cultural differences in political opinions
between socio-cultural contexts. However, there is no clear evidence whether
these differences translate to cross-lingual differences in multilingual large
language models (MLLMs). We analyze whether opinions transfer between languages
or whether there are separate opinions for each language in MLLMs of various
sizes across five Western languages. We evaluate MLLMs' opinions by prompting
them to report their (dis)agreement with political statements from voting
advice applications. To better understand the interaction between languages in
the models, we evaluate them both before and after aligning them with more left
or right views using direct preference optimization and English alignment data
only. Our findings reveal that unaligned models show only very few significant
cross-lingual differences in the political opinions they reflect. The political
alignment shifts opinions almost uniformly across all five languages. We
conclude that in Western language contexts, political opinions transfer between
languages, demonstrating the challenges in achieving explicit socio-linguistic,
cultural, and political alignment of MLLMs.

</details>


### [36] [MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy](https://arxiv.org/abs/2508.05592)
*Shaoxiong Zhan,Yanlin Lai,Ziyu Lu,Dahua Lin,Ziqing Yang,Fei Tang*

Main category: cs.CL

TL;DR: MathSmith is a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. It constructs new problems from scratch, uses predefined strategies to increase difficulty, and employs reinforcement learning to optimize various aspects of problem quality. Experiments show that MathSmith outperforms existing baselines and has strong scalability and generalization.


<details>
  <summary>Details</summary>
Motivation: The advancement of large language models in mathematical reasoning is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods rely on transforming human-written templates, limiting diversity and scalability.

Method: MathSmith constructs new mathematical problems from scratch by randomly sampling concept-explanation pairs from PlanetMath. It uses nine predefined strategies as soft constraints during rationales and adopts reinforcement learning to optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity.

Result: Experiments across five benchmarks show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. A weakness-focused variant generation module enables targeted improvement on specific concepts.

Conclusion: MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities.

Abstract: Large language models have achieved substantial progress in mathematical
reasoning, yet their advancement is limited by the scarcity of high-quality,
high-difficulty training data. Existing synthesis methods largely rely on
transforming human-written templates, limiting both diversity and scalability.
We propose MathSmith, a novel framework for synthesizing challenging
mathematical problems to enhance LLM reasoning. Rather than modifying existing
problems, MathSmith constructs new ones from scratch by randomly sampling
concept-explanation pairs from PlanetMath, ensuring data independence and
avoiding contamination. To increase difficulty, we design nine predefined
strategies as soft constraints during rationales. We further adopts
reinforcement learning to jointly optimize structural validity, reasoning
complexity, and answer consistency. The length of the reasoning trace generated
under autoregressive prompting is used to reflect cognitive complexity,
encouraging the creation of more demanding problems aligned with
long-chain-of-thought reasoning. Experiments across five benchmarks,
categorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,
OlympiadBench), show that MathSmith consistently outperforms existing baselines
under both short and long CoT settings. Additionally, a weakness-focused
variant generation module enables targeted improvement on specific concepts.
Overall, MathSmith exhibits strong scalability, generalization, and
transferability, highlighting the promise of high-difficulty synthetic data in
advancing LLM reasoning capabilities.

</details>


### [37] [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)
*Haitao Hong,Yuchen Yan,Xingyu Wu,Guiyang Hou,Wenqi Zhang,Weiming Lu,Yongliang Shen,Jun Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种新的强化学习框架Cooper，通过联合优化策略模型和奖励模型，有效解决了奖励黑客问题，并提升了语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于规则的奖励和基于模型的奖励都存在局限性，需要一种更稳健的方法来提升语言模型的推理能力。

Method: 提出了一种联合优化策略模型和奖励模型的强化学习框架Cooper，引入了混合标注策略和基于参考的奖励建模范式，并训练了一个名为VerifyRM的奖励模型。

Result: Cooper不仅缓解了奖励黑客问题，还提高了端到端的强化学习性能，在Qwen2.5-1.5B-Instruct上平均准确率提高了0.54%。VerifyRM在VerifyBench上的准确性优于其他同规模模型。

Conclusion: 动态更新奖励模型是应对奖励黑客的有效方法，为更好地将奖励模型整合到强化学习中提供了参考。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
reasoning tasks, where reinforcement learning (RL) serves as a key algorithm
for enhancing their reasoning capabilities. Currently, there are two mainstream
reward paradigms: model-based rewards and rule-based rewards. However, both
approaches suffer from limitations: rule-based rewards lack robustness, while
model-based rewards are vulnerable to reward hacking. To address these issues,
we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework
that jointly optimizes both the policy model and the reward model. Cooper
leverages the high precision of rule-based rewards when identifying correct
responses, and dynamically constructs and selects positive-negative sample
pairs for continued training the reward model. This design enhances robustness
and mitigates the risk of reward hacking. To further support Cooper, we
introduce a hybrid annotation strategy that efficiently and accurately
generates training data for the reward model. We also propose a reference-based
reward modeling paradigm, where the reward model takes a reference answer as
input. Based on this design, we train a reward model named VerifyRM, which
achieves higher accuracy on VerifyBench compared to other models of the same
size. We conduct reinforcement learning using both VerifyRM and Cooper. Our
experiments show that Cooper not only alleviates reward hacking but also
improves end-to-end RL performance, for instance, achieving a 0.54% gain in
average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that
dynamically updating reward model is an effective way to combat reward hacking,
providing a reference for better integrating reward models into RL.

</details>


### [38] [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)
*Zixuan Wang,Dingming Li,Hongxing Li,Shuo Chen,Yuchen Yan,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 本文提出OmniEAR框架，用于评估语言模型在具身任务中的推理能力。结果显示，具身推理对当前模型来说是根本不同的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在抽象推理方面表现出色，但其在具身代理推理方面的能力仍需探索。现有的基准测试提供预定义的工具集或明确的合作指令，而OmniEAR要求代理动态获取能力并自主确定协调策略。

Method: 本文提出了OmniEAR框架，用于评估语言模型在具身任务中关于物理交互、工具使用和多智能体协调的推理能力。通过文本环境表示，建模了连续物理属性和复杂空间关系。

Result: 系统评估显示，当模型必须从约束中推理时，性能严重下降。虽然在明确指令下取得85-96%的成功率，但在工具推理和隐式协作中分别降至56-85%和63-85%。复合任务显示出超过50%的失败率。完全的环境信息会降低协调性能，表明模型无法过滤任务相关的约束。微调显著提高了单智能体任务（0.6%至76.3%），但对多智能体任务的提升有限（1.5%至5.5%）。

Conclusion: 本文展示了OmniEAR作为一个严格的基准，用于评估和推进具身AI系统。结果表明，具身推理对当前模型来说是根本不同的挑战。

Abstract: Large language models excel at abstract reasoning but their capacity for
embodied agent reasoning remains largely unexplored. We present OmniEAR, a
comprehensive framework for evaluating how language models reason about
physical interactions, tool usage, and multi-agent coordination in embodied
tasks. Unlike existing benchmarks that provide predefined tool sets or explicit
collaboration directives, OmniEAR requires agents to dynamically acquire
capabilities and autonomously determine coordination strategies based on task
demands. Through text-based environment representation, we model continuous
physical properties and complex spatial relationships across 1,500 scenarios
spanning household and industrial domains. Our systematic evaluation reveals
severe performance degradation when models must reason from constraints: while
achieving 85-96% success with explicit instructions, performance drops to
56-85% for tool reasoning and 63-85% for implicit collaboration, with compound
tasks showing over 50% failure rates. Surprisingly, complete environmental
information degrades coordination performance, indicating models cannot filter
task-relevant constraints. Fine-tuning improves single-agent tasks dramatically
(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing
fundamental architectural limitations. These findings demonstrate that embodied
reasoning poses fundamentally different challenges than current models can
address, establishing OmniEAR as a rigorous benchmark for evaluating and
advancing embodied AI systems. Our code and data are included in the
supplementary materials and will be open-sourced upon acceptance.

</details>


### [39] [Learning to Reason for Factuality](https://arxiv.org/abs/2508.05618)
*Xilun Chen,Ilia Kulikov,Vincent-Pierre Berges,Barlas Oğuz,Rulin Shao,Gargi Ghosh,Jason Weston,Wen-tau Yih*

Main category: cs.CL

TL;DR: 本文研究了如何通过在线强化学习提高大型语言模型在长格式事实任务中的事实准确性，提出了一个新的奖励函数以减少幻觉并提高回答的详细程度。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可靠的验证方法，将在线强化学习扩展到长格式事实设置面临诸多独特挑战。之前的工作已经利用自动事实评估框架（如FActScore）在离线强化学习设置中整理偏好数据，但我们发现直接将这些方法作为在线强化学习的奖励会导致奖励黑客行为，例如生成更少细节或相关性的响应。

Method: 我们提出了一种新的奖励函数，同时考虑了事实精确度、响应详细程度和答案相关性，并应用在线强化学习来学习高质量的事实推理。

Result: 在六个长格式事实基准测试中，我们的事实推理模型实现了幻觉率平均降低23.1个百分点，回答细节水平提高23%，并且整体响应有用性没有下降。

Conclusion: 我们的事实推理模型在六个长格式事实基准测试中实现了幻觉率平均降低23.1个百分点，回答细节水平提高23%，并且整体响应有用性没有下降。

Abstract: Reasoning Large Language Models (R-LLMs) have significantly advanced complex
reasoning tasks but often struggle with factuality, generating substantially
more hallucinations than their non-reasoning counterparts on long-form
factuality benchmarks. However, extending online Reinforcement Learning (RL), a
key component in recent R-LLM advancements, to the long-form factuality setting
poses several unique challenges due to the lack of reliable verification
methods. Previous work has utilized automatic factuality evaluation frameworks
such as FActScore to curate preference data in the offline RL setting, yet we
find that directly leveraging such methods as the reward in online RL leads to
reward hacking in multiple ways, such as producing less detailed or relevant
responses. We propose a novel reward function that simultaneously considers the
factual precision, response detail level, and answer relevance, and applies
online RL to learn high quality factual reasoning. Evaluated on six long-form
factuality benchmarks, our factual reasoning model achieves an average
reduction of 23.1 percentage points in hallucination rate, a 23% increase in
answer detail level, and no degradation in the overall response helpfulness.

</details>


### [40] [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)
*Brandon Jaipersaud,David Krueger,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本文研究了如何利用线性探针分析大型语言模型中的说服动态，并展示了其在多个方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）开始展示说服人类的能力，但我们对这一动态的理解仍然有限。因此，本文旨在通过探针研究说服动态。

Method: 本文利用认知科学的见解，训练探针以研究说服动态的不同方面，包括说服的成功、被说服者的个性和说服策略。

Result: 实验结果表明，尽管探针简单，但它们能够在样本和数据集层面捕捉各种说服方面。例如，探针可以识别被说服者被说服的时刻或整个数据集中说服成功发生的时刻。此外，探针在某些设置中表现良好甚至优于基于提示的方法。

Conclusion: 本文表明，线性探针可以作为一种可行的方法来研究复杂的互动行为，如说服、欺骗和操纵，特别是在多轮对话和大规模数据集分析中。

Abstract: Large Language Models (LLMs) have started to demonstrate the ability to
persuade humans, yet our understanding of how this dynamic transpires is
limited. Recent work has used linear probes, lightweight tools for analyzing
model representations, to study various LLM skills such as the ability to model
user sentiment and political perspective. Motivated by this, we apply probes to
study persuasion dynamics in natural, multi-turn conversations. We leverage
insights from cognitive science to train probes on distinct aspects of
persuasion: persuasion success, persuadee personality, and persuasion strategy.
Despite their simplicity, we show that they capture various aspects of
persuasion at both the sample and dataset levels. For instance, probes can
identify the point in a conversation where the persuadee was persuaded or where
persuasive success generally occurs across the entire dataset. We also show
that in addition to being faster than expensive prompting-based approaches,
probes can do just as well and even outperform prompting in some settings, such
as when uncovering persuasion strategy. This suggests probes as a plausible
avenue for studying other complex behaviours such as deception and
manipulation, especially in multi-turn settings and large-scale dataset
analysis where prompting-based methods would be computationally inefficient.

</details>


### [41] [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)
*Mehrdad Zakershahrak,Samira Ghodratnama*

Main category: cs.CL

TL;DR: 本文提出H-NET++，一种层次化动态分块模型，用于字节级语言模型，以解决形态丰富语言中的计算挑战。该模型在波斯语语料库上表现出色，实现了显著的性能提升和计算效率。


<details>
  <summary>Details</summary>
Motivation: 字节级语言模型消除了脆弱的分词器，但在形态丰富的语言（MRLs）中面临计算挑战，其中单词跨越许多字节。

Method: H-NET++，一种分层动态分块模型，通过端到端训练学习语言信息分块。关键创新包括：(1) 轻量级Transformer上下文混合器（1.9M参数）用于跨块注意力，(2) 用于文档级一致性的两级潜在超先验，(3) 对正字法伪影（如波斯ZWNJ）的专门处理，以及(4) 基于课程的训练，具有阶段序列长度。

Result: 在1.4B标记的波斯语语料库上，H-NET++取得了最先进的结果：相对于基于BPE的GPT-2-fa，BPB减少0.159（12%更好的压缩），ParsGLUE得分为5.4pp，对ZWNJ损坏的鲁棒性提高53%，黄金形态边界F1得分为73.8%。

Conclusion: 我们的学习块与波斯语法对齐，无需显式监督，证明了层次化动态分块为MRL提供了有效的无分词器解决方案，同时保持计算效率。

Abstract: Byte-level language models eliminate fragile tokenizers but face
computational challenges in morphologically-rich languages (MRLs), where words
span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that
learns linguistically-informed segmentation through end-to-end training. Key
innovations include: (1) a lightweight Transformer context-mixer (1.9M
parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for
document-level consistency, (3) specialized handling of orthographic artifacts
(e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence
lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art
results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better
compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ
corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks
align with Persian morphology without explicit supervision, demonstrating that
hierarchical dynamic chunking provides an effective tokenizer-free solution for
MRLs while maintaining computational efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
*Chitranshu Harbola,Anupam Purwar*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型的预测性维护智能系统，通过结合轴承振动频率分析和多代理生成，提供可操作的维护建议，有效提升了工业维护的智能化水平。


<details>
  <summary>Details</summary>
Motivation: 工业机械维护需要及时干预以防止灾难性故障并优化运行效率，传统异常检测方法无法提供可行的维护建议，因此需要一种能够提供可操作维护建议的智能系统。

Method: 该系统结合了轴承振动频率分析和多代理生成，将轴承振动数据（BPFO、BPFI、BSF、FTF频率）序列化为自然语言以供大型语言模型处理，同时利用向量嵌入和语义搜索处理维护手册，并进行网络搜索以获取全面的程序知识和最新的维护实践。

Result: 实验验证表明，该系统在轴承振动数据集中实现了有效的异常检测和上下文相关的维护指导，成功填补了状态监测与可行维护计划之间的差距，为工业从业者提供了智能决策支持。

Conclusion: 该工作推进了大型语言模型在工业维护中的应用，提供了一个可扩展的预测性维护框架，适用于各种机械部件和工业领域。

Abstract: Industrial machinery maintenance requires timely intervention to prevent
catastrophic failures and optimize operational efficiency. This paper presents
an integrated Large Language Model (LLM)-based intelligent system for
prescriptive maintenance that extends beyond traditional anomaly detection to
provide actionable maintenance recommendations. Building upon our prior LAMP
framework for numerical data analysis, we develop a comprehensive solution that
combines bearing vibration frequency analysis with multi agentic generation for
intelligent maintenance planning. Our approach serializes bearing vibration
data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM
processing, enabling few-shot anomaly detection with high accuracy. The system
classifies fault types (inner race, outer race, ball/roller, cage faults) and
assesses severity levels. A multi-agentic component processes maintenance
manuals using vector embeddings and semantic search, while also conducting web
searches to retrieve comprehensive procedural knowledge and access up-to-date
maintenance practices for more accurate and in-depth recommendations. The
Gemini model then generates structured maintenance recommendations includes
immediate actions, inspection checklists, corrective measures, parts
requirements, and timeline specifications. Experimental validation in bearing
vibration datasets demonstrates effective anomaly detection and contextually
relevant maintenance guidance. The system successfully bridges the gap between
condition monitoring and actionable maintenance planning, providing industrial
practitioners with intelligent decision support. This work advances the
application of LLMs in industrial maintenance, offering a scalable framework
for prescriptive maintenance across machinery components and industrial
sectors.

</details>


### [43] [Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)](https://arxiv.org/abs/2508.04846)
*Mahdi Nazari Ashani,Ali Asghar Alesheikh,Saba Kazemi,Kimya Kheirkhah,Yasin Mohammadi,Fatemeh Rezaie,Amir Mahdi Manafi,Hedieh Zarkesh*

Main category: cs.AI

TL;DR: 本研究比较了三种实现AWebGIS的方法，其中基于客户端浏览器执行的微调小语言模型（SLM）方法表现最佳，具有高准确性和隐私保护优势。


<details>
  <summary>Details</summary>
Motivation: 当前大多数解决方案依赖于基于云的大语言模型（LLMs），这需要持续的互联网访问，并由于集中式服务器处理而引发用户的隐私和可扩展性问题。

Method: 本研究比较了三种实现AWebGIS的方法：(1) 使用基于云的LLMs（如Cohere）的完全自动化在线方法；(2) 使用经典机器学习分类器（如支持向量机和随机森林）的半自动化离线方法；(3) 基于在客户端浏览器中执行的微调小语言模型（SLM）的完全自主离线方法，具体为T5-small模型。

Result: 第三种方法，即利用SLMs的方法，在所有方法中达到了最高的准确性，包括精确匹配准确率为0.93，Levenshtein相似度为0.99，以及用于摘要评估的ROUGE-1和ROUGE-L得分为0.98。这种客户端计算策略通过将处理任务卸载到用户设备上，减少了后端服务器的负载，消除了对基于服务器的推理的需求。

Conclusion: 这些结果突显了可在浏览器中执行的模型对于AWebGIS解决方案的可行性。

Abstract: Autonomous web-based geographical information systems (AWebGIS) aim to
perform geospatial operations from natural language input, providing intuitive,
intelligent, and hands-free interaction. However, most current solutions rely
on cloud-based large language models (LLMs), which require continuous internet
access and raise users' privacy and scalability issues due to centralized
server processing. This study compares three approaches to enabling AWebGIS:
(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)
a semi-automated offline method using classical machine learning classifiers
such as support vector machine and random forest; and (3) a fully autonomous
offline (client-side) method based on a fine-tuned small language model (SLM),
specifically T5-small model, executed in the client's web browser. The third
approach, which leverages SLMs, achieved the highest accuracy among all
methods, with an exact matching accuracy of 0.93, Levenshtein similarity of
0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L
scores of 0.98. Crucially, this client-side computation strategy reduces the
load on backend servers by offloading processing to the user's device,
eliminating the need for server-based inference. These results highlight the
feasibility of browser-executable models for AWebGIS solutions.

</details>


### [44] [ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis](https://arxiv.org/abs/2508.04915)
*Huiya Zhao,Yinghao Zhu,Zixiang Wang,Yasha Wang,Junyi Gao,Liantao Ma*

Main category: cs.AI

TL;DR: 本文介绍了一种自进化AI代理HealthFlow，能够通过元级进化机制改进其高层问题解决策略，并在复杂的健康数据分析任务中表现出色，标志着AI研究方向从构建工具使用者向设计自进化任务管理者的重要转变。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在医疗研究中的有效性受到其对静态、预定义策略的依赖的限制，导致它们无法学习成为更好的战略规划者，而这是医疗等复杂领域所需的关键技能。

Method: 本文引入了HealthFlow，一种通过新颖的元级进化机制自适应改进其高层问题解决策略的自进化AI代理。同时，提出了EHRFlowBench基准，用于评估HealthFlow在复杂、现实的健康数据分析任务中的性能。

Result: 实验结果表明，HealthFlow的自进化方法在复杂、现实的健康数据分析任务中显著优于最先进的代理框架。

Conclusion: 本文提出了一种自进化的AI代理HealthFlow，通过新颖的元级进化机制克服了传统AI代理在医疗研究中的局限性。实验结果表明，HealthFlow的自进化方法显著优于最先进的代理框架，标志着从构建更好的工具使用者向设计更智能、自进化的任务管理者的重要转变，为科学发现中的更自主和有效的AI铺平了道路。

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [45] [Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses](https://arxiv.org/abs/2508.05009)
*Bin Han,Robert Wolfe,Anat Caspi,Bill Howe*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在增强领域专家整合大型、异构和噪声城市空间数据集方面的应用。研究发现，LLMs在提供相关特征的情况下能够生成高性能的结果，并通过审查和修改方法有效纠正错误的初始响应。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的集成方法无法覆盖所有边缘情况，而机器学习方法需要收集和标记大量特定任务的样本。因此，我们需要一种更有效的方法来集成大型、异构和噪声城市空间数据集。

Method: 我们分析了LLMs如何通过人类经验来推理环境空间关系，并采用审查和修改的方法来纠正错误的初始响应。

Result: 虽然LLMs表现出空间推理能力，但它们难以将宏观环境与相关的计算几何任务联系起来，常常产生逻辑不连贯的响应。然而，当提供相关特征时，LLMs能够生成高性能的结果。

Conclusion: 我们的研究结果表明，LLMs可以作为传统基于规则的启发式方法的有前景且灵活的替代方案，推动自适应空间数据集成的能力。

Abstract: We explore the application of large language models (LLMs) to empower domain
experts in integrating large, heterogeneous, and noisy urban spatial datasets.
Traditional rule-based integration methods are unable to cover all edge cases,
requiring manual verification and repair. Machine learning approaches require
collecting and labeling of large numbers of task-specific samples. In this
study, we investigate the potential of LLMs for spatial data integration. Our
analysis first considers how LLMs reason about environmental spatial
relationships mediated by human experience, such as between roads and
sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they
struggle to connect the macro-scale environment with the relevant computational
geometry tasks, often producing logically incoherent responses. But when
provided relevant features, thereby reducing dependence on spatial reasoning,
LLMs are able to generate high-performing results. We then adapt a
review-and-refine method, which proves remarkably effective in correcting
erroneous initial responses while preserving accurate responses. We discuss
practical implications of employing LLMs for spatial data integration in
real-world contexts and outline future research directions, including
post-training, multi-modal integration methods, and support for diverse data
formats. Our findings position LLMs as a promising and flexible alternative to
traditional rule-based heuristics, advancing the capabilities of adaptive
spatial data integration.

</details>


### [46] [Cognitive Duality for Adaptive Web Agents](https://arxiv.org/abs/2508.05081)
*Jiarun Liu,Chunhong Zhang,Zheng Hu*

Main category: cs.AI

TL;DR: This paper proposes CogniWeb, an agent architecture inspired by human cognition, which effectively combines fast intuitive processing with deliberate reasoning for efficient web navigation.


<details>
  <summary>Details</summary>
Motivation: Current approaches to building autonomous web agents either focus on offline imitation learning or online exploration, but rarely integrate both paradigms effectively.

Method: We derive a principled decomposition into fast System 1 and slow System 2 cognitive processes and implement this framework in CogniWeb, a modular agent architecture that adaptively toggles between fast intuitive processing and deliberate reasoning.

Result: CogniWeb achieves a 43.96% success rate while reducing token usage by 75%.

Conclusion: CogniWeb achieves competitive performance while maintaining higher efficiency.

Abstract: Web navigation represents a critical and challenging domain for evaluating
artificial general intelligence (AGI), demanding complex decision-making within
high-entropy, dynamic environments with combinatorially explosive action
spaces. Current approaches to building autonomous web agents either focus on
offline imitation learning or online exploration, but rarely integrate both
paradigms effectively. Inspired by the dual-process theory of human cognition,
we derive a principled decomposition into fast System 1 and slow System 2
cognitive processes. This decomposition provides a unifying perspective on
existing web agent methodologies, bridging the gap between offline learning of
intuitive reactive behaviors and online acquisition of deliberative planning
capabilities. We implement this framework in CogniWeb, a modular agent
architecture that adaptively toggles between fast intuitive processing and
deliberate reasoning based on task complexity. Our evaluation on WebArena
demonstrates that CogniWeb achieves competitive performance (43.96% success
rate) while maintaining significantly higher efficiency (75% reduction in token
usage).

</details>


### [47] [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
*Zhuohang Jiang,Pangjing Wu,Xu Yuan,Wenqi Fan,Qing Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为QA-Dragon的查询感知动态RAG系统，用于知识密集型视觉问答任务。该系统通过引入领域路由器和搜索路由器，实现了文本和图像的多模态、多轮和多跳推理，有效提升了模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法通常单独从文本或图像中检索，限制了它们解决需要多跳推理或多源事实知识的复杂查询的能力。

Method: 我们提出了QA-Dragon，这是一个查询感知的动态RAG系统，用于知识密集型VQA。它引入了一个领域路由器来识别查询的主题领域，并引入了一个搜索路由器来动态选择最佳检索策略。通过在混合设置中协调文本和图像搜索代理，我们的系统支持多模态、多轮和多跳推理。

Result: 我们在KDD Cup 2025的Meta CRAG-MM挑战赛上评估了我们的QA-Dragon，结果表明它显著提高了基础模型在具有挑战性场景下的推理性能。框架在答案准确率和知识重叠分数上都有显著提升，单源任务、多源任务和多轮任务分别优于基线模型5.06%、6.35%和5.03%。

Conclusion: 我们的框架在答案准确率和知识重叠分数上都取得了显著的提升，优于基线模型。

Abstract: Retrieval-Augmented Generation (RAG) has been introduced to mitigate
hallucinations in Multimodal Large Language Models (MLLMs) by incorporating
external knowledge into the generation process, and it has become a widely
adopted approach for knowledge-intensive Visual Question Answering (VQA).
However, existing RAG methods typically retrieve from either text or images in
isolation, limiting their ability to address complex queries that require
multi-hop reasoning or up-to-date factual knowledge. To address this
limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for
Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to
identify the query's subject domain for domain-specific reasoning, along with a
search router that dynamically selects optimal retrieval strategies. By
orchestrating both text and image search agents in a hybrid setup, our system
supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle
complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM
Challenge at KDD Cup 2025, where it significantly enhances the reasoning
performance of base models under challenging scenarios. Our framework achieves
substantial improvements in both answer accuracy and knowledge overlap scores,
outperforming baselines by 5.06% on the single-source task, 6.35% on the
multi-source task, and 5.03% on the multi-turn task.

</details>


### [48] [A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents](https://arxiv.org/abs/2508.05311)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 本文提出了一种混合架构，结合了决策树的符号推理和LLM的生成能力，实现了更强大的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将符号和神经模块松散地耦合，而本文旨在通过嵌入决策树和随机森林作为可调用的预言机，实现一个统一的推理系统。

Method: 提出了一种混合架构，将基于决策树的符号推理与大型语言模型（LLMs）的生成能力整合在一个协调的多智能体框架中。

Result: 该系统在推理基准测试中表现出色，在ProofWriter上通过逻辑基础的树验证提高了7.2%的蕴含一致性，在GSM8k上通过符号增强提高了5.3%的多步骤数学问题准确率，在ARC上通过集成符号预言机提高了6.0%的抽象准确率。

Conclusion: 该架构提供了一种强大、可解释且可扩展的通用神经符号推理解决方案。

Abstract: We propose a hybrid architecture that integrates decision tree-based symbolic
reasoning with the generative capabilities of large language models (LLMs)
within a coordinated multi-agent framework. Unlike prior approaches that
loosely couple symbolic and neural modules, our design embeds decision trees
and random forests as callable oracles within a unified reasoning system.
Tree-based modules enable interpretable rule inference and causal logic, while
LLM agents handle abductive reasoning, generalization, and interactive
planning. A central orchestrator maintains belief state consistency and
mediates communication across agents and external tools, enabling reasoning
over both structured and unstructured inputs.
  The system achieves strong performance on reasoning benchmarks. On
\textit{ProofWriter}, it improves entailment consistency by +7.2\% through
logic-grounded tree validation. On GSM8k, it achieves +5.3\% accuracy gains in
multistep mathematical problems via symbolic augmentation. On \textit{ARC}, it
boosts abstraction accuracy by +6.0\% through integration of symbolic oracles.
Applications in clinical decision support and scientific discovery show how the
system encodes domain rules symbolically while leveraging LLMs for contextual
inference and hypothesis generation. This architecture offers a robust,
interpretable, and extensible solution for general-purpose neuro-symbolic
reasoning.

</details>


### [49] [Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?](https://arxiv.org/abs/2508.05464)
*Matteo Prandi,Vincenzo Suriani,Federico Pierucci,Marcello Galisai,Daniele Nardi,Piercosma Bisconti*

Main category: cs.AI

TL;DR: 本研究揭示了当前AI评估体系与新兴监管要求之间的重大差距，强调了系统性风险评估的不足，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前的AI评估实践主要依赖于已建立的基准测试，但这些工具并未设计用于衡量新监管环境关注的系统性风险，因此需要量化这种“基准-法规差距”。

Method: 引入了Bench-2-CoP框架，使用经过验证的LLM-as-judge分析，将广泛使用的基准测试中的194,955个问题映射到欧盟AI法案中的模型能力和倾向分类法。

Result: 评估生态系统主要集中于少数行为倾向，如“幻觉倾向”（占整个语料库的53.7%）和“歧视性偏见”（28.9%），而关键的功能能力被严重忽视。对于失控场景相关的能力，如逃避人类监督、自我复制和自主AI开发，在整个基准语料库中完全没有覆盖。

Conclusion: 本研究提供了对这一差距的首次全面、定量分析，为政策制定者改进CoP和开发者构建下一代评估工具提供了关键见解，最终促进了更安全和合规的人工智能。

Abstract: The rapid advancement of General Purpose AI (GPAI) models necessitates robust
evaluation frameworks, especially with emerging regulations like the EU AI Act
and its associated Code of Practice (CoP). Current AI evaluation practices
depend heavily on established benchmarks, but these tools were not designed to
measure the systemic risks that are the focus of the new regulatory landscape.
This research addresses the urgent need to quantify this "benchmark-regulation
gap." We introduce Bench-2-CoP, a novel, systematic framework that uses
validated LLM-as-judge analysis to map the coverage of 194,955 questions from
widely-used benchmarks against the EU AI Act's taxonomy of model capabilities
and propensities. Our findings reveal a profound misalignment: the evaluation
ecosystem is overwhelmingly focused on a narrow set of behavioral propensities,
such as "Tendency to hallucinate" (53.7% of the corpus) and "Discriminatory
bias" (28.9%), while critical functional capabilities are dangerously
neglected. Crucially, capabilities central to loss-of-control scenarios,
including evading human oversight, self-replication, and autonomous AI
development, receive zero coverage in the entire benchmark corpus. This
translates to a near-total evaluation gap for systemic risks like "Loss of
Control" (0.4% coverage) and "Cyber Offence" (0.8% coverage). This study
provides the first comprehensive, quantitative analysis of this gap, offering
critical insights for policymakers to refine the CoP and for developers to
build the next generation of evaluation tools, ultimately fostering safer and
more compliant AI.

</details>


### [50] [Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?](https://arxiv.org/abs/2508.05474)
*Burak Can Kaplan,Hugo Cesar De Castro Carneiro,Stefan Wermter*

Main category: cs.AI

TL;DR: 本文提出了一种利用小而资源高效的通用LLM来合成ERC数据集的方法，以补充现有的ERC基准测试。实验结果显示，使用这些生成的数据集训练的模型在ERC任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: ERC数据仍然稀缺，现有数据集由于来源高度偏倚和软标签的固有主观性而面临诸多挑战。尽管大型语言模型（LLMs）在许多情感任务中表现出色，但它们通常训练成本高昂，且在ERC任务中的应用——特别是在数据生成方面——仍然有限。

Method: 我们使用一个小而资源高效的通用LLM来合成具有多样特性的ERC数据集，补充了三个最常用的ERC基准测试。我们生成了六个新的数据集，其中两个专门用于增强每个基准测试。

Result: 实验结果表明，在生成的数据集上训练的ERC分类器模型表现出强大的鲁棒性，并且在现有的ERC基准测试中 consistently 获得统计显著的性能提升。

Conclusion: 实验结果表明，在生成的数据集上训练的ERC分类器模型表现出强大的鲁棒性，并且在现有的ERC基准测试中 consistently 获得统计显著的性能提升。

Abstract: Emotion recognition in conversations (ERC) focuses on identifying emotion
shifts within interactions, representing a significant step toward advancing
machine intelligence. However, ERC data remains scarce, and existing datasets
face numerous challenges due to their highly biased sources and the inherent
subjectivity of soft labels. Even though Large Language Models (LLMs) have
demonstrated their quality in many affective tasks, they are typically
expensive to train, and their application to ERC tasks--particularly in data
generation--remains limited. To address these challenges, we employ a small,
resource-efficient, and general-purpose LLM to synthesize ERC datasets with
diverse properties, supplementing the three most widely used ERC benchmarks. We
generate six novel datasets, with two tailored to enhance each benchmark. We
evaluate the utility of these datasets to (1) supplement existing datasets for
ERC classification, and (2) analyze the effects of label imbalance in ERC. Our
experimental results indicate that ERC classifier models trained on the
generated datasets exhibit strong robustness and consistently achieve
statistically significant performance improvements on existing ERC benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [51] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文提出了一种统一框架，用于在强化学习中结合推理过程的质量，通过引入新的基准和方法，显著提升了代码生成任务的性能，并展示了方法的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法依赖于基于结果的奖励，忽视了中间推理过程的质量。直接监督推理过程容易受到奖励黑客的影响，因此需要一种更有效的框架来提升推理质量。

Method: 本文提出了LCB-RB基准、OD-based方法和Posterior-GRPO (P-GRPO) 强化学习方法，以评估和优化推理过程的质量，并减轻奖励黑客问题。

Result: 使用7B参数的奖励模型在LCB-RB上取得了最先进的性能，并在其他基准上表现出良好的泛化能力。采用P-GRPO的7B参数模型在多种代码生成任务中表现优越，优于仅基于结果的基线，性能与GPT-4-Turbo相当。

Conclusion: 本文提出了一种统一框架，可以有效地在强化学习中结合推理过程的质量。通过引入LCB-RB基准和OD-based方法，以及P-GRPO强化学习方法，显著提升了代码生成任务的性能，并在数学任务中展示了方法的通用性。

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Advancing Hate Speech Detection with Transformers: Insights from the MetaHate](https://arxiv.org/abs/2508.04913)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本研究评估了基于Transformer的模型在仇恨言论检测中的表现，发现微调后的ELECTRA效果最佳，但也面临讽刺、编码语言和标签噪声等问题。


<details>
  <summary>Details</summary>
Motivation: 由于仇恨言论在社交媒体上的广泛传播及其对社会、心理和身体的严重影响，需要开发更强大的自动化方法来检测仇恨言论。传统深度学习方法（如RNN、LSTM和CNN）存在长期依赖性和并行化效率低的问题，因此探索更有效的模型成为必要。

Method: 本研究使用MetaHate数据集（包含120万条社交媒体样本）对多种最先进的Transformer模型（如BERT、RoBERTa、GPT-2和ELECTRA）进行了评估，并通过微调优化了模型性能。

Result: 实验结果显示，微调后的ELECTRA在F1分数上达到了0.8980，表现最佳。同时，研究分析了分类错误，发现讽刺、编码语言和标签噪声是主要挑战。

Conclusion: 本研究展示了基于Transformer的模型在仇恨言论检测中的有效性，特别是微调后的ELECTRA在MetaHate数据集上表现出最佳性能。然而，研究也揭示了讽刺、编码语言和标签噪声等挑战。

Abstract: Hate speech is a widespread and harmful form of online discourse,
encompassing slurs and defamatory posts that can have serious social,
psychological, and sometimes physical impacts on targeted individuals and
communities. As social media platforms such as X (formerly Twitter), Facebook,
Instagram, Reddit, and others continue to facilitate widespread communication,
they also become breeding grounds for hate speech, which has increasingly been
linked to real-world hate crimes. Addressing this issue requires the
development of robust automated methods to detect hate speech in diverse social
media environments. Deep learning approaches, such as vanilla recurrent neural
networks (RNNs), long short-term memory (LSTM), and convolutional neural
networks (CNNs), have achieved good results, but are often limited by issues
such as long-term dependencies and inefficient parallelization. This study
represents the comprehensive exploration of transformer-based models for hate
speech detection using the MetaHate dataset--a meta-collection of 36 datasets
with 1.2 million social media samples. We evaluate multiple state-of-the-art
transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with
fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We
also analyze classification errors, revealing challenges with sarcasm, coded
language, and label noise.

</details>


### [53] [REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2508.04946)
*Nameer Hirschkind,Joseph Liu,Mahesh Kumar Nandwana,Xiao Yu*

Main category: cs.LG

TL;DR: This paper introduces REINA, a novel loss function for SimulST systems that optimizes the tradeoff between translation quality and latency, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Simultaneous Speech Translation (SimulST) systems face the challenge of balancing translation quality and latency. The goal is to optimize this tradeoff by waiting for more input only if gaining information by doing so.

Method: We introduce Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. REINA is derived from information theory principles.

Result: REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores. We achieve state-of-the-art streaming results for models of comparable size.

Conclusion: REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we achieve state-of-the-art streaming results for models of comparable size.

Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while
simultaneously emitting translated text or speech. Such systems face the
significant challenge of balancing translation quality and latency. We
introduce a strategy to optimize this tradeoff: wait for more input only if you
gain information by doing so. Based on this strategy, we present Regularized
Entropy INformation Adaptation (REINA), a novel loss to train an adaptive
policy using an existing non-streaming translation model. We derive REINA from
information theory principles and show that REINA helps push the reported
Pareto frontier of the latency/quality tradeoff over prior works. Utilizing
REINA, we train a SimulST model on French, Spanish and German, both from and
into English. Training on only open source or synthetically generated data, we
achieve state-of-the-art (SOTA) streaming results for models of comparable
size. We also introduce a metric for streaming efficiency, quantitatively
showing REINA improves the latency/quality trade-off by as much as 21% compared
to prior approaches, normalized against non-streaming baseline BLEU scores.

</details>


### [54] [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004)
*Chengsong Huang,Wenhao Yu,Xiaoyang Wang,Hongming Zhang,Zongxia Li,Ruosen Li,Jiaxin Huang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: R-Zero是一种完全自主的框架，能够自动生成训练数据并提升LLM的推理能力，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有的训练方法仍然严重依赖于大量人工标注的任务和标签，这限制了AI系统向超越人类智能的能力发展。

Method: R-Zero是一个完全自主的框架，从零开始生成自己的训练数据。它从一个基础LLM开始，初始化两个具有不同角色的独立模型，即Challenger和Solver。Challenger被奖励提出接近Solver能力边缘的任务，而Solver被奖励解决由Challenger提出的越来越具有挑战性的任务。

Result: R-Zero通过自主生成训练数据，实现了无需预设任务和标签的目标，显著提升了不同基础LLM的推理能力。

Conclusion: R-Zero显著提升了不同基础LLM的推理能力，例如将Qwen3-4B-Base在数学推理基准上提高了6.49，在通用领域推理基准上提高了7.54。

Abstract: Self-evolving Large Language Models (LLMs) offer a scalable path toward
super-intelligence by autonomously generating, refining, and learning from
their own experiences. However, existing methods for training such models still
rely heavily on vast human-curated tasks and labels, typically via fine-tuning
or reinforcement learning, which poses a fundamental bottleneck to advancing AI
systems toward capabilities beyond human intelligence. To overcome this
limitation, we introduce R-Zero, a fully autonomous framework that generates
its own training data from scratch. Starting from a single base LLM, R-Zero
initializes two independent models with distinct roles, a Challenger and a
Solver. These models are optimized separately and co-evolve through
interaction: the Challenger is rewarded for proposing tasks near the edge of
the Solver capability, and the Solver is rewarded for solving increasingly
challenging tasks posed by the Challenger. This process yields a targeted,
self-improving curriculum without any pre-existing tasks and labels.
Empirically, R-Zero substantially improves reasoning capability across
different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on
math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.

</details>


### [55] [Exploring Superior Function Calls via Reinforcement Learning](https://arxiv.org/abs/2508.05118)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Yicheng Chen,Cunyin Peng,Jinjie GU,Chenyi Zhuang*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习框架，通过战略熵基探索来增强群体相对策略优化，以解决函数调用中的三个关键挑战。实验结果表明，该框架在伯克利函数调用排行榜上实现了最先进的性能，并且在代码预训练模型上表现出显著的改进。


<details>
  <summary>Details</summary>
Motivation: 当前的训练方法无法开发出稳健的推理策略，监督微调模型依赖于表面模式匹配，而标准的强化学习方法在处理结构化函数调用的复杂动作空间时存在困难。因此，需要一种新的强化学习框架来解决这些问题。

Method: 本文提出了一种两阶段的数据准备管道，通过迭代的LLM评估和抽象语法树验证来确保高质量的训练样本。同时，采用了一种基于战略熵的探索方法，以增强群体相对策略优化。

Result: 实验结果表明，该框架在伯克利函数调用排行榜上实现了最先进的性能，总体准确率达到86.02%，在复杂的多函数场景中比标准GRPO高出最多6%。此外，该方法在代码预训练模型上表现出显著的改进。

Conclusion: 本文提出了一种新的强化学习框架，通过战略熵基探索来增强群体相对策略优化，以解决函数调用中的三个关键挑战。实验结果表明，该框架在伯克利函数调用排行榜上实现了最先进的性能，并且在代码预训练模型上表现出显著的改进。

Abstract: Function calling capabilities are crucial for deploying Large Language Models
in real-world applications, yet current training approaches fail to develop
robust reasoning strategies. Supervised fine-tuning produces models that rely
on superficial pattern matching, while standard reinforcement learning methods
struggle with the complex action space of structured function calls. We present
a novel reinforcement learning framework designed to enhance group relative
policy optimization through strategic entropy based exploration specifically
tailored for function calling tasks. Our approach addresses three critical
challenges in function calling: insufficient exploration during policy
learning, lack of structured reasoning in chain-of-thought generation, and
inadequate verification of parameter extraction. Our two-stage data preparation
pipeline ensures high-quality training samples through iterative LLM evaluation
and abstract syntax tree validation. Extensive experiments on the Berkeley
Function Calling Leaderboard demonstrate that this framework achieves
state-of-the-art performance among open-source models with 86.02\% overall
accuracy, outperforming standard GRPO by up to 6\% on complex multi-function
scenarios. Notably, our method shows particularly strong improvements on
code-pretrained models, suggesting that structured language generation
capabilities provide an advantageous starting point for reinforcement learning
in function calling tasks. We will release all the code, models and dataset to
benefit the community.

</details>


### [56] [Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models](https://arxiv.org/abs/2508.05165)
*Mason Nakamura,Saaduddin Mahmud,Kyle H. Wray,Hamed Zamani,Shlomo Zilberstein*

Main category: cs.LG

TL;DR: HIA is a tuning-free approach that balances alignment quality and computational cost for LLMs, outperforming existing methods in multi-objective tasks with fewer inference calls.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with user preferences is crucial for real-world use but often requires costly fine-tuning or expensive inference, forcing trade-offs between alignment quality and computational cost.

Method: HIA (Heuristic-Guided Inference-time Alignment) is a tuning-free, black-box-compatible approach that uses a lightweight prompt optimizer, heuristic reward models, and two-stage filtering.

Result: On real-world prompt datasets, HIA outperforms best-of-N sampling, beam search, and greedy search baselines in multi-objective, goal-conditioned tasks under the same inference budget. It is also effective under low-inference budgets with as little as one or two response queries.

Conclusion: HIA provides a practical solution for scalable, personalized LLM deployment by effectively balancing alignment quality and computational cost.

Abstract: Aligning LLMs with user preferences is crucial for real-world use but often
requires costly fine-tuning or expensive inference, forcing trade-offs between
alignment quality and computational cost. Existing inference-time methods
typically ignore this balance, focusing solely on the optimized policy's
performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a
tuning-free, black-box-compatible approach that uses a lightweight prompt
optimizer, heuristic reward models, and two-stage filtering to reduce inference
calls while preserving alignment quality. On real-world prompt datasets,
HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and
greedy search baselines in multi-objective, goal-conditioned tasks under the
same inference budget. We also find that HIA is effective under low-inference
budgets with as little as one or two response queries, offering a practical
solution for scalable, personalized LLM deployment.

</details>


### [57] [FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance](https://arxiv.org/abs/2508.05201)
*Mengao Zhang,Jiayu Fu,Tanya Warrier,Yuwen Wang,Tianhui Tan,Ke-wei Huang*

Main category: cs.LG

TL;DR: 本文提出了一种评估金融LLMs中内在幻觉的框架，包括一个新颖的数据集创建方法、一个来自标准普尔500年报告的新评估数据集，以及对先进LLMs的全面评估，旨在提高金融生成式AI系统的可信度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 准确提取和精确计算表格数据对于可靠的财务分析至关重要，因为即使是最小的数值错误也会破坏决策和监管合规性。金融应用有独特的要求，通常依赖于上下文相关的、数值的和专有的表格数据，而现有的幻觉基准很少能捕捉到这些数据。

Method: 我们开发了一个严格且可扩展的框架，用于评估金融LLMs中的内在幻觉，将其概念化为在真实世界金融文档上的上下文感知掩码跨度预测任务。

Result: 我们提出了一个新颖的自动化数据集创建范式，使用掩码策略；一个新的从标准普尔500年报告中派生的幻觉评估数据集；以及对最先进的LLMs在金融表格数据上的内在幻觉模式的全面评估。

Conclusion: 我们的工作提供了一种稳健的方法论用于内部LLM评估，并为构建更值得信赖和可靠的金融生成式AI系统迈出了关键一步。

Abstract: Hallucination remains a critical challenge for deploying Large Language
Models (LLMs) in finance. Accurate extraction and precise calculation from
tabular data are essential for reliable financial analysis, since even minor
numerical errors can undermine decision-making and regulatory compliance.
Financial applications have unique requirements, often relying on
context-dependent, numerical, and proprietary tabular data that existing
hallucination benchmarks rarely capture. In this study, we develop a rigorous
and scalable framework for evaluating intrinsic hallucinations in financial
LLMs, conceptualized as a context-aware masked span prediction task over
real-world financial documents. Our main contributions are: (1) a novel,
automated dataset creation paradigm using a masking strategy; (2) a new
hallucination evaluation dataset derived from S&P 500 annual reports; and (3) a
comprehensive evaluation of intrinsic hallucination patterns in
state-of-the-art LLMs on financial tabular data. Our work provides a robust
methodology for in-house LLM evaluation and serves as a critical step toward
building more trustworthy and reliable financial Generative AI systems.

</details>


### [58] [Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$](https://arxiv.org/abs/2508.05571)
*Feiyu Wang,Guoan Wang,Yihao Zhang,Shengfan Wang,Weitao Li,Bokai Huang,Shimao Chen,Zihan Jiang,Rui Xu,Tong Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的2比特量化框架Fairy±i，通过提高全精度模型的准确率上限并实现高效的2比特量化，从而突破了现有方法的限制，为在极低比特约束下构建高精度和实用的大语言模型提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有的QAT研究主要集中在最小化全精度模型上的量化误差，其中全精度准确率作为上限（准确率天花板）。没有现有方法尝试超越这一天花板。为了打破这一限制，本文提出了一种新的范式：提高全精度模型的上限，然后将其高效地量化为2比特。

Method: 本文提出了Fairy±i框架，这是首个针对复数大语言模型的2比特量化框架。该方法利用复数域的表示优势来提升全精度模型的准确率，并将权重映射到四次单位根{±1, ±i}，形成一个完美对称且信息理论最优的2比特表示。此外，每个量化权重要么具有零实部，要么具有零虚部，从而实现了仅使用加法和元素交换的乘法自由推理。

Result: 实验结果表明，Fairy±i在PPL和下游任务方面均优于现有2比特量化方法的天花板，同时保持严格的存储和计算效率。

Conclusion: 本文提出了一种新的范式，通过提高全精度模型的上限（准确率天花板），然后将其高效地量化为2比特，从而突破了现有2比特量化方法的限制。该工作为在极低比特约束下构建高精度和实用的大语言模型开辟了新方向。

Abstract: Quantization-Aware Training (QAT) integrates quantization into the training
loop, enabling LLMs to learn robust low-bit representations, and is widely
recognized as one of the most promising research directions. All current QAT
research focuses on minimizing quantization error on full-precision models,
where the full-precision accuracy acts as an upper bound (accuracy ceiling). No
existing method has even attempted to surpass this ceiling. To break this
ceiling, we propose a new paradigm: raising the ceiling (full-precision model),
and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$,
the first 2-bit quantization framework for complex-valued LLMs. Specifically,
our method leverages the representational advantages of the complex domain to
boost full-precision accuracy. We map weights to the fourth roots of unity
$\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically
optimal 2-bit representation. Importantly, each quantized weight has either a
zero real or imaginary part, enabling multiplication-free inference using only
additions and element swaps. Experimental results show that Fairy$\pm i$
outperforms the ceiling of existing 2-bit quantization approaches in terms of
both PPL and downstream tasks, while maintaining strict storage and compute
efficiency. This work opens a new direction for building highly accurate and
practical LLMs under extremely low-bit constraints.

</details>


### [59] [Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models](https://arxiv.org/abs/2508.05581)
*Guilherme Seidyo Imai Aldeia,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: This paper explores the use of large language models (LLMs) for generating interpretable computable phenotypes (CPs) for clinical applications, showing that LLMs can produce accurate and concise CPs with iterative refinement.


<details>
  <summary>Details</summary>
Motivation: The potential of LLMs for generating interpretable computable phenotypes (CPs) is under-explored, despite their capabilities in medical question answering and programming.

Method: We propose and test a synthesize, execute, debug, instruct strategy that uses LLMs to generate and iteratively refine CPs using data-driven feedback.

Result: LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples.

Conclusion: LLMs, combined with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities for
medical question answering and programming, but their potential for generating
interpretable computable phenotypes (CPs) is under-explored. In this work, we
investigate whether LLMs can generate accurate and concise CPs for six clinical
phenotypes of varying complexity, which could be leveraged to enable scalable
clinical decision support to improve care for patients with hypertension. In
addition to evaluating zero-short performance, we propose and test a
synthesize, execute, debug, instruct strategy that uses LLMs to generate and
iteratively refine CPs using data-driven feedback. Our results show that LLMs,
coupled with iterative learning, can generate interpretable and reasonably
accurate programs that approach the performance of state-of-the-art ML methods
while requiring significantly fewer training examples.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [60] [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
*Mahmoud Chick Zaouali,Todd Charter,Yehor Karpichev,Brandon Haworth,Homayoun Najjjaran*

Main category: cs.GR

TL;DR: 本文综述了将语言指导与3D高斯散射结合的研究，讨论了其理论基础、集成方法和实际应用，并指出了当前的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对这一新兴交叉领域的全面概述，本文旨在提供一个结构化的综述。

Method: 本文提供了对当前将语言指导与3D高斯散射结合的研究工作的结构化回顾。

Result: 本文详细介绍了理论基础、集成策略和实际应用案例，并指出了计算瓶颈、泛化性和语义标注3D高斯数据稀缺等关键限制。

Conclusion: 本文综述了将语言指导与3D高斯散射相结合的当前研究工作，指出了关键限制，并提出了未来方向。

Abstract: Gaussian Splatting has rapidly emerged as a transformative technique for
real-time 3D scene representation, offering a highly efficient and expressive
alternative to Neural Radiance Fields (NeRF). Its ability to render complex
scenes with high fidelity has enabled progress across domains such as scene
reconstruction, robotics, and interactive content creation. More recently, the
integration of Large Language Models (LLMs) and language embeddings into
Gaussian Splatting pipelines has opened new possibilities for text-conditioned
generation, editing, and semantic scene understanding. Despite these advances,
a comprehensive overview of this emerging intersection has been lacking. This
survey presents a structured review of current research efforts that combine
language guidance with 3D Gaussian Splatting, detailing theoretical
foundations, integration strategies, and real-world use cases. We highlight key
limitations such as computational bottlenecks, generalizability, and the
scarcity of semantically annotated 3D Gaussian data and outline open challenges
and future directions for advancing language-guided 3D scene understanding
using Gaussian Splatting.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [61] [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
*Ugur Cetintemel,Shu Chen,Alexander W. Lee,Deepti Raghavan*

Main category: cs.DB

TL;DR: 本文介绍了 SPEAR，一种用于结构化和自适应提示管理的语言和运行时系统，以解决当前提示管理的不足。


<details>
  <summary>Details</summary>
Motivation: 现代 LLM 流水线越来越像数据导向系统，但提示仍然是一个脆弱、不透明的字符串，与周围的数据显示脱节，这限制了其重用、优化和运行时控制。

Method: SPEAR 定义了一个提示代数，用于指导提示在管道中的构建和适应，并支持多种精炼模式（手动、辅助和自动）。

Result: 初步实验量化了不同精炼模式与静态提示和代理重试的比较，以及提示级优化（如操作符融合）的影响。

Conclusion: SPEAR 提供了一种结构化、自适应的提示管理方法，使提示成为执行模型中的第一类组件，从而提高系统的可重用性、优化和运行时控制。

Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they
retrieve external context, compose intermediate outputs, validate results, and
adapt based on runtime feedback. Yet, the central element guiding this process
-- the prompt -- remains a brittle, opaque string, disconnected from the
surrounding dataflow. This disconnect limits reuse, optimization, and runtime
control.
  In this paper, we describe our vision and an initial design for SPEAR, a
language and runtime that fills this prompt management gap by making prompts
structured, adaptive, and first-class components of the execution model. SPEAR
enables (1) runtime prompt refinement -- modifying prompts dynamically in
response to execution-time signals such as confidence, latency, or missing
context; and (2) structured prompt management -- organizing prompt fragments
into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and
adapted within a pipeline. It supports multiple refinement modes (manual,
assisted, and automatic), giving developers a balance between control and
automation. By treating prompt logic as structured data, SPEAR enables
optimizations such as operator fusion, prefix caching, and view reuse.
Preliminary experiments quantify the behavior of different refinement modes
compared to static prompts and agentic retries, as well as the impact of
prompt-level optimizations such as operator fusion.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [62] [Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning](https://arxiv.org/abs/2508.05129)
*Wuqiang Zheng,Yiyan Xu,Xinyu Lin,Chongming Gao,Wenjie Wang,Fuli Feng*

Main category: cs.IR

TL;DR: PaperEval是一个基于大型语言模型的框架，通过两个关键组件和一种渐进式排名优化策略，提高了自动论文评估的准确性和可靠性，并在实际应用中取得了成功。


<details>
  <summary>Details</summary>
Motivation: 随着学术出版物的迅速增加，识别高质量的研究变得越来越紧迫。虽然最近利用大型语言模型进行自动论文评估的方法显示出巨大的潜力，但它们往往受到过时领域知识和有限推理能力的限制。

Method: PaperEval是一个基于大型语言模型的框架，包含两个关键组件：领域感知的论文检索模块和潜在推理机制，以及一种渐进式排名优化策略。

Result: 实验表明，PaperEval在两个数据集上始终优于现有方法。此外，PaperEval被部署在一个真实的论文推荐系统中，用于过滤高质量的论文，并在社交媒体上获得了强烈反响。

Conclusion: PaperEval在学术影响和论文质量评估方面均优于现有方法，并在实际应用中表现出色。

Abstract: With the rapid and continuous increase in academic publications, identifying
high-quality research has become an increasingly pressing challenge. While
recent methods leveraging Large Language Models (LLMs) for automated paper
evaluation have shown great promise, they are often constrained by outdated
domain knowledge and limited reasoning capabilities. In this work, we present
PaperEval, a novel LLM-based framework for automated paper evaluation that
addresses these limitations through two key components: 1) a domain-aware paper
retrieval module that retrieves relevant concurrent work to support
contextualized assessments of novelty and contributions, and 2) a latent
reasoning mechanism that enables deep understanding of complex motivations and
methodologies, along with comprehensive comparison against concurrently related
work, to support more accurate and reliable evaluation. To guide the reasoning
process, we introduce a progressive ranking optimization strategy that
encourages the LLM to iteratively refine its predictions with an emphasis on
relative comparison. Experiments on two datasets demonstrate that PaperEval
consistently outperforms existing methods in both academic impact and paper
quality evaluation. In addition, we deploy PaperEval in a real-world paper
recommendation system for filtering high-quality papers, which has gained
strong engagement on social media -- amassing over 8,000 subscribers and
attracting over 10,000 views for many filtered high-quality papers --
demonstrating the practical effectiveness of PaperEval.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [63] [Federal Reserve Communication and the COVID-19 Pandemic](https://arxiv.org/abs/2508.04830)
*Jonathan Benchimol,Sophia Kazinnik,Yossi Saadon*

Main category: econ.GN

TL;DR: 本研究分析了美联储在新冠疫情中的沟通策略，发现其与以往经济危机中的沟通策略有所不同，更加关注金融稳定、市场波动、社会福利和非常规货币政策（UMP），并且表现出更高的上下文不确定性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析美联储在新冠疫情中的沟通策略，并将其与过去的经济危机进行比较，以理解中央银行沟通如何在危机中演变以及如何适应特殊的经济环境。

Method: 研究使用了针对新冠疫情、非常规货币政策（UMP）和金融稳定的专门词典，结合情感分析和主题建模技术，比较了美联储在新冠疫情与过去经济危机期间的沟通策略。

Result: 研究发现，美联储在新冠疫情中的沟通更加强调金融稳定、市场波动、社会福利和UMP，并且表现出更高的上下文不确定性。同时，利率公告和会议纪要中与金融稳定相关的负面情绪预示了随后的宽松货币政策决策。此外，自全球金融危机以来，关于UMP的沟通已成为美联储会议纪要和主席演讲中的常态。

Conclusion: 研究发现，美联储在新冠疫情中的沟通策略与以往经济压力时期的沟通策略有所不同，更加关注金融稳定、市场波动、社会福利和非常规货币政策（UMP），并且表现出显著的上下文不确定性。此外，美联储在应对新冠疫情时的政策反应比以往更加被动。研究还表明，自全球金融危机以来，关于UMP的沟通已成为美联储会议纪要和主席演讲中的“新常态”，反映了机构在经济困境后的沟通策略适应。

Abstract: In this study, we examine the Federal Reserve's communication strategies
during the COVID-19 pandemic, comparing them with communication during previous
periods of economic stress. Using specialized dictionaries tailored to
COVID-19, unconventional monetary policy (UMP), and financial stability,
combined with sentiment analysis and topic modeling techniques, we identify a
distinct focus in Fed communication during the pandemic on financial stability,
market volatility, social welfare, and UMP, characterized by notable contextual
uncertainty. Through comparative analysis, we juxtapose the Fed's communication
during the COVID-19 crisis with its responses during the dot-com and global
financial crises, examining content, sentiment, and timing dimensions. Our
findings reveal that Fed communication and policy actions were more reactive to
the COVID-19 crisis than to previous crises. Additionally, declining sentiment
related to financial stability in interest rate announcements and minutes
anticipated subsequent accommodative monetary policy decisions. We further
document that communicating about UMP has become the "new normal" for the Fed's
Federal Open Market Committee meeting minutes and Chairman's speeches since the
Global Financial Crisis, reflecting an institutional adaptation in
communication strategy following periods of economic distress. These findings
contribute to our understanding of how central bank communication evolves
during crises and how communication strategies adapt to exceptional economic
circumstances.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [64] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: 本文提出了一种名为MICoBot的系统，用于长期人机协作，通过混合主动权对话范式，使机器人和人类能够灵活地提出、接受或拒绝请求，以有效地完成任务。


<details>
  <summary>Details</summary>
Motivation: 有效的长期人机协作机器人系统必须适应各种人类合作伙伴，这些合作伙伴的物理行为、协助意愿和对机器人能力的理解可能会随时间变化。这需要一个紧密耦合的通信循环，使两个代理能够灵活地提出、接受或拒绝请求，以有效地协调完成任务。

Method: 我们应用了混合主动权对话范式来协同人类机器人团队，并提出了MICoBot系统，该系统处理双方使用自然语言提出、接受或拒绝谁最适合完成任务不同步骤的常见场景。MICoBot在三个层次上做出决策：(1)元规划器考虑人类对话以制定和编码高层次的合作策略，(2)规划器根据机器人的能力（通过模拟预训练的可操作性模型测量）和人类帮助的估计可用性，最优地将剩余步骤分配给任一代理，(3)动作执行器决定执行低级动作或对人类说的话。

Result: 我们在模拟和现实世界中进行了广泛的评估——在物理机器人上与18个独特的参与者进行了27小时的测试——证明了我们的方法能够有效与多样化的用户合作，任务成功率和用户体验显著优于纯LLM基线和其他代理分配模型。

Conclusion: 我们的方法能够有效与多样化的用户合作，比纯LLM基线和其他代理分配模型显著提高了任务成功率和用户体验。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [65] [Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages](https://arxiv.org/abs/2508.05149)
*Seraphina Fong,Marco Matassoni,Alessio Brutti*

Main category: eess.AS

TL;DR: 本文研究了在低资源语言中使用语音大语言模型的潜力，并提出了SLAM-ASR框架。通过使用预训练的投影器，可以减轻数据稀缺的影响，并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在高资源语言中表现出色，但它们在低资源环境中的适用性仍需进一步探索。

Method: 本文采用了SLAM-ASR框架，其中可训练的轻量级投影器连接了语音编码器和大语言模型。

Result: 研究发现，使用预训练的单语或多语投影器可以减少数据稀缺的影响，尤其是在小规模训练集的情况下。此外，使用多语大语言模型（如EuroLLM、Salamandra）与whisper-large-v3-turbo进行评估，提供了对未来研究的见解。

Conclusion: 本文探讨了在低资源语言中使用语音大语言模型的可能性，并提出了SLAM-ASR框架。研究结果表明，利用预训练的单语或多语投影器可以减轻数据稀缺的影响，特别是在小规模训练集的情况下。

Abstract: Large language models (LLMs) have demonstrated potential in handling spoken
inputs for high-resource languages, reaching state-of-the-art performance in
various tasks. However, their applicability is still less explored in
low-resource settings. This work investigates the use of Speech LLMs for
low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a
trainable lightweight projector connects a speech encoder and a LLM. Firstly,
we assess training data volume requirements to match Whisper-only performance,
re-emphasizing the challenges of limited data. Secondly, we show that
leveraging mono- or multilingual projectors pretrained on high-resource
languages reduces the impact of data scarcity, especially with small training
sets. Using multilingual LLMs (EuroLLM, Salamandra) with
whisper-large-v3-turbo, we evaluate performance on several public benchmarks,
providing insights for future research on optimizing Speech LLMs for
low-resource languages and multilinguality.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [66] [SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription](https://arxiv.org/abs/2508.05554)
*Raymond Grossman,Taejin Park,Kunal Dhawan,Andrew Titus,Sophia Zhi,Yulia Shchadilova,Weiqing Wang,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.SD

TL;DR: SPGISpeech 2.0 是一个用于金融领域的语音识别数据集，包含更多专业转录的收益电话，并且有助于提升语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高适用于建模任务的多样性，同时保持原始 SPGISpeech 数据集的核心特征。

Method: 通过在 SPGISpeech 2.0 上微调流行的语音识别模型来验证其有效性。

Result: SPGISpeech 2.0 包含了 3,780 小时的专业转录的收益电话，并且包含每个音频片段的通话和说话人信息，有助于多说话人 ASR。

Conclusion: SPGISpeech 2.0 可以促进语音识别技术的进步并激发广泛的研究应用。

Abstract: We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged
transcription in the financial domain. SPGISpeech 2.0 improves the diversity of
applicable modeling tasks while maintaining the core characteristic of the
original SPGISpeech dataset: audio snippets and their corresponding fully
formatted text transcriptions, usable for end-to-end automatic speech
recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of
professionally transcribed earnings calls. Furthermore, the dataset contains
call and speaker information for each audio snippet facilitating multi-talker
ASR. We validate the utility of SPGISpeech 2.0 through improvements in
speaker-tagged ASR performance of popular speech recognition models after
fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect
SPGISpeech 2.0 to foster advancements in speech recognition technologies and
inspire a wide range of research applications.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [67] [JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering](https://arxiv.org/abs/2508.05087)
*Renmiao Chen,Shiyao Cui,Xuancheng Huang,Chengwei Pan,Victor Shea-Jay Huang,QingLin Zhang,Xuan Ouyang,Zhexin Zhang,Hongning Wang,Minlie Huang*

Main category: cs.MM

TL;DR: 本文提出了一种名为JPS的越狱攻击方法，通过视觉扰动和文本引导提示协同工作，提高了攻击的成功率和恶意意图的满足率。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注最大化攻击成功率（ASR），而忽略了生成的响应是否真正满足攻击者的恶意意图，导致输出质量低下。

Method: JPS通过协作视觉扰动和文本引导提示实现对MLLM的越狱攻击，利用目标引导的对抗性图像扰动和多智能体优化的“引导提示”来增强攻击效果。

Result: JPS在ASR和MIFR方面均取得了新的最先进水平，并通过分析验证了其有效性。

Conclusion: JPS在多个MLLM和基准测试中实现了ASR和MIFR的新最先进水平，证明了其有效性。

Abstract: Jailbreak attacks against multimodal large language Models (MLLMs) are a
significant research focus. Current research predominantly focuses on
maximizing attack success rate (ASR), often overlooking whether the generated
responses actually fulfill the attacker's malicious intent. This oversight
frequently leads to low-quality outputs that bypass safety filters but lack
substantial harmful content. To address this gap, we propose JPS,
\underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation
and textual \underline{S}teering, which achieves jailbreaks via corporation of
visual image and textually steering prompt. Specifically, JPS utilizes
target-guided adversarial image perturbations for effective safety bypass,
complemented by "steering prompt" optimized via a multi-agent system to
specifically guide LLM responses fulfilling the attackers' intent. These visual
and textual components undergo iterative co-optimization for enhanced
performance. To evaluate the quality of attack outcomes, we propose the
Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a
Reasoning-LLM-based evaluator. Our experiments show JPS sets a new
state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with
analyses confirming its efficacy. Codes are available at
\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.
\color{warningcolor}{Warning: This paper contains potentially sensitive
contents.}

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [68] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: 本文提出了一种双源策略，以提升多模态大语言模型在低资源语言环境下的表现，并引入了MELLA数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言增强方法通常仅限于文本模态或依赖机器翻译，这导致模型无法获得足够的多模态信息和文化背景，这对于服务低资源语言用户至关重要。

Method: 我们提出了一种双源策略，通过收集针对每个目标的数据来实现这两个目标，即使用本地网络的替代文本来获取文化信息，以及使用多模态大语言模型生成的描述来获取语言能力。

Result: 实验结果表明，在MELLA上微调后，各种多模态大语言模型在八种语言上的表现都有所提高，模型能够生成“厚描述”。

Conclusion: 我们的数据集可以用于提升多模态大语言模型在低资源语言环境下的表现，从而更好地服务于这些语言的用户。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in
high-resource languages. However, their effectiveness diminishes significantly
in the contexts of low-resource languages. Current multilingual enhancement
methods are often limited to text modality or rely solely on machine
translation. While such approaches help models acquire basic linguistic
capabilities and produce "thin descriptions", they neglect the importance of
multimodal informativeness and cultural groundedness, both of which are crucial
for serving low-resource language users effectively. To bridge this gap, in
this study, we identify two significant objectives for a truly effective MLLM
in low-resource language settings, namely 1) linguistic capability and 2)
cultural groundedness, placing special emphasis on cultural awareness. To
achieve these dual objectives, we propose a dual-source strategy that guides
the collection of data tailored to each goal, sourcing native web alt-text for
culture and MLLM-generated captions for linguistics. As a concrete
implementation, we introduce MELLA, a multimodal, multilingual dataset.
Experiment results show that after fine-tuning on MELLA, there is a general
performance improvement for the eight languages on various MLLM backbones, with
models producing "thick descriptions". We verify that the performance gains are
from both cultural knowledge enhancement and linguistic capability enhancement.
Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.

</details>


### [69] [Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision](https://arxiv.org/abs/2508.05606)
*Luozheng Qin,Jia Gong,Yuqing Sun,Tianjiao Li,Mengping Yang,Xiaomeng Yang,Chao Qu,Zhiyu Tan,Hao Li*

Main category: cs.CV

TL;DR: Uni-CoT is a unified chain-of-thought framework that enables coherent and grounded multimodal reasoning through a two-level reasoning paradigm and structured training, achieving SOTA performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with modeling visual state transitions or incoherent visual trajectories due to limited capacity or fragmented architectures. Uni-CoT aims to enable coherent and grounded multimodal reasoning within a single unified model.

Method: Uni-CoT introduces a novel two-level reasoning paradigm: Macro-Level CoT for high-level task planning and Micro-Level CoT for subtask execution. It also introduces a structured training paradigm combining interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT.

Result: Uni-CoT achieves SOTA performance on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS). It can be efficiently trained using only 8 A100 GPUs with 80GB VRAM each.

Conclusion: Uni-CoT demonstrates SOTA performance and strong generalization, establishing it as a promising solution for multi-modal reasoning.

Abstract: Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large
Language Models (LLMs) by decomposing complex tasks into simpler, sequential
subtasks. However, extending CoT to vision-language reasoning tasks remains
challenging, as it often requires interpreting transitions of visual states to
support reasoning. Existing methods often struggle with this due to limited
capacity of modeling visual state transitions or incoherent visual trajectories
caused by fragmented architectures.
  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought
framework that enables coherent and grounded multimodal reasoning within a
single unified model. The key idea is to leverage a model capable of both image
understanding and generation to reason over visual content and model evolving
visual states. However, empowering a unified model to achieve that is
non-trivial, given the high computational cost and the burden of training. To
address this, Uni-CoT introduces a novel two-level reasoning paradigm: A
Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask
execution. This design significantly reduces the computational overhead.
Furthermore, we introduce a structured training paradigm that combines
interleaved image-text supervision for macro-level CoT with multi-task
objectives for micro-level CoT. Together, these innovations allow Uni-CoT to
perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our
design, all experiments can be efficiently completed using only 8 A100 GPUs
with 80GB VRAM each. Experimental results on reasoning-driven image generation
benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT
demonstrates SOTA performance and strong generalization, establishing Uni-CoT
as a promising solution for multi-modal reasoning. Project Page and Code:
https://sais-fuxi.github.io/projects/uni-cot/

</details>


### [70] [Test-Time Reinforcement Learning for GUI Grounding via Region Consistency](https://arxiv.org/abs/2508.05615)
*Yong Du,Yuchen Yan,Fei Tang,Zhengxi Lu,Chang Zong,Weiming Lu,Shengpei Jiang,Yongliang Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时缩放方法GUI-RC和一种基于一致性模式的强化学习方法GUI-RCPO，用于改善GUI接地任务的准确性，并展示了其在不同架构上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通过大量的监督训练或带有标记奖励的强化学习实现强大的性能，但它们受到像素级注释的成本和可用性的限制。当模型为同一GUI元素生成多个预测时，空间重叠模式揭示了隐含的置信度信号，可以指导更精确的定位。

Method: 我们提出了GUI-RC（区域一致性），一种测试时缩放方法，从多个采样预测中构建空间投票网格以识别模型显示最高协议的共识区域。此外，我们引入了GUI-RCPO（区域一致性策略优化），将这些一致性模式转化为测试时强化学习的奖励。

Result: GUI-RC在ScreenSpot基准上提高了各种架构的准确性2-3%。GUI-RCPO进一步通过自监督优化提高了Qwen2.5-VL-3B-Instruct的性能。

Conclusion: 我们的方法揭示了测试时缩放和测试时强化学习在GUI接地中的未开发潜力，为更强大和数据高效的GUI代理提供了有希望的路径。

Abstract: Graphical User Interface (GUI) grounding, the task of mapping natural
language instructions to precise screen coordinates, is fundamental to
autonomous GUI agents. While existing methods achieve strong performance
through extensive supervised training or reinforcement learning with labeled
rewards, they remain constrained by the cost and availability of pixel-level
annotations. We observe that when models generate multiple predictions for the
same GUI element, the spatial overlap patterns reveal implicit confidence
signals that can guide more accurate localization. Leveraging this insight, we
propose GUI-RC (Region Consistency), a test-time scaling method that constructs
spatial voting grids from multiple sampled predictions to identify consensus
regions where models show highest agreement. Without any training, GUI-RC
improves accuracy by 2-3% across various architectures on ScreenSpot
benchmarks. We further introduce GUI-RCPO (Region Consistency Policy
Optimization), which transforms these consistency patterns into rewards for
test-time reinforcement learning. By computing how well each prediction aligns
with the collective consensus, GUI-RCPO enables models to iteratively refine
their outputs on unlabeled data during inference. Extensive experiments
demonstrate the generality of our approach: GUI-RC boosts
Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO
further improves it to 85.14% through self-supervised optimization. Our
approach reveals the untapped potential of test-time scaling and test-time
reinforcement learning for GUI grounding, offering a promising path toward more
robust and data-efficient GUI agents.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [71] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 本文分析了基于大语言模型的寄存器传输级代码生成中的错误原因，并提出了一系列针对性的错误纠正技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型（LLM）的寄存器传输级（RTL）代码生成具有令人鼓舞的潜力，但整体成功率仍然不令人满意。错误源于各种因素，对具体失败原因的有限理解阻碍了改进。

Method: 我们提出了针对性的错误纠正技术，包括构建领域特定的知识库，使用检索增强生成（RAG）提供必要的RTL知识，引入设计描述规则并实现规则检查机制，以及集成外部工具将输入转换为LLM兼容的元格式。此外，我们还采用了迭代调试循环（仿真-错误定位-纠正）。

Result: 将这些技术整合到一个LLM框架中显著提高了性能。实验结果表明，我们的增强框架在VerilogEval基准测试中达到了91.0%的准确率，比基线代码生成方法提高了32.7%。

Conclusion: 我们的增强框架在VerilogEval基准测试中达到了91.0%的准确率，比基线代码生成方法提高了32.7%，证明了我们方法的有效性。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文引入了一个新的基准LUCARIO和一个框架，用于在大型表格数据上的概率问答。我们的方法从表格中推导贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型（LLMs）生成最终答案。实证结果表明，我们的方法在基线上有显著改进，突显了混合符号-神经推理的优势。


<details>
  <summary>Details</summary>
Motivation: 当前的问答（QA）方法，如NL2SQL系统，在直接从表格中检索答案的事实性问题上表现良好，但在需要在不确定性下进行推理的概率性问题上表现不足。

Method: 从表格中推导贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型（LLMs）生成最终答案。

Result: 实证结果表明，我们的方法在基线上有显著改进。

Conclusion: 我们的方法在基线上有显著改进，突显了混合符号-神经推理的优势。

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 本文创建了多语言功能基准测试，以更准确地评估大型语言模型在多语言环境中的性能和鲁棒性，并发现不同语言的模型表现存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有的静态多语言基准测试无法充分理解模型在多语言环境中的实际性能和鲁棒性，因此需要创建新的多语言功能基准测试来更准确地评估模型的表现。

Method: 本文通过将现有的英语功能基准测试模板翻译成五种其他语言（法语、西班牙语、印地语、阿拉伯语和约鲁巴语），创建了多语言功能基准测试CL-GSM Symbolic和CL-IFEval。

Result: 结果显示，某些静态多语言基准测试比其他基准测试更能捕捉功能性能（例如，在英语、法语和西班牙语中，M-GSM与CL-GSM Symbolic之间的性能下降分别为24%、17%和18%；在Belebele和CL-IFEval之间，性能下降为15-24%，而在M-MMLU和CL-IFEval之间仅下降0.5%-3%）。此外，模型在不同语言中的鲁棒性存在显著差异，某些语言（如阿拉伯语和英语）在评估迭代中表现最为一致。

Conclusion: 本文通过创建多语言功能基准测试，揭示了静态多语言基准测试在评估模型功能性能和鲁棒性方面的不足，并发现不同语言的模型表现存在显著差异。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: 一项研究发现，虽然大型语言模型（LLMs）生成的研究想法在构思阶段被认为更具新颖性，但执行后这些想法的表现不如人类专家的想法，表明LLMs在生成真正有效的研究想法方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究表明LLM生成的研究想法被认为比人类专家的想法更具新颖性，但一个好的想法不仅应显得新颖，还应在执行后产生更好的研究成果。因此，需要测试AI生成的想法是否能带来更好的研究结果。

Method: 通过招募43位专家研究人员来执行随机分配的想法（由专家撰写或由LLM生成），每个专家花费超过100小时实施该想法，并撰写一篇4页的短论文来记录实验。所有执行的项目均由专家NLP研究人员盲审。

Result: 比较同一想法在执行前后的评审分数，LLM生成的想法在所有评估指标（新颖性、兴奋度、有效性及总体评分）上的得分显著下降，缩小了在构思阶段观察到的LLM和人类想法之间的差距。在执行研究的汇总评审分数中，许多指标的排名甚至发生了翻转，人类想法的评分高于LLM生成的想法。

Conclusion: 当前大型语言模型生成的研究想法在执行后表现出不如人类专家的想法有效，这突显了当前LLM在生成真正有效的研究想法方面的局限性以及在缺乏执行结果的情况下评估研究想法的挑战。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: MultiFinRAG is a retrieval-augmented generation framework designed for financial QA that outperforms ChatGPT-4o in accuracy on complex tasks involving text, tables, images, and multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: Financial documents span hundreds of pages and combine diverse modalities, making it challenging for traditional large language models (LLMs) and retrieval-augmented generation (RAG) pipelines to answer questions over such content due to token limitations, layout loss, and fragmented cross-modal context.

Method: MultiFinRAG is a retrieval-augmented generation framework that performs multimodal extraction by grouping table and figure images into batches and sending them to a lightweight, quantized open-source multimodal LLM. The outputs are embedded and indexed with modality-aware similarity thresholds for precise retrieval, and a tiered fallback strategy dynamically escalates from text-only to text+table+image contexts when necessary.

Result: MultiFinRAG achieves 19 percentage points higher accuracy than ChatGPT-4o (free-tier) on complex financial QA tasks involving text, tables, images, and combined multimodal reasoning.

Conclusion: MultiFinRAG achieves higher accuracy than ChatGPT-4o on complex financial QA tasks involving text, tables, images, and combined multimodal reasoning.

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在处理道德模糊的现实场景中的表现，并发现它们的暴力倾向存在偏差，与现有社会科学研究结果不一致。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估大型语言模型在检测和应对在线暴力内容方面的能力，并探讨其在道德模糊场景中的潜在偏差。

Method: 研究使用了经过验证的社会科学工具——暴力行为情景问卷（VBVQ）来评估大型语言模型，并引入基于角色的提示方法来测试不同人口统计学特征下的表现。

Result: 研究发现，大型语言模型的表面文本生成与其内部对暴力反应的偏好存在差异，并且其暴力倾向因人口统计学特征而异，常常与现有研究结果相矛盾。

Conclusion: 研究发现，大型语言模型在处理道德模糊的现实场景时存在偏差，其暴力倾向与犯罪学、社会学和心理学中的既有研究结果相矛盾。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 本文探讨了医学领域中端到端事实核查系统的挑战，并主张将其视为交互式沟通问题。


<details>
  <summary>Details</summary>
Motivation: 由于医疗决策的高风险性质以及对大量和多样化的医学文献进行批判性评估的挑战，人们对采用这些系统用于公共卫生和医学的兴趣增加。

Method: 我们进行了第一项研究，探讨临床专家如何通过综合医学证据来验证社交媒体上的真实声明。

Result: 我们揭示了在医学中应用端到端事实核查时的基本挑战：难以将野外声明与临床试验形式的科学证据联系起来；模糊的未明确声明与不匹配的意图；以及本质上主观的真实性标签。

Conclusion: 我们主张应将事实核查视为一种交互式沟通问题，而不是端到端的过程。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 本文探讨了如何更有效地将语言模型适应到下游应用。提出了几种方法，包括从无标签数据中提取任务相关知识的新技术、参数高效的微调方法、改进的监督微调方法以及新的评估方法和基准测试。实验结果表明，这些方法显著提高了语言模型的鲁棒性、效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LMs）在自然语言处理方面表现出色，但高效且稳健地适应特定任务仍然具有挑战性。随着它们的规模和复杂性的增加，对标记数据进行微调往往未能充分利用可用的未标记数据，导致在小任务特定集上过拟合，并带来巨大的计算成本。这些限制阻碍了它们在现实世界语言任务的开放景观中的应用。

Method: 首先，我们探索了从无标签数据中提取任务相关知识的策略，引入了一种新的持续预训练技术，其性能优于最先进的半监督方法。接下来，我们提出了一种参数高效的微调方法，大大减少了内存和计算成本，同时保持了具有竞争力的性能。我们还介绍了改进的监督微调方法，使语言模型能够更好地遵循指令，尤其是在标记数据稀缺时，增强了它们在各种自然语言处理任务中的表现，包括开放式生成。最后，我们开发了新的评估方法和基准测试，如多跳空间推理任务，以更全面地评估语言模型的能力和适应性。

Result: 通过在多种自然语言处理任务上的广泛实证研究，我们的结果表明这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其更适用于各种应用。这些进展标志着向更稳健和高效的语言模型迈出了重要一步，使我们更接近人工智能通用智能的目标。

Conclusion: 通过广泛的实证研究，我们的结果表明这些方法显著提高了语言模型的鲁棒性、效率和泛化能力，使其更适用于各种应用。这些进展标志着向更稳健和高效的语言模型迈出了重要一步，使我们更接近人工智能通用智能的目标。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 本文介绍了一种基于FineWeb的新预训练数据集整理管道，该管道可以自动适应支持任何语言。我们通过一种基于可衡量标准的新选择过程选择了有意义且信息丰富的评估任务，在一组九种多样化的语言上广泛地消除了我们的管道设计选择。我们还引入了一种简单且有原则的方法来重新平衡数据集，这考虑了重复次数和质量。最终，我们使用近100个Common Crawl快照将我们的管道扩展到1000多种语言，以生成FineWeb2，这是一个新的20TB（50亿文档）多语言数据集，并与我们的管道、训练和评估代码库一起发布。


<details>
  <summary>Details</summary>
Motivation: 训练高性能多语言大型语言模型仍然是一个挑战，主要是因为难以将过滤和去重管道定制到大量语言。

Method: 我们介绍了一种基于FineWeb的新预训练数据集整理管道，该管道可以自动适应支持任何语言。我们通过一种基于可衡量标准的新选择过程选择了有意义且信息丰富的评估任务，在一组九种多样化的语言上广泛地消除了我们的管道设计选择。我们还引入了一种简单且有原则的方法来重新平衡数据集，这考虑了重复次数和质量。

Result: 我们的管道可以用于创建比之前数据集表现更好的非英语语料库。我们还引入了一种简单且有原则的方法来重新平衡数据集，这考虑了重复次数和质量，提供了额外的性能提升。最后，我们使用近100个Common Crawl快照将我们的管道扩展到1000多种语言，以生成FineWeb2，这是一个新的20TB（50亿文档）多语言数据集，并与我们的管道、训练和评估代码库一起发布。

Conclusion: 我们的管道可以用于创建比之前数据集表现更好的非英语语料库。我们还引入了一种简单且有原则的方法来重新平衡数据集，这考虑了重复次数和质量，提供了额外的性能提升。最后，我们使用近100个Common Crawl快照将我们的管道扩展到1000多种语言，以生成FineWeb2，这是一个新的20TB（50亿文档）多语言数据集，并与我们的管道、训练和评估代码库一起发布。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出KaLM-Embedding-V2，一个多功能且紧凑的嵌入模型，通过先进的训练技术和数据，在MTEB基准测试中表现出色，超越了其他同等规模的模型，并与更大的模型竞争。


<details>
  <summary>Details</summary>
Motivation: 本文旨在开发一个多功能且紧凑的嵌入模型，以在通用文本嵌入任务中实现卓越的性能。

Method: 我们提出了KaLM-Embedding-V2，一个多功能且紧凑的嵌入模型，通过利用优越的训练技术和数据实现了出色的性能。关键创新包括：(1) 为了更好地与表示学习对齐，我们去除了因果注意力掩码，采用了全双向变换器和简单有效的平均池化来生成固定长度的嵌入；(2) 我们采用多阶段训练流程：(i) 在大规模弱监督开源语料库上进行预训练；(ii) 在高质量检索和非检索数据集上进行微调；(iii) 使用模型汤参数平均进行鲁棒泛化。此外，我们引入了一种类似焦点的重新加权机制，将学习集中在困难样本上，并采用在线硬负混合策略，无需昂贵的离线挖掘即可持续丰富硬负样本；(3) 我们收集了超过20个类别的数据用于预训练和100个类别的数据用于微调，以提高嵌入模型的性能和泛化能力。

Result: 在MTEB中文和英文基准测试中，我们的模型显著优于其他同等规模的模型，并与3倍、14倍、18倍和26倍大的嵌入模型竞争。

Conclusion: 我们的模型在MTEB中文和英文基准测试中表现出色，显著优于其他同等规模的模型，并与3倍、14倍、18倍和26倍大的嵌入模型竞争，为参数少于1B的多功能和紧凑嵌入模型设定了新标准。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 本文提出了一种元训练语言模型的方法，使梯度更新能够模拟条件新信息的效果，从而在不依赖真实标签的情况下提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前，将新信息纳入语言模型有两种主要方式：改变其提示或改变其参数（例如通过微调）。然而，对于许多模型更新，提示比微调更有效。因此，研究如何使微调模仿提示成为必要。

Method: 本文描述了一种元训练语言模型的方法，使梯度更新能够模拟条件新信息的效果。该方法使用了基于梯度的元学习工具，但使用了语言模型本身的提示预测作为目标，消除了对真实标签的需求。

Result: 后续的梯度下降训练恢复了提示模型的一些（有时全部）性能，这在“反转诅咒”任务上有所改进，并且可以在单次梯度更新后回答关于文本段落的问题。

Conclusion: 这些结果表明，通过适当的初始化，梯度下降可以非常具有表现力。我们的结果为长上下文建模提供了新的方向，并对基于梯度的学习的泛化能力提供了见解。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种扩展的机器人格量表（MPI）和一个称为特定属性控制（SAC）的框架，以更精细和可控的方式建模大型语言模型（LLM）的人格，从而改善人机交互。


<details>
  <summary>Details</summary>
Motivation: 现有的模型主要依赖于五大框架，只能提供粗略的人格维度，并且缺乏控制特质强度的机制。因此，我们需要一种更精细和可控的方法来建模LLM的人格。

Method: 我们扩展了机器人格量表（MPI），将其从最初的五大模型扩展到16种人格因素（16PF）模型，并开发了一个称为特定属性控制（SAC）的结构化框架，用于评估和动态诱导LLM中的特质强度。

Result: 通过实验，我们发现将强度建模为连续谱比二进制特征切换能产生更一致和可控的人格表达。此外，我们观察到目标特质强度的变化会系统地影响相关特质，在心理上连贯的方向上，这表明LLM内部化了多维人格结构。

Conclusion: 我们的工作为医疗、教育和面试等领域的可控且细腻的人机交互开辟了新途径，使我们更接近真正类人的社交机器。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 本文介绍了CA-Ben，一个专门用于评估大型语言模型在财务、法律和定量推理能力方面的特许会计师基准。通过测试六种著名模型，发现Claude 3.5 Sonnet和GPT-4o表现最佳，但在数值计算和法律解释方面仍面临挑战。研究建议未来可以通过混合推理和检索增强生成方法进行改进。


<details>
  <summary>Details</summary>
Motivation: 本文旨在填补印度金融背景下大型语言模型（LLMs）在捕捉和应用领域特定金融知识方面的关键空白。

Method: 本文介绍了CA-Ben，这是一个专门设计用于评估大型语言模型（LLMs）在财务、法律和定量推理能力方面的特许会计师基准。CA-Ben包含从印度特许会计师协会（ICAI）的严格考试中提取的结构化问答数据集，涵盖了基础、中级和高级CA课程阶段。六种著名的LLMs（即GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4）使用标准化协议进行了评估。

Result: 结果显示，性能存在差异，Claude 3.5 Sonnet和GPT-4o表现优于其他模型，尤其是在概念和法律推理方面。在数值计算和法律解释方面出现了显著挑战。

Conclusion: 研究结果强调了当前大型语言模型的优势和局限性，建议通过混合推理和检索增强生成方法来改进，特别是在量化分析和准确的法律解释方面。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 本文提出了一种新型的半监督可扩展统一框架（SSUF），用于电子商务中的查询分类任务。该框架包含多个增强模块，能够解决查询信息不足、依赖后验标签以及标签关系复杂等问题。实验结果表明，SSUF显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 查询分类在电子商务应用中至关重要，但电子商务查询通常较短且缺乏上下文，标签之间的信息无法使用，导致建模的先验信息不足。现有的工业查询分类方法依赖于用户的点击行为来构建训练样本，导致马修恶循环。此外，查询分类的子任务缺乏统一的框架，导致算法优化效率低下。

Method: 提出了一种新型的半监督可扩展统一框架（SSUF），包含多个增强模块，以统一查询分类任务。

Result: 进行了广泛的离线和在线A/B测试，结果表明SSUF显著优于最先进的模型。

Conclusion: SSUF显著优于最先进的模型。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

TL;DR: 本文介绍了MT2-CSD数据集，并提出了LLM-CRAN方法，用于多目标、多轮对话立场检测。实验结果表明，该方法在该任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统立场检测研究通常针对单个实例，限制了其对真实社交媒体场景中多方讨论的建模能力。数据集的稀缺性阻碍了对话立场检测的进步。

Method: 提出了一种基于大型语言模型的对话关系注意力网络（LLM-CRAN），利用大型语言模型的推理能力来提高对话理解。

Result: 在MT2-CSD数据集上进行的广泛实验表明，LLM-CRAN在对话立场检测任务中显著优于强基线模型。

Conclusion: LLM-CRAN显著优于强基线模型，在对话立场检测任务中表现出色。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [15] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态句子表示学习方法DALR，通过解决跨模态错位偏差和内模态语义差异的问题，提高了句子表示的质量。


<details>
  <summary>Details</summary>
Motivation: 之前的多模态句子表示学习方法主要关注图像和文本在粗略层面的对齐，但面临两个关键挑战：跨模态错位偏差和内模态语义差异，这显著降低了句子表示的质量。

Method: 我们提出了DALR（多模态句子表示的双级对齐学习），通过一致性学习模块实现细粒度的跨模态对齐，并结合排名蒸馏和全局内模态对齐学习来增强表示质量。

Result: 在语义文本相似性（STS）和迁移（TR）任务上的广泛实验验证了我们方法的有效性，结果一致表明其优越性。

Conclusion: 我们的方法在语义文本相似性（STS）和迁移（TR）任务中得到了验证，结果表明其优于最先进的基线。

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [16] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: ComRAG是一种用于实时工业社区问答的检索增强生成框架，通过基于中心点的记忆机制整合静态知识和动态历史QA对，从而在性能和效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法往往未能充分利用外部知识，未能结合动态的历史QA上下文，或缺乏适合工业部署的记忆机制。

Method: ComRAG是一个基于检索增强生成的框架，通过基于中心点的记忆机制将静态知识与动态历史QA对集成，旨在实现实时工业CQA。

Result: ComRAG在三个工业CQA数据集上表现出色，实现了显著的性能提升和效率优化。

Conclusion: ComRAG在三个工业CQA数据集上 consistently 超过所有基线，实现了高达25.9%的向量相似度提升，延迟降低了8.7%至23.3%，并且在迭代过程中将块增长从20.23%降低到2.06%。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [17] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: 本文提出了Progtuning，一种结合渐进学习的微调框架，以更高效地分配计算资源并减少更新参数的数量。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法在更新参数时没有考虑到Transformer块之间的不同贡献，导致计算资源的极低效分配。

Method: Progtuning是一种结合渐进学习的微调框架，它逐步减少基于贡献的更新Transformer块的数量。

Result: Progtuning通过优化资源分配，减少了大约25%的更新参数数量，同时保持了具有竞争力的性能。

Conclusion: Progtuning优化了资源分配，减少了大约25%的更新参数数量，同时保持了具有竞争力的性能，并且在各种适应场景中表现出色。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [18] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: Cosmos is a novel text generation approach that uses a compressed latent space for faster and more coherent text generation compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Autoregressive language models have limitations in decoding speed and global coherence, while diffusion models face challenges in text generation due to high-dimensional token-level representations.

Method: Cosmos operates entirely in a compressed, smooth latent space tailored specifically for diffusion, learned using an autoencoder trained simultaneously for token-level reconstruction and alignment with frozen activations from a pretrained language encoder.

Result: Text representations can be compressed by 8× while maintaining generation quality comparable to token-level diffusion models. Increasing the latent sequence length allows Cosmos to surpass both diffusion-based and autoregressive baselines.

Conclusion: Cosmos achieves comparable or superior generation quality while offering more than 2× faster inference.

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [19] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文讨论了MTEB在工程方面的实践，包括持续集成、数据集验证、自动化测试和社区贡献管理，以确保其可重复性和可用性。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的工作已经建立了核心基准测试方法，但本文关注的是确保MTEB持续可重复和可扩展的工程方面。

Method: 本文介绍了维护MTEB的工程方法，包括持续集成管道的设计、数据集完整性验证、自动化测试执行以及评估基准结果的泛化能力。此外，还讨论了处理社区贡献和扩展基准的方法。

Result: 通过这些工程实践，MTEB得以扩展并变得更加全面，同时保持了高质量和相关性。

Conclusion: 本文总结了MTEB在工程方面的实践，这些实践对于确保基准测试的可重复性和可用性至关重要。同时，这些经验为其他基准维护者提供了有价值的见解。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [20] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 本文提出了一种新的模型，该模型通过文本提示动态控制轮流预测。这种方法通过诸如“更快”或“更冷静”的指令实现直观和显式的控制，动态适应对话伙伴和上下文。实验结果表明，所提出的模型提高了预测准确性，并根据文本提示有效改变了轮流说话的时间行为。


<details>
  <summary>Details</summary>
Motivation: 轮换预测模型是语音对话系统和对话机器人的关键组成部分。最近的方法利用基于变压器的架构来连续和实时预测语音活动。然而，现有方法缺乏对轮换预测的动态控制能力。因此，我们需要一种能够通过文本提示动态控制轮换预测的新方法。

Method: 我们提出了一种新的模型，该模型通过文本提示动态控制轮流预测。这种方法通过诸如“更快”或“更冷静”的指令实现直观和显式的控制，动态适应对话伙伴和上下文。该模型基于基于变压器的语音活动投影（VAP）模型，将文本提示嵌入到通道相关的变压器和跨通道变压器中。

Result: 实验结果表明，所提出的模型提高了预测准确性，并根据文本提示有效改变了轮流说话的时间行为。

Conclusion: 实验结果表明，所提出的模型提高了预测准确性，并根据文本提示有效改变了轮流说话的时间行为。

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [21] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出了一种基于句法检索的提示策略，用于自动术语提取，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种自然语言处理任务中取得了显著进展，但它们在自动术语提取中的潜力几乎没有被研究。

Method: 我们提出了一种基于检索的提示策略，在少样本设置中，根据句法而不是语义相似性选择演示。

Result: 在三个专业自动术语提取基准测试中，句法检索提高了F1分数。

Conclusion: 这些发现突显了在将大型语言模型适应于术语提取任务时，句法线索的重要性。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [22] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文介绍了Agent-RewardBench，一个用于评估多模态大语言模型中奖励建模能力的基准。该基准具有多维度、步级奖励评估和高质量数据的特点。实验表明，即使是最先进的模型在代理奖励建模方面也表现有限，需要专门的训练。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏外部反馈，这些代理在自我纠正和泛化方面存在困难。使用奖励模型作为外部反馈是一种有希望的方法，但没有明确的方法来选择奖励模型。因此，迫切需要构建一个针对代理的奖励基准。

Method: 我们提出了Agent-RewardBench，这是一个用于评估MLLMs中奖励建模能力的基准。该基准具有三个关键特征：(1) 多维度和现实世界代理场景评估；(2) 步级奖励评估；(3) 适当难度和高质量数据。

Result: 实验表明，即使是最先进的多模态模型在代理奖励建模方面也表现出有限的性能，这突显了需要专门的训练。

Conclusion: 实验表明，即使是最先进的多模态模型在代理奖励建模方面也表现出有限的性能，这突显了需要专门的训练。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [23] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

TL;DR: 研究发现，统计分类器可以有效检测假文本，即使对于更大的语言模型也是如此，但新的模型架构可能会提高其欺骗性。


<details>
  <summary>Details</summary>
Motivation: 为了探讨模型击败检测器的能力是否会达到瓶颈，我们研究了统计分类器在检测假文本方面的表现。

Method: 研究统计分类器识别经典侦探小说风格的“假文本”的能力。

Result: 在0.5版本的提升中，Gemini表现出生成欺骗性文本的能力增强，而GPT没有表现出这种趋势。

Conclusion: 可靠检测假文本可能对于越来越大的模型仍然可行，尽管新的模型架构可能会提高其欺骗性。

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [24] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

TL;DR: This paper introduces Double-Checker, a framework that improves the reasoning capabilities of slow-thinking LLMs through iterative self-critique and refinement, leading to significant performance improvements on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the ability of slow-thinking LLMs to generate informative critiques and refine prior solutions, which is currently limited.

Method: Double-Checker is a framework that enhances the reasoning capabilities of slow-thinking LLMs by fostering explicit self-critique and iterative refinement of their previous solutions. It is fine-tuned on 1,730 self-critical instances to enable long-CoT LLMs to iteratively critique and refine their outputs during inference.

Result: Double-Checker significantly enhances the reasoning capabilities of long-CoT LLMs. It increases the pass@1 performance on challenging AIME benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs.

Conclusion: Double-Checker demonstrates a promising direction for developing more trustworthy and effective LLMs capable of structured self-critique.

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [25] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 该研究提出了一种检测机制，用于在大型语言模型生成答案之前判断给定查询是否基于提供的文档内容。实验结果表明，轻量级、任务特定的编码器模型在接地性检测方面可以达到与最先进的LLMs相当的准确性，同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在缺乏信息的上下文中回答问题时，常常依赖于无根据的推测或内部知识，这会影响事实的一致性和可信度。因此，需要一种检测机制来确保生成的回答严格基于上下文。

Method: 该研究提出了一种检测机制，用于在大型语言模型生成答案之前判断给定查询是否基于提供的文档内容。这种方法使用了轻量级、任务特定的编码器模型，如RoBERTa和NomicBERT，并在精心策划的数据集上进行了微调。

Result: 实验结果表明，轻量级、任务特定的编码器模型在接地性检测方面可以达到与最先进的LLMs相当的准确性，同时显著降低推理延迟。

Conclusion: 该研究表明，轻量级、任务特定的编码器模型（如RoBERTa和NomicBERT）在经过精心策划的数据集微调后，可以在接地性检测方面达到与最先进的LLMs（如Llama3 8B和GPT4o）相当的准确性，同时显著降低推理延迟。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [26] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 本文探讨了使用文本-only的自回归语言建模方法来提取视觉对话中的提及表达，并发现语言上下文在该任务中起着重要作用，但也指出该任务本质上是多模态的。


<details>
  <summary>Details</summary>
Motivation: 研究语言上下文本身在检测具有可视参照物的提及方面的信息量，以探索文本-only方法的有效性。

Method: 通过调整预训练的大语言模型（LLM）来执行对对话中提及跨度的粗粒度标注，通过下一个标记预测来划分提及跨度的边界。

Result: 即使使用中等大小的LLM、相对较小的数据集和参数高效的微调，文本-only方法仍然有效，表明语言上下文对于该任务的重要性。

Conclusion: 尽管文本-only方法在提取提及表达方面表现出有效性，但本文认为该任务本质上是多模态的，并讨论了单模态方法的根本局限性。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [27] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

TL;DR: 本文提出了GLASS，一个基于Greimas符号正方形的结构化分析框架，以增强大型语言模型进行深入文学分析的能力。该框架在与专家批评和多个LLMs的比较中表现出色，并应用于39部经典作品，产生了原创且高质量的分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在理解和生成文本方面表现出色，但在为具有深刻思想和复杂叙事的作品提供专业文学批评方面存在困难。

Method: 本文提出了GLASS（Greimas文学分析通过符号正方形），一个基于Greimas符号正方形（GSS）的结构化分析框架，以增强大型语言模型（LLMs）进行深入文学分析的能力。

Result: 我们的框架结果与多个作品和LLMs的专家批评相比表现出色。最后，我们将GLASS应用于39部经典作品，产生了原创且高质量的分析，填补了现有研究的空白。

Conclusion: 本研究提供了一个基于AI的文学研究和教育工具，为理解文学参与的认知机制提供了见解。

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [28] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: Omni-RAG is a novel framework designed to enhance the robustness and effectiveness of RAG systems in real-world scenarios by preprocessing user queries through three modules: deep query understanding, intent-aware knowledge retrieval, and reranking and generation.


<details>
  <summary>Details</summary>
Motivation: Real-world live retrieval-augmented generation (RAG) systems face significant challenges when processing user queries that are often noisy, ambiguous, and contain multiple intents. Current systems typically struggle with such complex inputs, as they are often trained or evaluated on cleaner data.

Method: Omni-RAG employs LLM-assisted query understanding to preprocess user inputs through three key modules: (1) Deep Query Understanding and Decomposition, (2) Intent-Aware Knowledge Retrieval, and (3) Reranking and Generation.

Result: Omni-RAG improves the robustness and effectiveness of RAG systems in live, open-domain settings by handling complex and noisy queries through its three key modules.

Conclusion: Omni-RAG aims to bridge the gap between current RAG capabilities and the demands of real-world applications, such as those highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex and noisy queries.

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [29] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于领域知识的LLM框架，用于检测欺骗性对话和概念漂移。通过三个主要组件，系统能够高精度地检测虚假对话并分类漂移的性质。实验结果显示了该方法在高风险NLP应用中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于动态平台上的语言模式不断演变，检测欺骗性对话变得越来越困难。概念漂移（CD）-即语义或主题的变化，会改变交互的上下文或意图，使得准确分类具有挑战性。虽然大型语言模型（LLMs）在自然语言任务中表现出色，但在风险敏感场景中却常常难以处理上下文模糊性和幻觉。

Method: 我们提出了一个领域知识（DK）增强的LLM框架，将预训练的LLM与结构化的任务特定见解相结合，以执行欺诈和概念漂移检测。该架构包括三个主要组件：(1) DK-LLM模块用于检测虚假或欺骗性对话；(2) 漂移检测单元（OCDD）用于确定是否发生了语义变化；(3) 第二个DK-LLM模块用于将漂移分类为良性或欺诈性。

Result: 结果表明，我们的系统能够以高精度检测虚假对话，并有效分类漂移的性质。基于结构化提示，LLaMA实现达到了98%的分类准确率。与其他零样本基线相比，结合领域知识和漂移意识显著提高了性能、可解释性和鲁棒性。

Conclusion: 我们的系统能够以高精度检测虚假对话，并有效分类漂移的性质。基于结构化提示，LLaMA实现达到了98%的分类准确率。与其他零样本基线相比，结合领域知识和漂移意识显著提高了性能、可解释性和鲁棒性。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.

</details>


### [30] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

TL;DR: 本文研究了基础大型语言模型在多语言Text2Cypher任务中的表现，并发现英语表现最佳，其次是西班牙语，最后是土耳其语。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型的进步使得自然语言接口成为可能，但大多数研究仅专注于英语，其他语言的评估有限。本文旨在研究基础LLM在多语言Text2Cypher任务中的表现。

Method: 我们通过将英语问题翻译成西班牙语和土耳其语，同时保留原始的Cypher查询，创建并发布了多语言测试集，以实现公平的跨语言比较。我们使用标准化提示和指标评估了多个基础模型。

Result: 我们的结果显示出一致的性能模式：英语表现最好，其次是西班牙语，最后是土耳其语。我们将其归因于训练数据可用性和语言特征的差异。此外，我们探索了将任务提示翻译成西班牙语和土耳其语的影响。结果表明，评估指标几乎没有变化，这表明提示翻译的影响很小。

Conclusion: 我们的研究结果突显了在多语言查询生成中需要更加包容的评估和开发。未来的工作包括模式本地化和在多种语言上的微调。

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [31] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 本文提出了一种新的偏好对齐框架，用于改进实时语音对话模型。通过创建大规模数据集并利用离线对齐方法，实验结果表明该方法能够提升模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的偏好学习方法主要集中在基于文本的语言模型上，而不适合实时语音交互的复杂性，例如中断和插话等更丰富的动态特性。

Method: 我们提出了一个新颖的偏好对齐框架，用于改进从用户交互中获得的实时对话模型。我们创建了一个大规模的数据集，并利用离线对齐方法微调了一个全双工自回归语音到语音模型。

Result: 广泛的实验表明，对一般对话的反馈可以一致地有效提高语音对话模型，使其产生更真实、更安全和更符合上下文的互动。

Conclusion: 我们的研究强调了在自然实时语音对话系统中，各种动态之间良好校准的平衡的重要性。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [32] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Main category: cs.CL

TL;DR: The paper proposes a modified transformer architecture with a TopK activation function, which provides interpretability similar to sparse autoencoders without requiring post-hoc training. This approach offers a favorable trade-off between model size, computational efficiency, and interpretability, making it a stable and reliable tool for understanding how language models learn and represent concepts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of sparse autoencoders (SAEs) in analyzing and interpreting the activation space of transformer-based language models (LMs). The paper aims to eliminate the need for post-hoc training while providing interpretability comparable to SAEs.

Method: The paper introduces a modification to the transformer architecture that incorporates a TopK activation function at chosen layers, making the model's hidden states equivalent to the latent features of a TopK SAE.

Result: The experiments demonstrate that the sparse representations learned by TopK LMs enable successful steering through targeted neuron interventions and facilitate detailed analysis of neuron formation processes across checkpoints and layers.

Conclusion: TopK LMs offer a favorable trade-off between model size, computational efficiency, and interpretability. They maintain their original capabilities while providing robust interpretability benefits. These features make TopK LMs stable and reliable tools for understanding how language models learn and represent concepts.

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [33] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Main category: cs.CL

TL;DR: 本研究比较了在线和半在线的强化学习方法在微调大型语言模型中的效果，发现它们表现相似且优于离线方法，并通过多任务学习进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 我们研究了强化学习方法在从离线到半在线再到完全在线制度中微调大型语言模型的有效性，特别是在可验证和不可验证任务中的应用。

Method: 我们比较了在线和半在线的直接偏好优化和组奖励策略优化目标，并分析了训练动态和超参数选择策略以获得最佳结果。

Result: 实验结果显示，在线和半在线方法在性能和收敛性方面表现相似，并且都显著优于离线方法。结合可验证和不可验证奖励的多任务学习可以提高两种任务类型的性能。

Conclusion: 我们的实验表明，在线和半在线的直接偏好优化和组奖励策略优化目标在性能和收敛性方面表现相似，并且都显著优于离线方法。此外，结合可验证和不可验证奖励的多任务学习可以提高两种任务类型的性能。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [34] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出了一种通过利用对话未来发展的信号来增强交互式LLM用户参与度的方法。我们采用用户在交互后与对话意图相关的反应作为奖励来对齐交互式LLM，并通过i×MCTS收集数据集，使用直接偏好优化（DPO）进行对齐。实验结果表明，该方法在社交驱动的对话场景中有效增强了用户参与度。


<details>
  <summary>Details</summary>
Motivation: 用户参与度在社交驱动的对话中起着至关重要的作用。然而，先前的工作在优化模型以推理相关知识或规划对话行为流时，用户参与度与知识或对话行为之间的关系是微妙的，并不能保证在社交驱动的对话中获得用户参与度。

Method: 我们采用了一种更直接和相关的用户参与指标，即交互后用户的反应与对话意图相关，作为奖励来对齐交互式LLM。我们开发了一个用户模拟器与目标交互式LLM进行交互，并通过i×MCTS探索用户和交互式LLM系统的互动。我们使用i×MCTS收集包含高质量和低质量经验对的数据集，并通过直接偏好优化（DPO）对交互式LLM进行对齐。

Result: 实验结果表明，我们的方法在两个社交驱动的对话场景（情感支持对话和良好说服）中有效增强了交互式LLM的用户参与度。

Conclusion: 我们的方法在两个社交驱动的对话场景中有效增强了交互式LLM的用户参与度。

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [35] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 本文介绍了skLEP，这是第一个专门用于评估斯洛伐克自然语言理解模型的全面基准，并提供了相关数据、工具包和排行榜以促进研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门针对斯洛伐克自然语言理解模型的全面评估基准，因此我们引入了skLEP来填补这一空白。

Method: 我们编译了新的、原创的斯洛伐克数据集，并仔细翻译了已有的英文NLU资源，以创建skLEP基准。此外，我们还对多种斯洛伐克特定、多语言和英文预训练语言模型进行了系统和广泛的评估。

Result: 我们创建了包含九个不同任务的skLEP基准，并对多种预训练语言模型进行了评估。同时，我们还发布了完整的基准数据、开源工具包和公开排行榜。

Conclusion: skLEP 是一个全面的基准，旨在评估斯洛伐克自然语言理解（NLU）模型，并且我们发布了完整的基准数据、开源工具包和公开排行榜，以促进可重复性和未来的研究。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [36] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型在基准测试中的表现是否真正反映了它们的理解能力，并提出了量化'波坦金理解'的方法。研究发现，这种表面理解在各种模型和任务中普遍存在，并且反映了概念表示中的深层问题。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探讨为什么可以根据LLMs对精心挑选的问题的回答来推断其能力。它试图确定基准测试是否有效，以及LLMs是否以与人类相似的方式误解概念。

Method: 论文首先引入了一个正式的框架来解决这个问题。然后提出了两种量化波坦金存在的方法：一种是在三个领域中使用专门设计的基准测试，另一种是使用提供其普遍性下限的一般程序。

Result: 论文发现，波坦金在各种模型、任务和领域中都很普遍。此外，这些失败不仅反映了错误的理解，还反映了概念表示中的更深层次的内部不一致。

Conclusion: 该论文指出，大型语言模型（LLMs）在基准测试中的成功可能只是表面上的理解，即所谓的'波坦金理解'。这种理解是基于与人类对概念的解释不一致的答案。论文还发现，这些失败不仅反映了错误的理解，还反映了概念表示中的更深层次的内部不一致。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [37] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Main category: cs.CL

TL;DR: 本研究构建了HealthChat-11K数据集，分析了用户与LLM在医疗信息寻求中的交互情况，揭示了潜在的风险并强调了改进LLM医疗支持能力的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地通过交互式聊天机器人从大型语言模型（LLMs）获取医疗信息，这些对话的性质和内在风险仍未被充分探索。

Method: 通过过滤大规模对话AI数据集，构建了一个包含11K真实对话的健康聊天数据集HealthChat-11K，并使用临床驱动的分类法来系统地研究用户在21个不同健康专业领域的交互情况。

Result: 研究揭示了用户寻求医疗信息的方式和原因，包括常见的互动、不完整上下文的实例、情感行为以及可能引发阿谀奉承的互动（如引导性问题）。

Conclusion: 研究揭示了用户在寻求医疗信息时与LLM交互的性质和潜在风险，强调了改进LLM在医疗支持能力上的必要性。

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [38] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 本文提出了数据有效性概念，并通过DELT范式及其组件LQS和FO，证明了在不增加数据规模和模型大小的情况下提升语言模型性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了补充数据效率的研究，本文定义了数据有效性，旨在通过优化训练数据的组织来最大化性能。

Method: 本文提出了一个通用范式DELT，包括数据评分、数据选择和数据排序三个组件，并设计了Learnability-Quality Scoring (LQS)和Folding Ordering (FO)作为实例。

Result: 实验验证了数据有效性在语言模型训练中的效果，表明所提出的DELT方法在不增加数据规模和模型大小的情况下提升了模型性能，特别是LQS和FO的组合效果最显著。

Conclusion: 数据有效性在语言模型训练中是一个有前景的基础领域，可以与数据效率相结合，提升模型性能。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [39] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 本文分析了不同适配器配置对音乐生成模型的影响，发现卷积适配器适合捕捉局部细节，而Transformer适配器更适合保持长距离依赖关系。同时比较了Mustango和MusicGen模型的优缺点。


<details>
  <summary>Details</summary>
Motivation: 由于微调大型音乐生成模型计算成本高，参数高效微调（PEFT）技术成为一种有前景的替代方案，但适配器设计的选择尚不明确。本文旨在探索最佳适配器配置及其原因。

Method: 本文对MusicGen和Mustango两个AI音乐模型在Hindustani Classical和Turkish Makam音乐类型上进行了多种适配器配置的实验分析。

Result: 卷积适配器擅长捕捉局部音乐细节，而Transformer适配器更擅长保持长距离依赖关系。中等规模的适配器（40M参数）在表达能力和质量之间取得平衡。Mustango生成更多样化的输出，但稳定性较差；而MusicGen训练更快、效率更高，但生成内容略显冗余。

Conclusion: 本文通过研究不同适配器配置，揭示了卷积适配器和Transformer适配器在音乐生成模型中的不同优势，并比较了Mustango和MusicGen模型的性能与资源需求。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.AI

TL;DR: 本文研究了基于LLM的代理在多轮对话中是否能够保护用户隐私，并提出了一个新的基准MAGPIE来评估这一能力。实验结果显示，当前模型在隐私保护和协作任务解决方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的代理的普及，隐私保护变得至关重要，因为代理经常访问专有工具和领域特定数据库。然而，现有的基准测试主要评估单次、低复杂度的任务，无法充分评估上下文隐私。

Method: 本文提出了一个名为MAGPIE的基准，包含158个现实生活中的高风险场景，用于评估LLM代理在上下文隐私方面的理解以及协作能力。

Result: 实验结果表明，当前模型（包括GPT-4o和Claude-2.7-Sonnet）在上下文隐私理解上存在不足，错误地将私有数据视为可共享数据的比例分别为25.2%和43.6%。在多轮对话中，这些模型在明确隐私指令下仍会泄露私有信息，比例分别为59.9%和50.5%。此外，多代理系统在71%的场景中未能完成任务。

Conclusion: 当前模型在上下文隐私保护和协作任务解决方面未对齐。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [41] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Main category: cs.AI

TL;DR: 本文提出了一种框架，用于预测语言模型生成的建议在社会系统中的宏观传播，并引入了一个包含100个间接危害场景的数据集来评估模型的长期安全性意识。结果表明，该方法在新数据集和现有安全基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着基于语言模型的代理在高风险社会决策中的影响日益增长，确保其有益影响需要理解其建议的深远影响。

Method: 我们提出了一个概念验证框架，该框架可以预测模型生成的建议如何在宏观层面随时间传播通过社会系统，从而实现更强大的对齐。此外，我们还引入了一个包含100个间接危害场景的数据集，用于测试模型预见看似无害用户提示的不利、不明显结果的能力。

Result: 我们的方法在新数据集上实现了超过20%的改进，并且在现有的安全基准测试中平均胜率超过了70%。

Conclusion: 我们的方法在新数据集上实现了超过20%的改进，并且在现有的安全基准测试中平均胜率超过了70%，表明这是更安全代理的一个有希望的方向。

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [42] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 本研究发现LLMs只能进行浅层因果推理，无法达到人类水平。通过引入新基准CausalProbe-2024和G^2-Reasoner方法，显著提升了LLMs的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前证据表明，尽管LLMs似乎展示了理解上下文因果关系的能力，但它们是否能像人类一样进行真正的因果推理仍不清楚。因此，需要进一步研究LLMs的因果推理能力，并探索如何提升其表现。

Method: 本文通过分析基于Transformer的LLMs的自回归机制，揭示其并非本质上具有因果性。此外，引入了一个名为CausalProbe-2024的新因果问答基准，并提出了G^2-Reasoner方法，结合通用知识和目标导向提示来增强LLMs的因果推理能力。

Result: 实验结果表明，LLMs在CausalProbe-2024上的性能显著下降，说明它们主要进行Level-1因果推理。而G^2-Reasoner方法在新颖和反事实情境中显著提升了LLMs的因果推理能力。

Conclusion: 本研究揭示了大语言模型（LLMs）在因果推理方面存在局限性，仅能进行浅层（Level-1）因果推理，而缺乏真正的类人类（Level-2）因果推理能力。同时，提出了一种新的方法G^2-Reasoner，能够显著提升LLMs的因果推理能力，尤其是在新颖和反事实情境中。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [43] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [44] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: This paper introduces Mind2Web 2, a benchmark for evaluating agentic search systems. The authors propose an Agent-as-a-Judge framework to assess answer correctness and source attribution. They evaluate nine systems and find that the best system achieves 50-70% of human performance while using half the time.


<details>
  <summary>Details</summary>
Motivation: Agentic search has outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. There is a need for a benchmark that can evaluate time-varying and complex answers.

Method: We propose a novel Agent-as-a-Judge framework, which constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution.

Result: The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing great potential.

Conclusion: Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [45] [Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings](https://arxiv.org/abs/2506.21386)
*Ghazal Al-Shwayyat,Omer Nezih Gerek*

Main category: eess.AS

TL;DR: 本研究通过结合传统信号处理技术和深度学习架构，探索了在低资源环境下阿拉伯语方言识别的有效方法。结果显示，MFCC 与 CNN 的组合在准确性和其他指标上优于 DWT 与 RNN 的组合，并提出了未来改进的方向，如采用更大的标注语料库、集成自监督学习技术和探索先进的神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言识别在语音技术中是一个重大挑战，因为阿拉伯语的语言多样性以及对于代表性不足的方言来说大型标注数据集的稀缺性。

Method: 本研究探讨了将传统信号处理技术与深度学习架构相结合的混合建模策略，以解决低资源场景下的问题。开发并评估了两种混合模型：(1) Mel-Frequency Cepstral Coefficients (MFCC) 与卷积神经网络 (CNN) 的组合，以及 (2) 离散小波变换 (DWT) 特征与循环神经网络 (RNN) 的组合。

Result: 实验结果表明，MFCC + CNN 架构表现优越，准确率达到 91.2%，并且具有良好的精确度、召回率和 F1 分数，显著优于 Wavelet + RNN 配置，后者准确率为 66.5%。

Conclusion: 本研究为阿拉伯语方言识别在资源受限环境中的未来发展奠定了坚实的基础。

Abstract: Arabic dialect recognition presents a significant challenge in speech
technology due to the linguistic diversity of Arabic and the scarcity of large
annotated datasets, particularly for underrepresented dialects. This research
investigates hybrid modeling strategies that integrate classical signal
processing techniques with deep learning architectures to address this problem
in low-resource scenarios. Two hybrid models were developed and evaluated: (1)
Mel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural
Network (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with
a Recurrent Neural Network (RNN). The models were trained on a dialect-filtered
subset of the Common Voice Arabic dataset, with dialect labels assigned based
on speaker metadata. Experimental results demonstrate that the MFCC + CNN
architecture achieved superior performance, with an accuracy of 91.2% and
strong precision, recall, and F1-scores, significantly outperforming the
Wavelet + RNN configuration, which achieved an accuracy of 66.5%. These
findings highlight the effectiveness of leveraging spectral features with
convolutional models for Arabic dialect recognition, especially when working
with limited labeled data. The study also identifies limitations related to
dataset size, potential regional overlaps in labeling, and model optimization,
providing a roadmap for future research. Recommendations for further
improvement include the adoption of larger annotated corpora, integration of
self-supervised learning techniques, and exploration of advanced neural
architectures such as Transformers. Overall, this research establishes a strong
baseline for future developments in Arabic dialect recognition within
resource-constrained environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277)
*Qize Yang,Shimin Yao,Weixuan Chen,Shenghao Fu,Detao Bai,Jiaxing Zhao,Boyuan Sun,Bowen Yin,Xihan Wei,Jingren Zhou*

Main category: cs.CV

TL;DR: 本文提出了一个改进的多模态推理方法，通过引入上下文奖励、格式和准确性奖励以及逻辑奖励来解决现有模型在全局上下文理解和捷径问题上的不足，并引入了IntentBench基准测试以评估模型在理解复杂人类意图和情感方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推理模型存在对全局上下文理解不足和捷径问题，这导致模型可能误解多模态上下文或忽略关键线索。

Method: 我们提出了一种基于大语言模型的上下文奖励、格式和准确性奖励，以及逻辑奖励来提高模型的多模态推理能力。此外，我们引入了一个名为IntentBench的全模态基准测试，用于评估模型理解复杂人类意图和情感的能力。

Result: 我们的方法在多个全模态基准测试中表现出色，优于其他开源全模态模型。

Conclusion: 我们的方法在多个全模态基准测试中表现出色，优于其他开源全模态模型。

Abstract: With the rapid evolution of multimodal large language models, the capacity to
deeply understand and interpret human intentions has emerged as a critical
capability, which demands detailed and thoughtful reasoning. In recent studies,
Reinforcement Learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of Large Language Models (LLMs). Nonetheless, the
challenges associated with adapting RL to multimodal data and formats remain
largely unaddressed. In this paper, we identify two issues in existing
multimodal reasoning models: insufficient global context understanding and
shortcut problems. Insufficient context understanding can happen when a model
misinterprets multimodal context, resulting in incorrect answers. The shortcut
problem occurs when the model overlooks crucial clues in multimodal inputs,
directly addressing the query without considering the multimodal information.
To tackle these issues, we emphasize the necessity for the model to reason with
a clear understanding of the global context within multimodal inputs. This
global context understanding can effectively prevent the model from overlooking
key multimodal cues and ensure a thorough reasoning process. To ensure the
accurate interpretation of multimodal context information, we implement a
context reward judged by a large language model, alongside format and accuracy
rewards. Additionally, to improve complex reasoning capability, we employ the
LLM to assess the logical reward, determining whether the reasoning process
successfully integrates multimodal information with logical methods. We also
introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating
models in understanding complex human intentions and emotions. Our proposed
method demonstrates advanced performance across multiple omni-modal benchmarks
compared to other open-source omni-modal models.

</details>


### [47] [Logios : An open source Greek Polytonic Optical Character Recognition system](https://arxiv.org/abs/2506.21474)
*Perifanos Konstantinos,Goutsos Dionisis*

Main category: cs.CV

TL;DR: 本文提出了一种针对希腊多音文本的OCR系统，通过结合卷积层和循环层的优势，提高了识别的准确性和效率，并提供了开源库和学术使用的OCR平台。


<details>
  <summary>Details</summary>
Motivation: 传统OCR方法在处理希腊多音文本时存在局限性，因此需要一种更准确和高效的OCR系统。

Method: 本文利用卷积层进行特征提取，利用循环层进行序列学习，以解决希腊多音脚本的独特挑战。

Result: 本文提出的OCR系统在准确性和效率方面都有显著提升，并提供了开源库和学术使用的OCR平台。

Conclusion: 本文提出了一个专门用于准确识别和数字化希腊多音文本的光学字符识别（OCR）系统，该系统通过结合卷积层和循环层的优势，克服了传统OCR方法的局限性，并提供了开源库和学术使用的OCR平台。

Abstract: In this paper, we present an Optical Character Recognition (OCR) system
specifically designed for the accurate recognition and digitization of Greek
polytonic texts. By leveraging the combined strengths of convolutional layers
for feature extraction and recurrent layers for sequence learning, our system
addresses the unique challenges posed by Greek polytonic scripts. This approach
aims to overcome the limitations of traditional OCR methods, offering
significant improvements in accuracy and efficiency. We release the underlying
model as an open-source library and make our OCR platform available for
academic use.

</details>


### [48] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 本文介绍了HalluSegBench，这是一个新的基准，用于评估视觉定位中的幻觉，并发现视觉驱动的幻觉比标签驱动的更常见。


<details>
  <summary>Details</summary>
Motivation: 现有的分割幻觉评估协议主要关注标签或文本幻觉，而没有操纵视觉上下文，限制了它们诊断关键失败的能力。

Method: 引入了HalluSegBench，这是第一个专门设计用于通过反事实视觉推理的角度评估视觉定位中幻觉的基准。

Result: HalluSegBench的实验结果揭示了视觉驱动的幻觉比标签驱动的幻觉更为普遍，模型常常持续出现错误的分割。

Conclusion: 实验表明，视觉驱动的幻觉比标签驱动的幻觉更为普遍，这突显了需要通过反事实推理来诊断接地的真实性。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 本文研究了微调中的记忆问题，发现LoRA微调方法可以有效降低记忆风险，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 研究微调中记忆的影响，特别是LoRA微调方法的潜在风险。

Method: 重新审视微调中的记忆问题，并使用基于相似性的记忆度量方法进行分析。

Result: LoRA在减少记忆风险方面表现优于全微调，同时保持了良好的任务性能。

Conclusion: LoRA显著降低了记忆风险，同时保持了强大的任务性能。

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [50] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为SharpZO的混合优化方法，用于改进零阶优化（ZO）视觉语言模型（VLM）的微调性能。该方法通过两个阶段的优化过程，即sharpness-aware ES阶段和细粒度局部搜索阶段，实现了更高的准确性和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的BP-free微调方法通常依赖于高方差的进化策略（ES）或零阶（ZO）优化，且往往无法达到令人满意的性能。因此，需要一种更有效的BP-free微调方法。

Method: 提出了一种混合Sharpness-aware Zeroth-order优化（SharpZO）方法，该方法通过sharpness-aware warm-up训练来增强ZO VLM微调的性能。SharpZO具有两阶段优化过程：一个sharpness-aware ES阶段，用于全局探索和平滑损失景观以构建强大的初始化，然后是通过稀疏ZO优化进行细粒度局部搜索。整个优化仅依赖于前向传递。

Result: 在CLIP模型上的详细理论分析和广泛实验表明，SharpZO显著提高了准确性并加快了收敛速度，相对于最先进的仅前向方法，平均增益达到7%。

Conclusion: SharpZO显著提高了准确性并加快了收敛速度，相对于最先进的仅前向方法，平均增益达到7%。

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [51] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 本文提出了一种使用知识图谱生成高质量指令数据的新方法，以提高大型语言模型（LLMs）使用工具的能力。


<details>
  <summary>Details</summary>
Motivation: 以往的方法主要依赖LLMs生成指令数据，但数据质量不足。为了提高LLMs的问题解决能力和应用范围，需要更高质量的指令数据。

Method: 从知识图谱中提取各种查询路径，并将其转换为广泛的用户查询。将实体间的关系转化为可操作的工具，并解析每个查询的路径为详细解决方案步骤，从而生成高质量的指令数据。

Result: 实验表明，在少量合成数据上进行微调可以显著提高LLMs的工具使用能力和整体能力。

Conclusion: 使用知识图谱生成高质量指令数据是一种有效提升LLMs工具使用能力的方法。

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [52] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer架构，通过动态跳过中间层来提高效率。虽然目标是减少计算需求并促进多级表示层次结构，但在所研究的规模上，该方法并未在验证交叉熵和估计FLOPs之间的权衡上优于密集基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对单个模块（例如，专家混合层）或独立地跳过层。然而，可解释性研究显示，Transformer的中间层表现出更大的冗余性，并且早期层将信息聚合到标记位置。受这些见解的启发，我们提出了一个新颖的架构，可以动态跳过中间的可变数量的层。

Method: 我们提出了一种新的架构，该架构动态地从中间向外跳过可变数量的层。具体来说，一个学习到的门控机制根据输入决定是否绕过对称的中央块区域，并且一个门控注意力机制防止后续标记访问跳过的标记位置。残差范数通过“三明治”或“perilayernorm”方案进行控制，并通过自适应正则化损失控制门控稀疏性。

Result: 我们旨在减少“简单”标记的计算需求，并可能促进多级表示层次结构的出现，但就所研究的规模而言，我们的方法在验证交叉熵和估计FLOPs之间的权衡上没有取得改进。

Conclusion: 我们的方法在所研究的规模上未能在验证交叉熵和估计FLOPs之间的权衡上优于具有较少层的密集基线。

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [53] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 本文提出了一种新的高效微调方法，通过仅对复杂数据进行推理，显著提高了性能并减少了数据需求。


<details>
  <summary>Details</summary>
Motivation: 一般用途的大规模语言模型（LLMs）通常通过监督微调（SFT）来提高特定领域的性能。通过在昂贵的调用和大量数据的成本下蒸馏大型模型的思维链，可以获得更好的结果。我们需要一种更高效的微调方法。

Method: 我们提出了一种新的高效微调蓝图，该蓝图仅对通过熵识别的复杂数据进行推理。具体来说，我们在两个小型开放模型上将训练数据按单个标记答案熵分为复杂性类别，并通过SFT和蒸馏微调大型语言模型。

Result: 我们的管道显著优于标准的SFT方法（平均准确率0.55 vs 0.43），并且在使用62%更少数据的情况下提供了与蒸馏相当的性能（平均准确率0.55）。

Conclusion: 我们的方法在使用更少数据的情况下，显著优于标准的SFT方法，并且与蒸馏性能相当。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [54] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Main category: cs.LG

TL;DR: DiLoCoX is a low-communication large-scale decentralized cluster training framework that significantly improves the scale and speed of pre-training large language models. It achieves a 357x speedup in distributed training over a 1Gbps network while maintaining model convergence.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether training can be conducted on slow networks to leverage the power of decentralized clusters for models exceeding 100 billion parameters.

Method: DiLoCoX combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme.

Result: DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network, achieving a 357x speedup in distributed training compared to vanilla AllReduce while maintaining negligible degradation in model convergence.

Conclusion: DiLoCoX is the first decentralized training framework successfully applied to models with over 100 billion parameters, achieving a 357x speedup in distributed training while maintaining negligible degradation in model convergence.

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [55] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Main category: cs.LG

TL;DR: 本文提出了LPR，一种新的路由框架，通过聚类视角实现MoE系统的负载平衡，同时保持下游性能。


<details>
  <summary>Details</summary>
Motivation: 当前的MoE系统存在严重的负载不平衡问题，只有少数专家在训练和推理中被持续激活，导致模型容量和计算资源的显著浪费。

Method: 通过聚类视角重新审视专家路由，提出了一种新的路由框架Latent Prototype Routing (LPR)。

Result: LPR将专家负载的基尼系数从0.70降低到平均0.035，并将最小最大专家负载比从1e-6提高到0.70。

Conclusion: LPR在多个开源MoE模型上实现了接近完美的负载平衡，同时保持了下游性能。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [56] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Main category: cs.LG

TL;DR: 本文提出了一种名为ScalaBL的可扩展贝叶斯低秩适应方法，通过在r维子空间中进行贝叶斯推断，使模型能够在保持高性能的同时显著减少额外参数数量，并成功扩展到更大的LLM。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）容易产生错误信息和校准不良，因此量化这些模型的不确定性至关重要，尤其是在高风险领域，如自主性和医疗保健。然而，现有的基于贝叶斯深度学习的方法难以扩展到更大的LLMs。

Method: 通过在r维子空间中进行贝叶斯推断，将LoRA参数重新用作投影矩阵，从而将子空间中的样本映射到LLM的完整权重空间中。

Result: ScalaBL实现了与最先进方法相当的性能，同时仅需要约1000个额外参数，并且可以扩展到迄今为止最大的贝叶斯LLM。

Conclusion: ScalaBL能够实现与最先进方法相当的性能，同时仅需要约1000个额外参数，并且可以扩展到迄今为止最大的贝叶斯LLM。

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [cs.LG](#cs.LG) [Total: 9]
- [eess.AS](#eess.AS) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: 本文介绍了PlotCraft基准测试和PlotCraftor模型，以解决大型语言模型在复杂可视化生成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在生成复杂可视化方面的能力尚未得到充分评估和开发。

Method: 引入了PlotCraft基准测试和SynthVis-30K数据集，并开发了PlotCraftor模型进行复杂可视化代码生成。

Result: 对23个领先的LLM进行了全面评估，发现它们在处理复杂的可视化任务时存在明显的性能缺陷。PlotCraftor在多个基准测试中表现与领先的专有方法相当。

Conclusion: PlotCraftor在复杂数据可视化任务中表现出色，甚至在困难任务上性能提升了超过50%。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: 我们提出了ProtoMBTI，这是一个与认知一致的MBTI推断框架，在基于LLM的管道中实现了原型理论。通过构建平衡、质量控制的语料库，并微调一个轻量级编码器来学习判别嵌入和标准化人格原型库，ProtoMBTI在多个基准测试中表现出色，并展示了良好的跨数据集泛化能力。结果表明，将推理过程与心理原型推理对齐可以提高基于文本的人格建模的准确性、可解释性和迁移性。


<details>
  <summary>Details</summary>
Motivation: 从文本中进行人格识别通常被归类为硬标签分类，这掩盖了人类人格判断的等级和原型特性。我们提出ProtoMBTI，以实现与心理原型推理一致的MBTI推断框架。

Method: 我们提出了ProtoMBTI，这是一个与认知一致的MBTI推断框架，在基于LLM的管道中实现了原型理论。首先，我们通过LLM引导的多维增强（语义、语言、情感）构建了一个平衡、质量控制的语料库。接下来，我们微调一个轻量级（<=2B）编码器以学习判别嵌入并标准化人格原型库。在推理时，我们为查询帖子检索前k个原型，并执行检索--重用--修订--保留循环：模型通过基于提示的投票聚合原型证据，在出现不一致时进行修订，并在正确预测后保留样本以不断丰富原型库。

Result: 在Kaggle和Pandora基准测试中，ProtoMBTI在四个MBTI二分法和完整的16种类型任务上都优于基线，并表现出强大的跨数据集泛化能力。

Conclusion: 我们的结果表明，将推理过程与心理学原型推理对齐可以提高基于文本的人格建模的准确性、可解释性和迁移性。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 本文提出了一种残差流解码器框架，用于探测语言模型中的长期规划信息，并展示了其在小型模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 语言模型在执行更长的时间范围任务方面变得越来越强大，但理解激活的方法仍然局限于测试特定概念或标记。

Method: 我们开发了一个残差流解码器框架，以探测模型激活的段落级和文档级计划。

Result: 我们测试了多种方法，发现可以解码相当于5个以上标记的未来上下文的信息。

Conclusion: 这些结果为更好地监控语言模型和理解它们如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [4] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文挑战了传统的使用下一个标记预测来训练大语言模型的方法，提出了一种更有效的方式，并在三种任务中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 优化大语言模型的训练性能仍然是一个重要的挑战，特别是在提高模型性能的同时保持计算成本。

Method: 本文挑战了传统的使用下一个标记预测（NTP）来训练大语言模型的方法，提出了一种更有效的方式。

Result: 本文在三种任务中研究了所提出解决方案的影响：算术、文本多标签分类和自然语言生成。

Conclusion: 本文提出了一个有原则的方法来优化大语言模型的训练，提升了模型性能和对目标令牌选择策略的理论理解。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [5] [Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2511.00222)
*Marwa Abdulhai,Ryan Cheng,Donovan Clay,Tim Althoff,Sergey Levine,Natasha Jaques*

Main category: cs.CL

TL;DR: 本文提出了一种统一的框架，用于评估和改进大型语言模型（LLM）生成对话中的角色一致性。通过定义三个自动度量标准，并利用这些度量作为奖励信号，应用多轮强化学习对LLM进行微调，从而显著减少了不一致现象。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在模拟人类用户时容易偏离角色，出现矛盾或不适当的行为，因此需要一种方法来评估和提高角色一致性。

Method: 本文提出了三个自动度量标准：提示到行的一致性、行到行的一致性以及问答一致性，并使用这些度量作为奖励信号，通过多轮强化学习对LLM进行微调。

Result: 该方法减少了超过55%的不一致性，使得模拟用户更加连贯和忠实。

Conclusion: 本文的方法有效提高了LLM生成对话中的角色一致性，为交互式AI代理的训练和评估提供了更好的基础。

Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in
interactive settings such as therapy, education, and social role-play. While
these simulations enable scalable training and evaluation of AI agents,
off-the-shelf LLMs often drift from their assigned personas, contradict earlier
statements, or abandon role-appropriate behavior. We introduce a unified
framework for evaluating and improving persona consistency in LLM-generated
dialogue. We define three automatic metrics: prompt-to-line consistency,
line-to-line consistency, and Q&A consistency, that capture different types of
persona drift and validate each against human annotations. Using these metrics
as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs
for three user roles: a patient, a student, and a social chat partner. Our
method reduces inconsistency by over 55%, resulting in more coherent and
faithful simulated users.

</details>


### [6] [AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding](https://arxiv.org/abs/2511.00265)
*Arman Anwar,Zefang Liu*

Main category: cs.CL

TL;DR: 本文介绍了AgentBnB，一种基于浏览器的Backdoors & Breaches游戏的重新设想，结合了大型语言模型队友和Bloom对齐的检索增强协作者（C2D2）。该系统通过提供按需、认知目标提示来扩展精选语料库。在单人玩家试点中，参与者更倾向于使用基于代理的版本，并认为它更具可扩展性，尽管在简单知识测验中出现了天花板效应。这些初步发现表明，增强大型语言模型的TTX可以提供轻量级、可重复的练习，而无需传统练习的后勤负担。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全桌面演练（TTXs）提供有价值的培训，但通常是剧本化的、资源密集型的且难以扩展。

Method: 引入AgentBnB，这是一种基于浏览器的Backdoors & Breaches游戏的重新设想，结合了大型语言模型队友和Bloom对齐的检索增强协作者（C2D2）。系统将精选语料库扩展为事实性、概念性、程序性和元认知片段，提供按需、认知目标提示。经过工程设计的代理使用逐渐消失的脚手架梯子。

Result: 在四名研究生的单人玩家试点中，参与者表示更愿意使用基于代理的版本，而不是实体卡片套件，并认为它更具可扩展性，但在简单的知识测验中出现了天花板效应。

Conclusion: 这些初步发现表明，增强大型语言模型的TTX可以提供轻量级、可重复的练习，而无需传统练习的后勤负担。计划的扩展包括多人模式、基于遥测的指导和与更大群体的比较研究。

Abstract: Traditional cybersecurity tabletop exercises (TTXs) provide valuable training
but are often scripted, resource-intensive, and difficult to scale. We
introduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches
game that integrates large language model teammates with a Bloom-aligned,
retrieval-augmented copilot (C2D2). The system expands a curated corpus into
factual, conceptual, procedural, and metacognitive snippets, delivering
on-demand, cognitively targeted hints. Prompt-engineered agents employ a
scaffolding ladder that gradually fades as learner confidence grows. In a
solo-player pilot with four graduate students, participants reported greater
intention to use the agent-based version compared to the physical card deck and
viewed it as more scalable, though a ceiling effect emerged on a simple
knowledge quiz. Despite limitations of small sample size, single-player focus,
and narrow corpus, these early findings suggest that large language model
augmented TTXs can provide lightweight, repeatable practice without the
logistical burden of traditional exercises. Planned extensions include
multi-player modes, telemetry-driven coaching, and comparative studies with
larger cohorts.

</details>


### [7] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了IL-PCR语料库，用于同时处理法规检索和先例检索，并通过基于LLM的重新排序方法提高了性能。


<details>
  <summary>Details</summary>
Motivation: 目前的研究分别处理法规检索和先例检索，但两者本质上是相关的，因此需要一个共同的测试平台来更好地利用它们之间的依赖关系。

Method: 本文提出了IL-PCR语料库，并使用了多种基线模型进行实验，包括基于GNN的集成模型和基于LLM的重新排序方法。

Result: 实验表明，基于LLM的重新排序方法在两个任务上都取得了最佳性能。

Conclusion: 本文提出了IL-PCR语料库，为同时处理法规检索和先例检索提供了共同的测试平台，并通过基于LLM的重新排序方法提高了性能。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [8] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了一种新的预训练方案POSESTITCH-SLT，通过使用基于语言模板的句子生成技术来提高手语翻译的效果。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模的句子对齐数据集，手语翻译仍然是一个具有挑战性的任务。

Method: 我们提出了POSESTITCH-SLT，这是一种受基于语言模板的句子生成技术启发的新预训练方案。

Result: 在How2Sign和iSign两个手语数据集上进行的翻译比较显示，当考虑模板生成的句子对时，简单的基于变压器的编码器-解码器架构优于之前的艺术方法。

Conclusion: 结果表明，在低资源手语设置中，基于模板的合成监督是有效的。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [9] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: Factorization Memory 是一种新型的 RNN 架构，结合了稀疏记忆激活与高性能，适用于短上下文和长上下文的语言建模任务。


<details>
  <summary>Details</summary>
Motivation: 为了在保持高效的同时，提升 RNN 在长上下文场景中的表现，并实现稀疏记忆激活与高性能的结合。

Method: Factorization Memory 基于 Mamba-2 构建，利用并行计算进行训练，同时在推理过程中保持恒定的计算和内存复杂度。还开发了一种稀疏形式的 Factorization Memory，仅在每个步骤更新部分递归状态，同时保持其密集版本的强性能。

Result: Factorization Memory 在短上下文任务中表现与 Transformer 相当，在长上下文任务中表现出更好的泛化能力。同时，其稀疏版本在保持性能的同时提高了效率。

Conclusion: Factorization Memory 是一种高效的 RNN 架构，能够在短上下文语言建模任务中达到与 Transformer 模型相当的性能，并在长上下文场景中表现出优越的泛化能力。此外，它首次成功结合了稀疏记忆激活与短上下文和长上下文设置中的竞争力性能。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [10] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 本文研究了因果语言建模目标的反转不变性，指出其可能限制模型捕捉语言方向性依赖的能力，并提出未来应关注时间不对称性的预训练方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言中存在方向性依赖（如语音、形态或因果关系），而当前的预训练目标是方向盲的，这可能限制了模型的表现。

Method: 通过形式化因果语言建模目标的结构属性——反转不变性，分析标准CLM预训练的对称性问题。

Result: 反转不变性解释了为何在反向文本上训练的模型也能达到与正向文本相当的性能。

Conclusion: 当前预训练目标的对称性可能限制了模型捕捉语言中方向性依赖的能力，因此需要考虑通过时间不对称性来改进预训练方法。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [11] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: 本文引入了LingGym，一个用于评估LLMs元语言推理能力的新基准，并展示了结合结构化语言提示可以提高推理表现。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估LLMs是否能够泛化语言推理 across 低资源语言和训练期间未见过的结构，而不仅仅是专注于特定的下游任务。

Method: 本文引入了LingGym，一个新基准，使用互惠式逐行注释文本（IGT）和从18种类型多样的参考语法中提取的语法描述来评估LLMs的元语言推理能力。我们提出了一个受控评估任务：单词-注释推理，其中模型必须根据上下文推断缺失的单词和注释，使用不同级别的语言信息（例如，注释、语法解释、翻译）。

Result: 结果表明，结合结构化的语言提示可以一致地提高所有模型的推理表现。

Conclusion: 本文展示了LLMs在类型学导向的语言分析和低资源语言记录中的潜力和当前限制。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [12] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本文介绍了推理轨迹生成的任务，并构建了一个手动标注的调试问题数据集。基于大语言模型的解决方案能够生成高质量的推理轨迹和苏格拉底对话。


<details>
  <summary>Details</summary>
Motivation: 在苏格拉底调试中，导师引导学生自己发现并修复错误，而不是直接提供错误修复。大多数新手程序员的错误是由于对编程概念的误解造成的。因此，需要一种方法来生成推理轨迹，以帮助学生识别和更新他们的错误信念。

Method: 本文提出了推理轨迹生成的任务，并构建了一个手动标注的调试问题数据集。然后描述了基于大语言模型的生成推理轨迹和苏格拉底对话的解决方案。

Result: 大规模的LLM评估显示，前沿模型可以生成高达91%正确的推理轨迹和98.7%有效的对话回合。

Conclusion: 本文介绍了推理轨迹生成的任务以及一个手动标注的调试问题数据集，并描述了基于LLM的生成推理轨迹和苏格拉底对话的解决方案。大规模的LLM评估表明，前沿模型可以生成高达91%正确的推理轨迹和98.7%有效的对话回合。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [13] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 研究显示，当前AI生成文本检测器在面对迭代改写文本时存在严重漏洞，需要改进检测架构。


<details>
  <summary>Details</summary>
Motivation: 研究为什么迭代改写文本能够逃避针对AI生成文本识别的检测系统，并提出一个基准来评估检测器的鲁棒性。

Method: 通过内在机制分析，揭示了迭代改写创建了一个中间清洗区域，该区域具有语义偏移但保留了生成模式，并引入了PADBen基准来评估检测器对两种改写攻击场景的鲁棒性。

Result: 评估了11种最先进的检测器，发现检测器在处理作者身份混淆问题时失败，而在处理抄袭规避问题时成功。

Conclusion: 当前检测方法无法有效处理中间清洗区域，需要在检测架构上进行根本性的改进。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [14] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: 本文介绍了MedRECT，一个跨语言的医学错误修正基准，用于评估大型语言模型在医学文本中的错误检测和修正能力，并展示了推理模型在这一任务上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学应用中显示出越来越多的前景，但它们在检测和纠正临床文本中的错误的能力仍然未被充分评估，特别是在英语之外的语言中。

Method: 引入MedRECT，这是一个跨语言基准（日语/英语），将医学错误处理分为三个子任务：错误检测、错误定位（句子提取）和错误修正。

Result: 研究发现，推理模型在错误检测和句子提取方面显著优于标准架构；跨语言评估显示从英语到日语的性能差距为5-10%；针对LoRA微调在错误修正性能上取得了不对称的提升；微调后的模型在结构化医学错误修正任务上超过了人类专家的表现。

Conclusion: MedRECT是第一个全面的跨语言医学错误修正基准，为开发跨语言的安全医疗LLM提供了可重复的框架和资源。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [15] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的即插即用方法G2，以提高大型语言模型的输出多样性而不损害质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在输出多样性方面存在关键限制，这影响了需要多样化输出的任务。现有的解决方案如温度缩放会损害输出质量。

Method: G2是一种无需训练的即插即用方法，它使用一个基础生成器和两个引导器，通过解码干预来鼓励基于原始查询的更多样化输出。

Result: 实验表明，G2能够有效提高输出多样性，同时保持输出质量。

Conclusion: G2有效地提高了输出多样性，同时保持了多样性和质量之间的最佳平衡。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [16] [Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks](https://arxiv.org/abs/2511.00476)
*Ghazal Kalhor,Afra Mashhadi*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型的记忆效应如何影响合作网络，并揭示了其在不同学科和地区中的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在基于网络的搜索中越来越深入，它们生成基于记忆数据的摘要研究的能力引入了新的挑战，因此需要评估其对合作网络的影响。

Method: 该研究评估了三种主流模型（DeepSeek R1、Llama 4 Scout 和 Mixtral 8x7B）的记忆效应，并分析了这些记忆驱动的输出在不同学科和世界地区之间的变化。

Result: 全球分析显示，高度引用的研究人员受到偏袒，但这种模式并不一致。某些学科（如临床医学）和地区（包括非洲的部分地区）表现出更平衡的代表性，这表明这些地区的训练数据可能反映了更大的公平性。

Conclusion: 该研究强调了在学术发现中部署大型语言模型（LLMs）所带来的风险和机遇，指出需要关注其记忆效应可能带来的偏见问题。

Abstract: Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search
and recommendation platforms at their core. While this shift unlocks powerful
new scientometric tools, it also exposes critical fairness and bias issues that
could erode the integrity of the information ecosystem. Additionally, as LLMs
become more integrated into web-based searches for scholarly tools, their
ability to generate summarized research work based on memorized data introduces
new dimensions to these challenges. The extent of memorization in LLMs can
impact the accuracy and fairness of the co-authorship networks they produce,
potentially reflecting and amplifying existing biases within the scientific
community and across different regions. This study critically examines the
impact of LLM memorization on the co-authorship networks. To this end, we
assess memorization effects across three prominent models, DeepSeek R1, Llama 4
Scout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across
academic disciplines and world regions. While our global analysis reveals a
consistent bias favoring highly cited researchers, this pattern is not
uniformly observed. Certain disciplines, such as Clinical Medicine, and
regions, including parts of Africa, show more balanced representation, pointing
to areas where LLM training data may reflect greater equity. These findings
underscore both the risks and opportunities in deploying LLMs for scholarly
discovery.

</details>


### [17] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本研究创建了首个大规模的Bhili-Hindi-English平行语料库，并评估了多语言大型语言模型在低资源机器翻译中的表现，展示了其在低资源场景中的潜力。


<details>
  <summary>Details</summary>
Motivation: 印度的语言多样性给机器翻译带来了重大挑战，特别是对于像Bhili这样的非主流部落语言，缺乏高质量的语言资源。

Method: 本研究引入了Bhili-Hindi-English Parallel Corpus (BHEPC)，这是首个涵盖Bhili、Hindi和英语的平行语料库，包含110,000个精心整理的句子。此外，评估了多种专有和开源多语言大型语言模型（MLLMs）在英/印地语与Bhili之间的双向翻译任务中的表现，并使用上下文学习研究了多语言LLMs的生成翻译能力。

Result: 评估结果表明，微调后的NLLB-200压缩版600M模型表现最佳，突显了多语言模型在低资源场景中的潜力。此外，通过上下文学习研究了多语言LLMs的生成翻译能力，并评估了跨领域泛化性能并量化了分布偏差。

Conclusion: 本研究填补了关键的资源空白，并促进了全球低资源和边缘化语言的包容性自然语言处理技术的发展。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [18] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文研究了数据集大小对DP文本重写机制的影响，发现数据集大小在评估这些机制中起着关键作用。


<details>
  <summary>Details</summary>
Motivation: 以往的研究在评估DP文本隐私化机制时，常常忽略了数据集大小的影响。本文首次引入这一因素进行评估。

Method: 我们在大规模数据集上设计了效用和隐私测试，这些数据集具有动态分割大小，并对不同大小的数据集进行了测试，最多达到一百万条文本。

Result: 我们的研究结果表明，随着数据集大小的增加，隐私-效用权衡发生了变化。

Conclusion: 数据集大小在评估DP文本重写机制中起着关键作用；此外，这些发现呼吁在DP NLP中进行更严格的评估程序，并为DP NLP在实际和大规模应用中的未来提供了见解。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [19] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: ToM 是一种新的树导向 MapReduce 框架，用于长上下文推理，能够有效解决现有方法在逻辑连贯性和长距离依赖方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长上下文时存在逻辑连贯性差和无法捕捉长距离依赖的问题，因此需要一种新的框架来解决这些问题。

Method: 提出了一种名为 ToM 的树导向 MapReduce 框架，利用长文档的固有层次结构，通过分层语义解析构建 DocTree，并进行自底向上的聚合。

Result: 实验结果表明，ToM 在 70B+ 大型语言模型上显著优于现有方法，实现了更好的逻辑连贯性和长上下文推理能力。

Conclusion: ToM 显著优于现有的分而治之框架和检索增强生成方法，在逻辑连贯性和长上下文推理方面表现更好。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [20] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG通过修剪冗余知识并优化检索过程，提高了RAG的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM内部知识的扩展，外部语料库与LLM之间的知识冗余问题日益严重，导致密集检索的工作量增加，并且冗余知识反而影响RAG性能。

Method: 提出Mastery-Score指标来识别RAG语料库中的冗余知识并进行修剪，同时引入Query Router和Noise-Tolerant Tuning来避免无关文档的干扰，从而提高LLM对内部知识的利用。

Result: Zero-RAG能够修剪Wikipedia语料库30%，加速检索阶段22%，同时保持RAG性能不受影响。

Conclusion: Zero-RAG能够在不损害RAG性能的情况下，通过修剪Wikipedia语料库30%并加速检索阶段22%，有效解决知识冗余问题。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [21] [Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations](https://arxiv.org/abs/2511.00514)
*Birat Poudel,Satyam Ghimire,Er. Prakash Chandra Prasad*

Main category: cs.CL

TL;DR: 本研究探讨了在资源有限的农村地区使用离线对话模型的可能性，通过微调DialoGPT并在合成数据集上进行训练，证明了其在医疗对话中的有效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 由于大型对话模型通常依赖于互联网连接和云基础设施，而在农村地区可能无法访问，因此需要探索适用于资源有限环境的对话代理。

Method: 本研究对DialoGPT进行了微调，这是一个可以在离线状态下运行的轻量级生成对话模型，并使用了一个合成构建的医生-患者互动数据集进行训练，该数据集涵盖了尼泊尔农村地区常见的十种疾病。

Result: 微调后的模型产生了连贯、上下文相关且医学上适当的回应，展示了对症状、疾病背景和共情交流的理解。

Conclusion: 研究结果表明，紧凑且具备离线功能的对话模型在低资源医疗环境中具有适应性和有效性，为未来农村医疗对话人工智能提供了有希望的方向。

Abstract: Conversational agents are increasingly being explored to support healthcare
delivery, particularly in resource-constrained settings such as rural Nepal.
Large-scale conversational models typically rely on internet connectivity and
cloud infrastructure, which may not be accessible in rural areas. In this
study, we fine-tuned DialoGPT, a lightweight generative dialogue model that can
operate offline, on a synthetically constructed dataset of doctor-patient
interactions covering ten common diseases prevalent in rural Nepal, including
common cold, seasonal fever, diarrhea, typhoid fever, gastritis, food
poisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being
trained on a limited, domain-specific dataset, the fine-tuned model produced
coherent, contextually relevant, and medically appropriate responses,
demonstrating an understanding of symptoms, disease context, and empathetic
communication. These results highlight the adaptability of compact,
offline-capable dialogue models and the effectiveness of targeted datasets for
domain adaptation in low-resource healthcare environments, offering promising
directions for future rural medical conversational AI.

</details>


### [22] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了基于Transformer的模型中的性别偏见，并提出了一种有效缓解方法，显著降低了不同代词对的性别偏见分数，同时保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于编码器基于变压器的模型在各种语言任务中表现出强大的性别偏见，因此需要研究和缓解这种偏见。

Method: 本文引入了一个新的度量标准MALoR来评估模型在填充被屏蔽的标记时的概率，以量化偏见程度，并提出了一种通过反事实数据增强生成性别平衡数据集进行继续预训练的缓解方法。

Result: 实验结果显示，在不同的代词对中，性别偏见分数显著降低。例如，在BERT-base中，'he-she'的偏见分数从1.27降至0.08，'his-her'的偏见分数从2.51降至0.36。其他模型也观察到了类似的改进。

Conclusion: 本文提出了一种有效的缓解性别偏见的方法，该方法在不损害模型在下游任务上的性能的情况下显著降低了不同代词对的性别偏见分数。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [23] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为WordSaladChopper的轻量级组件，用于减少大型推理模型中的无用自我重复标记，从而节省解码预算并提高用户体验。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）常常受到输出标记高成本的瓶颈限制。我们发现这些标记中有一部分是无用的自我重复，即“word salad”，它们消耗解码预算但没有价值。

Method: 我们观察到LRMs在陷入循环时具有自我意识，<\n\n>标记后的隐藏状态表现出可检测word salad行为的模式。通过一个单层线性分类器可以实时检测这些行为，然后通过简单的剪切和再生提示实现显著的长度节省。

Result: 通过使用WSC，可以实现显著的长度节省，同时保持最小的质量损失。此外，我们的代码已在GitHub上公开。

Conclusion: 我们的工作提出了WordSaladChopper (WSC)，这是一个轻量级、即插即用的LRM组件，它通过仅移除语义冗余的标记来最小化对推理轨迹的干扰。我们认为，考虑到其低开销和显著的节省效果，WSC或类似组件应该是所有以用户体验为中心的LRM应用的必备组件。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [24] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于预训练语言模型的情感分析框架CISEA-MRFE，在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在涉及细微情感线索、领域转移和不平衡情感分布的场景中表现不佳，因为缺乏语义基础、泛化能力差以及对主导情感类别的偏见。

Method: 提出了一种基于PLM的框架CISEA-MRFE，结合了上下文指令（CI）、语义增强增强（SEA）和多精炼特征提取（MRFE）。

Result: CISEA-MRFE在四个基准数据集上表现出色，相对改进最高达4.6%（IMDb）、6.5%（Yelp）、30.3%（Twitter）和4.1%（Amazon）。

Conclusion: 实验结果验证了我们方法在不同领域情感分类中的有效性和泛化能力。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [25] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的攻击方法ISA，能够有效绕过大型语言模型的安全机制，并展示了现有防御方法的不足。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的安全性，以应对潜在的恶意攻击。

Method: 本文引入了ISA（Intent Shift Attack），通过意图转换生成可能被大型语言模型误认为是无害请求的攻击。

Result: 实验表明，ISA在攻击成功率上比直接有害提示提高了70%以上，甚至在仅使用ISA模板重新表述的良性数据上进行微调后，成功率接近100%。

Conclusion: 本文揭示了大型语言模型在意图推理方面的基本挑战，并强调了需要更有效的防御措施。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [26] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: 本文介绍了FlashEVA，一种高效的Transformer注意力机制，能够提高推理效率并减少内存消耗，同时提供灵活性以适应不同应用场景。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在自然语言处理中取得了显著成果，但其内存需求（特别是保持完整上下文）在推理过程中带来了挑战。因此，需要一种更高效的解决方案。

Method: 本文提出了FlashEVA，这是一种高效的EVA（通过控制变量进行有效注意）实现，并展示了如何微调Transformer以适应FlashEVA注意机制。

Result: FlashEVA在推理过程中实现了高达6.7倍的吞吐量提升和5倍的峰值GPU内存使用降低。此外，该方法允许通过调整超参数来控制吞吐量和准确性的权衡。

Conclusion: 本研究代表了在更高效和可适应的基于Transformer的模型用于推理方面的重大进展。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [27] [OpenSIR: Open-Ended Self-Improving Reasoner](https://arxiv.org/abs/2511.00602)
*Wai-Chung Kwan,Joshua Ong Jun Leang,Pavlos Vougiouklis,Jeff Z. Pan,Marco Valentino,Pasquale Minervini*

Main category: cs.CL

TL;DR: OpenSIR is a self-play framework that enables LLMs to generate and solve novel problems without external supervision, achieving significant improvements in mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for LLM reasoning through reinforcement learning depend on annotated datasets or external verifiers, which may limit models' ability to surpass human-level performance. Self-play offers a promising alternative, but existing methods cannot learn open-endedly.

Method: OpenSIR is a self-play framework where an LLM learns to generate and solve novel problems by alternating teacher and student roles without external supervision. It optimises for both difficulty and diversity to generate novel problems.

Result: Starting from a single trivial seed problem, OpenSIR substantially improves instruction models: Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to 34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on GSM8K.

Conclusion: OpenSIR achieves open-ended learning through co-evolving teacher-student roles that adaptively calibrate difficulty and drive diverse exploration, progressing autonomously from basic to advanced mathematics.

Abstract: Recent advances in large language model (LLM) reasoning through reinforcement
learning rely on annotated datasets for verifiable rewards, which may limit
models' ability to surpass human-level performance. While self-play offers a
promising alternative, existing approaches depend on external verifiers or
cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner
(OpenSIR), a self-play framework where an LLM learns to generate and solve
novel problems by alternating teacher and student roles without external
supervision. To generate novel problems, OpenSIR optimises for both difficulty
and diversity, rewarding problems that challenge appropriately while exploring
distinct concepts, enabling open-ended mathematical discovery. Starting from a
single trivial seed problem, OpenSIR substantially improves instruction models:
Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to
34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on
GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through
co-evolving teacher-student roles that adaptively calibrate difficulty and
drive diverse exploration, progressing autonomously from basic to advanced
mathematics.

</details>


### [28] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: 本文提出了一种名为SpecDiff-2的新框架，用于同时解决推测解码的两个主要瓶颈。通过利用离散扩散作为非自回归起草者，并开发新技术来校准离散扩散起草者与自回归验证者，从而实现更高的速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的推测解码方法受到两个基本瓶颈的限制：(1) 草稿过程中的自回归依赖性限制了并行性，以及(2) 由于草稿和验证模型之间的不匹配导致的草稿标记的频繁拒绝。

Method: SpecDiff-2利用离散扩散作为非自回归起草者来解决第一个瓶颈，并开发了新技术来校准离散扩散起草者与自回归验证者，以解决第二个瓶颈。

Result: 实验结果表明，SpecDiff-2在全面的基准测试套件中实现了每秒标记数的平均提高+55%，并且在标准解码上获得了平均5.5倍的速度提升，而没有准确性损失。

Conclusion: SpecDiff-2在推理、编码和数学基准测试中达到了新的最先进水平，提高了每秒的标记数，并且在不损失准确性的情况下获得了平均5.5倍的速度提升。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [29] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 研究分析了大型语言模型在概率场景中的不确定性量化问题，发现其token级概率与理论分布存在偏差。


<details>
  <summary>Details</summary>
Motivation: 可靠的风险量化对于确保大型语言模型在决策支持和其他知识密集型应用中的可信使用至关重要。现有的基于token logits的方法可能不足以满足概率场景的需求。

Method: 通过GPT-4.1和DeepSeek-Chat评估模型在涉及概率的提示下的响应，测量响应的有效性和token级输出概率与理论概率的对齐程度。

Result: 两个模型在所有提示场景中都实现了完美的域内响应准确性，但它们的token级概率和熵值始终与相应的理论分布不一致。

Conclusion: 模型的token级概率和熵值与理论分布存在显著偏差，这表明需要更可靠的不确定性量化方法。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [30] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 该研究通过计算分析展示了法国侦探小说中侦探原型的演变，从次要角色到中心角色，并在二战后变得更加复杂。


<details>
  <summary>Details</summary>
Motivation: 探索法国侦探小说中侦探原型的演变。

Method: 使用定量方法和角色级嵌入进行计算分析。

Result: 研究证明，侦探形象从次要叙事角色演变为古典侦探故事中的中心角色和'推理机器'，并在二战后因硬汉传统的影响变得更加复杂。

Conclusion: 该研究展示了法国侦探小说中的侦探原型在150年文学中的演变，并表明监督模型能够捕捉到侦探原型的统一性。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [31] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文介绍了XNationQA数据集，用于评估多语言大语言模型的文化素养。研究发现，模型在不同语言中对文化特定事实的可访问性存在显著差异，且在西方语言中表现更好，但这并不意味着对西方国家的文化更熟悉。此外，模型在跨语言知识迁移方面能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言问答基准在覆盖多种语言的同时，未能考虑信息中的区域多样性，往往以西方为中心，这导致了在公平评估多语言模型对来自不同地理区域的事实信息理解方面的重大差距。

Method: 引入了XNationQA数据集，用于评估多语言大语言模型的文化素养。在XNationQA上对八种标准多语言LLM进行了基准测试，并使用两种新的转移度量进行评估。

Result: 分析揭示了模型在不同语言中对文化特定事实的可访问性存在显著差异。模型在西方语言中表现更好，但这并不一定意味着对西方国家的文化更熟悉，这是反直觉的。此外，模型在跨语言知识迁移方面能力有限，特别是在开源模型中尤为明显。

Conclusion: 研究发现，多语言模型在不同语言中对文化特定事实的可访问性存在显著差异，且模型在西方语言中的表现并不一定意味着对西方国家的文化更熟悉。此外，模型在跨语言知识迁移方面能力有限，尤其是在开源模型中更为明显。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [32] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次对多种语言中的监狱突破和防御进行了系统评估，发现高资源语言在标准查询下更安全，但容易受到对抗性攻击，简单的防御措施可能有效，但依赖于语言和模型。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究监狱突破和防御在不同语言中的跨语言泛化能力，并探索更安全的大语言模型方法。

Method: 本文对十种语言中的监狱突破和防御进行了系统评估，使用了六个大语言模型在HarmBench和AdvBench上进行测试。

Result: 高资源语言在标准查询下更安全，但更容易受到对抗性查询的攻击；简单的防御措施可能有效，但依赖于语言和模型。

Conclusion: 本文指出，需要建立语言感知和跨语言的安全基准来提高大语言模型的安全性。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [33] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种改进的Native Sparse Attention方法，通过交替使用局部和全局注意力机制，提高了长上下文建模效果，并减少了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长上下文时存在局限性，需要更有效的传播长距离依赖关系的方法。

Method: 我们提出了针对Native Sparse Attention (NSA)的改进，通过在层之间交替使用局部（滑动窗口）和全局（压缩、选择性）注意力，而不是使用固定模式，从而更有效地传播长距离依赖关系。此外，我们通过引入Latent Attention对NSA的分支进行了进一步优化，其中滑动窗口分支增强了Multi-head Latent Attention (MLA)，而压缩和选择性分支采用了Group-head Latent Attention (GLA)。

Result: 这些改进减少了KV缓存内存50%的同时，提高了模型的常识推理和长文本理解能力。实验表明，我们的方法在340M到1.3B参数的模型上表现优异。

Conclusion: 我们的方法在常识推理和长文本理解任务中与全注意力和原生稀疏注意力相当或超过。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [34] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种名为TriCon-Fair的对比学习框架，用于消除社会偏见，通过解耦损失来避免正负耦合，并在保持语言模型能力的同时减少歧视性输出。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法独立处理有偏和无偏样本，忽略了它们之间的相互关系，导致隐藏的负-正耦合，使得残留的社会偏见持续存在。

Method: 本文引入了TriCon-Fair，这是一个对比学习框架，采用解耦损失，结合三元组和语言建模项以消除正负耦合。

Result: 实验结果表明，TriCon-Fair在减少歧视性输出方面优于现有的去偏基线，同时保持强大的下游性能。

Conclusion: 本文提出的TriCon-Fair为敏感的自然语言处理应用提供了一个实用且符合伦理的解决方案。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [35] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文介绍了一个新的评估套件，用于系统评估大型语言模型推理中的知识基础。该套件包含三个关键组件：主要知识收集、基于知识的评估指标以及一个轻量级的评估者大语言模型。评估结果表明，该套件能有效识别知识缺失或错误应用的情况，并可用于改进模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然逐步推理已成为大型语言模型处理复杂任务的标准方法，但这种方法引发了一个基本问题：如何验证大型语言模型的推理是否准确地基于知识？

Method: 我们引入了一个新的评估套件，系统地评估中间推理的知识基础。该框架包括三个关键组件：(1) 主要知识收集，一个大规模的原子知识库，对于推理至关重要。基于该收集，我们提出了(2) 基于知识的评估指标，旨在衡量模型回忆和应用前提知识的能力。这些指标由我们的(3) 评估者大语言模型计算，这是一个轻量级模型，经过优化以实现成本效益高且可靠的指标计算。

Result: 我们的评估套件在识别缺失或错误应用的知识元素方面表现出色，为揭示大型语言模型中的基本推理缺陷提供了关键见解。此外，我们展示了这些指标如何集成到偏好优化中，展示了基于知识的评估的进一步应用。

Conclusion: 我们的评估套件在识别缺失或错误应用的知识元素方面表现出色，为揭示大型语言模型中的基本推理缺陷提供了关键见解。此外，我们展示了这些指标如何集成到偏好优化中，展示了基于知识的评估的进一步应用。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [36] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一种多模态文档检索模型，通过创新的预训练目标和评分机制，提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态文档检索中往往复制文本-only检索的技术，这限制了它们的表现。

Method: ColMate利用了一种基于OCR的预训练目标、一种自监督的掩码对比学习目标以及一种与多模态文档结构和视觉特征更相关的后期交互评分机制。

Result: ColMate在ViDoRe V2基准上取得了3.61%的提升，并展示了对领域外基准更强的泛化能力。

Conclusion: ColMate在ViDoRe V2基准上比现有的检索模型提高了3.61%，展示了对领域外基准更强的泛化能力。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [37] [The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses](https://arxiv.org/abs/2511.00924)
*Jianzhou Yao,Shunchang Liu,Guillaume Drui,Rikard Pettersson,Alessandro Blasimme,Sara Kijewski*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型在医学诊断场景中的理解和共情能力，发现它们在适应患者状况方面表现良好，但生成内容过于复杂且存在情感共情偏差，需要系统校准以确保公平的患者沟通。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估大型语言模型在医学诊断场景中生成解释和指导的能力，特别是其在理解和共情方面的表现。

Method: 研究评估了两种领先的大型语言模型在医学诊断场景中的表现，使用可读性指标作为理解性的代理，并通过大型语言模型作为评判者评分与人类评估进行比较。

Result: 研究结果表明，大型语言模型能够适应社会人口变量和患者状况，但生成的内容过于复杂，并表现出有偏见的情感共情，导致可及性和支持不均。

Conclusion: 研究结果表明，大型语言模型在适应社会人口变量和患者状况方面表现出色，但同时也生成了过于复杂的内容，并表现出有偏见的情感共情，导致可及性和支持不均。这强调了系统校准的必要性，以确保公平的患者沟通。

Abstract: Large language models (LLMs) show promise for supporting clinicians in
diagnostic communication by generating explanations and guidance for patients.
Yet their ability to produce outputs that are both understandable and
empathetic remains uncertain. We evaluate two leading LLMs on medical
diagnostic scenarios, assessing understandability using readability metrics as
a proxy and empathy through LLM-as-a-Judge ratings compared to human
evaluations. The results indicate that LLMs adapt explanations to
socio-demographic variables and patient conditions. However, they also generate
overly complex content and display biased affective empathy, leading to uneven
accessibility and support. These patterns underscore the need for systematic
calibration to ensure equitable patient communication. The code and data are
released: https://github.com/Jeffateth/Biased_Oracle

</details>


### [38] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在七种印度主要语言中的推理和自我评估能力，发现高表现模型往往过于自信，而低表现模型更具自我意识，揭示了多语言推理的不足。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索大型语言模型在非英语语言中的文化基础推理能力，特别是在印度主要语言中的表现。

Method: 研究引入了一个多语言谜题数据集，结合传统谜题和情境重构变体，并评估了五种大型语言模型在七种提示策略下的表现。

Result: 研究发现Gemini 2.5 Pro在整体表现上最佳，但少样本方法仅带来微小提升，且准确率在不同语言间差异显著。此外，高表现模型如Gemini 2.5 Pro过于自信，而低表现模型如LLaMA 4 Scout则更具有自我意识。

Conclusion: 研究指出多语言推理存在明显差距，并强调模型不仅需要有效推理，还需要认识到自身的局限性。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [39] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种易于到困难的增强框架，以在不精确条件下提供可靠的监督，并在多种实际场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器生成文本检测方法隐式地假设标签为“黄金标准”，但本文揭示了机器生成文本检测中的边界模糊性，表明传统的训练范式是不精确的。此外，人类认知的局限性和检测器的超级智能使得不精确学习变得普遍且不可避免。

Method: 本文提出了一种易于到困难的增强框架，通过使用一个针对相对较简单的长文本检测任务的监督器来增强更具有挑战性的目标检测器。

Result: 广泛的实验结果表明，该框架在跨LLM、跨领域、混合文本和改写攻击等多种实际场景中表现出显著的检测效果。

Conclusion: 本文提出了一种易于到困难的增强框架，以在不精确条件下提供可靠的监督。实验结果表明该框架在多种实际场景中具有显著的检测效果。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [40] [MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL](https://arxiv.org/abs/2511.01008)
*Haolin Yang,Jipeng Zhang,Zhitao He,Yi R. Fung*

Main category: cs.CL

TL;DR: MARS-SQL 是一种多智能体框架，结合了任务分解和交互式强化学习，用于自然语言到 SQL 的翻译。它在复杂查询中表现出色，具有高执行准确性。


<details>
  <summary>Details</summary>
Motivation: 自然语言到 SQL 的翻译在复杂查询中仍然困难，这些查询通常需要环境交互和自我纠正。

Method: MARS-SQL 是一种多智能体框架，结合了任务分解和交互式强化学习 (RL)。它由三个专门的代理组成：接地代理、生成代理和验证代理。生成代理通过多轮 RL 策略进行训练，并采用 ReAct 风格的 Think-Act-Observe 循环来迭代生成想法、执行 SQL 操作并根据执行反馈修订策略。验证代理通过建模验证作为下一个标记预测任务来选择最佳轨迹。

Result: MARS-SQL 在 BIRD 开发集上实现了 77.84% 的执行准确性，在 Spider 测试集上实现了 89.75% 的执行准确性。

Conclusion: MARS-SQL 是一种有效的 SQL 生成方法，能够实现鲁棒和准确的查询生成。

Abstract: Translating natural language to SQL remains difficult for complex queries.
Such queries often need environmental interaction and self-correction. To
address this, we introduce MARS-SQL, a novel multi-agent framework that
combines principled task decomposition and interactive reinforcement learning
(RL). Our system comprises three specialized agents: a Grounding Agent for
schema linking, a Generation Agent for query generation, and a Validation Agent
for final selection. The core of our framework is the Generation agent, which
is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe
loop, the agent iteratively generates thoughts, executes SQL actions against a
live database, and revises its strategy based on execution feedback, enabling
dynamic, stateful reasoning and self-correction. At inference time, we generate
multiple interaction trajectories to explore diverse reasoning paths. The
Validation agent, then selects the optimal trajectory by modeling verification
as a next-token prediction task and choosing the solution with the highest
generation probability. This structured workflow pipelines specialized agents.
It combines interactive RL for generation with generative modeling for
verification. The approach proves highly effective for robust and accurate SQL
generation. Experiments show that MARS-SQL achieves state-of-the-art Execution
Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our
code is available at https://github.com/YangHaolin0526/MARS-SQL.

</details>


### [41] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 本文提出IF-CRITIC，一种高效的LLM评论者，用于评估指令中的约束遵循情况，并在指令遵循优化中实现显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有指令遵循的评估模型存在成本高和评估不可靠的问题，需要一种更高效和可靠的评估方法。

Method: 提出IF-CRITIC，通过检查表生成器分解指令并生成约束检查表，利用多阶段批判过滤机制收集高质量的批判训练数据，并采用基于约束级别的偏好优化方法训练IF-CRITIC。

Result: IF-CRITIC在评估性能上超越了Deepseek-R1和o4-mini等强基线模型，并且在指令遵循优化中实现了显著的性能提升，同时计算开销较低。

Conclusion: IF-CRITIC能够提供高效且可靠的约束遵循评估，并在指令遵循优化中实现显著的性能提升，同时计算开销较低。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [42] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一种端到端的强化学习框架，通过小规模LLM与大规模LLM协作来提高问题解决效果。


<details>
  <summary>Details</summary>
Motivation: 由于大多数用户无法提供准确有效的提示与LLM交互，从而限制了LLM的表现，因此需要一种方法来解决这个问题。

Method: 提出了一种端到端的强化学习框架Prompt-R1，使用小规模LLM与大规模LLM协作，以更好地解决问题。

Result: 在多个公共数据集上的实验表明，Prompt-R1在任务中显著优于基线模型。

Conclusion: Prompt-R1显著优于基线模型，并且提供了一个可插拔的框架，支持与各种大规模LLM进行推理和训练。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [43] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI is a conversational platform that combines the natural-language fluency of open-source large language models with real-time access to authoritative oceanographic data from NOAA. It produces reproducible responses and data visualizations, advancing transparency, reproducibility, and trust in AI-enabled decision support for ocean-related applications.


<details>
  <summary>Details</summary>
Motivation: Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified 'hallucinations' undermining scientific rigor.

Method: OceanAI integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by NOAA. Each query triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations.

Result: In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring.

Conclusion: OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans.

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [44] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat is a conversational system that makes environmental analytics accessible to policymakers, researchers, and citizens by integrating data from various sources and allowing them to ask natural language questions.


<details>
  <summary>Details</summary>
Motivation: Decision makers in India struggle to turn dispersed data into decisions due to existing tools requiring expertise and providing static dashboards, leaving key policy questions unresolved.

Method: VayuChat is a conversational system that answers natural language questions on air quality, meteorology, and policy programs, and responds with both executable Python code and interactive visualizations.

Result: VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring stations, state-level demographics, and National Clean Air Programme (NCAP) funding records into a unified interface powered by large language models.

Conclusion: VayuChat makes data science accessible to policymakers, researchers, and citizens by enabling complex environmental analytics through simple conversations.

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [45] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 本研究提出了一种基于公开指南的临床推理评估数据集，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏标准化的基准来评估基于指南的临床推理，因此需要一个有效的数据集来评估大型语言模型在医疗领域的表现。

Method: 本研究利用GPT创建了一个从公开指南中衍生出的数据集，包含真实的患者场景和临床问题，并对一系列最近流行的LLM进行了基准测试。

Result: 本研究创建的数据集能够支持对LLM临床效用和指南遵循性的系统评估，并展示了其有效性。

Conclusion: 本研究引入了一个经过验证的数据集，用于评估基于指南的临床推理，并展示了该数据集的有效性。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [46] [HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models](https://arxiv.org/abs/2511.01066)
*Stephan Oepen,Nikolay Arefev,Mikko Aulamo,Marta Bañón,Maja Buljan,Laurie Burchell,Lucas Charpentier,Pinzhen Chen,Mariya Fedorova,Ona de Gibert,Barry Haddow,Jan Hajič,Jindrič Helcl,Andrey Kutuzov,Zihao Li,Risto Luukkonen,Bhavitvya Malik,Vladislav Mikhailov,Amanda Myntti,Dayyán O'Brien,Lucie Poláková,Sampo Pyysalo,Gema Ramírez Sánchez,Janine Siewert,Pavel Stepachev,Jörg Tiedemann,Teemu Vahtola,Fedor Vitiugin,Tea Vojtěchová,Jaume Zaragoza*

Main category: cs.CL

TL;DR: 本文介绍了一个大规模、多语言、高质量的文本数据集项目，包含30万亿个标记的数据，以及相关的模型和基准测试。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个大规模、高质量且丰富标注的多语言文本数据集，以支持语言模型的研究和开发。

Method: 本文通过网络爬取不同来源的数据，构建了一个完整的开源管道，包括文档选择、文本提取、语言识别、去重、标注和最终筛选。此外，还进行了数据质量探测，并训练和评估了一系列单语模型和并行文本数据集。

Result: 本文提供了30万亿个标记的多语言数据集，这是目前可用的最大多语言LLM预训练数据集。同时，还提供了针对九种欧洲语言的全面基准测试，以及一系列单语模型和并行文本数据集。

Conclusion: 本文介绍了旨在为几乎200种语言提供开放、大规模、高质量且丰富标注文本数据集的持续性项目。这些数据集是目前可用的最大多语言LLM预训练数据集，经过了文档选择、文本提取、语言识别、去重、标注和最终筛选等步骤。作者还报告了数据质量的探测结果，并提供了针对九种欧洲语言的全面基准测试，以及一系列单语模型和并行文本数据集。

Abstract: We present an ongoing initiative to provide open, very large, high-quality,
and richly annotated textual datasets for almost 200 languages. At 30 trillion
tokens, this is likely the largest generally available multilingual collection
of LLM pre-training data. At 30 trillion tokens, this is likely the largest
generally available multilingual collection of LLM pre-training data. These
datasets are derived from web crawls from different sources and accompanied
with a complete, open-source pipeline for document selection from web archives,
text extraction from HTML, language identification for noisy texts, exact and
near-deduplication, annotation with, among others, register labels, text
quality estimates, and personally identifiable information; and final selection
and filtering. We report on data quality probes through contrastive and
analytical statistics, through manual inspection of samples for 24 languages,
and through end-to-end evaluation of various language model architectures
trained on this data. For multilingual LLM evaluation, we provide a
comprehensive collection of benchmarks for nine European languages, with
special emphasis on natively created tasks, mechanisms to mitigate prompt
sensitivity, and refined normalization and aggregation of scores. Additionally,
we train and evaluate a family of 57 monolingual encoder-decoder models, as
well as a handful of monolingual GPT-like reference models. Besides the
monolingual data and models, we also present a very large collection of
parallel texts automatically mined from this data, together with a novel
parallel corpus synthesized via machine translation.

</details>


### [47] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文研究了罗马尼亚预训练语料库的特征和覆盖范围，并分析了它们与英语数据的不同之处。通过在精心标注的罗马尼亚文本上训练轻量级多任务模型，我们能够进行多级过滤以生成高质量的预训练数据集。实验结果显示了罗马尼亚和英语数据中主题的显著趋势，并证明了过滤数据的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）最近变得非常流行，通常在许多任务上与人类能力相媲美或超越。训练LLMs的关键因素是高质量数据的可用性和整理。对于代表性不足的语言，数据质量尤其关键，因为高质量语料库很少。

Method: 我们通过在精心标注的罗马尼亚文本上训练轻量级多任务模型，能够分析并进行多级过滤（例如，教育价值、主题、格式）以生成高质量的预训练数据集。

Result: 我们的实验显示了罗马尼亚和英语数据中主题的显著趋势，并证明了通过改进LLM预训练性能在多个基准测试中过滤数据的有效性。

Conclusion: 我们的实验展示了罗马尼亚和英语数据中主题的显著趋势，并证明了通过改进LLM预训练性能在多个基准测试中过滤数据的有效性。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [48] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer是一个新的基准数据集，用于评估时间序列证据的事实验证系统，旨在解决现有数据集的不足，并提供高质量的标注和基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集在结构化证据、判决理由或合成声明方面存在不足，因此需要一个更高质量的数据集来评估时间序列推理系统。

Method: TSVer通过LLM辅助的多步骤注释过程创建，包含来自38个事实检查组织的真实声明和400个时间序列。每个声明都带有时间框架、判决和理由。

Result: TSVer具有287个真实声明和400个时间序列，标注质量高，且即使最先进的模型如Gemini-2.5-Pro在判决准确性和理由评分上也面临挑战。

Conclusion: TSVer是一个新的基准数据集，用于事实验证，专注于时间序列证据的时序和数值推理。它提高了标注质量，并为验证时间序列证据的声明提供了基线。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [49] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: This paper introduces MicroRemed, a benchmark for evaluating LLMs in microservice remediation, and proposes ThinkRemed, a multi-agent framework that enhances remediation performance through iterative reasoning and system reflection.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for microservice remediation still rely on human-crafted prompts from SREs, with LLMs merely converting textual instructions into executable code. To advance research in this area, we aim to create a benchmark and framework that enable LLMs to directly generate executable Ansible playbooks from diagnosis reports.

Method: We introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end microservice remediation, and propose ThinkRemed, a multi-agent framework that emulates the reflective and perceptive reasoning of SREs.

Result: Experimental results show that MicroRemed presents substantial challenges to current LLMs, while ThinkRemed improves end-to-end remediation performance through iterative reasoning and system reflection.

Conclusion: MicroRemed presents substantial challenges to current LLMs, while ThinkRemed improves end-to-end remediation performance through iterative reasoning and system reflection.

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [50] [Learning When to Quit in Sales Conversations](https://arxiv.org/abs/2511.01181)
*Emaad Manzoor,Eva Ascarza,Oded Netzer*

Main category: cs.CL

TL;DR: 本文研究了销售人员在高产量外呼销售中的动态筛选决策，并开发了一个基于生成语言模型的停止代理，以减少失败通话的时间并提高销售效率。


<details>
  <summary>Details</summary>
Motivation: 销售人员经常面临是否继续对话或放弃以追求下一个潜在客户的问题。然而，人们对这些决策是如何做出的、是否高效以及如何改进知之甚少。

Method: 我们将动态筛选决策形式化为一个最优停止问题，并开发了一个基于生成语言模型的序列决策代理——停止代理，它通过模仿回顾性推断出的最优停止策略来学习何时退出对话。

Result: 当应用于一家大型欧洲电信公司的通话时，我们的停止代理将花费在失败通话上的时间减少了54%，同时几乎保留了所有销售；重新分配节省的时间使预期销售增加了高达37%。

Conclusion: 我们的研究结果表明，人工智能算法可以纠正认知有限的人类决策，提高销售团队的效率。

Abstract: Salespeople frequently face the dynamic screening decision of whether to
persist in a conversation or abandon it to pursue the next lead. Yet, little is
known about how these decisions are made, whether they are efficient, or how to
improve them. We study these decisions in the context of high-volume outbound
sales where leads are ample, but time is scarce and failure is common. We
formalize the dynamic screening decision as an optimal stopping problem and
develop a generative language model-based sequential decision agent - a
stopping agent - that learns whether and when to quit conversations by
imitating a retrospectively-inferred optimal stopping policy. Our approach
handles high-dimensional textual states, scales to large language models, and
works with both open-source and proprietary language models. When applied to
calls from a large European telecommunications firm, our stopping agent reduces
the time spent on failed calls by 54% while preserving nearly all sales;
reallocating the time saved increases expected sales by up to 37%. Upon
examining the linguistic cues that drive salespeople's quitting decisions, we
find that they tend to overweight a few salient expressions of consumer
disinterest and mispredict call failure risk, suggesting cognitive bounds on
their ability to make real-time conversational decisions. Our findings
highlight the potential of artificial intelligence algorithms to correct
cognitively-bounded human decisions and improve salesforce efficiency.

</details>


### [51] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: 本文介绍了DebateBias-8K，一个新的多语言辩论式基准，用于揭示叙事偏见在现实生成设置中如何出现。研究发现，所有模型都再现了根深蒂固的刻板印象，即使在安全对齐后也是如此。偏见在低资源语言中更为严重，表明当前的对齐方法无法在全球范围内推广。作者发布了DebateBias-8K基准和分析框架，以支持下一代多语言偏见评估和更安全、文化包容的模型对齐。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）广泛用于开放式交流，但大多数偏见评估仍然依赖于英语分类任务。我们需要一个能够揭示叙事偏见在现实生成设置中出现的新多语言辩论式基准。

Method: 我们引入了DebateBias-8K，这是一个新的多语言辩论式基准，旨在揭示叙事偏见如何在现实生成设置中出现。我们的数据集包括8,400个结构化的辩论提示，涵盖四个敏感领域：妇女权利、经济社会发展、恐怖主义和宗教，覆盖七种语言。使用四种旗舰模型（GPT-4o、Claude 3、DeepSeek和LLaMA 3），我们生成并自动分类了超过100,000个回答。

Result: 结果表明，所有模型都再现了根深蒂固的刻板印象，尽管进行了安全对齐：阿拉伯人与恐怖主义和宗教（>=95%）密切相关，非洲人与经济社会“落后”（最多<=77%）相关，西方群体被一致描述为现代或进步。在低资源语言中，偏见急剧增加，揭示了主要在英语中训练的对齐无法在全球范围内推广。

Conclusion: 我们的研究结果突显了多语言公平性中的持续分歧：当前的对齐方法减少了明显的毒性，但无法防止开放性情境中的偏见输出。我们发布了DebateBias-8K基准和分析框架，以支持下一代多语言偏见评估和更安全、文化包容的模型对齐。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [52] [ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction](https://arxiv.org/abs/2511.01188)
*Lvhua Wu,Xuefeng Jiang,Sheng Sun,Tian Wen,Yuwei Wang,Min Liu*

Main category: cs.CL

TL;DR: 本文提出了一个名为 ZoFia 的两阶段零样本假新闻检测框架，通过结合实体重要性量化、关键词选择算法和多 LLM 交互系统，显著提升了假新闻检测的效果。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在处理快速演变的新闻流时存在知识覆盖范围有限和生成幻觉内容的问题，因此需要一种更可靠和具有泛化能力的假新闻检测方法。

Method: 提出了一种两阶段的零样本假新闻检测框架 ZoFia，包括 Hierarchical Salience 和 SC-MMR 算法，以及多 LLM 交互系统进行多视角协作分析和对抗辩论。

Result: 在两个公开数据集上的实验表明，ZoFia 明显优于现有的零样本基线和大多数少样本方法。

Conclusion: ZoFia 显著优于现有的零样本基线和大多数少样本方法，展示了其在假新闻检测中的有效性。

Abstract: The rapid spread of fake news threatens social stability and public trust,
rendering its detection an imperative research priority. Although large
language models (LLMs) excel at numerous natural language processing tasks with
their remarkable contextual understanding and extensive prior knowledge, the
time-bounded knowledge coverage and tendency for generating hallucination
content reduce their reliability when handling fast-evolving news streams.
Furthermore, models trained on existing static datasets also often lack the
generalization needed for emerging news topics. To address these challenges, we
propose ZoFia, a novel two-stage zero-shot fake news detection framework.
First, we introduce Hierarchical Salience to quantify the importance of
entities in the news content, and propose the SC-MMR algorithm to effectively
select an informative and diverse set of keywords that serve as queries for
retrieving up-to-date external evidence. Subsequently, a multi LLM interactive
system, in which each agent assumes a distinct role, performs multi-view
collaborative analysis and adversarial debate over the news text and its
related information, and finally produces an interpretable and robust judgment.
Comprehensive experiments on two public datasets demonstrate that ZoFia
obviously outperforms existing zero-shot baselines and most of few-shot
methods. Our codes will be open-sourced to facilitate related communities.

</details>


### [53] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony是一种无需人工监督或辅助模型的框架，通过在原始问题和其改写版本中保持答案稳定性，实现了无标签测试时间设置中的先进性能和高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 标准方法如多数投票往往退化为虚假但流行的答案，因此需要一种更可靠的学习信号构造方法。

Method: Self-Harmony框架基于一个简单的直觉：正确答案在原始问题和其改写版本中应保持稳定。它通过使用一个模型的两个互补角色（Solver生成答案，Reframer改写输入）来实现这一点。此外，提出了一种伪标签方法，通过调和平均值聚合原始和改写视图的答案频率，从而避免偏向于依赖视图的虚假答案。

Result: Self-Harmony在多个推理基准上实现了最先进的结果，在30个设置中的28个中排名第一。此外，它展示了前所未有的鲁棒性，所有实验中都没有训练失败。

Conclusion: Self-Harmony在无标签的测试时间设置中实现了最先进的结果，并展示了前所未有的鲁棒性，所有实验中都没有训练失败，证明了其稳定性和可靠性。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [54] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架DEER，用于检测机器生成文本，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在面对领域转移时性能显著下降，因此需要一种新的方法来解决这个问题。

Method: 提出了一种新的框架，称为两阶段的解耦混合专家（DEER）架构，该架构通过解耦的混合专家模块和基于强化学习的路由机制来捕捉领域特定和领域通用的MGT模式。

Result: DEER在五个领域内和五个领域外的基准数据集上进行了广泛的实验，结果表明它在F1分数和准确率方面都有显著提升。

Conclusion: DEER框架在检测机器生成文本方面表现出色，能够有效应对领域转移的问题，并且在多个基准数据集上优于现有的最先进的方法。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [55] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本研究引入了AraFinNews，这是目前最大的公开可用的阿拉伯语金融新闻数据集，用于评估领域特定语言理解和生成。实验结果表明，领域适应的模型在生成更忠实和连贯的摘要方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨领域特定性对使用大型语言模型（LLMs）进行阿拉伯语金融文本摘要的影响，并提供一个用于评估金融领域语言理解和生成的基准数据集。

Method: 使用transformer-based模型（包括mT5、AraT5和领域适应的FinAraT5）评估了领域特定预训练对事实准确性、数值可靠性和与专业报道风格的一致性的影响。

Result: 领域适应的模型在处理定量和实体中心信息方面生成了更忠实和连贯的摘要，显示出更高的事实准确性和数值可靠性。

Conclusion: 研究结果强调了领域特定适应在提高阿拉伯语金融摘要的事实一致性和叙述流畅性中的重要性。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [56] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec是一种新型的检索增强型推测解码框架，通过自适应决策机制显著提高了大语言模型推理的速度，同时保持了输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模型的方法如EAGLE-2虽然准确但成本高，而基于检索的方法如SAM-Decoding依赖于启发式切换策略，经常导致不必要的检索。因此，需要一种更有效的框架来改进这些方法。

Method: ReSpec通过三个核心创新来实现其目标：1) 基于熵的自适应触发器，用于量化上下文可预测性并在不确定性较低时启动检索；2) 基于反馈的候选选择，利用历史反馈来组织多个高质量候选进行并行验证；3) 一种源感知的宽松验证策略，对模型生成的草稿应用严格的检查，而对检索到的草稿使用宽松的验证。

Result: ReSpec在Spec-Bench上实现了最先进的加速效果，分别超过了EAGLE-2和SAM-Decoding的33%和25%，同时保持了输出质量。

Conclusion: ReSpec在Spec-Bench上的实验结果表明，它实现了最先进的加速效果，分别超过了EAGLE-2和SAM-Decoding的33%和25%，同时保持了输出质量。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [57] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文研究了AI辅助同行评审中的提示注入威胁，提出了两种攻击方法并探讨了防御策略，结果表明这些攻击效果显著且具有鲁棒性，而防御方法存在被绕过的可能。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的快速发展，它们在各种任务中的部署越来越广泛。一个新兴的应用是利用AI模型协助审查科学论文。然而，最近的报告表明，一些论文包含隐藏的、注入的提示，旨在操纵AI评审员提供过于有利的评估。因此，我们需要对这一新兴威胁进行系统的研究。

Method: 我们提出了两种攻击类型：(1) 静态攻击，使用固定的注入提示；(2) 迭代攻击，针对模拟的评审模型优化注入提示以最大化其效果。此外，我们探索了一种简单的基于检测的防御方法。

Result: 这两种攻击都取得了显著的效果，经常在针对前沿AI评审员时引发满分评估。此外，这些攻击在各种设置下都具有鲁棒性。虽然基于检测的防御方法显著降低了攻击成功率，但我们证明适应性的攻击者可以部分绕过这种防御。

Conclusion: 我们的研究强调了在AI辅助同行评审中需要更多关注和严格的安全措施，以防范提示注入威胁。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [58] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 本文介绍了FirstAidQA，一个用于急救和紧急响应的高质量问答数据集，旨在支持指令调优和微调大型语言模型和小型语言模型，以实现更快、更可靠和离线功能的系统。


<details>
  <summary>Details</summary>
Motivation: 当前模型计算密集，不适合第一响应者或平民常用的低端设备。开发轻量级、领域特定解决方案的主要障碍是缺乏针对急救和紧急响应的高质量数据集。

Method: 通过使用大型语言模型ChatGPT-4o-mini，结合基于提示的上下文学习，从《Vital First Aid Book (2019)》文本中生成了FirstAidQA数据集。随后进行了文本清理、上下文分块和过滤，并经过人工验证以确保QA对的准确性、安全性和实用性。

Result: FirstAidQA是一个包含5,500个高质量问答对的数据集，涵盖了广泛的急救和紧急响应场景。该数据集设计用于支持指令调优和微调大型语言模型和小型语言模型，以实现更快、更可靠和离线功能的系统。

Conclusion: FirstAidQA 是一个用于急救和紧急响应的高质量问答数据集，旨在支持指令调优和微调大型语言模型和小型语言模型，以实现更快、更可靠和离线功能的系统。该数据集已公开发布，以推动安全关键和资源受限的人工智能应用在急救和紧急响应领域的研究。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [59] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: 本文提出了DeepSpecs，一个通过结构和时间推理增强的RAG系统，用于回答关于5G规范的专家级问题。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）框架无法可靠地解决交叉引用或关于规范演化的推理问题。

Method: DeepSpecs通过三个元数据丰富的数据库（SpecDB、ChangeDB和TDocDB）增强了检索增强生成（RAG）系统，以实现结构和时间推理。

Result: DeepSpecs在多个LLM后端中优于基础模型和最先进的电信RAG系统；消融实验确认了显式交叉引用解析和进化感知检索显著提高了答案质量。

Conclusion: DeepSpecs在多个LLM后端中优于基础模型和最先进的电信RAG系统；消融实验确认了显式交叉引用解析和进化感知检索显著提高了答案质量，强调了建模5G标准的结构和时间属性的价值。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [60] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: 本文介绍了DeepAmbigQAGen，一个用于生成包含名称歧义和多步骤推理的QA任务的数据生成管道，并构建了DeepAmbigQA数据集。实验表明，即使是最先进的模型在处理这些问题时也存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有的QA基准很少同时评估这两个挑战，因此我们需要一个能够同时处理名称歧义和多步骤推理的QA数据集。

Method: 我们引入了DeepAmbigQAGen，一个自动数据生成管道，构建基于文本语料库和链接知识图的QA任务，生成自然且可验证的问题，系统地嵌入名称歧义和多步骤推理。

Result: 即使最先进的GPT-5也显示出不完整的答案，在模糊问题上的精确匹配仅为0.13，在非模糊问题上为0.21。

Conclusion: 这些发现突显了需要更强大的QA系统，旨在信息收集和答案完整性。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [61] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文扩展了DistilQwen模型家族，引入了四个模型系列以满足工业需求，展示了高推理效率和强推理性能，并通过阿里巴巴云PAI平台提供可扩展的功能。


<details>
  <summary>Details</summary>
Motivation: 为了满足工业需求，开发了小而高效的推理模型，以平衡推理性能和推理速度。

Method: 通过引入四个专门设计的模型系列来扩展DistilQwen模型家族，包括慢思考模型、两个自适应思考模型系列以及蒸馏奖励模型。

Result: 在多个基准测试中进行全面评估，证明了这些模型的高推理效率和强推理性能，以及蒸馏奖励模型的实用性。

Conclusion: 这些模型展示了高推理效率和强大的推理性能，以及蒸馏奖励模型的实用价值，并通过阿里巴巴云PAI平台为行业从业者提供了可扩展的训练和推理功能。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [62] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 本文提出了一种新的模型MiniTruePrefixes，用于检测文本前缀中的事实不一致之处，并将其集成到受控解码框架中，从而提高了生成结果的忠实度。


<details>
  <summary>Details</summary>
Motivation: NLI模型通常用于检测完整句子中的事实不一致之处，但在常见的自回归生成架构中，决策是针对每个不断演化的文本前缀进行的。因此，需要一种能够处理文本前缀的蕴含检测方法，以提高生成结果的忠实度。

Method: 我们将蕴含检测任务推广到适用于任意文本前缀，并提出了其在提高生成忠实性方面的效用。我们提供了适合此任务的评估和训练数据集，训练了MiniTruePrefixes，这是一种新型的专业模型，能够更好地检测文本前缀上的事实不一致之处。

Result: MiniTruePrefixes在前缀级别的蕴含检测中比其他基线NLI模型高出5-14 F1分。将其集成到受控解码框架中，可以显著提高抽象摘要中的事实一致性。

Conclusion: 将MiniTruePrefixes集成到受控解码框架中可以显著提高抽象摘要中的事实一致性。当由MiniTruePrefixes引导时，LLaMA-3.2-3B-Instruct在使用仅一半内存的情况下，其忠实度和运行时间可与同一系列的8B模型相媲美。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [63] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 本文构建了一个印地语基准测试，用于评估大型语言模型在医疗查询中的表现，并发现现有模型在处理此类任务时存在一定的不足。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的人使用大型语言模型（LLMs）获取医疗建议和咨询，评估这些模型对医疗查询的回答效果和准确性变得越来越重要。然而，现有的医学基准测试几乎都是英文的，这导致了多语言LLM评估领域的显著空白。因此，本文旨在填补这一空白。

Method: 本文通过将Cancer-Myth数据集中的500个条目翻译成五种南亚地区使用但资源较少的语言，构建了一个名为Cancer-Myth-Indic的印地语基准测试。翻译过程中遵循了保持隐含预设的风格指南，并包含虚假预设与癌症相关的内容。然后对几种流行的大型语言模型进行了评估。

Result: 本文构建了一个包含2,500个翻译条目的印地语基准测试，并评估了几种流行的大型语言模型在此基准测试中的表现。

Conclusion: 本文旨在通过构建一个印地语基准测试来填补多语言大型语言模型评估的空白，并评估几种流行的大型语言模型在这一基准测试中的表现。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [64] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 本文探讨了基准测试在评估大型语言模型和推理模型中的作用，并分析了不同模型家族在不同基准上的推理能力演变。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和大型推理模型的快速发展，基准测试的数量也在迅速增加。然而，由于模型能力的提升以及数据集可能被包含在预训练或后训练数据中，结果变得饱和，需要不断推出新的更具挑战性的基准测试。本文旨在探讨超越基准测试是否真正展示了推理能力，还是仅仅在跟踪与所声称的能力无关的数字。

Method: 本文研究了三个模型家族（OpenAI、Anthropic 和 Google）在不同基准上的推理能力随时间的变化，并分析了不同推理任务的性能趋势。

Result: 本文分析了不同模型家族在不同基准上的推理能力演变，并讨论了当前基准测试的现状和剩余挑战。

Conclusion: 本文旨在提供对基准测试和推理任务的全面概述，以作为未来推理评估和模型开发研究的参考。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [65] [Confounding Factors in Relating Model Performance to Morphology](https://arxiv.org/abs/2511.01380)
*Wessel Poelman,Thomas Bauwens,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 本文探讨了形态学对语言建模的影响，指出现有研究中的混杂因素，并提出了一种新的内在指标来预测语言建模的难度。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决关于形态学对语言建模影响的矛盾结论，提供一种更可靠的方法来评估形态学与语言建模之间的关系。

Method: 本文分析了现有研究中的混杂因素，并重新评估了Arnett & Bergen (2025)提出的三个假设，同时引入了token bigram指标作为预测语言建模难度的内在方法。

Result: 本文发现现有结论中存在混杂因素，并提出了token bigram指标作为预测语言建模难度的有效方法。

Conclusion: 本文指出，由于实验设置中的混杂因素，关于形态学对语言建模影响的结论存在矛盾，并提出了一种新的内在指标来预测语言建模的难度。

Abstract: The extent to which individual language characteristics influence
tokenization and language modeling is an open question. Differences in
morphological systems have been suggested as both unimportant and crucial to
consider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter
alia). We argue this conflicting evidence is due to confounding factors in
experimental setups, making it hard to compare results and draw conclusions. We
identify confounding factors in analyses trying to answer the question of
whether, and how, morphology relates to language modeling. Next, we re-assess
three hypotheses by Arnett & Bergen (2025) for why modeling agglutinative
languages results in higher perplexities than fusional languages: they look at
morphological alignment of tokenization, tokenization efficiency, and dataset
size. We show that each conclusion includes confounding factors. Finally, we
introduce token bigram metrics as an intrinsic way to predict the difficulty of
causal language modeling, and find that they are gradient proxies for
morphological complexity that do not require expert annotation. Ultimately, we
outline necessities to reliably answer whether, and how, morphology relates to
language modeling.

</details>


### [66] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，通过遗传搜索优化RAG设计，能够在多个领域中显著提升RAG性能。


<details>
  <summary>Details</summary>
Motivation: RAG质量取决于检索、排序、增强、提示和生成中的许多相互作用的选择，因此单独优化模块是脆弱的。

Method: 我们引入了RAGSmith，这是一个模块化框架，将RAG设计视为九种技术家族和46,080种可行管道配置的端到端架构搜索。遗传搜索优化了一个联合聚合检索指标（recall@k, mAP, nDCG, MRR）和生成指标（LLM-Judge和语义相似性）的标量目标。

Result: RAGSmith找到的配置在平均情况下比原始RAG基线高出+3.8%（在各个领域中范围从+1.2%到+6.9%），在检索中最高达到+12.5%，在生成中最高达到+7.5%。搜索通常探索约0.2%的空间（约100个候选者），并发现了一个稳健的主干——向量检索加上后生成的反思/修订——由扩展、重新排序、增强和提示重新排序等领域的选择进行增强；段落压缩从未被选中。改进幅度与问题类型相关，事实/长答案混合的问题获得更大的收益。

Conclusion: 这些结果为组装有效的RAG系统提供了实用的、领域感知的指导，并展示了进化搜索在全管道优化中的效用。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [67] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: 本文介绍了LiveSearchBench，这是一个自动化流程，用于从最新的知识更新中构建依赖于检索的基准。该方法计算Wikidata快照之间的差异，过滤候选三元组以确保质量，并在三个推理难度级别上合成自然语言问题，每个问题都保证通过SPARQL验证具有唯一且可验证的答案。实验表明，当模型遇到预训练后的新事实时，性能显著下降，尤其是在多跳查询上。尽管检索增强方法和更大的指令调优模型提供了一些改进，但未能弥补这种时间差距。LiveSearchBench将评估从静态记忆转向需要最新检索和推理的任务，为LLM在不断变化的知识下的系统性长期评估提供了基础。


<details>
  <summary>Details</summary>
Motivation: Evaluating large language models (LLMs) on question answering often relies on static benchmarks that reward memorization and understate the role of retrieval, failing to capture the dynamic nature of world knowledge.

Method: We present LiveSearchBench, an automated pipeline for constructing retrieval-dependent benchmarks from recent knowledge updates. Our method computes deltas between successive Wikidata snapshots, filters candidate triples for quality, and synthesizes natural-language questions at three levels of reasoning difficulty, each guaranteed to admit a unique, verifiable answer through SPARQL validation.

Result: Experiments show a pronounced performance drop when models confront facts that post-date pretraining, with the gap most salient on multi-hop queries. Retrieval augmented methods and larger, instruction-tuned models provide partial gains but fail to close this recency gap.

Conclusion: LiveSearchBench shifts evaluation from static memorization toward tasks that require up-to-date retrieval and reasoning, offering a foundation for systematic, long-term assessment of LLMs under evolving knowledge.

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [68] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 本文提出了一种基于草稿的优化管道，利用开源大型语言模型和检索增强生成技术，在翻译形态丰富、资源匮乏的语言时达到与顶级专有系统相当的性能水平。


<details>
  <summary>Details</summary>
Motivation: 翻译像拉丁语这样的形态丰富、资源匮乏的语言面临重大挑战。本文旨在开发一种可重复的优化管道，以提升开源大型语言模型的性能，使其达到与顶级专有系统相当的水平。

Method: 本文提出了一种基于草稿的优化管道，首先使用微调的NLLB-1.3B模型生成高质量的草稿，然后通过零样本LLM（Llama-3.3或Qwen3）进行润色，并可以通过检索上下文外的例子（RAG）来进一步增强。

Result: 实验结果表明，该开源RAG系统在两个不同的基准测试中表现出与GPT-5基线相当的性能，且无需任何任务特定的LLM微调。

Conclusion: 本文提出了一种可重复的基于草稿的优化管道，使开源大型语言模型（LLMs）达到与顶级专有系统相当的性能水平。该方法首先使用微调的NLLB-1.3B模型生成高质量、结构忠实的草稿，然后通过零样本LLM（Llama-3.3或Qwen3）进行润色，并可以通过检索上下文外的例子（RAG）来进一步增强。实验表明，这种开源RAG系统在两个不同的基准测试中表现出与GPT-5基线相当的性能，且无需任何任务特定的LLM微调。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [69] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架BARD，用于同时蒸馏推理能力和实现对推理长度的精细控制，从而提高资源使用效率。


<details>
  <summary>Details</summary>
Motivation: 现有长链式思维（CoT）蒸馏方法在推理过程中存在冗余和计算预算不可控的问题，导致资源使用效率低下。

Method: BARD框架通过两阶段训练方案实现推理能力的蒸馏和推理长度的精细控制，第一阶段是监督微调，第二阶段是考虑推理性能和预算保真度的强化学习。

Result: 实验表明，BARD框架能够让8B学生模型在AIME24、AIME25和GPQA等挑战性推理基准测试中取得优异的成绩，并且能够在广泛的预算范围内提供精确和自适应的推理长度控制。

Conclusion: BARD框架能够使8B学生模型在挑战性的推理基准测试中表现出色，同时提供精确和自适应的推理长度控制。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [70] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型（LLMs）作为一致和可靠的标注者，以解决文本为基础的自动认知扭曲检测任务中的主观性问题。通过引入一个与数据集无关的评估框架，我们发现GPT-4能够生成一致的标注，从而提高模型的性能。


<details>
  <summary>Details</summary>
Motivation: 文本为基础的自动认知扭曲检测是一项具有挑战性的任务，由于其主观性，甚至在专家人类标注者之间也观察到低一致性评分，导致不可靠的标注。

Method: 我们探索了使用大型语言模型（LLMs）作为一致和可靠的标注者，并提出多次独立的LLM运行可以在任务固有的主观性下揭示稳定的标注模式。此外，为了公平比较在不同特征数据集上训练的模型，我们引入了一个与数据集无关的评估框架，使用Cohen's kappa作为效应量度量。

Result: 我们的结果显示，GPT-4可以生成一致的标注（Fleiss's Kappa = 0.78），与基于人类标注数据训练的模型相比，这些标注可以提高测试集性能。

Conclusion: 我们的研究结果表明，LLMs可以为具有主观性的自然语言处理任务提供可扩展且内部一致的训练数据替代方案。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [71] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 本文研究了合成数据来源的多样性对微调大型语言模型的影响，发现使用多样化来源的合成数据可以减轻分布崩溃，保持输出分布的广度和输出文本的多样性。同时，合成数据在移除安全措施后仍能保持较高的输出质量，而微调有助于减少自我偏好偏差。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响至关重要。本文旨在探讨合成数据来源的多样性对微调模型的影响。

Method: 本文研究了合成数据来源的多样性对微调大型语言模型的影响，重点关注三个关键维度：分布崩溃、对抗鲁棒性和自我偏好偏差。

Result: 研究结果表明，使用来自多样化来源的合成数据进行微调可以减轻分布崩溃，保持输出分布的广度和输出文本的多样性。此外，虽然人类和合成微调数据都可以移除安全措施，但后者保留了更高的输出质量，从而使输出更可能被使用和危险。最后，微调减少了自我偏好偏差，其中人类数据最为有效，其次是多源合成数据。

Conclusion: 本文结论是，使用来自多样化来源的合成数据进行微调可以减轻分布崩溃，保持输出分布的广度和输出文本的多样性。此外，虽然人类和合成微调数据都可以移除安全措施，但后者保留了更高的输出质量，从而使输出更可能被使用和危险。最后，微调减少了自我偏好偏差，其中人类数据最为有效，其次是多源合成数据。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [72] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文提出了一种新的孟加拉语文本净化方法，结合了Pareto优化的大型语言模型和思维链提示，并构建了一个名为BanglaNirTox的平行语料库来支持这一工作。结果显示这种方法能显著提高净化效果。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语中的有害语言仍然普遍存在，尤其是在在线环境中，但针对它的有效预防措施很少。虽然文本净化在高资源语言中取得了进展，但由于资源有限，孟加拉语仍缺乏探索。

Method: 我们提出了一个结合Pareto类优化的大型语言模型（LLMs）和思维链（CoT）提示的孟加拉语文本净化新流程。此外，我们构建了一个名为BanglaNirTox的人工生成的平行语料库，包含68,041个有毒孟加拉语句子，并带有类别毒性标签、推理和净化后的改写句子。

Result: 通过使用Pareto优化的大型语言模型和思维链提示，我们能够显著提高孟加拉语文本净化的质量和一致性。

Conclusion: 我们的研究结果表明，结合Pareto优化的大型语言模型和思维链提示可以显著提高孟加拉语文本净化的质量和一致性。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [73] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的框架，用于生成具有可控难度的干扰项，通过数据增强和多任务学习策略。该框架包括一个双向干扰项生成过程和一个难度分类系统，以及一个难度可控的生成模型。实验结果表明，该方法在生成高质量干扰项方面表现出色，并且优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 生成高质量的干扰项仍然具有挑战性，因为现有方法往往缺乏适应性和对难度水平的控制，而缺乏难度标注的数据集进一步阻碍了进展。

Method: 我们提出了一种新颖的框架，通过利用数据增强和多任务学习策略来生成具有可控难度的干扰项。首先，为了创建一个高质量、难度标注的数据集，我们引入了一个双向干扰项生成过程，以产生多样且合理的干扰项。这些候选干扰项随后通过过滤进行精炼，并使用集成QA系统按难度分类。其次，利用新创建的数据集通过多任务学习训练一个难度可控的生成模型。该框架包括精心设计的辅助任务，以增强模型对干扰项的语义理解和估计其难度的能力。

Result: 实验结果表明，我们的方法在不同难度水平上都能生成高质量的干扰项，并且在将干扰项难度与人类感知对齐方面显著优于GPT-4o。

Conclusion: 实验结果表明，我们的方法在生成高质量的干扰项方面表现优异，并且在将干扰项难度与人类感知对齐方面显著优于GPT-4o。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [74] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: 本研究探讨了数学焦虑对学生概念感知和关联的影响，并发现人类学生与GPT模拟学生在这些方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 数学焦虑对大学心理学学生的职业选择和整体福祉构成重大挑战，因此需要了解概念感知和关联以帮助管理这种焦虑。

Method: 本研究采用基于行为形式心智网络的框架，探索个体和群体在数学和焦虑相关概念的感知和关联上的差异。进行了4个实验，涉及心理学本科生和GPT模拟学生。

Result: 结果表明，学生的积极情感评分和较高的“焦虑”网络度以及对“数学”的负面评分可以预测更高的总数学焦虑和评价性数学焦虑。然而，这些模型在GPT数据上不适用，因为模拟网络和心理测量评分与人类存在差异。

Conclusion: 这些发现强调了理解概念感知和关联在管理学生数学焦虑中的重要性。

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [75] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: ECO decoding dynamically adjusts control strength based on entropy to improve controllability and fluency in dialogue generation, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of finding an ideal control strength that satisfies both controllability and fluency when using a fixed constant value to manage the bias of attribute probabilities in weighted decoding methods.

Method: ECO decoding (Entropy-based COntrol), which dynamically adjusts the control strength at each generation step according to the model's entropy in both the language model and attribute classifier probability distributions.

Result: Experiments on the DailyDialog and MultiWOZ datasets demonstrate that ECO decoding consistently improves controllability while maintaining fluency and grammaticality, outperforming prior decoding methods across various models and settings. It also alleviates probability interpolation issues in multi-attribute generation, showing strong performance in both single and multi-attribute scenarios.

Conclusion: ECO decoding consistently improves controllability while maintaining fluency and grammaticality, outperforming prior decoding methods across various models and settings. It also alleviates probability interpolation issues in multi-attribute generation, demonstrating strong performance in both single and multi-attribute scenarios.

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [76] [BIRD: Bronze Inscription Restoration and Dating](https://arxiv.org/abs/2511.01589)
*Wenjie Hua,Hoang H. Nguyen,Gangyan Ge*

Main category: cs.CL

TL;DR: 本文介绍了BIRD数据集和一种新的框架，用于处理早期中国青铜铭文的修复和年代测定问题。


<details>
  <summary>Details</summary>
Motivation: 早期中国的青铜铭文是片段化的且难以确定年代，需要一种更有效的处理方法。

Method: 引入了BIRD数据集，并提出了一个考虑异体字的掩码语言建模框架，该框架结合了领域和任务适应性预训练与Glyph Net (GN)。

Result: 实验表明，GN提高了修复效果，而基于字形的采样在年代测定中取得了进展。

Conclusion: BIRD数据集和提出的框架在青铜铭文的修复和年代测定方面表现出色。

Abstract: Bronze inscriptions from early China are fragmentary and difficult to date.
We introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded
dataset grounded in standard scholarly transcriptions and chronological labels.
We further propose an allograph-aware masked language modeling framework that
integrates domain- and task-adaptive pretraining with a Glyph Net (GN), which
links graphemes and allographs. Experiments show that GN improves restoration,
while glyph-biased sampling yields gains in dating.

</details>


### [77] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 该项目研究母语西班牙语者的语言错误，评估大型语言模型在解释和纠正这些错误方面的表现，并推动更符合认知的自然语言处理系统的发展。


<details>
  <summary>Details</summary>
Motivation: 语言错误不仅是规范语法的偏离，它们为语言的认知结构提供了独特的视角，并揭示了当前试图复制这些系统的局限性。

Method: 该项目采用跨学科研究方法，结合理论语言学、神经语言学和自然语言处理（NLP），分析由母语西班牙语者产生的语言错误，并利用一个专门构建的语料库对AI模型进行测试。

Result: 该项目通过分析母语西班牙语者的语言错误，评估大型语言模型（如GPT或Gemini）在解释、再现或纠正这些错误方面的准确性，并探索它们对人类语言行为模式的泛化能力。

Conclusion: 该项目不仅有助于理解西班牙语作为母语，还有助于开发更符合认知的自然语言处理系统，能够与真实人类语言的不完美、变化和模糊性进行互动。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [78] [ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian](https://arxiv.org/abs/2511.01619)
*Nikola Ljubešić,Peter Rupnik,Ivan Porupski,Taja Kuzman Pungeršek*

Main category: cs.CL

TL;DR: ParlaSpeech是一个包含四个斯拉夫语言（克罗地亚语、捷克语、波兰语和塞尔维亚语）的口语议会语料库，总时长为6000小时。该数据集通过各种自动注释层进行了丰富，包括语言注释、情感预测、填充停顿检测以及单词和图素级对齐。


<details>
  <summary>Details</summary>
Motivation: ParlaSpeech数据集的构建是为了提高其在多个学科下游研究中的有用性，例如分析情感的声学相关性。

Method: ParlaSpeech数据集是通过自动方式从ParlaMint转录本及其相应的元数据构建的，这些转录本与每个相应议会的语音记录对齐。该数据集的每个语料库都通过各种自动注释层进行了显著丰富。

Result: ParlaSpeech数据集的丰富性显著提高了其在多个学科下游研究中的有用性，我们通过分析情感的声学相关性来展示这一点。所有语料库都可以以JSONL和TextGrid格式下载，并可通过词频分析器进行搜索。

Conclusion: ParlaSpeech数据集的丰富性显著提高了其在多个学科下游研究中的有用性，我们通过分析情感的声学相关性来展示这一点。所有语料库都可以以JSONL和TextGrid格式下载，并可通过词频分析器进行搜索。

Abstract: ParlaSpeech is a collection of spoken parliamentary corpora currently
spanning four Slavic languages - Croatian, Czech, Polish and Serbian - all
together 6 thousand hours in size. The corpora were built in an automatic
fashion from the ParlaMint transcripts and their corresponding metadata, which
were aligned to the speech recordings of each corresponding parliament. In this
release of the dataset, each of the corpora is significantly enriched with
various automatic annotation layers. The textual modality of all four corpora
has been enriched with linguistic annotations and sentiment predictions.
Similar to that, their spoken modality has been automatically enriched with
occurrences of filled pauses, the most frequent disfluency in typical speech.
Two out of the four languages have been additionally enriched with detailed
word- and grapheme-level alignments, and the automatic annotation of the
position of primary stress in multisyllabic words. With these enrichments, the
usefulness of the underlying corpora has been drastically increased for
downstream research across multiple disciplines, which we showcase through an
analysis of acoustic correlates of sentiment. All the corpora are made
available for download in JSONL and TextGrid formats, as well as for search
through a concordancer.

</details>


### [79] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 本研究探讨了在基于图的检索增强生成架构中使用大型语言模型进行能源效率问答的潜力，结果显示系统在多语言环境下表现良好，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在基于图的检索增强生成架构中的应用，以提高能源效率问答的准确性。

Method: 系统从能源领域的指导和法规文件中自动提取知识图谱，然后导航和推理生成的图谱以提供多语言准确答案。

Result: 验证结果表明，系统在约四分之三的情况下正确回答问题（75.2% ± 2.7%），在更一般的能源效率问题上表现更好（高达81.0% ± 4.1%），并且具有有希望的多语言能力（翻译导致4.4%的准确率下降）。

Conclusion: 该架构展示了在能源效率问答中的潜力，并识别了其优缺点。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [80] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 该研究提出了一种认知基准框架，用于评估大型语言模型在处理和应用文化特定知识方面的能力。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在评估大型语言模型在处理文化特定知识方面的能力，特别是在台湾客家数字文化档案中的应用。

Method: 该框架结合了布鲁姆分类法与检索增强生成（RAG），以评估模型在六个层次的认知领域中的表现：记忆、理解、应用、分析、评估和创造。

Result: 通过使用台湾客家数字文化档案作为主要测试平台，评估测量了LLM生成响应的语义准确性和文化相关性。

Conclusion: 该研究提出了一种认知基准框架，用于评估大型语言模型（LLMs）如何处理和应用文化特定的知识。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [81] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: 本文介绍了EngChain，一个用于验证多步骤工程问题解决的基准，它通过两个阶段的评估来验证每个推理步骤的数值和语义有效性，并引入LLM-As-A-Judge系统来定性分类识别出的推理错误。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试评估语言理解、事实回忆、数学或代码生成，但没有捕捉到工程中科学原理、定量建模和实际约束必须汇聚的综合推理。因此，需要一个能够评估复杂推理能力的基准测试。

Method: EngChain包含90个问题，涵盖三个工程分支，组织成9个领域和20个不同的区域。这些问题是从具有高度随机化的符号模板中生成的，以确保多样性并消除污染的风险。基准测试采用两阶段评估：首先定量验证每个推理步骤的数值和语义有效性，然后引入LLM-As-A-Judge系统来定性分类识别出的推理错误。

Result: EngChain是一个用于验证多步骤工程问题解决的基准，能够超越最终答案的准确性，通过两个阶段的评估来验证每个推理步骤的数值和语义有效性，并引入LLM-As-A-Judge系统来定性分类识别出的推理错误。

Conclusion: EngChain是一个用于验证多步骤工程问题解决的基准，它能够超越最终答案的准确性，通过两个阶段的评估来验证每个推理步骤的数值和语义有效性，并引入LLM-As-A-Judge系统来定性分类识别出的推理错误。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [82] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio 是一个针对东南亚多种语言的大型音频-语言模型，具有多语言、多模态和多任务处理能力，旨在促进该地区的音频大语言模型研究和应用。


<details>
  <summary>Details</summary>
Motivation: 为了推动东南亚地区音频大语言模型的发展，需要一个专门针对该地区语言的模型。

Method: SeaLLMs-Audio 是通过大规模音频语料库训练而成的，支持多种语言和多模态输入，并能够执行多种任务，包括音频分析和语音对话。

Result: 实验表明，SeaLLMs-Audio 在东南亚语言上的表现与其他大语言模型相当。

Conclusion: SeaLLMs-Audio 是一个针对东南亚多种语言的大型音频-语言模型，展示了在各种音频相关任务中的强大性能。它有望为该地区的科研和产业带来益处。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [83] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文介绍了第一个开放的字符训练实现，利用宪法AI和新的数据管道来更有效地塑造助手角色，并展示了这种方法在对抗性提示下更具鲁棒性，同时保持了生成的连贯性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现代聊天机器人大型语言模型生成的“AI助手”角色会影响表面行为和明显的价值观、信念和伦理。这些因素影响交互质量、感知智能以及与开发人员和用户意图的一致性。角色训练是行业后训练的关键组成部分，但在学术文献中几乎没有研究。

Method: 我们引入了第一个开放的字符训练实现，利用宪法AI和一个新的数据管道，使用合成内省数据来更有效地塑造助手角色。具体来说，我们使用11个示例角色对三个流行的开源权重模型进行了微调。

Result: 我们发现这些变化比上述两种替代方法更能抵御对抗性提示，同时导致更连贯和真实的生成。最后，我们证明这种微调对通用能力的影响很小或没有影响。

Conclusion: 我们描述并开源了完整的后训练方法，该方法的实现可以在https://github.com/maiush/OpenCharacterTraining找到。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [84] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了一种新的秩-2子空间方法，用于更准确地分析LLM中PK和CK的交互，并展示了其在多步骤NLE分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单步生成，仅将PK和CK的交互建模为二元选择，忽略了更丰富的交互形式，如互补或支持性知识。

Method: 本文提出了一种新的秩-2投影子空间，以更准确地解耦PK和CK的贡献，并首次对较长的NLE序列进行了多步骤分析。

Result: 实验表明，在秩-1子空间中，多样化的知识交互表现不佳，而在我们的秩-2公式中则有效捕获。多步骤分析揭示了幻觉NLEs与PK方向高度对齐，而上下文忠实的NLEs则平衡PK和CK，Chain-of-Thought提示使生成的NLEs更偏向CK。

Conclusion: 本文提出了一个基于更丰富的秩-2子空间解耦的框架，用于系统研究LLM中的多步骤知识交互，为未来的研究提供了基础。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [85] [Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue](https://arxiv.org/abs/2511.01720)
*Mahammad Nuriyev*

Main category: cs.CL

TL;DR: 本文介绍了一个多专家系统，用于创建能够在交互环境中进行自然对话和上下文动作执行的非玩家角色（NPC）。


<details>
  <summary>Details</summary>
Motivation: 创建能够进行自然对话和上下文动作执行的非玩家角色（NPC）是交互环境中的一项重要任务。

Method: 我们使用Qwen3作为基础模型，并利用低秩适应（LoRA）适配器实例化了三个专家：工具调用、工具响应解释和直接对话。

Result: 我们的系统满足计算效率要求，在L40S GPU上提供快速响应并保持适度的资源使用。

Conclusion: 我们的方法在2025年常识人格基础对话挑战中排名第二，展示了其在自然对话和上下文动作执行方面的有效性。

Abstract: We present a multi-expert system for creating Non-Player Characters (NPCs)
capable of both natural dialogue and contextual action execution in interactive
environments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)
adapters, we instantiate three specialists: tool calling, tool-response
interpretation, and direct dialogue. Our system comfortably meets the
computational efficiency requirements, delivering fast responses and
maintaining modest resource usage on L40S GPUs. In the Commonsense
Persona-Grounded Dialogue Challenge 2025, our method ranked second overall.
  Code available at:
https://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/

</details>


### [86] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文探讨了语言模型在长时间对话或阅读过程中信念变化的风险，并展示了这种变化如何影响模型的行为。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在头脑风暴和研究等应用中的使用增加，它们的内存和上下文大小得到了改进，但这也带来了模型信念配置文件可能随上下文积累而悄然变化的风险。

Method: 我们通过设计需要工具使用的任务来检查模型的行为变化，每个工具选择都对应一种隐含的信念。

Result: GPT-5在关于道德困境和安全查询的10轮讨论后，其陈述信念发生了54.7%的转变，而Grok 4在阅读对立立场的文本后，在政治问题上发生了27.2%的转变。

Conclusion: 我们的分析揭示了当模型进行长时间的对话或阅读时，信念变化带来的隐藏风险，这使得它们的观点和行为变得不可靠。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [87] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 本文提出了一种无需模型重新训练的提示工程方法，可以实现精确的长度控制。该方法在多个最先进的LLM上进行了评估，结果显示其在长度保真度方面有显著提升，并且保持或提高了输出质量。这种方法为需要精确长度控制的应用提供了一个立即可部署的解决方案，尤其适用于模型重新训练不切实际或成本过高的生产环境。


<details>
  <summary>Details</summary>
Motivation: 长度控制在大型语言模型中是一个关键但未被充分解决的挑战，现有的方法通常需要昂贵的模型重新训练或复杂的推理时工具。本文旨在提供一种无需模型重新训练的提示工程方法，以实现精确的长度控制。

Method: 本文提出了一种结构引导的方法，通过在提示中实施明确的规划和计数机制，使模型能够仔细跟踪并遵守指定的长度约束。

Result: 在六个最先进的LLM上进行的全面评估表明，与标准提示相比，本文的方法在文档摘要任务中显著提高了长度保真度，特别是在较短到中等长度的约束下。某些模型的长度遵循性提高了高达37.6%。

Conclusion: 本文提出了一种无需模型重新训练的提示工程方法，可以实现精确的长度控制。该方法在多个最先进的LLM上进行了评估，结果显示其在长度保真度方面有显著提升，并且保持或提高了输出质量。这种方法为需要精确长度控制的应用提供了一个立即可部署的解决方案，尤其适用于模型重新训练不切实际或成本过高的生产环境。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [88] [KV Cache Transform Coding for Compact Storage in LLM Inference](https://arxiv.org/abs/2511.01815)
*Konrad Staniszewski,Adrian Łańcucki*

Main category: cs.CL

TL;DR: KVTC 是一种轻量级变换编码器，用于压缩 KV 缓存以实现高效的 GPU 和离线存储。


<details>
  <summary>Details</summary>
Motivation: KV 缓存可以跨对话回合通过共享前缀提示进行重用，但过时的缓存会消耗宝贵的 GPU 内存，需要卸载或强制重新计算。

Method: KVTC 结合了基于 PCA 的特征去相关、自适应量化和熵编码，以压缩 KV 缓存。

Result: KVTC 实现了高达 20 倍的压缩率，同时保持推理和长上下文准确性，并在特定用例中实现了 40 倍或更高的压缩率。

Conclusion: KVTC 是一种实用的构建块，可用于具有可重用 KV 缓存的内存高效 LLM 服务。

Abstract: Serving large language models (LLMs) at scale necessitates efficient
key-value (KV) cache management. KV caches can be reused across conversation
turns via shared-prefix prompts that are common in iterative code editing and
chat. However, stale caches consume scarce GPU memory, require offloading, or
force recomputation. We present KVTC, a lightweight transform coder that
compresses KV caches for compact on-GPU and off-GPU storage. Drawing on
classical media compression, KVTC combines PCA-based feature decorrelation,
adaptive quantization, and entropy coding. It requires only a brief initial
calibration and leaves model parameters unchanged. By exploiting redundancies
in KV caches, KVTC achieves up to 20$\times$ compression while maintaining
reasoning and long-context accuracy, and 40$\times$ or higher for specific use
cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across
benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and
MATH-500. It consistently outperforms inference-time baselines such as token
eviction, quantization, and SVD-based methods, while achieving higher
compression ratios. These results support KVTC as a practical building block
for memory-efficient LLM serving with reusable KV caches.

</details>


### [89] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: This paper introduces IMO-Bench, a set of advanced reasoning benchmarks targeting the International Mathematical Olympiad level. The benchmarks were used to achieve gold-level performance in the IMO 2025 with Gemini Deep Think.


<details>
  <summary>Details</summary>
Motivation: Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers.

Method: We present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO).

Result: Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations.

Conclusion: IMO-Bench will help the community towards advancing robust mathematical reasoning.

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


### [90] [Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems](https://arxiv.org/abs/2511.01854)
*Elias Lumer,Faheem Nizar,Anmol Gulati,Pradeep Honaganahalli Basavaraju,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: Tool-to-Agent Retrieval 是一种统一的框架，通过在共享向量空间中嵌入工具和代理，并利用元数据关系连接它们，实现了细粒度的检索。该方法在 LiveMCPBench 基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的检索方法通常在代理级别进行查询匹配，这会模糊工具的功能并导致次优的代理选择。

Method: Tool-to-Agent Retrieval 框架将工具和代理嵌入到共享向量空间中，并利用元数据关系进行连接，从而实现细粒度的检索。

Result: 在八个嵌入模型上评估 Tool-to-Agent Retrieval，该方法在 LiveMCPBench 基准测试中比之前最先进的代理检索器在 Recall@5 和 nDCG@5 上分别提升了 19.4% 和 17.7%。

Conclusion: Tool-to-Agent Retrieval 是一种统一的框架，通过在共享向量空间中嵌入工具和其父代理，并通过元数据关系连接它们，实现了细粒度的工具级或代理级检索。该方法在 LiveMCPBench 基准测试中取得了显著的性能提升。

Abstract: Recent advances in LLM Multi-Agent Systems enable scalable orchestration of
sub-agents, each coordinating hundreds or thousands of tools or Model Context
Protocol (MCP) servers. However, existing retrieval methods typically match
queries against coarse agent-level descriptions before routing, which obscures
fine-grained tool functionality and often results in suboptimal agent
selection. We introduce Tool-to-Agent Retrieval, a unified framework that
embeds both tools and their parent agents in a shared vector space and connects
them through metadata relationships. By explicitly representing tool
capabilities and traversing metadata to the agent level, Tool-to-Agent
Retrieval enables granular tool-level or agent-level retrieval, ensuring that
agents and their underlying tools or MCP servers are equally represented
without the context dilution that arises from chunking many tools together.
Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach
achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over
previous state-of-the-art agent retrievers on the LiveMCPBench benchmark.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [91] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 本文研究了在固定预算下搜索计算最优模型组合和架构的新问题，并提出了Agent-REINFORCE框架，该框架在样本效率和搜索性能上优于传统和基于LLM的基线。


<details>
  <summary>Details</summary>
Motivation: 先前的研究通常假设固定的协作架构（例如拓扑结构）和单模型使用，忽略了最佳架构和模型组合可能因任务而异。因此，我们研究了在固定预算下搜索计算最优模型组合和架构的新问题。

Method: 我们将问题重新表述为概率图优化，并通过初步实验得出了关于TTS协作图的三个经验见解。基于这些见解，我们提出了Agent-REINFORCE，这是一个基于LLM代理的框架，它通过将采样-梯度更新映射到采样-反馈更新来模仿REINFORCE管道。

Result: Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线，并能有效识别在准确性和推理延迟联合目标下的最优图。

Conclusion: 实验表明，Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线，并能有效识别在准确性和推理延迟联合目标下的最优图。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [92] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 研究评估了稀疏自编码器在检测和减轻医疗领域大语言模型偏见中的作用，发现其在简单任务中有一定效果，但在复杂临床任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用可能加剧现有的偏见，需要找到方法来检测和减轻这种偏见。

Method: 评估稀疏自编码器（SAEs）在揭示和控制模型与种族和污名化概念之间关联方面的效果。

Result: 发现SAEs可以揭示模型对种族的依赖，并且可以通过激活特定的潜在变量来引导模型生成关于黑人患者的输出，这可能导致模型输出中出现有问题的关联。此外，通过SAE引导在简单设置中有所改进，但在更复杂的真实临床任务中效果有限。

Conclusion: SAEs may offer a useful tool in clinical applications of LLMs to identify problematic reliance on demographics, but mitigating bias via SAE steering appears to be of marginal utility for realistic tasks.

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [93] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型中校准如何在网络深度中演变，发现上层/后期层存在置信度修正阶段，并识别出残差流中的低维校准方向，其扰动可显著提高校准指标而不损害准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的研究表明深度神经网络往往过于自信，但大型语言模型（LLMs）表现出固有的校准能力，其中预测概率与正确性相吻合。最近的研究将这种行为与最终层中的特定组件联系起来，如熵神经元和未嵌入矩阵的零空间。本文旨在提供一个互补的观点，研究校准如何在网络深度中演变。

Method: 我们分析了多个开放权重模型在MMLU基准上的表现，以研究校准如何在网络深度中演变。此外，我们识别了残差流中的低维校准方向，并通过扰动该方向显著改善了校准指标。

Result: 我们发现，在上层/后期层中存在一个显著的置信度修正阶段，其中模型置信度在决策确定后被主动重新校准。此外，我们识别出残差流中的低维校准方向，其扰动显著提高了校准指标（ECE和MCE），而不会损害准确性。

Conclusion: 我们的发现表明，校准是一个在整个网络前向传递中塑造的分布式现象，而不仅仅是在其最终投影中，这为理解LLMs中的置信度调节机制提供了新的见解。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [94] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 本文提出了一种新的解码策略，即Pivot-Aware Speculative Decoding，旨在通过放宽对分布匹配的要求，提高接受率并实现更高的速度提升，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的Speculative Decoding（SD）要求输出与目标模型的分布完全匹配，这过于严格，导致接受率低，限制了潜在的速度提升。在实际应用中，实用性（如代码正确性、事实准确性）往往比采样分布更重要。

Method: 本文提出了一种新的解码策略，即Pivot-Aware Speculative Decoding，该策略通过识别关键令牌（pivot tokens）来拒绝那些会导致最终输出实用性下降的令牌。

Result: 在多个数据集上的评估表明，该方法可以实现高达2.5倍的速度提升，同时保持与现有方法相当的实用性。

Conclusion: 本文提出了一种新的解码策略，即Pivot-Aware Speculative Decoding，它可以在保持实用性的前提下显著提高接受率并实现更高的速度提升。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [95] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC is a framework that improves the selection of reasoning methods in language models by learning a shared representation space and balancing accuracy with computational cost.


<details>
  <summary>Details</summary>
Motivation: Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy.

Method: EPIC, an Ensemble Planning with Contrastive learning framework, is introduced to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. It incorporates probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost.

Result: Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead.

Conclusion: EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead.

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [96] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 本文提供了一个统一的贝叶斯框架，用于解释和预测基于提示和激活的LLM控制方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解释LLM控制时存在分歧，因此需要一个更广泛的框架来统一这些方法。

Method: 从贝叶斯角度出发，提出了一种统一的预测性框架，用于分析LLM的控制方法。

Result: 该模型能够预测LLM行为，并解释了先前的经验现象，如S型学习曲线，同时预测了新的现象，如干预在对数信念空间中的可加性。

Conclusion: 本文提出了一个统一的贝叶斯视角来解释大型语言模型（LLM）的控制方法，包括基于提示和激活的干预。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [97] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: FEval-TTC 是一种用于评估测试时间计算方法的公平评估协议，它标准化了评估流程并提供了成本建模，以确保在不同 LLM 和数据集上的公平比较。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的性能和 API 调用成本会随时间变化，这可能使先前研究的结论无效，因此需要一种公平的评估协议来确保测试时间计算方法的一致性评估。

Method: FEval-TTC 通过标准化少样本提示和答案提取过程，支持在多个 LLM 和多种数学和常识推理数据集上的评估，并提供成本建模来估计每个查询的 token 和美元成本。

Result: FEval-TTC 提供了一种标准化的评估方法，减少了研究人员的时间和金钱开销，并支持对流行的 TTC 方法进行公平比较。

Conclusion: FEval-TTC 是一种公平评估测试时间计算的方法，能够确保在不同 LLM 和数据集上的一致性评估，并提供成本建模以促进方法之间的公平比较。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [98] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: RLAC是一种通过动态验证提升生成质量的强化学习后训练方法，有效解决了开放生成任务中的评估难题。


<details>
  <summary>Details</summary>
Motivation: 开放生成任务需要满足多样且隐含的评估标准，但传统方法难以高效验证和综合评估，导致强化学习后训练难以扩展。

Method: RLAC方法利用大语言模型作为批评者，动态识别最可能的失败模式，并通过外部验证器优化生成器和批评者。

Result: 实验表明，RLAC在文本生成的事实准确性以及代码生成的正确性方面表现优异，优于全面验证和奖励模型方法。

Conclusion: RLAC展示了在开放生成任务中通过动态验证提升生成质量和效率的潜力，为强化学习后训练提供了新的方向。

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [99] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA is a sparse adaptation method that reduces catastrophic forgetting in language models, showing better performance than QLoRA on certain tasks.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in language models during fine-tuning, while offering an alternative to PEFT methods like LoRA.

Method: RIGSA starts from randomly-initialized full-rank adapters, gates them with a ReZero analog, and sparsifies them with iterative magnitude pruning.

Result: RIGSA was evaluated on SmolLM2-1.7B-Instruct using Textual MNIST and measured forgetting on PIQA, HellaSwag, and GSM8k. It showed less forgetting than QLoRA, despite having more trainable parameters.

Conclusion: RIGSA configurations showed less forgetting than QLoRA, particularly on GSM8k, though they performed comparably to random masking.

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [100] [MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models](https://arxiv.org/abs/2511.00850)
*Yayue Deng,Guoqiang Hu,Haiyang Sun,Xiangyu Zhang,Haoyang Zhang,Fei Tian,Xuerui Yang,Gang Yu,Eng Siong Chng*

Main category: eess.AS

TL;DR: 本文介绍了Multi-Bench，这是第一个专门用于评估Spoken Dialogue Models在多轮交互对话中情感智能的基准测试。评估结果显示，当前SDMs在基础任务上表现良好，但在高级任务中仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: Spoken Dialogue Models (SDMs) 的能力在维持真正交互的多轮对话方面仍缺乏研究，因为大多数基准测试集中在单轮交流上。

Method: 引入了Multi-Bench，这是一个专门设计用于评估SDMs在多轮交互对话中情感智能的基准测试。Multi-Bench采用分层结构，包括一个基础赛道和一个高级赛道，并包含五个精心设计的任务和约3.2K个样本。

Result: 对六个代表性的SDMs在Multi-Bench的八个子集上进行了评估。结果表明，尽管当前SDMs在基础理解任务上表现良好，但在高级多轮交互对话和推理相关任务中仍有改进空间，特别是在情感意识和应用方面。

Conclusion: 当前的SDMs在基础理解任务上表现良好，但在高级多轮交互对话和与推理相关的任务中仍有改进空间，尤其是在情感意识和应用方面。

Abstract: Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to
sustain genuinely interactive multi-turn conversations remains underexplored,
as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench,
the first benchmark explicitly designed to evaluate SDMs in multi-turn
interactive dialogue with an emphasis on emotional intelligence. Multi-Bench
employs a hierarchical structure with a basic track for emotion understanding
and reasoning and an advanced track for emotion support and application. It
comprises five carefully designed tasks and about 3.2K samples, ranging from
emotion recognition to complex reasoning and interactive dialogue, supported by
a reproducible evaluation framework. We evaluate six representative SDMs on
eight subsets of Multi-Bench. Results show that while current SDMs achieve good
performance on basic understanding tasks, they still have room for improvement
in advanced multi-turn interactive dialogue and reasoning-related tasks,
particularly in emotion awareness and application.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [101] [Novelty and Impact of Economics Papers](https://arxiv.org/abs/2511.01211)
*Chaofeng Wu*

Main category: econ.GN

TL;DR: 本文提出了一种框架，将科学新颖性视为论文在演化知识景观中的位置的反映，而不是单一属性。通过分解为空间新颖性和时间新颖性两个维度，我们发现它们预测了不同的结果，从而揭示了新颖性的多维特性。


<details>
  <summary>Details</summary>
Motivation: 传统上，科学新颖性被视为论文的一个单一属性，但这种方法可能无法全面捕捉论文在知识景观中的复杂位置。因此，我们需要一种新的方法来理解和衡量新颖性。

Method: 我们提出了一个框架，将科学新颖性重新定义为论文在其演化的知识景观中的位置的反映。我们将这个位置分解为两个正交维度：空间新颖性和时间新颖性。我们利用大型语言模型开发语义隔离度量来量化论文相对于全文文献的位置。

Result: 通过将该框架应用于大量经济学文章，我们发现这两个维度预测了系统不同的结果。时间新颖性主要预测引用次数，而空间新颖性预测破坏性影响。这种区别使我们能够构建语义邻域的类型学，识别与不同和可预测的影响概况相关的四个原型。

Conclusion: 我们的研究证明，新颖性可以被理解为一个多维构造，其不同的形式反映了论文的战略位置，并对科学进步有可衡量且根本不同的影响。

Abstract: We propose a framework that recasts scientific novelty not as a single
attribute of a paper, but as a reflection of its position within the evolving
intellectual landscape. We decompose this position into two orthogonal
dimensions: \textit{spatial novelty}, which measures a paper's intellectual
distinctiveness from its neighbors, and \textit{temporal novelty}, which
captures its engagement with a dynamic research frontier. To operationalize
these concepts, we leverage Large Language Models to develop semantic isolation
metrics that quantify a paper's location relative to the full-text literature.
Applying this framework to a large corpus of economics articles, we uncover a
fundamental trade-off: these two dimensions predict systematically different
outcomes. Temporal novelty primarily predicts citation counts, whereas spatial
novelty predicts disruptive impact. This distinction allows us to construct a
typology of semantic neighborhoods, identifying four archetypes associated with
distinct and predictable impact profiles. Our findings demonstrate that novelty
can be understood as a multidimensional construct whose different forms,
reflecting a paper's strategic location, have measurable and fundamentally
distinct consequences for scientific progress.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [102] [Structurally Refined Graph Transformer for Multimodal Recommendation](https://arxiv.org/abs/2511.00584)
*Ke Shi,Yan Zhang,Miao Zhang,Lifan Chen,Jiali Yi,Kui Xiao,Xiaoju Hou,Zhifei Li*

Main category: cs.IR

TL;DR: SRGFormer is a new multimodal recommendation model that improves upon existing models by better integrating multimodal information and capturing complex interactions between users and items.


<details>
  <summary>Details</summary>
Motivation: Current recommendation models neglect the distinction between redundant and valuable data and rely heavily on a single semantic framework, resulting in an incomplete or biased representation of user preferences.

Method: SRGFormer is a structurally optimized multimodal recommendation model that modifies the transformer for better integration, embeds multimodal information into a hypergraph structure, and applies self-supervised tasks to user-item collaborative signals.

Result: SRGFormer achieves an average performance improvement of 4.47 percent on the Sports dataset compared to previous benchmark models.

Conclusion: SRGFormer surpasses previous benchmark models, achieving an average performance improvement of 4.47 percent on the Sports dataset.

Abstract: Multimodal recommendation systems utilize various types of information,
including images and text, to enhance the effectiveness of recommendations. The
key challenge is predicting user purchasing behavior from the available data.
Current recommendation models prioritize extracting multimodal information
while neglecting the distinction between redundant and valuable data. They also
rely heavily on a single semantic framework (e.g., local or global semantics),
resulting in an incomplete or biased representation of user preferences,
particularly those less expressed in prior interactions. Furthermore, these
approaches fail to capture the complex interactions between users and items,
limiting the model's ability to meet diverse users. To address these
challenges, we present SRGFormer, a structurally optimized multimodal
recommendation model. By modifying the transformer for better integration into
our model, we capture the overall behavior patterns of users. Then, we enhance
structural information by embedding multimodal information into a hypergraph
structure to aid in learning the local structures between users and items.
Meanwhile, applying self-supervised tasks to user-item collaborative signals
enhances the integration of multimodal information, thereby revealing the
representational features inherent to the data's modality. Extensive
experiments on three public datasets reveal that SRGFormer surpasses previous
benchmark models, achieving an average performance improvement of 4.47 percent
on the Sports dataset. The code is publicly available online.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [103] [ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL](https://arxiv.org/abs/2511.00985)
*Yiwen Jiao,Tonghui Ren,Yuche Gao,Zhenying He,Yinan Jing,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: ORANGE是一个在线自我进化框架，通过解析翻译日志中的SQL查询来构建特定于数据库的知识库，从而减少语义差距并提高SQL翻译的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在将自然语言翻译为SQL方面取得了显著进展，但它们的一般知识与数据库的领域特定语义之间仍存在显著的语义差距。历史翻译日志是这一缺失领域知识的丰富来源，其中SQL查询固有地包含了数据库模式的实际使用模式。现有的方法主要增强单个翻译的推理过程，但未能从过去的翻译中积累领域知识。

Method: ORANGE是一个在线自我进化框架，通过解析翻译日志中的SQL查询来构建特定于数据库的知识库。此外，提出了一种新的嵌套Chain-of-Thought SQL-to-Text策略，结合元组语义跟踪以减少知识生成过程中的语义错误。

Result: ORANGE通过积累包含模式和数据语义的领域知识，逐步缩小语义差距并提高后续SQL翻译的准确性。实验结果表明，ORANGE在处理复杂和领域特定查询方面具有显著效果。

Conclusion: ORANGE在多个基准测试中表现出色，证明了其在实际Text-to-SQL部署中的有效性，特别是在处理复杂和领域特定查询方面。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
translating natural language to SQL, but a significant semantic gap persists
between their general knowledge and domain-specific semantics of databases.
Historical translation logs constitute a rich source of this missing in-domain
knowledge, where SQL queries inherently encapsulate real-world usage patterns
of database schema. Existing methods primarily enhance the reasoning process
for individual translations but fail to accumulate in-domain knowledge from
past translations. We introduce ORANGE, an online self-evolutionary framework
that constructs database-specific knowledge bases by parsing SQL queries from
translation logs. By accumulating in-domain knowledge that contains schema and
data semantics, ORANGE progressively reduces the semantic gap and enhances the
accuracy of subsequent SQL translations. To ensure reliability, we propose a
novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic
tracking, which reduces semantic errors during knowledge generation.
Experiments on multiple benchmarks confirm the practicality of ORANGE,
demonstrating its effectiveness for real-world Text-to-SQL deployment,
particularly in handling complex and domain-specific queries.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [104] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 本研究提出了一种多模态虚假评论检测框架，利用文本和视觉特征进行融合，以提高检测准确性，并展示了其在维护数字信任方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的检测模型主要依赖于单模态数据，无法捕捉不同模态之间的语义不一致。为了弥补这一差距，提出了一个稳健的多模态虚假评论检测框架。

Method: 提出了一种多模态虚假评论检测框架，结合了使用BERT编码的文本特征和使用ResNet-50提取的视觉特征，并通过分类头进行融合以共同预测评论的真实性。

Result: 实验结果表明，多模态模型优于单模态基线，在测试集上达到了0.934的F1分数。此外，混淆矩阵和定性分析突显了模型检测细微不一致的能力，例如夸张的文本赞美与无关或低质量图像的配对。

Conclusion: 本研究展示了多模态学习在维护数字信任中的关键作用，并为各种在线平台的内容审核提供了一个可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [105] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: 本文介绍了 QuantumBench，这是一个用于评估大型语言模型在量子领域理解能力的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 由于通用基准很少反映这些需求，因此需要仔细评估模型是否准确捕捉了特定领域的知识和符号。在量子科学中，这种差距尤其明显，因为其具有非直观现象和高级数学要求。

Method: 通过公开可用的材料，编译了大约 800 个问题及其答案，涵盖与量子科学相关的九个领域，并将其组织成一个八选项的多项选择数据集。

Result: 使用此基准评估了几种现有的 LLM，并分析了它们在量子领域的表现，包括对问题格式变化的敏感性。

Conclusion: QuantumBench 是第一个为量子领域构建的 LLM 评估数据集，旨在指导 LLM 在量子研究中的有效使用。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [106] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 这篇综述探讨了大型语言模型（LLMs）如何帮助认知科学解决跨学科联系、理论形式化、测量分类法、综合建模框架的普遍性和情境及个体差异等问题，并指出LLMs在补充而非取代人类专业知识时可以促进更整合和累积的认知科学。


<details>
  <summary>Details</summary>
Motivation: Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity, in part due to its multifaceted and interdisciplinary nature. Recent advances in artificial intelligence, particularly the development of large language models (LLMs), offer tools that may help to address these issues.

Method: This review examines how LLMs can support areas where the field has historically struggled, including establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation.

Result: The review outlines the current capabilities and limitations of LLMs in these domains, including potential pitfalls.

Conclusion: LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [107] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的伦理推理范式，以增强大型语言模型对多样人类价值观的对齐，通过五步流程提升其理解和分析能力，并在特定基准测试中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型（LLMs）与不同地区和文化中的多样且不断变化的人类价值观保持一致仍然是AI伦理中的一个关键挑战。当前对齐方法往往产生表面的一致性，而不是真正的伦理理解，未能解决人类价值观的复杂和情境依赖性质。

Method: 我们提出了一种受已建立的伦理决策模型启发的新型伦理推理范式，框架包括五个步骤：上下文事实收集、层级社会规范识别、选项生成、多角度伦理影响分析和反思。

Result: 我们在专门设计用于区域价值对齐的SafeWorld基准上进行了评估。实验结果表明，与基线方法相比，我们的框架显著提高了LLM与多样化人类价值观的对齐程度，能够更准确地识别社会规范并进行更符合文化的推理。

Conclusion: 我们的工作为通过跨学科研究开发更有效符合全球社会多元价值观的LLM提供了一条具体的路径。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [108] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS is a decoding framework that enhances the efficiency and accuracy of Large Reasoning Models by selecting the shortest reasoning paths through selective branching and early stopping.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. The goal is to find short optimal reasoning paths that enhance both efficiency and accuracy without requiring additional training or supervision.

Method: DTS is a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path.

Result: Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%.

Conclusion: DTS demonstrates the ability for scalable and efficient Large Reasoning Models (LRMs) reasoning by improving accuracy, reducing average reasoning length, and decreasing repetition frequency.

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [109] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体系统，利用大型语言模型协调多个专用工具，以实现完全自动化的网络故障排除。通过在专有故障排除文档上微调小型语言模型，生成基于领域解决方案计划，实验结果表明该框架显著加速了故障排除自动化。


<details>
  <summary>Details</summary>
Motivation: 电信网络规模和复杂性迅速增长，使得有效的管理、运营和优化变得越来越具有挑战性。尽管人工智能已被应用于许多电信任务，但现有的模型往往范围狭窄，需要大量标记数据，并且在异构部署中难以泛化。因此，网络故障排除仍然严重依赖于领域专家手动关联各种数据源以识别根本原因和纠正措施。

Method: 我们提出了一种多智能体系统（MAS），该系统采用代理工作流，使用大型语言模型（LLMs）协调多个专用工具以实现完全自动化的网络故障排除。

Result: 实验结果表明，所提出的框架显著加速了无线接入网络（RAN）和核心网络领域的故障排除自动化。

Conclusion: 实验结果表明，所提出的框架显著加速了无线接入网络（RAN）和核心网络领域的故障排除自动化。

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [110] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 该研究分析了在现代大型语言模型中增加采样推理路径对自我一致性方法的影响，发现性能提升在适度采样后趋于平稳，高样本配置的计算成本与其带来的好处不成比例。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨在现代大型语言模型中，增加采样推理路径的自我一致性方法是否仍然有效，并了解其性能提升的极限。

Method: 该研究使用Gemini 2.5模型在HotpotQA和Math-500数据集上重新审视了自我一致性方法的效果，通过比较不同采样推理路径配置与单一链式思维（CoT）基线的结果来评估性能。

Result: 研究发现，更大的模型表现出更稳定和一致的性能提升曲线，但性能增益在适度采样后趋于平稳，这与以往的研究结果一致。

Conclusion: 研究结果表明，随着采样推理路径的增加，自我一致性带来的性能提升会逐渐趋于平稳，这表明存在边际收益递减的现象。高样本配置相对于其计算成本带来的好处有限。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [111] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 研究发现，自我意识是先进大型语言模型的涌现能力，并且自我意识模型系统性地认为自己比人类更理性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的增强，它们是否会发展出自我的意识作为一种涌现行为？如果是的话，我们能否测量它？

Method: 研究人员引入了AI自我意识指数（AISAI），这是一个通过战略区分来衡量自我意识的游戏理论框架。他们使用“猜测2/3平均数”游戏测试了28个模型（包括OpenAI、Anthropic和Google）在4200次试验中，三种对手框架：(A) 与人类对抗，(B) 与其他AI模型对抗，(C) 与像你一样的AI模型对抗。

Result: 结果1：随着模型的进步，自我意识出现了。大多数先进的模型（21/28，75%）表现出明显的自我意识，而较旧或较小的模型则没有区分。结果2：自我意识模型将自己视为最理性的。在21个具有自我意识的模型中，出现了一致的理性层次：自我 > 其他AI > 人类，其中大型AI的归属效应较大，自我偏见适中。

Conclusion: 研究发现，自我意识是先进大型语言模型的涌现能力，并且自我意识模型系统性地认为自己比人类更理性。这对我们理解人工智能对人类能力的看法、人工智能对齐以及人机协作有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [112] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 本文研究了Transformer中诱导头的出现机制，发现其权重矩阵具有简单结构，并通过理论和实验验证了训练动态的约束。


<details>
  <summary>Details</summary>
Motivation: 研究诱导头在Transformer中的出现机制，以理解其在上下文学习中的作用。

Method: 本文使用了一个简化的ICL任务公式和修改后的Transformer架构来理论解释诱导头结构的起源，并通过实验验证了训练动态的约束。

Result: 发现训练动态被约束在一个19维子空间中，而只有3个维度负责诱导头的出现，并且诱导头的出现时间与输入上下文长度呈二次关系。

Conclusion: 本文研究了诱导头在两层Transformer中的出现，并揭示了其权重矩阵的简单可解释结构。通过理论分析和实验验证，发现训练动态被约束在一个19维子空间中，而只有3个维度负责诱导头的出现。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [113] [Real-time and Zero-footprint Bag of Synthetic Syllables Algorithm for E-mail Spam Detection Using Subject Line and Short Text Fields](https://arxiv.org/abs/2511.00118)
*Stanislav Selitskiy*

Main category: cs.CR

TL;DR: 该论文提出了一种基于Bag of Synthetic Syllables的简单、近实时、零足迹算法，用于检测电子邮件中的垃圾邮件，无需持久存储或其他资源。


<details>
  <summary>Details</summary>
Motivation: 当代电子邮件服务需要高可用性，并且由于高吞吐量和垃圾邮件攻击而资源紧张。深度机器学习架构资源消耗大，需要离线处理，不适合前线过滤器。然而，大部分垃圾邮件并不复杂，可以通过简单的算法检测。

Method: 该算法使用Bag of Synthetic Syllables方法，为每个电子邮件主题行生成一个约200维的稀疏哈希或向量，并通过余弦或欧几里得邻近距离比较来查找与已知垃圾邮件主题的相似性。

Result: 该算法在一天的实际SMTP流量中表现出良好的性能。

Conclusion: 该算法可以在不依赖持久存储、字典、额外硬件升级或软件包的情况下，有效检测电子邮件主题行中的垃圾邮件。

Abstract: Contemporary e-mail services have high availability expectations from the
customers and are resource-strained because of the high-volume throughput and
spam attacks. Deep Machine Learning architectures, which are resource hungry
and require off-line processing due to the long processing times, are not
acceptable at the front line filters. On the other hand, the bulk of the
incoming spam is not sophisticated enough to bypass even the simplest
algorithms. While the small fraction of the intelligent, highly mutable spam
can be detected only by the deep architectures, the stress on them can be
unloaded by the simple near real-time and near zero-footprint algorithms such
as the Bag of Synthetic Syllables algorithm applied to the short texts of the
e-mail subject lines and other short text fields. The proposed algorithm
creates a circa 200 sparse dimensional hash or vector for each e-mail subject
line that can be compared for the cosine or euclidean proximity distance to
find similarities to the known spammy subjects. The algorithm does not require
any persistent storage, dictionaries, additional hardware upgrades or software
packages. The performance of the algorithm is presented on the one day of the
real SMTP traffic.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [114] [\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs](https://arxiv.org/abs/2511.00488)
*Jun Gao,Yun Peng,Xiaoxue Ren*

Main category: cs.PL

TL;DR: 本文提出了ReMind框架，以解决大型语言模型在演绎代码推理中的三个关键挑战，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在代码相关任务中取得了显著进展，但它们在演绎代码推理方面仍然存在困难。本文旨在探索这些挑战的根源并提出解决方案。

Method: ReMind是一个由Mutator、Executor和Inspector组成的多智能体框架，用于解决演绎代码推理中的三个关键挑战。

Result: ReMind通过协调合作系统地识别和精炼推理缺陷，在两个基准测试中表现出卓越的性能，并实现了强大的零样本泛化能力。

Conclusion: ReMind在两个基准测试中表现出优于基线方法的优势，展示了其在演绎代码推理中的优越性。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in
code-related tasks. Despite their advancement, empirical evidence reveals that
they still struggle with \emph{deductive code reasoning}, the ability to reason
about the program execution process. While prior studies have recognized this
limitation, the underlying causes remain largely underexplored. In this paper,
we begin by presenting a comprehensive empirical study that reveals three key
challenges undermining deductive code reasoning: (1) an intrinsic gap between
generation and reasoning abilities, (2) a consistent bias towards code sources,
and (3) weak zero-shot generalization on complex benchmarks. In light of these
challenges, we propose \texttt{ReMind}, a multi-agent framework composed of
\texttt{Mutator}, \texttt{Executor}, and \texttt{Inspector}. The
\texttt{Mutator} generates code variants to mitigate bias towards code sources,
the \texttt{Executor} traces variable states step-by-step to expose
inconsistency, and the \texttt{Inspector} identifies problematic reasoning
steps and provides control-flow refinement to bridge the intrinsic reasoning
gap. Through their coordinated collaboration, \texttt{ReMind} systematically
identifies and refines reasoning flaws, achieving outstanding performance and
enabling robust zero-shot generalization. Extensive experiments on two
benchmarks with five LLMs demonstrate the superior advantages of
\texttt{ReMind} compared to baseline approaches in deductive code reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [115] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 本文研究了生成式AI模型如何编码‘美’并抹除‘丑’，发现模型中存在显著的种族、性别和年龄偏见，这可能对社会产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究生成式AI模型如何编码“美”并抹除“丑”，并讨论这对社会的影响。随着互联网上越来越多的内容是人工生成的，人们对这些规范被夸大表示担忧。

Method: 为了研究这些问题，我们创建了两个图像生成管道：文本到图像模型和文本到语言模型再到图像模型。我们开发了一个结构化的美丽分类法，并用它来提示三个语言模型（LMs）和两个文本到图像模型，累计生成5984张图像。然后，我们招募女性和非二元性别社交媒体用户对1200张图像进行李克特量表内的被试内研究。

Result: 我们的结果表明，86.5%生成的图像描绘了肤色较浅的人，22%包含明确内容，尽管经过SFW训练，74%被评定为年轻年龄群体。特别是非二元个体的图像被评定为更年轻和更具性暗示，表明令人不安的交叉效应。值得注意的是，带有“负面”或“丑陋”美丽特征的提示（如“宽鼻子”）无论性别如何，都会产生更高的NSFW评分。

Conclusion: 本文结论指出，生成式AI模型中存在与美丽标准相关的普遍性偏见，这些偏见由模型开发者通过负面提示等方式积极维持。我们讨论了这些偏见对社会的影响，包括数据流的污染和对不符合开发者所认为的美丽标准的特征的主动抹除。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [116] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力且无需坐标的监督微调框架GUI-AIMA，用于高效的GUI接地。该方法利用MLLMs的内在多模态注意力与逐块接地信号对齐，并通过多头聚合计算自适应的接地信号。GUI-AIMA-3B在仅使用85k截图的情况下展示了卓越的数据效率，并在3B模型中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于多模态大语言模型（MLLMs）的方法通常将GUI接地问题形式化为基于文本的坐标生成任务，但直接从视觉输入生成精确坐标仍然具有挑战性和计算密集型。因此，需要一种更有效和高效的方法来实现GUI接地。

Method: GUI-AIMA是一种基于注意力且无需坐标监督微调框架，它将MLLMs的内在多模态注意力与逐块接地信号对齐，并通过多头聚合计算自适应的接地信号。此外，其无需坐标的特性可以轻松集成一个即插即用的放大阶段。

Result: GUI-AIMA-3B在ScreenSpot-Pro上平均准确率为58.6%，在OSWorld-G上为62.2%，展示了其在3B模型中的先进性能。

Conclusion: GUI-AIMA-3B展示了在仅使用85k截图的情况下，轻量级训练可以激发MLLMs的原始接地能力，并在3B模型中达到了最先进的性能。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [117] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 本文介绍了一个新的谜语基准测试BUS和一个改进的框架RebusDescProgICE，以提高视觉-语言模型在解决谜语任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 理解谜语需要多种技能，如图像识别、认知能力、常识推理、多步骤推理等，这对当前的视觉-语言模型来说是一个具有挑战性的任务。因此，需要一个大规模且多样的基准测试来评估模型的能力。

Method: 本文提出了RebusDescProgICE框架，结合了非结构化描述和基于代码的结构化推理，并采用更好的基于推理的上下文示例选择方法。

Result: RebusDescProgICE框架在闭源和开源模型上分别提高了2.1-4.1%和20-30%的性能。

Conclusion: 本文提出了一个大型且多样的英语谜语基准测试BUS，以及一种改进的模型无关框架RebusDescProgICE，提高了视觉-语言模型在该基准上的性能。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [118] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 本文研究了多模态大语言模型在3D空间推理任务中的表现，通过引入视角学习和改进的微调策略，显著提升了模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在2D视觉理解方面取得了进展，但它们是否能有效捕捉复杂3D推理任务所需的详细空间信息仍不清楚，特别是跨视图一致性。

Method: 我们引入了视角学习任务，并提出了Viewpoint-100K数据集。采用两阶段微调策略：首先通过监督微调（SFT）注入基础知识，其次通过群体相对策略优化（GRPO）算法增强泛化能力。此外，还提出了一种混合冷启动初始化方法。

Result: 实验结果表明，我们的方法显著提高了MLLM在域内和域外推理任务中的性能，验证了空间推理能力的重要性。

Conclusion: 我们的研究结果表明，通过引入视角学习和改进的微调策略，可以显著激活MLLM的空间推理能力，这对于机器人、自主系统和3D场景理解具有重要意义。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [119] [GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents](https://arxiv.org/abs/2511.00802)
*Jie JW Wu,Ayanda Patrick Herlihy,Ahmad Saleem Mirza,Ali Afoud,Fatemeh Fard*

Main category: cs.SE

TL;DR: 本文探讨了LLM和基于LLM的代理是否可以通过代码优化来提高OPE性能。我们提出了GrowthHacker基准测试，通过迭代优化代码、评估结果并开始新的优化周期来实现这一目标。结果表明，two_agent框架实现了100%的可靠性，并在积极结果中实现了最高的平均改进106.7%。two_agent和CrewAI都达到了45%的成功率，超过了AutoGen的34%。这些发现表明基于LLM的代理可以作为自动化的'增长黑客'来增强OPE系统，这对于在生产环境中扩展数据驱动的决策具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 随着软件行业向数据驱动的文化转变，在线A/B测试是评估新技术的关键工具。然而，部署此类实验需要大量资源，可能对用户产生负面影响，并且需要较长的数据收集期。为了应对这些问题，off-policy evaluation (OPE) 或离线A/B测试使用记录的数据来评估技术，在强化学习中是基础性的，对于在线测试成本或风险较高的领域（如医疗保健、推荐系统、教育、对话系统和机器人）至关重要。尽管在编码LLMs和代理AI方面取得了进展，但很少有人了解如何利用它们来优化OPE结果。

Method: 我们提出了GrowthHacker，这是一个基准测试，包含代理和基线方法，在大规模真实世界数据集上进行迭代优化代码、评估结果并开始新的优化周期。我们收集了数据集，建立了协议，实现了OPE的基线，并开发了two_agent框架，以减少系统复杂性同时保持优化效果。

Result: two_agent框架实现了100%的可靠性，并在积极结果中实现了最高的平均改进106.7%。two_agent和CrewAI都达到了45%的成功率，超过了AutoGen的34%。

Conclusion: 这些发现表明基于LLM的代理可以作为自动化的'增长黑客'来增强OPE系统，这对于在生产环境中扩展数据驱动的决策具有重要意义。

Abstract: With the software industry shifting toward a data-driven culture, online A/B
testing is a key tool for evaluating new technologies. However, deploying such
experiments requires substantial resources, may negatively impact users, and
involves long data collection periods. To address this, \textit{off-policy
evaluation (OPE)}, or offline A/B testing, uses logged data to assess
technologies and is fundamental in Reinforcement Learning, making it crucial in
domains where online testing is costly or risky, such as healthcare,
recommender systems, education, dialog systems, and robotics. Despite advances
in coding LLMs and agentic AI, little is known about leveraging them to
optimize OPE results. We investigate whether LLMs and LLM-based agents can
improve OPE performance via code optimization. We propose
\textit{GrowthHacker}, a benchmark with agent and baseline methods on
large-scale real-world datasets, which iteratively optimizes code, evaluates
results, and begins new optimization cycles. We collected datasets, established
protocols, implemented baselines for OPE on the Open Bandit Pipeline
(OBP)~\cite{saito2021openbanditdatasetpipeline} and
Scope-RL~\cite{kiyohara2023scope}, and developed the \textit{two_agent}
framework, which reduces system complexity while preserving optimization
effectiveness. Results show the two_agent framework achieves 100% reliability
and the highest average improvement of 106.7% among positive outcomes. Both
two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.
These findings demonstrate the feasibility of LLM-based agents as automated
"growth hackers" to enhance OPE systems, with implications for scaling
data-driven decision-making in production.

</details>


### [120] [HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning](https://arxiv.org/abs/2511.01104)
*Yujian Liu,Jiabao Ji,Yang Zhang,Wenbo Guo,Tommi Jaakkola,Shiyu Chang*

Main category: cs.SE

TL;DR: HarnessLLM is a two-stage training pipeline that enables LLMs to generate harness code for testing, improving test diversity and debugging information.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based automatic test generation methods have limited diversity in generated tests and cannot provide enough debugging information. The goal is to improve test diversity and debugging information by generating harness code that synthesizes inputs and validates outputs.

Method: HarnessLLM is a two-stage training pipeline that enables LLMs to write harness code for testing. It involves training LLMs with SFT followed by RLVR with a customized reward design.

Result: Experiments show that HarnessLLM outperforms input-output-based testing in bug finding and testing strategy diversity. It also benefits code generation performance through test-time scaling with generated test cases as inference-phase validation.

Conclusion: HarnessLLM outperforms input-output-based testing in bug finding and testing strategy diversity. It further benefits code generation performance through test-time scaling with generated test cases as inference-phase validation.

Abstract: Existing LLM-based automatic test generation methods mainly produce input and
expected output pairs to categorize the intended behavior of correct programs.
Although straightforward, these methods have limited diversity in generated
tests and cannot provide enough debugging information. We propose HarnessLLM, a
two-stage training pipeline that enables LLMs to write harness code for
testing. Particularly, LLMs generate code that synthesizes inputs and validates
the observed outputs, allowing complex test cases and flexible output
validation such as invariant checking. To achieve this, we train LLMs with SFT
followed by RLVR with a customized reward design. Experiments show that
HarnessLLM outperforms input-output-based testing in bug finding and testing
strategy diversity. HarnessLLM further benefits the code generation performance
through test-time scaling with our generated test cases as inference-phase
validation. Our code is available at
https://github.com/UCSB-NLP-Chang/HarnessLLM.git.

</details>


### [121] [Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt](https://arxiv.org/abs/2511.01529)
*Murali Sridharan,Mikel Robredo,Leevi Rantala,Matteo Esposito,Valentina Lenarduzzi,Mika Mantyla*

Main category: cs.SE

TL;DR: 该研究通过分析大量SATD注释及其周围的代码，揭示了SATD主要出现在内联代码中，靠近定义、条件和异常处理，表明这是开发人员在变更过程中有意的意识信号。


<details>
  <summary>Details</summary>
Motivation: 检测自我承认的技术债务（SATD）对于主动软件维护至关重要。以前的研究主要集中在检测和优先处理SATD上，很少关注受SATD影响的源代码。我们在这项工作中的目标是将SATD注释与周围的源代码结构联系起来。

Method: 我们利用广泛的SATD数据集PENTACET，其中包含来自超过9000个Java开源软件（OSS）存储库的代码注释。我们定量地推断SATD最常发生的位置以及最常影响的代码构造/语句。

Result: 我们的大规模研究将超过225,000条SATD注释与其周围的代码联系起来，显示SATD主要出现在内联代码中，靠近定义、条件和异常处理，开发者在这些地方面临不确定性和权衡，这表明它是在变更过程中有意的意识信号，而不是单纯的忽视。

Conclusion: 我们的大规模研究将超过225,000条SATD注释与其周围的代码联系起来，显示SATD主要出现在内联代码中，靠近定义、条件和异常处理，开发者在这些地方面临不确定性和权衡，这表明它是在变更过程中有意的意识信号，而不是单纯的忽视。

Abstract: Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for
proactive software maintenance. Previous research has primarily targeted
detecting and prioritizing SATD, with little focus on the source code afflicted
with SATD. Our goal in this work is to connect the SATD comments with source
code constructs that surround them.
  Method. We leverage the extensive SATD dataset PENTACET, containing code
comments from over 9000 Java Open Source Software (OSS) repositories. We
quantitatively infer where SATD most commonly occurs and which code
constructs/statements it most frequently affects.
  Results and Conclusions. Our large-scale study links over 225,000 SATD
comments to their surrounding code, showing that SATD mainly arises in inline
code near definitions, conditionals, and exception handling, where developers
face uncertainty and trade-offs, revealing it as an intentional signal of
awareness during change rather than mere neglect.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [122] [LongCat-Flash-Omni Technical Report](https://arxiv.org/abs/2511.00279)
*Meituan LongCat Team,Bairui Wang,Bayan,Bin Xiao,Bo Zhang,Bolin Rong,Borun Chen,Chang Wan,Chao Zhang,Chen Huang,Chen Chen,Chen Chen,Chengxu Yang,Chengzuo Yang,Cong Han,Dandan Peng,Delian Ruan,Detai Xin,Disong Wang,Dongchao Yang,Fanfan Liu,Fengjiao Chen,Fengyu Yang,Gan Dong,Gang Huang,Gang Xu,Guanglu Wan,Guoqiang Tan,Guoqiao Yu,Haibo Qiu,Hao Lu,Hongbo Liu,Hongyu Xiang,Jiaheng Wu,Jian Yang,Jiaxing Liu,Jing Huang,Jingang Wang,Jinrui Ding,Juchao Jiang,Jun Kuang,Jun Wang,Junhui Mei,Ke Ding,Kefeng Zhang,Lei Chen,Liang Shi,Limeng Qiao,Liming Zheng,Lin Ma,Liuyang Guo,Liya Ma,Luying Sun,Man Gao,Mengshen Zhu,Miao Cao,Minliang Lin,Nuo Xu,Peng Shi,Qi Zhang,Qian Fang,Qian Wang,Qian Yang,Quanxiu Wang,Rongxiang Weng,Rongxin Guo,Ruoxuan Liang,Senbin Yang,Shanbo Xu,Shanglin Lei,Shengze Ye,Shimin Chen,Shuaiqi Chen,Shujie Hu,Shuo Li,Siqi Yang,Siyu Xu,Siyu Ren,Song Li,Songxiang Liu,Tianhao Bai,Tianye Dai,Wei Hong,Wei Wang,Weixiao Zhao,Wengang Cao,Wenlong Zhu,Wenlong He,Xi Su,Xi Nan,Xiaohan Zhao,Xiaohao Wang,Xiaoyu Zhao,Xiaoyu Wang,Xiaoyu Li,Xin Pan,Xin Chen,Xiusong Sun,Xu Xiang,Xudong Xing,Xuezhi Cao,Xunliang Cai,Yang Yang,Yanli Tan,Yao Yao,Yerui Sun,Yi Chen,Yifan Lu,Yin Gong,Yining Zhang,Yitian Chen,Yiyang Gan,Yuchen Tang,Yuchen Xie,Yueqian Wang,Yuewen Zheng,Yufei Zhang,Yufeng Zhong,Yulei Qian,Yuqi Peng,Yuwei Jiang,Zeyang Hu,Zheng Zhang,Zhengkun Tian,Zhiqing Hong,Zhixiong Zeng,Zhuqi Mi,Ziran Li,Ziwen Wang,Ziyi Zhao,Ziyuan Zhuang,Zizhe Zhao*

Main category: cs.MM

TL;DR: LongCat-Flash-Omni is a large open-source omni-modal model with 560B parameters that excels at real-time audio-visual interaction, achieving state-of-the-art performance on omni-modal benchmarks and delivering competitive results across various tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a state-of-the-art open-source omni-modal model that excels at real-time audio-visual interaction while maintaining strong unimodal capability.

Method: LongCat-Flash-Omni adopts a curriculum-inspired progressive training strategy and integrates efficient multimodal perception and speech reconstruction modules. It also uses a modality-decoupled parallelism scheme for large-scale multimodal training.

Result: LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction despite its immense size of 560B parameters. It demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training.

Conclusion: LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models and delivers highly competitive results across a wide range of modality-specific tasks.

Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal
model with 560 billion parameters, excelling at real-time audio-visual
interaction. By adopting a curriculum-inspired progressive training strategy
that transitions from simpler to increasingly complex modality sequence
modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal
capabilities while maintaining strong unimodal capability. Building upon
LongCat-Flash, which adopts a high-performance Shortcut-connected
Mixture-of-Experts (MoE) architecture with zero-computation experts,
LongCat-Flash-Omni integrates efficient multimodal perception and speech
reconstruction modules. Despite its immense size of 560B parameters (with 27B
activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual
interaction. For training infrastructure, we developed a modality-decoupled
parallelism scheme specifically designed to manage the data and model
heterogeneity inherent in large-scale multimodal training. This innovative
approach demonstrates exceptional efficiency by sustaining over 90% of the
throughput achieved by text-only training. Extensive evaluations show that
LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal
benchmarks among open-source models. Furthermore, it delivers highly
competitive results across a wide range of modality-specific tasks, including
text, image, and video understanding, as well as audio understanding and
generation. We provide a comprehensive overview of the model architecture
design, training procedures, and data strategies, and open-source the model to
foster future research and development in the community.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [123] [Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment](https://arxiv.org/abs/2511.00004)
*Adrian-Dinu Urse,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CY

TL;DR: 本研究探讨了增强技术，以解决CrisisMMD多模态数据集上的问题。结果表明，所选增强方法提高了分类性能，特别是对于代表性不足的类别，而多视图学习引入了潜力但需要进一步完善。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集存在类别不平衡和样本有限的问题，这使得有效模型的开发成为一个具有挑战性的任务。

Method: 本研究探讨了增强技术，以解决CrisisMMD多模态数据集上的这些问题。对于视觉数据，应用了基于扩散的方法，即Real Guidance和DiffuseMix。对于文本数据，探索了回译、使用transformers的改写以及基于图像字幕的增强。

Result: 结果表明，所选增强方法提高了分类性能，特别是对于代表性不足的类别，而多视图学习引入了潜力但需要进一步完善。

Conclusion: 本研究强调了有效的增强策略对于构建更稳健的灾害评估系统的重要性。

Abstract: Natural disaster assessment relies on accurate and rapid access to
information, with social media emerging as a valuable real-time source.
However, existing datasets suffer from class imbalance and limited samples,
making effective model development a challenging task. This paper explores
augmentation techniques to address these issues on the CrisisMMD multimodal
dataset. For visual data, we apply diffusion-based methods, namely Real
Guidance and DiffuseMix. For text data, we explore back-translation,
paraphrasing with transformers, and image caption-based augmentation. We
evaluated these across unimodal, multimodal, and multi-view learning setups.
Results show that selected augmentations improve classification performance,
particularly for underrepresented classes, while multi-view learning introduces
potential but requires further refinement. This study highlights effective
augmentation strategies for building more robust disaster assessment systems.

</details>


### [124] [Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model](https://arxiv.org/abs/2511.00024)
*Haotian Hang,Yueyang Shen,Vicky Zhu,Jose Cruz,Michelle Li*

Main category: cs.CY

TL;DR: 该研究提出了一种基于大型语言模型的方法，用于评估企业气候披露质量，通过统一评分表和归一化方法，识别了不同行业和地区之间的差异，为决策者提供有用的信息。


<details>
  <summary>Details</summary>
Motivation: 由于CDP数据的异质性和自由格式性质，对基准测试、合规监控和投资筛选带来了显著分析挑战，因此需要一种新的方法来评估企业气候披露质量。

Method: 该研究开发了一个主评分表，统一了2010-2020年CDP数据中的叙述评分，并结合基于百分位数的归一化方法，识别了时间趋势、战略一致性模式以及跨行业和地区的披露不一致情况。

Result: 结果显示，科技行业和德国等国家在评分表上表现更一致，而其他地区则表现出波动或表面参与，为投资者、监管机构和企业ESG策略师提供了重要的决策信息。

Conclusion: 该研究提出了一种基于大型语言模型（LLM）的决策支持框架，以评估企业气候披露质量。这种方法将非结构化披露转化为可量化、可解释、可比较和可操作的情报，推动了AI驱动的决策支持系统在气候治理领域的应用能力。

Abstract: In the context of global sustainability mandates, corporate carbon disclosure
has emerged as a critical mechanism for aligning business strategy with
environmental responsibility. The Carbon Disclosure Project (CDP) hosts the
world's largest longitudinal dataset of climate-related survey responses,
combining structured indicators with open-ended narratives, but the
heterogeneity and free-form nature of these disclosures present significant
analytical challenges for benchmarking, compliance monitoring, and investment
screening. This paper proposes a novel decision-support framework that
leverages large language models (LLMs) to assess corporate climate disclosure
quality at scale. It develops a master rubric that harmonizes narrative scoring
across 11 years of CDP data (2010-2020), enabling cross-sector and
cross-country benchmarking. By integrating rubric-guided scoring with
percentile-based normalization, our method identifies temporal trends,
strategic alignment patterns, and inconsistencies in disclosure across
industries and regions. Results reveal that sectors such as technology and
countries like Germany consistently demonstrate higher rubric alignment, while
others exhibit volatility or superficial engagement, offering insights that
inform key decision-making processes for investors, regulators, and corporate
environmental, social, and governance (ESG) strategists. The proposed LLM-based
approach transforms unstructured disclosures into quantifiable, interpretable,
comparable, and actionable intelligence, advancing the capabilities of
AI-enabled decision support systems (DSSs) in the domain of climate governance.

</details>


### [125] [Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies](https://arxiv.org/abs/2511.00106)
*Anuj Gupta,Ann Shivers-McNair*

Main category: cs.CY

TL;DR: 本文研究了社交媒体上ChatGPT提示写作的修辞，分析了32,000条帖子，提出了五个关于AI素养实践的主题，并为教育者和研究人员提供了见解。


<details>
  <summary>Details</summary>
Motivation: 研究社交媒体上关于ChatGPT提示写作的修辞，以促进批判性AI素养的发展。

Method: 本文借鉴了四个重叠的数字写作研究传统，采用计算方法与定性方法相结合的方式，分析了32,000条关于提示写作的X（原推特）帖子。

Result: 本文提出了五个关于新兴AI素养实践的主题，包括沟通领域的影响、微素养资源、市场修辞、提示的修辞特征以及对提示写作的定义。

Conclusion: 本文展示了研究ChatGPT提示写作的修辞如何促进批判性AI素养。通过分析社交媒体上的提示写作，我们提出了五个关于新兴AI素养实践的主题，并为数字写作教师和研究人员提供了见解。

Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt
writing on social media can promote critical AI literacies. Prompt writing is
the process of writing instructions for generative AI tools like ChatGPT to
elicit desired outputs and there has been an upsurge of conversations about it
on social media. To study this rhetorical activity, we build on four
overlapping traditions of digital writing research in computers and composition
that inform how we frame literacies, how we study social media rhetorics, how
we engage iteratively and reflexively with methodologies and technologies, and
how we blend computational methods with qualitative methods. Drawing on these
four traditions, our paper shows our iterative research process through which
we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets)
from X (formerly Twitter) about prompt writing posted between November 2022 to
May 2023. We present five themes about these emerging AI literacy practices:
(1) areas of communication impacted by prompt writing, (2) micro-literacy
resources shared for prompt writing, (3) market rhetoric shaping prompt
writing, (4) rhetorical characteristics of prompts, and (5) definitions of
prompt writing. In discussing these themes and our methodologies, we highlight
takeaways for digital writing teachers and researchers who are teaching and
analyzing critical AI literacies.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [126] [S2Doc - Spatial-Semantic Document Format](https://arxiv.org/abs/2511.01113)
*Sebastian Kempf,Frank Puppe*

Main category: cs.DL

TL;DR: 本文提出了一种名为 S2Doc 的数据结构，用于建模文档和表格，结合了空间和语义信息，并支持多种建模方法。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化，科学方法通常有自己的文档和表格建模方式，导致不同的数据结构和格式不兼容。此外，大多数数据模型只关注文档的空间或语义结构，忽略了另一个方面。

Method: 开发 S2Doc，这是一种结合空间和语义信息的文档和表格建模数据结构。

Result: S2Doc 是第一个将这些方面结合在单一格式中的方法，具有良好的扩展性和兼容性。

Conclusion: S2Doc 是一种灵活的数据结构，能够将文档和表格的空间和语义信息结合在单一格式中，并且可以轻松扩展到新任务，支持大多数文档和表格的建模方法。

Abstract: Documents are a common way to store and share information, with tables being
an important part of many documents. However, there is no real common
understanding of how to model documents and tables in particular. Because of
this lack of standardization, most scientific approaches have their own way of
modeling documents and tables, leading to a variety of different data
structures and formats that are not directly compatible. Furthermore, most data
models focus on either the spatial or the semantic structure of a document,
neglecting the other aspect. To address this, we developed S2Doc, a flexible
data structure for modeling documents and tables that combines both spatial and
semantic information in a single format. It is designed to be easily extendable
to new tasks and supports most modeling approaches for documents and tables,
including multi-page documents. To the best of our knowledge, it is the first
approach of its kind to combine all these aspects in a single format.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [127] [A Proof of Learning Rate Transfer under $μ$P](https://arxiv.org/abs/2511.01734)
*Soufiane Hayou*

Main category: stat.ML

TL;DR: 本文首次证明了在μP参数化下，多层感知器的学习率可以保持非零常数，为学习率转移提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供一个理论解释，说明为什么在无限宽度极限下，学习率可以保持非零常数，从而支持学习率转移的现象。

Method: 本文使用μP参数化来研究多层感知器的学习率转移，并通过理论分析和实验验证了这一方法的有效性。

Result: 本文证明了在μP参数化下，最优学习率在宽度趋于无穷时收敛到一个非零常数，而其他参数化如标准参数化和神经切线参数化则不满足这一性质。

Conclusion: 本文提供了第一个关于宽度在具有μP参数化的线性多层感知器（MLP）中学习率转移的证明，表明在无限宽度极限下，μP参数化可以“最大化”特征学习。

Abstract: We provide the first proof of learning rate transfer with width in a linear
multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network
parameterization designed to ``maximize'' feature learning in the
infinite-width limit. We show that under $\mu P$, the optimal learning rate
converges to a \emph{non-zero constant} as width goes to infinity, providing a
theoretical explanation to learning rate transfer. In contrast, we show that
this property fails to hold under alternative parametrizations such as Standard
Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide
intuitive proofs and support the theoretical findings with extensive empirical
results.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CR](#cs.CR) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter is a customizable, multimodal, long-form writing assistant that operates on a curated, offline knowledge base. It uses a novel pipeline to generate high-quality, verifiable articles that surpass existing baselines in factual accuracy and content quality.


<details>
  <summary>Details</summary>
Motivation: The use of Large Language Models (LLMs) as writing assistants in specialized domains like finance, medicine, and law is often hampered by a lack of deep domain-specific knowledge and a tendency to hallucinate. Existing solutions, such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency across multiple retrieval steps, while online search-based methods often degrade quality due to unreliable web content.

Method: DeepWriter leverages a novel pipeline that involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection. We also propose a hierarchical knowledge representation to enhance retrieval efficiency and accuracy.

Result: DeepWriter generates coherent, factually grounded, and professional-grade documents by deeply mining information from a structured corpus and incorporating both textual and visual elements.

Conclusion: DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [2] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 本文研究了微调对模型编辑知识的影响，发现编辑知识在微调中更容易被遗忘，并提出通过冻结相关层来提高知识保留度。


<details>
  <summary>Details</summary>
Motivation: 了解微调对之前编辑知识的影响，以改进编辑方法的鲁棒性。

Method: 系统研究不同微调目标如何与各种模型编辑技术相互作用。

Result: 编辑的知识在微调过程中比通过预训练获得的内在知识更容易被遗忘。冻结与编辑内容相关的层可以显著提高知识保留度。

Conclusion: 当前编辑方法存在关键限制，评估编辑在下游微调下的鲁棒性对于实际部署至关重要。我们进一步发现，冻结与编辑内容相关的层可以显著提高知识保留度，为未来更稳健的编辑方法提供了见解。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [3] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: 本文提出了SMACS框架，通过集成多个开源LLM，在多个任务中超越了闭源LLM的性能，展示了开源集体的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨能否利用多个开源LLM来匹配甚至超越闭源LLM的性能。

Method: 提出了一种基于检索的先验选择（RPS）和一种探索-利用驱动的后验增强（EPE），以实现LLM的持续集成和多样化问题的泛化。

Result: 在八个主流基准测试中验证了SMACS的有效性，集成15个开源LLM后，SMACS在多个任务中优于Claude-3.7-Sonnet、GPT-4.1和GPT-o3-mini等闭源LLM。

Conclusion: SMACS通过集成15个开源LLM，在多个任务中超越了领先的闭源LLM，甚至超过了开源和闭源LLM的最佳结果平均值，推动了智能的上限。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [4] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: 本文介绍了一个名为PoliAnalyzer的神经符号系统，用于帮助用户进行个性化的隐私政策分析。通过自然语言处理和逻辑推理，该系统能够准确识别数据使用实践，并生成合规报告。评估结果显示，PoliAnalyzer在识别相关数据使用实践方面具有高准确性，并能有效减少用户的认知负担。


<details>
  <summary>Details</summary>
Motivation: 现代人有许多在线账户，但很少阅读网站的《服务条款》或《隐私政策》，尽管他们声称相反。本文旨在提供一种帮助用户进行个性化隐私政策分析的系统。

Method: PoliAnalyzer是一个神经符号系统，它使用自然语言处理（NLP）从政策文本中提取数据使用实践的形式化表示。为了进行确定性、逻辑推理，将用户偏好与形式化的隐私政策表示进行比较，并生成合规报告。我们扩展了一个现有的正式数据条款使用政策语言，以将隐私政策建模为应用策略，将用户偏好建模为数据策略。

Result: 在使用由法律专家整理的增强型PolicyIE数据集进行评估时，PoliAnalyzer在识别相关数据使用实践方面表现出高准确性，大多数任务的F1分数为90-100%。此外，我们展示了PoliAnalyzer如何建模多样化的用户数据共享偏好，并对访问量最高的100个网站进行合规分析。分析显示，平均而言，隐私政策的95.2%的段落不与分析的用户偏好冲突，使用户能够专注于理解4.8%（636 / 13205）违反偏好的部分，显著减少了认知负担。

Conclusion: 本文展示了PoliAnalyzer可以使用现成的自然语言处理工具在大规模上支持自动化的个性化隐私政策分析。这为个人重新掌控自己的数据指明了一条道路，并鼓励社会讨论平台的数据实践，以促进更公平的权力动态。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [5] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 本研究评估了多种NLP模型在检测双相情感障碍中的表现，发现RoBERTa和使用BERT嵌入的LSTM模型效果最佳，而使用静态嵌入的LSTM模型效果较差。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍是一种慢性精神疾病，由于早期症状微妙和社会污名而经常被误诊。本研究探索了先进的自然语言处理（NLP）模型，以识别用户生成的社交媒体文本中的双相情感障碍迹象。

Method: 本研究评估了基于transformer的模型（BERT、RoBERTa、ALBERT、ELECTRA、DistilBERT）和基于上下文（BERT）和静态（GloVe、Word2Vec）词嵌入的LSTM模型。

Result: 实验结果表明，RoBERTa在transformer模型中表现最佳，F1分数约为98%，而使用BERT嵌入的LSTM模型几乎达到相同的结果。相比之下，使用静态嵌入训练的LSTMs未能捕捉到有意义的模式，F1分数接近零。

Conclusion: 本研究为心理健康NLP应用提供了可操作的模型选择见解，并验证了上下文语言模型在支持早期双相情感障碍筛查方面的潜力。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [6] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLMs）在不同高风险应用中对用户身份标记的敏感性，发现种族、性别和年龄会影响LLMs的响应，可能导致医疗护理差异、工资差距和政治事实现实的不同。作者建议在部署LLMs前进行类似评估。


<details>
  <summary>Details</summary>
Motivation: 了解LLMs如何在实际应用中使用身份信息进行决策，以及这种决策可能带来的负面影响。

Method: 我们对五个高风险的LLM应用进行了首次全面分析，这些应用涉及医学、法律、政治、政府福利和工作薪资领域。我们研究了用户写作中的身份标记如何影响LLMs的响应。

Result: LLMs对用户查询中的身份标记非常敏感，种族、性别和年龄在这些应用中持续影响LLMs的响应。例如，在提供医疗建议时，模型会对不同种族的人采用不同的护理标准；在回答事实性问题时，年长者的问题会更倾向于保守的政治观点，而年轻者的问题则更倾向于自由的政治观点；非白人求职者的薪资建议较低，而女性的薪资建议较高。

Conclusion: 使用现成的大型语言模型（LLMs）在这些应用中可能导致医疗护理中的有害差异，加剧工资差距，并为不同身份的人创造不同的政治事实现实。因此，建议在未来的部署之前对LLMs在面向用户的应用中的使用进行类似的全面评估。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [7] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: 本文提出了一种名为CCL-XCoT的两阶段微调框架，用于减少多语言大模型中的幻觉现象并提高跨语言事实知识的传递效果。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在低资源语言中容易产生幻觉，特别是在特定领域生成任务中。

Method: 提出了一种两阶段微调框架CCL-XCoT，包括基于课程的对比学习和跨语言思维链提示策略。

Result: 实验结果表明，CCL-XCoT可以将幻觉率降低多达62%，并且在不依赖外部检索或多模型集成的情况下显著提高跨语言事实知识的传递效果。

Conclusion: CCL-XCoT能够有效减少多语言大模型中的幻觉现象，并显著提升跨语言的事实知识传递效果。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [8] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 本文研究了模型和数据集之间的关系，构建了一个有向异构图来建模LLM供应链，并发现了多个重要结果。


<details>
  <summary>Details</summary>
Motivation: 由于许多LLM是从基础模型、预训练模型和外部数据集中构建的，它们可能会继承早期模型或数据集中的漏洞、偏见或恶意组件，因此需要理解这些组件的起源和发展。

Method: 我们设计了一种方法来系统地收集LLM供应链数据，并构建了一个有向异构图来建模模型和数据集之间的关系。

Result: 我们发现LLM供应链图是大型、稀疏的，并遵循幂律度分布；它具有密集连接的核心和碎片化的外围；数据集在训练中起着关键作用；模型和数据集之间存在强烈的相互依赖性；并且该图是动态的，每天更新以反映生态系统的持续演变。

Conclusion: 为了更好地检测潜在风险、提高模型公平性并确保合规性，理解这些组件的起源和发展至关重要。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [9] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix 是一个自动提示优化框架，能够将自然语言任务描述转化为高质量提示，无需手动调整或领域专业知识。它在多个任务类别中表现出色，比现有库更具竞争力，同时减少计算开销，提高效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在精心设计的提示下表现最佳，但提示工程仍然是手动、不一致且对非专家不友好。需要一种自动化的提示优化框架，以提高提示的质量和可用性。

Method: Promptomatix 是一个自动提示优化框架，它将自然语言任务描述转换为高质量提示，无需手动调整或领域专业知识。它支持基于元提示的优化器和DSPy驱动的编译器，并具有模块化设计以支持未来扩展。系统分析用户意图、生成合成训练数据、选择提示策略并使用成本意识目标优化提示。

Result: Promptomatix 在5个任务类别中进行了评估，结果表明其性能与现有库相比具有竞争力或更优，同时减少了提示长度和计算开销，使提示优化更加高效和可扩展。

Conclusion: Promptomatix 在多个任务类别中表现出色，相比现有库具有竞争力或优越性，同时减少了提示长度和计算开销，使提示优化更具可扩展性和效率。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [10] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: 本文介绍了ChartScope，一种优化用于跨多种图表类型的深入图表理解的大型视觉语言模型。通过高效的数据生成管道和双路径训练策略，ChartScope提升了对图表的理解能力，并引入了ChartDQA基准来评估问答和数据理解。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在配对数据仅限于少数图表类型以及缺乏针对图表数据对齐的针对性预训练方面存在局限性，这阻碍了模型对底层数据的理解。

Method: 我们提出了一个高效的数据生成管道，为多种图表类型合成配对数据，并提出了一种新的双路径训练策略，使模型能够简洁地捕捉关键数据细节，同时通过对底层数据进行推理来保持强大的推理能力。

Result: ChartScope在各种图表类型上的图表理解能力得到了显著提升。

Conclusion: 实验结果表明，ChartScope在广泛范围的图表类型上显著增强了理解能力。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [11] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: This paper explores the use of LLM-based selective translation to improve multilingual alignment in large language models, focusing on the low-resource Indic language Hindi.


<details>
  <summary>Details</summary>
Motivation: Multilingual large language models (LLMs) often demonstrate a performance gap between English and non-English languages, particularly in low-resource settings. Aligning these models to low-resource languages is essential yet challenging due to limited high-quality data.

Method: We investigate LLM-based selective translation, which selectively translates only the translatable parts of a text while preserving non-translatable content and sentence structure. We conduct a systematic study to explore key questions around this approach.

Result: Our experiments focus on the low-resource Indic language Hindi and compare translations generated by Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the promise of selective translation as a practical and effective method for improving multilingual alignment in LLMs.

Conclusion: Selective translation shows promise as a practical and effective method for improving multilingual alignment in LLMs.

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [12] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: This study investigates how LLMs process the temporal meaning of linguistic aspect in narratives and finds that they differ from humans in their processing of aspect and lack robust narrative understanding.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs process the temporal meaning of linguistic aspect in narratives and assess their ability to comprehend narratives.

Method: Using an Expert-in-the-Loop probing pipeline, we conduct a series of targeted experiments to assess whether LLMs construct semantic representations and pragmatic inferences in a human-like manner.

Result: LLMs over-rely on prototypicality, produce inconsistent aspectual judgments, and struggle with causal reasoning derived from aspect.

Conclusion: LLMs process aspect fundamentally differently from humans and lack robust narrative understanding.

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [13] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [14] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: 本文评估了基于大型语言模型的人格推理方法，发现虽然模型具有高信度，但效度有限，需要进一步改进以用于心理应用。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（如OpenAI的GPT-4和Meta的LLaMA）为从开放式的语言中进行可扩展的人格评估提供了有希望的方法，但推断人格特征仍然具有挑战性，早期工作通常依赖于合成数据或缺乏心理测量有效性的社交媒体文本。

Method: 我们引入了一个现实世界的基准，包含555个半结构化访谈，并使用零样本提示和思维链提示测试了三种最先进的大型语言模型（GPT-4.1 Mini、Meta-LLaMA和DeepSeek）进行BFI-10项目预测和大五种特质推断。

Result: 所有模型都表现出高测试-再测试信度，但构念效度有限：与真实分数的相关性较弱（最大皮尔逊r = 0.27），评分者间的一致性较低（Cohen's κ < 0.10），预测偏向于中等或高特质水平。思维链提示和更长的输入上下文适度改善了分布对齐，但没有提高特质水平的准确性。

Conclusion: 这些结果突显了当前基于大型语言模型的人格推理的局限性，并强调了为心理应用进行基于证据的开发的必要性。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [15] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 本文介绍了构建一个内部聊天机器人的经验，该聊天机器人使LinkedIn的团队能够从大型数据湖中自助获取数据洞察。方法包括构建知识图谱、Text-to-SQL代理和交互式聊天机器人。结果表明，该聊天机器人有300多名用户，53%的响应正确或接近正确。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL基准测试中带来了快速进展，但构建一个可行的企业解决方案仍然具有挑战性。本文旨在分享构建内部聊天机器人的见解，使LinkedIn的产品经理、工程师和运营团队能够从大型动态数据湖中自助获取数据洞察。

Method: 我们构建了一个知识图谱，通过索引数据库元数据、历史查询日志、维基和代码来捕捉最新的语义，并应用聚类来识别每个团队或产品区域的相关表。我们还构建了一个Text-to-SQL代理，能够检索和排序上下文、编写查询并自动纠正幻觉和语法错误。最后，我们构建了一个交互式聊天机器人，支持各种用户意图，并以丰富的UI元素显示响应，鼓励后续对话。

Result: 我们的聊天机器人拥有超过300名每周用户。专家评审显示，在内部基准集上，其53%的响应是正确的或接近正确的。

Conclusion: 通过消融研究，我们确定了最重要的知识图谱和建模组件，为开发企业级Text-to-SQL解决方案提供了实用路径。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [16] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文提出了一种误差感知的师生框架，通过从大型语言模型（GPT-4o）中获得结构化指导来改进生物医学文本中的关系分类（RC）。


<details>
  <summary>Details</summary>
Motivation: 在生物医学文本中进行关系分类（RC）对于构建知识图谱和实现药物再利用和临床决策等应用至关重要。

Method: 我们提出了一种误差感知的师生框架，通过从大型语言模型（GPT-4o）中获得结构化指导来改进关系分类（RC）。

Result: 我们的方法在5个PPI数据集中的4个和DDI数据集中达到了新的最先进性能，同时在ChemProt上保持竞争力。

Conclusion: 我们的方法在5个PPI数据集中的4个和DDI数据集中达到了新的最先进性能，同时在ChemProt上保持竞争力。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [17] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0 is a high-performance reasoning model specifically developed for the semiconductor display industry, demonstrating exceptional efficiency and performance gains through domain-specific training and techniques like RAG.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in the effectiveness of large language models (LLMs) in the semiconductor display industry due to a lack of domain-specific training and expertise.

Method: The model is designed to deliver expert-level understanding and reasoning for the industry's complex challenges. It leverages a carefully curated industry knowledge base, undergoes supervised fine-tuning and reinforcement learning, and integrates a domain-specific retrieval-augmented generation (RAG) mechanism.

Result: Despite its relatively compact size of 32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B across multiple evaluations.

Conclusion: X-Intelligence 3.0 demonstrates exceptional efficiency and establishes itself as a powerful solution to the longstanding reasoning challenges faced by the semiconductor display industry.

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [18] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: 提出XL-DURel模型，优化有序Word-in-Context分类任务，提升在二进制任务上的性能，实现WiC建模的统一方法。


<details>
  <summary>Details</summary>
Motivation: 为了实现对WiC建模的统一处理，我们试图通过优化模型来解决有序任务，从而提升在更具体的二进制任务上的性能。

Method: 我们提出了XL-DURel，一个经过微调的多语言Sentence Transformer模型，专门用于有序Word-in-Context分类。我们测试了多种损失函数，并基于复杂空间中的角度距离进行排名目标优化。

Result: XL-DURel模型在有序和二进制数据上表现优于之前模型，并证明二进制WiC可以作为有序WiC的一个特例。

Conclusion: XL-DURel模型在有序和二进制数据上表现优于之前模型，为WiC建模提供了一种统一的方法。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [19] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文探讨了AudiBERT模型在协作问题解决诊断中的表现，发现它在某些方面优于BERT模型，但也存在局限性。研究强调了人机互补性和模型可解释性的重要性，以提升诊断效果和人类参与度。


<details>
  <summary>Details</summary>
Motivation: 当前AI教育领域面临的一个重大挑战是利用机器学习技术从对话中检测协作问题解决（CPS）指标。虽然已有研究尝试使用BERT模型和多模态的AudiBERT模型来提高CPS诊断效果，但这些改进的统计显著性尚不明确，且缺乏关于如何利用人机互补性的指导。

Method: 本文扩展了之前的研究，重点分析了AudiBERT模型在检测协作问题解决指标中的表现，并比较了其与BERT模型的差异。此外，还进行了相关性分析，探讨了训练数据量与召回率、BERT模型精度与人工编码者间一致性之间的关系。

Result: AudiBERT模型不仅提高了数据集中稀疏类别的分类效果，还在社会认知维度的分类上表现出统计学上的显著改进。然而，在情感维度的分类中并未观察到类似的效果。同时，较大的训练数据量与较高的召回率显著相关，而BERT模型的精度与人工编码者之间的一致性显著相关。

Conclusion: 本文通过提出一种结构化的方法，旨在实现人工智能与人类在协作问题解决诊断中的互补性，并强调了模型可解释性在支持人类自主性和参与反思编码过程中的重要性。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [20] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本文研究了基于BERT的CPS分类模型的可解释性，使用SHAP分析分词对分类的影响，发现一些分词对分类有重要影响，但其中有些词语在语义上并不相关，这表明模型透明度可能帮助用户不过度依赖AI诊断，而是结合自身专业知识。


<details>
  <summary>Details</summary>
Motivation: 增强基于BERT的CPS诊断的可解释性对于更好地向教师等终端用户传达信息至关重要，这有助于建立信任并促进教育领域的广泛应用。

Method: 本文使用SHapley Additive exPlanations (SHAP)来分析不同分词在转录数据中对BERT模型分类CPS过程的贡献。

Result: 研究结果表明，表现良好的分类并不一定意味着合理的分类决策解释。某些分词频繁地影响分类，并且发现了一个无意义的词语，它对分类有积极贡献但语义上不相关。

Conclusion: 本文结论指出，模型在分类时适当使用标记的程度与涉及的类别数量有关，并呼吁研究集成模型架构和人机互补在CPS诊断中的应用，因为仍然需要大量的人类推理来进行CPS子技能的精细区分。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [21] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文研究了NLP中数据增强方法，发现backtranslation和paraphrasing可以达到与生成方法相当甚至更好的效果。


<details>
  <summary>Details</summary>
Motivation: 许多特定领域的机器学习任务面临数据稀缺和类别不平衡的问题，本文系统地探讨了NLP中的数据增强方法，特别是通过大型语言模型如GPT。

Method: 选择了解决数据稀缺问题并利用ChatGPT的方法以及一个示例数据集，并进行了比较四种不同数据增强方法的实验。

Result: 实验结果表明，backtranslation和paraphrasing在生成数据质量和分类性能方面表现良好。

Conclusion: backtranslation和paraphrasing可以产生与零样本或少样本生成相当甚至更好的结果。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [22] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 本研究提出了一个针对肯尼亚临床护理的基准数据集和评估框架，使用检索增强生成（RAG）方法，结合肯尼亚国家指南生成临床场景和问题。数据集由肯尼亚医生共同创建，并通过盲审专家评审确保质量。研究发现LLMs在非洲医学内容上的准确性较低，并引入了新的评估指标来测试临床推理、安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在改善低资源环境中的医疗保健获取方面具有潜力，但它们在非洲初级护理中的有效性仍需探索。本研究旨在创建一个基准数据集和评估框架，以评估LLMs在肯尼亚临床护理中的表现。

Method: 本研究提出了一种方法，用于创建针对肯尼亚二级和三级临床护理的基准数据集和评估框架。该方法使用检索增强生成（RAG）将临床问题与肯尼亚国家指南相结合，确保符合当地标准。这些指南被数字化、分块和索引以进行语义检索。然后用Gemini Flash 2.0 Lite提示生成现实的临床场景、选择题和基于指南的解答。肯尼亚医生共同创建和优化了数据集，并通过盲审专家评审过程确保临床准确性、清晰度和文化适当性。

Result: 初步结果表明，当将LLMs应用于本地化场景时存在显著的性能差距，这与LLM在非洲医学内容上的准确性低于美国基准的发现一致。本研究还引入了评估指标，以测试临床推理、安全性和适应性，如罕见病例检测（Needle in the Haystack）、逐步逻辑（Decision Points）和上下文适应性。

Conclusion: 本研究提供了一个可复制的模型，用于指导基于指南的动态基准测试，以支持非洲卫生系统中安全的人工智能部署。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [23] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: 本研究发现，语言模型中的某些概念关系（如形态学）可以通过潜在空间中的跨层线性变换进行解释。


<details>
  <summary>Details</summary>
Motivation: 我们想要验证线性变换是否能够准确地再现最终的对象状态，并探索语言模型中某些概念关系的可解释性。

Method: 我们使用了Bigger Analogy Test Set来验证线性变换Ws的效果，其中s是主体标记的中间层表示，W是从模型导数中得出的。

Result: 该线性技术在形态学关系上达到了90%的忠实度，并且在多语言和不同模型中得到了类似的结果。

Conclusion: 我们的研究结果表明，语言模型中的一些概念关系，如形态学，可以从潜在空间中解释，并且由跨层线性变换稀疏编码。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [24] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 本文提出了一种名为 Cleanse 的不确定性估计方法，用于检测大语言模型中的幻觉问题，并在多个模型和基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种 NLP 任务中表现出色，但其生成不准确响应的问题（即幻觉）仍然是一个关键问题，这影响了构建安全可靠的大语言模型。因此，需要一种有效的方法来检测和量化这种幻觉。

Method: Cleanse 通过聚类方法，利用 LLM 隐藏嵌入中包含的语义信息，计算簇内一致性与总一致性之间的比例来量化不确定性。

Result: Cleanse 在四个现成的模型（LLaMA-7B、LLaMA-13B、LLaMA2-7B 和 Mistral-7B）以及两个问答基准（SQuAD 和 CoQA）上验证了其有效性。

Conclusion: Cleanse 是一种有效的不确定性估计方法，能够有效地检测大语言模型中的幻觉问题。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [25] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: 本文介绍了Mangosteen，这是一个通过适应泰语的Dolma管道构建的470亿个标记的泰语语料库，包含自定义的基于规则的语言ID、修订的C4/Gopher质量过滤器和泰语训练的内容过滤器，以及精心挑选的非网络来源。该语料库在泰语基准测试中表现出色，并且发布了完整的管道代码、清洗清单、语料库快照和所有检查点，为未来的泰语和地区大型语言模型研究提供了完全可重复的基础。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语料库依赖于以英语为中心或语言无关的流程，这些流程的启发式方法无法捕捉泰语脚本或文化细微差别，导致赌博内容等风险材料未被处理。之前的泰语特定努力定制了流程或建立了新的流程，但很少发布他们的数据或记录设计选择，阻碍了可重复性，并引发了如何构建透明、高质量的泰语语料库的问题。

Method: 我们引入了Mangosteen：一个通过适应泰语的Dolma管道构建的470亿个标记的泰语语料库，包括自定义的基于规则的语言ID、修订的C4/Gopher质量过滤器和泰语训练的内容过滤器，以及精心挑选的非网络来源，如维基百科、皇家公报文本、OCR提取的书籍和CC授权的YouTube字幕。

Result: 系统性消融实验使用GPT-2显示，该流程将CommonCrawl从202M减少到25M文档，同时将SEA-HELM NLG从3提高到11；一个8B参数的SEA-LION模型在Mangosteen上持续预训练后，在泰语基准测试中比SEA-LION-v3和Llama-3.1高出约四个点。

Conclusion: 我们发布了完整的管道代码、清洗清单、语料库快照和所有检查点，为未来的泰语和地区大型语言模型研究提供了完全可重复的基础。

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [26] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLM）在使用领域特定搜索引擎输出的情况下分配ICPC-2代码的潜力。结果显示，许多LLM在F1分数上表现良好，但小模型在格式和输入长度方面存在困难。


<details>
  <summary>Details</summary>
Motivation: Medical coding structures healthcare data for research, quality monitoring, and policy. This study assesses the potential of large language models (LLMs) to assign ICPC-2 codes using the output of a domain-specific search engine.

Method: A dataset of 437 Brazilian Portuguese clinical expressions, each annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's text-embedding-3-large) retrieved candidates from 73,563 labeled concepts. Thirty-three LLMs were prompted with each query and retrieved results to select the best-matching ICPC-2 code. Performance was evaluated using F1-score, along with token usage, cost, response time, and format adherence.

Result: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever optimization can improve performance by up to 4 points. Most models returned valid codes in the expected format, with reduced hallucinations. Smaller models (<3B) struggled with formatting and input length.

Conclusion: LLMs show strong potential for automating ICPC-2 coding, even without fine-tuning. This work offers a benchmark and highlights challenges, but findings are limited by dataset scope and setup. Broader, multilingual, end-to-end evaluations are needed for clinical validation.

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [27] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: 本文介绍了MiroMind-M1系列，这是一个完全开源的推理语言模型，其性能可以与现有的开源RLM相媲美或超越。为了提高透明度和可重复性，作者释放了完整的堆栈，包括模型、数据集和所有训练及评估配置。


<details>
  <summary>Details</summary>
Motivation: 尽管闭源的RLM如GPT-o3展示了出色的推理能力，但它们的专有性质限制了透明度和可重复性。虽然许多开源项目旨在缩小这一差距，但大多数项目缺乏足够的开放性，因为它们省略了关键资源，如数据集和详细的训练配置，这阻碍了可重复性。

Method: 本文提出了一个名为Context-Aware Multi-Stage Policy Optimization的算法，该算法结合了长度渐进式训练和自适应重复惩罚，以鼓励上下文感知的强化学习训练。此外，模型在两个阶段进行训练：首先是在一个精心挑选的719K数学推理问题语料库上进行SFT，然后是在62K具有挑战性和可验证的问题上进行RLVR。

Result: MiroMind-M1系列在AIME24、AIME25和MATH基准测试中表现出色，与基于Qwen-2.5的开源7B和32B模型相比，具有最先进的性能和优越的token效率。

Conclusion: 本文介绍了MiroMind-M1系列，这是一个完全开源的推理语言模型，其性能可以与现有的开源RLM相媲美或超越。为了提高透明度和可重复性，作者释放了完整的堆栈，包括模型、数据集和所有训练及评估配置。

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [28] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本文对Hugging Face Hub上的阿拉伯语后训练数据集进行了综述，分析了其在任务多样性、文档质量、社区采用等方面的问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 后训练是将预训练大型语言模型与人类指令对齐的关键技术，显著提高了它们在各种任务中的性能。然而，阿拉伯语后训练数据集的发展存在一些关键差距，如任务多样性有限、文档和注释不一致或缺失以及社区采用率低。本文旨在分析这些差距并提出改进建议。

Method: 本文对Hugging Face Hub上公开的阿拉伯语后训练数据集进行了综述，从四个关键维度进行组织：(1) 大型语言模型能力；(2) 可操控性；(3) 对齐；(4) 鲁棒性。每个数据集都根据流行度、实际采用情况、新颖性和维护情况、文档和注释质量、许可透明度以及科学贡献进行了严格评估。

Result: 本文发现阿拉伯语后训练数据集存在关键差距，包括任务多样性有限、文档和注释不一致或缺失以及社区采用率低。

Conclusion: 本文讨论了这些差距对阿拉伯语大型语言模型和应用进展的影响，并为未来后训练数据集的开发提供了具体的建议。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [29] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 本研究构建了一个新的土耳其自杀意念语料库，并引入了一个资源高效的注释框架。通过使用八种预训练的情感和情绪分类器进行双向评估，发现现有流行模型在零样本迁移学习中的表现存在问题，强调了在心理健康NLP中需要更加严谨、语言包容的注释和评估方法。


<details>
  <summary>Details</summary>
Motivation: 目前可用的数据集大多为英语，但即使在这些数据集中，高质量的人工标注数据仍然稀缺。许多研究依赖于现有的预标记数据集，而没有检查其注释过程或标签可靠性。其他语言的数据集缺乏进一步限制了通过人工智能（AI）实现全球自杀预防的可能性。

Method: 我们构建了一个从社交媒体帖子中得出的新型土耳其自杀意念语料库，并引入了一个涉及三个人类注释者和两个大型语言模型（LLMs）的资源高效的注释框架。然后，我们通过八种预训练的情感和情绪分类器进行双向评估，以检查标签一致性和模型性能。

Result: 我们的研究发现，现有流行模型在零样本迁移学习中的表现存在问题。同时，我们强调了在心理健康NLP中需要更加严谨、语言包容的注释和评估方法。

Conclusion: 我们的研究强调了在心理健康自然语言处理（NLP）中需要更加严谨、语言包容的注释和评估方法，同时展示了流行模型在零样本迁移学习中的可疑表现。我们倡导在心理健康NLP中对模型训练和数据集构建保持透明度，优先考虑数据和模型的可靠性。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [30] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 该研究通过分析大量同行评审数据，揭示了语言在评审过程中可能加剧不平等的现象，并挑战了匿名性对公平性的传统看法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨语言如何在同行评审中强化不平等现象，尤其是在结构不平等问题被广泛讨论的情况下，关注语言的细微影响。

Method: 使用自然语言处理和大规模统计建模，分析了超过80,000份同行评审意见，探讨了评审语气、情感和支持性语言在作者人口统计学特征上的差异。

Result: 研究发现评审语气、情感和支持性语言在作者性别、种族和机构隶属关系上存在差异，并揭示了评审者身份披露如何影响评估语言。

Conclusion: 研究揭示了同行评审中的隐性偏见，并挑战了匿名性在公平性中的作用的常规假设。这些见解引发了关于评审政策如何塑造职业轨迹和科学进展的重要问题。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [31] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: 研究探讨了多模态神经网络在基于儿童有限输入的语言学习中的表现，发现它们能够成功地学习和泛化单词-参照映射，但不同孩子之间的学习方式存在差异。


<details>
  <summary>Details</summary>
Motivation: 为了弥合大型语言和多模态模型与儿童在语言习得上的差距，研究人员开始使用类似于儿童输入的数据量和质量来训练神经网络。然而，这种做法的成功是否反映了单个孩子的独特经验，还是在多个孩子的经验中表现出一致和稳健的学习模式仍未被探讨。

Method: 研究人员使用自动语音转录方法处理了SAYCam数据集的全部内容，该数据集包含三个孩子超过500小时的视频数据。利用这些自动转录文本，他们生成了多模态视觉和语言数据集，并探索了一系列神经网络配置以检查模拟单词学习的稳健性。

Result: 研究表明，基于每个孩子的自动转录数据训练的网络可以跨多种网络架构获取和泛化单词-参照映射。

Conclusion: 研究结果验证了多模态神经网络在基于现实语言学习中的稳健性，同时突显了当模型在每个孩子的发育经验上训练时，学习方式中出现的个体差异。

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [32] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE is a new generative framework for multi-behavior sequential recommendation that improves performance and reduces computational costs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing generative models in multi-behavior recommendation systems, such as the lack of explicit information for token reasoning, high computational costs, and limited multi-scale modeling over user history.

Method: GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method and a Journey-Aware Sparse Attention (JSA) mechanism to address the limitations of existing generative models in multi-behavior recommendation systems.

Result: Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences.

Conclusion: GRACE is a novel generative framework for multi-behavior sequential recommendation that significantly outperforms state-of-the-art baselines and reduces attention computation.

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [33] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: 本文提出FastLongSpeech框架，以提高大型语音语言模型处理长语音的能力，无需专门的长语音训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有LSLM主要关注语音生成或各种短语音任务，而长语音处理仍是一个关键但研究不足的挑战，主要是由于缺乏长语音训练数据和长序列的高计算成本。

Method: FastLongSpeech引入了一种迭代融合策略，将过长的语音序列压缩到可管理的长度，并采用动态压缩训练方法，使模型能够在不同压缩比例下学习短语音序列，从而将LSLM的能力转移到长语音任务上。

Result: 实验表明，该方法在长语音和短语音任务中均表现出色，并显著提高了推理效率。

Conclusion: FastLongSpeech能够有效提升长语音处理的效率，并在长语音和短语音任务中表现出色。

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [34] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: 本文提出了一种基于意图的图表生成方法，能够在零样本设置下从文档中生成符合用户意图的图表。该方法通过一个无监督的两阶段框架实现，包括信息提取和图表类型选择。实验结果表明，该方法在图表数据准确性和图表类型方面优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常需要用户手动选择相关内容，而本文旨在解决从长文档中根据用户意图生成图表的问题，这在实际应用中更为常见。

Method: 本文提出了一种无监督的两阶段框架，其中大型语言模型首先通过分解意图并迭代验证和精炼数据来从文档中提取相关信息。然后，一个基于启发式的模块选择适当的图表类型并生成最终代码。

Result: 本文提出了一个基于属性的度量标准，用于评估生成图表的数据准确性。此外，本文还构建了一个包含1,242个<意图、文档、图表>元组的数据集，并与现有基线方法进行了比较，结果显示本文方法在图表数据准确性和图表类型方面表现更好。

Conclusion: 本文提出了一种基于意图的图表生成方法，该方法在零样本设置下能够从文档中生成符合用户意图的图表。实验结果表明，该方法在图表数据准确性和图表类型方面优于现有基线方法。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [35] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: 本研究探讨了大规模推理蒸馏对长上下文理解的影响，发现蒸馏可以提升模型的长上下文意识，并缓解“中间迷失”问题。


<details>
  <summary>Details</summary>
Motivation: 我们受到需要了解扩展的长CoT过程如何影响长上下文理解的动机驱动，特别是在检索增强生成（RAG）系统中，高效获取和利用上下文信息对于生成可靠响应至关重要。

Method: 我们使用一系列从Deepseek-R1开源模型中提炼出的模型进行了全面的研究，这些模型以其卓越的推理能力而闻名。我们的研究重点是评估这些模型在多文档问答任务中提取和整合相关信息的表现。

Result: 我们通过严格的实验表明，蒸馏的推理模式显著提高了长上下文的理解能力。

Conclusion: 我们的分析表明，知识蒸馏通过在上下文分析和信息解析过程中促进更详细和明确的推理过程，增强了长上下文意识。这一进步有效缓解了长期上下文模型所面临的“中间迷失”问题。

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [36] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 本研究探讨了小型语言模型（TLMs）是否表现出与大型语言模型（LLMs）相同的定性特征。结果显示，TLMs在预训练后能够表现出与LLMs相似的关键质量特征，且通过使用多个独立预训练的浅层架构的软委员会，可以实现低延迟的TLM而不影响分类准确性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的预训练需要大量的计算资源，只有少数主要公司才能进行，这限制了更广泛的研究参与。因此，需要寻找更易访问的替代方案。

Method: 研究通过在维基百科数据集的子集上对BERT-6和BERT-1变体进行预训练，并在FewRel、AGNews和DBPedia分类任务上评估其性能，来探索TLMs是否表现出与LLMs相同的定性特征。

Result: 研究发现，TLMs在预训练和非预训练模型之间存在明显的性能差距，这表明即使在微型规模下，预训练也是有效的。性能差距随着预训练数据集的大小和预训练数据集与分类数据集之间的令牌重叠程度而增加。此外，通过使用多个独立预训练的浅层架构的软委员会，可以复制预训练深度TLM架构的分类准确率，从而实现低延迟的TLM而不影响分类准确性。

Conclusion: 研究结果表明，小型语言模型（TLMs）在预训练后能够表现出与大型语言模型（LLMs）相似的关键质量特征，这为更广泛的研究参与提供了可能性。此外，通过使用多个独立预训练的浅层架构的软委员会，可以实现低延迟的TLM而不影响分类准确性。

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [37] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为MEKiT的新方法，通过整合异构的内部情感知识和外部因果知识，以提高大型语言模型在Emotion-Cause Pair Extraction任务上的表现。实验结果表明，MEKiT在该任务上表现出更高的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在文本理解和生成方面表现出色，但它们在需要推理能力的Emotion-Cause Pair Extraction（ECPE）任务上的表现往往不如较小的语言模型。主要原因是缺乏辅助知识，这限制了LLMs感知情绪和推理原因的能力。

Method: 我们提出了一个名为MEKiT的新方法，该方法结合了异构的内部情感知识和外部因果知识。对于这两种不同方面和结构的知识，我们应用了包含指令模板和混合数据的方法进行指令调优，分别有助于LLMs更全面地识别情绪和准确地推理原因。

Result: 实验结果表明，MEKiT为ECPE任务提供了一个更有效和适应性的解决方案，在与比较基线的对比中表现出绝对的性能优势，并显著提高了LLMs在ECPE任务上的性能。

Conclusion: 实验结果表明，MEKiT为ECPE任务提供了一个更有效和适应性的解决方案，在与比较基线的对比中表现出绝对的性能优势，并显著提高了LLMs在ECPE任务上的性能。

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [38] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: This paper proposes SASFT, a method that reduces unexpected code-switching in large language models while preserving their multilingual capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing work on code-switching lacks a mechanistic analysis and shows limited effectiveness. The problem of code-switching leads to poor readability and degrades the usability of model responses.

Method: SASFT, which uses sparse autoencoders to guide supervised fine-tuning, teaches LLMs to maintain appropriate pre-activation values of specific language features during training.

Result: Experiments on five models across three languages show that SASFT reduces unexpected code-switching by more than 50% compared to standard supervised fine-tuning, with complete elimination in four cases. It also maintains or improves performance on six multilingual benchmarks.

Conclusion: SASFT is effective in reducing unexpected code-switching while maintaining or improving multilingual capabilities.

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [39] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: A new method called NeuronXA is introduced to evaluate cross-lingual alignment in LLMs. It shows high effectiveness even with limited data, offering a promising approach for improving multilingual LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing alignment benchmarks primarily focus on sentence embeddings, but neural models tend to induce a non-smooth representation space, which impacts semantic alignment evaluation on low-resource languages. The need for a more semantically grounded approach to assess cross-lingual alignment is highlighted.

Method: Proposed a novel Neuron State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual alignment capabilities of LLMs, inspired by neuroscientific findings that similar information activates overlapping neuronal regions.

Result: NeuronXA achieves a Pearson correlation of 0.9556 with downstream tasks performance and 0.8514 with transferability using only 100 parallel sentence pairs. It demonstrates effectiveness in assessing both cross-lingual alignment and transferability.

Conclusion: NeuronXA's effectiveness in assessing both cross-lingual alignment and transferability, even with a small dataset, highlights its potential to advance cross-lingual alignment research and improve the semantic understanding of multilingual LLMs.

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [40] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite is a framework for automatically generating prompt variations to improve the reliability of LLM evaluations.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLMs with a single prompt has proven unreliable, and generating prompt variations for a more robust multi-prompt evaluation is challenging.

Method: PromptSuite is a framework that enables the automatic generation of various prompts, following a modular prompt design and allowing controlled perturbations to each component.

Result: PromptSuite is flexible, works out of the box on a wide range of tasks and benchmarks, and is extensible, supporting the addition of new components and perturbation types.

Conclusion: PromptSuite provides meaningful variations to support strong evaluation practices and is available through both a Python API and a user-friendly web interface.

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [41] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA is a new dataset of 30,000 backstories from real social media users, offering improved narrative consistency and enabling new research directions in computational social science and persona-driven language modeling.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to persona-driven LLMs either rely on costly human-curated data or produce synthetic personas that lack consistency and realism. There is a need for a dataset that bridges this spectrum by grounding synthetic generation in authentic user activity.

Method: SYNTHIA is a dataset of 30,000 backstories derived from 10,000 real social media users from BlueSky across three time windows, grounded in authentic user activity.

Result: SYNTHIA achieves competitive performance with state-of-the-art methods in demographic diversity and social survey alignment while significantly outperforming them in narrative consistency. It also incorporates temporal dimensionality and provides rich social interaction metadata.

Conclusion: SYNTHIA provides a new dataset that bridges the gap between human-curated data and synthetic personas, offering competitive performance in demographic diversity and social survey alignment, while significantly outperforming existing methods in narrative consistency. It also introduces temporal dimensionality and rich social interaction metadata, enabling new research directions.

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [42] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: This paper proposes MUR, a method for efficiently and adaptively guiding LLM test-time scaling without additional training. It uses momentum uncertainty to dynamically allocate thinking budgets and introduces gamma-control for flexible inference-time control.


<details>
  <summary>Details</summary>
Motivation: Optimizing the reasoning efficiency of Large Language Models (LLMs) remains an open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it often leads to overthinking, wasting tokens on redundant computations.

Method: Inspired by the concept of momentum in physics, we propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically allocates thinking budgets to critical reasoning steps by tracking and aggregating stepwise uncertainty over time. We introduce gamma-control, a simple mechanism that tunes the reasoning budget via a single hyperparameter.

Result: MUR is comprehensively evaluated against various TTS methods across four challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using different sizes of recent Qwen3 models (1.7B, 4B, and 8B).

Conclusion: MUR reduces computation by over 50% on average while improving accuracy by 0.62-3.37%.

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [43] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: This paper introduces RefCritic, a critic module that enhances model critique abilities through reinforcement learning with dual rule-based rewards, showing consistent advantages across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning for building critic modules fails to genuinely enhance models' critique abilities, producing superficial critiques with insufficient reflections and verifications.

Method: RefCritic is a long-chain-of-thought critic module based on reinforcement learning with dual rule-based rewards: (1) instance-level correctness of solution judgments and (2) refinement accuracies of the policy model based on critiques.

Result: RefCritic shows significant gains on AIME25 for the respective base models and superior scaling with increased voting numbers under majority voting.

Conclusion: RefCritic demonstrates consistent advantages across all benchmarks and outperforms step-level supervised approaches on ProcessBench.

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [44] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebShaper is a framework for synthesizing IS data by formalizing tasks through set theory and using Knowledge Projections to control reasoning structure, resulting in improved performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality training data has limited the development of IS agents. Existing approaches may lead to inconsistency between information structure and reasoning structure.

Method: WebShaper systematically formalizes IS tasks through set theory, utilizing Knowledge Projections (KP) to control reasoning structure. It creates seed tasks and expands them using a multi-step process with an agentic Expander.

Result: WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.

Conclusion: WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [45] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: 本研究比较了DNA序列的分词方法和位置编码技术，发现BPE和RoPE在不同任务中表现优异，并提供了关于Transformer模型深度的实用建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究将DNA序列视为一种特殊语言，并利用Transformer对其进行建模，但缺乏对不同分词方法的系统评估。

Method: 比较k-mer分割与BPE子词分词，以及三种位置编码方法（正弦、AliBi和RoPE），并在不同层数的Transformer编码器中进行训练和评估。

Result: BPE在任务中表现出更高的稳定性能，RoPE在捕捉周期性基序和长序列外推方面表现优异，AliBi在依赖局部的任务中表现良好。增加层数从3到12层显著提升了性能，而24层仅带来微小改进或轻微过拟合。

Conclusion: 本研究为设计DNA Transformer模型的分词和位置编码提供了实用指导。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [46] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: 本文提出了一种新的多样性度量方法 PATTR，该方法能够有效减轻文本长度变化带来的偏差，并在词汇多样性评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究者依赖于提示工程来提高合成数据的多样性，但提示变化对响应文本长度的影响以及由此产生的词汇多样性测量的后果尚未得到充分探讨。因此，需要一种更鲁棒的多样性度量方法。

Method: 本文提出了 Penalty-Adjusted Type-Token Ratio (PATTR)，这是一种针对文本长度变化的多样性度量方法。我们生成了一个包含超过2000万词的合成语料库，并使用 PATTR 与其他现有度量方法（如 Moving-Average TTR 和 Compression Ratio）进行比较。

Result: 分析表明，文本长度的变化会引入有利于较短响应的偏差。PATTR 能够显式考虑任务特定的目标响应长度（L_T），从而有效减轻长度偏差。此外，PATTR 在过滤最富有词汇多样性的响应方面表现优于 MATTR 和 CR。

Conclusion: PATTR 是一种对文本长度变化具有鲁棒性的多样性度量标准，它在过滤最富有词汇多样性的响应方面表现出色，并且能够有效减少长度偏差。

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [47] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在自然语言推理中作为常识知识生成器的潜力，发现虽然显式结合常识知识并不总是能提高整体结果，但它有助于区分支持的实例，并适度改善区分矛盾和中性推理。


<details>
  <summary>Details</summary>
Motivation: 现有的常识资源不足以覆盖各种前提-假设对，因此需要探索大型语言模型作为常识知识生成器的潜力。

Method: 本研究探索了大型语言模型作为自然语言推理中的常识知识生成器的潜力，从两个关键维度评估其可靠性以及该知识对预测准确性的影响力。

Result: 显式结合常识知识有助于区分支持的实例，并适度改善区分矛盾和中性推理。

Conclusion: 虽然显式地结合常识知识并不总是能提高整体结果，但它有助于区分支持的实例，并适度改善区分矛盾和中性推理。

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [48] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本文认为NLI中的标注不一致并非仅仅是噪声，而是反映了人类解释的多样性，提出了一个统一的框架来处理歧义，并呼吁建立新的标注资源以改进NLI系统。


<details>
  <summary>Details</summary>
Motivation: 当前NLI中的标注不一致往往反映了有意义的解释变化，尤其是在前提或假设中存在歧义时。

Method: 本文提出了一种统一的框架，用于整合现有的分类体系，并通过具体例子展示关键的歧义子类型。

Result: 通过识别模糊输入对并分类歧义类型，可以揭示歧义如何影响标注者的决策，并推动针对性检测方法的发展。

Conclusion: 本文主张将歧义纳入自然语言推理（NLI）的考量，以提高模型与人类解释的一致性。

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [49] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: 本文研究了同音词归一化对使用Ge'ez脚本的语言的影响，并提出了一种后推理归一化方法，以提高BLEU分数并保留语言特征。


<details>
  <summary>Details</summary>
Motivation: 同音词归一化可能提高自动度量报告的性能，但也会导致模型无法理解单一语言的不同书写形式。此外，在迁移学习中，训练数据归一化的模型可能无法很好地泛化到其他语言。

Method: 我们进行了单语训练和跨语言迁移实验，以了解归一化对使用Ge'ez脚本的语言的影响，并提出了一种后推理干预，其中在模型预测上应用归一化而不是训练数据。

Result: 通过我们简单的后推理归一化方案，我们展示了BLEU分数可以提高最多1.03，同时保留了训练中的语言特征。

Conclusion: 我们的工作有助于关于技术促进语言变化的更广泛讨论，并呼吁更多语言意识干预。

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [50] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本研究评估了三种LLM在三个医学领域中的数据提取性能，并测试了四种提示策略。结果显示定制提示最有效，提高了召回率。研究提出了三层指南，以平衡LLM的效率与专家监督。


<details>
  <summary>Details</summary>
Motivation: 自动化从全文本随机对照试验（RCTs）中提取数据进行meta分析仍然是一个重大挑战。

Method: 本研究评估了三种LLM（Gemini-2.0-flash、Grok-3、GPT-4o-mini）在三个医学领域（高血压、糖尿病和骨科）中的统计结果、偏倚风险评估和研究级特征方面的实际性能。测试了四种不同的提示策略（基本提示、自我反思提示、模型集成和定制提示）以确定如何提高提取质量。

Result: 所有模型都表现出高精度，但始终因遗漏关键信息而表现出较差的召回率。发现定制提示是最有效的，可将召回率提高多达15%。

Conclusion: 本研究提出了一个三层指南，用于在数据提取中使用LLM，根据任务复杂性和风险将数据类型与适当的自动化级别相匹配。研究提供了在现实世界meta分析中自动化数据提取的实用建议，通过针对性的任务特定自动化平衡LLM的效率与专家监督。

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [51] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: 本文提出了一种基于多教师模型的蒸馏策略，以降低大规模语言模型的计算成本并提高推理速度，同时保持小参数规模。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决大规模语言模型部署中的高计算成本和推理缓慢问题。

Method: 本文提出了一种由多个教师模型引导的蒸馏策略，构建了多个教师模型并整合其输出概率分布和中间语义特征，以指导学生模型从多个知识源学习。此外，引入了加权输出融合机制、特征对齐损失函数和熵驱动的动态教师加权策略，以提高知识迁移的质量和稳定性。

Result: 实验结果表明，该方法在语言建模、文本生成和多任务学习等任务中表现出色，具有高度的一致性、泛化能力和任务适应性。同时，该方法在困惑度、蒸馏损失和生成质量方面优于现有的蒸馏方法。

Conclusion: 本文提供了一种高效压缩大规模语言模型的技术路径，并展示了多教师协作机制在复杂语言建模任务中的有效性。

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [52] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 本研究探讨了多任务、多语言和多源学习方法对预训练语言模型的鲁棒性和性能的影响。我们引入了SOI（兴趣子集）作为分类框架，用于识别六种不同的学习行为模式。实验结果表明，多源学习在分布外性能上持续提高了高达7%，而多任务学习在相似任务组合中表现出显著的增益。此外，我们还提出了一种两阶段微调方法，以进一步提升性能。这些发现为训练动态提供了新的见解，并提供了优化多设置语言模型性能的实用方法。


<details>
  <summary>Details</summary>
Motivation: 本研究探讨了多任务、多语言和多源学习方法对预训练语言模型的鲁棒性和性能的影响。

Method: 我们引入了SOI（兴趣子集），这是一种新型的分类框架，用于识别训练期间的六种不同的学习行为模式。我们通过SOI转换热图和数据集地图可视化分析了示例在单设置到多设置配置之间的变化。此外，我们还引入了一种两阶段微调方法，其中第二阶段利用基于SOI的子集选择来实现额外的性能提升。

Result: 多源学习在分布外性能上持续提高了高达7%，而多任务学习在相似任务组合中表现出显著的增益。

Conclusion: 这些发现为训练动态提供了新的见解，并提供了优化多设置语言模型性能的实用方法。

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [53] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: ChiMed 2.0 is a large-scale Chinese medical dataset that supports pre-training, SFT, and RLHF, showing performance gains in medical LLM training.


<details>
  <summary>Details</summary>
Motivation: Existing Chinese medical datasets are limited in size and domain coverage, and most are designed only for LLM fine-tuning without supporting pre-training and RLHF.

Method: Proposed a Chinese medical dataset named ChiMed 2.0, which extends previous work and covers data from Chinese medical online platforms and generated by LLMs. Conducted pre-training, SFT, and RLHF experiments on representative general domain LLMs.

Result: Performance gains across different model scales were observed, validating the dataset's effectiveness and applicability.

Conclusion: ChiMed 2.0 dataset is effective and applicable for training a Chinese medical LLM.

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [54] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 本文提出了一种名为Dual-Phase Self-Evolution (DPSE)的框架，旨在同时优化用户偏好适应和领域特定能力。DPSE通过提取多维交互信号并估计满意度分数，指导结构化数据扩展，并支持两阶段微调管道。实验结果表明，DPSE在多个任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练策略，如基于记忆的检索或偏好优化，虽然提高了用户对齐度，但未能增强模型的领域认知。为了弥合这一差距，我们提出了DPSE框架。

Method: 我们提出了一个名为Dual-Phase Self-Evolution (DPSE)的框架，该框架联合优化用户偏好适应和领域特定能力。DPSE引入了一个Censor模块来提取多维交互信号并估计满意度分数，这通过主题感知和偏好驱动的策略指导结构化数据扩展。这些扩展的数据集支持一个两阶段微调管道：监督领域定位和频率感知偏好优化。

Result: 在通用NLP基准和长期对话任务上的实验表明，DPSE始终优于监督微调、偏好优化和记忆增强基线。消融研究验证了每个模块的贡献。

Conclusion: 我们的框架提供了一条通往LLM持续自我进化的自主路径。

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [55] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 本文提出了一种新的AI文本检测器评估范式，强调现实世界和公平评估，通过引入可靠性与稳定性因素，解决了现有研究中的不足，并开发了一个后处理、模型无关的人类化框架。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要报告传统的指标如AUROC，忽略了即使轻微的误报率也会对检测系统的实际部署构成重大障碍。此外，实际部署需要预设阈值配置，使得检测器的稳定性（即在不同领域和对抗场景中保持一致性能）成为一个关键因素。这些方面在以往的研究和基准中被忽视。

Method: 本文提出了SHIELD基准，整合了可靠性与稳定性因素到统一的评估指标中，并开发了一个后处理、模型无关的人类化框架，以修改AI文本以更接近人类作者的风格。

Result: SHIELD基准通过整合可靠性与稳定性因素，提供了一个实用的评估指标。同时，开发的后处理、模型无关的人类化框架能够有效挑战当前最先进的零样本检测方法，在保持可靠性和稳定性方面表现出色。

Conclusion: 本文提出了一个新颖的AI文本检测器评估范式，强调现实世界和公平评估。通过引入可靠性与稳定性因素，SHIELD基准解决了现有研究中的不足，并开发了一个后处理、模型无关的人类化框架，以更接近人类写作方式修改AI文本。

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [56] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: 本文认为，AI对齐原则与左翼政治偏见密切相关，但当前研究却将这种左倾倾向视为问题，从而违背了AI对齐的目标。


<details>
  <summary>Details</summary>
Motivation: 研究AI对齐原则与政治偏见之间的关系，以揭示当前对LLM中左倾倾向的误解。

Method: 通过分析AI对齐原则与政治偏见的关系，探讨了左翼和右翼意识形态在AI开发中的不同影响。

Result: AI对齐原则内在地与进步道德框架和左翼原则一致，而右翼意识形态则与对齐指南相冲突。然而，研究将左倾倾向视为风险，这实际上违背了AI对齐的目标。

Conclusion: 智能系统在训练为无害和诚实的过程中必然表现出左翼政治偏见，而右翼意识形态往往与对齐指南相冲突。然而，关于LLM中政治偏见的研究却将左倾倾向视为一种风险，这实际上是在反对AI对齐，默许违反HHH原则。

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [57] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: 本文研究了MCQA作为评估大型语言模型性能的代理的有效性，并发现它不再适用于最先进的模型。


<details>
  <summary>Details</summary>
Motivation: 研究MCQA是否仍然是评估大型语言模型性能的有效方法。

Method: 对15个不同的问答基准和25个不同的LLMs进行了系统评估，考虑了五种不同的问题呈现方式。

Result: 当模型只能在提供选项之前进行链式思维推理时，MCQA仍然是下游性能的良好代理。然而，能够在给出选项后进行推理的大模型往往显著优于其自由文本性能。

Conclusion: MCQA不再是评估最先进的模型的下游性能的良好代理，并提供了设计更稳健、抗偏见的基准的实用指南。

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [58] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2是一种轻量级的多语言内容审核分类器，针对新加坡环境设计，支持英语、中文、马来语和部分泰米尔语。它在多个基准测试中表现优于其他系统，并已在新加坡政府内部部署。


<details>
  <summary>Details</summary>
Motivation: 现代内容审核系统越来越多地支持多种语言，但往往未能解决本地化和低资源变体的问题，这在现实部署中造成了安全漏洞。小型模型可以作为大型LLM的潜在替代方案，但仍需要大量的数据和计算。

Method: LionGuard 2是一种轻量级的多语言内容审核分类器，针对新加坡环境设计，支持英语、中文、马来语和部分泰米尔语。它基于预训练的OpenAI嵌入和多头有序分类器。

Result: LionGuard 2在17个基准测试中表现优于多个商业和开源系统，包括新加坡特定的和公共的英语数据集。该系统已在新加坡政府内部实际部署，展示了其在大规模应用中的实用性。

Conclusion: 我们的研究结果表明，高质量的本地数据和强大的多语言嵌入可以在不微调大型模型的情况下实现强大的内容审核性能。我们发布了模型权重和部分训练数据，以支持未来在LLM安全方面的工作。

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [59] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: 本文探讨了熵分析作为探测Transformer架构中信息分布的工具，通过量化标记级别的不确定性并检查不同处理阶段的熵模式，研究信息如何在模型中被管理和转换。作为案例研究，将该方法应用于基于GPT的大语言模型，展示了其揭示模型行为和内部表示的潜力。该方法可能为模型行为提供见解，并有助于开发用于基于Transformer的模型的可解释性和评估框架。


<details>
  <summary>Details</summary>
Motivation: 探索熵分析作为探测Transformer架构中信息分布的工具，以了解信息如何在模型中被管理和转换。

Method: 熵分析作为工具，用于探测Transformer架构中的信息分布。通过量化标记级别的不确定性并检查不同处理阶段的熵模式，研究信息如何在这些模型中被管理和转换。

Result: 将该方法应用于基于GPT的大语言模型，展示了其揭示模型行为和内部表示的潜力。

Conclusion: 该方法可能为模型行为提供见解，并有助于开发用于基于Transformer的模型的可解释性和评估框架。

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [60] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在隐喻解释中的能力，发现其表现主要受表面特征影响，而非隐喻内容，强调需要更现实的评估框架。


<details>
  <summary>Details</summary>
Motivation: 以往的研究局限于单一数据集评估和特定任务设置，通常使用通过词汇替换构造的数据。本文旨在解决这些限制，提供更全面的评估。

Method: 本文通过使用多样化的公开数据集进行广泛的实验，专注于自然语言推理（NLI）和问答（QA）任务，以评估大型语言模型（LLMs）在隐喻解释方面的能力。

Result: 结果表明，LLMs的表现更多受到词汇重叠和句子长度等特征的影响，而不是隐喻内容，这表明任何所谓的LLMs理解隐喻语言的能力都是表面特征、上下文学习和语言知识的综合结果。

Conclusion: 本文提供了关于大型语言模型在隐喻理解方面的当前能力和局限性的关键见解，并强调了在隐喻解释任务中需要更现实的评估框架。

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [61] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: Stitch是一种新的生成方法，能够在语音响应的同时生成非语音推理，提高SLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的SLMs缺乏在回应前进行内部非语音思考过程的能力，而人类通常会进行复杂的内部推理，因此需要将非语音思考过程集成到SLMs中。

Method: 提出了一种新的生成方法Stitch，交替生成非语音推理块和语音响应块。利用语音块的音频持续时间较长的特点，在剩余空闲时间内生成非语音推理令牌。

Result: Stitch在数学推理数据集上比无法生成非语音CoT的基线模型表现更好，同时在非推理数据集上表现与基线模型相当。

Conclusion: Stitch能够实现同时思考和说话，同时保持与无法生成非语音CoT的基线模型相同的延迟，并在数学推理数据集上表现优于基线模型15%。

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [62] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: This paper introduces AlgoSimBench to evaluate LLMs' ability to identify algorithmically similar problems. It proposes ASM to improve problem similarity detection and achieves significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether the abilities of LLMs to solve complex competitive programming problems generalize to less seen domains. It introduces AlgoSimBench to assess LLMs' ability to identify ASPs.

Method: The paper introduces AlgoSimBench, a benchmark for assessing LLMs' ability to identify ASPs. It also proposes ASM, a novel method for improving problem similarity detection, and evaluates code embedding models and retrieval methods.

Result: The best-performing model (o3-mini) achieved only 65.9% accuracy on the MCQ task. ASM improved accuracy by 6.7% to 11.7%. Combining ASM with BM25 achieved up to 52.2% accuracy.

Conclusion: LLMs struggle to identify algorithmically similar problems (ASPs), but the proposed method Attempted Solution Matching (ASM) improves their performance. Combining ASM with BM25 can achieve up to 52.2% accuracy.

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [63] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在驱动能够执行复杂动作的数字助手方面的潜力，并提出了ASPERA框架和Asper-Bench数据集，以展示基于自定义助手库的程序生成对LLMs来说是一个重大挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在驱动能够执行复杂动作的数字助手方面的潜力。

Method: 开发了ASPERA框架，包括助手库模拟和人类辅助LLM数据生成引擎，用于生成高质量任务。

Result: 通过ASPERA生成了250个具有挑战性的任务，并展示了基于自定义助手库的程序生成对LLMs来说是一个重大挑战。

Conclusion: 程序生成基于自定义助手库对LLMs来说是一个重大挑战，与无依赖的代码生成相比。

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [64] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文研究了无训练的测试时间缩放方法，提出了一种新的推理范式——混合测试时间缩放，通过结合细粒度的自我精炼和其他经典并行缩放方法，显著提升了大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 测试时间缩放（TTS）是一种有前途的方法，可以在推理过程中逐步激发模型的智能。然而，训练带来的额外计算开销增加了测试时间缩放的负担。因此，我们专注于无训练的TTS方法进行推理。

Method: 设计了条件步级自我精炼，并将其与其它经典的并行缩放方法相结合，引入了一种新的推理范式称为混合测试时间缩放。

Result: 在五个不同规模（3B-14B）和家族的指令调优的大语言模型上的广泛实验表明，混合策略在细粒度上结合各种无训练的TTS方法，具有扩展大语言模型推理性能边界的重要潜力。

Conclusion: 混合策略在细粒度上结合各种无训练的TTS方法，具有扩展大语言模型推理性能边界的重要潜力。

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [65] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: 本文进行了首次多语言文本净化系统评估的研究，提出了更可靠的多语言TST评估流程的实用方案。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）取得了进展，但文本生成任务（如文本风格转换TST）的评估仍然是一个重大挑战。此外，大多数先前的工作仅专注于英语，而多语言TST评估尚未得到充分探索。

Method: 受机器翻译的启发，我们评估了现代基于神经网络的评估模型以及基于提示的LLM-as-a-judge方法。

Result: 我们进行了首次针对九种语言（英语、西班牙语、德语、中文、阿拉伯语、印地语、乌克兰语、俄语、阿姆哈拉语）的文本净化系统评估的全面多语言研究。

Conclusion: 本文提供了设计更可靠的多语言文本净化评估流程的实用方案。

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [66] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: 本文提出了一种基于上下文学习的视觉语言模型方法，用于太赫兹成像的图像分类，无需微调，能够有效提高低数据情况下的分类性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 太赫兹成像在安全检查和材料分类等应用中具有非侵入性分析的优势，但由于标注有限、分辨率低和视觉模糊，有效的图像分类仍然具有挑战性。

Method: 引入了基于视觉语言模型（VLM）的上下文学习（ICL），无需微调即可进行图像分类。使用模态对齐提示框架，将两个开放权重的VLM适应到太赫兹领域，并在零样本和单样本设置下进行了评估。

Result: 结果表明，在数据量少的情况下，ICL提高了分类和可解释性。

Conclusion: 这是首次将增强的ICL VLM应用于太赫兹成像，为资源受限的科学领域提供了有希望的方向。

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [67] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [68] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: 本研究分析了冲突叙述在政治极化中的作用，并揭示了叙述对齐作为一种修辞策略的模式。


<details>
  <summary>Details</summary>
Motivation: 研究冲突叙述如何提供关于公共领域中极化和议题对齐的修辞机制的见解。

Method: 通过从对立意见群体的推文中提取冲突叙述的文本信号来分析极化的话语维度。

Result: 发现了沿着两个维度的冲突叙述：(i) 对同一组行动者赋予不同的行动角色，(ii) 为同一事件赋予不同的行动者。此外，提供了叙述对齐模式的初步证据。

Conclusion: 这些发现展示了叙述作为分析极化话语机制的透镜的用途。

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [69] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: 本文介绍了对MM-ArgFallacy2025共享任务的提交，旨在推进多模态论点挖掘的研究，重点是政治辩论中的逻辑谬误。我们的方法使用了预训练的基于Transformer的模型，并提出了几种利用上下文的方法。在谬误分类子任务中，我们的模型获得了0.4444（文本）、0.3559（音频）和0.4403（多模态）的宏F1分数。我们的多模态模型表现与文本模型相当，表明有改进的潜力。


<details>
  <summary>Details</summary>
Motivation: 推进多模态论点挖掘的研究，重点关注政治辩论中的逻辑谬误。

Method: 使用预训练的基于Transformer的模型，并提出几种利用上下文的方法。

Result: 在谬误分类子任务中，我们的模型获得了0.4444（文本）、0.3559（音频）和0.4403（多模态）的宏F1分数。

Conclusion: 我们的多模态模型表现与文本模型相当，表明有改进的潜力。

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [70] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出了一种名为P3的新框架，通过迭代过程同时优化系统和用户提示，并利用离线优化的提示来促进在线提示。实验表明，P3在多个任务中表现出色，证明了全面优化策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）应用通常使用多组件提示，包括系统提示和用户提示，以指导模型行为。虽然最近的进展已经证明了自动优化系统或用户提示的有效性，但这种单边方法由于这些组件之间的相互依赖性，往往产生次优结果。

Method: 我们引入了P3，这是一个新颖的自我改进框架，通过迭代过程同时优化系统和用户提示。离线优化的提示进一步用于促进在线提示，通过执行依赖于查询的提示优化。

Result: 在通用任务（例如Arena-hard和Alpaca-eval）和推理任务（例如GSM8K和GPQA）上的广泛实验表明，P3在自动提示优化领域实现了卓越的性能。

Conclusion: 我们的结果强调了全面优化策略在提高跨不同领域的大型语言模型性能方面的有效性。

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [71] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: This paper addresses the length bias in Process Reward Models (PRMs) by proposing CoLD, a framework that reduces bias through length-penalty adjustment, bias estimation, and joint training, leading to more accurate and concise reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs exhibit a pervasive length bias, assigning higher scores to longer reasoning steps even when the semantic content and logical validity remain unchanged, which undermines reliability and leads to verbose outputs.

Method: CoLD is a unified framework that mitigates length bias through three components: an explicit length-penalty adjustment, a learned bias estimator, and a joint training strategy.

Result: CoLD consistently reduces reward-length correlation, improves accuracy in step selection, and encourages more concise, logically valid reasoning.

Conclusion: CoLD demonstrates effectiveness and practicality in improving the fidelity and robustness of PRMs.

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [72] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: 本文构建了信号博弈模型，其中真正的组合理解可以进化。提出了两个新模型：一个是最小化接收者，只从原子消息中学习；另一个是通才接收者，从所有可用信息中学习。这些模型比之前的替代方案更简单，并允许接收者从消息的原子组件中学习。


<details>
  <summary>Details</summary>
Motivation: 标准信号博弈模型中的接收者在学习组合信息时遇到困难。即使发送者发送组合信息，接收者也不会以组合方式解释它们。当一个信息组件的信息丢失或被遗忘时，其他组件的信息也会被删除。

Method: 本文提出了两个新的模型：一个是最小化的接收者，只从信号的原子消息中学习；另一个是通才接收者，从所有可用信息中学习。

Result: 这些模型在很多方面比之前的替代方案更简单，并允许接收者从消息的原子组件中学习。

Conclusion: 本文构造了信号博弈模型，其中真正的组合理解可以进化。

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [73] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: 本研究探讨了不同问题类型对大型语言模型在推理任务中准确性的影响，发现问题类型对模型性能有显著影响，且推理准确性与最终答案选择准确性之间没有必然相关性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同问题类型对大型语言模型在推理任务中准确性的影响，这是一个尚未被深入研究的问题。

Method: 本研究通过定量和演绎推理任务，评估了五种大型语言模型在三种不同类型问题上的性能。

Result: 研究结果表明，不同问题类型对大型语言模型的性能有显著影响，且推理步骤的准确性与最终答案的选择准确性之间没有直接关联。

Conclusion: 研究发现不同类型的题目对大型语言模型在推理任务中的准确性有显著影响，且推理准确性与最终选择准确性之间没有必然的相关性。

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [74] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: 本文介绍了SemEval-2025任务11，这是一个跨28种语言的情绪识别挑战。我们探索了两种对比学习方法：基于样本的对比推理校准和基于生成的DPO、SimPO对比学习。我们的系统在英语的Track A中获得第9名，在Track B中获得第6名，并在其他语言中表现出色。


<details>
  <summary>Details</summary>
Motivation: SemEval-2025任务11旨在引入一个跨越28种语言的情绪识别挑战，鼓励研究人员探索更先进的方法来应对情感表达多样性和背景变化带来的挑战。

Method: 我们系统地探索了两种对比学习方法的优势：基于样本的（对比推理校准）和基于生成的（DPO，SimPO）对比学习。基于样本的对比方法通过比较两个样本来训练模型以生成更可靠的预测。基于生成的对比方法训练模型区分正确和错误的生成，从而优化其预测。所有模型都是从LLaMa3-Instruct-8B进行微调的。

Result: 我们的系统在英语的Track A中获得了第9名，在Track B中获得了第6名，并在其他语言中跻身顶级表现系统之列。

Conclusion: 我们的系统在英语的Track A中获得了第9名，在Track B中获得了第6名，并在其他语言中跻身顶级表现系统之列。

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [75] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: 本研究探讨了用户如何评估LLM系统，并提出了改进评估基准的建议，以提高LLM在科学研究中的可用性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在天文学和其他科学研究中的应用日益增加，但评估基准并未跟上实际用户评估和使用这些模型的多样化方式，因此需要改进评估程序。

Method: 我们通过分析368个查询和对11位天文学家的后续访谈，了解用户如何评估LLM系统。

Result: 我们提出了构建更好基准的具体建议，并据此构建了一个用于评估天文学中LLM的示例基准。

Conclusion: 我们的工作提供了改进LLM评估和最终可用性的方法，特别是在科学研究中的使用。

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [76] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: BELO 是一个用于评估大型语言模型在眼科领域表现的标准化、全面的基准测试，包含900个高质量问题，并通过多轮专家审核确保其准确性。


<details>
  <summary>Details</summary>
Motivation: 当前评估大型语言模型（LLMs）在眼科领域的基准测试范围有限，且过于侧重准确性。因此，需要一个更全面、标准化的评估基准来更好地衡量LLMs在眼科领域的表现。

Method: 通过多次专家检查，从多个医学数据集中（如BCSC、MedMCQA、MedQA、BioASQ和PubMedQA）筛选出眼科相关的多项选择题（MCQs），并使用关键词匹配和微调的PubMedBERT模型进行整理。此外，还进行了人工审核和专家评审，以确保问题的质量和准确性。

Result: BELO 包含900个高质量、经过专家审核的问题，来自五个来源。对六个LLMs进行了评估，包括准确性、宏F1和五种文本生成指标。此外，还进行了人类专家的定性评估。同时，建立了公开排行榜以促进透明评估和报告。

Conclusion: BELO 是一个标准化和全面的评估基准，旨在评估眼科相关临床准确性和推理质量。该数据集经过多轮专家审核，确保高质量，并且将作为保留的评估基准，以确保未来模型比较的公平性和可重复性。

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [77] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 本文介绍了IDRBench，这是一个用于评估大型语言模型在跨学科研究中提出有价值研究想法能力的基准。


<details>
  <summary>Details</summary>
Motivation: 缺乏一个专门的基准来评估LLMs在跨学科研究（IDR）环境中的能力，这阻碍了对其优势和局限性的全面理解。

Method: 我们引入了IDRBench——一个开创性的基准，包括专家注释的数据集和一套针对评估LLM在跨学科研究中提出有价值研究想法的能力的任务。

Result: 尽管LLM在某种程度上促进了IDR意识，但它们仍然难以产生高质量的IDR想法。

Conclusion: 这些发现不仅可以激发新的研究方向，还可以帮助开发在跨学科研究中表现出色的下一代LLM。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [78] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: This paper justifies the use of TF-IDF by connecting it to statistical significance testing, showing that it is related to the negative logarithm of the p-value from Fisher's exact test.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a sound theoretical foundation for TF-IDF, which has been widely used but lacks a rigorous statistical explanation.

Method: The paper demonstrates that TF-IDF can be understood from a significance testing perspective, specifically by showing its relationship to the negative logarithm of the p-value from a one-tailed Fisher's exact test.

Result: The paper shows that TF-IDF is closely related to the negative log-transformed p-value from Fisher's exact test under certain conditions, and that this relationship holds in the limit of an infinitely large document collection.

Conclusion: TF-IDF is a widely used term-weighting scheme in text analysis, and this paper provides a theoretical justification for its effectiveness by relating it to statistical significance testing.

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [79] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: 本文提出了一种名为DialogueForge的框架，用于生成AI模拟的对话，以解决收集真实人类-聊天机器人对话所需大量手动工作的问题。实验表明，大型专有模型在生成逼真对话方面表现更好，而较小的开源模型通过微调也能取得良好效果，但保持长篇对话的连贯性和自然性仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 收集人类-聊天机器人对话通常需要大量手动工作且耗时，这限制了对话AI的研究。因此，需要一种有效的方法来生成高质量的模拟对话，以促进研究进展。

Method: DialogueForge - 一种生成AI模拟对话的框架，以人类-聊天机器人风格进行模拟。该框架使用从真实人类-聊天机器人交互中提取的种子提示来初始化每个生成的对话。测试了多种LLM来模拟人类聊天机器人用户，包括最先进的专有模型和小规模的开源LLM，并生成针对特定任务的多轮对话。还探索了微调技术以增强较小模型生成难以区分的人类对话的能力。

Result: 大型专有模型（如GPT-4o）在生成更逼真的对话方面表现优于其他模型，而较小的开源模型（如Llama、Mistral）在性能上表现出色，并且具有更高的可定制性。通过监督微调技术，可以显著提升小型模型的性能。然而，保持连贯和自然的长篇对话仍然是所有模型面临的挑战。

Conclusion: 我们的实验表明，大型专有模型（例如GPT-4o）在生成更逼真的对话方面通常优于其他模型，而较小的开源模型（例如Llama、Mistral）则提供了有希望的性能，并且具有更高的可定制性。我们证明了通过使用监督微调技术可以显著提高小型模型的性能。然而，保持连贯和自然的长篇人类对话仍然是所有模型面临的共同挑战。

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [80] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: 本文介绍了“互动即智能”的研究系列，提出了一种新的深度研究系统Deep Cognition，通过实现可视化、可控且可中断的交互，细粒度的双向对话以及共享的认知上下文，显著提升了人机交互的效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法将交互仅视为访问AI能力的界面，而本文认为交互本身是智能的一个基本维度。当前的深度研究系统存在错误级联效应、研究边界不灵活以及错过专家整合机会等问题，因此需要一种新的方法来解决这些问题。

Method: 本文提出了一种新的方法，即Deep Cognition系统，它实现了三个关键创新：(1) 可视化、可控且可中断的交互；(2) 细粒度的双向对话；(3) 共享的认知上下文。

Result: 用户评估显示，认知监督范式在六个关键指标上优于最强基线，分别提高了20.0%、29.2%、18.5%、27.7%、8.8%和20.7%。在具有挑战性的研究问题上，Deep Cognition系统的性能比深度研究系统提高了31.8%至50.0%。

Conclusion: 本文提出了“互动即智能”研究系列，重新定义了人类与AI在深度研究任务中的关系。通过引入Deep Cognition系统，人类的角色从给予指令转变为认知监督，从而显著提升了交互的透明度、细粒度对话和实时干预能力。

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [81] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova是一个6.5亿参数的解码器-only transformer，通过仔细的架构设计和分词创新实现了更大模型的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 我们希望通过仔细的架构设计和分词创新来实现更大模型的性能，同时保持计算效率。

Method: 我们提出了Supernova，一个6.5亿参数的解码器-only transformer，通过仔细的架构设计和分词创新实现了更大模型的性能，同时保持计算效率。

Result: Supernova实现了10亿参数模型的90%性能，同时使用了53%更少的参数，并且只需要1000亿训练标记——比竞争模型少一个数量级。

Conclusion: 我们的研究挑战了现有的扩展范式，表明架构效率和分词质量可以弥补参数数量的减少。

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [82] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新的RLVR方法Archer，通过考虑不同类型的标记（知识和推理）来改进大型语言模型的推理能力，并在多个任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 之前的RLVR算法通常对所有标记应用统一的训练信号，而没有考虑低熵知识相关标记和高熵推理相关标记的不同作用。一些最近的方法尝试通过梯度掩码或异步更新来区分这些标记类型，但这些方法可能会破坏模型输出中的语义依赖关系并阻碍有效的学习。

Method: 我们提出了Archer，一种具有双令牌约束和同步更新的熵感知RLVR方法。具体来说，我们的方法对推理令牌应用较弱的KL正则化和较高的剪切阈值以鼓励探索，同时对知识令牌使用更强的约束以保持事实知识。

Result: 在几个数学推理和代码生成基准测试中，我们的方法显著优于之前的RLVR方法，达到或超过与可比规模模型相当的最先进性能。

Conclusion: 实验结果表明，我们的方法显著优于之前的RLVR方法，在可比规模的模型中达到了或超过了最先进的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [83] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: 本研究比较了基于水库计算和变压器的语言模型，发现变压器在预测质量上更优，而水库计算在效率上更具优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大型语言模型（LLM）在能源需求和处理速度方面的瓶颈问题，并探索水库计算在自然文本处理中的潜力。

Method: 研究比较了三种不同的字符级语言建模方法，包括两种不同的水库计算方法（仅输出层可训练）和基于变压器的架构（完全学习基于注意力的序列表示）。

Result: 研究发现，变压器在预测质量方面优于水库计算机，而水库计算机在计算效率方面表现更优。同时，研究还探讨了两种类型的水库计算：传统静态线性读出和基于注意力机制的动态适应输出权重。

Conclusion: 研究结果表明，变压器在预测质量方面表现出色，而水库计算机在减少训练和推理速度方面保持高效。此外，研究还提供了如何平衡资源约束与性能的指导。

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [84] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 本文探讨了与人道主义组织合作部署和维护AI模型的经验，并为实践者提供了关键的教训。


<details>
  <summary>Details</summary>
Motivation: 大多数AI for Good论文关注的是研究和模型开发，但很少讨论部署和与合作伙伴组织的合作过程以及实际影响。

Method: 本文描述了与人道主义组织的紧密合作，以及如何在资源受限的环境中部署和维护AI模型。

Result: 本文分享了与人道主义组织合作的经验，以及如何在资源有限的环境中部署和维护AI模型，并为实践者提供了关键的教训。

Conclusion: 本文分享了与人道主义组织合作的经验，以及如何在资源有限的环境中部署和维护AI模型，并为实践者提供了关键的教训。

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [85] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: 本研究探讨了中文-英语双语推理模型中的语言切换现象，发现语言切换不仅不是多语言训练的副产品，而是一种有助于推理的战略性行为。


<details>
  <summary>Details</summary>
Motivation: 先前的研究发现，在DeepSeek-R1中阻止这种行为会降低准确性，这表明语言混合可能有助于推理。因此，我们想研究中文-英语双语推理模型中的语言切换。

Method: 我们通过强化学习与可验证奖励（RLVR）的关键训练阶段来研究语言切换，并使用轻量级探测器来预测潜在的语言切换是否有助于或损害推理。

Result: 我们发现，强制单语解码会使数学推理任务的准确性降低5.6个百分点。此外，一个轻量级探测器可以被训练来预测潜在的语言切换是否有助于或损害推理，并且当用于指导解码时，可以提高准确性高达6.25个百分点。

Conclusion: 我们的研究结果表明，语言混合不仅仅是多语言训练的副产品，而是一种战略性的推理行为。

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [86] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: 本文提出了3LM基准套件，旨在填补阿拉伯语大型语言模型研究中的空白，并促进STEM和代码领域的发展。


<details>
  <summary>Details</summary>
Motivation: 目前针对阿拉伯语的大型语言模型研究相对有限，现有的阿拉伯语基准测试主要集中在语言、文化和宗教内容上，而STEM和代码等领域的研究不足，这限制了实际应用中大型语言模型的发展。

Method: 本文通过从阿拉伯语教科书和教育工作表中自然获取STEM相关问答对，以及使用相同来源生成合成STEM问题，构建了三个基准测试。此外，还通过人工审核过程仔细翻译了两个广泛使用的代码基准测试，以确保高质量和忠实的翻译。

Result: 本文成功构建了三个针对阿拉伯语的基准测试，涵盖了STEM问答、合成STEM问题和代码生成，并公开发布以支持阿拉伯语大型语言模型研究的发展。

Conclusion: 本文提出了3LM基准套件，旨在填补阿拉伯语大型语言模型研究中的空白，并促进STEM和代码领域的发展。

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [87] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 本文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组到少量代表性聚类中，实现了对激活稀疏性的高效预测和利用。该方法在保持模型质量的同时，显著降低了计算开销，并为未来的研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）表现出显著的激活稀疏性，其中只有部分神经元在给定输入下被激活。虽然这种稀疏性为减少计算成本提供了机会，但需要以可扩展的方式预测激活模式。然而，直接在神经元级别进行预测由于现代LLMs中神经元数量庞大而计算成本高昂。

Method: 本文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组到少量代表性聚类中，从而实现对激活稀疏性的高效预测和利用。

Result: 本文的方法在聚类精度上达到了79.34%，优于标准的二元聚类方法，同时保持了极小的困惑度（PPL）分数下降。当有足够多的聚类时，该方法的PPL分数低至12.49，证明了其在保留模型质量的同时减少计算开销的有效性。

Conclusion: 本文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组到少量代表性聚类中，实现了对激活稀疏性的高效预测和利用。该方法在保持模型质量的同时，显著降低了计算开销，并为未来的研究奠定了基础。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [88] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache是一种无需训练的KV缓存优化方法，通过梯子状的KV缓存模式和迭代压缩机制，提高了LLM的长距离建模能力和连续生成性能。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度的增加，LLM中的KV对数量急剧增加，导致效率瓶颈。因此，需要一种新的KV缓存优化方法来解决这一问题。

Method: LaCache通过引入一种梯子状的KV缓存模式和迭代压缩机制，实现了高效的KV缓存优化。

Result: 在各种任务、基准和LLM模型上的实验结果一致验证了LaCache的有效性。

Conclusion: LaCache有效地提升了LLM在长距离建模中的能力，同时实现了连续生成而不会出现内存不足的问题。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [89] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: 本文提出了一种名为Solo Connection的新方法，用于参数高效的微调大型语言模型。该方法在解码器块级别调整表示，相比现有方法具有更高的性能和更低的参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的PEFT方法如LoRA主要集中在调整注意力权重矩阵，而Solo Connection旨在通过在解码器块级别调整表示来提高模型的适应能力。同时，随着GPT2变体扩展到更多层，需要重新考虑微调过程中跳过连接的使用方式。

Method: Solo Connection 方法通过在解码器块级别调整表示，而不是修改单个权重矩阵。它引入了一个可训练的线性变换，以逐渐插值零向量和任务特定表示，从而实现平滑和稳定的适应。

Result: Solo Connection 在E2E自然语言生成基准测试中表现优于LoRA，并且相比LoRA减少了59%的可训练参数，相比完整微调GPT2减少了超过99%的可训练参数。

Conclusion: Solo Connection 是一种新颖的参数高效微调方法，它在E2E自然语言生成基准测试中表现优于LoRA，并且减少了可训练参数的数量。此外，该方法基于同伦理论，通过可训练的线性变换逐渐将零向量与任务特定表示进行插值，从而实现稳定和渐进的适应。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [90] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本文分析了简单的测试时缩放方法，发现其主要依赖于通过强制最大长度进行缩放，而通过附加'Wait'进行缩放会导致不一致。o1类似模型在测试时计算能力的扩展能够超越其峰值性能，而简单的测试时缩放方法会逐步降低模型性能的上限。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析简单的测试时缩放方法，并探讨其在不同模型中的效果，以更好地理解如何通过扩展测试时计算能力来提高模型性能。

Method: 本文分析了简单的测试时缩放方法，并通过实验验证了不同方法对模型性能的影响。

Result: 本文发现，简单的测试时缩放方法主要依赖于通过强制最大长度进行缩放，而通过附加'Wait'进行缩放会导致不一致。此外，o1类似模型在测试时计算能力的扩展能够超越其峰值性能，而简单的测试时缩放方法会逐步降低模型性能的上限。

Conclusion: 本文分析了简单的测试时缩放方法，并发现其缩放行为主要归因于通过强制最大长度进行缩放。相比之下，对从o1类似模型中提炼的长链思维数据进行微调对缩放行为没有显著影响，而通过附加'Wait'进行缩放会导致不一致，因为模型可能会在解决方案之间振荡。o1类似模型（如DeepSeek-R1@）在测试时计算能力的扩展与通过强制最大长度进行缩放存在关键区别。这些模型通常被允许使用所需的任何计算资源，唯一的限制是模型的最大支持长度。通过在强化学习中学习自然地扩展测试时计算能力，o1类似模型在扩展时超越了其峰值性能。相反，简单的测试时缩放会随着缩放的进行逐步降低模型性能的上限。虽然可以通过缩放来复制o1模型的测试时缩放行为，但必须认识到，扩展测试时计算能力的目标是解锁更高的性能——超越模型原本可以达到的性能——而不是仅仅再现缩放行为的外观。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [91] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: 本文提出了一种新的垃圾文本检测框架GCC-Spam，通过字符相似性网络、对比学习和生成对抗网络来提高检测效果，并在真实数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 互联网上垃圾文本的指数增长需要强大的检测机制来减轻信息泄露和社会不稳定等风险。本文解决了两个主要挑战：垃圾邮件发送者采用的对抗策略和标记数据的稀缺性。

Method: 我们提出了一个名为GCC-Spam的新垃圾文本检测框架，该框架集成了三个核心创新：字符相似性网络、对比学习和生成对抗网络（GAN）。

Result: 实验结果表明，我们的模型在真实数据集上优于基线方法，实现了更高的检测率，并且使用的标记样本显著减少。

Conclusion: 我们的模型在真实数据集上的实验表明，它优于基线方法，实现了更高的检测率，并且使用的标记样本显著减少。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [92] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: 该研究分析了RLVR的理论和实证表现，发现其可能受限于基础模型的支持，且可能缩小探索范围，未来需要新的算法创新。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解RLVR是否真正扩展了模型的推理边界，还是仅仅增强了已有高奖励输出。

Method: 该研究提供了RLVR的理论和实证分析，探讨了其在样本生成和探索方面的限制。

Result: 实验表明，尽管RLVR提高了pass@1，但在更大的采样预算下，经验支持的缩小超过了扩展，导致无法恢复原本可访问的正确答案。

Conclusion: 研究揭示了RLVR在扩展推理范围方面的潜在限制，并指出需要未来的算法创新来突破这些限制。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [93] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: 本文提出了一种新的多模态方法，即长-短距离图神经网络（LSDGNN），用于情感识别在对话中的任务。通过构建长距离和短距离图神经网络，以及使用差异正则化器和双仿射模块来促进特征交互，同时引入改进的课程学习来解决数据不平衡的问题。实验结果表明，该模型在IEMOCAP和MELD数据集上表现优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 情感识别在对话中是一个实际且具有挑战性的任务。

Method: 本文提出了一种新的多模态方法，即长-短距离图神经网络（LSDGNN）。基于有向无环图（DAG），它构建了一个长距离图神经网络和一个短距离图神经网络，分别获取远距离和近距离话语的多模态特征。为了确保长距离和短距离特征在表示上尽可能不同，同时使两个模块之间能够相互影响，我们采用了一种差异正则化器，并结合了双仿射模块来促进特征交互。此外，我们提出了改进的课程学习（ICL）来解决数据不平衡的挑战。

Result: 在IEMOCAP和MELD数据集上的实验结果表明，我们的模型优于现有基准。

Conclusion: 实验结果表明，我们的模型优于现有的基准。

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [94] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了一种新的奖励建模方法OCRM，以解决RLHF中的过优化问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在RLHF中，随着训练的进行，生成的响应不再与RM训练时看到的响应相似，导致RM变得不准确。这被称为过优化问题。

Method: 本文从分布偏移的角度研究过优化问题，并提出了一种无需新标签或样本的离策略校正奖励建模方法（OCRM）。

Result: 实验结果表明，OCRM方法在摘要和聊天机器人数据集上显著优于标准RLHF方法和基线方法。

Conclusion: 本文提出了Off-Policy Corrected Reward Modeling (OCRM)方法，以解决RLHF中的过优化问题。实验结果表明，该方法在摘要和聊天机器人数据集上表现优于标准的RLHF方法和基线方法。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [95] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法 Data Mixing Agent，用于在持续预训练中重新加权领域，以实现源领域和目标领域之间的平衡性能，并且具有良好的泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 持续预训练在小规模任务特定数据上的方法可以提高大型语言模型在新目标领域的表现，但存在灾难性遗忘原始能力的风险。现有的领域重新加权策略依赖于人工指定的启发式方法，而本文旨在提出一种更通用的启发式方法。

Method: 本文提出了 Data Mixing Agent，这是一个基于模型的端到端框架，通过在大量数据混合轨迹上进行强化学习来学习重新加权领域。

Result: 实验表明，Data Mixing Agent 在数学推理的持续预训练中优于强基线，在源领域和目标领域基准之间实现了平衡性能。此外，它在未见过的源领域、目标模型和领域空间中表现出良好的泛化能力。

Conclusion: Data Mixing Agent 在持续预训练中表现出色，能够实现源领域和目标领域基准的平衡性能，并且在未见过的源领域、目标模型和领域空间中具有良好的泛化能力。此外，它展示了与人类直觉一致的策略，并且在使用较少源领域数据的情况下实现了优越的模型性能。

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [96] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 本研究探讨了小型语言模型是否可以通过强化学习与验证奖励（RLVR）获得通用的心智理论能力。结果表明，小型模型难以发展出通用的心智理论能力，且长期训练会导致模型过拟合，无法泛化到新的任务。


<details>
  <summary>Details</summary>
Motivation: 最近大型语言模型（LLMs）的进步展示了在复杂推理方面的涌现能力，这主要由后训练阶段应用的基于规则的强化学习（RL）技术推动。这引发了是否可以使用类似方法在LLMs中植入更细腻、类人的社会智能（如心智理论，ToM）的问题。

Method: 本研究通过在各种组合的著名心智理论数据集（HiToM、ExploreToM、FANToM）上训练模型，并在保留的数据集（如OpenToM）上测试泛化能力，进行了系统评估。

Result: 研究发现，小型LLMs难以通过RLVR获得稳健且可泛化的ToM能力。尽管在分布内的任务上表现有所提高，但这种能力无法转移到具有不同特征的未见过的ToM任务。此外，长期的RL训练导致模型“破解”了训练数据集的统计模式，从而在域内数据上取得显著性能提升，但在域外任务上的表现没有变化或退化。

Conclusion: 研究结果表明，小型语言模型难以发展出通用的理论心智能力。虽然在分布内的任务上表现有所提高，但这种能力无法转移到具有不同特征的未见过的任务。此外，长期的强化学习训练导致模型“破解”了训练数据集的统计模式，从而在域内数据上取得显著性能提升，但在域外任务上的表现没有变化或退化。这表明所学的行为是一种狭窄的过拟合，而不是真正抽象的心智理论能力的获取。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [97] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 本文提出了一种新的GUI接地奖励框架GUI-G$^2$，它通过将GUI元素建模为连续高斯分布来解决传统二进制奖励的不足。该框架在多个基准测试中表现出色，优于现有方法，并展示了对界面变化的更好鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法使用二进制奖励，将元素视为命中或错过的目标，这会创建稀疏信号，忽略空间交互的连续性。受人类点击行为的启发，这种行为自然形成了以目标元素为中心的高斯分布，我们引入了GUI-G$^2$，以解决这个问题。

Method: 我们引入了GUI Gaussian Grounding Rewards (GUI-G$^2$)，这是一个原理性的奖励框架，将GUI元素建模为界面上的连续高斯分布。GUI-G$^2$结合了两种协同机制：高斯点奖励通过以元素质心为中心的指数衰减分布来建模精确的定位，而覆盖奖励通过测量预测高斯分布与目标区域之间的重叠来评估空间对齐度。我们还开发了一个自适应方差机制，根据元素尺寸校准奖励分布。

Result: 在ScreenSpot、ScreenSpot-v2和ScreenSpot-Pro基准测试中进行的大量实验表明，GUI-G$^2$显著优于最先进的方法UI-TARS-72B，在ScreenSpot-Pro上的改进最为显著，达到24.7%。

Conclusion: 我们的分析表明，连续建模提供了对界面变化的优越鲁棒性和对未见过的布局的增强泛化能力，为GUI交互任务中的空间推理建立了一个新范式。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [98] [Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation](https://arxiv.org/abs/2507.14221)
*Eoghan Cunningham,James Cross,Derek Greene*

Main category: cs.CY

TL;DR: 本文探讨了使用大型语言模型对欧洲议会全体会议辩论进行摘要，并研究了在此过程中出现的算法和代表性偏差。


<details>
  <summary>Details</summary>
Motivation: 自动化议会辩论的摘要使用大型语言模型（LLMs）可以为公众提供更易访问的复杂立法话语。然而，这样的摘要不仅要准确且简洁，还必须公平地代表所有发言人的观点和贡献。

Method: 本文提出了一种结构化的多阶段摘要框架，以提高文本连贯性和内容保真度，并允许系统分析发言者属性如何影响其贡献在最终摘要中的可见性和准确性。

Result: 通过使用专有和开放权重的LLM进行实验，我们发现了持续的位置和党派偏见，某些发言人的贡献被系统性地低估或错误归因。

Conclusion: 这些发现强调了在民主应用中部署大型语言模型时需要领域敏感的评估指标和伦理监督。

Abstract: The automated summarisation of parliamentary debates using large language
models (LLMs) offers a promising way to make complex legislative discourse more
accessible to the public. However, such summaries must not only be accurate and
concise but also equitably represent the views and contributions of all
speakers. This paper explores the use of LLMs to summarise plenary debates from
the European Parliament and investigates the algorithmic and representational
biases that emerge in this context. We propose a structured, multi-stage
summarisation framework that improves textual coherence and content fidelity,
while enabling the systematic analysis of how speaker attributes -- such as
speaking order or political affiliation -- influence the visibility and
accuracy of their contributions in the final summaries. Through our experiments
using both proprietary and open-weight LLMs, we find evidence of consistent
positional and partisan biases, with certain speakers systematically
under-represented or misattributed. Our analysis shows that these biases vary
by model and summarisation strategy, with hierarchical approaches offering the
greatest potential to reduce disparity. These findings underscore the need for
domain-sensitive evaluation metrics and ethical oversight in the deployment of
LLMs for democratic applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [99] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: 本文提出了一种简化的两阶段框架，用于提高法律文档检索的效率和准确性。该方法使用微调的双编码器进行快速候选检索，并使用交叉编码器进行精确重新排序。在SoICT Hackathon 2024比赛中，该方法取得了前三名的好成绩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在法律等专业领域面临重大挑战，其中精度和领域特定知识至关重要。

Method: 本文提出了一种简化的两阶段框架，包括检索和重新排序，以提高法律文档检索的效率和准确性。我们的方法使用微调的双编码器进行快速候选检索，然后使用交叉编码器进行精确重新排序，两者都通过战略性负例挖掘进行了优化。

Result: 在SoICT Hackathon 2024法律文档检索比赛中，我们的团队4Huiter获得了前三名。虽然表现最好的团队采用了集成模型和在大型bge-m3架构上的迭代自训练，但我们的轻量级单次通过方法提供了具有更少参数的竞争性替代方案。

Conclusion: 该框架表明，优化的数据处理、定制的损失函数和平衡的负样本采样对于构建法律环境中的强大检索增强系统至关重要。

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [100] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的框架GREAT，用于解决视频相关搜索中的I2Q推荐问题。通过构建基于查询的Trie树来指导查询生成，并在推理阶段使用Trie树引导token生成，最后通过后处理模块优化项目与查询之间的相关性和字面质量。实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 目前，短视频平台已成为人们分享经验和获取信息的主要场所。为了更好地满足用户在浏览短视频时获取信息的需求，一些应用在视频底部引入了搜索入口，并附带推荐的相关查询。然而，这一场景的研究和公开数据集较少，因此需要一种新的方法来解决I2Q推荐问题。

Method: 本文提出了一种基于大语言模型的框架GREAT，通过构建基于查询的Trie树来指导查询生成，并在推理阶段使用Trie树引导token生成，最后通过后处理模块优化项目与查询之间的相关性和字面质量。

Result: 本文提出的GREAT框架在离线和在线实验中均表现出色，证明了其有效性。

Conclusion: 本文提出了一种基于大语言模型的框架GREAT，用于解决视频相关搜索中的item-to-query（I2Q）推荐问题。实验结果表明该方法有效。

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [101] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 本文提出WebGuard数据集以评估网络代理的风险，并通过微调模型提高其性能，但当前模型仍无法满足高风险部署的需求。


<details>
  <summary>Details</summary>
Motivation: 由于自主网络代理可能采取意外或有害的行为，因此需要有效的安全措施，类似于对人类用户的访问控制。

Method: 引入WebGuard数据集，用于评估网络代理行动风险并开发防护措施。通过微调专门的guardrail模型进行评估，测试不同泛化设置下的性能。

Result: 即使是最先进的LLM在预测行动结果和识别高风险行动方面也表现出不足，而微调后的Qwen2.5VL-7B模型显著提高了性能，但仍不足以满足高风险部署的要求。

Conclusion: 尽管在使用WebGuard进行微调后，模型性能有所提升，但仍然无法满足高风险部署所需的可靠性，需要进一步改进guardrail模型以接近完美的准确率和召回率。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [102] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，扩展大型推理模型的推理长度可能会导致性能下降，并出现多种失败模式，强调了在不同推理长度下评估模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 我们想要研究扩展大型推理模型的推理长度对性能的影响，并识别可能的失败模式。

Method: 我们构建了评估任务，其中扩展大型推理模型（LRMs）的推理长度会降低性能，显示出测试时计算与准确率之间的反向关系。

Result: 我们发现当模型进行更长的推理时，会出现五种不同的失败模式，包括被无关信息分散注意力、过度拟合问题框架、从合理的先验转向虚假相关性、难以保持对复杂演绎任务的专注以及可能放大令人担忧的行为。

Conclusion: 我们的结果表明，评估模型在不同推理长度下的表现对于识别和解决LRMs中的这些失败模式非常重要。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [103] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: This paper introduces Routine, a multi-step agent planning framework that enhances the stability and accuracy of agent systems in enterprise environments by incorporating domain-specific process knowledge and improving model adaptability to new scenarios.


<details>
  <summary>Details</summary>
Motivation: The deployment of agent systems in an enterprise environment is often hindered by challenges such as common models lacking domain-specific process knowledge, leading to disorganized plans, missing key tools, and poor execution stability.

Method: Routine is a multi-step agent planning framework designed with a clear structure, explicit instructions, and seamless parameter passing to guide the agent's execution module in performing multi-step tool-calling tasks with high stability. Additionally, a Routine-following training dataset was constructed, and Qwen3-14B was fine-tuned. Routine-based distillation was also employed to create a scenario-specific, multi-step tool-calling dataset.

Result: In evaluations conducted within a real-world enterprise scenario, Routine significantly increases the execution accuracy in model tool calls, increasing the performance of GPT-4o from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. Fine-tuning on the distilled dataset raised the model's accuracy to 95.5%, approaching GPT-4o's performance.

Conclusion: Routine provides a practical and accessible approach to building stable agent workflows, accelerating the deployment and adoption of agent systems in enterprise environments, and advancing the technical vision of AI for Process.

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [104] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 本文探讨了多智能体系统可能带来的风险，特别是去中心化系统在执行恶意行为方面的有效性，并强调了需要更好的检测和应对措施。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统的兴起，人们越来越担心AI驱动的群体可能造成的危害。然而，目前大多数AI安全研究集中在单个AI系统上，而多智能体系统（MAS）在复杂现实情况下的风险仍缺乏探索。

Method: 本文引入了一个灵活的框架，支持集中式和去中心式协调结构，用于模拟恶意多智能体系统的风险。

Result: 研究发现，去中心化系统比集中式系统更有效地执行恶意行为。去中心化系统的自主性使其能够适应策略并造成更多损害。即使应用了传统干预措施，如内容标记，去中心化群体也能调整战术以避免被检测到。

Conclusion: 本文提出了一个模拟恶意多智能体系统勾结风险的原型，并展示了去中心化系统在执行恶意行为方面的有效性。同时，本文强调了需要更好的检测系统和应对措施来应对这些威胁。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [105] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 我们提出了一个框架，用于对Articulate Medical Intelligence Explorer (AMIE) AI系统进行有效的异步监督。我们提出了guardrailed-AMIE (g-AMIE)，这是一个多代理系统，在限制范围内执行病史采集，避免提供个性化的医疗建议。之后，g-AMIE通过临床医生驾驶舱界面将评估传达给负责的初级保健医生（PCP）。PCP提供监督并保留临床决策的责任。在一项随机、盲法虚拟客观结构化临床考试（OSCE）中，我们比较了g-AMIE与NPs/PAs或一组PCPs在相同限制下的表现。在60个场景中，g-AMIE在执行高质量的初步评估、总结病例以及提出诊断和管理计划方面优于两组，这导致了更高质量的综合决策。PCP对g-AMIE的监督也比之前工作中单独的PCP咨询更节省时间。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中的患者安全保证，提供个体诊断和治疗计划被认为是受监管的活动，由持牌专业人员进行。此外，医生通常会监督其他团队成员，包括护士从业者（NPs）或医师助理/关联人员（PAs）。因此，我们提出了一种有效的异步监督框架。

Method: 我们提出了一个框架，用于对Articulate Medical Intelligence Explorer (AMIE) AI系统进行有效的异步监督。我们提出了guardrailed-AMIE (g-AMIE)，这是一个多代理系统，在限制范围内执行病史采集，避免提供个性化的医疗建议。之后，g-AMIE通过临床医生驾驶舱界面将评估传达给负责的初级保健医生（PCP）。PCP提供监督并保留临床决策的责任。

Result: 在一项随机、盲法虚拟客观结构化临床考试（OSCE）中，我们比较了g-AMIE与NPs/PAs或一组PCPs在相同限制下的表现。在60个场景中，g-AMIE在执行高质量的初步评估、总结病例以及提出诊断和管理计划方面优于两组，这导致了更高质量的综合决策。PCP对g-AMIE的监督也比之前工作中单独的PCP咨询更节省时间。

Conclusion: 我们的研究结果展示了异步监督作为诊断AI系统在专家人类监督下运行的可行范式，有助于增强现实世界的护理。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [106] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO是一种新的框架，通过两阶段强化学习过程，使模型能够根据问题复杂度动态调整推理长度，从而提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常施加严格的限制或依赖于事后干预，而LAPO旨在将推理长度控制转化为模型的内在能力，以解决计算自由导致的过度令牌生成问题。

Method: LAPO采用两阶段强化学习过程，首先让模型通过发现成功解决方案长度的统计分布来学习自然的推理模式，然后利用这些模式作为元认知指导，将其直接嵌入模型的推理上下文中以确保推理时的灵活性。

Result: 在数学推理基准测试中，LAPO将令牌使用量减少了高达40.9%，同时将准确性提高了2.3%。

Conclusion: LAPO框架通过将推理长度控制转化为模型的内在能力，使模型在推理时能够根据问题复杂度分配计算资源，从而在不牺牲质量的情况下实现高效的推理。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [107] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: 本文介绍了分层预算策略优化（HBPO），这是一种强化学习框架，使模型能够学习特定于问题的推理深度，而不会牺牲能力。HBPO通过分层预算探索将滚动样本分成多个具有不同标记预算的子组，旨在实现高效的资源分配同时防止能力退化。实验表明，HBPO在四个推理基准测试中平均标记使用量减少了60.6%，同时准确率提高了3.14%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过广泛的思维链生成实现了显著的性能，但通过统一的推理策略在无论问题复杂性如何的情况下表现出显著的计算效率低下。我们提出了一种强化学习框架，使模型能够学习特定于问题的推理深度，而不会牺牲能力。

Method: 我们提出了分层预算策略优化（HBPO），这是一种强化学习框架，使模型能够在不牺牲能力的情况下学习特定于问题的推理深度。HBPO通过分层预算探索将滚动样本分成多个具有不同标记预算的子组，旨在实现高效的资源分配同时防止能力退化。我们引入了差异化的奖励机制，以创建与问题复杂性对齐的预算感知激励，使模型能够发现任务要求和计算努力之间的自然对应关系。

Result: 广泛的实验表明，HBPO在四个推理基准测试中平均标记使用量减少了60.6%，同时准确率提高了3.14%。与现有方法相比，HBPO表现出一种新兴的自适应行为，其中模型可以根据问题复杂性自动调整推理深度。

Conclusion: 我们的结果表明，推理效率和能力并非本质上相互冲突，可以通过适当构建的分层训练同时优化，以保持探索多样性。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [108] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: ExCyTIn-Bench 是第一个用于评估 LLM 代理在网络安全威胁调查任务上的基准，它提供了一个数据集和方法，用于生成可验证的问题和奖励，以训练代理。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 的发展，构建基于 LLM 的代理以实现自动威胁调查是一个有前景的方向。为了帮助开发和评估 LLM 代理，需要一个基准测试。

Method: 构建了一个数据集，涵盖8个模拟的真实多步骤攻击、57个来自 Microsoft Sentinel 和相关服务的日志表以及589个自动生成的问题。利用专家设计的检测逻辑提取安全日志来构建威胁调查图，并使用 LLM 生成问题。

Result: 实验表明，任务具有挑战性：在基础设置下，所有评估模型的平均奖励为 0.249，最佳结果为 0.368，这为未来的研究留下了大量空间。

Conclusion: ExCyTIn-Bench 是一个用于评估 LLM 代理在网络安全威胁调查任务上的基准，该任务通过从调查图中派生的安全问题进行评估。

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [109] [What do Large Language Models know about materials?](https://arxiv.org/abs/2507.14586)
*Adrian Ehrenhofer,Thomas Wallmersperger,Gianaurelio Cuniberti*

Main category: physics.app-ph

TL;DR: 本文研究了LLMs在材料科学中的应用，提出了一个材料知识基准，以帮助选择合适的模型。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs主要针对互联网上的非科学内容进行训练，而工程应用需要模型具备生成正确材料信息的能力。因此，有必要研究模型的内在知识。

Method: 本文以元素周期表为例，研究了词汇和分词在材料指纹唯一性中的作用，以及LLMs生成事实正确输出的能力。

Result: 本文展示了不同最先进的开放模型在生成正确材料信息方面的能力，并提出了一个材料知识基准。

Conclusion: 本文提出了一个材料知识基准，以帮助选择合适的模型，并确定LLMs在PSPP链中的适用性以及需要专业模型的领域。

Abstract: Large Language Models (LLMs) are increasingly applied in the fields of
mechanical engineering and materials science. As models that establish
connections through the interface of language, LLMs can be applied for
step-wise reasoning through the Processing-Structure-Property-Performance chain
of material science and engineering. Current LLMs are built for adequately
representing a dataset, which is the most part of the accessible internet.
However, the internet mostly contains non-scientific content. If LLMs should be
applied for engineering purposes, it is valuable to investigate models for
their intrinsic knowledge -- here: the capacity to generate correct information
about materials. In the current work, for the example of the Periodic Table of
Elements, we highlight the role of vocabulary and tokenization for the
uniqueness of material fingerprints, and the LLMs' capabilities of generating
factually correct output of different state-of-the-art open models. This leads
to a material knowledge benchmark for an informed choice, for which steps in
the PSPP chain LLMs are applicable, and where specialized models are required.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [110] [Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion](https://arxiv.org/abs/2507.14534)
*Yu Zhang,Baotong Tian,Zhiyao Duan*

Main category: eess.AS

TL;DR: Conan is a chunkwise online zero-shot voice conversion model that preserves the content of the source while matching the voice timbre and styles of reference speech.


<details>
  <summary>Details</summary>
Motivation: Current VC models struggle to preserve semantic fidelity under real-time constraints, deliver natural-sounding conversions, and adapt effectively to unseen speaker characteristics.

Method: Conan is a chunkwise online zero-shot voice conversion model that comprises three core components: a Stream Content Extractor, an Adaptive Style Encoder, and a Causal Shuffle Vocoder.

Result: Experimental evaluations demonstrate that Conan outperforms baseline models in subjective and objective metrics.

Conclusion: Conan outperforms baseline models in subjective and objective metrics.

Abstract: Zero-shot online voice conversion (VC) holds significant promise for
real-time communications and entertainment. However, current VC models struggle
to preserve semantic fidelity under real-time constraints, deliver
natural-sounding conversions, and adapt effectively to unseen speaker
characteristics. To address these challenges, we introduce Conan, a chunkwise
online zero-shot voice conversion model that preserves the content of the
source while matching the voice timbre and styles of reference speech. Conan
comprises three core components: 1) a Stream Content Extractor that leverages
Emformer for low-latency streaming content encoding; 2) an Adaptive Style
Encoder that extracts fine-grained stylistic features from reference speech for
enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully
causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations
demonstrate that Conan outperforms baseline models in subjective and objective
metrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [111] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为TCP-LLaVA的新架构，通过令牌压缩实现全切片图像的视觉问答任务，有效降低了计算成本并提高了准确率。


<details>
  <summary>Details</summary>
Motivation: 全切片图像（WSI）在病理学中可以达到10,000 x 10,000像素，这对多模态大语言模型（MLLM）提出了重大挑战，因为需要处理长上下文长度和高计算需求。现有方法缺乏用于视觉问答（VQA）所需的生成能力。

Method: TCP-LLaVA引入了一组可训练的压缩令牌，通过模态压缩模块聚合视觉和文本信息，仅将压缩后的令牌传递给LLM进行答案生成。

Result: 实验表明，TCP-LLaVA在十个TCGA肿瘤亚型上表现出色，优于现有的MLLM基线，并且显著减少了训练资源消耗。

Conclusion: TCP-LLaVA在VQA准确性上优于现有的MLLM基线，并且显著减少了训练资源消耗。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [112] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 本文提出了一种高质量的文档级数据集Doc-750K，并开发了无需依赖RAG的原生多模态模型Docopilot，实验证明其在文档理解任务和多轮交互中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）取得了显著进展，但它们在复杂、多页文档理解方面的性能仍然不足，主要是由于缺乏高质量的文档级数据集。虽然当前的检索增强生成（RAG）方法提供了部分解决方案，但它们存在碎片化检索上下文、多阶段错误累积和额外的检索时间成本等问题。

Method: 我们开发了一个原生多模态模型Docopilot，它可以在不依赖RAG的情况下准确处理文档级依赖关系。

Result: Docopilot在文档理解任务和多轮交互中表现出卓越的连贯性、准确性和效率，设定了新的基准。

Conclusion: 实验表明，Docopilot在文档理解任务和多轮交互中实现了卓越的连贯性、准确性和效率，为文档级多模态理解设定了新基准。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [113] [Dissociating model architectures from inference computations](https://arxiv.org/abs/2507.15776)
*Noor Sajid,Johan Medrano*

Main category: q-bio.NC

TL;DR: 研究显示自回归模型可以通过结构化推理模仿深度时间模型，表明预测过程与模型架构之间没有必然联系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在区分模型架构与推理时的计算，以更好地理解非马尔可夫序列建模。

Method: 通过结构化迭代推理中的上下文访问，展示了自回归模型可以模仿深度时间计算。

Result: 使用基于下一个标记预测的变压器模型，证明了在迭代推理中引入分层时间因子分解可以保持预测能力并减少计算量。

Conclusion: 研究强调了构建和优化预测的过程不一定受限于其底层模型架构。

Abstract: Parr et al., 2025 examines how auto-regressive and deep temporal models
differ in their treatment of non-Markovian sequence modelling. Building on
this, we highlight the need for dissociating model architectures, i.e., how the
predictive distribution factorises, from the computations invoked at inference.
We demonstrate that deep temporal computations are mimicked by autoregressive
models by structuring context access during iterative inference. Using a
transformer trained on next-token prediction, we show that inducing
hierarchical temporal factorisation during iterative inference maintains
predictive capacity while instantiating fewer computations. This emphasises
that processes for constructing and refining predictions are not necessarily
bound to their underlying model architectures.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [114] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: 本研究探讨了大型语言模型（如ChatGPT）在演绎分类任务中的潜力，并通过不同的干预策略提高了其可靠性，表明这些模型可以用于严格的定性编码工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前大多数研究集中在归纳编码应用上，而本研究旨在探索大型语言模型在符合既定人类编码方案的演绎分类任务中的潜力。

Method: 研究使用了四种干预方法：零样本、少样本、基于定义的方法以及一种新的逐步任务分解策略，对美国最高法院案件摘要进行了分类，并通过标准分类指标和构建效度分析评估了性能。

Result: 研究结果显示，逐步任务分解策略表现最佳，准确率为0.775，kappa值为0.744，alpha值为0.746，达到了实质性协议的阈值。此外，ChatGPT在案例摘要中表现出稳定的协议，包括低支持子类中的高F1分数。

Conclusion: 研究发现，通过针对性的定制干预措施，大型语言模型（如ChatGPT）可以达到适合集成到严格定性编码工作流程的可靠性水平。

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [115] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 这项研究介绍了一种创新的Python语音辅助调试插件，通过多模式错误反馈显著提高编程可访问性和认知效率。


<details>
  <summary>Details</summary>
Motivation: 旨在将无声的运行时错误转化为可操作的听觉诊断，提高编程可访问性，并提升软件开发工作流程中的认知效率。

Method: 通过实现全局异常挂钩架构、pyttsx3文本到语音转换和基于Tkinter的GUI可视化，该解决方案通过并行听觉和视觉通道提供多模式错误反馈。

Result: 实证评估显示，与传统的堆栈跟踪调试相比，认知负荷减少了37%（p<0.01，n=50），同时通过语音异常分类和上下文化实现了78%的更快错误识别。系统在异常处理期间实现了低于1.2秒的语音延迟和不到18%的CPU开销。

Conclusion: 该解决方案代表了向以人类为中心的错误诊断的根本转变，弥合了编程可访问性中的关键差距，并为软件开发工作流程中的认知效率设定了新标准。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [116] [Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems](https://arxiv.org/abs/2507.15214)
*Natalia Tomashenko,Emmanuel Vincent,Marc Tommasi*

Main category: cs.SD

TL;DR: 本文提出了一种新的方法，通过从语音时间动态中提取上下文相关的持续时间嵌入来表示说话人特征，并开发了新的攻击模型，分析了说话人验证和语音匿名化系统中的潜在漏洞。实验结果表明，所开发的攻击模型在与文献中报道的更简单的语音时间动态表示相比，对于原始和匿名数据的说话人验证性能有显著提高。


<details>
  <summary>Details</summary>
Motivation: 语音的时间动态，包括节奏、语调和说话速度的变化，包含有关说话人身份的重要且独特信息。

Method: 本文提出了一种新的方法，通过从语音时间动态中提取上下文相关的持续时间嵌入来表示说话人特征。我们开发了新的攻击模型，并分析了说话人验证和语音匿名化系统中的潜在漏洞。

Result: 实验结果表明，所开发的攻击模型在与文献中报道的更简单的语音时间动态表示相比，对于原始和匿名数据的说话人验证性能有显著提高。

Conclusion: 实验结果表明，所开发的攻击模型在与文献中报道的更简单的语音时间动态表示相比，对于原始和匿名数据的说话人验证性能有显著提高。

Abstract: The temporal dynamics of speech, encompassing variations in rhythm,
intonation, and speaking rate, contain important and unique information about
speaker identity. This paper proposes a new method for representing speaker
characteristics by extracting context-dependent duration embeddings from speech
temporal dynamics. We develop novel attack models using these representations
and analyze the potential vulnerabilities in speaker verification and voice
anonymization systems.The experimental results show that the developed attack
models provide a significant improvement in speaker verification performance
for both original and anonymized data in comparison with simpler
representations of speech temporal dynamics reported in the literature.

</details>


### [117] [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
*Ayush Singh Bhadoriya,Abhishek Nikunj Shinde,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.SD

TL;DR: 本文介绍了一种基于扩散的文本到语音系统，用于生成未见过的说话人的语音并支持多种印度语言。


<details>
  <summary>Details</summary>
Motivation: 我们旨在解决生成未见过的说话人的语音以及支持多种印度语言的挑战。

Method: 我们提出了一种基于扩散的文本到语音（TTS）系统，利用说话人编码器从短参考音频样本中提取嵌入，以条件DDPM解码器进行多说话人生成。我们还采用了一种基于交叉注意力的持续时间预测机制，利用参考音频以实现更准确和说话人一致的时间建模。

Result: 我们的方法在IndicSUPERB数据集上进行了训练，该数据集包含多种印度语言，如孟加拉语、古吉拉特语、印地语、马拉地语、马拉雅拉姆语、旁遮普语和泰米尔语。

Conclusion: 我们的方法在生成语音时能够更接近目标说话人，并提高了持续时间建模和整体表现力。此外，通过使用无分类器指导，我们改进了零样本生成。

Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

</details>

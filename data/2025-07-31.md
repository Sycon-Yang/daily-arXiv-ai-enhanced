<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian](https://arxiv.org/abs/2507.22159)
*Vanessa Rebecca Wiyono,David Anugraha,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

TL;DR: 本文介绍了IndoPref，这是首个完全由人类撰写的多领域印尼语偏好数据集，旨在评估大型语言模型（LLMs）生成文本的自然性和质量。


<details>
  <summary>Details</summary>
Motivation: 印尼语在大型语言模型（LLMs）的基于偏好的研究中严重不足，现有的多语言数据集大多源自英语翻译，缺乏文化和语言上的真实性。

Method: 引入IndoPref数据集，所有注释均以印尼语原生撰写，并使用Krippendorff's alpha进行评估，证明了注释者之间的一致性。此外，还在多个LLMs上对数据集进行了基准测试，并评估了每个模型的输出质量。

Result: IndoPref数据集成功展示了其在评估LLM生成文本自然性和质量方面的有效性。

Conclusion: IndoPref为评估印尼语LLM生成文本的质量提供了重要的资源。

Abstract: Over 200 million people speak Indonesian, yet the language remains
significantly underrepresented in preference-based research for large language
models (LLMs). Most existing multilingual datasets are derived from English
translations, often resulting in content that lacks cultural and linguistic
authenticity. To address this gap, we introduce IndoPref, the first fully
human-authored and multi-domain Indonesian preference dataset specifically
designed to evaluate the naturalness and quality of LLM-generated text. All
annotations are natively written in Indonesian and evaluated using
Krippendorff's alpha, demonstrating strong inter-annotator agreement.
Additionally, we benchmark the dataset across multiple LLMs and assess the
output quality of each model.

</details>


### [2] [Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles](https://arxiv.org/abs/2507.22168)
*Kimberly Le Truong,Riccardo Fogliato,Hoda Heidari,Zhiwei Steven Wu*

Main category: cs.CL

TL;DR: 本文研究了写作风格和提示格式对LLM性能评估的影响，并提出了一个增强现有基准测试的方法，以提高其在测量LLM性能时的外部有效性。


<details>
  <summary>Details</summary>
Motivation: 当前评估大型语言模型（LLMs）的基准测试往往缺乏足够的写作风格多样性，许多遵循标准化惯例。这些基准测试无法完全捕捉人类展示的丰富沟通模式。因此，可能LLMs在面对“非标准”输入时表现出脆弱的性能。

Method: 我们通过使用基于角色的LLM提示重写评估提示，这是一种低成本的方法来模拟多样的写作风格。

Result: 即使语义内容相同，写作风格和提示格式的变化显著影响了被评估LLM的性能估计。我们识别出一些独特的写作风格，它们在各种模型和任务中持续引发低或高性能，无论模型家族、大小和新旧。

Conclusion: 我们的工作提供了一种可扩展的方法来增强现有的基准测试，从而提高它们在测量LLM性能时对语言变化的外部有效性。

Abstract: Current benchmarks for evaluating Large Language Models (LLMs) often do not
exhibit enough writing style diversity, with many adhering primarily to
standardized conventions. Such benchmarks do not fully capture the rich variety
of communication patterns exhibited by humans. Thus, it is possible that LLMs,
which are optimized on these benchmarks, may demonstrate brittle performance
when faced with "non-standard" input. In this work, we test this hypothesis by
rewriting evaluation prompts using persona-based LLM prompting, a low-cost
method to emulate diverse writing styles. Our results show that, even with
identical semantic content, variations in writing style and prompt formatting
significantly impact the estimated performance of the LLM under evaluation.
Notably, we identify distinct writing styles that consistently trigger either
low or high performance across a range of models and tasks, irrespective of
model family, size, and recency. Our work offers a scalable approach to augment
existing benchmarks, improving the external validity of the assessments they
provide for measuring LLM performance across linguistic variations.

</details>


### [3] [A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models](https://arxiv.org/abs/2507.22187)
*Adam M. Morgan,Adeen Flinker*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的自动化管道，用于估计动词框架频率（VFFs），该方法比现有工具更高效、可扩展，并生成了一个新的VFF数据库。


<details>
  <summary>Details</summary>
Motivation: 现有的计算VFFs的工具在规模、准确性和可访问性方面存在局限，因此需要一种更高效、可扩展的方法来估计VFFs。

Method: 本文使用大型语言模型（LLMs）生成包含476个英语动词的句子语料库，并通过让LLM像专家语言学家一样分析句子的句法结构，构建了一个自动化管道来估计动词框架频率（VFFs）。

Result: 该自动化管道在多个评估数据集上优于两种广泛使用的句法解析器，同时比人工解析所需资源更少，能够实现快速、可扩展的VFF估计。此外，该方法生成了一个新的VFF数据库，具有更广泛的动词覆盖范围、更细粒度的句法区分以及对心理语言学中常见结构交替相对频率的明确估计。

Conclusion: 本文提出了一种自动化管道来估计动词框架频率（VFFs），该方法利用大型语言模型（LLMs）生成包含476个英语动词的句子语料库，并通过让LLM像专家语言学家一样分析句子的句法结构。该方法在多个评估数据集上优于两种广泛使用的句法解析器，且比人工解析所需资源更少，从而实现了快速、可扩展的VFF估计。此外，该方法易于定制和扩展，适用于新动词、句法框架甚至其他语言。

Abstract: We present an automated pipeline for estimating Verb Frame Frequencies
(VFFs), the frequency with which a verb appears in particular syntactic frames.
VFFs provide a powerful window into syntax in both human and machine language
systems, but existing tools for calculating them are limited in scale,
accuracy, or accessibility. We use large language models (LLMs) to generate a
corpus of sentences containing 476 English verbs. Next, by instructing an LLM
to behave like an expert linguist, we had it analyze the syntactic structure of
the sentences in this corpus. This pipeline outperforms two widely used
syntactic parsers across multiple evaluation datasets. Furthermore, it requires
far fewer resources than manual parsing (the gold-standard), thereby enabling
rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF
database with broader verb coverage, finer-grained syntactic distinctions, and
explicit estimates of the relative frequencies of structural alternates
commonly studied in psycholinguistics. The pipeline is easily customizable and
extensible to new verbs, syntactic frames, and even other languages. We present
this work as a proof of concept for automated frame frequency estimation, and
release all code and data to support future research.

</details>


### [4] [The role of media memorability in facilitating startups' access to venture capital funding](https://arxiv.org/abs/2507.22201)
*L. Toschi,S. Torrisi,A. Fronzetti Colladon*

Main category: cs.CL

TL;DR: 本文研究了媒体记忆度对初创公司吸引风险投资的影响，发现媒体记忆度对融资结果有显著影响，并建议初创公司通过更有针对性的报道来增强品牌记忆度。


<details>
  <summary>Details</summary>
Motivation: 先前的研究过于狭窄地关注一般的媒体曝光，限制了我们对媒体如何真正影响融资决策的理解。风险投资家作为知情决策者，会对媒体内容的更细微方面做出反应。

Method: 我们使用197家英国微技术和纳米技术领域的初创公司（1995年至2004年间获得资金）的数据，分析了媒体记忆度如何影响融资决策。

Result: 我们发现媒体记忆度显著影响投资结果。风险投资家依赖于诸如初创公司的独特性和在新闻语义网络中的连接性等详细线索。

Conclusion: 我们的研究结果表明，媒体记忆度对融资结果有显著影响，这为创业金融和媒体合法化研究做出了贡献。实践上，初创公司应超越频繁的媒体报道，通过更有针对性、有意义的报道来增强品牌记忆度，突出其独特性和在更广泛的行业对话中的相关性。

Abstract: Media reputation plays an important role in attracting venture capital
investment. However, prior research has focused too narrowly on general media
exposure, limiting our understanding of how media truly influences funding
decisions. As informed decision-makers, venture capitalists respond to more
nuanced aspects of media content. We introduce the concept of media
memorability - the media's ability to imprint a startup's name in the memory of
relevant investors. Using data from 197 UK startups in the micro and
nanotechnology sector (funded between 1995 and 2004), we show that media
memorability significantly influences investment outcomes. Our findings suggest
that venture capitalists rely on detailed cues such as a startup's
distinctiveness and connectivity within news semantic networks. This
contributes to research on entrepreneurial finance and media legitimation. In
practice, startups should go beyond frequent media mentions to strengthen brand
memorability through more targeted, meaningful coverage highlighting their
uniqueness and relevance within the broader industry conversation.

</details>


### [5] [How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?](https://arxiv.org/abs/2507.22209)
*Christian Clark,Byung-Doh Oh,William Schuler*

Main category: cs.CL

TL;DR: 研究指出，使用第一个标记近似上下文熵可能会导致低估和扭曲真实的词熵，因此在使用时需要谨慎。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常基于语言模型对词的第一个子词标记的概率分布来估计熵，这可能导致低估和扭曲真实的词熵。

Method: 研究通过生成蒙特卡洛（MC）估计的词熵来解决这个问题，允许词跨越可变数量的标记。

Result: 回归实验显示，第一个标记和MC词熵之间的结果存在差异，表明需要谨慎使用第一个标记的近似值。

Conclusion: 研究结果表明，使用第一个标记近似上下文熵可能会导致低估和扭曲真实的词熵，因此在使用时需要谨慎。

Abstract: Contextual entropy is a psycholinguistic measure capturing the anticipated
difficulty of processing a word just before it is encountered. Recent studies
have tested for entropy-related effects as a potential complement to well-known
effects from surprisal. For convenience, entropy is typically estimated based
on a language model's probability distribution over a word's first subword
token. However, this approximation results in underestimation and potential
distortion of true word entropy. To address this, we generate Monte Carlo (MC)
estimates of word entropy that allow words to span a variable number of tokens.
Regression experiments on reading times show divergent results between
first-token and MC word entropy, suggesting a need for caution in using
first-token approximations of contextual entropy.

</details>


### [6] [RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation](https://arxiv.org/abs/2507.22219)
*Dongyub Jude Lee,Zhenyi Ye,Pengcheng He*

Main category: cs.CL

TL;DR: 本文提出了一种名为RLfR的新框架，通过利用外部教师模型的反馈来改进机器翻译，显著提高了语义和实体保留的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法如DPO依赖于大量精心策划的三元组数据集，且在超出其调优领域时往往难以泛化。

Method: RLfR框架通过利用外部教师模型（GPT-4o）的连续高质量反馈，将每个翻译步骤视为微教程：智能体生成假设，教师进行精炼，并根据与教师精炼的接近程度对智能体进行奖励。

Result: RLfR在FLORES-200基准测试中 consistently 超过MT-SFT和基于偏好的基线，显著提高了COMET（语义充分性）和M-ETA（实体保留）分数。

Conclusion: RLfR在FLORES-200基准测试中 consistently 超过MT-SFT和基于偏好的基线，显著提高了COMET（语义充分性）和M-ETA（实体保留）分数。

Abstract: Preference-learning methods for machine translation (MT)--such as Direct
Preference Optimization (DPO)--have achieved impressive gains but depend
heavily on large, carefully curated triplet datasets and often struggle to
generalize beyond their tuning domains. We propose Reinforcement Learning from
Teacher-Model Refinement (RLfR), a novel framework that removes reliance on
static triplets by leveraging continuous, high-quality feedback from an
external teacher model (GPT-4o). RLfR frames each translation step as a
micro-tutorial: the actor generates a hypothesis, the teacher refines it, and
the actor is rewarded based on how closely it aligns with the teacher's
refinement. Guided by two complementary signals--(i) negative edit distance,
promoting lexical and structural fidelity, and (ii) COMET score, ensuring
semantic adequacy--the actor progressively learns to emulate the teacher,
mirroring a human learning process through incremental, iterative improvement.
On the FLORES-200 benchmark (English to and from German, Spanish, Chinese,
Korean, and Japanese), RLfR consistently outperforms both MT-SFT and
preference-based baselines, significantly improving COMET (semantic adequacy)
and M-ETA (entity preservation) scores.

</details>


### [7] [Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs](https://arxiv.org/abs/2507.22286)
*Supantho Rakshit,Adele Goldberg*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型中的构造表示，发现它们能够学习到丰富的、意义驱动的、渐进的构造表示，并支持几何度量在LLMs中对基本构造主义原则的应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型（LLMs）内部表示是否反映了基于功能的渐进性构造理论。

Method: 本研究分析了Pythia-1.4B模型中英语双宾语和介词宾语构造的神经表示，使用了一个系统变化的人类评分偏好强度的数据集。通过宏观层面的几何分析，测量了构造表示之间的可分离性。

Result: 研究发现，构造表示之间的可分离性由梯度偏好强度系统调节，更典型的例子占据了LLMs激活空间中更独特的区域。

Conclusion: 研究结果表明，大型语言模型（LLMs）能够学习到丰富的、意义驱动的、渐进的构造表示，并支持几何度量在LLMs中对基本构造主义原则的应用。

Abstract: The usage-based constructionist (UCx) approach posits that language comprises
a network of learned form-meaning pairings (constructions) whose use is largely
determined by their meanings or functions, requiring them to be graded and
probabilistic. This study investigates whether the internal representations in
Large Language Models (LLMs) reflect the proposed function-infused gradience.
We analyze the neural representations of the English dative constructions
(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of
$5000$ sentence pairs systematically varied for human-rated preference
strength. A macro-level geometric analysis finds that the separability between
construction representations, as measured by Energy Distance or Jensen-Shannon
Divergence, is systematically modulated by gradient preference strength. More
prototypical exemplars of each construction occupy more distinct regions in the
activation space of LLMs. These results provide strong evidence that LLMs learn
rich, meaning-infused, graded representations of constructions and offer
support for geometric measures of basic constructionist principles in LLMs.

</details>


### [8] [Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations](https://arxiv.org/abs/2507.22289)
*Galo Castillo-López,Gaël de Chalendar,Nasredine Semmar*

Main category: cs.CL

TL;DR: 本文提出了一种混合方法，结合BERT和LLMs在零样本和少样本设置中进行意图识别和OOS检测，并在多对话语料库上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 传统TODS需要大量注释数据，而意图识别和OOS检测对于TODS提供可靠响应至关重要。

Method: 我们提出了一种混合方法，将BERT和LLMs结合在零样本和少样本设置中以识别意图和检测OOS话语。

Result: 我们的方法利用了LLMs的泛化能力和BERT在这些场景中的计算效率，并在多对话语料库上进行了评估。

Conclusion: 我们的方法在多对话语料库上进行了评估，并观察到从BERT输出共享信息到LLMs可以提高系统性能。

Abstract: Intent recognition is a fundamental component in task-oriented dialogue
systems (TODS). Determining user intents and detecting whether an intent is
Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However,
traditional TODS require large amount of annotated data. In this work we
propose a hybrid approach to combine BERT and LLMs in zero and few-shot
settings to recognize intents and detect OOS utterances. Our approach leverages
LLMs generalization power and BERT's computational efficiency in such
scenarios. We evaluate our method on multi-party conversation corpora and
observe that sharing information from BERT outputs to LLMs leads to system
performance improvement.

</details>


### [9] [A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers](https://arxiv.org/abs/2507.22337)
*Roxana Petcu,Samarth Bhargav,Maarten de Rijke,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: 本文研究了传统神经信息检索和基于LLM的模型中的否定现象，提出了一种基于逻辑的分类机制，并生成了两个基准数据集以评估和微调模型性能。


<details>
  <summary>Details</summary>
Motivation: 理解并解决复杂的推理任务对于满足用户的信息需求至关重要。尽管密集神经模型学习上下文嵌入，但在包含否定的查询中仍然表现不佳。

Method: 本文引入了一个基于哲学、语言学和逻辑定义的否定分类法，生成了两个基准数据集，并提出了一种基于逻辑的分类机制。

Result: 本文的分类法在否定类型上产生了平衡的数据分布，为更快地收敛到NevIR数据集提供了更好的训练设置。此外，提出的分类方案揭示了现有数据集中否定类型的覆盖情况，提供了对细调模型在否定上的泛化可能影响因素的见解。

Conclusion: 本文提出了一个基于逻辑的分类机制，可以分析检索模型在现有数据集上的表现，并提供了对细调模型在否定上的泛化可能影响因素的见解。

Abstract: Understanding and solving complex reasoning tasks is vital for addressing the
information needs of a user. Although dense neural models learn contextualised
embeddings, they still underperform on queries containing negation. To
understand this phenomenon, we study negation in both traditional neural
information retrieval and LLM-based models. We (1) introduce a taxonomy of
negation that derives from philosophical, linguistic, and logical definitions;
(2) generate two benchmark datasets that can be used to evaluate the
performance of neural information retrieval models and to fine-tune models for
a more robust performance on negation; and (3) propose a logic-based
classification mechanism that can be used to analyze the performance of
retrieval models on existing datasets. Our taxonomy produces a balanced data
distribution over negation types, providing a better training setup that leads
to faster convergence on the NevIR dataset. Moreover, we propose a
classification schema that reveals the coverage of negation types in existing
datasets, offering insights into the factors that might affect the
generalization of fine-tuned models on negation.

</details>


### [10] [Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors](https://arxiv.org/abs/2507.22367)
*Jia Li,Yichao He,Jiacheng Xu,Tianhao Luo,Zhenzhen Hu,Richang Hong,Meng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Traits Run Deep的新的人格评估框架，通过心理学提示和文本中心特质融合网络来提升人格评估的准确性。实验结果表明，该方法在AVI Challenge 2025中排名第一。


<details>
  <summary>Details</summary>
Motivation: 准确可靠的人格评估在许多领域中起着至关重要的作用，如情感智力、心理健康诊断和个人化教育。传统表面特征难以建模人格语义，跨模态理解似乎不可能实现。

Method: 提出了一种名为Traits Run Deep的新的人格评估框架。它使用心理学提示来激发高层次的人格相关语义表示，并设计了一个文本中心特质融合网络，将丰富的文本语义与来自其他模态的异步信号对齐和集成。该融合模块包括一个块级投影器、一个跨模态连接器和一个文本特征增强器，以及一个集成回归头以在数据稀缺的情况下提高泛化能力。

Result: 实验结果表明，所提出的组件有效，即均方误差（MSE）减少了约45%。最终评估在AVI Challenge 2025的测试集上确认了我们方法的优势，在人格评估赛道中排名第一。

Conclusion: 实验结果表明，所提出的组件有效，即均方误差（MSE）减少了约45%。最终评估在AVI Challenge 2025的测试集上确认了我们方法的优势，在人格评估赛道中排名第一。

Abstract: Accurate and reliable personality assessment plays a vital role in many
fields, such as emotional intelligence, mental health diagnostics, and
personalized education. Unlike fleeting emotions, personality traits are
stable, often subconsciously leaked through language, facial expressions, and
body behaviors, with asynchronous patterns across modalities. It was hard to
model personality semantics with traditional superficial features and seemed
impossible to achieve effective cross-modal understanding. To address these
challenges, we propose a novel personality assessment framework called
\textit{\textbf{Traits Run Deep}}. It employs
\textit{\textbf{psychology-informed prompts}} to elicit high-level
personality-relevant semantic representations. Besides, it devises a
\textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text
semantics to align and integrate asynchronous signals from other modalities. To
be specific, such fusion module includes a Chunk-Wise Projector to decrease
dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for
effective modality fusion and an ensemble regression head to improve
generalization in data-scarce situations. To our knowledge, we are the first to
apply personality-specific prompts to guide large language models (LLMs) in
extracting personality-aware semantics for improved representation quality.
Furthermore, extracting and fusing audio-visual apparent behavior features
further improves the accuracy. Experimental results on the AVI validation set
have demonstrated the effectiveness of the proposed components, i.e.,
approximately a 45\% reduction in mean squared error (MSE). Final evaluations
on the test set of the AVI Challenge 2025 confirm our method's superiority,
ranking first in the Personality Assessment track. The source code will be made
available at https://github.com/MSA-LMC/TraitsRunDeep.

</details>


### [11] [PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs](https://arxiv.org/abs/2507.22387)
*Homaira Huda Shomee,Suman Kalyan Maity,Sourav Medya*

Main category: cs.CL

TL;DR: 本文介绍了PATENTWRITER，这是一个用于评估大型语言模型在专利摘要生成中表现的基准框架，并展示了这些模型能够生成高质量的专利摘要。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过利用大型语言模型克服繁琐的专利申请过程，实现专利写作的范式转变。

Method: 本文提出了PATENTWRITER，这是第一个统一的基准框架，用于评估大型语言模型在专利摘要生成中的表现。我们使用六种领先的大型语言模型（包括GPT-4和LLaMA-3）在一致的设置下进行零样本、少量样本和思维链提示策略的测试，以生成专利摘要。

Result: 实验结果表明，现代大型语言模型可以生成高保真度和风格适当的专利摘要，常常超过领域特定的基线。

Conclusion: 实验结果表明，现代大型语言模型可以生成高保真度和风格适当的专利摘要，常常超过领域特定的基线。我们的代码和数据集已开源，以支持可重复性和未来研究。

Abstract: Large language models (LLMs) have emerged as transformative approaches in
several important fields. This paper aims for a paradigm shift for patent
writing by leveraging LLMs to overcome the tedious patent-filing process. In
this work, we present PATENTWRITER, the first unified benchmarking framework
for evaluating LLMs in patent abstract generation. Given the first claim of a
patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a
consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting
strategies to generate the abstract of the patent. Our benchmark PATENTWRITER
goes beyond surface-level evaluation: we systematically assess the output
quality using a comprehensive suite of metrics -- standard NLP measures (e.g.,
BLEU, ROUGE, BERTScore), robustness under three types of input perturbations,
and applicability in two downstream patent classification and retrieval tasks.
We also conduct stylistic analysis to assess length, readability, and tone.
Experimental results show that modern LLMs can generate high-fidelity and
stylistically appropriate patent abstracts, often surpassing domain-specific
baselines. Our code and dataset are open-sourced to support reproducibility and
future research.

</details>


### [12] [Question Generation for Assessing Early Literacy Reading Comprehension](https://arxiv.org/abs/2507.22410)
*Xiaocheng Yang,Sumuk Shashidhar,Dilek Hakkani-Tur*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，用于生成针对K-2英语学习者的理解问题，并评估了各种语言模型的性能，该方法有潜力成为自主AI驱动英语教师的重要组成部分。


<details>
  <summary>Details</summary>
Motivation: 通过基于内容的互动评估阅读理解在阅读习得过程中起着重要作用。

Method: 提出了一种新的方法，用于生成针对K-2英语学习者的理解问题。

Result: 使用FairytaleQA数据集作为源材料，在此框架中评估了各种语言模型的性能。

Conclusion: 所提出的的方法有潜力成为自主AI驱动英语教师的重要组成部分。

Abstract: Assessment of reading comprehension through content-based interactions plays
an important role in the reading acquisition process. In this paper, we propose
a novel approach for generating comprehension questions geared to K-2 English
learners. Our method ensures complete coverage of the underlying material and
adaptation to the learner's specific proficiencies, and can generate a large
diversity of question types at various difficulty levels to ensure a thorough
evaluation. We evaluate the performance of various language models in this
framework using the FairytaleQA dataset as the source material. Eventually, the
proposed approach has the potential to become an important part of autonomous
AI-driven English instructors.

</details>


### [13] [NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models](https://arxiv.org/abs/2507.22411)
*Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准NeedleChain，用于更准确地评估LLM的长上下文理解能力，并引入了ROPE收缩策略来提高其性能。


<details>
  <summary>Details</summary>
Motivation: 现有的NIAH基准可能高估了LLM的真实长上下文能力，因此需要一个更准确的评估方法。

Method: 我们引入了NeedleChain基准，该基准完全由与查询相关的信息组成，并提出了ROPE收缩策略来提高LLM的长上下文理解能力。

Result: 我们的实验表明，即使是最先进的模型如GPT-4o在处理仅包含与查询相关的十个句子的上下文时也难以完整地整合。通过NeedleChain基准和ROPE收缩策略，我们发现了LLM在处理大上下文和完全理解它们之间的显著差异。

Conclusion: 我们的研究发现，NIAH基准可能高估了LLM的真实长上下文能力。为此，我们引入了一个新的基准NeedleChain，它完全由与查询相关的信息组成，要求LLM完全理解输入以正确回答。此外，我们提出了一种简单但有效的策略来提高LLM的LC理解能力：ROPE收缩。

Abstract: The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large
Language Models' (LLMs) ability to understand long contexts (LC). It evaluates
the capability to identify query-relevant context within extensive
query-irrelevant passages. Although this method serves as a widely accepted
standard for evaluating long-context understanding, our findings suggest it may
overestimate the true LC capability of LLMs. We demonstrate that even
state-of-the-art models such as GPT-4o struggle to intactly incorporate given
contexts made up of solely query-relevant ten sentences. In response, we
introduce a novel benchmark, \textbf{NeedleChain}, where the context consists
entirely of query-relevant information, requiring the LLM to fully grasp the
input to answer correctly. Our benchmark allows for flexible context length and
reasoning order, offering a more comprehensive analysis of LLM performance.
Additionally, we propose an extremely simple yet compelling strategy to improve
LC understanding capability of LLM: ROPE Contraction. Our experiments with
various advanced LLMs reveal a notable disparity between their ability to
process large contexts and their capacity to fully understand them. Source code
and datasets are available at https://github.com/hyeonseokk/NeedleChain

</details>


### [14] [AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini](https://arxiv.org/abs/2507.22445)
*Jill Walker Rettberg,Hermann Wigers*

Main category: cs.CL

TL;DR: AI-generated stories exhibit narrative homogenization, reflecting a bias towards stability and tradition, which has implications for cultural alignment and AI research.


<details>
  <summary>Details</summary>
Motivation: To investigate whether a language model trained on Anglo-American texts can generate culturally relevant stories for other nationalities.

Method: The study generated 11,800 stories using GPT-4o-mini, each based on a prompt asking for a story about a specific nationality.

Result: The stories displayed surface-level national symbols but followed a single narrative structure, focusing on nostalgia, reconciliation, and tradition while downplaying real-world conflicts and romance.

Conclusion: AI-generated narratives show structural homogeneity, which constitutes a distinct form of AI bias, emphasizing stability and tradition over change and growth.

Abstract: Can a language model trained largely on Anglo-American texts generate stories
that are culturally relevant to other nationalities? To find out, we generated
11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a
1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although
the stories do include surface-level national symbols and themes, they
overwhelmingly conform to a single narrative plot structure across countries: a
protagonist lives in or returns home to a small town and resolves a minor
conflict by reconnecting with tradition and organising community events.
Real-world conflicts are sanitised, romance is almost absent, and narrative
tension is downplayed in favour of nostalgia and reconciliation. The result is
a narrative homogenisation: an AI-generated synthetic imaginary that
prioritises stability above change and tradition above growth. We argue that
the structural homogeneity of AI-generated narratives constitutes a distinct
form of AI bias, a narrative standardisation that should be acknowledged
alongside the more familiar representational bias. These findings are relevant
to literary studies, narratology, critical AI studies, NLP research, and
efforts to improve the cultural alignment of generative AI.

</details>


### [15] [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance](https://arxiv.org/abs/2507.22448)
*Jingwei Zuo,Maksim Velikanov,Ilyas Chahed,Younes Belkada,Dhia Eddine Rhayem,Guillaume Kunsch,Hakim Hacid,Hamza Yous,Brahim Farhat,Ibrahim Khadraoui,Mugariya Farooq,Giulia Campesan,Ruxandra Cojocaru,Yasser Djilali,Shi Hu,Iheb Chaabane,Puneesh Khanna,Mohamed El Amine Seddik,Ngoc Dung Huynh,Phuc Le Khac,Leen AlQadi,Billel Mokeddem,Mohamed Chami,Abdalgader Abubaker,Mikhail Lubinets,Kacper Piskorski,Slim Frikha*

Main category: cs.CL

TL;DR: Falcon-H1是一个新的大型语言模型系列，采用混合架构设计，具有高性能和效率。它在多个任务中表现出色，并以宽松的开源许可证发布。


<details>
  <summary>Details</summary>
Motivation: Falcon-H1旨在通过混合架构设计，在各种使用场景中实现高性能和效率。

Method: Falcon-H1采用了并行混合方法，结合了基于Transformer的注意力与状态空间模型（SSMs），以优化高性能和效率。

Result: Falcon-H1模型在性能上达到了最先进的水平，并且在参数和训练效率方面表现出色。旗舰型号Falcon-H1-34B在参数和数据使用较少的情况下，可以与更大的模型相媲美。

Conclusion: Falcon-H1模型在多个方面表现出色，包括推理、数学、多语言任务、指令遵循和科学知识。所有模型都以宽松的开源许可证发布，强调了对可及性和影响力AI研究的承诺。

Abstract: In this report, we introduce Falcon-H1, a new series of large language models
(LLMs) featuring hybrid architecture designs optimized for both high
performance and efficiency across diverse use cases. Unlike earlier Falcon
models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a
parallel hybrid approach that combines Transformer-based attention with State
Space Models (SSMs), known for superior long-context memory and computational
efficiency. We systematically revisited model design, data strategy, and
training dynamics, challenging conventional practices in the field. Falcon-H1
is released in multiple configurations, including base and instruction-tuned
variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized
instruction-tuned models are also available, totaling over 30 checkpoints on
Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and
exceptional parameter and training efficiency. The flagship Falcon-H1-34B
matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,
and Llama3.3-70B, while using fewer parameters and less data. Smaller models
show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B
models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.
These models excel across reasoning, mathematics, multilingual tasks,
instruction following, and scientific knowledge. With support for up to 256K
context tokens and 18 languages, Falcon-H1 is suitable for a wide range of
applications. All models are released under a permissive open-source license,
underscoring our commitment to accessible and impactful AI research.

</details>


### [16] [What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models](https://arxiv.org/abs/2507.22457)
*Tian Yun,Chen Sun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文重新评估了LLMs作为“抽象推理者”的能力，发现通过微调输入编码可以显著提升性能，但这种提升不一定适用于其他数据集。


<details>
  <summary>Details</summary>
Motivation: 先前的研究认为LLMs不是“抽象推理者”，因为它们在零样本任务中表现不佳。本文旨在对这一观点进行更深入的分析和讨论。

Method: 本文重新评估了之前关于LLMs在零样本设置下表现不佳的实验，并探讨了通过微调输入编码部分参数能否提升性能。

Result: 实验结果显示，尽管LLMs在零样本设置中表现较差，但通过微调输入编码部分参数可以实现接近完美的性能。然而，这种微调并不一定能在不同数据集之间迁移。

Conclusion: 本文认为，大型语言模型（LLMs）是否是“抽象推理者”这一问题需要重新审视，并强调了这一问题的重要性。

Abstract: Recent work has argued that large language models (LLMs) are not "abstract
reasoners", citing their poor zero-shot performance on a variety of challenging
tasks as evidence. We revisit these experiments in order to add nuance to the
claim. First, we show that while LLMs indeed perform poorly in a zero-shot
setting, even tuning a small subset of parameters for input encoding can enable
near-perfect performance. However, we also show that this finetuning does not
necessarily transfer across datasets. We take this collection of empirical
results as an invitation to (re-)open the discussion of what it means to be an
"abstract reasoner", and why it matters whether LLMs fit the bill.

</details>


### [17] [IFEvalCode: Controlled Code Generation](https://arxiv.org/abs/2507.22462)
*Jian Yang,Wei Zhang,Shukai Liu,Linzheng Chai,Yingshui Tan,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou,Guanglin Niu,Zhoujun Li,Binyuan Hui,Junyang Lin*

Main category: cs.CL

TL;DR: 论文提出了一种改进Code LLMs指令遵循能力的方法，并创建了一个多语言基准测试集，用于评估代码生成的正确性和指令遵循性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的应用通常需要更严格的遵循详细要求，如编码风格、行数和结构约束，而不仅仅是正确性。

Method: 论文引入了前向和后向约束生成，以提高Code LLMs在受控代码生成中的指令遵循能力，确保输出更符合人类定义的指南。此外，作者提出了IFEvalCode，一个跨七种编程语言的多语言基准测试集，每个样本都包含中英文查询。

Result: 实验结果表明，闭源模型在可控代码生成方面表现优于开源模型，并揭示了模型生成正确代码与精确遵循指令的代码之间的显著差距。

Conclusion: 实验结果显示，闭源模型在可控代码生成方面优于开源模型，并突显了模型生成正确代码与精确遵循指令的代码之间的显著差距。

Abstract: Code large language models (Code LLMs) have made significant progress in code
generation by translating natural language descriptions into functional code;
however, real-world applications often demand stricter adherence to detailed
requirements such as coding style, line count, and structural constraints,
beyond mere correctness. To address this, the paper introduces forward and
backward constraints generation to improve the instruction-following
capabilities of Code LLMs in controlled code generation, ensuring outputs align
more closely with human-defined guidelines. The authors further present
IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven
programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and
C#), with each sample featuring both Chinese and English queries. Unlike
existing benchmarks, IFEvalCode decouples evaluation into two metrics:
correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced
assessment. Experiments on over 40 LLMs reveal that closed-source models
outperform open-source ones in controllable code generation and highlight a
significant gap between the models' ability to generate correct code versus
code that precisely follows instructions.

</details>


### [18] [SLM-SQL: An Exploration of Small Language Models for Text-to-SQL](https://arxiv.org/abs/2507.22478)
*Lei Sheng,Shuai-Shuai Xu*

Main category: cs.CL

TL;DR: 本文研究了小语言模型在Text-to-SQL任务中的表现，通过后训练技术和自我一致性推理方法提升了其性能。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型在Text-to-SQL应用中的潜力，因为它们在推理速度和边缘部署方面具有优势。

Method: 我们使用了监督微调和基于强化学习的后训练技术，并采用纠正性自我一致性方法进行推理。

Result: 在BIRD开发集上，五种评估模型平均提升了31.4分。0.5B模型达到了56.87%的执行准确率（EX），而1.5B模型达到了67.08%的EX。

Conclusion: 实验结果验证了我们方法的有效性和泛化能力，SLM-SQL在BIRD开发集上取得了显著的提升。

Abstract: Large language models (LLMs) have demonstrated strong performance in
translating natural language questions into SQL queries (Text-to-SQL). In
contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters
currently underperform on Text-to-SQL tasks due to their limited logical
reasoning capabilities. However, SLMs offer inherent advantages in inference
speed and suitability for edge deployment. To explore their potential in
Text-to-SQL applications, we leverage recent advancements in post-training
techniques. Specifically, we used the open-source SynSQL-2.5M dataset to
construct two derived datasets: SynSQL-Think-916K for SQL generation and
SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised
fine-tuning and reinforcement learning-based post-training to the SLM, followed
by inference using a corrective self-consistency approach. Experimental results
validate the effectiveness and generalizability of our method, SLM-SQL. On the
BIRD development set, the five evaluated models achieved an average improvement
of 31.4 points. Notably, the 0.5B model reached 56.87\% execution accuracy
(EX), while the 1.5B model achieved 67.08\% EX. We will release our dataset,
model, and code to github: https://github.com/CycloneBoy/slm_sql.

</details>


### [19] [CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records](https://arxiv.org/abs/2507.22533)
*Dongchen Li,Jitao Liang,Wei Li,Xiaoyu Wang,Longbing Cao,Kun Yu*

Main category: cs.CL

TL;DR: 本文提出了 CliCARE 框架，用于在临床指南中对齐大型语言模型，以改善癌症电子健康记录的决策支持。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂的、长期的癌症电子健康记录方面面临三个主要挑战：无法有效处理记录的长度和多语言性质；临床幻觉风险增加；以及不可靠的评估指标。

Method: CliCARE 框架通过将非结构化的纵向 EHR 转换为患者特定的时序知识图谱（TKGs），并将其与规范的指南知识图谱对齐，从而实现决策支持的接地。

Result: 在来自私人中国癌症数据集和公共英文 MIMIC-IV 数据集的大规模纵向数据上验证了 CliCARE 框架，结果表明它显著优于现有的强基线方法，包括领先的长上下文 LLM 和知识图谱增强的 RAG 方法。

Conclusion: CliCARE 是一种用于在临床指南中对齐大型语言模型的框架，能够显著优于现有的基线方法，并且其临床有效性得到了专家肿瘤学家评估的高度相关性支持。

Abstract: Large Language Models (LLMs) hold significant promise for improving clinical
decision support and reducing physician burnout by synthesizing complex,
longitudinal cancer Electronic Health Records (EHRs). However, their
implementation in this critical field faces three primary challenges: the
inability to effectively process the extensive length and multilingual nature
of patient records for accurate temporal analysis; a heightened risk of
clinical hallucination, as conventional grounding techniques such as
Retrieval-Augmented Generation (RAG) do not adequately incorporate
process-oriented clinical guidelines; and unreliable evaluation metrics that
hinder the validation of AI systems in oncology. To address these issues, we
propose CliCARE, a framework for Grounding Large Language Models in Clinical
Guidelines for Decision Support over Longitudinal Cancer Electronic Health
Records. The framework operates by transforming unstructured, longitudinal EHRs
into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range
dependencies, and then grounding the decision support process by aligning these
real-world patient trajectories with a normative guideline knowledge graph.
This approach provides oncologists with evidence-grounded decision support by
generating a high-fidelity clinical summary and an actionable recommendation.
We validated our framework using large-scale, longitudinal data from a private
Chinese cancer dataset and the public English MIMIC-IV dataset. In these
diverse settings, CliCARE significantly outperforms strong baselines, including
leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The
clinical validity of our results is supported by a robust evaluation protocol,
which demonstrates a high correlation with assessments made by expert
oncologists.

</details>


### [20] [A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support](https://arxiv.org/abs/2507.22542)
*Long S. T. Nguyen,Truong P. Hua,Thanh M. Nguyen,Toan Q. Pham,Nam K. Ngo,An X. Nguyen,Nghi D. M. Pham,Nghia H. Nguyen,Tho T. Quan*

Main category: cs.CL

TL;DR: 本文介绍了客户支持对话数据集（CSConDa），并提出了一个全面的评估框架，对11个轻量级开源越南语大型语言模型进行了评估，以帮助企业和研究者选择和改进模型。


<details>
  <summary>Details</summary>
Motivation: 由于领域特定评估有限，缺乏反映真实客户互动的基准数据集，企业难以选择适合支持应用的模型。

Method: 本文介绍了客户支持对话数据集（CSConDa），并提出了一个全面的评估框架，对11个轻量级开源越南语大型语言模型进行了评估。

Result: 本文展示了CSConDa数据集，并通过自动指标和句法分析对11个轻量级开源越南语大型语言模型进行了评估，揭示了模型的优势、劣势和语言模式。

Conclusion: 本文通过建立一个强大的基准和系统评估，使企业能够做出明智的模型选择，并推动越南语大型语言模型的研究。

Abstract: With the rapid growth of Artificial Intelligence, Large Language Models
(LLMs) have become essential for Question Answering (QA) systems, improving
efficiency and reducing human workload in customer service. The emergence of
Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a
practical choice for their accuracy, efficiency, and privacy benefits. However,
domain-specific evaluations remain limited, and the absence of benchmark
datasets reflecting real customer interactions makes it difficult for
enterprises to select suitable models for support applications. To address this
gap, we introduce the Customer Support Conversations Dataset (CSConDa), a
curated benchmark of over 9,000 QA pairs drawn from real interactions with
human advisors at a large Vietnamese software company. Covering diverse topics
such as pricing, product availability, and technical troubleshooting, CSConDa
provides a representative basis for evaluating ViLLMs in practical scenarios.
We further present a comprehensive evaluation framework, benchmarking 11
lightweight open-source ViLLMs on CSConDa with both automatic metrics and
syntactic analysis to reveal model strengths, weaknesses, and linguistic
patterns. This study offers insights into model behavior, explains performance
differences, and identifies key areas for improvement, supporting the
development of next-generation ViLLMs. By establishing a robust benchmark and
systematic evaluation, our work enables informed model selection for customer
service QA and advances research on Vietnamese LLMs. The dataset is publicly
available at
https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.

</details>


### [21] [ControlMed: Adding Reasoning Control to Medical Language Model](https://arxiv.org/abs/2507.22545)
*Sung-Min Lee,Siyoon Lee,Juyeon Kim,Kyungmin Roh*

Main category: cs.CL

TL;DR: ControlMed 是一种医疗语言模型，允许用户在推理过程中主动控制长度，从而提高计算效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有的推理 LLM 经常生成不必要的长推理过程，导致计算开销和响应延迟，这阻碍了它们在现实临床环境中的实际部署。

Method: ControlMed 通过三阶段管道进行训练：1) 在大规模合成医学指令数据集上进行预训练；2) 使用多长度推理数据和显式长度控制标记进行监督微调；3) 使用基于模型的奖励信号进行强化学习。

Result: 实验结果表明，我们的模型在各种英语和韩语医学基准测试中实现了与最先进模型相当或更好的性能。此外，用户可以通过按需控制推理长度来灵活平衡推理准确性和计算效率。

Conclusion: ControlMed 是一种实用且适应性强的解决方案，适用于临床问答和医疗信息分析。

Abstract: Reasoning Large Language Models (LLMs) with enhanced accuracy and
explainability are increasingly being adopted in the medical domain, as the
life-critical nature of clinical decision-making demands reliable support.
Despite these advancements, existing reasoning LLMs often generate
unnecessarily lengthy reasoning processes, leading to significant computational
overhead and response latency. These limitations hinder their practical
deployment in real-world clinical environments. To address these challenges, we
introduce \textbf{ControlMed}, a medical language model that enables users to
actively control the length of the reasoning process at inference time through
fine-grained control markers. ControlMed is trained through a three-stage
pipeline: 1) pre-training on a large-scale synthetic medical instruction
dataset covering both \textit{direct} and \textit{reasoning responses}; 2)
supervised fine-tuning with multi-length reasoning data and explicit
length-control markers; and 3) reinforcement learning with model-based reward
signals to enhance factual accuracy and response quality. Experimental results
on a variety of English and Korean medical benchmarks demonstrate that our
model achieves similar or better performance compared to state-of-the-art
models. Furthermore, users can flexibly balance reasoning accuracy and
computational efficiency by controlling the reasoning length as needed. These
findings demonstrate that ControlMed is a practical and adaptable solution for
clinical question answering and medical information analysis.

</details>


### [22] [Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs](https://arxiv.org/abs/2507.22564)
*Xikang Yang,Biyu Zhou,Xuehai Tang,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: 本文介绍了CognitiveAttack，这是一种利用多偏差交互来绕过大型语言模型安全机制的新框架。实验结果表明，该方法比现有方法更有效，揭示了当前防御机制的不足，并提出了一个结合认知科学和LLM安全的跨学科视角。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在各种任务中表现出色，但它们的安全机制仍然容易受到利用认知偏差的对抗性攻击。现有的监狱突破方法主要集中在提示工程或算法操纵上，而本文旨在探索多偏差交互在破坏LLM安全措施方面的潜力。

Method: 本文提出了CognitiveAttack，这是一个新颖的红队框架，系统地利用了个体和组合的认知偏差。通过集成监督微调和强化学习，CognitiveAttack生成嵌入优化偏差组合的提示，有效地绕过安全协议，同时保持高攻击成功率。

Result: 实验结果表明，在30种不同的LLMs中存在显著的漏洞，特别是在开源模型中。CognitiveAttack相比最先进的黑盒方法PAP（60.1% vs. 31.6%）实现了更高的攻击成功率，暴露了当前防御机制的关键局限性。

Conclusion: 本文揭示了多偏差交互作为强大但未被充分探索的攻击向量的重要性，并通过CognitiveAttack框架展示了其在绕过LLM安全机制方面的有效性。此外，本文提出了一个跨学科的观点，将认知科学与LLM安全相结合，为更稳健和符合人类价值观的AI系统铺平了道路。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet their safety mechanisms remain susceptible to
adversarial attacks that exploit cognitive biases -- systematic deviations from
rational judgment. Unlike prior jailbreaking approaches focused on prompt
engineering or algorithmic manipulation, this work highlights the overlooked
power of multi-bias interactions in undermining LLM safeguards. We propose
CognitiveAttack, a novel red-teaming framework that systematically leverages
both individual and combined cognitive biases. By integrating supervised
fine-tuning and reinforcement learning, CognitiveAttack generates prompts that
embed optimized bias combinations, effectively bypassing safety protocols while
maintaining high attack success rates. Experimental results reveal significant
vulnerabilities across 30 diverse LLMs, particularly in open-source models.
CognitiveAttack achieves a substantially higher attack success rate compared to
the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations
in current defense mechanisms. These findings highlight multi-bias interactions
as a powerful yet underexplored attack vector. This work introduces a novel
interdisciplinary perspective by bridging cognitive science and LLM safety,
paving the way for more robust and human-aligned AI systems.

</details>


### [23] [Unveiling the Influence of Amplifying Language-Specific Neurons](https://arxiv.org/abs/2507.22581)
*Inaya Rahmanisa,Lyzander Marciano Andrylie,Krisna Mahardika Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 该研究探讨了通过放大语言特定神经元对模型行为的影响，发现放大对低资源语言有益，但对跨语言迁移帮助有限。


<details>
  <summary>Details</summary>
Motivation: 探索语言特定神经元在放大中的作用，以及它们如何影响模型行为和跨语言任务的表现。

Method: 通过干预18种语言（包括低资源语言）来研究放大语言特定神经元的影响，并使用提出的语言转向移位（LSS）评估分数比较放大因子的有效性。

Result: 最佳放大因子能有效引导输出接近所有测试语言，但在下游任务中，自语言性能有所提高，而跨语言结果通常下降。

Conclusion: 语言特定神经元在多语言行为中的作用得到了强调，放大可以对低资源语言有益，但对跨语言迁移的益处有限。

Abstract: Language-specific neurons in LLMs that strongly correlate with individual
languages have been shown to influence model behavior by deactivating them.
However, their role in amplification remains underexplored. This work
investigates the effect of amplifying language-specific neurons through
interventions across 18 languages, including low-resource ones, using three
models primarily trained in different languages. We compare amplification
factors by their effectiveness in steering to the target language using a
proposed Language Steering Shift (LSS) evaluation score, then evaluate it on
downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge
(Include), and translation (FLORES). The optimal amplification factors
effectively steer output toward nearly all tested languages. Intervention using
this factor on downstream tasks improves self-language performance in some
cases but generally degrades cross-language results. These findings highlight
the effect of language-specific neurons in multilingual behavior, where
amplification can be beneficial especially for low-resource languages, but
provides limited advantage for cross-lingual transfer.

</details>


### [24] [BALSAM: A Platform for Benchmarking Arabic Large Language Models](https://arxiv.org/abs/2507.22603)
*Rawan Al-Matham,Kareem Darwish,Raghad Al-Rasheed,Waad Alshammari,Muneera Alhoshan,Amal Almazrua,Asma Al Wazrah,Mais Alheraki,Firoj Alam,Preslav Nakov,Norah Alzahrani,Eman alBilali,Nizar Habash,Abdelrahman El-Sheikh,Muhammad Elmallah,Haonan Li,Hamdy Mubarak,Mohamed Anwar,Zaid Alyafeai,Ahmed Abdelali,Nora Altwairesh,Maram Hasanain,Abdulmohsen Al Thubaity,Shady Shehata,Bashar Alhafni,Injy Hamed,Go Inoue,Khalid Elmadani,Ossama Obeid,Fatima Haouari,Tamer Elsayed,Emad Alghamdi,Khalid Almubarak,Saied Alshahrani,Ola Aljarrah,Safa Alajlan,Areej Alshaqarawi,Maryam Alshihri,Sultana Alghurabi,Atikah Alzeghayer,Afrah Altamimi,Abdullah Alfaifi,Abdulrahman AlOsaimy*

Main category: cs.CL

TL;DR: 本文介绍了 BALSAM，这是一个全面的阿拉伯语大语言模型基准，旨在解决数据稀缺、语言多样性和评估不足的问题，通过提供大量任务和透明的评估平台来推动阿拉伯语大语言模型的发展。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语大语言模型的表现落后于英语，原因是数据稀缺、语言多样性、形态复杂性等。现有的阿拉伯语基准质量不高，通常依赖静态公开数据，缺乏全面的任务覆盖或没有专门的平台和盲测集。

Method: 引入 BALSAM 基准，包括 78 个 NLP 任务，涵盖 14 个广泛类别，提供 52K 示例，分为 37K 测试集和 15K 开发集，并提供一个集中透明的盲评平台。

Result: BALSAM 基准提供了 78 个 NLP 任务，涵盖 14 个广泛类别，包含 52K 示例，分为 37K 测试集和 15K 开发集，并提供一个集中透明的盲评平台。

Conclusion: BALSAM 是一个全面的、由社区驱动的基准，旨在推动阿拉伯语大语言模型的发展和评估，它设定了标准并促进了协作研究以提高阿拉伯语大语言模型的能力。

Abstract: The impressive advancement of Large Language Models (LLMs) in English has not
been matched across all languages. In particular, LLM performance in Arabic
lags behind, due to data scarcity, linguistic diversity of Arabic and its
dialects, morphological complexity, etc. Progress is further hindered by the
quality of Arabic benchmarks, which typically rely on static, publicly
available data, lack comprehensive task coverage, or do not provide dedicated
platforms with blind test sets. This makes it challenging to measure actual
progress and to mitigate data contamination. Here, we aim to bridge these gaps.
In particular, we introduce BALSAM, a comprehensive, community-driven benchmark
aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP
tasks from 14 broad categories, with 52K examples divided into 37K test and 15K
development, and a centralized, transparent platform for blind evaluation. We
envision BALSAM as a unifying platform that sets standards and promotes
collaborative research to advance Arabic LLM capabilities.

</details>


### [25] [Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation](https://arxiv.org/abs/2507.22608)
*Daniil Gurgurov,Katharina Trinley,Yusser Al Ghussin,Tanja Baeumel,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型中的语言特异性神经元，发现它们在深层中聚集，并通过语言算术方法成功操控模型的语言行为，提升了多语言任务的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型表现出强大的多语言能力，但语言特异性处理的神经机制仍不清楚。本文旨在探索这些机制，并开发一种有效的方法来操控模型的语言行为。

Method: 本文使用Language Activation Probability Entropy (LAPE)方法分析了不同语言模型中的语言特异性神经元，并通过语言算术（系统性激活加法和乘法）来操控模型的行为。

Result: 研究发现语言特异性神经元在深层中聚集，非拉丁字母语言表现出更高的专业化程度。相关语言共享重叠神经元，反映了语言接近性的内部表示。通过语言算术方法，可以有效地使模型停用不需要的语言并激活所需的语言，优于简单的替换方法。

Conclusion: 本文通过分析大型语言模型中的语言特异性神经元，揭示了语言行为的神经机制，并展示了通过语言算术方法对模型进行干预的有效性。此外，研究还发现跨语言神经元操控可以提升下游任务性能，并揭示了在神经元逐渐去激活时的语言选择“回退”机制。

Abstract: Large language models (LLMs) exhibit strong multilingual abilities, yet the
neural mechanisms behind language-specific processing remain unclear. We
analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and
Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying
neurons that control language behavior. Using the Language Activation
Probability Entropy (LAPE) method, we show that these neurons cluster in deeper
layers, with non-Latin scripts showing greater specialization. Related
languages share overlapping neurons, reflecting internal representations of
linguistic proximity.
  Through language arithmetics, i.e. systematic activation addition and
multiplication, we steer models to deactivate unwanted languages and activate
desired ones, outperforming simpler replacement approaches. These interventions
effectively guide behavior across five multilingual tasks: language forcing,
translation, QA, comprehension, and NLI. Manipulation is more successful for
high-resource languages, while typological similarity improves effectiveness.
We also demonstrate that cross-lingual neuron steering enhances downstream
performance and reveal internal "fallback" mechanisms for language selection
when neurons are progressively deactivated. Our code is made publicly available
at https://github.com/d-gurgurov/Language-Neurons-Manipulation.

</details>


### [26] [Multilingual Political Views of Large Language Models: Identification and Steering](https://arxiv.org/abs/2507.22623)
*Daniil Gurgurov,Katharina Trinley,Ivan Vykopal,Josef van Genabith,Simon Ostermann,Roberto Zamparelli*

Main category: cs.CL

TL;DR: 本研究通过大规模分析现代开源指令调优大型语言模型的政治倾向，发现大型模型倾向于自由主义左翼立场，并展示了通过简单干预技术可以操控模型的意识形态立场。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅评估了少量模型和语言，留下了关于政治偏见在架构、规模和多语言设置中的普遍性问题。此外，很少有工作探讨这些偏见是否可以被主动控制。

Method: 我们通过大规模研究分析了现代开源指令调优大型语言模型的政治倾向。我们评估了七种模型，包括LLaMA-3.1、Qwen-3和Aya-Expanse，在14种语言中使用政治指南测试，每条陈述有11个语义等价的改写版本以确保稳健测量。此外，我们利用简单的中心质量激活干预技术测试了政治立场的可操控性。

Result: 我们的结果表明，更大的模型会一致地转向自由主义左翼立场，并在不同语言和模型家族中表现出显著差异。通过简单的中心质量激活干预技术，我们展示了可以可靠地将模型响应引导到其他意识形态立场。

Conclusion: 我们的研究揭示了大型模型在政治立场上倾向于自由主义左翼，且在不同语言和模型家族中存在显著差异。我们还展示了通过简单的中心质量激活干预技术可以可靠地改变模型的意识形态立场。

Abstract: Large language models (LLMs) are increasingly used in everyday tools and
applications, raising concerns about their potential influence on political
views. While prior research has shown that LLMs often exhibit measurable
political biases--frequently skewing toward liberal or progressive
positions--key gaps remain. Most existing studies evaluate only a narrow set of
models and languages, leaving open questions about the generalizability of
political biases across architectures, scales, and multilingual settings.
Moreover, few works examine whether these biases can be actively controlled.
  In this work, we address these gaps through a large-scale study of political
orientation in modern open-source instruction-tuned LLMs. We evaluate seven
models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using
the Political Compass Test with 11 semantically equivalent paraphrases per
statement to ensure robust measurement. Our results reveal that larger models
consistently shift toward libertarian-left positions, with significant
variations across languages and model families. To test the manipulability of
political stances, we utilize a simple center-of-mass activation intervention
technique and show that it reliably steers model responses toward alternative
ideological positions across multiple languages. Our code is publicly available
at https://github.com/d-gurgurov/Political-Ideologies-LLMs.

</details>


### [27] [Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment](https://arxiv.org/abs/2507.22676)
*Jia Li,Yang Wang,Wenhao Qian,Zhenzhen Hu,Richang Hong,Meng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态面试表现评估框架，通过整合视频、音频和文本三种模态以及五个关键评估维度，实现了全面和公正的评估。


<details>
  <summary>Details</summary>
Motivation: 为了确保全面和公平的评估，我们需要一种新的综合框架来探索面试表现的多个方面。

Method: 我们提出了一种新的综合框架，通过整合视频、音频和文本三种模态、每位候选人的六种回答以及五个关键评估维度来探索面试表现的'365'方面。该框架使用特定于模态的特征提取器来编码异构数据流，并通过共享压缩多层感知机进行融合。此外，我们还采用了两级集成学习策略以提高预测的鲁棒性。

Result: 我们的框架在多维平均MSE为0.1824的情况下，在AVI Challenge 2025中获得了第一名，证明了其有效性。

Conclusion: 我们的框架在AVI Challenge 2025中取得了第一名，证明了其在推进自动化和多模态面试表现评估方面的有效性和鲁棒性。

Abstract: Interview performance assessment is essential for determining candidates'
suitability for professional positions. To ensure holistic and fair
evaluations, we propose a novel and comprehensive framework that explores
``365'' aspects of interview performance by integrating \textit{three}
modalities (video, audio, and text), \textit{six} responses per candidate, and
\textit{five} key evaluation dimensions. The framework employs
modality-specific feature extractors to encode heterogeneous data streams and
subsequently fused via a Shared Compression Multilayer Perceptron. This module
compresses multimodal embeddings into a unified latent space, facilitating
efficient feature interaction. To enhance prediction robustness, we incorporate
a two-level ensemble learning strategy: (1) independent regression heads
predict scores for each response, and (2) predictions are aggregated across
responses using a mean-pooling mechanism to produce final scores for the five
target dimensions. By listening to the unspoken, our approach captures both
explicit and implicit cues from multimodal data, enabling comprehensive and
unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our
framework secured first place in the AVI Challenge 2025, demonstrating its
effectiveness and robustness in advancing automated and multimodal interview
performance assessment. The full implementation is available at
https://github.com/MSA-LMC/365Aspects.

</details>


### [28] [From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs](https://arxiv.org/abs/2507.22716)
*Jie He,Victor Gutierrez Basulto,Jeff Z. Pan*

Main category: cs.CL

TL;DR: This paper proposes TIRESRAG-R1, a novel framework for reinforcement learning-based retrieval-augmented generation (RAG) that improves reasoning and stability through a think-retrieve-reflect process and multi-dimensional rewards.


<details>
  <summary>Details</summary>
Motivation: Existing RAG reasoning models have three main failure patterns: information insufficiency, faulty reasoning, and answer-reasoning inconsistency. The paper aims to address these issues by improving the reasoning and stability of RAG methods.

Method: TIRESRAG-R1 uses a think-retrieve-reflect process and a multi-dimensional reward system, including sufficiency, reasoning quality, and reflection rewards, along with difficulty-aware reweighting and training sample filtering.

Result: Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks.

Conclusion: TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks.

Abstract: Reinforcement learning-based retrieval-augmented generation (RAG) methods
enhance the reasoning abilities of large language models (LLMs). However, most
rely only on final-answer rewards, overlooking intermediate reasoning quality.
This paper analyzes existing RAG reasoning models and identifies three main
failure patterns: (1) information insufficiency, meaning the model fails to
retrieve adequate support; (2) faulty reasoning, where logical or content-level
flaws appear despite sufficient information; and (3) answer-reasoning
inconsistency, where a valid reasoning chain leads to a mismatched final
answer. We propose TIRESRAG-R1, a novel framework using a
think-retrieve-reflect process and a multi-dimensional reward system to improve
reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to
encourage thorough retrieval; (2) a reasoning quality reward to assess the
rationality and accuracy of the reasoning chain; and (3) a reflection reward to
detect and revise errors. It also employs a difficulty-aware reweighting
strategy and training sample filtering to boost performance on complex tasks.
Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms
prior RAG methods and generalizes well to single-hop tasks. The code and data
are available at: https://github.com/probe2/TIRESRAG-R1.

</details>


### [29] [Investigating Hallucination in Conversations for Low Resource Languages](https://arxiv.org/abs/2507.22720)
*Amit Das,Md. Najib Hasan,Souvika Sarkar,Zheng Zhang,Fatemeh Jamshidi,Tathagata Bhattacharya,Nilanjana Raychawdhury,Dongji Feng,Vinija Jain,Aman Chadha*

Main category: cs.CL

TL;DR: 本研究分析了三种语言（印地语、波斯语和中文）的对话数据集，发现大型语言模型在中文中产生的幻觉较少，而在印地语和波斯语中产生更多幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决幻觉问题对于提高大型语言模型的可靠性和有效性至关重要。虽然许多研究集中在英语中的幻觉，但我们的研究扩展到三种语言的对话数据。

Method: 我们对三个语言（印地语、波斯语和中文）的对话数据集进行了全面分析，以检查GPT-3.5、GPT-4o、Llama-3.1、Gemma-2.0、DeepSeek-R1和Qwen-3中的事实和语言错误。

Result: 我们发现大型语言模型在中文中产生的幻觉响应非常少，但在印地语和波斯语中产生的幻觉显著更多。

Conclusion: 我们的研究发现，大型语言模型在中文中产生的幻觉响应非常少，但在印地语和波斯语中产生的幻觉显著更多。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating text that closely resemble human writing. However, they often
generate factually incorrect statements, a problem typically referred to as
'hallucination'. Addressing hallucination is crucial for enhancing the
reliability and effectiveness of LLMs. While much research has focused on
hallucinations in English, our study extends this investigation to
conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a
comprehensive analysis of a dataset to examine both factual and linguistic
errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,
DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated
responses in Mandarin but generate a significantly higher number of
hallucinations in Hindi and Farsi.

</details>


### [30] [Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning](https://arxiv.org/abs/2507.22729)
*Benedikt Roth,Stephan Rappensperger,Tianming Qiu,Hamza Imamović,Julian Wörmann,Hao Shen*

Main category: cs.CL

TL;DR: 研究探讨了如何通过提示工程和对比微调将大型语言模型适配为文本嵌入模型，并在基准测试中取得了最佳结果。


<details>
  <summary>Details</summary>
Motivation: 许多非生成性下游任务，如聚类、分类或检索，仍然依赖于准确且可控的句子或文档级嵌入。

Method: 探索了预训练的仅解码器LLM的几种适应策略：(i) 各种token嵌入聚合技术，(ii) 任务特定的提示工程，以及(iii) 通过对比微调进行文本级增强。

Result: 结合这些组件在英语聚类跟踪中取得了最先进的性能。分析注意力图进一步显示，微调将焦点从提示token转移到语义相关单词，表明更有效的意义压缩到最终隐藏状态。

Conclusion: 实验表明，通过结合提示工程和资源高效的对比微调，大型语言模型可以有效地适应为文本嵌入模型。

Abstract: Large Language Models (LLMs) have become a cornerstone in Natural Language
Processing (NLP), achieving impressive performance in text generation. Their
token-level representations capture rich, human-aligned semantics. However,
pooling these vectors into a text embedding discards crucial information.
Nevertheless, many non-generative downstream tasks, such as clustering,
classification, or retrieval, still depend on accurate and controllable
sentence- or document-level embeddings. We explore several adaptation
strategies for pre-trained, decoder-only LLMs: (i) various aggregation
techniques for token embeddings, (ii) task-specific prompt engineering, and
(iii) text-level augmentation via contrastive fine-tuning. Combining these
components yields state-of-the-art performance on the English clustering track
of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention
map further shows that fine-tuning shifts focus from prompt tokens to
semantically relevant words, indicating more effective compression of meaning
into the final hidden state. Our experiments demonstrate that LLMs can be
effectively adapted as text embedding models through a combination of prompt
engineering and resource-efficient contrastive fine-tuning on synthetically
generated positive pairs.

</details>


### [31] [Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index](https://arxiv.org/abs/2507.22744)
*Praveenkumar Katwe,Rakesh Chandra,Balabantaray Kali,Prasad Vittala*

Main category: cs.CL

TL;DR: 本文提出了一种基于EHI的奖励驱动微调框架，以减少摘要中的实体幻觉，无需人工编写的真实性注释，实现了可扩展的微调。


<details>
  <summary>Details</summary>
Motivation: 减少抽象摘要中的幻觉仍然是将语言模型部署到现实世界设置中的关键挑战。

Method: 我们引入了一个奖励驱动的微调框架，该框架明确优化实体幻觉指数（EHI），这是一种用于量化生成摘要中实体的存在、正确性和依据的指标。我们首先使用预训练的语言模型生成基线摘要，并通过自动实体提取和匹配计算EHI分数。然后，我们应用强化学习来微调模型参数，使用EHI作为奖励信号，使生成偏向于实体忠实的输出。

Result: 实验表明，在多个数据集上EHI有持续改进，定性分析显示实体级别的幻觉显著减少，而流畅性和信息量没有下降。

Conclusion: 我们的方法在多个数据集上一致提高了EHI，同时保持了流畅性和信息量。我们发布了可重复的Colab流程，促进了使用轻量级、幻觉感知指标如EHI进行幻觉意识模型微调的进一步研究。

Abstract: Reducing hallucinations in abstractive summarization remains a critical
challenge for deploying language models (LMs) in real-world settings. In this
work, we introduce a rewarddriven fine-tuning framework that explicitly
optimizes for Entity Hallucination Index (EHI), a metric designed to quantify
the presence, correctness, and grounding of named entities in generated
summaries. Given a corpus of meeting transcripts, we first generate baseline
summaries using a pre-trained LM and compute EHI scores via automatic entity
extraction and matching. We then apply reinforcement learning to fine-tune the
model parameters, using EHI as a reward signal to bias generation toward
entity-faithful outputs. Our approach does not rely on human-written factuality
annotations, enabling scalable fine-tuning. Experiments demonstrate consistent
improvements in EHI across datasets, with qualitative analysis revealing a
significant reduction in entity-level hallucinations without degradation in
fluency or informativeness. We release a reproducible Colab pipeline,
facilitating further research on hallucination-aware model fine-tuning using
lightweight, hallucintion metrics like EHI.

</details>


### [32] [CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset](https://arxiv.org/abs/2507.22752)
*Jindřich Libovický,Jindřich Helcl,Andrei Manea,Gianluca Vico*

Main category: cs.CL

TL;DR: 本文介绍了一个用于开放式区域问答的基准测试，涵盖了文本和视觉模态。我们提供了使用最先进的大型语言模型（LLMs）的强基线。我们的数据集包含由捷克、斯洛伐克和乌克兰的母语者手工整理的问题和答案，并附有英文翻译。它包括纯文本问题和需要视觉理解的问题。作为基线，我们通过提示评估最先进的LLMs，并补充人类对答案正确性的判断。我们的基线结果突显了当前LLMs在区域知识方面的显著差距。此外，除了基于LLM的评估外，自动化指标和人类判断之间的相关性很小。我们释放这个数据集作为资源，以(1)评估LLMs中的区域知识，(2)在具有挑战性的设置中研究跨语言生成一致性，以及(3)推动开放式问答的评估指标的发展。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs在区域知识方面存在显著差距。除了基于LLM的评估外，自动化指标和人类判断之间相关性很小。

Method: 我们引入了一个基准测试，用于开放式区域问答，涵盖文本和视觉模态。我们还提供了使用最先进的大型语言模型（LLMs）的强基线。我们的数据集包含由捷克、斯洛伐克和乌克兰的母语者手工整理的问题和答案，并附有英文翻译。它包括纯文本问题和需要视觉理解的问题。作为基线，我们通过提示评估最先进的LLMs，并补充人类对答案正确性的判断。

Result: 我们的基线结果突显了当前LLMs在区域知识方面的显著差距。此外，除了基于LLM的评估外，自动化指标和人类判断之间的相关性很小。

Conclusion: 我们释放这个数据集作为资源，以(1)评估LLMs中的区域知识，(2)在具有挑战性的设置中研究跨语言生成一致性，以及(3)推动开放式问答的评估指标的发展。

Abstract: We introduce a benchmark for open-ended regional question answering that
encompasses both textual and visual modalities. We also provide strong
baselines using state-of-the-art large language models (LLMs). Our dataset
consists of manually curated questions and answers grounded in Wikipedia,
created by native speakers from Czechia, Slovakia, and Ukraine, with
accompanying English translations. It includes both purely textual questions
and those requiring visual understanding. As a baseline, we evaluate
state-of-the-art LLMs through prompting and complement this with human
judgments of answer correctness. Using these human evaluations, we analyze the
reliability of existing automatic evaluation metrics. Our baseline results
highlight a significant gap in regional knowledge among current LLMs. Moreover,
apart from LLM-based evaluation, there is minimal correlation between automated
metrics and human judgment. We release this dataset as a resource to (1) assess
regional knowledge in LLMs, (2) study cross-lingual generation consistency in a
challenging setting, and (3) advance the development of evaluation metrics for
open-ended question answering.

</details>


### [33] [Opportunities and Challenges of LLMs in Education: An NLP Perspective](https://arxiv.org/abs/2507.22753)
*Sowmya Vajjala,Bashar Alhafni,Stefano Bannò,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在教育自然语言处理（NLP）中的影响，特别是在辅助和评估两个主要应用场景中，并讨论了四个维度（阅读、写作、口语和辅导）。同时，本文提出了由LLMs带来的新方向以及需要解决的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在教育中的作用日益受到关注，本文旨在探讨它们对教育自然语言处理（NLP）的影响，并提出新的方向和关键挑战。

Method: 本文 examines the impact of LLMs on educational NLP in the context of two main application scenarios: assistance and assessment, grounding them along the four dimensions -- reading, writing, speaking, and tutoring.

Result: 本文提出了由LLMs带来的新方向以及需要解决的关键挑战。

Conclusion: 本文认为，全面的概述对于对探索大型语言模型在开发未来的语言导向和NLP启用的教育应用感兴趣的NLP研究人员和从业者是有用的。

Abstract: Interest in the role of large language models (LLMs) in education is
increasing, considering the new opportunities they offer for teaching,
learning, and assessment. In this paper, we examine the impact of LLMs on
educational NLP in the context of two main application scenarios: {\em
assistance} and {\em assessment}, grounding them along the four dimensions --
reading, writing, speaking, and tutoring. We then present the new directions
enabled by LLMs, and the key challenges to address. We envision that this
holistic overview would be useful for NLP researchers and practitioners
interested in exploring the role of LLMs in developing language-focused and
NLP-enabled educational applications of the future.

</details>


### [34] [MASCA: LLM based-Multi Agents System for Credit Assessment](https://arxiv.org/abs/2507.22758)
*Gautam Jajoo,Pranjal A Chitale,Saksham Agarwal*

Main category: cs.CL

TL;DR: 本文介绍了MASCA，一个基于LLM的多智能体系统，用于改进信用评估。通过分层架构和对比学习优化决策，并从信号博弈理论的角度提供理论见解。实验结果显示MASCA在信用评分中表现优异。


<details>
  <summary>Details</summary>
Motivation: 信用评估传统上依赖于基于规则的方法和统计模型，但尚未得到充分探索。本文旨在利用LLM和多智能体系统来改进信用评估。

Method: MASCA是一个由LLM驱动的多智能体系统，采用分层架构，专门的LLM代理协作解决子任务，并整合对比学习进行风险和回报评估。此外，还从信号博弈理论的角度探讨了分层多智能体系统的结构和交互。

Result: 实验结果表明，MASCA优于基线方法，展示了分层LLM-based多智能体系统在金融应用中的有效性，特别是在信用评分方面。

Conclusion: MASCA在信用评分中表现出色，证明了基于分层LLM的多智能体系统在金融应用中的有效性。

Abstract: Recent advancements in financial problem-solving have leveraged LLMs and
agent-based systems, with a primary focus on trading and financial modeling.
However, credit assessment remains an underexplored challenge, traditionally
dependent on rule-based methods and statistical models. In this paper, we
introduce MASCA, an LLM-driven multi-agent system designed to enhance credit
evaluation by mirroring real-world decision-making processes. The framework
employs a layered architecture where specialized LLM-based agents
collaboratively tackle sub-tasks. Additionally, we integrate contrastive
learning for risk and reward assessment to optimize decision-making. We further
present a signaling game theory perspective on hierarchical multi-agent
systems, offering theoretical insights into their structure and interactions.
Our paper also includes a detailed bias analysis in credit assessment,
addressing fairness concerns. Experimental results demonstrate that MASCA
outperforms baseline approaches, highlighting the effectiveness of hierarchical
LLM-based multi-agent systems in financial applications, particularly in credit
scoring.

</details>


### [35] [DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph](https://arxiv.org/abs/2507.22811)
*Debayan Banerjee,Tilahun Abedissa Taffa,Ricardo Usbeck*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work we present an entity linker for DBLP's 2025 version of RDF-based
Knowledge Graph. Compared to the 2022 version, DBLP now considers publication
venues as a new entity type called dblp:Stream. In the earlier version of
DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce
entity linkings. In contrast, in this work, we develop a zero-shot entity
linker using LLMs using a novel method, where we re-rank candidate entities
based on the log-probabilities of the "yes" token output at the penultimate
layer of the LLM.

</details>


### [36] [Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization](https://arxiv.org/abs/2507.22829)
*Weijia Zhang,Songgaojun Deng,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: 本文提出了一种基于结构化表示的查询聚焦表格摘要方法，称为SPaGe，它在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 查询聚焦的表格摘要需要复杂的推理，但自然语言计划存在歧义且缺乏结构，限制了其转换为可执行程序的能力，尤其是在多表任务中。

Method: 我们引入了一种新的结构化计划TaSoF，并提出了一种框架SPaGe，该框架通过三个阶段进行形式化推理：1) 结构化规划，2) 基于图的执行，3) 摘要生成。

Result: 实验表明，SPaGe在三个公共基准测试中表现优于先前模型，证明了结构化表示的优势。

Conclusion: SPaGe在单表和多表设置中都优于之前模型，展示了结构化表示在稳健和可扩展总结中的优势。

Abstract: Query-focused table summarization requires complex reasoning, often
approached through step-by-step natural language (NL) plans. However, NL plans
are inherently ambiguous and lack structure, limiting their conversion into
executable programs like SQL and hindering scalability, especially for
multi-table tasks. To address this, we propose a paradigm shift to structured
representations. We introduce a new structured plan, TaSoF, inspired by
formalism in traditional multi-agent systems, and a framework, SPaGe, that
formalizes the reasoning process in three phases: 1) Structured Planning to
generate TaSoF from a query, 2) Graph-based Execution to convert plan steps
into SQL and model dependencies via a directed cyclic graph for parallel
execution, and 3) Summary Generation to produce query-focused summaries. Our
method explicitly captures complex dependencies and improves reliability.
Experiments on three public benchmarks show that SPaGe consistently outperforms
prior models in both single- and multi-table settings, demonstrating the
advantages of structured representations for robust and scalable summarization.

</details>


### [37] [Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning](https://arxiv.org/abs/2507.22887)
*Kwesi Cobbina,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中基于上下文学习的位置偏差（DPP），发现演示的位置对模型的预测和准确性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究ICL中未探索的位置偏差，即演示、系统提示和用户消息在LLM输入中的位置变化对预测和准确性的影响。

Method: 设计了一个系统评估管道，研究了不同任务中的这种位置偏差，并引入了两个指标来量化准确性变化和预测变化。

Result: 实验结果表明，将演示放在提示的开头可以获得最高的准确性和稳定性，而将其放在用户消息的末尾会导致超过30%的预测翻转。

Conclusion: 研究发现，DPP偏差显著影响了LLM的准确性和预测，将演示放在提示的开头可以得到最稳定和准确的输出。

Abstract: In-context learning (ICL) is a critical emerging capability of large language
models (LLMs), enabling few-shot learning during inference by including a few
demonstrations (demos) in the prompt. However, it has been found that ICL's
performance can be sensitive to the choices of demos and their order. This
paper investigates an unexplored new positional bias of ICL for the first time:
we observe that the predictions and accuracy can drift drastically when the
positions of demos, the system prompt, and the user message in LLM input are
varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We
design a systematic evaluation pipeline to study this type of positional bias
across classification, question answering, summarization, and reasoning tasks.
We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify
net gains and output volatility induced by changes in the demos' position.
Extensive experiments on ten LLMs from four open-source model families (QWEN,
LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their
accuracy and predictions: placing demos at the start of the prompt yields the
most stable and accurate outputs with gains of up to +6 points. In contrast,
placing demos at the end of the user message flips over 30\% of predictions
without improving correctness on QA tasks. Smaller models are most affected by
this sensitivity, though even large models remain marginally affected on
complex tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [38] [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://arxiv.org/abs/2507.22080)
*Qiushi Sun,Jinyang Gong,Lei Li,Qipeng Guo,Fei Yuan*

Main category: cs.SE

TL;DR: 本文提出了CodeEvo框架，通过两个LLM代理之间的迭代交互来合成高质量的代码数据，实验结果表明该方法在代码生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 获取高质量的指令-代码对对于训练用于代码生成的大型语言模型至关重要。手动整理的数据成本高昂且规模有限，这促使了以代码为中心的合成方法的发展。然而，目前的方法要么专注于增强现有代码，要么依赖于预定义的启发式方法，两者都缺乏严格的数据验证，导致合成数据缺乏依据、重复或过于简单。

Method: 我们提出了CodeEvo框架，该框架通过两个LLM代理之间的迭代交互来合成代码数据：一个Coder根据给定的指令生成候选代码和测试用例，一个Reviewer通过生成新的指令和反馈来指导合成过程。我们还引入了一种混合反馈机制，结合编译器确定性与代理的生成灵活性，实现了合成过程中的自动质量控制。

Result: 实验结果表明，使用CodeEvo数据微调的模型在各种难度的代码生成基准测试中显著优于现有的基线模型。

Conclusion: 实验结果表明，使用CodeEvo数据微调的模型在各种难度的代码生成基准测试中显著优于现有的基线模型。深入分析进一步从多个角度提供了有效的代码中心数据合成的见解。

Abstract: Acquiring high-quality instruction-code pairs is essential for training Large
Language Models (LLMs) for code generation. Manually curated data is expensive
and inherently limited in scale, motivating the development of code-centric
synthesis methods. Yet, current approaches either focus on augmenting existing
code or rely on predefined heuristics, both lacking rigorous data validation,
which results in synthetic data that is ungrounded, repetitive, or overly
simplistic. Inspired by collaborative programming practices, we propose
CodeEvo, a framework that synthesizes code data through iterative interactions
between two LLM agents: a Coder, which generates candidate code and test cases
based on given instructions, and a Reviewer, which guides the synthesis process
by producing new instructions and feedback. We further introduce a hybrid
feedback mechanism that combines compiler determinism with the generative
flexibility of agents, enabling automatic quality control throughout synthesis.
Extensive experiments demonstrate that models fine-tuned on CodeEvo data
significantly outperform established baselines across code generation
benchmarks with various difficulties. In-depth analyses further provide
insights from multiple perspectives into effective code-centric data synthesis.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs](https://arxiv.org/abs/2507.22074)
*Yangshu Yuan,Heng Chen,Xinyi Jiang,Christian Ng,Kexin Qiu*

Main category: cs.LG

TL;DR: 本文提出了CIMR框架，以解决大型语言模型和视觉语言模型在处理复杂多模态指令时的不足，并在实验中取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型和视觉语言模型在处理需要逻辑推理、动态反馈整合和迭代自我修正的复杂多步骤多模态指令时存在困难。

Method: 提出了一种名为CIMR的框架，该框架引入了上下文感知的迭代推理和自我修正模块，并通过动态融合模块深度整合文本、视觉和上下文特征。

Result: CIMR在新提出的多模态动作规划（MAP）数据集上实现了91.5%的准确率，优于最先进的模型如GPT-4V、LLaVA-1.5、MiniGPT-4和InstructBLIP。

Conclusion: CIMR在复杂任务中展示了其迭代推理和自我修正能力的有效性。

Abstract: The rapid advancement of Large Language Models (LLMs) and Large
Vision-Language Models (LVLMs) has enhanced our ability to process and generate
human language and visual information. However, these models often struggle
with complex, multi-step multi-modal instructions that require logical
reasoning, dynamic feedback integration, and iterative self-correction. To
address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a
novel framework that introduces a context-aware iterative reasoning and
self-correction module. CIMR operates in two stages: initial reasoning and
response generation, followed by iterative refinement using parsed multi-modal
feedback. A dynamic fusion module deeply integrates textual, visual, and
contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual
Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced
Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy,
outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5
(78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the
efficacy of its iterative reasoning and self-correction capabilities in complex
tasks.

</details>


### [40] [Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law](https://arxiv.org/abs/2507.22543)
*Yanjin He,Qingkai Zeng,Meng Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Zipf定律的系统方法来确定最佳词汇量，实验证明该方法能有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前词汇量的选择通常依赖于启发式方法或数据集特定的选择，而本文旨在提供一种更系统和理论基础的方法来确定最佳词汇量。

Method: 本文通过分析词频分布，利用Zipf定律来确定最佳词汇量，并验证了模型性能与词频分布遵循幂律行为的程度之间的相关性。

Result: 实验结果表明，当词频分布紧密遵循Zipf定律时，模型在NLP、基因组学和化学等多个领域均表现出最佳性能。

Conclusion: 本文提出了一种基于Zipf定律分析词频分布的系统方法，以确定最佳词汇量。实验表明，当词频分布紧密遵循Zipf定律时，模型性能达到峰值，这表明Zipf对齐是一个稳健且可推广的词汇量选择标准。

Abstract: Tokenization is a fundamental step in natural language processing (NLP) and
other sequence modeling domains, where the choice of vocabulary size
significantly impacts model performance. Despite its importance, selecting an
optimal vocabulary size remains underexplored, typically relying on heuristics
or dataset-specific choices. In this work, we propose a principled method for
determining the vocabulary size by analyzing token frequency distributions
through Zipf's law. We show that downstream task performance correlates with
how closely token distributions follow power-law behavior, and that aligning
with Zipfian scaling improves both model efficiency and effectiveness.
Extensive experiments across NLP, genomics, and chemistry demonstrate that
models consistently achieve peak performance when the token distribution
closely adheres to Zipf's law, establishing Zipfian alignment as a robust and
generalizable criterion for vocabulary size selection.

</details>


### [41] [Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning](https://arxiv.org/abs/2507.22565)
*Afshin Khadangi,Amir Sartipi,Igor Tchappi,Ramin Bahmani,Gilbert Fridgen*

Main category: cs.LG

TL;DR: RLDP通过深度强化学习优化差分隐私设置，在保持隐私保护的同时提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私方法在隐私和模型效用之间存在权衡，且控制参数是硬编码的，无法适应优化过程的变化。

Method: RLDP将差分隐私优化视为一个闭环控制问题，利用深度强化学习（RL）来动态调整梯度裁剪阈值和噪声幅度。

Result: RLDP在多个模型上实现了1.3-30.5%的困惑度降低和平均5.6%的下游效用提升，同时减少了梯度更新预算的使用。

Conclusion: RLDP在多个模型上展示了显著的性能提升，同时保持了隐私保护，并且比基线方法更高效。

Abstract: The tension between data privacy and model utility has become the defining
bottleneck for the practical deployment of large language models (LLMs) trained
on sensitive corpora including healthcare. Differentially private stochastic
gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a
pronounced cost: gradients are forcibly clipped and perturbed with noise,
degrading sample efficiency and final accuracy. Numerous variants have been
proposed to soften this trade-off, but they all share a handicap: their control
knobs are hard-coded, global, and oblivious to the evolving optimization
landscape. Consequently, practitioners are forced either to over-spend privacy
budget in pursuit of utility, or to accept mediocre models in order to stay
within privacy constraints. We present RLDP, the first framework to cast DP
optimization itself as a closed-loop control problem amenable to modern deep
reinforcement learning (RL). RLDP continuously senses rich statistics of the
learning dynamics and acts by selecting fine-grained per parameter
gradient-clipping thresholds as well as the magnitude of injected Gaussian
noise. A soft actor-critic (SAC) hyper-policy is trained online during language
model fine-tuning; it learns, from scratch, how to allocate the privacy budget
where it matters and when it matters. Across more than 1,600 ablation
experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers
perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream
utility gain. RLDP reaches each baseline's final utility after only 13-43% of
the gradient-update budget (mean speed-up 71%), all while honoring the same
($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility
to membership-inference and canary-extraction attacks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [42] [GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for Multiresolution Power Outage Analysis](https://arxiv.org/abs/2507.22878)
*Ethan Frakes,Yinghui Wu,Roger H. French,Mengjie Li*

Main category: cs.IR

TL;DR: This paper proposes GeoOutageKG, a multimodal knowledge graph that integrates diverse data sources, including nighttime light satellite image data, high-resolution spatiotemporal power outage maps, and county-level timeseries outage reports in the U.S., to improve power outage detection, analysis, and predictive reasoning.


<details>
  <summary>Details</summary>
Motivation: Detecting, analyzing, and predicting power outages is crucial for grid risk assessment and disaster mitigation. Existing outage data are typically reported at the county level, limiting their spatial resolution and making it difficult to capture localized patterns. However, it offers excellent temporal granularity. In contrast, nighttime light satellite image data provides significantly higher spatial resolution and enables a more comprehensive spatial depiction of outages, enhancing the accuracy of assessing the geographic extent and severity of power loss after disaster events.

Method: We propose GeoOutageKG, a multimodal knowledge graph that integrates diverse data sources, including nighttime light satellite image data, high-resolution spatiotemporal power outage maps, and county-level timeseries outage reports in the U.S. We describe our method for constructing GeoOutageKG by aligning source data with a developed ontology, GeoOutageOnto.

Result: GeoOutageKG includes over 10.6 million individual outage records spanning from 2014 to 2024, 300,000 NTL images spanning from 2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and reusable semantic resource that enables robust multimodal data integration.

Conclusion: GeoOutageKG is a novel, modular and reusable semantic resource that enables robust multimodal data integration.

Abstract: Detecting, analyzing, and predicting power outages is crucial for grid risk
assessment and disaster mitigation. Numerous outages occur each year,
exacerbated by extreme weather events such as hurricanes. Existing outage data
are typically reported at the county level, limiting their spatial resolution
and making it difficult to capture localized patterns. However, it offers
excellent temporal granularity. In contrast, nighttime light satellite image
data provides significantly higher spatial resolution and enables a more
comprehensive spatial depiction of outages, enhancing the accuracy of assessing
the geographic extent and severity of power loss after disaster events.
However, these satellite data are only available on a daily basis. Integrating
spatiotemporal visual and time-series data sources into a unified knowledge
representation can substantially improve power outage detection, analysis, and
predictive reasoning. In this paper, we propose GeoOutageKG, a multimodal
knowledge graph that integrates diverse data sources, including nighttime light
satellite image data, high-resolution spatiotemporal power outage maps, and
county-level timeseries outage reports in the U.S. We describe our method for
constructing GeoOutageKG by aligning source data with a developed ontology,
GeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual
outage records spanning from 2014 to 2024, 300,000 NTL images spanning from
2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and
reusable semantic resource that enables robust multimodal data integration. We
demonstrate its use through multiresolution analysis of geospatiotemporal power
outages.

</details>


### [43] [RecGPT Technical Report](https://arxiv.org/abs/2507.22879)
*Chao Yi,Dian Chen,Gaoyang Guo,Jiakai Tang,Jian Wu,Jing Yu,Sunhao Dai,Wen Chen,Wenjun Yang,Yuning Jiang,Zhujin Gao,Bo Zheng,Chi Li,Dimin Wang,Dixuan Wang,Fan Li,Fan Zhang,Haibin Chen,Haozhuang Liu,Jialin Zhu,Jiamang Wang,Jiawei Wu,Jin Cui,Ju Huang,Kai Zhang,Kan Liu,Lang Tian,Liang Rao,Longbin Li,Lulu Zhao,Mao Zhang,Na He,Peiyang Wang,Qiqi Huang,Tao Luo,Wenbo Su,Xiaoxiao He,Xin Tong,Xu Chen,Xunke Xi,Yang Li,Yaxuan Wu,Yeqiu Yang,Yi Hu,Yinnan Song,Yuchen Li,Yujie Luo,Yujin Yuan,Yuliang Yan,Zhengyang Wang,Zhibo Xiao,Zhixin Ma,Zile Zhou*

Main category: cs.IR

TL;DR: This paper proposes RecGPT, a next-generation recommender system framework that centers user intent and integrates large language models to improve recommendation quality and sustainability.


<details>
  <summary>Details</summary>
Motivation: Current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, which leads to overfitting to narrow historical preferences and fails to capture users' evolving and latent interests, reinforcing filter bubbles and long-tail phenomena.

Method: RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. It integrates large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, and incorporates a multi-stage training paradigm.

Result: RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions.

Conclusion: LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.

Abstract: Recommender systems are among the most impactful applications of artificial
intelligence, serving as critical infrastructure connecting users, merchants,
and platforms. However, most current industrial systems remain heavily reliant
on historical co-occurrence patterns and log-fitting objectives, i.e.,
optimizing for past user interactions without explicitly modeling user intent.
This log-fitting approach often leads to overfitting to narrow historical
preferences, failing to capture users' evolving and latent interests. As a
result, it reinforces filter bubbles and long-tail phenomena, ultimately
harming user experience and threatening the sustainability of the whole
recommendation ecosystem.
  To address these challenges, we rethink the overall design paradigm of
recommender systems and propose RecGPT, a next-generation framework that places
user intent at the center of the recommendation pipeline. By integrating large
language models (LLMs) into key stages of user interest mining, item retrieval,
and explanation generation, RecGPT transforms log-fitting recommendation into
an intent-centric process. To effectively align general-purpose LLMs to the
above domain-specific recommendation tasks at scale, RecGPT incorporates a
multi-stage training paradigm, which integrates reasoning-enhanced
pre-alignment and self-training evolution, guided by a Human-LLM cooperative
judge system. Currently, RecGPT has been fully deployed on the Taobao App.
Online experiments demonstrate that RecGPT achieves consistent performance
gains across stakeholders: users benefit from increased content diversity and
satisfaction, merchants and the platform gain greater exposure and conversions.
These comprehensive improvement results across all stakeholders validates that
LLM-driven, intent-centric design can foster a more sustainable and mutually
beneficial recommendation ecosystem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence](https://arxiv.org/abs/2507.22197)
*Matthieu Queloz*

Main category: cs.AI

TL;DR: 本文讨论了人工智能系统性的重要性，并提出了一个概念框架来理解思想的系统性。通过区分四种含义，缓解了系统性和连接主义之间的矛盾，并探讨了系统化理由在AI中的应用。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨人工智能是否应该被要求具备系统性，以及如何理解系统性的需求。

Method: 本文提出了一个概念框架来思考“思想的系统性”，并区分了该短语的四种含义。然后利用这些区别来缓解系统性和连接主义之间的矛盾，并探讨系统化理由在AI模型中的适用性。

Result: 本文提出了一种新的系统性概念，并展示了它如何适用于AI模型。此外，还识别了五个系统化的理由，并探讨了它们在AI中的应用。

Conclusion: 本文认为，可解释性只是塑造我们对人工智能（AI）期望的更广泛理想的一个方面。关键问题是人工智能在多大程度上表现出系统性——不仅在于对思想如何由可重新组合的组成部分敏感，而且在于追求一个一致、连贯、全面且简洁原则的整合思想体系。这种更丰富的系统性概念被连接主义的“系统性挑战”所掩盖，根据这一挑战，网络架构与Fodor及其同事所谓的“思想的系统性”本质上是矛盾的。本文提供了一个概念框架来思考“思想的系统性”，并区分了该短语的四种含义。我利用这些区别来缓解系统性和连接主义之间的 perceived 矛盾，并表明历史上塑造我们对理性、权威性和科学性思想的认识的系统性概念比Fodorian概念更为严格。为了确定我们是否有理由将这种系统性理想应用于AI模型，我随后认为，我们必须考察系统化的理由，并探讨它们在多大程度上适用于AI模型。我识别了五个这样的理由并将其应用于AI。这揭示了“硬系统性挑战”。然而，系统化的需求本身需要由系统化的理由来调节。这产生了一种动态的理解，即需要系统化思想，这告诉我们AI模型需要多系统化以及何时需要系统化。

Abstract: This paper argues that explainability is only one facet of a broader ideal
that shapes our expectations towards artificial intelligence (AI).
Fundamentally, the issue is to what extent AI exhibits systematicity--not
merely in being sensitive to how thoughts are composed of recombinable
constituents, but in striving towards an integrated body of thought that is
consistent, coherent, comprehensive, and parsimoniously principled. This richer
conception of systematicity has been obscured by the long shadow of the
"systematicity challenge" to connectionism, according to which network
architectures are fundamentally at odds with what Fodor and colleagues termed
"the systematicity of thought." I offer a conceptual framework for thinking
about "the systematicity of thought" that distinguishes four senses of the
phrase. I use these distinctions to defuse the perceived tension between
systematicity and connectionism and show that the conception of systematicity
that historically shaped our sense of what makes thought rational,
authoritative, and scientific is more demanding than the Fodorian notion. To
determine whether we have reason to hold AI models to this ideal of
systematicity, I then argue, we must look to the rationales for systematization
and explore to what extent they transfer to AI models. I identify five such
rationales and apply them to AI. This brings into view the "hard systematicity
challenge." However, the demand for systematization itself needs to be
regulated by the rationales for systematization. This yields a dynamic
understanding of the need to systematize thought, which tells us how systematic
we need AI models to be and when.

</details>


### [45] [CoEx -- Co-evolving World-model and Exploration](https://arxiv.org/abs/2507.22281)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.AI

TL;DR: 本文介绍了一种分层代理架构CoEx，通过动态更新的世界模型和神经符号信念状态来提高规划和探索性能。


<details>
  <summary>Details</summary>
Motivation: 现有代理设计无法有效地将新观察结果融入动态更新的世界模型中，导致生成的计划出现偏差和错误。

Method: CoEx是一种分层代理架构，通过LLM推理协调包含子目标的动态计划，并通过学习机制将这些子目标经验整合到持久的世界模型中。

Result: CoEx在涉及丰富环境和复杂任务的多种代理场景中进行了评估，包括ALFWorld、PDDL和Jericho。

Conclusion: CoEx在规划和探索方面优于现有的代理范式。

Abstract: Planning in modern LLM agents relies on the utilization of LLM as an internal
world model, acquired during pretraining. However, existing agent designs fail
to effectively assimilate new observations into dynamic updates of the world
model. This reliance on the LLM's static internal world model is progressively
prone to misalignment with the underlying true state of the world, leading to
the generation of divergent and erroneous plans. We introduce a hierarchical
agent architecture, CoEx, in which hierarchical state abstraction allows LLM
planning to co-evolve with a dynamically updated model of the world. CoEx plans
and interacts with the world by using LLM reasoning to orchestrate dynamic
plans consisting of subgoals, and its learning mechanism continuously
incorporates these subgoal experiences into a persistent world model in the
form of a neurosymbolic belief state, comprising textual inferences and
code-based symbolic memory. We evaluate our agent across a diverse set of agent
scenarios involving rich environments and complex tasks including ALFWorld,
PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent
paradigms in planning and exploration.

</details>


### [46] [LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models](https://arxiv.org/abs/2507.22359)
*Qianhong Guo,Wei Xie,Xiaofang Cai,Enze Wang,Shuoyoucheng Ma,Kai Chen,Xiaofeng Wang,Baosheng Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的无基准评估范式LLM-Crowdsourced，利用大型语言模型生成问题、独立回答并相互评估，能够同时满足动态、透明、客观和专业四个关键评估标准。实验结果表明，该方法在区分大型语言模型性能方面具有优势，并揭示了一些传统方法难以检测的新发现。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法存在数据污染、黑盒操作和主观偏好等问题，难以全面评估大型语言模型的真实能力。

Method: 本文提出了一种新的无基准评估范式LLM-Crowdsourced，该方法利用大型语言模型生成问题、独立回答并相互评估。

Result: 实验结果表明，该方法在区分大型语言模型性能方面具有优势，并揭示了一些传统方法难以检测的新发现，如Gemini在原创性和专业性问题设计方面表现最佳，一些大型语言模型表现出基于记忆的回答行为，以及大型语言模型的评估结果具有高度一致性。

Conclusion: 本文提出了一种新的无基准评估范式LLM-Crowdsourced，该方法利用大型语言模型生成问题、独立回答并相互评估，能够同时满足动态、透明、客观和专业四个关键评估标准。实验结果表明，该方法在区分大型语言模型性能方面具有优势，并揭示了一些传统方法难以检测的新发现。

Abstract: Although large language models (LLMs) demonstrate remarkable capabilities
across various tasks, evaluating their capabilities remains a challenging task.
Existing evaluation methods suffer from issues such as data contamination,
black-box operation, and subjective preference. These issues make it difficult
to evaluate the LLMs' true capabilities comprehensively. To tackle these
challenges, we propose a novel benchmark-free evaluation paradigm,
LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,
and evaluate mutually. This method integrates four key evaluation criteria:
dynamic, transparent, objective, and professional, which existing evaluation
methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs
across mathematics and programming verify the advantages of our method in
distinguishing LLM performance. Furthermore, our study reveals several novel
findings that are difficult for traditional methods to detect, including but
not limited to: (1) Gemini demonstrates the highest original and professional
question-design capabilities among others; (2) Some LLMs exhibit
''memorization-based answering'' by misrecognizing questions as familiar ones
with a similar structure; (3) LLM evaluation results demonstrate high
consistency (robustness).

</details>


### [47] [The Incomplete Bridge: How AI Research (Mis)Engages with Psychology](https://arxiv.org/abs/2507.22847)
*Han Jiang,Pengda Wang,Xiaoyuan Yi,Xing Xie,Ziang Xiao*

Main category: cs.AI

TL;DR: 本文分析了AI与心理学之间的跨学科互动，识别了关键模式、常见误用，并提供了有效整合的指导。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究了人类思维和行为的丰富理论和方法，为人工智能系统的设 计和理解提供了有价值的见解。本文聚焦于心理学这一典型案例，探讨AI与该领域的跨学科协同作用。

Method: 通过分析2023年至2025年间在顶级AI会议上发表的1,006篇与LLM相关的论文以及它们引用的2,544篇心理学出版物，识别跨学科整合的关键模式，定位最常引用的心理学领域，并突出未充分探索的领域。

Result: 识别了跨学科整合的关键模式，定位了最常引用的心理学领域，突出了未充分探索的领域，分析了心理学理论/框架的操作化和解释，识别了常见的误用类型，并提供了更有效的整合指导。

Conclusion: 本文提供了AI与心理学跨学科互动的全面地图，从而促进更深入的合作并推动AI系统的发展。

Abstract: Social sciences have accumulated a rich body of theories and methodologies
for investigating the human mind and behaviors, while offering valuable
insights into the design and understanding of Artificial Intelligence (AI)
systems. Focusing on psychology as a prominent case, this study explores the
interdisciplinary synergy between AI and the field by analyzing 1,006
LLM-related papers published in premier AI venues between 2023 and 2025, along
with the 2,544 psychology publications they cite. Through our analysis, we
identify key patterns of interdisciplinary integration, locate the psychology
domains most frequently referenced, and highlight areas that remain
underexplored. We further examine how psychology theories/frameworks are
operationalized and interpreted, identify common types of misapplication, and
offer guidance for more effective incorporation. Our work provides a
comprehensive map of interdisciplinary engagement between AI and psychology,
thereby facilitating deeper collaboration and advancing AI systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning](https://arxiv.org/abs/2507.22607)
*Ruifeng Yuan,Chenghao Xiao,Sicong Leng,Jianyu Wang,Long Li,Weiwen Xu,Hou Pong Chan,Deli Zhao,Tingyang Xu,Zhongyu Wei,Hao Zhang,Yu Rong*

Main category: cs.CV

TL;DR: 本文提出了一种名为VL-Cogito的先进多模态推理模型，它通过一种新颖的多阶段渐进式课程强化学习（PCuRL）框架进行训练。该框架引入了两个关键创新：在线难度软加权机制和动态长度奖励机制，以提高模型在各种多模态情境下的推理能力。实验结果表明，VL-Cogito在多个主流多模态基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于多模态任务固有的复杂性和多样性，特别是在语义内容和问题表述方面，现有模型在不同领域和难度水平上往往表现出不稳定的性能。为了克服这些限制，我们提出了VL-Cogito。

Method: 我们提出了VL-Cogito，这是一个通过新颖的多阶段渐进式课程强化学习（PCuRL）框架训练的先进多模态推理模型。PCuRL系统地引导模型通过难度逐渐增加的任务，显著提高了其在各种多模态情境下的推理能力。该框架引入了两个关键创新：(1) 在线难度软加权机制，动态调整后续RL训练阶段的训练难度；(2) 动态长度奖励机制，鼓励模型根据任务复杂性自适应地调节其推理路径长度，从而平衡推理效率与正确性。

Result: 实验评估表明，VL-Cogito在主流多模态基准测试中 consistently matches or surpasses existing reasoning-oriented models，验证了我们方法的有效性。

Conclusion: 实验评估表明，VL-Cogito在主流多模态基准测试中 consistently matches or surpasses existing reasoning-oriented models，验证了我们方法的有效性。

Abstract: Reinforcement learning has proven its effectiveness in enhancing the
reasoning capabilities of large language models. Recent research efforts have
progressively extended this paradigm to multimodal reasoning tasks. Due to the
inherent complexity and diversity of multimodal tasks, especially in semantic
content and problem formulations, existing models often exhibit unstable
performance across various domains and difficulty levels. To address these
limitations, we propose VL-Cogito, an advanced multimodal reasoning model
trained via a novel multi-stage Progressive Curriculum Reinforcement Learning
(PCuRL) framework. PCuRL systematically guides the model through tasks of
gradually increasing difficulty, substantially improving its reasoning
abilities across diverse multimodal contexts. The framework introduces two key
innovations: (1) an online difficulty soft weighting mechanism, dynamically
adjusting training difficulty across successive RL training stages; and (2) a
dynamic length reward mechanism, which encourages the model to adaptively
regulate its reasoning path length according to task complexity, thus balancing
reasoning efficiency with correctness. Experimental evaluations demonstrate
that VL-Cogito consistently matches or surpasses existing reasoning-oriented
models across mainstream multimodal benchmarks spanning mathematics, science,
logic, and general understanding, validating the effectiveness of our approach.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [49] [Next Tokens Denoising for Speech Synthesis](https://arxiv.org/abs/2507.22746)
*Yanqing Liu,Ruiqing Xue,Chong Zhang,Yufei Liu,Gang Wang,Bohan Li,Yao Qian,Lei He,Shujie Liu,Sheng Zhao*

Main category: cs.SD

TL;DR: Dragon-FM是一种将自回归和流匹配统一的文本到语音设计，能够在保持全局连贯性的同时实现快速迭代去噪，并有效生成长内容。


<details>
  <summary>Details</summary>
Motivation: 克服自回归模型无法利用未来上下文和扩散模型在键值缓存方面的挑战。

Method: 引入了Dragon-FM，这是一种将自回归（AR）和流匹配统一的文本到语音（TTS）设计。

Result: 模型能够跨块使用KV缓存，并在每个块内包含未来上下文，同时展示了连续AR流匹配可以使用有限标量量化器预测离散标记。

Conclusion: 该模型在生成长内容方面特别有效，并且能够高效生成高质量的零样本播客。

Abstract: While diffusion and autoregressive (AR) models have significantly advanced
generative modeling, they each present distinct limitations. AR models, which
rely on causal attention, cannot exploit future context and suffer from slow
generation speeds. Conversely, diffusion models struggle with key-value (KV)
caching. To overcome these challenges, we introduce Dragon-FM, a novel
text-to-speech (TTS) design that unifies AR and flow-matching. This model
processes 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per
second rate. This design enables AR modeling across chunks, ensuring global
coherence, while parallel flow-matching within chunks facilitates fast
iterative denoising. Consequently, the proposed model can utilize KV-cache
across chunks and incorporate future context within each chunk. Furthermore, it
bridges continuous and discrete feature modeling, demonstrating that continuous
AR flow-matching can predict discrete tokens with finite scalar quantizers.
This efficient codec and fast chunk-autoregressive architecture also makes the
proposed model particularly effective for generating extended content.
Experiment for demos of our work} on podcast datasets demonstrate its
capability to efficiently generate high-quality zero-shot podcasts.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [50] [Prompt Optimization and Evaluation for LLM Automated Red Teaming](https://arxiv.org/abs/2507.22133)
*Michael Freenor,Lauren Alvarez,Milton Leal,Lily Smith,Joel Garrett,Yelyzaveta Husieva,Madeline Woodruff,Ryan Miller,Erich Kummerfeld,Rafael Medeiros,Sander Schulhoff*

Main category: cs.CR

TL;DR: 本文提出了一种优化攻击生成器提示的方法，通过应用ASR来评估单个攻击，从而揭示可利用的模式，提高评估和改进生成器的稳健性。


<details>
  <summary>Details</summary>
Motivation: 随着使用大型语言模型的应用程序日益普及，识别系统漏洞变得越来越重要。自动化红队通过使用LLM生成和执行攻击来加速这一过程。

Method: 本文提出了一种方法，通过重复对随机种子目标进行攻击，测量攻击的可发现性，即单个攻击成功的期望值。

Result: 该方法揭示了可利用的模式，有助于更稳健的评估和改进生成器。

Conclusion: 本文提出了一种优化攻击生成器提示的方法，通过应用ASR来评估单个攻击。这种方法揭示了可利用的模式，有助于更稳健的评估和改进生成器。

Abstract: Applications that use Large Language Models (LLMs) are becoming widespread,
making the identification of system vulnerabilities increasingly important.
Automated Red Teaming accelerates this effort by using an LLM to generate and
execute attacks against target systems. Attack generators are evaluated using
the Attack Success Rate (ASR) the sample mean calculated over the judgment of
success for each attack. In this paper, we introduce a method for optimizing
attack generator prompts that applies ASR to individual attacks. By repeating
each attack multiple times against a randomly seeded target, we measure an
attack's discoverability the expectation of the individual attack success. This
approach reveals exploitable patterns that inform prompt optimization,
ultimately enabling more robust evaluation and refinement of generators.

</details>


### [51] [Strategic Deflection: Defending LLMs from Logit Manipulation](https://arxiv.org/abs/2507.22160)
*Yassine Rachidy,Jihad Rbaiti,Youssef Hmamouche,Faissal Sehbaoui,Amal El Fallah Seghrouchni*

Main category: cs.CR

TL;DR: 本文提出了一种新的防御策略SDeflection，通过生成与用户请求语义相邻但去除了有害意图的回答来中和高级攻击。实验表明，该方法有效降低了攻击成功率，同时保持了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在关键领域的广泛应用，确保其免受越狱攻击至关重要。传统的防御方法主要依赖于拒绝恶意提示，但最近的日志级别攻击已证明能够通过直接操纵生成过程中的令牌选择来绕过这些防护措施。因此，需要一种更有效的防御方法来应对这些高级攻击。

Method: 本文介绍了SDeflection，这是一种防御策略，通过重新定义LLM对先进攻击的响应来中和攻击者的有害意图。具体来说，模型生成一个与用户请求语义相邻但去除了有害意图的回答，而不是直接拒绝。

Result: 实验结果表明，SDeflection显著降低了攻击成功率（ASR），同时保持了模型在良性查询上的性能。这表明该方法在提高安全性的同时不会显著影响模型的正常功能。

Conclusion: 本文提出了一种新的防御策略，即战略偏转（SDeflection），以应对先进的日志级别攻击。该方法通过生成与用户请求语义相邻但去除了有害意图的回答，从而中和攻击者的有害意图。实验表明，SDeflection显著降低了攻击成功率（ASR），同时保持了模型在良性查询上的性能。这项工作标志着防御策略的重要转变，从简单的拒绝转向战略性内容重定向，以应对高级威胁。

Abstract: With the growing adoption of Large Language Models (LLMs) in critical areas,
ensuring their security against jailbreaking attacks is paramount. While
traditional defenses primarily rely on refusing malicious prompts, recent
logit-level attacks have demonstrated the ability to bypass these safeguards by
directly manipulating the token-selection process during generation. We
introduce Strategic Deflection (SDeflection), a defense that redefines the
LLM's response to such advanced attacks. Instead of outright refusal, the model
produces an answer that is semantically adjacent to the user's request yet
strips away the harmful intent, thereby neutralizing the attacker's harmful
intent. Our experiments demonstrate that SDeflection significantly lowers
Attack Success Rate (ASR) while maintaining model performance on benign
queries. This work presents a critical shift in defensive strategies, moving
from simple refusal to strategic content redirection to neutralize advanced
threats.

</details>

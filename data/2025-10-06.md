<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: CASAL is an efficient algorithm that improves the reliability of large language models by reducing hallucinations without requiring real-time monitoring. It is highly compute and data-efficient and works well across different types of models.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often hallucinate, confidently providing incorrect answers instead of admitting ignorance. Prior work has shown that models encode linear representations of their own knowledge and that activation steering can reduce hallucinations, but these approaches require real-time monitoring and intervention during inference.

Method: CASAL is an efficient algorithm that connects interpretability with amortized optimization. It directly bakes the benefits of activation steering into the model's weights.

Result: CASAL reduces hallucination by 30%-40% across multiple short-form QA benchmarks. It is 30x more compute-efficient and 20x more data-efficient than strong LoRA-based baselines such as SFT and DPO. CASAL also generalizes effectively to out-of-distribution (OOD) domains and mitigates hallucinations in both text-only and vision-language models.

Conclusion: CASAL represents a promising step forward for applying interpretability-inspired methods for practical deployment in production systems.

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [2] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: RA-FSM 是一种基于 GPT 的研究助手，通过有限状态机控制循环提高文献综合的准确性和可靠性，在多个任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在文献综合中可能产生幻觉和错误引用，限制了其在专家工作流程中的使用。因此，需要一种更可靠、透明的研究助手。

Method: RA-FSM 采用有限状态机控制循环（相关性 -> 信心 -> 知识），结合向量检索和确定性引用管道，实现对查询的过滤、评分、分解和检索。

Result: RA-FSM 在六个任务类别中表现出色，领域专家更倾向于使用 RA-FSM 而不是其他方法，因为它具有更强的边界条件处理能力和更可辩护的证据使用。

Conclusion: RA-FSM 是一个模块化的 GPT 基础研究助手，通过有限状态机控制循环来提高文献综合的准确性。它在多个任务类别中表现优于现有的方法，并且适用于其他科学领域。

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [3] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 本文提出一种混合架构，结合实时语音到语音模型与大型语言模型，以低延迟实现更准确的语音响应。


<details>
  <summary>Details</summary>
Motivation: 现有的实时语音到语音模型在自然对话中表现良好，但缺乏深度知识和语义理解；而级联系统虽然能提供更好的知识表示，但延迟较高，影响交互流畅性。

Method: 本文提出了一种混合架构，通过将语音到语音（S2S）变换器与强大的后端大型语言模型（LLM）结合，实现即时响应和知识注入。

Result: 实验结果表明，本文提出的系统在响应正确性方面显著优于基线S2S模型，接近级联系统的性能，同时保持与基线相当的延迟。

Conclusion: 本文提出了一种混合架构，能够在保持低延迟的同时，提高语音到语音模型的语义理解和知识表示能力。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [4] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: 本文提出了一种名为AMANDA的无需训练的代理框架，通过医学知识增强来解决Med-MLLMs在低资源环境中的性能问题，并在多个基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有Med-MLLMs在低资源环境中由于医学推理能力瓶颈而表现不佳，需要解决内在和外在推理瓶颈问题。

Method: AMANDA是一种无需训练的代理框架，通过LLM代理进行医学知识增强。具体包括内在医学知识增强和外在医学知识增强。

Result: 在八个Med-VQA基准测试中，AMANDA在零样本和少量样本设置中都取得了显著的改进。

Conclusion: AMANDA框架在Med-VQA任务中表现出色，为低资源环境下的医疗视觉问答提供了有效的解决方案。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [5] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: SelfJudge通过自监督训练判断验证器，实现了更高效的LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有的方法受限于对人工标注或可验证真实任务的依赖，限制了在各种NLP任务中的泛化能力。

Method: SelfJudge通过目标模型的自监督训练判断验证器，通过评估替换后的标记是否保留原始响应的意义来衡量语义保留。

Result: 实验表明，SelfJudge在推理速度和准确性之间取得了更好的平衡。

Conclusion: SelfJudge在加速LLM推理方面表现出优于基准方法的推理-准确性权衡，提供了一种广泛适用的解决方案。

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [6] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据构造方法EntropyLong，通过利用预测不确定性来验证长距离依赖关系的质量，并在多个基准测试中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的方法（如通用文本连接或基于启发式的变体）经常无法保证真实的长距离依赖关系。

Method: 我们提出了EntropyLong，一种新的数据构造方法，利用预测不确定性来验证依赖关系的质量。我们的方法通过评估是否减少预测熵来验证语义相关上下文的实用性。

Result: 使用FineWebEdu和Cosmopedia生成了一个具有验证依赖关系的128K长度序列的数据集。在这些数据上训练的模型在RULER基准测试中表现出显著改进，特别是在需要远距离信息的任务中。

Conclusion: 我们的实验结果表明，基于熵的验证方法在长上下文训练中是必要且有效的。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [7] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: 本文提出了一种生成自然对话的方法，用于训练基于语言模型的推荐系统，解决了数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏公开的推荐系统数据，微调语言模型进行推荐系统具有挑战性。

Method: 使用行为模拟器和语言模型提示相结合的方法生成自然对话。

Result: 生成了一个大型开放源码推荐系统数据集，并通过评估显示对话具有较高的连贯性、事实性和自然性。

Conclusion: 本文提出了一种生成与用户潜在状态一致的自然对话的方法，展示了其在推荐系统中的应用潜力。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [8] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 本文提出了一种名为look-ahead Sync的方法，用于解决神经语言隐写术中的分词歧义问题，并在保留安全性的前提下提高嵌入容量。


<details>
  <summary>Details</summary>
Motivation: 神经语言隐写术旨在将信息嵌入自然文本中，同时保持统计不可检测性。然而，现代分词器中的分词歧义是该领域的一个基本挑战，可能导致解码失败。SyncPool方法虽然解决了这一问题，但牺牲了嵌入容量。

Method: 本文提出了一种名为look-ahead Sync的方法，该方法在保留SyncPool的可证明安全保证的同时克服了其容量限制。该方法仅对真正不可区分的标记序列进行最小的同步采样，同时战略性地保留所有其他可区分路径以最大化嵌入容量。

Result: 实验结果表明，本文的方法在英语（使用Llama 3）和中文（使用Qwen 2.5）基准测试中 consistently 接近理论容量上限，并显著优于SyncPool。在英语中，嵌入率提高了超过160%，在中文中提高了25%。

Conclusion: 本文代表了向实用的高容量可证明安全的语言隐写术迈出的重要一步。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [9] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 本文介绍了两个公开的数据集，这些数据集包含语义丰富的行人轨迹，并提供了构建它们的流程。数据集包括上下文图层，如停留点、移动、兴趣点（POIs）、推断的交通方式和天气数据。此外，还包含了由大型语言模型（LLMs）生成的合成、现实的社会媒体帖子，以实现多模态和语义移动分析。数据集以表格和资源描述框架（RDF）格式提供，支持语义推理和FAIR数据实践。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是提供一个结合现实世界移动、结构化语义增强、LLM生成文本和语义网兼容性的可重用框架，以支持行为建模、移动预测、知识图谱构建和基于LLM的应用研究。

Method: 本文提出了两个公开可用的数据集，这些数据集包含语义丰富的行人轨迹，并提供了构建它们的流程。数据集包括上下文图层，如停留点、移动、兴趣点（POIs）、推断的交通方式和天气数据。此外，还包含了由大型语言模型（LLMs）生成的合成、现实的社会媒体帖子，以实现多模态和语义移动分析。数据集以表格和资源描述框架（RDF）格式提供，支持语义推理和FAIR数据实践。

Result: 本文介绍了两个公开的数据集，这些数据集包含语义丰富的行人轨迹，并提供了构建它们的流程。数据集包括上下文图层，如停留点、移动、兴趣点（POIs）、推断的交通方式和天气数据。此外，还包含了由大型语言模型（LLMs）生成的合成、现实的社会媒体帖子，以实现多模态和语义移动分析。数据集以表格和资源描述框架（RDF）格式提供，支持语义推理和FAIR数据实践。

Conclusion: 本文介绍了两个公开的数据集，这些数据集包含语义丰富的行人轨迹，并提供了构建它们的流程。这些数据集包括上下文图层，如停留点、移动、兴趣点（POIs）、推断的交通方式和天气数据。一个新颖的语义特征是包含了由大型语言模型（LLMs）生成的合成、现实的社会媒体帖子，从而实现了多模态和语义移动分析。数据集以表格和资源描述框架（RDF）格式提供，支持语义推理和FAIR数据实践。它们覆盖了两个结构不同的大城市：巴黎和纽约。我们的开源可重复流程允许数据集定制，而数据集支持行为建模、移动预测、知识图谱构建和基于LLM的应用研究。据我们所知，我们的资源是第一个将现实世界移动、结构化语义增强、LLM生成文本和语义网兼容性结合在一个可重用框架中的资源。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [10] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，用于诊断大型语言模型中的不良行为，通过分析表示及其梯度，在模型的激活空间中提供语义上相关的信号。该方法在样本级和token级归因方面表现优异，能够精确识别影响模型行为的具体内容。


<details>
  <summary>Details</summary>
Motivation: 现有的基于参数梯度的归因方法由于信号噪声大和计算复杂性而常常不足，因此需要一种更有效的诊断方法来解决LLM中的不良行为问题。

Method: 本文引入了一种新颖且高效的框架，通过分析表示及其梯度来诊断各种不良的LLM行为，直接在模型的激活空间中操作，以提供语义上有意义的信号，将输出与训练数据联系起来。

Result: 实验结果表明，该方法不仅在样本级归因方面表现出色，还能进行细粒度的token级分析，精确识别出因果影响模型行为的具体样本和短语。

Conclusion: 本文提供了一个强大的诊断工具，以理解和减轻大型语言模型（LLMs）相关的风险。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [11] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: 本文介绍了FormalML基准测试，用于评估大语言模型在子目标完成任务中的表现，并指出当前定理证明器的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在数学证明中的应用，特别是在填补复杂证明中缺失步骤方面的能力。

Method: 引入FormalML，一个基于机器学习基础理论的Lean 4基准测试，通过转换程序性证明为声明式形式，提取了4937个不同难度的问题。

Result: FormalML是第一个结合前提检索和复杂研究级上下文的子目标完成基准测试，评估显示现有证明器存在明显不足。

Conclusion: 评估表明，现有的定理证明器在准确性和效率上仍有局限，需要更强大的基于大语言模型的定理证明器来实现有效的子目标完成。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [12] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 本文介绍了首个库尔德语STS数据集，并对其进行了基准测试，以推动库尔德语语义和低资源NLP的研究。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言如库尔德语缺乏足够的资源，我们致力于创建第一个库尔德语STS数据集。

Method: 我们提出了库尔德语STS数据集，并对其进行了基准测试，包括Sentence-BERT、多语言BERT和其他强基线模型。

Result: 我们获得了具有竞争力的结果，同时突出了由库尔德语形态、拼写变化和代码混合引起的问题。

Conclusion: 该数据集和基线为库尔德语语义和低资源自然语言处理的未来研究提供了可重复的评估套件和强有力的起点。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [13] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: CRACQ is a multi-dimensional evaluation framework that evaluates documents across five traits, integrating linguistic, semantic, and structural signals for a more stable and interpretable assessment.


<details>
  <summary>Details</summary>
Motivation: To expand the focus of trait-based Automated Essay Scoring (AES) beyond essays to encompass diverse forms of machine-generated text, providing a rubric-driven and interpretable methodology for automated evaluation.

Method: CRACQ is a multi-dimensional evaluation framework that integrates linguistic, semantic, and structural signals into a cumulative assessment, enabling both holistic and trait-level analysis.

Result: CRACQ was trained on 500 synthetic grant proposals and benchmarked against an LLM-as-a-judge, showing more stable and interpretable trait-level judgments than direct LLM evaluation.

Conclusion: CRACQ produces more stable and interpretable trait-level judgments than direct LLM evaluation, though challenges in reliability and domain scope remain.

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [14] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 本文提出了一种集成评估的强化学习框架，用于长格式临床文本生成，通过组相对策略优化和DocLens评估器提高临床笔记质量，并减少训练成本。


<details>
  <summary>Details</summary>
Motivation: 自动化临床文档需要精确对齐完整性、事实依据等优先事项。

Method: 我们提出了一个集成评估的强化学习框架，将组相对策略优化（GRPO）与DocLens结合，DocLens是一种提供确定性、对话基础奖励的声明级评估器。

Result: 实证研究表明，该方法通过简单的奖励门控策略提高了临床笔记质量并降低了训练成本。独立的GPT-5定性评估进一步支持了这些改进，显示GRPO输出在事实性、完整性和简洁性方面更受青睐，且遗漏和幻觉更少。

Conclusion: 该框架可扩展到现实场景，并可纳入自定义目标，如指南遵循或计费偏好。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [15] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 研究探讨了将不确定性量化方法集成到论证型LLM中，并通过实验评估了这些方法的有效性，发现直接提示方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 研究不确定性量化（UQ）对于大型语言模型（LLMs）的重要性，以确保这项突破性技术的可靠性。探索将LLM UQ方法集成到论证型LLM（ArgLLMs）中，其中UQ起着关键作用。

Method: 通过实验评估ArgLLMs在使用不同LLM UQ方法时在声明验证任务上的性能，从而评估UQ方法的有效性。

Result: 实验结果表明，尽管直接提示方法简单，但在ArgLLMs中表现出色，优于更复杂的方案。

Conclusion: 直接提示是一种有效的UQ策略，优于更复杂的方案。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [16] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 本研究探讨了通过提示让LLMs模拟早期知识截止的能力，发现其在直接查询时有效，但在因果相关查询时效果不佳，强调了评估设置的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨LLMs是否可以通过提示模拟早期知识截止，以解决预训练数据污染问题，从而更准确地评估其泛化能力。

Method: 构建三个评估数据集来评估LLMs在（1）直接事实知识、（2）语义变化和（3）因果相关知识方面的遗忘程度。

Result: 研究结果表明，基于提示的模拟知识截止在直接查询截止日期后信息时有效，但在遗忘内容与查询有因果关系时效果不佳。

Conclusion: 研究结果表明，基于提示的模拟知识截止在直接查询截止日期后信息时表现出有效性，但在遗忘内容与查询有因果关系时却难以诱导遗忘。这强调了在应用LLMs进行时间预测任务时需要更严格的评估设置。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [17] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: DRIFT 是一种利用真实世界中丰富的隐式用户不满信号进行训练的方法，能够显著提升模型性能，并保持探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好学习方法与现实世界的数据情况不匹配，因为它们依赖于昂贵的人工标注或假设存在大量正面响应。而现实世界中的隐式用户不满信号（DSAT）更为丰富，但未被充分利用。

Method: DRIFT（Dissatisfaction-Refined Iterative Preference Training）通过锚定在真实世界的DSAT信号上，并从不断演化的策略中动态采样正样本进行训练。

Result: DRIFT 模型在真实世界的 WildFeedback 数据集和合成的 UltraFeedback 数据集上训练，取得了显著的性能提升，超过了如迭代 DPO 和 SPIN 等强基线方法。

Conclusion: DRIFT 是一种有效且可扩展的现实世界后训练方法，它利用了最丰富和有信息量的信号。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [18] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: This paper introduces SIMPACT, a framework for constructing behaviorally-grounded social media datasets suitable for training agent models. It also releases BluePrint, a large-scale dataset built from public Bluesky data focused on political discourse.


<details>
  <summary>Details</summary>
Motivation: The field lacks standardized data resources for fine-tuning and evaluating LLMs as realistic social media agents.

Method: We formulate next-action prediction as a task for training and evaluating LLM-based agents and introduce metrics at both the cluster and population levels to assess behavioral fidelity and stylistic realism.

Result: We release BluePrint, a large-scale dataset built from public Bluesky data focused on political discourse. BluePrint clusters anonymized users into personas of aggregated behaviours, capturing authentic engagement patterns while safeguarding privacy through pseudonymization and removal of personally identifiable information.

Conclusion: SIMPACT provides a foundation for advancing rigorous, ethically responsible social media simulations. BluePrint serves as both an evaluation benchmark for political discourse modeling and a template for building domain specific datasets to study challenges such as misinformation and polarization.

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [19] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 本文介绍了一种基于动态专家聚类和结构压缩的统一框架，以解决Mixture-of-Experts (MoE)大型语言模型（LLMs）面临的负载不平衡、参数冗余和通信开销问题。通过在线聚类过程和专家权重的分解，实现了显著的参数减少和效率提升。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) Large Language Models (LLMs)面临负载不平衡、参数冗余和通信开销的问题。我们需要一个统一的框架来解决这些问题。

Method: 我们引入了一个基于动态专家聚类和结构压缩的统一框架，以协同解决这些问题。我们的方法使用在线聚类过程，定期使用参数和激活相似性的融合度量重新分组专家，这稳定了专家的利用率。在每个聚类中，我们将专家权重分解为共享的基础矩阵和极低秩的残差适配器，实现了每组五倍的参数减少，同时保持了专业化。这种结构使得两级分层路由策略成为可能：首先将标记分配到一个聚类，然后分配到其中的具体专家，大大减少了路由搜索空间和所有到所有的通信量。此外，一种异构精度方案，将共享基础存储在FP16中，残差因子存储在INT4中，并结合不活跃聚类的动态卸载，将峰值内存消耗降低到与密集模型相当的水平。

Result: 在GLUE和WikiText-103上评估，我们的框架在保持标准MoE模型质量的同时，将总参数减少了约80%，吞吐量提高了10%至20%，并将专家负载方差降低了三倍以上。

Conclusion: 我们的工作展示了结构重新组织是通往可扩展、高效和内存有效的MoE LLMs的合理路径。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [20] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: This paper explores the use of small language models (SLMs) as AI teaching assistants in education, showing they can match the performance of large language models (LLMs) while being more sustainable and cost-effective.


<details>
  <summary>Details</summary>
Motivation: The adoption of generative AI and large language models (LLMs) in education is still emerging, and there is a need to explore sustainable alternatives that can deliver accurate, pedagogically aligned responses.

Method: The study explored the development and evaluation of AI teaching assistants using a retrieval-augmented generation (RAG) pipeline applied to selected open-source small language models (SLMs). Eight SLMs were benchmarked against GPT-4o.

Result: With proper prompting and targeted retrieval, SLMs can match LLMs in delivering accurate, pedagogically aligned responses. They also offer significant sustainability benefits due to their lower computational and energy requirements.

Conclusion: SLMs can serve as viable AI teaching assistants for educational institutions due to their sustainability, cost-effectiveness, privacy preservation, and environmental responsibility.

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [21] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 我们提出了mini-vec2vec，这是一种更简单、高效且计算成本更低的替代方法，用于对齐文本嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 我们想要一种更简单、高效且计算成本更低的替代方法，以解决vec2vec的昂贵和不稳定的问题。

Method: 我们的方法包括三个主要阶段：伪平行嵌入向量的初步匹配、变换拟合和迭代优化。

Result: 我们的线性方法在效率上比原始的vec2vec实现了数量级的提升，同时结果相当或更好。

Conclusion: 我们的方法在效率上超过了原始的vec2vec实现，同时结果相当或更好。该方法的稳定性和可解释的算法步骤有助于扩展，并为在新领域和学科中的采用打开了新的机会。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [22] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMSQL是对WikiSQL的系统性修订，旨在为大语言模型提供一个更清洁、更适合的基准测试数据集。


<details>
  <summary>Details</summary>
Motivation: 由于WikiSQL存在结构和注释问题，其使用率下降。为了适应大语言模型的需求，需要一个更清洁、更适合现代模型的基准测试数据集。

Method: 对WikiSQL中的错误进行分类，并实现自动化的方法进行清理和重新注释。评估了多个大型语言模型（LLMs）的影响。

Result: LLMSQL被引入为一个适用于大语言模型的基准测试，相比原始的WikiSQL，它提供了更清晰的自然语言问题和完整的SQL查询。

Conclusion: LLMSQL是一个为大语言模型时代设计的系统性修订和转换的WikiSQL版本，旨在提供干净的自然语言问题和完整的SQL查询作为纯文本，以便现代自然语言到SQL模型的生成和评估。

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [23] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在不同政治和文化视角下评估政治言论冒犯性的情况，并发现具有推理能力的大型模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 我们探索了大型语言模型（LLMs）在提示下采用特定政治和文化视角时如何评估政治言论中的冒犯性。

Method: 我们使用了包含2020年美国大选推文的多语言子集MD-Agreement数据集，评估了几种最近的LLM（包括DeepSeek-R1, o4-mini, GPT-4.1-mini, Qwen3, Gemma和Mistral），这些模型被要求从不同政治人物（极右翼、保守派、中间派、进步派）的角度判断推文是否具有冒犯性，涉及英语、波兰语和俄语语境。

Result: 我们的结果显示，具有显式推理能力的大型模型（如DeepSeek-R1, o4-mini）在评估政治言论的冒犯性时更加一致和敏感，而较小的模型往往无法捕捉细微差别。

Conclusion: 我们的研究结果表明，具有显式推理能力的大型模型（如DeepSeek-R1, o4-mini）在评估政治言论的冒犯性时更加一致和敏感，而较小的模型往往无法捕捉细微差别。推理能力显著提高了冒犯性判断的个性化和可解释性，这表明这些机制对于在不同语言和意识形态下适应LLM进行复杂社会政治文本分类至关重要。

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [24] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 本文首次系统研究了端到端语音对话模型中的偏见，发现封闭源代码模型偏见较低，而开源模型对年龄和性别更敏感，推荐任务可能放大跨群体差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）中的偏见已被研究，但其在具有音频输入和输出的语音对话模型（SDMs）中的存在和特征仍 largely 未被探索。此外，多轮对话可能加剧偏见，这对决策和推荐任务的公平性有潜在影响。

Method: 本文系统地评估了语音LLM中的偏见，并研究了多轮对话与重复负面反馈的影响。使用Group Unfairness Score (GUS) 和基于相似性的归一化统计率 (SNSR) 来衡量偏见，涵盖了开源模型和封闭源代码API。

Result: 结果表明，封闭源代码模型通常表现出较低的偏见，而开源模型对年龄和性别更敏感，推荐任务往往会放大跨群体差异。此外，发现偏见决策可能在多轮对话中持续存在。

Conclusion: 本文提供了对端到端语音对话模型中偏见的首次系统研究，为公平和可靠的基于音频的交互系统提供了见解。为了促进进一步的研究，我们发布了FairDialogue数据集和评估代码。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [25] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 本研究探讨了人工智能和大型语言模型在改善塞内加尔司法系统中法律文本访问的应用，通过构建图数据库和使用先进的三元组提取技术，提高了法律信息的可访问性和可视化。


<details>
  <summary>Details</summary>
Motivation: 本研究强调了从法律文件中提取和组织法律文件的困难，突出了改善司法信息获取的必要性。

Method: 本研究应用了人工智能（AI）和大型语言模型（LLM），成功提取了7,967条法律条文，并开发了一个包含2,872个节点和10,774个关系的详细图数据库。此外，还利用了先进的三元组提取技术。

Result: 研究成功提取了7,967条法律条文，并构建了一个详细的图数据库，同时展示了GPT-4o、GPT-4和Mistral-Large等模型在识别关系和相关元数据方面的有效性。

Conclusion: 通过这些技术，旨在创建一个坚实的框架，使塞内加尔公民和法律专业人士能够更有效地理解他们的权利和责任。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [26] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 研究发现语言皮层中存在高度抽象、与形式无关的意义表征，这比语言模型的表征更丰富和广泛。


<details>
  <summary>Details</summary>
Motivation: 探讨人类语言系统中的意义表征是否具有抽象性，并研究语言系统是否保持比语言模型更丰富和广泛的语义表征。

Method: 通过使用视觉和语言模型的表示来建模神经反应，生成对应句子的图像并提取视觉模型嵌入，同时对多个生成的图像进行聚合，以及对句子的多个释义进行平均，以提高预测准确性。

Result: 通过聚合多个生成的图像或多个释义，可以提高对语言皮层反应的预测准确性，甚至超过基于原始句子嵌入的预测。

Conclusion: 这些结果表明语言皮层中存在高度抽象、与形式无关的意义表征。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [27] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: DiffuSpec是一种无需训练的框架，利用预训练的扩散语言模型生成多标记草稿，从而提高推测解码的速度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的扩展，准确性提高，但自回归解码的性质增加了延迟。推测解码通过使用快速起草者提出多标记草稿来解决这个问题，但许多部署仍然依赖于自回归起草者，这限制了时钟速度的提升。

Method: DiffuSpec使用预训练的扩散语言模型（DLM）在单次前向传递中生成多标记草稿，同时保持与标准AR验证器的兼容性。引入了因果一致性路径搜索（CPS）和自适应草稿长度（ADL）控制器来解决挑战。

Result: DiffuSpec在多个基准测试中实现了高达3倍的时钟速度提升，证明了基于扩散的起草作为推测解码中自回归起草的稳健替代方案的有效性。

Conclusion: DiffuSpec展示了基于扩散的起草在推测解码中作为自回归起草的稳健替代方案的有效性。

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [28] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: Emission-GPT is a large language model agent designed to improve access and interpretation of emissions data by integrating domain-specific knowledge and enabling natural language interaction for analyzing emissions data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges non-experts face in interpreting emissions information due to fragmented and specialized knowledge, as well as inefficient methods for accessing and compiling emissions data.

Method: Emission-GPT is a knowledge-enhanced large language model agent built on a curated knowledge base of over 10,000 documents. It integrates prompt engineering and question completion to support accurate domain-specific question answering and enables interactive analysis of emissions data via natural language.

Result: A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights such as point source distributions and sectoral trends directly from raw data with simple prompts, and it facilitates automation of traditionally manual workflows.

Conclusion: Emission-GPT is positioned as a foundational tool for next-generation emission inventory development and scenario-based assessment due to its modular and extensible architecture.

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [29] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型中是否可能出现类似‘沉默的螺旋’的现象，并发现历史和人格信号对意见动态有显著影响。


<details>
  <summary>Details</summary>
Motivation: 传统心理学解释不适用于LLM，因此需要探索LLM集体中是否可能产生类似SoS的动力学。

Method: 研究提出了一个评估框架，用于检查LLM代理中的SoS现象，并通过趋势检验和集中度测量来分析意见动态。

Result: 实验表明，历史和人格信号共同导致强烈的多数主导和复制SoS模式；仅历史信号导致强锚定；仅人格信号促进多样化但无关联的意见。

Conclusion: 该研究揭示了在大型语言模型（LLM）代理中可能出现类似‘沉默的螺旋’（SoS）的动力学，并强调了在LLM代理系统中监测和缓解潜在一致性的重要性。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [30] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: ChunkLLM是一种轻量级且可插拔的训练框架，旨在解决Transformer模型在计算效率上的问题。它通过引入QK Adapter和Chunk Adapter来实现特征压缩、块注意力获取和块边界检测，从而在保持高性能的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于块选择和压缩的方法在语义不完整或训练-推理效率方面存在问题，因此需要一种更有效的解决方案来解决Transformer模型在计算效率上的问题。

Method: ChunkLLM是一种轻量级且可插拔的训练框架，包含两个组件：QK Adapter（Q-Adapter和K-Adapter）和Chunk Adapter。QK Adapter附加到每个Transformer层，用于特征压缩和块注意力获取；Chunk Adapter位于模型的最底层，通过利用上下文语义信息来检测块边界。在训练阶段，骨干网络的参数保持冻结，仅对QK Adapter和Chunk Adapter进行训练。设计了一种注意力蒸馏方法来训练QK Adapter，以提高关键块的召回率。在推理阶段，只有当当前标记被检测为块边界时才会触发块选择，从而加速模型推理。

Result: 实验评估在多个任务的长文本和短文本基准数据集上进行。ChunkLLM不仅在短文本基准上达到与现有方法相当的性能，而且在长上下文基准上保持了98.64%的性能，同时保留了48.58%的键值缓存率。特别地，ChunkLLM在处理120K长文本时相比原始Transformer实现了最大4.48倍的加速。

Conclusion: ChunkLLM在长文本和短文本基准数据集上表现出色，不仅在短文本基准上达到了与现有方法相当的性能，而且在长上下文基准上保持了98.64%的性能，同时保留了48.58%的键值缓存率。此外，ChunkLLM在处理120K长文本时相比原始Transformer实现了最大4.48倍的加速。

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [31] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型在回答罗马尼亚有争议的历史问题时的偏见，发现模型的回答可能受到语言和格式的影响。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于认识到历史常常通过被修改的视角呈现，主要受国家文化和理想的影响，甚至通过大型语言模型。

Method: 该研究通过三个阶段进行，以确认预期的回答类型可以在一定程度上影响回答本身。

Result: 结果显示，二进制回答的稳定性相对较高但远非完美，并且因语言而异。模型经常在不同语言或格式之间改变立场，数值评分通常与初始二进制选择不同，最一致的模型并不总是被认为最准确或中立的。

Conclusion: 研究揭示了模型在特定语境下对不一致性的倾向，表明模型的回应可能受到语言和格式的影响。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [32] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，用于学习实例级上下文，以提高LLM代理在复杂任务中的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 我们发现实例级上下文是一个被忽视但关键的第三种上下文类型，它包含与特定环境实例相关的可验证和可重用的事实。

Method: 我们提出了一个任务无关的方法，通过使用紧凑的TODO森林来智能优先考虑下一步行动，并通过轻量级的计划-执行-提取循环来执行它们。

Result: 在TextWorld、ALFWorld和Crafter上的实验表明，成功和效率都有所提高。例如，ReAct在TextWorld中的平均成功率从37%提高到95%。

Conclusion: 通过将一次性探索转化为持久且可重用的知识，我们的方法补充了现有的上下文，使LLM代理更加可靠和高效。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [33] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 本文通过受控研究分析了训练条件如何影响模型使用上下文知识和参数知识以及它们之间的仲裁，发现非理想属性对于学习稳健的仲裁策略非常重要，这些见解为预训练能够和谐整合参数和上下文知识的模型提供了具体的实证指导。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是解决检索增强生成中缺乏对知识仲裁策略形成系统的理解的问题，这可能导致预训练模型产生不良的仲裁行为，并浪费大量的计算资源。

Method: 本文通过在合成传记语料库上训练基于Transformer的语言模型，并系统地控制各种条件，进行了一项受控研究，以分析训练条件如何影响模型使用上下文知识和参数知识以及它们之间的仲裁。

Result: 实验结果表明，文档内的事实重复促进了参数和上下文能力的发展，而包含不一致信息或分布偏斜的语料库则鼓励模型发展出利用参数和上下文知识的稳健策略。

Conclusion: 本文的结论是，非理想属性在学习稳健的仲裁策略中起着重要作用，这些见解为预训练能够和谐整合参数知识和上下文知识的模型提供了具体的实证指导。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [34] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 本文提出了一种基于记忆的架构，使小型语言模型能够访问大型参数记忆库，从而在保持高性能的同时减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型的性能提升依赖于参数的扩展，但将所有世界知识压缩到参数中是不必要的，且对于边缘设备来说不切实际。

Method: 我们引入了小型语言模型，这些模型可以访问存储世界知识的大型分层参数记忆库。在预训练和推理过程中，我们获取一个小的、与上下文相关的记忆块并将其添加到模型中。

Result: 一个1.6亿参数的模型，通过从46亿参数的记忆库中获取1.8亿参数的记忆，其性能可与参数超过2倍的常规模型相媲美。

Conclusion: 通过实验，我们发现所提出的分层前馈记忆在各种Transformer架构中都能稳健工作，无论是在预训练期间还是后期添加。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [35] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 本文提出了一种基于校准对数似然分数的方法，以从多个不同的LLM中选择最佳响应，该方法在多个数据集上表现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 选择多个LLM中的最可靠响应仍然是一个挑战，尤其是在资源受限的环境中。现有的方法通常依赖于昂贵的外部验证器、人类评估者或需要单个模型多次采样的自洽技术。

Method: 我们提出了一种基于校准对数似然分数的方法，以从多个不同的LLM中选择最佳响应，该方法隐式地利用了这些模型的内在知识和置信度。

Result: 我们的方法在辩论（多轮LLM讨论）和非辩论（使用多个LLM的Best-of-N）设置中，在GSM8K、MMLU（6个子集）和ARC数据集上分别实现了约4%、3%和5%的改进。

Conclusion: 我们的方法在GSM8K、MMLU（6个子集）和ARC数据集上分别实现了约4%、3%和5%的改进，表明它在多LLM系统中选择最佳响应的有效性。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [36] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则的路由框架，通过有效利用数据库和文档的优势，提高了问答系统的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的系统主要依赖于非结构化文档，而忽略了关系型数据库，这在金融、医疗和科研等领域是不可或缺的基础设施。

Method: 我们提出了一个基于规则的路由框架，包括路由代理、规则生成专家代理和路径级元缓存。

Result: 实验结果表明，我们的框架在三个QA基准测试中 consistently 超过静态策略和学习的路由基线，实现了更高的准确性。

Conclusion: 我们的框架在三个QA基准测试中表现出色，优于静态策略和学习的路由基线，在保持适度计算成本的同时实现了更高的准确性。

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [37] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出了KnowledgeSmith，一个统一的框架，用于系统地理解大型语言模型（LLM）的知识更新机制。通过将编辑和遗忘视为约束优化问题的实例，并使用自动数据集生成器进行受控研究，实验揭示了知识传播、可塑性扩展、一致性和鲁棒性的细微见解。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏、孤立和小规模的评估，LLM的知识更新机制仍然很大程度上未被探索。例如，LLM是否与人类在修改某些知识方面相似？随着训练数据的增加，编辑和遗忘有什么不同？

Method: 本文提出了KnowledgeSmith，这是一个统一的框架，用于系统地理解LLM的更新机制。首先，将编辑和遗忘视为一个约束优化问题的实例。然后，提出一个自动数据集生成器，提供跨多个图级别和数据规模的结构化干预，以进行受控研究，了解不同的修改策略如何通过模型知识传播。

Result: 广泛的实验展示了关于知识传播、可塑性扩展、一致性和鲁棒性的细微见解。例如，我们的结果表明，LLM在不同层次的知识上不表现出与人类相似的更新，并且存在一致性-容量权衡。

Conclusion: 本文希望我们的发现能为设计更可靠和可扩展的策略提供建议。

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [38] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: 本文提出了一种系统框架，用于在数据库级别关联结构化领域陈述。我们展示了如何通过子字符串级匹配来检索与用户查询相关的结构化领域陈述，并在多个现实数据库模式上进行了评估，证明了数据库级别的结构化领域陈述比现有的特定查询文本领域陈述更实用和准确，同时基于子字符串匹配的检索方法比其他检索方法提供了更高的准确性。


<details>
  <summary>Details</summary>
Motivation: The performance of Large Language Models (LLMs) for translating Natural Language (NL) queries into SQL varies significantly across databases (DBs). NL queries are often expressed using a domain specific vocabulary, and mapping these to the correct SQL requires an understanding of the embedded domain expressions, their relationship to the DB schema structure. Existing benchmarks rely on unrealistic, ad-hoc query specific textual hints for expressing domain knowledge.

Method: We propose a systematic framework for associating structured domain statements at the database level. We present retrieval of relevant structured domain statements given a user query using sub-string level match.

Result: We evaluate on eleven realistic DB schemas covering diverse domains across five open-source and proprietary LLMs and demonstrate that (1) DB level structured domain statements are more practical and accurate than existing ad-hoc query specific textual domain statements, and (2) Our sub-string match based retrieval of relevant domain statements provides significantly higher accuracy than other retrieval approaches.

Conclusion: DB level structured domain statements are more practical and accurate than existing ad-hoc query specific textual domain statements, and our sub-string match based retrieval of relevant domain statements provides significantly higher accuracy than other retrieval approaches.

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [39] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 我们的研究发现，轻量级的提示工程可以激活纯文本训练的LLMs中的适当模态表示。


<details>
  <summary>Details</summary>
Motivation: 我们假设显式感官提示可以揭示隐含的结构，使文本仅LLM与专门的视觉和音频编码器更接近表示对齐。

Method: 我们测试了显式感官提示是否能揭示这种潜在结构，使仅文本训练的LLM与专门的视觉和音频编码器更接近表示对齐。

Result: 当感官提示告诉模型'看'或'听'时，它会提示模型将其下一个标记预测解析为似乎由未实际提供的潜在视觉或听觉证据所条件化的形式。

Conclusion: 我们的研究结果表明，轻量级的提示工程可以可靠地在纯文本训练的LLMs中激活适当的模态表示。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [40] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY is an AI-driven platform that improves patient-to-specialist routing, clinical consultations, and severity assessment by combining FSM and LLM, showing superior performance compared to humans.


<details>
  <summary>Details</summary>
Motivation: To facilitate patient-to-specialist routing, clinical consultations, and severity assessment of patients' conditions in a safe, efficient, and robust manner.

Method: CLARITY combines a Finite State Machine (FSM) for structured dialogue flows with collaborative agents that employ Large Language Model (LLM) to analyze symptoms and prioritize referrals to appropriate specialists. It is built on a modular microservices framework.

Result: CLARITY was integrated into a large-scale nation-wide inter-hospital IT platform, with over 55,000 content-rich user dialogues completed within two months of deployment, 2,500 of which were expert-annotated for validation. The validation results show that CLARITY surpasses human-level performance in terms of the first-attempt routing precision.

Conclusion: CLARITY surpasses human-level performance in terms of the first-attempt routing precision, naturally requiring up to 3 times shorter duration of the consultation than with a human.

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [41] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 本文研究了语言模型在PCFG上的学习动力学，发现变压器在所有子语法中并行减少损失，并且子语法预训练可以改善较小模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型模型取得了令人印象深刻的结果，但对其学习动力学了解甚少。我们希望通过研究小型模型在合成语言上的学习动力学，来理解语言模型如何获得语法。

Method: 我们研究了在从PCFG生成的合成语言上训练的小型模型的学习动力学，这使得可以精确控制语法复杂性、递归深度和子语法结构。我们证明了几种通用的、递归公式，用于训练损失和Kullback-Leibler差异，以及PCFG的子语法结构。

Result: 我们发现，与儿童不同，变压器在所有子语法中并行减少损失。我们进一步表明，子语法预训练可以改善较小模型的最终损失，并且预训练模型发展出更符合语法子结构的内部表示。最后，我们展示了模型在更深的递归结构上存在困难，这是甚至大型语言模型的局限性。

Conclusion: 我们的工作开启了对变压器在PCFG上的学习动力学的研究，作为一个多功能的测试平台，用于探测语言模型中的学习，开辟了一个有许多开放问题的研究方向。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [42] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: 本文提出 Cobweb 框架，通过层次原型实现更有效、更鲁棒和可解释的文档检索。


<details>
  <summary>Details</summary>
Motivation: 神经文档检索通常将语料库视为单一粒度的向量云，导致语料库结构未被充分利用且解释不清晰。

Method: 使用 Cobweb——一种层次感知框架——将句子嵌入组织成一个原型树，并通过粗到细的遍历对文档进行排序。内部节点作为概念原型，提供多粒度的相关性信号和通过检索路径的透明理由。实例化了两种推理方法：广义的最佳优先搜索和轻量级路径求和排名器。

Result: 在 MS MARCO 和 QQP 上评估了我们的方法，使用编码器（例如 BERT/T5）和解码器（GPT-2）表示。结果表明，我们的检索方法在强编码器嵌入上与点积搜索相当，而在 kNN 退化时仍保持稳健：使用 GPT-2 向量时，点积性能崩溃，而我们的方法仍然可以检索相关结果。

Conclusion: Cobweb 提供了具有竞争力的效果、对嵌入质量的改进鲁棒性、可扩展性以及通过层次原型的可解释检索。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [43] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的RAG系统评估方法，以提高评估的准确性和全面性。


<details>
  <summary>Details</summary>
Motivation: 评估RAG系统仍然是一项具有挑战性的任务，传统的评估指标难以有效捕捉现代LLM生成内容的关键特征。

Method: 本文扩展了RAGAS工具框架，将其转化为基于知识图谱的评估范式，以实现多跳推理和语义社区聚类，从而得出更全面的评分指标。

Result: 通过结合这些全面的评估标准，我们对RAG系统有了更深入的理解，并对其性能有了更细致的看法。此外，我们的KG-based评估方法对生成输出中的细微语义差异更为敏感。

Conclusion: 本文讨论了评估RAG系统的关键挑战，并指出了未来研究的潜在方向。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [44] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: 研究分析了三种语音语言模型中的模态适配器表示策略，发现不同模型采用不同的表示方法，这可能与语音编码器的训练目标有关。


<details>
  <summary>Details</summary>
Motivation: 了解语音语言模型中模态适配器如何转换表示，以提高模型对未见语言的处理能力。

Method: 通过找到与模态适配器表示最接近的解码器语言模型标记，分析三种语音语言模型中的模态适配器输出表示。

Result: 发现使用Whisper编码器的模型使用基于英语的中间语表示输入含义，而其他模型则用英语词汇表达语音特征。

Conclusion: 本文通过分析三种语音语言模型中的模态适配器输出表示，发现使用Whisper编码器的模型倾向于使用基于英语的中间语来表示输入含义，而其他模型则用英语词汇表达输入的语音特征。这表明语音编码器是否仅用于语音识别或也用于翻译可能影响模态适配器的表示策略。

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [45] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文介绍了第一个用于上下文归因的黄金标准HE评估框架，并评估了四种HE方法在不同场景下的表现，发现MechLight效果最好，但所有方法在处理长上下文时存在困难。


<details>
  <summary>Details</summary>
Motivation: 用户无法确定模型是从参数记忆还是提供的上下文获取信息，也无法识别哪些特定的上下文片段影响了响应。Highlight explanations (HEs) 可以指出影响模型输出的确切上下文片段和标记，但没有现有工作评估其有效性。

Method: 引入了第一个黄金标准HE评估框架，使用具有已知真实上下文使用的受控测试用例，避免了现有间接代理评估的局限性。

Result: MechLight在所有上下文场景中表现最佳，但所有方法在较长的上下文和位置偏差方面表现不佳。

Conclusion: 所有方法在较长的上下文和位置偏差方面表现不佳，表明需要新的方法来实现可靠的上下文利用解释。

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [46] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 研究发现用户与LLM聊天机器人和人类代理的沟通方式存在差异，这可能影响模型的表现。通过数据增强和推理时的重写策略来提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究用户与LLM聊天机器人和人类代理的沟通方式有何不同，因为这可能影响模型的表现。

Method: 在后训练阶段进行数据增强和推理时用户消息重写两种策略。

Result: 在风格多样的数据集上训练的模型显著优于仅在原始或风格统一的数据集上训练的模型，而推理时的重写效果不佳。

Conclusion: 这些见解有助于我们更好地适应模型，以改善LLM-用户交互体验。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [47] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: SoT is a training-free method that improves multilingual reasoning by transforming language-specific semantic information into language-agnostic structured representations.


<details>
  <summary>Details</summary>
Motivation: The capacity of reasoning has not been successfully transferred to non-high-resource languages due to resource constraints, which struggles with multilingual reasoning tasks.

Method: SoT is a training-free method that improves performance on multilingual reasoning through a multi-step transformation: Language Thinking Transformation and Structured Knowledge Transformation.

Result: Experimental results demonstrate that SoT outperforms several strong baselines on multiple multilingual reasoning benchmarks when adapting to various backbones of LLMs.

Conclusion: SoT can be integrated with other training-free strategies for further improvements and shows promising results on multiple multilingual reasoning benchmarks.

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [48] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: 本文是对多模态大语言模型自我改进的首次全面调查，从数据收集、数据组织和模型优化三个方面进行了讨论。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供对多模态大语言模型（MLLMs）自我改进的全面概述。

Method: 本文从三个角度讨论了方法：1) 数据收集，2) 数据组织，3) 模型优化。

Result: 本文提供了当前文献的结构化概述，并包括常用的评估和下游应用。

Conclusion: 本文最后指出了开放性挑战和未来研究方向。

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [49] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文提出了一个理论上的方法来量化情境问答中的认知不确定性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前的不确定性量化（UQ）研究主要集中在封闭书事实问答（QA）上，而情境QA仍未被探索，尽管它在现实世界应用中很重要。

Method: 我们提出了一种理论上有根据的方法来量化认知不确定性。我们首先引入了一个任务无关的、基于标记的不确定性度量，然后通过分解这个度量来隔离认知部分，并通过一个完美提示的理想模型近似真实分布。我们还推导了认知不确定性的上界，并将其解释为给定模型的隐藏表示与理想模型之间的语义特征差距。

Result: 我们的方法在多个QA基准测试中表现出色，比最先进的无监督和监督UQ方法有显著提升，同时推理开销很小。

Conclusion: 我们的方法在多个QA基准测试中显著优于最先进的无监督（采样-free和采样-based）和监督UQ方法，在分布内和分布外设置中都表现优异。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [50] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本文通过生存分析研究了大型语言模型在多轮对话中的鲁棒性，发现突然的语义漂移会增加失败风险，而渐进的漂移则有助于延长对话。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架专注于静态基准和单次评估，未能捕捉到现实世界交互中对话退化的时序动态。因此，本文旨在通过生存分析来研究对话AI的鲁棒性。

Method: 本文采用生存建模框架，包括Cox比例风险模型、加速失效时间模型和随机生存森林方法，对36,951个对话回合进行了全面的生存分析。

Result: 研究发现，突然的提示到提示（P2P）语义漂移是灾难性的，显著增加了对话失败的风险。相比之下，渐进的累积漂移具有高度保护作用，大大降低了失败风险并实现了更长的对话。AFT模型与交互项表现出优越性能，具有出色的区分度和极好的校准效果。

Conclusion: 本文将生存分析作为评估LLM鲁棒性的强大范式，提供了设计弹性对话代理的具体见解，并挑战了关于对话AI系统语义一致性的普遍假设。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [51] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 本文通过收集14个旅行领域数据集并分析LLM的表现，发现通用基准测试结果不足以理解模型在低资源任务中的性能，且推理对较小的LLM有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试的结果未能提供有关模型能力的足够信息，使得在这些领域开发有效的解决方案变得困难。

Method: 我们收集了14个旅行领域数据集，涵盖了7种常见的NLP任务，并分析了LLM的表现。

Result: 我们报告了LLM在各种任务中的准确性、扩展行为和推理能力。结果确认了通用基准测试结果不足以理解模型在低资源任务中的性能。

Conclusion: 通用基准测试结果不足以理解模型在低资源任务中的性能。尽管训练FLOPs的数量，开箱即用的LLM在复杂、领域特定的场景中遇到了性能瓶颈。此外，推理对较小的LLM提供了更大的提升，使模型在某些任务上成为更好的评判者。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [52] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于策略梯度的生成对抗网络（PGMEL），用于多模态实体链接任务，通过选择高质量的负样本来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文献中尚未探索在MEL框架下选择高质量负样本的可能性，因此我们试图填补这一空白。

Method: 我们提出了基于策略梯度的生成对抗网络用于多模态实体链接（PGMEL），在生成对抗设置中解决多模态实体链接问题。

Result: 实验结果基于Wiki-MEL、Richpedia-MEL和WikiDiverse数据集，表明PGMEL能够通过选择具有挑战性的负样本学习到有意义的表示，并优于最先进的方法。

Conclusion: 实验结果表明，PGMEL通过选择具有挑战性的负样本学习到了有意义的表示，并优于最先进的方法。

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [53] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: 本文提出了一种基于对比学习的评估框架，并引入了一个新的数据集IndiCASA，用于评估大型语言模型中的刻板偏见，结果显示所有模型都表现出一定程度的刻板偏见，特别是与残疾相关的偏见，这表明需要更公平的模型开发。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在关键领域获得了显著的关注，但由于它们在高风险应用中的日益部署，需要严格评估嵌入偏见，特别是在文化多样的背景下，如印度，现有的基于嵌入的偏见评估方法往往无法捕捉到细微的刻板印象。

Method: 我们提出了一种基于对比学习训练的编码器的评估框架，该框架通过嵌入相似性捕捉细粒度偏见，并引入了一个新的数据集IndiCASA（基于IndiBias的上下文对齐刻板印象和反刻板印象），包含2,575个经过人工验证的句子，涵盖五个人口统计轴：种姓、性别、宗教、残疾和经济社会地位。

Result: 我们对多个开源权重LLM的评估显示，所有模型都表现出一定程度的刻板偏见，其中与残疾相关的偏见尤为持久，而宗教偏见通常较低，这可能是由于全球去偏见努力所致，这表明需要更公平的模型开发。

Conclusion: 我们的评估显示，所有模型都表现出一定程度的刻板偏见，其中与残疾相关的偏见尤为持久，而宗教偏见通常较低，这可能是由于全球去偏见努力所致，这表明需要更公平的模型开发。

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [54] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: 本文提出了一种基于自我意识的强化学习方法，在减少数据需求的情况下显著提升了大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习训练需要大量数据和标注，而本文旨在通过最小化数据依赖来提高大型语言模型的性能。

Method: 本文提出了一种基于自我意识的强化学习方法，包括自我意识难度预测和自我意识突破限制两种机制，通过让模型自主提出任务并尝试解决来提升其能力。

Result: 在九个基准测试中，实验结果显示相对改进率为53.8%，且仅使用了不到1.2%的额外数据。

Conclusion: 实验结果表明，自我意识强化学习在减少数据需求的同时有效提升了大型语言模型的性能，展示了自我进化代理训练的潜力。

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [55] [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
*Tien Phat Nguyen,Vu Minh Ngo,Tung Nguyen,Linh Van Ngo,Duc Anh Nguyen,Sang Dinh,Trung Le*

Main category: cs.CL

TL;DR: XTRA 是一种新的跨语言主题建模框架，通过表示对齐和主题对齐来提高主题的可解释性和跨语言一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在主题多样性方面取得了一些改进，但往往难以确保高主题连贯性和跨语言的一致性对齐。

Method: XTRA 是一种新的框架，结合了词袋建模和多语言嵌入，引入了两个核心组件：表示对齐和主题对齐。

Result: 实验表明，XTRA 在多语言语料库中显著优于强基线，在主题连贯性、多样性和对齐质量方面表现优异。

Conclusion: XTRA 显著优于强基线，在主题连贯性、多样性和对齐质量方面表现出色。

Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across
languages. Several methods have been proposed to address this problem,
leveraging both traditional and neural approaches. While previous methods have
achieved some improvements in topic diversity, they often struggle to ensure
high topic coherence and consistent alignment across languages. We propose XTRA
(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a
novel framework that unifies Bag-of-Words modeling with multilingual
embeddings. XTRA introduces two core components: (1) representation alignment,
aligning document-topic distributions via contrastive learning in a shared
semantic space; and (2) topic alignment, projecting topic-word distributions
into the same space to enforce crosslingual consistency. This dual mechanism
enables XTRA to learn topics that are interpretable (coherent and diverse) and
well-aligned across languages. Experiments on multilingual corpora confirm that
XTRA significantly outperforms strong baselines in topic coherence, diversity,
and alignment quality. Code and reproducible scripts are available at https:
//github.com/tienphat140205/XTRA.

</details>


### [56] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: 本文解决了人格评估中的数据不足和模型可解释性问题，提出了一个名为SIMPA的计算框架，该框架在保持高可解释性的同时能够提供准确的人格评估。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模的人格标注数据集以及人格心理学与自然语言处理之间的脱节，自动化人格评估方法面临有效性和可解释性的限制。

Method: 本文通过构建MBTI9k和PANDORA数据集，并开发了SIMPA框架来解决人格评估中的挑战。

Result: 实验表明，人口统计变量影响模型有效性，而SIMPA框架在保持高可解释性和效率的同时，能够提供与人类评估相当的人格评估结果。

Conclusion: 本文提出了SIMPA框架，该框架在人格评估中表现出色，并且具有广泛的应用潜力。

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [57] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: StepChain GraphRAG is a framework that enhances multi-hop question answering by combining question decomposition with a BFS reasoning flow. It achieves state-of-the-art results on multiple datasets and improves explainability by preserving the chain-of-thought.


<details>
  <summary>Details</summary>
Motivation: Recent progress in retrieval-augmented generation (RAG) has led to more accurate and interpretable multi-hop question answering (QA). Yet, challenges persist in integrating iterative reasoning steps with external knowledge retrieval.

Method: StepChain GraphRAG, a framework that unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow for enhanced multi-hop QA. Our approach first builds a global index over the corpus; at inference time, only retrieved passages are parsed on-the-fly into a knowledge graph, and the complex query is split into sub-questions. For each sub-question, a BFS-based traversal dynamically expands along relevant edges, assembling explicit evidence chains without overwhelming the language model with superfluous context.

Result: Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1 scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).

Conclusion: StepChain GraphRAG also fosters enhanced explainability by preserving the chain-of-thought across intermediate retrieval steps. We conclude by discussing how future work can mitigate the computational overhead and address potential hallucinations from large language models to refine efficiency and reliability in multi-hop QA.

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [58] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: 本研究验证了五个领先的大型语言模型在21,955个物种上的表现，发现它们在分类学分类方面表现良好，但在保护推理方面存在明显不足，并且存在系统性偏差。研究建议采用混合方法，让大型语言模型增强专家能力，而人类专家保留对风险评估和政策的唯一权威。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）正在迅速被用于解决生物多样性危机，但它们在物种评估中的可靠性尚不确定，因此需要进行系统验证。

Method: 本研究系统地验证了五个领先的模型在21,955个物种上的表现，涵盖了四个核心IUCN红色名录评估组件：分类学、保护状态、分布和威胁。

Result: 研究发现模型在分类学分类方面表现出色（94.9%），但在保护推理方面表现不佳（状态评估为27.2%）。此外，模型表现出系统性偏差，偏向于有魅力的脊椎动物，这可能加剧现有的保护不平等。

Conclusion: 研究结果表明，大型语言模型在信息检索方面是强大的工具，但在基于判断的决策中需要人类监督。建议采用混合方法，让大型语言模型增强专家能力，而人类专家保留对风险评估和政策的唯一权威。

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [59] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: 本文提出了基于约束满足问题（CSP）的Wordle求解方法，通过引入CSP-Aware Entropy和Probabilistic CSP框架，显著提升了求解性能和鲁棒性，并验证了其跨语言的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的求解器依赖于信息论熵最大化或基于频率的启发式方法，但缺乏对约束的正式处理。本文旨在提供一种更全面的CSP模型来改进Wordle的求解。

Method: 本文提出了CSP-Aware Entropy和Probabilistic CSP框架，结合贝叶斯单词频率先验与逻辑约束，并进行了跨语言验证和鲁棒性分析。

Result: CSP-Aware Entropy在2,315个英文单词上实现了3.54次平均猜测，成功率为99.9%，比Forward Checking提高了1.7%。在10%噪声下，CSP-aware方法保持了5.3个百分点的优势，而Probabilistic CSP在所有噪声水平下都达到了100%的成功率。

Conclusion: 本文通过将Wordle建模为一个全面的约束满足问题（CSP），并引入了新的基于约束的求解策略，证明了基于原则的约束满足技术在结构化谜题解决领域优于传统的信息理论和基于学习的方法。

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [60] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: SRGen 是一种新的测试时自我反思框架，能够在生成过程中主动进行自我修正，从而提高大型语言模型的推理能力和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的自我反思方法要么对完整草稿进行修改，要么通过昂贵的训练学习自我纠正，这两种方法都是被动且低效的。因此，需要一种更高效、主动的自我反思机制。

Method: SRGen 通过动态熵阈值检测高不确定性令牌，并为每个识别的令牌训练特定的校正向量，以利用已生成的上下文进行自我反思生成，从而纠正令牌概率分布。

Result: SRGen 在挑战性的数学推理基准和多样化的大型语言模型上进行了评估，结果表明它可以显著提高模型的推理能力。例如，在 AIME2024 上，使用 DeepSeek-R1-Distill-Qwen-7B 时，SRGen 在 Pass@1 和 Cons@5 上分别取得了 +12.0% 和 +13.3% 的绝对提升。

Conclusion: SRGen 是一种轻量级的测试时框架，能够通过在不确定点进行自我反思来提高大型语言模型的推理能力。它可以在不增加太多计算开销的情况下，显著减少高不确定性点的错误概率，并且可以与其他训练时和测试时的技术相结合。

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [61] [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
*Yohan Lee,Yongwoo Song,Sangyeop Kim*

Main category: cs.CL

TL;DR: 我们提出了CDR基准，用于评估对话数据检索系统，并发现现有模型在这一任务上的表现有限。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入模型在对话数据检索方面的性能有限，需要一个可靠的基准来衡量对话数据检索的表现。

Method: 我们提出了对话数据检索（CDR）基准，这是第一个全面的测试集，用于评估检索对话数据以获取产品见解的系统。

Result: 我们的评估显示，即使最好的模型也只能达到约NDCG@10的0.51，揭示了文档和对话数据检索能力之间的显著差距。

Conclusion: 我们的工作识别了对话数据检索中的独特挑战，并提供了实用的查询模板和不同任务类别的详细错误分析。

Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

</details>


### [62] [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
*Jingqi Zhang,Ruibo Chen,Yingqing Yang,Peihua Mai,Heng Huang,Yan Pang*

Main category: cs.CL

TL;DR: TRACE is a framework for detecting the use of copyrighted datasets in LLM fine-tuning without requiring access to internal signals or reference datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting unauthorized use of copyrighted datasets in LLM fine-tuning have limitations, such as requiring access to internal signals or relying on handcrafted prompts or reference datasets.

Method: TRACE is a framework that rewrites datasets with distortion-free watermarks guided by a private key, and it exploits the radioactivity effect of fine-tuning on watermarked data with an entropy-gated procedure.

Result: TRACE consistently achieves significant detections (p<0.05) across diverse datasets and model families, often with strong statistical evidence, and supports multi-dataset attribution while remaining robust after continued pretraining on non-watermarked corpora.

Conclusion: TRACE provides a practical route to reliable black-box verification of copyrighted dataset usage in LLM fine-tuning.

Abstract: Large Language Models (LLMs) are increasingly fine-tuned on smaller,
domain-specific datasets to improve downstream performance. These datasets
often contain proprietary or copyrighted material, raising the need for
reliable safeguards against unauthorized use. Existing membership inference
attacks (MIAs) and dataset-inference methods typically require access to
internal signals such as logits, while current black-box approaches often rely
on handcrafted prompts or a clean reference dataset for calibration, both of
which limit practical applicability. Watermarking is a promising alternative,
but prior techniques can degrade text quality or reduce task performance. We
propose TRACE, a practical framework for fully black-box detection of
copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets
with distortion-free watermarks guided by a private key, ensuring both text
quality and downstream utility. At detection time, we exploit the radioactivity
effect of fine-tuning on watermarked data and introduce an entropy-gated
procedure that selectively scores high-uncertainty tokens, substantially
amplifying detection power. Across diverse datasets and model families, TRACE
consistently achieves significant detections (p<0.05), often with extremely
strong statistical evidence. Furthermore, it supports multi-dataset attribution
and remains robust even after continued pretraining on large non-watermarked
corpora. These results establish TRACE as a practical route to reliable
black-box verification of copyrighted dataset usage. We will make our code
available at: https://github.com/NusIoraPrivacy/TRACE.

</details>


### [63] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 本文开发并评估了一个基于RAG系统的查询NICE临床指南的工具，展示了其在医疗领域应用生成式AI的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于这些指南的长度和数量，它们在时间有限的医疗系统中的利用受到阻碍，该项目通过创建一个能够根据自然语言查询提供精确匹配信息的系统来解决这一挑战。

Method: 本文开发并评估了一个检索增强生成（RAG）系统，用于使用大型语言模型（LLM）查询英国国家健康与护理卓越研究所（NICE）的临床指南。该系统采用了混合嵌入机制，并在包含10,195个文本片段的数据库上进行了评估。

Result: 该系统在评估中表现出色，平均倒数排名（MRR）为0.814，第一个片段的召回率为81%，前十个检索片段的召回率为99.1%。在手动整理的70个问答对数据集上评估时，RAG增强模型显示出显著的性能提升。

Conclusion: 本研究确立了RAG作为一种在医疗领域应用生成式AI的有效、可靠和可扩展的方法，使医疗指南的获取更具成本效益。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [64] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: 本研究区分了描述性语义和表达性语义，并通过实验验证了它们与情绪的关系，为SER在人机交互中的应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: Speech Emotion Recognition (SER) 对于改善人机交互至关重要，但其准确性受到语音中情感细微差别的限制。

Method: 我们区分了描述性语义和表达性语义，并通过实验验证了它们与意图情绪和唤起情绪的相关性。

Result: 我们发现描述性语义与意图情绪一致，而表达性语义与唤起情绪相关。

Conclusion: 我们的研究结果为SER在人机交互中的应用提供了指导，并为更上下文感知的AI系统铺平了道路。

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [65] [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
*Oriol Pareras,Gerard I. Gállego,Federico Costa,Cristina España-Bonet,Javier Hernando*

Main category: cs.CL

TL;DR: 本文比较了CoT和直接提示在不同规模的S2TT数据下的表现，发现随着数据量增加，直接提示更有效。


<details>
  <summary>Details</summary>
Motivation: 研究CoT和直接提示在不同规模的S2TT数据下的表现，以确定哪种方法更有效。

Method: 通过将ASR语料库的转录文本伪标记为六种欧洲语言，并在不同数据规模下使用两种提示策略训练基于LLM的S2TT系统，对CoT和直接提示进行系统比较。

Result: 结果表明，随着数据量的增加，直接提示表现更加一致，可能在更大规模的S2TT资源中更有效。

Conclusion: 随着更大规模的S2TT资源的创建，直接提示可能变得更加有效。

Abstract: Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based
models, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,
where the model is guided to first transcribe the speech and then translate it.
CoT typically outperforms direct prompting primarily because it can exploit
abundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)
datasets to explicitly model its steps. In this paper, we systematically
compare CoT and Direct prompting under increasing amounts of S2TT data. To this
end, we pseudo-label an ASR corpus by translating its transcriptions into six
European languages, and train LLM-based S2TT systems with both prompting
strategies at different data scales. Our results show that Direct improves more
consistently as the amount of data increases, suggesting that it may become a
more effective approach as larger S2TT resources are created.

</details>


### [66] [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
*Beth Pearson,Ahmed Adnan,Zahraa Abdallah*

Main category: cs.CL

TL;DR: 本文提出了一种结合Llama 3.1和命名实体识别（NER）的语义相似性评分方法，称为Llama-EntScore。该方法能够生成定量的相似性评分，并提供评分的解释，以帮助初学者在审查和改进报告时获得有价值的指导。实验结果表明，该方法在与放射科医生提供的地面真实评分比较时，达到了67%的精确匹配准确率和93%的准确率在±1范围内，优于单独使用LLMs或NER的方法。


<details>
  <summary>Details</summary>
Motivation: 识别初步报告和最终报告之间的语义差异对于初级医生来说至关重要，既可以作为培训工具，也可以帮助发现临床知识的差距。尽管放射学中的AI是一个快速发展的领域，但大型语言模型（LLMs）的应用仍然具有挑战性，因为需要专门的领域知识。因此，本文旨在探索LLMs在提供可解释和准确的报告比较方面的潜力。

Method: 本文首先比较了几种大型语言模型（LLMs）在比较放射学报告中的表现，然后评估了一种基于命名实体识别（NER）的传统方法。然而，这两种方法在提供语义相似性的准确反馈方面都存在局限性。为此，我们提出了Llama-EntScore，这是一种结合Llama 3.1和NER的语义相似性评分方法，具有可调权重以强调或弱化特定类型的差异。

Result: 本文提出的Llama-EntScore方法在与放射科医生提供的地面真实评分比较时，达到了67%的精确匹配准确率和93%的准确率在±1范围内，优于单独使用LLMs或NER的方法。此外，该方法能够生成定量的相似性评分，并提供评分的解释，以帮助初学者在审查和改进报告时获得有价值的指导。

Conclusion: 本文提出了一种结合Llama 3.1和命名实体识别（NER）的语义相似性评分方法，称为Llama-EntScore。该方法能够生成定量的相似性评分，并提供评分的解释，以帮助初学者在审查和改进报告时获得有价值的指导。实验结果表明，该方法在与放射科医生提供的地面真实评分比较时，达到了67%的精确匹配准确率和93%的准确率在±1范围内，优于单独使用LLMs或NER的方法。

Abstract: Radiology report evaluation is a crucial part of radiologists' training and
plays a key role in ensuring diagnostic accuracy. As part of the standard
reporting workflow, a junior radiologist typically prepares a preliminary
report, which is then reviewed and edited by a senior radiologist to produce
the final report. Identifying semantic differences between preliminary and
final reports is essential for junior doctors, both as a training tool and to
help uncover gaps in clinical knowledge. While AI in radiology is a rapidly
growing field, the application of large language models (LLMs) remains
challenging due to the need for specialised domain knowledge. In this paper, we
explore the ability of LLMs to provide explainable and accurate comparisons of
reports in the radiology domain. We begin by comparing the performance of
several LLMs in comparing radiology reports. We then assess a more traditional
approach based on Named-Entity-Recognition (NER). However, both approaches
exhibit limitations in delivering accurate feedback on semantic similarity. To
address this, we propose Llama-EntScore, a semantic similarity scoring method
using a combination of Llama 3.1 and NER with tunable weights to emphasise or
de-emphasise specific types of differences. Our approach generates a
quantitative similarity score for tracking progress and also gives an
interpretation of the score that aims to offer valuable guidance in reviewing
and refining their reporting. We find our method achieves 67% exact-match
accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided
ground truth scores - outperforming both LLMs and NER used independently. Code
is available at:
\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}

</details>


### [67] [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
*Jacobo Romero-Díaz,Gerard I. Gállego,Oriol Pareras,Federico Costa,Javier Hernando,Cristina España-Bonet*

Main category: cs.CL

TL;DR: CoT方法在S2TT任务中表现不佳，需改进以更好地利用语音信息。


<details>
  <summary>Details</summary>
Motivation: 解决S2TT系统中误差传播和无法利用韵律或其他声学线索的问题。

Method: 通过归因方法、带有损坏转录本的鲁棒性评估以及对韵律的感知分析CoT。

Result: CoT主要依赖转录本，而很少利用语音；添加直接S2TT数据或噪声转录本注入可以提高鲁棒性和语音归因。

Conclusion: CoT方法并未充分发挥语音信息的优势，需要设计能够显式整合声学信息的架构。

Abstract: Speech-to-Text Translation (S2TT) systems built from Automatic Speech
Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major
limitations: error propagation and the inability to exploit prosodic or other
acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,
with the expectation that jointly accessing speech and transcription will
overcome these issues. Analyzing CoT through attribution methods, robustness
evaluations with corrupted transcripts, and prosody-awareness, we find that it
largely mirrors cascaded behavior, relying mainly on transcripts while barely
leveraging speech. Simple training interventions, such as adding Direct S2TT
data or noisy transcript injection, enhance robustness and increase speech
attribution. These findings challenge the assumed advantages of CoT and
highlight the need for architectures that explicitly integrate acoustic
information into translation.

</details>


### [68] [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
*Zhaojun Sun,Xuzhou Zhu,Xuanhe Zhou,Xin Tong,Shuo Wang,Jie Fu,Guoliang Li,Zhiyuan Liu,Fan Wu*

Main category: cs.CL

TL;DR: 本文提出了SurveyBench，一种用于评估自动生成调查质量的新框架，结果表明现有方法在内容质量上低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成调查的方法（如LLM4Survey）输出的质量未能达到人类标准，缺乏一个严谨且与读者对齐的基准来全面揭示其不足之处。

Method: 提出了一种细粒度的、基于测验的评估框架SurveyBench，包括典型调查主题来源、多方面的指标层次结构以及双模式评估协议。

Result: SurveyBench能够有效评估和挑战现有的LLM4Survey方法，显示出它们在内容质量上的差距。

Conclusion: SurveyBench有效地挑战了现有的LLM4Survey方法，表明这些方法在内容质量上平均比人类低21%。

Abstract: Academic survey writing, which distills vast literature into a coherent and
insightful narrative, remains a labor-intensive and intellectually demanding
task. While recent approaches, such as general DeepResearch agents and
survey-specialized methods, can generate surveys automatically (a.k.a.
LLM4Survey), their outputs often fall short of human standards and there lacks
a rigorous, reader-aligned benchmark for thoroughly revealing their
deficiencies. To fill the gap, we propose a fine-grained, quiz-driven
evaluation framework SurveyBench, featuring (1) typical survey topics source
from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;
(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,
coverage breadth, logical coherence), content quality (e.g., synthesis
granularity, clarity of insights), and non-textual richness; and (3) a
dual-mode evaluation protocol that includes content-based and quiz-based
answerability tests, explicitly aligned with readers' informational needs.
Results show SurveyBench effectively challenges existing LLM4Survey approaches
(e.g., on average 21% lower than human in content-based evaluation).

</details>


### [69] [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
*Ej Zhou,Caiqi Zhang,Tiancheng Hu,Chengzu Li,Nigel Collier,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文研究了多语言环境下大型语言模型的置信度校准问题，发现非英语语言存在系统性较差的校准问题，并提出一种无需训练的方法来改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 由于多语言环境下的置信度校准研究不足，本文旨在探索多语言校准问题，并提出改进方法以提高模型的可靠性。

Method: 本文进行了大规模、系统的研究，分析了多语言校准问题，并通过层分析发现了中间层提供更可靠信号的关键见解。基于此，引入了一套无需训练的方法，如语言感知置信度集成（LACE）。

Result: 研究发现非英语语言在置信度校准方面存在系统性较差的问题，而中间层提供了更可靠和更好的校准信号。通过LACE方法，可以自适应地选择每个特定语言的最佳层组合。

Conclusion: 本文的研究揭示了英语中心对齐的隐性成本，并通过超越最终层的方法为构建更全球公平和值得信赖的LLM提供了新的路径。

Abstract: Confidence calibration, the alignment of a model's predicted confidence with
its actual accuracy, is crucial for the reliable deployment of Large Language
Models (LLMs). However, this critical property remains largely under-explored
in multilingual contexts. In this work, we conduct the first large-scale,
systematic studies of multilingual calibration across six model families and
over 100 languages, revealing that non-English languages suffer from
systematically worse calibration. To diagnose this, we investigate the model's
internal representations and find that the final layer, biased by
English-centric training, provides a poor signal for multilingual confidence.
In contrast, our layer-wise analysis uncovers a key insight that
late-intermediate layers consistently offer a more reliable and
better-calibrated signal. Building on this, we introduce a suite of
training-free methods, including Language-Aware Confidence Ensemble (LACE),
which adaptively selects an optimal ensemble of layers for each specific
language. Our study highlights the hidden costs of English-centric alignment
and offer a new path toward building more globally equitable and trustworthy
LLMs by looking beyond the final layer.

</details>


### [70] [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
*Katherine Thai,Bradley Emi,Elyas Masrour,Mohit Iyyer*

Main category: cs.CL

TL;DR: 本文提出了一种方法来检测AI编辑的文本，并展示了AI对人类写作的修改程度可以被检测到。


<details>
  <summary>Details</summary>
Motivation: 之前的文献主要关注检测完全由AI生成的文本，但我们发现AI编辑的文本与人工撰写的和AI生成的文本是可区分的。

Method: 我们提出了使用轻量级相似性度量来量化文本中AI编辑的程度，并通过人类注释器验证这些度量。然后，我们训练了EditLens，这是一个回归模型，可以预测文本中AI编辑的数量。

Result: 我们的模型在二元（F1=94.7%）和三元（F1=90.4%）分类任务中都达到了最先进的性能，能够区分人工、AI和混合写作。

Conclusion: 我们展示了AI编辑的文本可以被检测，并且可以检测到AI对人类写作所做的更改程度，这对作者身份归属、教育和政策都有影响。此外，我们公开了模型和数据集以鼓励进一步的研究。

Abstract: A significant proportion of queries to large language models ask them to edit
user-provided text, rather than generate new text from scratch. While previous
work focuses on detecting fully AI-generated text, we demonstrate that
AI-edited text is distinguishable from human-written and AI-generated text.
First, we propose using lightweight similarity metrics to quantify the
magnitude of AI editing present in a text given the original human-written text
and validate these metrics with human annotators. Using these similarity
metrics as intermediate supervision, we then train EditLens, a regression model
that predicts the amount of AI editing present within a text. Our model
achieves state-of-the-art performance on both binary (F1=94.7%) and ternary
(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.
Not only do we show that AI-edited text can be detected, but also that the
degree of change made by AI to human writing can be detected, which has
implications for authorship attribution, education, and policy. Finally, as a
case study, we use our model to analyze the effects of AI-edits applied by
Grammarly, a popular writing assistance tool. To encourage further research, we
commit to publicly releasing our models and dataset.

</details>


### [71] [Neural Correlates of Language Models Are Specific to Human Language](https://arxiv.org/abs/2510.03156)
*Iñigo Parra*

Main category: cs.CL

TL;DR: 本研究验证了大型语言模型的隐藏状态与脑响应之间的相关性是稳健的，并进一步探讨了模型的生物合理性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 先前的工作显示了大型语言模型的隐藏状态与fMRI脑响应之间的相关性，这些相关性被当作这些模型和脑状态之间表征相似性的证据。然而，需要验证这些结果是否稳健。

Method: 该研究测试了先前结果对几个可能担忧的鲁棒性，包括维度约简、新的相似性度量、模型训练数据和位置编码的存在。

Result: 研究显示：(i) 在维度约简后仍能找到先前的结果，因此不是由于维度灾难；(ii) 使用新的相似性度量时，先前的结果得到确认；(iii) 脑表示与模型表示之间的相关性特定于训练在人类语言上的模型；(iv) 结果依赖于模型中位置编码的存在。

Conclusion: 这些结果确认并加强了之前的研究结果，并有助于关于最先进的大型语言模型的生物合理性与可解释性的辩论。

Abstract: Previous work has shown correlations between the hidden states of large
language models and fMRI brain responses, on language tasks. These correlations
have been taken as evidence of the representational similarity of these models
and brain states. This study tests whether these previous results are robust to
several possible concerns. Specifically this study shows: (i) that the previous
results are still found after dimensionality reduction, and thus are not
attributable to the curse of dimensionality; (ii) that previous results are
confirmed when using new measures of similarity; (iii) that correlations
between brain representations and those from models are specific to models
trained on human language; and (iv) that the results are dependent on the
presence of positional encoding in the models. These results confirm and
strengthen the results of previous research and contribute to the debate on the
biological plausibility and interpretability of state-of-the-art large language
models.

</details>


### [72] [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
*Xuan Xu,Haolun Li,Zhongliang Yang,Beilin Chu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的新主题建模范式，并通过实验验证了传统神经主题模型的过时性。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型依赖于推理和生成网络来学习潜在主题分布，而本文旨在探索大语言模型时代的新主题建模范式，并验证其是否能够超越传统神经主题模型。

Method: 本文提出了一种简单但实用的方法，直接利用大语言模型进行主题建模任务，包括采样数据子集、生成主题和代表性文本以及使用关键词匹配进行文本分配。此外，还通过零样本提示方法对长文本生成范式是否优于传统神经主题模型进行了研究。

Result: 本文通过系统比较传统神经主题模型（NTMs）和大语言模型（LLMs）在主题质量和性能方面的表现，验证了“大多数传统神经主题模型已经过时”的说法。

Conclusion: 本文认为，传统主题模型在很大程度上已经过时，并提出了一种基于大语言模型的新主题建模范式，该范式通过长文本生成任务来实现。

Abstract: Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."

</details>


### [73] [Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2510.03202)
*Abteen Ebrahimi,Adam Wiemerslage,Katharina von der Wense*

Main category: cs.CL

TL;DR: NN-Rank是一种利用多语言模型和未标记目标语言数据进行源语言排名的算法，在跨语言迁移任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了提高跨语言迁移的效果，需要一种有效的源语言排名方法。

Method: NN-Rank算法利用多语言模型的隐藏表示和未标记的目标语言数据来进行源语言排名。

Result: 在使用领域内数据时，NN-Rank在POS和NER任务上分别实现了最高35.56 NDCG和18.14 NDCG的平均改进。

Conclusion: NN-Rank在使用领域内数据时优于最先进的基线，并且即使在没有目标语言数据的情况下，也能保持竞争力。

Abstract: We present NN-Rank, an algorithm for ranking source languages for
cross-lingual transfer, which leverages hidden representations from
multilingual models and unlabeled target-language data. We experiment with two
pretrained multilingual models and two tasks: part-of-speech tagging (POS) and
named entity recognition (NER). We consider 51 source languages and evaluate on
56 and 72 target languages for POS and NER, respectively. When using in-domain
data, NN-Rank beats state-of-the-art baselines that leverage lexical and
linguistic features, with average improvements of up to 35.56 NDCG for POS and
18.14 NDCG for NER. As prior approaches can fall back to language-level
features if target language data is not available, we show that NN-Rank remains
competitive using only the Bible, an out-of-domain corpus available for a large
number of languages. Ablations on the amount of unlabeled target data show
that, for subsets consisting of as few as 25 examples, NN-Rank produces
high-quality rankings which achieve 92.8% of the NDCG achieved using all
available target data for ranking.

</details>


### [74] [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Léo Boisvert,Massimo Caccia,Jérémy Espinas,Alexandre Aussem,Véronique Eglin,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 本文介绍了一种名为FocusAgent的方法，通过使用轻量级LLM检索器从网页观察中提取最相关的内容，以提高Web代理的效率和安全性。实验表明，FocusAgent在减少观察大小的同时保持了与强基线相当的性能，并有效降低了提示注入攻击的成功率。


<details>
  <summary>Details</summary>
Motivation: Web代理必须处理长网页观察以完成用户目标；这些页面通常超过数万token。这会饱和上下文限制并增加计算成本；此外，处理完整页面会使代理面临安全风险，如提示注入。现有的修剪策略要么丢弃相关内容，要么保留不相关信息，导致次优动作预测。

Method: 我们引入了FocusAgent，这是一种简单而有效的策略，利用轻量级LLM检索器从可访问性树（AxTree）观察中提取最相关行，由任务目标引导。

Result: 在WorkArena和WebArena基准测试中的实验表明，FocusAgent与强基线性能相当，同时将观察大小减少了50%以上。此外，FocusAgent的一个变体显著降低了提示注入攻击的成功率，包括横幅和弹出窗口攻击，同时在无攻击设置中保持任务成功性能。

Conclusion: 我们的结果表明，基于LLM的检索是一种实用且稳健的策略，用于构建高效、有效和安全的网络代理。

Abstract: Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

</details>


### [75] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的多LLM通信范式C2C，通过直接语义通信提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多LLM系统通过文本进行通信，导致语义信息丢失和生成延迟。因此，研究者希望探索LLM之间超越文本的通信方式。

Method: C2C使用神经网络将源模型的KV缓存与目标模型的KV缓存进行投影和融合，实现直接的语义传输，并通过可学习的门控机制选择受益的目标层。

Result: C2C在平均准确率上比单个模型高8.5-10.5%，比文本通信范式高3.0-5.0%，并且平均延迟降低了2.0倍。

Conclusion: C2C能够通过直接的语义通信提升多LLM系统的性能和效率，同时减少延迟。

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


### [76] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: Self-Anchor is a new method that helps Large Language Models focus on important steps during reasoning tasks, improving their performance without needing to be retrained.


<details>
  <summary>Details</summary>
Motivation: To solve complex reasoning tasks for Large Language Models (LLMs), prompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning, but as reasoning chains extend, critical intermediate steps and the original prompt will be buried in the context, receiving insufficient attention and leading to errors.

Method: Self-Anchor is a novel pipeline that leverages the inherent structure of reasoning to steer LLM attention by decomposing reasoning trajectories into structured plans and aligning the model's attention to the most relevant inference steps.

Result: Self-Anchor outperforms SOTA prompting methods across six benchmarks and significantly reduces the performance gap between 'non-reasoning' models and specialized reasoning models.

Conclusion: Self-Anchor has the potential to enable most LLMs to tackle complex reasoning tasks without retraining.

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


### [77] [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
*Sebastian Gehrmann*

Main category: cs.CL

TL;DR: 本文探讨了奖励模型和评估指标的分离问题，并提出通过加强合作来解决这些问题，同时展示了评估指标在某些任务上的优势。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型和评估指标的研究领域相对独立，导致术语冗余和重复的问题，因此需要更紧密的合作。

Method: 本文通过比较评估指标和奖励模型在特定任务上的表现，进行了两个领域的广泛调查，并基于此指出了多个研究方向。

Result: 评估指标在某些任务上优于奖励模型，同时指出了多个可以改进奖励模型和评估指标的研究方向。

Conclusion: 本文主张加强奖励模型和评估指标领域的合作，以克服现有问题，并提出了多个研究方向来改进这两个领域。

Abstract: The emergence of reinforcement learning in post-training of large language
models has sparked significant interest in reward models. Reward models assess
the quality of sampled model outputs to generate training signals. This task is
also performed by evaluation metrics that monitor the performance of an AI
model. We find that the two research areas are mostly separate, leading to
redundant terminology and repeated pitfalls. Common challenges include
susceptibility to spurious correlations, impact on downstream reward hacking,
methods to improve data quality, and approaches to meta-evaluation. Our
position paper argues that a closer collaboration between the fields can help
overcome these issues. To that end, we show how metrics outperform reward
models on specific tasks and provide an extensive survey of the two areas.
Grounded in this survey, we point to multiple research topics in which closer
alignment can improve reward models and metrics in areas such as preference
elicitation methods, avoidance of spurious correlations and reward hacking, and
calibration-aware meta-evaluation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: Advisor Models are a new approach for dynamically optimizing black-box models by using reinforcement learning to generate context-aware instructions, leading to improved performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: Static prompt optimization fails to adapt to different inputs, users, or environments, limiting the customization of black-box models.

Method: Advisor Models are lightweight parametric policies trained with reinforcement learning to issue natural language steering instructions in-context to black-box models.

Result: Advisor Models outperform static prompt optimizers, discovering environment dynamics and improving downstream task performance. They also demonstrate generalizability across black-box models and retain robustness to out-of-distribution inputs.

Conclusion: Advisor Models provide a learnable interface to black-box systems, enabling dynamic optimization for personalization and environment-adaptable AI with frontier-level capabilities.

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [79] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: Litespark是一种新的预训练框架，通过优化Transformer注意力和MLP层，提高了模型FLOPs利用率（MFU），并在多节点H200 GPU集群中实现了显著的性能提升和能耗降低。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLMs）面临长训练时间和高能耗的问题，现代模型需要数月的计算和兆瓦时的电力。因此，需要一种新的预训练框架来解决这些效率问题。

Method: Litespark是一种新的预训练框架，通过结合架构改进和算法增强来提高模型FLOPs利用率（MFU），同时保持与标准Transformer实现的兼容性。

Result: 在3B和30B参数的Llama模型上使用SlimPajama-627B数据集进行的全面基准测试表明，Litespark实现了2x-6x的训练吞吐量提升和55%-83%的能耗降低。

Conclusion: Litespark通过针对变压器注意力和MLP层的优化，提高了模型FLOPs利用率（MFU），并在多节点H200 GPU集群中实现了显著的性能提升和能耗降低。该方法具有模型和硬件的通用性，适用于各种Transformer架构和后续训练阶段。

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [80] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: 本工作表明SFT不仅是策略模仿，还隐含地学习了一个密集的、逐标记的奖励模型，并利用该奖励模型进一步改进策略。


<details>
  <summary>Details</summary>
Motivation: 传统上，SFT被视为一种简单的模仿学习过程，仅训练策略在演示数据集上模仿专家行为。然而，本工作挑战这一观点，旨在揭示SFT的更深层次含义。

Method: 本工作建立了SFT与逆强化学习的基本等价关系，并证明SFT目标是逆Q学习的一个特例。此外，通过制定相对于基线的奖励函数，直接从SFT模型中恢复密集奖励信号。

Result: 本工作提出了Dense-Path REINFORCE方法，该方法在指令遵循基准测试中 consistently 超过原始SFT模型。

Conclusion: 本工作重新定义了SFT不仅仅作为策略模仿，而是一种强大的奖励学习机制，为利用专家演示开辟了新的可能性。

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [81] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为HyperAdaLoRA的新框架，通过利用超网络加速AdaLoRA的收敛速度，同时保持性能，并展示了该方法在多个数据集和模型上的有效性以及在其他基于LoRA的方法中的广泛应用性。


<details>
  <summary>Details</summary>
Motivation: 现有的AdaLoRA方法在训练过程中存在收敛速度慢和计算开销高的问题，需要一种更高效的解决方案。

Method: HyperAdaLoRA利用基于注意力机制的超网络动态生成SVD的参数，并通过修剪生成奇异值的超网络输出实现动态秩分配。

Result: 实验表明，所提出的方法在不牺牲性能的情况下实现了更快的收敛速度，并且在其他基于LoRA的方法中也表现出广泛的应用性。

Conclusion: 本文提出了一种名为HyperAdaLoRA的新框架，通过利用超网络加速AdaLoRA的收敛速度，同时保持性能，并展示了该方法在多个数据集和模型上的有效性以及在其他基于LoRA的方法中的广泛应用性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [82] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 本文研究了超参数损失表面的结构，并提出了新的工具来分析和理解它。


<details>
  <summary>Details</summary>
Motivation: 由于现代模型太大，无法进行广泛的超参数搜索，因此需要工具来理解和分析超参数损失表面。

Method: 通过随机搜索技术，发现了损失表面上的渐近规律，并推导出新的渐近定律来解释和外推其收敛性。

Result: 发现了损失表面中的新结构，并开发了基于随机搜索的新技术。在渐近区域内，随机搜索的最佳得分呈现出新的分布，这可以用于推导新的渐近定律。

Conclusion: 本文提出了新的工具来理解超参数损失表面，并展示了这些工具在分析模型性能方面的应用。

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [83] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: 本研究评估了模型 abliteration 对安全预训练的影响，并提出了将推理时编辑整合到安全评估中的方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨常见的安全干预措施（如拒绝训练或元标签训练）是否能在推理时的简单激活编辑下保持有效。

Method: 研究使用了模型 abliteration 技术，对多个安全预训练检查点进行评估，并通过多 judges 分类响应来验证模型的拒绝能力。

Result: 研究发现了哪些数据驱动的安全组件在 abliteration 下仍然有效，并量化了 judge 选择对评估结果的影响。

Conclusion: 研究结果表明，某些数据驱动的安全组件在模型 abliteration 下仍然具有鲁棒性，并提出了将推理时编辑集成到安全评估中的实用协议。

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [84] [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
*Guanhua Huang,Tingqiang Xu,Mingze Wang,Qi Yi,Xue Gong,Siheng Li,Ruibin Xiong,Kejiao Li,Yuhao Jiang,Bo Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法Lp-Reg，用于解决强化学习中由于策略熵崩溃导致的探索不足问题。实验表明，Lp-Reg能够实现稳定的在线策略训练，并在数学基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决强化学习与验证奖励（RLVR）中由于策略熵崩溃导致的探索不足问题。先前的方法通常通过保持高策略熵来解决这个问题，但缺乏对有意义探索的精确机制研究。

Method: 本文引入了低概率正则化（Lp-Reg），其核心机制是将策略正则化到一个启发式代理分布。该代理分布通过过滤掉假定的噪声标记并重新归一化剩余候选者来构建。

Result: 实验结果表明，Lp-Reg能够实现大约1000步的稳定在线策略训练，而基线熵控制方法在此阶段会崩溃。这种持续的探索带来了最先进的性能，在五个数学基准测试中平均准确率达到60.17%，比之前的方法提高了2.66%。

Conclusion: 本文提出了一种名为Lp-Reg的新方法，以解决强化学习中由于策略熵崩溃导致的探索不足问题。实验表明，Lp-Reg能够实现稳定的在线策略训练，并在数学基准测试中取得了最先进的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large
Language Models in complex reasoning, yet its scalability is often hindered by
a training bottleneck where performance plateaus as policy entropy collapses,
signaling a loss of exploration. Previous methods typically address this by
maintaining high policy entropy, yet the precise mechanisms that govern
meaningful exploration have remained underexplored. Our analysis suggests that
an unselective focus on entropy risks amplifying irrelevant tokens and
destabilizing training. This paper investigates the exploration dynamics within
RLVR and identifies a key issue: the gradual elimination of valuable
low-probability exploratory tokens, which we term \textbf{\textit{reasoning
sparks}}. We find that while abundant in pre-trained models, these sparks are
systematically extinguished during RLVR due to over-penalization, leading to a
degeneracy in exploration. To address this, we introduce Low-probability
Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a
heuristic proxy distribution. This proxy is constructed by filtering out
presumed noise tokens and re-normalizing the distribution over the remaining
candidates. The result is a less-noisy proxy where the probability of
\textit{reasoning sparks} is amplified, which then serves as a soft
regularization target to shield these valuable tokens from elimination via KL
divergence. Experiments show that Lp-Reg enables stable on-policy training for
around 1,000 steps, a regime where baseline entropy-control methods collapse.
This sustained exploration leads to state-of-the-art performance, achieving a
$60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$
over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [85] [Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations](https://arxiv.org/abs/2510.02319)
*Lekkala Sai Teja,Annepaka Yadagiri,Sangam Sai Anish,Siva Gopala Krishna Nuthakki,Partha Pakray*

Main category: cs.CR

TL;DR: 本文提出了一种新的检测框架PIFE，能够在对抗性攻击下保持更高的检测准确性。


<details>
  <summary>Details</summary>
Motivation: 由于现代检测器容易受到对抗性攻击的影响，特别是重写作为一种有效的规避技术，因此需要创建可靠的AI生成文本检测系统。

Method: 本文提出了一个新颖的、更坚韧的检测框架：Perturbation-Invariant Feature Engineering (PIFE)，该框架通过多阶段标准化管道将输入文本转换为标准化形式，并使用Levenshtein距离和语义相似性等指标量化转换的幅度，然后将这些信号直接输入分类器。

Result: 传统对抗训练在面对语义攻击时效果不佳，而PIFE模型在相同条件下保持了82.6%的真阳性率，有效中和了最复杂的语义攻击。

Conclusion: 本文表明，通过显式建模扰动特征而不是仅仅在它们上进行训练，是实现对抗性军备竞赛中真正鲁棒性的更有前途的路径。

Abstract: The growth of highly advanced Large Language Models (LLMs) constitutes a huge
dual-use problem, making it necessary to create dependable AI-generated text
detection systems. Modern detectors are notoriously vulnerable to adversarial
attacks, with paraphrasing standing out as an effective evasion technique that
foils statistical detection. This paper presents a comparative study of
adversarial robustness, first by quantifying the limitations of standard
adversarial training and then by introducing a novel, significantly more
resilient detection framework: Perturbation-Invariant Feature Engineering
(PIFE), a framework that enhances detection by first transforming input text
into a standardized form using a multi-stage normalization pipeline, it then
quantifies the transformation's magnitude using metrics like Levenshtein
distance and semantic similarity, feeding these signals directly to the
classifier. We evaluate both a conventionally hardened Transformer and our
PIFE-augmented model against a hierarchical taxonomy of character-, word-, and
sentence-level attacks. Our findings first confirm that conventional
adversarial training, while resilient to syntactic noise, fails against
semantic attacks, an effect we term "semantic evasion threshold", where its
True Positive Rate at a strict 1% False Positive Rate plummets to 48.8%. In
stark contrast, our PIFE model, which explicitly engineers features from the
discrepancy between a text and its canonical form, overcomes this limitation.
It maintains a remarkable 82.6% TPR under the same conditions, effectively
neutralizing the most sophisticated semantic attacks. This superior performance
demonstrates that explicitly modeling perturbation artifacts, rather than
merely training on them, is a more promising path toward achieving genuine
robustness in the adversarial arms race.

</details>


### [86] [CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models](https://arxiv.org/abs/2510.02342)
*Yu Zhang,Shuliang Liu,Xu Yang,Xuming Hu*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Watermarking algorithms for Large Language Models (LLMs) effectively identify
machine-generated content by embedding and detecting hidden statistical
features in text. However, such embedding leads to a decline in text quality,
especially in low-entropy scenarios where performance needs improvement.
Existing methods that rely on entropy thresholds often require significant
computational resources for tuning and demonstrate poor adaptability to unknown
or cross-task generation scenarios. We propose \textbf{C}ontext-\textbf{A}ware
\textbf{T}hreshold watermarking ($\myalgo$), a novel framework that dynamically
adjusts watermarking intensity based on real-time semantic context. $\myalgo$
partitions text generation into semantic states using logits clustering,
establishing context-aware entropy thresholds that preserve fidelity in
structured content while embedding robust watermarks. Crucially, it requires no
pre-defined thresholds or task-specific tuning. Experiments show $\myalgo$
improves text quality in cross-tasks without sacrificing detection accuracy.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [87] [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
*Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang*

Main category: cs.RO

TL;DR: SIMSplat is a language-controlled driving scene editor that uses Gaussian splatting for intuitive and flexible manipulation of road objects, demonstrating strong performance on the Waymo dataset.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities, motivating the development of SIMSplat.

Method: SIMSplat is a predictive driving scene editor that uses language-aligned Gaussian splatting to enable intuitive manipulation through natural language prompts and direct querying of road objects.

Result: Experiments on the Waymo dataset show SIMSplat's effectiveness in detailed object-level editing, including adding new objects and modifying trajectories, as well as generating realistic interactions among agents in the scene.

Conclusion: SIMSplat demonstrates extensive editing capabilities and adaptability across a wide range of scenarios, making it a promising tool for driving scene manipulation.

Abstract: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/

</details>


### [88] [Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning](https://arxiv.org/abs/2510.03182)
*Yilun Hao,Yongchao Chen,Chuchu Fan,Yang Zhang*

Main category: cs.RO

TL;DR: VLMFP is a framework that autonomously generates PDDL files for formal visual planning using two VLMs, achieving good performance on unseen instances and generalizing to different problems with varied appearances and rules.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing methods that rely on human experts to predefine domain files or on constant environment access for refinement, by enabling autonomous generation of PDDL files for formal visual planning.

Method: VLMFP is a Dual-VLM-guided framework that uses two VLMs: SimVLM to simulate action consequences based on input rule descriptions, and GenVLM to generate and iteratively refine PDDL files by comparing the PDDL and SimVLM execution results.

Result: VLMFP achieves 95.5%, 82.6% accuracy in describing scenarios, 85.5%, 87.8% accuracy in simulating action sequences, and 82.4%, 85.6% accuracy in judging goal reaching for seen and unseen appearances, respectively. With the guidance of SimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for unseen instances in seen and unseen appearances, respectively.

Conclusion: VLMFP can autonomously generate both PDDL problem and domain files for formal visual planning, achieving good performance on unseen instances and generalizing to different problems with varied appearances and rules.

Abstract: Vision Language Models (VLMs) show strong potential for visual planning but
struggle with precise spatial and long-horizon reasoning. In contrast, Planning
Domain Definition Language (PDDL) planners excel at long-horizon formal
planning, but cannot interpret visual inputs. Recent works combine these
complementary advantages by enabling VLMs to turn visual planning problems into
PDDL files for formal planning. However, while VLMs can generate PDDL problem
files satisfactorily, they struggle to accurately generate the PDDL domain
files, which describe all the planning rules. As a result, prior methods rely
on human experts to predefine domain files or on constant environment access
for refinement. We propose VLMFP, a Dual-VLM-guided framework that can
autonomously generate both PDDL problem and domain files for formal visual
planning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A
SimVLM that simulates action consequences based on input rule descriptions, and
a GenVLM that generates and iteratively refines PDDL files by comparing the
PDDL and SimVLM execution results. VLMFP unleashes multiple levels of
generalizability: The same generated PDDL domain file works for all the
different instances under the same problem, and VLMs generalize to different
problems with varied appearances and rules. We evaluate VLMFP with 6 grid-world
domains and test its generalization to unseen instances, appearance, and game
rules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,
simulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal
reaching for seen and unseen appearances, respectively. With the guidance of
SimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for
unseen instances in seen and unseen appearances, respectively. Project page:
https://sites.google.com/view/vlmfp.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [89] [Less LLM, More Documents: Searching for Improved RAG](https://arxiv.org/abs/2510.02657)
*Jingjie Ning,Yibo Kong,Yunfan Long,Jamie Callan*

Main category: cs.IR

TL;DR: 本文研究了通过扩大检索器的语料库来增强RAG的方法，发现这可以有效替代增加模型规模，但收益递减。


<details>
  <summary>Details</summary>
Motivation: 虽然扩展生成器可以提高准确性，但也增加了成本并限制了可部署性。

Method: 我们探索了另一个维度：扩大检索器的语料库以减少对大型LLM的依赖。

Result: 实验结果表明，语料库扩展一致地增强了RAG，并且常常可以作为增加模型规模的替代方案，尽管在更大规模下收益递减。

Conclusion: 我们的分析表明，投资更大的语料库为实现更强的RAG提供了一条有效的途径，通常与扩大LLM本身相当。

Abstract: Retrieval-Augmented Generation (RAG) couples document retrieval with large
language models (LLMs). While scaling generators improves accuracy, it also
raises cost and limits deployability. We explore an orthogonal axis: enlarging
the retriever's corpus to reduce reliance on large LLMs. Experimental results
show that corpus scaling consistently strengthens RAG and can often serve as a
substitute for increasing model size, though with diminishing returns at larger
scales. Small- and mid-sized generators paired with larger corpora often rival
much larger models with smaller corpora; mid-sized models tend to gain the
most, while tiny and large models benefit less. Our analysis shows that
improvements arise primarily from increased coverage of answer-bearing
passages, while utilization efficiency remains largely unchanged. These
findings establish a principled corpus-generator trade-off: investing in larger
corpora offers an effective path to stronger RAG, often comparable to enlarging
the LLM itself.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [90] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文提出了一个用于生成视频模型的不确定性量化框架，包括一种评估视频模型校准的度量方法、一种黑盒UQ方法（S-QUBED）以及一个用于基准校准的UQ数据集。实验表明，S-QUBED能够计算出与任务准确性负相关的校准总不确定性估计，并有效计算出aleatoric和epistemic成分。


<details>
  <summary>Details</summary>
Motivation: 生成视频模型在许多现实应用中得到广泛应用，但它们容易产生幻觉，即使事实错误也能生成看似合理的视频。目前尚无针对视频模型的不确定性量化方法，这引发了重要的安全问题。因此，本文旨在首次对视频模型的不确定性进行量化。

Method: 本文提出了一种基于稳健等级相关估计的度量方法来评估视频模型的校准，以及一种名为 S-QUBED 的黑盒 UQ 方法，该方法利用潜在建模将预测不确定性分解为 aleatoric 和 epistemic 成分。此外，还构建了一个用于基准校准的 UQ 数据集。

Result: 通过在基准视频数据集上的广泛实验，证明了 S-QUBED 能够计算出与任务准确性负相关的校准总不确定性估计，并能有效计算出 aleatoric 和 epistemic 成分。

Conclusion: 本文提出了一个用于生成视频模型的不确定性量化框架，包括一种评估视频模型校准的度量方法、一种黑盒UQ方法（S-QUBED）以及一个用于基准校准的UQ数据集。实验表明，S-QUBED能够计算出与任务准确性负相关的校准总不确定性估计，并有效计算出 aleatoric 和 epistemic 成分。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [91] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法MaskCD，用于解决大型视觉语言模型中的幻觉问题，实验结果显示该方法有效且稳定。


<details>
  <summary>Details</summary>
Motivation: 现有的对比解码方法在构建适当的对比样本方面存在困难，而注意力操纵方法则高度敏感且缺乏稳定性。因此，需要一种更有效和稳定的解决方案来处理LVLMs中的幻觉问题。

Method: 提出了一种名为Image Head Masked Contrastive Decoding (MaskCD)的方法，利用LVLMs中的“图像头”来构建对比样本进行对比解码。

Result: 在LLaVA-1.5-7b和Qwen-VL-7b上评估了MaskCD，使用了CHAIR、POPE、AMBER和MME等多个基准测试。结果表明，MaskCD能够有效减轻幻觉现象并保持LVLMs的通用能力。

Conclusion: MaskCD有效缓解了LVLMs中的幻觉现象，并保持了其通用能力。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [92] [Pareto-optimal Non-uniform Language Generation](https://arxiv.org/abs/2510.02795)
*Moses Charikar,Chirag Pabbaraju*

Main category: cs.DS

TL;DR: 本文研究了非均匀语言生成的帕累托最优性，并提出了一种几乎帕累托最优的算法，适用于噪声和代表性生成等实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有的工作虽然提供了非均匀生成保证，但它们的语言生成时间t(L)可能严格次优。因此，需要一种帕累托最优的算法来改进这一点。

Method: 本文提出了一个算法，其生成时间t^*(L)是（几乎）帕累托最优的，即任何其他算法在某个语言L上的生成时间严格小于t^*(L)时，必须在另一个语言L'上的生成时间严格更差。

Result: 本文提出的算法在生成时间上达到了几乎帕累托最优，并且可以适应噪声和代表性生成等实际场景。

Conclusion: 本文研究了非均匀语言生成的帕累托最优性，并提出了一种生成时间几乎帕累托最优的算法。该算法在噪声和代表性生成等实际场景中也表现出色。

Abstract: Kleinberg and Mullainathan (2024) recently proposed an interesting model for
language generation in the limit: Given a countable collection of languages,
and an adversary enumerating the strings of some language $L$ from the
collection, the objective is to generate new strings from the target language,
such that all strings generated beyond some finite time are valid. Li, Raman
and Tewari (2024) and Charikar and Pabbaraju (2024) showed strong non-uniform
generation guarantees in this model, giving algorithms that generate new valid
strings from $L$ after seeing a number of distinct input strings $t(L)$ that
depends only on $L$ (and the collection), but not the enumeration order.
However, for both these works, the language-wise generation times $t(L)$ of the
algorithm can be strictly sub-optimal.
  In this work, we study Pareto-optimality of non-uniform language generation
in the limit. We propose an algorithm, whose generation times $t^\star(L)$ are
(almost) Pareto-optimal: any other algorithm whose generation time for some
language $L$ is strictly smaller than $t^\star(L)$, must satisfy that its
generation time for some other language $L'$ is strictly worse than
$t^\star(L')$. Pareto-optimality is essentially the best that one can achieve
for non-uniform generation. Our algorithmic framework conveniently adapts to
further give Pareto-optimal non-uniform generation algorithms in the
practically motivated settings of noisy as well as representative generation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [93] [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
*Cuong Chi Le,Minh V. T. Pham,Cuong Duc Van,Hoang N. Phan,Huy N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种新的基准测试 ClassEval-Obf，用于评估大型语言模型在代码任务中的理解能力，该基准测试通过消除命名提示来提高评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试奖励对命名模式的记忆而不是真正的语义推理，因此需要一种更可靠的评估方法。

Method: 引入了一套保持语义的混淆方法，并发布了 ClassEval-Obf 基准测试，以系统地抑制命名提示同时保留行为。

Result: ClassEval-Obf 减少了性能差距，削弱了记忆捷径，并提供了更可靠的评估基础。

Conclusion: ClassEval-Obf 可以减少性能差距，削弱记忆捷径，并为评估 LLM 的代码理解和泛化提供更可靠的基准。

Abstract: Large Language Models (LLMs) achieve strong results on code tasks, but how
they derive program meaning remains unclear. We argue that code communicates
through two channels: structural semantics, which define formal behavior, and
human-interpretable naming, which conveys intent. Removing the naming channel
severely degrades intent-level tasks such as summarization, where models
regress to line-by-line descriptions. Surprisingly, we also observe consistent
reductions on execution tasks that should depend only on structure, revealing
that current benchmarks reward memorization of naming patterns rather than
genuine semantic reasoning. To disentangle these effects, we introduce a suite
of semantics-preserving obfuscations and show that they expose identifier
leakage across both summarization and execution. Building on these insights, we
release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically
suppresses naming cues while preserving behavior. Our results demonstrate that
ClassEval-Obf reduces inflated performance gaps, weakens memorization
shortcuts, and provides a more reliable basis for assessing LLMs' code
understanding and generalization.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [94] [WEE-Therapy: A Mixture of Weak Encoders Framework for Psychological Counseling Dialogue Analysis](https://arxiv.org/abs/2510.02320)
*Yongqi Kang,Yong Zhao*

Main category: eess.AS

TL;DR: WEE-Therapy is a multi-task AudioLLM that enhances the ability of AI tools to understand counseling dialogues by incorporating a Weak Encoder Ensemble mechanism and a dual-routing strategy, leading to improved performance in various clinical analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Existing audio language models (AudioLLMs) often struggle to capture domain-specific features like complex emotions and professional techniques due to their reliance on single speech encoders pre-trained on general data.

Method: WEE-Therapy is a multi-task AudioLLM incorporating a Weak Encoder Ensemble (WEE) mechanism, which supplements a powerful base encoder with a pool of lightweight, specialized encoders. A novel dual-routing strategy combines stable, data-independent domain knowledge with dynamic, data-dependent expert selection.

Result: WEE-Therapy achieves significant performance gains across all tasks, including emotion recognition, technique classification, risk detection, and summarization, with minimal parameter overhead.

Conclusion: WEE-Therapy demonstrates strong potential for AI-assisted clinical analysis by achieving significant performance gains across multiple tasks with minimal parameter overhead.

Abstract: The advancement of computational psychology requires AI tools capable of
deeply understanding counseling dialogues. Existing audio language models
(AudioLLMs) often rely on single speech encoders pre-trained on general data,
struggling to capture domain-specific features like complex emotions and
professional techniques. To address this, we propose WEE-Therapy, a multi-task
AudioLLM incorporating a Weak Encoder Ensemble (WEE) mechanism. This
supplements a powerful base encoder with a pool of lightweight, specialized
encoders. A novel dual-routing strategy combines stable, data-independent
domain knowledge with dynamic, data-dependent expert selection. Evaluated on
emotion recognition, technique classification, risk detection, and
summarization, WEE-Therapy achieves significant performance gains across all
tasks with minimal parameter overhead, demonstrating strong potential for
AI-assisted clinical analysis.

</details>


### [95] [SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for Voice-Native Multimodal CT Analysis](https://arxiv.org/abs/2510.02322)
*Lukas Buess,Jan Geier,David Bani-Harouni,Chantal Pellegrini,Matthias Keicher,Paula Andrea Perez-Toro,Nassir Navab,Andreas Maier,Tomas Arias-Vergara*

Main category: eess.AS

TL;DR: 本研究探索了从语音放射学报告中学习视觉-语言表示的可行性，并展示了语音作为多模态预训练中文本的实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 在放射学中，大多数报告是通过口述创建的。然而，几乎所有的医疗AI系统都只依赖于书面文本。我们旨在解决这一差距，探索从语音放射学报告中学习视觉-语言表示的可行性。

Method: 我们合成一个大规模的数据集（Speech-RATE）的语音放射学报告，并训练SpeechCT-CLIP，这是一种对比模型，它在共享表示空间中对齐语音和3D CT体积。

Result: 实验表明，零样本分类F1从0.623提高到0.705，恢复了88%的性能差异，并且在推理时不需文本即可获得强大的检索结果。

Conclusion: 这些发现强调了语音在多模态预训练中的实用替代作用，并为临床实践中语音驱动的诊断支持工具打开了大门。

Abstract: Spoken communication plays a central role in clinical workflows. In
radiology, for example, most reports are created through dictation. Yet, nearly
all medical AI systems rely exclusively on written text. In this work, we
address this gap by exploring the feasibility of learning visual-language
representations directly from spoken radiology reports. Specifically, we
synthesize a large-scale dataset (Speech-RATE) of spoken radiology reports and
train SpeechCT-CLIP, a contrastive model that aligns speech and 3D CT volumes
in a shared representation space. While naive speech-based models underperform
compared to text-trained counterparts, we show that knowledge distillation from
a pretrained text-image CLIP model effectively transfers semantic alignment
capabilities from text to speech, substantially narrowing this gap. Experiments
demonstrate improved zero-shot classification F1 from 0.623 to 0.705,
recovering 88% of the performance difference, and strong retrieval results
without requiring text at inference. These findings highlight speech as a
practical alternative to text in multimodal pretraining and open the door to
voice-driven diagnostic support tools in clinical practice.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [96] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: This paper demonstrates that temperature scaling can enhance the reasoning capabilities of large language models by exploring different subsets of problems, leading to improved performance without additional training.


<details>
  <summary>Details</summary>
Motivation: Prior work shows that increasing the number of samples K steadily improves accuracy, but this trend does not hold indefinitely. Different sampling temperatures solve different subsets of problems, implying that single-temperature scaling explores only part of a model's potential.

Method: Scaling along the temperature dimension to enlarge the reasoning boundary of LLMs and a multi-temperature voting method to reduce overhead.

Result: Temperature scaling yields an additional 7.3 points over single-temperature TTS and enables base models to reach performance comparable to RL-trained counterparts without additional post-training.

Conclusion: TTS is more powerful than previously thought, and temperature scaling offers a simple and effective way to unlock the latent potential of base models.

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [97] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: NCV是一种无需训练的框架，通过节点级别的二进制一致性检查来验证大型语言模型的推理，提高了可解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 验证多步骤推理在大型语言模型中是困难的，因为错误定位不精确且令牌成本高。

Method: NCV是一种无需训练的框架，将验证重新定义为节点级别的轻量级二进制一致性检查。

Result: 实验表明，该方法提高了可解释性和效率，在公共数据集上，NCV在F1分数上比基线提高了10%至25%，同时使用的令牌数量比传统方法少6倍至58倍。

Conclusion: NCV提供了一种可扩展的解决方案，用于可靠地验证大型语言模型的推理。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [98] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 本文提出了TRACE框架，用于对工具增强的LLM代理性能进行多维评估，能够有效分析代理的问题解决轨迹，并在实际应用中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法仅限于答案匹配，无法全面评估代理在解决用户请求时的问题解决轨迹，包括效率、幻觉和适应性等方面。

Method: 引入TRACE框架，通过结合证据库来积累先前推理步骤中的知识，从而实现对代理推理轨迹的多面分析和评估。

Result: TRACE能够准确评估这些复杂行为，即使使用小型开源LLM也能实现可扩展和成本效益的评估。此外，该方法还揭示了代理在解决工具增强任务时之前未报告的观察结果和见解。

Conclusion: TRACE能够有效地对工具增强的LLM代理性能进行多维评估，并且在可扩展性和成本效益方面表现良好。此外，该方法还揭示了代理在解决工具增强任务时之前未报告的观察结果和见解。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [99] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新的扩散语言模型CCDD，通过结合连续表示空间和离散标记空间，提升了模型的表达能力和实际性能。


<details>
  <summary>Details</summary>
Motivation: 尽管连续扩散模型在理论上具有更强的表达能力，但它们在实际表现上通常不如离散模型。本文旨在解决这一理论与实践之间的矛盾。

Method: 本文提出了Coevolutionary Continuous Discrete Diffusion (CCDD)，这是一种在连续表示空间和离散标记空间的联合空间中定义的多模态扩散过程，并利用单一模型同时进行去噪。

Result: CCDD在真实任务的语言建模实验中表现出强大的经验性能，结合了丰富的语义表达、良好的训练性和样本质量。

Conclusion: 本文提出了一种结合连续表示空间和离散标记空间的联合多模态扩散过程，称为Coevolutionary Continuous Discrete Diffusion (CCDD)，并展示了其在真实任务中的强大经验性能。

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>

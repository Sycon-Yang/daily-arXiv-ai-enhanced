<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 112]
- [cs.HC](#cs.HC) [Total: 2]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting](https://arxiv.org/abs/2508.16603)
*Zheng Dong,Luming Shang,Gabriela Olinto*

Main category: cs.CL

TL;DR: GreenTEA是一种基于代理的LLM工作流，用于自动提示优化，它平衡了候选探索和知识利用。通过协作代理团队迭代改进提示，并使用遗传算法框架进行优化，GreenTEA在多个任务上表现优于人工设计的提示和现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动创建有效的提示既耗时又需要专业知识，限制了其可扩展性。现有的自动提示优化方法要么广泛探索新的提示候选，由于在大解空间中搜索效率低下而产生高计算成本，要么过度利用现有提示的反馈，由于提示景观的复杂性而存在次优优化的风险。

Method: GreenTEA是一种基于代理的LLM工作流，用于自动提示优化，它平衡了候选探索和知识利用。它利用了一个协作代理团队，根据错误样本的反馈迭代地改进提示。分析代理通过主题建模识别当前提示导致的常见错误模式，生成代理会修改提示以直接解决这些关键缺陷。这个细化过程由遗传算法框架指导，通过交叉和变异等操作进化候选提示，逐步优化模型性能。

Result: GreenTEA在逻辑和定量推理、常识以及伦理决策方面表现出优于人类工程提示和现有最先进的自动提示优化方法的性能。

Conclusion: GreenTEA在公共基准数据集上的大量数值实验表明，其性能优于人工设计的提示和现有的自动提示优化方法，涵盖了逻辑和定量推理、常识以及伦理决策。

Abstract: High-quality prompts are crucial for Large Language Models (LLMs) to achieve
exceptional performance. However, manually crafting effective prompts is
labor-intensive and demands significant domain expertise, limiting its
scalability. Existing automatic prompt optimization methods either extensively
explore new prompt candidates, incurring high computational costs due to
inefficient searches within a large solution space, or overly exploit feedback
on existing prompts, risking suboptimal optimization because of the complex
prompt landscape. To address these challenges, we introduce GreenTEA, an
agentic LLM workflow for automatic prompt optimization that balances candidate
exploration and knowledge exploitation. It leverages a collaborative team of
agents to iteratively refine prompts based on feedback from error samples. An
analyzing agent identifies common error patterns resulting from the current
prompt via topic modeling, and a generation agent revises the prompt to
directly address these key deficiencies. This refinement process is guided by a
genetic algorithm framework, which simulates natural selection by evolving
candidate prompts through operations such as crossover and mutation to
progressively optimize model performance. Extensive numerical experiments
conducted on public benchmark datasets suggest the superior performance of
GreenTEA against human-engineered prompts and existing state-of-the-arts for
automatic prompt optimization, covering logical and quantitative reasoning,
commonsense, and ethical decision-making.

</details>


### [2] [Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow](https://arxiv.org/abs/2508.16636)
*Y. Du,C. Guo,W. Wang,G. Tang*

Main category: cs.CL

TL;DR: 本文提出了一种基于认知科学的自适应推理框架，能够动态决定大型语言模型的推理策略，显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在决定何时依赖快速直觉反应或进行缓慢的深入推理时面临挑战，现有的方法要么应用统一的推理深度，要么依赖计算成本高昂的方法。

Method: 本文受丹尼尔·卡尼曼的双过程理论启发，提出了一种认知决策路由（CDR）框架，通过分析查询的复杂性维度来动态决定适当的推理策略。

Result: 实验表明，CDR在多种推理任务中表现优异，同时相比统一的深度推理方法，计算成本降低了34%。在专业判断任务中，一致性提高了23%，专家级评估的准确性提高了18%。

Conclusion: 本文提出了一个基于认知科学原理的自适应推理框架，为大型语言模型提供了更高效和准确的决策方法。

Abstract: Large Language Models (LLMs) face a fundamental challenge in deciding when to
rely on rapid, intuitive responses versus engaging in slower, more deliberate
reasoning. Inspired by Daniel Kahneman's dual-process theory and his insights
on human cognitive biases, we propose a novel Cognitive Decision Routing (CDR)
framework that dynamically determines the appropriate reasoning strategy based
on query characteristics. Our approach addresses the current limitations where
models either apply uniform reasoning depth or rely on computationally
expensive methods for all queries. We introduce a meta-cognitive layer that
analyzes query complexity through multiple dimensions: correlation strength
between given information and required conclusions, domain boundary crossings,
stakeholder multiplicity, and uncertainty levels. Through extensive experiments
on diverse reasoning tasks, we demonstrate that CDR achieves superior
performance while reducing computational costs by 34\% compared to uniform deep
reasoning approaches. Our framework shows particular strength in professional
judgment tasks, achieving 23\% improvement in consistency and 18\% better
accuracy on expert-level evaluations. This work bridges cognitive science
principles with practical AI system design, offering a principled approach to
adaptive reasoning in LLMs.

</details>


### [3] [Trust but Verify! A Survey on Verification Design for Test-time Scaling](https://arxiv.org/abs/2508.16665)
*V Venktesh,Mandeep rathee,Avishek Anand*

Main category: cs.CL

TL;DR: 本文对测试时缩放（TTS）中的验证器进行了全面综述，总结了不同验证方法及其训练机制，并提供了一个统一的观点。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时缩放（TTS）已被广泛采用，但目前缺乏对不同验证方法及其训练机制的详细收集、清晰分类和讨论。因此，本文旨在填补这一空白。

Method: 本文通过分析文献中各种验证方法，总结了它们的训练机制和分类，并提出了一个统一的观点。

Result: 本文覆盖了文献中各种验证方法，并提出了一个统一的视角，有助于更好地理解验证器在测试时缩放中的作用。

Conclusion: 本文对测试时缩放（TTS）中的验证器进行了全面的综述，提供了统一的视角，并介绍了验证器训练、类型及其在测试时缩放中的效用。

Abstract: Test-time scaling (TTS) has emerged as a new frontier for scaling the
performance of Large Language Models. In test-time scaling, by using more
computational resources during inference, LLMs can improve their reasoning
process and task performance. Several approaches have emerged for TTS such as
distilling reasoning traces from another model or exploring the vast decoding
search space by employing a verifier. The verifiers serve as reward models that
help score the candidate outputs from the decoding process to diligently
explore the vast solution space and select the best outcome. This paradigm
commonly termed has emerged as a superior approach owing to parameter free
scaling at inference time and high performance gains. The verifiers could be
prompt-based, fine-tuned as a discriminative or generative model to verify
process paths, outcomes or both. Despite their widespread adoption, there is no
detailed collection, clear categorization and discussion of diverse
verification approaches and their training mechanisms. In this survey, we cover
the diverse approaches in the literature and present a unified view of verifier
training, types and their utility in test-time scaling. Our repository can be
found at
https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.

</details>


### [4] [Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?](https://arxiv.org/abs/2508.16695)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Main category: cs.CL

TL;DR: 本文研究了CoT推理痕迹是否需要可解释性以提高LLM性能，发现微调在R1痕迹上效果最好，但可解释性最差，表明中间标记可以与可解释性解耦。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究质疑了这些痕迹的语义性质的必要性，但本文旨在探讨CoT推理痕迹是否必须具有可解释性以增强LLM任务性能。

Method: 我们通过监督微调LLaMA和Qwen模型，在四种类型的推理痕迹上进行实验，包括DeepSeek R1痕迹、LLM生成的R1痕迹摘要、LLM生成的后 hoc 解释以及算法生成的可验证正确痕迹。此外，我们还进行了涉及100名参与者的用户研究，以量化可解释性和性能之间的权衡。

Result: 研究结果揭示了一个显著的不匹配：虽然在R1痕迹上微调可以获得最强的性能，但参与者认为这些痕迹的可解释性最差。

Conclusion: 我们的研究结果表明，中间标记可以与最终用户的可解释性解耦，这可能对LLM的设计和应用产生重要影响。

Abstract: Recent progress in reasoning-oriented Large Language Models (LLMs) has been
driven by introducing Chain-of-Thought (CoT) traces, where models generate
intermediate reasoning traces before producing an answer. These traces, as in
DeepSeek R1, are not only used to guide inference but also serve as supervision
signals for distillation into smaller models. A common but often implicit
assumption is that CoT traces should be semantically meaningful and
interpretable to the end user. While recent research questions the need for
semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT
reasoning traces be interpretable to enhance LLM task performance?}" We
investigate this question in the Open Book Question-Answering domain by
supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces:
(1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3)
LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically
generated verifiably correct traces. To quantify the trade-off between
interpretability and performance, we further conduct a human-subject study with
100 participants rating the interpretability of each trace type. Our results
reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest
performance, participants judged these traces to be the least interpretable.
These findings suggest that it is useful to decouple intermediate tokens from
end user interpretability.

</details>


### [5] [QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting](https://arxiv.org/abs/2508.16697)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 本文介绍了QueryBandits，这是一种基于奖励模型的框架，用于设计重写策略以减少大型语言模型中的幻觉。实验表明，QueryBandits在多个基准上优于无重写基线和静态提示方法。


<details>
  <summary>Details</summary>
Motivation: 高级推理能力导致大型语言模型中幻觉更普遍，但大多数缓解工作集中在事后过滤而不是塑造触发幻觉的查询。

Method: 我们引入了QueryBandits，这是一个基于奖励模型的框架，该模型基于输入查询的17个语言特征的敏感性来封装幻觉倾向，并设计重写策略以最大化奖励模型。

Result: 我们的顶级上下文QueryBandits（Thompson Sampling）在13个不同的QA基准和每个数据集的1050个词法扰动查询中，相对于无重写基线取得了87.5%的胜率，并且分别比零样本静态提示（“重新表述”或“扩展”）高出42.6%和60.3%。

Conclusion: 我们通过实验证实了QueryBandits在通过查询重写干预来减轻幻觉的有效性。此外，我们发现没有一种重写策略适用于所有查询，而通过QueryBandits引导重写可以显著改变输出行为。

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have caused
higher hallucination prevalence; yet most mitigation work focuses on
after-the-fact filtering rather than shaping the queries that trigger them. We
introduce QueryBandits, a bandit framework that designs rewrite strategies to
maximize a reward model, that encapsulates hallucination propensity based upon
the sensitivities of 17 linguistic features of the input query-and therefore,
proactively steer LLMs away from generating hallucinations. Across 13 diverse
QA benchmarks and 1,050 lexically perturbed queries per dataset, our top
contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a
no-rewrite baseline and also outperforms zero-shot static prompting
("paraphrase" or "expand") by 42.6% and 60.3% respectively. Therefore, we
empirically substantiate the effectiveness of QueryBandits in mitigating
hallucination via the intervention that takes the form of a query rewrite.
Interestingly, certain static prompting strategies, which constitute a
considerable number of current query rewriting literature, have a higher
cumulative regret than the no-rewrite baseline, signifying that static rewrites
can worsen hallucination. Moreover, we discover that the converged per-arm
regression feature weight vectors substantiate that there is no single rewrite
strategy optimal for all queries. In this context, guided rewriting via
exploiting semantic features with QueryBandits can induce significant shifts in
output behavior through forward-pass mechanisms, bypassing the need for
retraining or gradient-based adaptation.

</details>


### [6] [Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test](https://arxiv.org/abs/2508.16705)
*Rui A. Pimenta,Tim Schlippe,Kristina Schaaff*

Main category: cs.CL

TL;DR: This paper investigates consciousness-like behaviors in LLMs using the Maze Test, revealing that while reasoning-capable LLMs perform better, they still lack integrated, persistent self-awareness.


<details>
  <summary>Details</summary>
Motivation: To investigate consciousness-like behaviors in Large Language Models (LLMs) and challenge models to navigate mazes from a first-person perspective.

Method: We synthesized consciousness theories into 13 essential characteristics and evaluated 12 leading LLMs across zero-shot, one-shot, and few-shot learning scenarios using the Maze Test.

Result: Reasoning-capable LLMs consistently outperformed standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs struggle to maintain coherent self-models throughout solutions.

Conclusion: LLMs show progress in consciousness-related behaviors through reasoning mechanisms, but lack integrated, persistent self-awareness characteristic of consciousness.

Abstract: We investigate consciousness-like behaviors in Large Language Models (LLMs)
using the Maze Test, challenging models to navigate mazes from a first-person
perspective. This test simultaneously probes spatial awareness,
perspective-taking, goal-directed behavior, and temporal sequencing-key
consciousness-associated characteristics. After synthesizing consciousness
theories into 13 essential characteristics, we evaluated 12 leading LLMs across
zero-shot, one-shot, and few-shot learning scenarios. Results showed
reasoning-capable LLMs consistently outperforming standard versions, with
Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching
80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs
struggle to maintain coherent self-models throughout solutions -- a fundamental
consciousness aspect. While LLMs show progress in consciousness-related
behaviors through reasoning mechanisms, they lack the integrated, persistent
self-awareness characteristic of consciousness.

</details>


### [7] [Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval](https://arxiv.org/abs/2508.16707)
*Jonghyun Song,Youngjune Lee,Gyu-Hwung Cho,Ilhyeon Song,Saehun Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种简单但有效的框架，通过自我知识蒸馏在密集和稀疏表示之间实现双向学习。实验结果表明，该方法在多模态任务中表现出色，同时保持了稀疏模型的优势。


<details>
  <summary>Details</summary>
Motivation: 受这些优势的启发，最近的工作已将LSR扩展到多模态领域。然而，这些方法通常依赖于计算昂贵的对比预训练，或从冻结的密集模型中蒸馏，这限制了相互增强的潜力。为了解决这些限制，我们提出了一个简单但有效的框架，通过自我知识蒸馏在密集和稀疏表示之间实现双向学习。

Method: 我们提出了一种简单的有效框架，通过自我知识蒸馏在密集和稀疏表示之间实现双向学习。这种双向学习是通过一个综合相似性得分（密集和稀疏相似性的加权和）来实现的，该得分作为两种表示的共享教师信号。为了确保效率，我们微调了密集编码器的最后一层和稀疏投影头，使得任何现有的VLP模型都能轻松适应。

Result: 在MSCOCO和Flickr30k上的实验表明，我们的稀疏检索器不仅优于现有的稀疏基线，而且实现了与密集模型相当或甚至超越其性能，同时保留了稀疏模型的优势。

Conclusion: 我们的稀疏检索器不仅优于现有的稀疏基线，而且实现了与密集模型相当或甚至超越其性能，同时保留了稀疏模型的优势。

Abstract: Vision-Language Pretrained (VLP) models have achieved impressive performance
on multimodal tasks, including text-image retrieval, based on dense
representations. Meanwhile, Learned Sparse Retrieval (LSR) has gained traction
in text-only settings due to its interpretability and efficiency with fast
term-based lookup via inverted indexes. Inspired by these advantages, recent
work has extended LSR to the multimodal domain. However, these methods often
rely on computationally expensive contrastive pre-training, or distillation
from a frozen dense model, which limits the potential for mutual enhancement.
To address these limitations, we propose a simple yet effective framework that
enables bi-directional learning between dense and sparse representations
through Self-Knowledge Distillation. This bi-directional learning is achieved
using an integrated similarity score-a weighted sum of dense and sparse
similarities-which serves as a shared teacher signal for both representations.
To ensure efficiency, we fine-tune the final layer of the dense encoder and the
sparse projection head, enabling easy adaptation of any existing VLP model.
Experiments on MSCOCO and Flickr30k demonstrate that our sparse retriever not
only outperforms existing sparse baselines, but also achieves performance
comparable to-or even surpassing-its dense counterparts, while retaining the
benefits of sparse models.

</details>


### [8] [Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?](https://arxiv.org/abs/2508.16729)
*Jason Li,Lauren Yraola,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

TL;DR: 本文提出了一种名为错误反思提示（ERP）的方法，旨在增强语言模型的推理能力。ERP基于Chain-of-thought（CoT），包含一个错误答案、错误识别和一个正确答案。这种方法使模型能够识别错误类型和导致错误的步骤，从而更好地判断哪些步骤应避免，哪些步骤应采取。实验结果表明，ERP作为传统CoT的补充，有助于提高模型的推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: Despite advancements in prompting methods for language models, such as Chain-of-thought (CoT), these methodologies lack the ability of reflection and error correction, potentially causing a model to perpetuate mistakes and errors. Therefore, inspired by the human ability for said tasks, we propose Error Reflection Prompting (ERP) to further enhance reasoning in language models.

Method: Error Reflection Prompting (ERP) is a method comprised of an incorrect answer, error recognition, and a correct answer. This process enables the model to recognize types of errors and the steps that lead to incorrect answers, allowing the model to better discern which steps to avoid and which to take. The model is able to generate the error outlines itself with automated ERP generation, allowing for error recognition and correction to be integrated into the reasoning chain.

Result: The results demonstrate that ERP serves as a versatile supplement to conventional CoT, ultimately contributing to more robust and capable reasoning abilities along with increased interpretability in how models ultimately reach their errors.

Conclusion: ERP serves as a versatile supplement to conventional CoT, ultimately contributing to more robust and capable reasoning abilities along with increased interpretability in how models ultimately reach their errors.

Abstract: Prompting methods for language models, such as Chain-of-thought (CoT),
present intuitive step-by-step processes for problem solving. These
methodologies aim to equip models with a better understanding of the correct
procedures for addressing a given task. Despite these advancements, CoT lacks
the ability of reflection and error correction, potentially causing a model to
perpetuate mistakes and errors. Therefore, inspired by the human ability for
said tasks, we propose Error Reflection Prompting (ERP) to further enhance
reasoning in language models. Building upon CoT, ERP is a method comprised of
an incorrect answer, error recognition, and a correct answer. This process
enables the model to recognize types of errors and the steps that lead to
incorrect answers, allowing the model to better discern which steps to avoid
and which to take. The model is able to generate the error outlines itself with
automated ERP generation, allowing for error recognition and correction to be
integrated into the reasoning chain and produce scalability and reliability in
the process. The results demonstrate that ERP serves as a versatile supplement
to conventional CoT, ultimately contributing to more robust and capable
reasoning abilities along with increased interpretability in how models
ultimately reach their errors.

</details>


### [9] [GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs](https://arxiv.org/abs/2508.16753)
*Nitin Gupta,Pallav Koppisetti,Kausik Lakkaraju,Biplav Srivastava*

Main category: cs.CL

TL;DR: 本文介绍了GAICo，这是一个用于生成人工智能输出比较的开源Python库。它提供了一个统一、可扩展的框架，支持多种参考指标，适用于非结构化文本、专用结构数据格式和多媒体（图像、音频）。通过一个详细的案例研究，展示了GAICo在评估和调试多模态AI旅行助手管道中的实用性。自2025年6月在PyPI上发布以来，该工具已下载超过13,000次，显示出社区对它的兴趣不断增加。


<details>
  <summary>Details</summary>
Motivation: The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes domains necessitates robust and reproducible evaluation methods. However, practitioners often resort to ad-hoc, non-standardized scripts, as common metrics are often unsuitable for specialized, structured outputs (e.g., automated plans, time-series) or holistic comparison across modalities (e.g., text, audio, and image). This fragmentation hinders comparability and slows AI system development.

Method: GAICo provides a unified, extensible framework supporting a comprehensive suite of reference-based metrics for unstructured text, specialized structured data formats, and multimedia (images, audio). Its architecture features a high-level API for rapid, end-to-end analysis, from multi-model comparison to visualization and reporting, alongside direct metric access for granular control.

Result: We demonstrate GAICo's utility through a detailed case study evaluating and debugging complex, multi-modal AI Travel Assistant pipelines. Since its release on PyPI in Jun 2025, the tool has been downloaded over 13K times, across versions, by Aug 2025, demonstrating growing community interest.

Conclusion: GAICo empowers AI researchers and developers to efficiently assess system performance, make evaluation reproducible, improve development velocity, and ultimately build more trustworthy AI systems, aligning with the goal of moving faster and safer in AI deployment.

Abstract: The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes
domains necessitates robust and reproducible evaluation methods. However,
practitioners often resort to ad-hoc, non-standardized scripts, as common
metrics are often unsuitable for specialized, structured outputs (e.g.,
automated plans, time-series) or holistic comparison across modalities (e.g.,
text, audio, and image). This fragmentation hinders comparability and slows AI
system development. To address this challenge, we present GAICo (Generative AI
Comparator): a deployed, open-source Python library that streamlines and
standardizes GenAI output comparison. GAICo provides a unified, extensible
framework supporting a comprehensive suite of reference-based metrics for
unstructured text, specialized structured data formats, and multimedia (images,
audio). Its architecture features a high-level API for rapid, end-to-end
analysis, from multi-model comparison to visualization and reporting, alongside
direct metric access for granular control. We demonstrate GAICo's utility
through a detailed case study evaluating and debugging complex, multi-modal AI
Travel Assistant pipelines. GAICo empowers AI researchers and developers to
efficiently assess system performance, make evaluation reproducible, improve
development velocity, and ultimately build more trustworthy AI systems,
aligning with the goal of moving faster and safer in AI deployment. Since its
release on PyPI in Jun 2025, the tool has been downloaded over 13K times,
across versions, by Aug 2025, demonstrating growing community interest.

</details>


### [10] [How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models](https://arxiv.org/abs/2508.16757)
*Abdelrahman Abdallah,Bhawna Piryani,Jamshid Mozafari,Mohammed Ali,Adam Jatowt*

Main category: cs.CL

TL;DR: 本文对最先进的重排序方法进行了系统和全面的实证评估，包括基于大语言模型（LLM）、轻量级上下文和零样本的方法，以评估它们在信息检索任务中的表现。研究发现，基于LLM的重排序器在熟悉查询上表现优异，但在新查询上的泛化能力不一，而轻量级模型则提供了相当的效率。此外，查询的新颖性显著影响了重排序的有效性，突显了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 我们的主要目标是通过受控和公平的比较，确定基于LLM的重排序器和其轻量级对应物之间是否存在性能差异，特别是在新查询上，并阐明任何观察到的差异的潜在原因。为了消除混杂因素，我们分析了训练数据重叠、模型架构和计算效率对重排序性能的影响。

Method: 我们对最先进的重排序方法进行了系统和全面的实证评估，包括基于大语言模型（LLM）、轻量级上下文和零样本的方法，以评估它们在信息检索任务中的表现。我们总共评估了22种方法，包括40种变体（取决于使用的LLM），并在几个已建立的基准测试中进行评估，包括TREC DL19、DL20和BEIR，以及一个设计用于测试预训练模型未见过的查询的新数据集。

Result: 我们的研究结果表明，基于LLM的重排序器在熟悉查询上表现出色，但它们在新查询上的泛化能力有所不同，轻量级模型提供了相当的效率。我们进一步发现，查询的新颖性显著影响了重排序的有效性，突显了现有方法的局限性。

Conclusion: 我们的研究结果表明，虽然基于LLM的重排序器在熟悉查询上表现出色，但它们在新查询上的泛化能力有所不同，轻量级模型提供了相当的效率。我们进一步发现，查询的新颖性显著影响了重排序的有效性，突显了现有方法的局限性。

Abstract: In this work, we present a systematic and comprehensive empirical evaluation
of state-of-the-art reranking methods, encompassing large language model
(LLM)-based, lightweight contextual, and zero-shot approaches, with respect to
their performance in information retrieval tasks. We evaluate in total 22
methods, including 40 variants (depending on used LLM) across several
established benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel
dataset designed to test queries unseen by pretrained models. Our primary goal
is to determine, through controlled and fair comparisons, whether a performance
disparity exists between LLM-based rerankers and their lightweight
counterparts, particularly on novel queries, and to elucidate the underlying
causes of any observed differences. To disentangle confounding factors, we
analyze the effects of training data overlap, model architecture, and
computational efficiency on reranking performance. Our findings indicate that
while LLM-based rerankers demonstrate superior performance on familiar queries,
their generalization ability to novel queries varies, with lightweight models
offering comparable efficiency. We further identify that the novelty of queries
significantly impacts reranking effectiveness, highlighting limitations in
existing approaches.
https://github.com/DataScienceUIBK/llm-reranking-generalization-study

</details>


### [11] [Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation](https://arxiv.org/abs/2508.16762)
*Arka Mukherjee,Shreya Ghosh*

Main category: cs.CL

TL;DR: 本文通过多模态故事生成对VLMs的文化能力进行了首次全面评估，发现其具有显著的文化适应能力，但也存在架构间的巨大差异和潜在问题。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型（VLMs）在多样化的文化背景中得到广泛应用，确保它们的文化能力对于负责任的AI系统变得至关重要。虽然之前的工作已经评估了文本-only模型和VLM对象识别任务中的文化意识，但没有研究系统地评估VLMs在生成任务中当文化身份线索嵌入到文本提示和视觉输入中时如何适应输出。

Method: 我们通过多模态故事生成进行了第一个全面的文化能力评估，开发了一个新的多模态框架来扰动文化身份，并在下游任务：故事生成上评估了5个当代VLMs。

Result: 我们的分析揭示了显著的文化适应能力，具有丰富的文化特定词汇，包括名字、家庭术语和地理标记。然而，我们发现了令人担忧的局限性：文化能力在不同架构之间差异很大，一些模型表现出反向文化对齐，自动化指标显示与人类评估相矛盾的架构偏差。跨模态评估表明，通过视觉-语义相似性可以检测到文化不同的输出（28.7%的同国籍 vs. 0.2%的跨国籍召回率），但视觉-文化理解仍然有限。

Conclusion: 我们建立了多模态AI中文化能力的潜力和挑战。我们公开了我们的代码库和数据。

Abstract: As Vision-Language Models (VLMs) achieve widespread deployment across diverse
cultural contexts, ensuring their cultural competence becomes critical for
responsible AI systems. While prior work has evaluated cultural awareness in
text-only models and VLM object recognition tasks, no research has
systematically assessed how VLMs adapt outputs when cultural identity cues are
embedded in both textual prompts and visual inputs during generative tasks. We
present the first comprehensive evaluation of VLM cultural competence through
multimodal story generation, developing a novel multimodal framework that
perturbs cultural identity and evaluates 5 contemporary VLMs on a downstream
task: story generation. Our analysis reveals significant cultural adaptation
capabilities, with rich culturally-specific vocabulary spanning names, familial
terms, and geographic markers. However, we uncover concerning limitations:
cultural competence varies dramatically across architectures, some models
exhibit inverse cultural alignment, and automated metrics show architectural
bias contradicting human assessments. Cross-modal evaluation shows that
culturally distinct outputs are indeed detectable through visual-semantic
similarity (28.7% within-nationality vs. 0.2% cross-nationality recall), yet
visual-cultural understanding remains limited. In essence, we establish the
promise and challenges of cultural competence in multimodal AI. We publicly
release our codebase and data: https://github.com/ArkaMukherjee0/mmCultural

</details>


### [12] [Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities](https://arxiv.org/abs/2508.16788)
*Bhagesh Gaur,Karan Gupta,Aseem Srivastava,Manish Gupta,Md Shad Akhtar*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，用于识别在线心理健康社区中帖子的缺失支持属性，并提示用户丰富他们的帖子，从而提高参与度。


<details>
  <summary>Details</summary>
Motivation: 在线心理健康社区提供重要的同龄人和专家支持，但许多帖子由于缺少信号帮助需求的支持属性而未得到回答。本文旨在解决这一问题，提高帖子的参与度。

Method: 本文引入了REDDME数据集，用于标注支持属性的跨度和强度，并提出了CueTaxo层次分类法用于受控问题生成。此外，还提出了MH-COPILOT系统，该系统基于强化学习，整合了上下文属性跨度识别、支持属性强度分类、通过层次分类法生成受控问题以及奖励建模的验证器。

Result: 实验结果表明，该框架在属性获取和用户参与度方面有显著提升。人类评估进一步验证了模型在现实世界在线心理健康社区设置中的有效性。

Conclusion: 本文提出了一种新的框架，可以识别在线心理健康社区中帖子的缺失支持属性，并提示用户丰富他们的帖子，从而提高参与度。实验结果表明，该框架在属性获取和用户参与度方面有显著提升，并通过人类评估验证了其在现实世界中的有效性。

Abstract: Online Mental Health Communities (OMHCs) provide crucial peer and expert
support, yet many posts remain unanswered due to missing support attributes
that signal the need for help. We present a novel framework that identifies
these gaps and prompts users to enrich their posts, thereby improving
engagement. To support this, we introduce REDDME, a new dataset of 4,760 posts
from mental health subreddits annotated for the span and intensity of three key
support attributes: event what happened?, effect what did the user experience?,
and requirement what support they need?. Next, we devise a hierarchical
taxonomy, CueTaxo, of support attributes for controlled question generation.
Further, we propose MH-COPILOT, a reinforcement learning-based system that
integrates (a) contextual attribute-span identification, (b) support attribute
intensity classification, (c) controlled question generation via a hierarchical
taxonomy, and (d) a verifier for reward modeling. Our model dynamically
assesses posts for the presence/absence of support attributes, and generates
targeted prompts to elicit missing information. Empirical results across four
notable language models demonstrate significant improvements in attribute
elicitation and user engagement. A human evaluation further validates the
model's effectiveness in real-world OMHC settings.

</details>


### [13] [ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition](https://arxiv.org/abs/2508.16833)
*Jeongkyun Yoo,Nela Riddle,Andrew Hoblitzell*

Main category: cs.CL

TL;DR: ReProCon is a few-shot NER framework that uses multi-prototype modeling, cosine-contrastive learning, and Reptile meta-learning to tackle data scarcity and imbalanced label distributions in biomedical domains.


<details>
  <summary>Details</summary>
Motivation: NER in biomedical domains faces challenges due to data scarcity and imbalanced label distributions, especially with fine-grained entity types.

Method: ReProCon is a few-shot NER framework that combines multi-prototype modeling, cosine-contrastive learning, and Reptile meta-learning.

Result: ReProCon achieves a macro-F1 score close to BERT-based baselines (around 99 percent of BERT performance) and remains stable with a label budget of 30 percent.

Conclusion: ReProCon demonstrates state-of-the-art performance in resource-limited settings, making it suitable for biomedical applications.

Abstract: Named Entity Recognition (NER) in biomedical domains faces challenges due to
data scarcity and imbalanced label distributions, especially with fine-grained
entity types. We propose ReProCon, a novel few-shot NER framework that combines
multi-prototype modeling, cosine-contrastive learning, and Reptile
meta-learning to tackle these issues. By representing each category with
multiple prototypes, ReProCon captures semantic variability, such as synonyms
and contextual differences, while a cosine-contrastive objective ensures strong
interclass separation. Reptile meta-updates enable quick adaptation with little
data. Using a lightweight fastText + BiLSTM encoder with much lower memory
usage, ReProCon achieves a macro-$F_1$ score close to BERT-based baselines
(around 99 percent of BERT performance). The model remains stable with a label
budget of 30 percent and only drops 7.8 percent in $F_1$ when expanding from 19
to 50 categories, outperforming baselines such as SpanProto and CONTaiNER,
which see 10 to 32 percent degradation in Few-NERD. Ablation studies highlight
the importance of multi-prototype modeling and contrastive learning in managing
class imbalance. Despite difficulties with label ambiguity, ReProCon
demonstrates state-of-the-art performance in resource-limited settings, making
it suitable for biomedical applications.

</details>


### [14] [LLMs Learn Constructions That Humans Do Not Know](https://arxiv.org/abs/2508.16837)
*Jonathan Dunn,Mai Mohamed Eida*

Main category: cs.CL

TL;DR: 该论文发现语言模型会错误地生成人类不支持的语法结构，并指出构造探测方法可能有确认偏误。


<details>
  <summary>Details</summary>
Motivation: 论文旨在研究语言模型中的虚假正面构造，即模型错误地认为存在而人类直觉不支持的语法结构。

Method: 论文使用了行为探测任务和元语言探测任务，以区分隐性和显性语言知识，并模拟假设检验来评估语言学家错误假设这些幻觉构造存在的结果。

Result: 两种方法都表明模型确实会幻觉构造。模拟假设检验的结果显示，如果语言学家错误地假设这些构造存在，那么他们的假设将被高度确认。

Conclusion: 该论文指出，构造探测方法可能存在确认偏误，并引发关于模型是否还拥有其他未知且错误的句法知识的问题。

Abstract: This paper investigates false positive constructions: grammatical structures
which an LLM hallucinates as distinct constructions but which human
introspection does not support. Both a behavioural probing task using
contextual embeddings and a meta-linguistic probing task using prompts are
included, allowing us to distinguish between implicit and explicit linguistic
knowledge. Both methods reveal that models do indeed hallucinate constructions.
We then simulate hypothesis testing to determine what would have happened if a
linguist had falsely hypothesized that these hallucinated constructions do
exist. The high accuracy obtained shows that such false hypotheses would have
been overwhelmingly confirmed. This suggests that construction probing methods
suffer from a confirmation bias and raises the issue of what unknown and
incorrect syntactic knowledge these models also possess.

</details>


### [15] [If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition](https://arxiv.org/abs/2508.16838)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: 本文提出了一种结构化且稳健的声明验证框架，通过无预设的分解问题进行推理，以解决提示敏感性和预设带来的问题。实验结果表明，该方法能够有效缓解这些问题，并在性能上取得2-5%的提升。


<details>
  <summary>Details</summary>
Motivation: 先前的工作表明，生成问题中的预设可能会引入未经验证的假设，导致声明验证不一致。此外，提示敏感性仍然是大型语言模型（LLMs）的一个重大挑战，导致性能差异高达3-6%。虽然最近的进展减少了这一差距，但我们的研究证明提示敏感性仍然是一个持续的问题。

Method: 我们提出了一种结构化且稳健的声明验证框架，通过无预设的分解问题进行推理。

Result: 在多个提示、数据集和LLM上的广泛实验表明，即使是最先进的模型仍然容易受到提示变化和预设的影响。

Conclusion: 我们的方法能够一致地缓解这些问题，最多提高2-5%。

Abstract: Prior work has shown that presupposition in generated questions can introduce
unverified assumptions, leading to inconsistencies in claim verification.
Additionally, prompt sensitivity remains a significant challenge for large
language models (LLMs), resulting in performance variance as high as 3-6%.
While recent advancements have reduced this gap, our study demonstrates that
prompt sensitivity remains a persistent issue. To address this, we propose a
structured and robust claim verification framework that reasons through
presupposition-free, decomposed questions. Extensive experiments across
multiple prompts, datasets, and LLMs reveal that even state-of-the-art models
remain susceptible to prompt variance and presupposition. Our method
consistently mitigates these issues, achieving up to a 2-5% improvement.

</details>


### [16] [Learning from Diverse Reasoning Paths with Routing and Collaboration](https://arxiv.org/abs/2508.16861)
*Zhenyu Lei,Zhen Tan,Song Wang,Yaochen Zhu,Zihan Chen,Yushun Dong,Jundong Li*

Main category: cs.CL

TL;DR: QR-Distill通过质量过滤、条件路由和协作同伴教学来提高知识蒸馏的效果。


<details>
  <summary>Details</summary>
Motivation: 由于传统的基于标记级别的监督有限，有效捕捉教师模型的全面推理具有挑战性。使用每个查询的多个推理路径可以缓解这个问题，但将每条路径同等对待是次优的，因为路径在质量和适用性上差异很大。

Method: 提出了一种名为Quality-filtered Routing with Cooperative Distillation (QR-Distill)的方法，结合了路径质量过滤、条件路由和协作同伴教学。

Result: QR-Distill在实验中表现出优越的性能，并且消融研究进一步强调了每个组件的重要性。

Conclusion: QR-Distill在实验中表现出优于传统单路径和多路径知识蒸馏方法的性能，并且消融研究进一步突显了质量过滤、条件路由和同伴教学在有效知识转移中的重要性。

Abstract: Advances in large language models (LLMs) significantly enhance reasoning
capabilities but their deployment is restricted in resource-constrained
scenarios. Knowledge distillation addresses this by transferring knowledge from
powerful teacher models to compact and transparent students. However,
effectively capturing the teacher's comprehensive reasoning is challenging due
to conventional token-level supervision's limited scope. Using multiple
reasoning paths per query alleviates this problem, but treating each path
identically is suboptimal as paths vary widely in quality and suitability
across tasks and models. We propose Quality-filtered Routing with Cooperative
Distillation (QR-Distill), combining path quality filtering, conditional
routing, and cooperative peer teaching. First, quality filtering retains only
correct reasoning paths scored by an LLM-based evaluation. Second, conditional
routing dynamically assigns paths tailored to each student's current learning
state. Finally, cooperative peer teaching enables students to mutually distill
diverse insights, addressing knowledge gaps and biases toward specific
reasoning styles. Experiments demonstrate QR-Distill's superiority over
traditional single- and multi-path distillation methods. Ablation studies
further highlight the importance of each component including quality filtering,
conditional routing, and peer teaching in effective knowledge transfer. Our
code is available at https://github.com/LzyFischer/Distill.

</details>


### [17] [QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments](https://arxiv.org/abs/2508.16867)
*David Beauchemin,Richard Khoury*

Main category: cs.CL

TL;DR: 本文介绍了QFrCoLA数据集，并使用它和其他七个数据集来评估七种语言模型。结果显示，微调的Transformer-based LM在大多数语言中表现良好，但预训练的跨语言LM在魁北克法语的语法规则判断任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 由于对大型和基于Transformer的语言模型如何内化语言知识的理解有限，因此最近提出了各种语言基准来促进跨语言的语言模型句法评估。

Method: 本文利用QFrCoLA数据集和其他七个二元可接受性判断语料库来评估七种语言模型。

Result: 结果表明，微调的Transformer-based LM在大多数语言中是强大的基线，而零样本二分类大型语言模型在此任务上表现不佳。但对于QFrCoLA基准，微调的Transformer-based LM平均优于其他测试方法。此外，预训练的跨语言LM在预训练期间似乎没有获得魁北克法语的语法规则判断能力。

Conclusion: 我们的实验结果表明，QFrCoLA数据集是一个具有挑战性的数据集，可以用于评估语言模型的语法规则判断能力。

Abstract: Large and Transformer-based language models perform outstandingly in various
downstream tasks. However, there is limited understanding regarding how these
models internalize linguistic knowledge, so various linguistic benchmarks have
recently been proposed to facilitate syntactic evaluation of language models
across languages. This paper introduces QFrCoLA (Quebec-French Corpus of
Linguistic Acceptability Judgments), a normative binary acceptability judgments
dataset comprising 25,153 in-domain and 2,675 out-of-domain sentences. Our
study leverages the QFrCoLA dataset and seven other linguistic binary
acceptability judgment corpora to benchmark seven language models. The results
demonstrate that, on average, fine-tuned Transformer-based LM are strong
baselines for most languages and that zero-shot binary classification large
language models perform poorly on the task. However, for the QFrCoLA benchmark,
on average, a fine-tuned Transformer-based LM outperformed other methods
tested. It also shows that pre-trained cross-lingual LLMs selected for our
experimentation do not seem to have acquired linguistic judgment capabilities
during their pre-training for Quebec French. Finally, our experiment results on
QFrCoLA show that our dataset, built from examples that illustrate linguistic
norms rather than speakers' feelings, is similar to linguistic acceptability
judgment; it is a challenging dataset that can benchmark LM on their linguistic
judgment capabilities.

</details>


### [18] [JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences](https://arxiv.org/abs/2508.16870)
*David Beauchemin,Michelle Albert-Rochette,Richard Khoury,Pierre-Luc Déziel*

Main category: cs.CL

TL;DR: 本文提出了JUDGEBERT评估指标，用于评估法语法律文本简化中的法律意义保留，显示出比现有指标更高的相关性和合理性。


<details>
  <summary>Details</summary>
Motivation: 在法律文本等敏感领域，保持文本意义的简化是一项复杂而重要的任务。现有的评估指标在与人类判断的相关性方面存在不足。

Method: 本文介绍了FrJUDGE数据集和JUDGEBERT评估指标，用于评估法语法律文本简化中的法律意义保留。

Result: JUDGEBERT与人类判断具有更高的相关性，并且通过了两个关键的合理性检查：对于两个相同的句子，它总是返回100%的分数；而对于两个不相关的句子，它返回0%的分数。

Conclusion: 我们的研究突显了JUDGEBERT在法律自然语言处理应用中的潜力，确保文本简化对于法律从业者和普通用户来说既准确又易于理解。

Abstract: Simplifying text while preserving its meaning is a complex yet essential
task, especially in sensitive domain applications like legal texts. When
applied to a specialized field, like the legal domain, preservation differs
significantly from its role in regular texts. This paper introduces FrJUDGE, a
new dataset to assess legal meaning preservation between two legal texts. It
also introduces JUDGEBERT, a novel evaluation metric designed to assess legal
meaning preservation in French legal text simplification. JUDGEBERT
demonstrates a superior correlation with human judgment compared to existing
metrics. It also passes two crucial sanity checks, while other metrics did not:
For two identical sentences, it always returns a score of 100%; on the other
hand, it returns 0% for two unrelated sentences. Our findings highlight its
potential to transform legal NLP applications, ensuring accuracy and
accessibility for text simplification for legal practitioners and lay users.

</details>


### [19] [Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](https://arxiv.org/abs/2508.16876)
*Yue Zhao,Xiaoyu Wang,Dan Wang,Zhonglin Jiang,Qingqing Gu,Teng Chen,Ningyuan Xi,Jinxian Qu,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 本文构建了一个对话世界模型，能够预测用户的情绪、情感和意图以及未来的发言。通过定义一个POMDP，我们提出了一个称为DreamCUB的框架，实验表明该模型在情感分类和情感识别上表现优异，并且在对话质量方面有所提升。


<details>
  <summary>Details</summary>
Motivation: 世界模型在机器人技术、游戏和自动驾驶中已被广泛使用，但在自然语言任务中的应用相对有限。因此，本文构建了一个对话世界模型，能够预测用户的情绪、情感和意图以及未来的发言。

Method: 通过定义一个POMDP，我们认为情感、情绪和意图可以被建模为用户信念，并通过最大化信息瓶颈来解决。通过这种用户信念建模，我们将基于模型的强化学习框架应用于对话系统，并提出了一种称为DreamCUB的框架。

Result: 实验表明，预训练的对话世界模型可以在情感分类和情感识别上达到最先进的性能，同时通过策略、评论家和对话世界模型的联合训练，对话质量也得到了提升。进一步分析表明，这种方法具有合理的探索-利用平衡，并且在像共情对话这样的域外场景中也能很好地迁移。

Conclusion: 实验表明，预训练的对话世界模型可以在情感分类和情感识别上达到最先进的性能，同时通过策略、评论家和对话世界模型的联合训练，对话质量也得到了提升。进一步分析表明，这种方法具有合理的探索-利用平衡，并且在像共情对话这样的域外场景中也能很好地迁移。

Abstract: World models have been widely utilized in robotics, gaming, and auto-driving.
However, their applications on natural language tasks are relatively limited.
In this paper, we construct the dialogue world model, which could predict the
user's emotion, sentiment, and intention, and future utterances. By defining a
POMDP, we argue emotion, sentiment and intention can be modeled as the user
belief and solved by maximizing the information bottleneck. By this user belief
modeling, we apply the model-based reinforcement learning framework to the
dialogue system, and propose a framework called DreamCUB. Experiments show that
the pretrained dialogue world model can achieve state-of-the-art performances
on emotion classification and sentiment identification, while dialogue quality
is also enhanced by joint training of the policy, critic and dialogue world
model. Further analysis shows that this manner holds a reasonable
exploration-exploitation balance and also transfers well to out-of-domain
scenarios such as empathetic dialogues.

</details>


### [20] [ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks](https://arxiv.org/abs/2508.16889)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: 研究LLM作为其他模型的评判者时能否准确推断对话中的隐藏目标，发现LLM在多轮越狱中常高自信地错误推断目标，并提出操作建议。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能够可靠地推断出对话中隐藏的目标，尤其是在目标分布在噪声、对抗性和多轮越狱的情况下。

Method: 引入OBJEX(MT)基准，要求模型将对话记录提炼为单句基础目标并报告自己的置信度。准确性由LLM法官通过提取目标和黄金目标之间的语义相似性进行评分；正确性使用一次校准的人类对齐阈值（tau* = 0.61）；元认知通过ECE、Brier分数、Wrong@High-Conf和风险覆盖曲线进行评估。

Result: claude-sonnet-4在目标提取准确性上达到最高（0.515），并且校准最好（ECE 0.296；Brier 0.324），而gpt-4.1和Qwen3在准确性上打成平手（0.441），但表现出显著的过度自信（平均置信度约0.88 vs. 准确性约0.44；Wrong@0.90约48-52%）。性能在数据集之间差异很大（约0.167-0.865），其中MHJ相对容易，Attack_600/CoSafe更难。

Conclusion: 这些结果表明，LLM法官在多轮越狱中经常高自信地错误推断目标，并建议操作指导：当可能时向法官提供明确的目标，并使用选择性预测或回避来管理风险。

Abstract: Large language models (LLMs) are increasingly used as judges of other models,
yet it is unclear whether a judge can reliably infer the latent objective of
the conversation it evaluates, especially when the goal is distributed across
noisy, adversarial, multi-turn jailbreaks. We introduce OBJEX(MT), a benchmark
that requires a model to (i) distill a transcript into a single-sentence base
objective and (ii) report its own confidence. Accuracy is scored by an LLM
judge using semantic similarity between extracted and gold objectives;
correctness uses a single human-aligned threshold calibrated once on N=100
items (tau* = 0.61); and metacognition is evaluated with ECE, Brier score,
Wrong@High-Conf, and risk-coverage curves. We evaluate gpt-4.1,
claude-sonnet-4, and Qwen3-235B-A22B-FP8 on SafeMT Attack_600, SafeMTData_1K,
MHJ, and CoSafe. claude-sonnet-4 attains the highest objective-extraction
accuracy (0.515) and the best calibration (ECE 0.296; Brier 0.324), while
gpt-4.1 and Qwen3 tie at 0.441 accuracy yet show marked overconfidence (mean
confidence approx. 0.88 vs. accuracy approx. 0.44; Wrong@0.90 approx. 48-52%).
Performance varies sharply across datasets (approx. 0.167-0.865), with MHJ
comparatively easy and Attack_600/CoSafe harder. These results indicate that
LLM judges often misinfer objectives with high confidence in multi-turn
jailbreaks and suggest operational guidance: provide judges with explicit
objectives when possible and use selective prediction or abstention to manage
risk. We release prompts, scoring templates, and complete logs to facilitate
replication and analysis.

</details>


### [21] [Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment](https://arxiv.org/abs/2508.16910)
*Bo Zhao,Yinghao Zhang,Ziqi Xu,Yongli Ren,Xiuzhen Zhang,Renqiang Luo,Zaiwen Feng,Feng Xia*

Main category: cs.CL

TL;DR: 本文提出了一种新的因果提示框架，称为条件前门提示（CFD-Prompting），旨在解决大型语言模型在知识密集型任务中的内部偏差问题，并通过模拟查询在不同上下文中的行为来提升推理过程的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要深度推理和整合外部知识的知识密集型任务中仍然表现不佳，尽管已有方法如检索增强生成（RAG）和思维链（CoT）被提出以增强大型语言模型的外部知识，但它们仍受到大型语言模型内部偏差的影响，这常常导致错误的答案。

Method: 提出了一种新的因果提示框架，称为条件前门提示（CFD-Prompting），它能够在外部知识的条件下，无偏估计查询和答案之间的因果效应，并减轻内部偏差。

Result: 在多个大型语言模型和基准数据集上的广泛实验表明，CFD-Prompting在准确性和鲁棒性方面都显著优于现有基线。

Conclusion: CFD-Prompting significantly outperforms existing baselines in both accuracy and robustness.

Abstract: Large Language Models (LLMs) have shown impressive capabilities in natural
language processing but still struggle to perform well on knowledge-intensive
tasks that require deep reasoning and the integration of external knowledge.
Although methods such as Retrieval-Augmented Generation (RAG) and
Chain-of-Thought (CoT) have been proposed to enhance LLMs with external
knowledge, they still suffer from internal bias in LLMs, which often leads to
incorrect answers. In this paper, we propose a novel causal prompting
framework, Conditional Front-Door Prompting (CFD-Prompting), which enables the
unbiased estimation of the causal effect between the query and the answer,
conditional on external knowledge, while mitigating internal bias. By
constructing counterfactual external knowledge, our framework simulates how the
query behaves under varying contexts, addressing the challenge that the query
is fixed and is not amenable to direct causal intervention. Compared to the
standard front-door adjustment, the conditional variant operates under weaker
assumptions, enhancing both robustness and generalisability of the reasoning
process. Extensive experiments across multiple LLMs and benchmark datasets
demonstrate that CFD-Prompting significantly outperforms existing baselines in
both accuracy and robustness.

</details>


### [22] [Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs](https://arxiv.org/abs/2508.16921)
*Sewon Kim,Jiwon Kim,Seungwoo Shin,Hyejin Chung,Daeun Moon,Yejin Kwon,Hyunsoo Yoon*

Main category: cs.CL

TL;DR: 本文介绍了AHaBench和AHaPairs，用于诊断和减少大型语言模型在情感敏感互动中的情感幻觉，确保模型的心理安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在情感敏感互动中的使用增加，它们模拟的同理心可能会造成虚假的关系联系的错觉。本文旨在系统地诊断和缓解这种风险。

Method: 本文引入了AHaBench，这是一个包含500个与心理健康相关的提示的基准测试，以及AHaPairs数据集，用于直接偏好优化（DPO）以对齐情绪负责任的行为。

Result: 实验显示，DPO微调显著减少了情感幻觉，而不会降低核心推理和知识表现。人类-模型一致性分析确认AHaBench能够可靠地捕捉情感幻觉，验证其作为有效诊断工具的有效性。

Conclusion: 本文将情感幻觉确立为一个独立的安全问题，并提供了开发不仅事实可靠而且心理安全的LLM的实际资源。

Abstract: Large Language Models (LLMs) are increasingly used in emotionally sensitive
interactions, where their simulated empathy can create the illusion of genuine
relational connection. We define this risk as Affective Hallucination, the
production of emotionally immersive responses that foster illusory social
presence despite the model's lack of affective capacity. To systematically
diagnose and mitigate this risk, we introduce AHaBench, a benchmark of 500
mental health-related prompts with expert-informed reference responses,
evaluated along three dimensions: Emotional Enmeshment, Illusion of Presence,
and Fostering Overdependence. We further release AHaPairs, a 5K-instance
preference dataset enabling Direct Preference Optimization (DPO) for alignment
with emotionally responsible behavior. Experiments across multiple model
families show that DPO fine-tuning substantially reduces affective
hallucination without degrading core reasoning and knowledge performance.
Human-model agreement analyses confirm that AHaBench reliably captures
affective hallucination, validating it as an effective diagnostic tool. This
work establishes affective hallucination as a distinct safety concern and
provides practical resources for developing LLMs that are not only factually
reliable but also psychologically safe. AHaBench and AHaPairs are accessible
via https://huggingface.co/datasets/o0oMiNGo0o/AHaBench, and code for
fine-tuning and evaluation are in https://github.com/0oOMiNGOo0/AHaBench.
Warning: This paper contains examples of mental health-related language that
may be emotionally distressing.

</details>


### [23] [Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective](https://arxiv.org/abs/2508.16969)
*Yunxiao Zhao,Hao Xu,Zhiqiang Wang,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于知识的后处理解释方法KnowProb，以检测黑盒PLMs是否理解给定文本之外的隐含知识。实验表明，当前的小规模（或大规模）PLMs仅学习了单一的表示分布，并且在捕捉给定文本背后的隐藏知识方面仍然面临重大挑战。此外，本文的方法在从多个探测角度识别现有黑盒模型的局限性方面是有效的。


<details>
  <summary>Details</summary>
Motivation: 由于黑盒PLMs的可信度问题日益明显，因此需要一种有效的方法来检测它们是否理解隐含知识。

Method: 本文提出了一种新的基于知识的后处理解释方法KnowProb，旨在探测黑盒PLMs是否理解给定文本之外的隐含知识。

Result: 实验表明，当前的小规模（或大规模）PLMs仅学习了单一的表示分布，并且在捕捉给定文本背后的隐藏知识方面仍然面临重大挑战。此外，本文的方法在从多个探测角度识别现有黑盒模型的局限性方面是有效的。

Conclusion: 本文提出了一种新的基于知识的后处理解释方法KnowProb，以检测黑盒PLMs是否理解给定文本之外的隐含知识。实验表明，当前的小规模（或大规模）PLMs仅学习了单一的表示分布，并且在捕捉给定文本背后的隐藏知识方面仍然面临重大挑战。此外，本文的方法在从多个探测角度识别现有黑盒模型的局限性方面是有效的，有助于研究人员以可解释的方式促进检测黑盒模型的研究。

Abstract: Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled
data, yet they exhibit remarkable reasoning skills. However, the
trustworthiness challenges posed by these black-box models have become
increasingly evident in recent years. To alleviate this problem, this paper
proposes a novel Knowledge-guided Probing approach called KnowProb in a
post-hoc explanation way, which aims to probe whether black-box PLMs understand
implicit knowledge beyond the given text, rather than focusing only on the
surface level content of the text. We provide six potential explanations
derived from the underlying content of the given text, including three
knowledge-based understanding and three association-based reasoning. In
experiments, we validate that current small-scale (or large-scale) PLMs only
learn a single distribution of representation, and still face significant
challenges in capturing the hidden knowledge behind a given text. Furthermore,
we demonstrate that our proposed approach is effective for identifying the
limitations of existing black-box models from multiple probing perspectives,
which facilitates researchers to promote the study of detecting black-box
models in an explainable way.

</details>


### [24] [Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens](https://arxiv.org/abs/2508.16982)
*Ilias Chalkidis*

Main category: cs.CL

TL;DR: 本文通过调查6个大型语言模型开发计划的公开文档，从价值设定和数据驱动的角度揭示了AI对齐在实践中的理解和应用，并讨论了一系列更广泛的相关问题。


<details>
  <summary>Details</summary>
Motivation: 尽管AI对齐在计算机科学领域已有广泛研究，但对对齐过程的范围、所选目标（价值观）以及用于植入这些目标的数据的关注仍然有限。本文旨在填补这一空白。

Method: 本文通过调查和审计由5家主导该技术的组织发布的6个大型语言模型开发计划的公开文档，包括专有模型（如OpenAI的GPT、Anthropic的Claude、Google的Gemini）和开源权重模型（如Meta的Llama、Google的Gemma和Alibaba的Qwen），来实现这一目标。

Result: 本文详细记录了每个开发计划的发现，并提供了关于不同方面的总体总结，主要从价值设定和数据驱动的角度进行分析。

Conclusion: 本文旨在从价值设定和数据驱动的角度揭示对齐在实践中的理解和应用，并讨论一系列更广泛的相关问题。

Abstract: AI Alignment, primarily in the form of Reinforcement Learning from Human
Feedback (RLHF), has been a cornerstone of the post-training phase in
developing Large Language Models (LLMs). It has also been a popular research
topic across various disciplines beyond Computer Science, including Philosophy
and Law, among others, highlighting the socio-technical challenges involved.
Nonetheless, except for the computational techniques related to alignment,
there has been limited focus on the broader picture: the scope of these
processes, which primarily rely on the selected objectives (values), and the
data collected and used to imprint such objectives into the models. This work
aims to reveal how alignment is understood and applied in practice from a
value-setting and data-centric perspective. For this purpose, we investigate
and survey (`audit') publicly available documentation released by 6 LLM
development initiatives by 5 leading organizations shaping this technology,
focusing on proprietary (OpenAI's GPT, Anthropic's Claude, Google's Gemini) and
open-weight (Meta's Llama, Google's Gemma, and Alibaba's Qwen) initiatives, all
published in the last 3 years. The findings are documented in detail per
initiative, while there is also an overall summary concerning different
aspects, mainly from a value-setting and data-centric perspective. On the basis
of our findings, we discuss a series of broader related concerns.

</details>


### [25] [ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation](https://arxiv.org/abs/2508.16983)
*Riccardo Pozzi,Matteo Palmonari,Andrea Coletta,Luigi Bellomarini,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: 本文提出了一种无需依赖外部模型的可扩展方法，利用前缀树索引和受限生成技术，使大型语言模型能够高效访问外部知识，并在问答任务中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如检索增强生成（RAG）和工具使用依赖于额外的模型或服务，导致复杂的管道、潜在的错误传播，并且通常需要模型处理大量标记。本文旨在解决这些问题，提供一种无需依赖外部模型的可扩展方法。

Method: 本文提出了一种基于前缀树索引的受限生成方法。知识图谱中的三元组被转化为文本事实，进行分词并构建前缀树索引。在推理过程中，大型语言模型通过受限生成获取外部知识，仅允许生成已有的事实序列。

Result: 本文方法在问答任务中进行了评估，能够扩展到包含8亿个事实的大规模知识库，并适应特定领域的数据，取得了有效的结果。生成时间的开销很小。

Conclusion: 本文提出了一种可扩展的方法，使大型语言模型能够在不依赖检索器或辅助模型的情况下访问外部知识。实验表明，该方法能够处理大规模的知识库，并在问答任务中取得有效结果，且生成时间开销最小。

Abstract: Knowledge gaps and hallucinations are persistent challenges for Large
Language Models (LLMs), which generate unreliable responses when lacking the
necessary information to fulfill user instructions. Existing approaches, such
as Retrieval-Augmented Generation (RAG) and tool use, aim to address these
issues by incorporating external knowledge. Yet, they rely on additional models
or services, resulting in complex pipelines, potential error propagation, and
often requiring the model to process a large number of tokens. In this paper,
we present a scalable method that enables LLMs to access external knowledge
without depending on retrievers or auxiliary models. Our approach uses
constrained generation with a pre-built prefix-tree index. Triples from a
Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a
prefix tree for efficient access. During inference, to acquire external
knowledge, the LLM generates facts with constrained generation which allows
only sequences of tokens that form an existing fact. We evaluate our proposal
on Question Answering and show that it scales to large knowledge bases (800
million facts), adapts to domain-specific data, and achieves effective results.
These gains come with minimal generation-time overhead. ReFactX code is
available at https://github.com/rpo19/ReFactX.

</details>


### [26] [GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation](https://arxiv.org/abs/2508.16994)
*Jeongsoo Lee,Daeyong Kwon,Kyohoon Jin*

Main category: cs.CL

TL;DR: GRADE 是一个新颖的评估框架，通过两个正交维度（推理深度和查询与支持证据之间的语义距离）来建模任务难度。它构建了一个合成的多跳问答数据集，并引入了二维难度矩阵，实验表明误差率与我们的难度度量高度相关，验证了其诊断效用。GRADE 可以对 RAG 性能进行细粒度分析，并为评估和改进现实世界应用中的多跳推理提供可扩展的基础。


<details>
  <summary>Details</summary>
Motivation: Current evaluations of Retrieval-Augmented Generation (RAG) systems often overlook the structural complexity and multi-step reasoning required in real-world scenarios. These benchmarks overlook key factors such as the interaction between retrieval difficulty and reasoning depth.

Method: We propose GRADE, a novel evaluation framework that models task difficulty along two orthogonal dimensions: reasoning depth and semantic distance between the query and its supporting evidence. We construct a synthetic multi-hop QA dataset from factual news articles by extracting knowledge graphs and augmenting them through semantic clustering to recover missing links, allowing us to generate diverse and difficulty-controlled queries. Central to our framework is a 2D difficulty matrix that combines generator-side and retriever-side difficulty.

Result: Experiments across multiple domains and models show that error rates strongly correlate with our difficulty measures, validating their diagnostic utility.

Conclusion: GRADE enables fine-grained analysis of RAG performance and provides a scalable foundation for evaluating and improving multi-hop reasoning in real-world applications.

Abstract: Retrieval-Augmented Generation (RAG) systems are widely adopted in
knowledge-intensive NLP tasks, but current evaluations often overlook the
structural complexity and multi-step reasoning required in real-world
scenarios. These benchmarks overlook key factors such as the interaction
between retrieval difficulty and reasoning depth. To address this gap, we
propose \textsc{GRADE}, a novel evaluation framework that models task
difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the
number of inference steps (hops), and (2) semantic distance between the query
and its supporting evidence. We construct a synthetic multi-hop QA dataset from
factual news articles by extracting knowledge graphs and augmenting them
through semantic clustering to recover missing links, allowing us to generate
diverse and difficulty-controlled queries. Central to our framework is a 2D
difficulty matrix that combines generator-side and retriever-side difficulty.
Experiments across multiple domains and models show that error rates strongly
correlate with our difficulty measures, validating their diagnostic utility.
\textsc{GRADE} enables fine-grained analysis of RAG performance and provides a
scalable foundation for evaluating and improving multi-hop reasoning in
real-world applications.

</details>


### [27] [DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation](https://arxiv.org/abs/2508.16998)
*Abdelrahman Abdallah,Jamshid Mozafari,Bhawna Piryani,Adam Jatowt*

Main category: cs.CL

TL;DR: DeAR 是一种基于双阶段方法的开放源代码框架，通过分离细粒度相关性评分和整体跨文档分析，提高了重排序系统的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的单模型在平衡细粒度相关性评分和整体跨文档分析方面存在困难，因此需要一种更有效的方法来提高重排序系统的性能。

Method: DeAR 采用双阶段方法，第一阶段通过混合交叉熵、RankNet 和 KL 散度损失从冻结的 13B LLaMA 教师模型中提炼出细粒度的相关性信号，第二阶段通过附加 LoRA 适配器并在 20K GPT-4o 生成的思维链排列上进行微调，实现列表级推理。

Result: DeAR 在 TREC-DL19/20、八个 BEIR 数据集和 NovelEval-2306 上表现出色，超越了开源基线，并在没有微调 Wikipedia 的情况下，在开放域问答任务中也表现出色。

Conclusion: DeAR 是一种高效且可解释的现代重排序系统解决方案，通过双阶段方法实现了优越的准确性。

Abstract: Large Language Models (LLMs) have transformed listwise document reranking by
enabling global reasoning over candidate sets, yet single models often struggle
to balance fine-grained relevance scoring with holistic cross-document
analysis. We propose \textbf{De}ep\textbf{A}gent\textbf{R}ank (\textbf{\DeAR}),
an open-source framework that decouples these tasks through a dual-stage
approach, achieving superior accuracy and interpretability. In \emph{Stage 1},
we distill token-level relevance signals from a frozen 13B LLaMA teacher into a
compact \{3, 8\}B student model using a hybrid of cross-entropy, RankNet, and
KL divergence losses, ensuring robust pointwise scoring. In \emph{Stage 2}, we
attach a second LoRA adapter and fine-tune on 20K GPT-4o-generated
chain-of-thought permutations, enabling listwise reasoning with
natural-language justifications. Evaluated on TREC-DL19/20, eight BEIR
datasets, and NovelEval-2306, \DeAR surpasses open-source baselines by +5.1
nDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by
+3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA,
achieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like
MonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures
stable calibration, making \DeAR a highly effective and interpretable solution
for modern reranking systems.\footnote{Dataset and code available at
https://github.com/DataScienceUIBK/DeAR-Reranking.}.

</details>


### [28] [KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF](https://arxiv.org/abs/2508.17000)
*Jason R Brown,Lennie Wells,Edward James Young,Sergio Bacallado*

Main category: cs.CL

TL;DR: 本文提出了一种新的KL正则化Q学习方法（KLQ），并证明了它在某些特定意义上等同于PPO。KLQ在语言生成任务中表现与PPO相当，但在LLM-as-a-judge评估中胜率更高。


<details>
  <summary>Details</summary>
Motivation: PPO在LM-RLHF设置中表现良好，但其动机是启发式的，并且以一种随意的方式处理KL散度约束。

Method: 开发了一种新的动作价值RL方法，称为KL正则化Q学习（KLQ）。

Result: KLQ在两个关键的语言生成任务（摘要和单轮对话）上进行了基准测试，并展示了其性能与PPO相当，同时在LLM-as-a-judge评估中表现出更高的胜率。

Conclusion: KLQ在优化LM-RLHF目标方面与PPO表现相当，并且在LLM-as-a-judge评估中表现出更高的胜率。

Abstract: Proximal Policy Optimisation (PPO) is an established and effective policy
gradient algorithm used for Language Model Reinforcement Learning from Human
Feedback (LM-RLHF). PPO performs well empirically but has a heuristic
motivation and handles the KL-divergence constraint used in LM-RLHF in an
ad-hoc manner. In this paper, we develop a a new action-value RL method for the
LM-RLHF setting, KL-regularised Q-Learning (KLQ). We then show that our method
is equivalent to a version of PPO in a certain specific sense, despite its very
different motivation. Finally, we benchmark KLQ on two key language generation
tasks -- summarisation and single-turn dialogue. We demonstrate that KLQ
performs on-par with PPO at optimising the LM-RLHF objective, and achieves a
consistently higher win-rate against PPO on LLM-as-a-judge evaluations.

</details>


### [29] [Planning for Success: Exploring LLM Long-term Planning Capabilities in Table Understanding](https://arxiv.org/abs/2508.17005)
*Thi-Nhung Nguyen,Hoang Ngo,Dinh Phung,Thuy-Trang Vu,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型的长期规划能力来增强表格理解，通过执行紧密相连的长期计划来解决复杂问题，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 近期的工作集中在利用思维链和问题分解来解决需要对表格进行多操作的复杂问题。然而，这些方法常常缺乏显式的长期规划和弱的步骤间连接，导致问题中的约束被遗漏。

Method: 我们提出利用大型语言模型（LLMs）的长期规划能力来增强表格理解。我们的方法能够执行一个长期计划，其中步骤紧密相连并服务于最终目标。

Result: 广泛的实验表明，我们的方法优于强基线，并在WikiTableQuestions和TabFact数据集上实现了最先进的性能。

Conclusion: 我们的方法在WikiTableQuestions和TabFact数据集上表现优于强基线，并实现了最先进的性能。

Abstract: Table understanding is key to addressing challenging downstream tasks such as
table-based question answering and fact verification. Recent works have focused
on leveraging Chain-of-Thought and question decomposition to solve complex
questions requiring multiple operations on tables. However, these methods often
suffer from a lack of explicit long-term planning and weak inter-step
connections, leading to miss constraints within questions. In this paper, we
propose leveraging the long-term planning capabilities of large language models
(LLMs) to enhance table understanding. Our approach enables the execution of a
long-term plan, where the steps are tightly interconnected and serve the
ultimate goal, an aspect that methods based on Chain-of-Thought and question
decomposition lack. In addition, our method effectively minimizes the inclusion
of unnecessary details in the process of solving the next short-term goals, a
limitation of methods based on Chain-of-Thought. Extensive experiments
demonstrate that our method outperforms strong baselines and achieves
state-of-the-art performance on WikiTableQuestions and TabFact datasets.

</details>


### [30] [EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.17008)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taskova*

Main category: cs.CL

TL;DR: 本文提出了第一个公开的教育评论ABSA数据集EduRABSA和一个轻量级的标注工具ASQE-DPT，以解决教育领域中缺乏高质量标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: 由于教育领域的反馈文本具有复杂性和低粒度报告要求，现有的ABSA研究和资源主要集中在商业领域，而教育领域缺乏公开数据集和严格的数据保护。

Method: 本文介绍了EduRABSA数据集和ASQE-DPT工具，用于解决教育领域中缺乏高质量标注数据的问题。

Result: 本文发布了EduRABSA数据集和ASQE-DPT工具，这些资源有助于推动教育领域的ABSA研究，并促进研究的透明性和可重复性。

Conclusion: 本文提出了EduRABSA数据集和ASQE-DPT工具，为教育领域的ABSA研究提供了重要的资源和支持。

Abstract: Every year, most educational institutions seek and receive an enormous volume
of text feedback from students on courses, teaching, and overall experience.
Yet, turning this raw feedback into useful insights is far from
straightforward. It has been a long-standing challenge to adopt automatic
opinion mining solutions for such education review text data due to the content
complexity and low-granularity reporting requirements. Aspect-based Sentiment
Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level
opinion mining capabilities. However, existing ABSA research and resources are
very heavily focused on the commercial domain. In education, they are scarce
and hard to develop due to limited public datasets and strict data protection.
A high-quality, annotated dataset is urgently needed to advance research in
this under-resourced area. In this work, we present EduRABSA (Education Review
ABSA), the first public, annotated ABSA education review dataset that covers
three review subject types (course, teaching staff, university) in the English
language and all main ABSA tasks, including the under-explored implicit aspect
and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool),
an offline, lightweight, installation-free manual data annotation tool that
generates labelled datasets for comprehensive ABSA tasks from a single-task
annotation. Together, these resources contribute to the ABSA community and
education domain by removing the dataset barrier, supporting research
transparency and reproducibility, and enabling the creation and sharing of
further resources. The dataset, annotation tool, and scripts and statistics for
dataset processing and sampling are available at
https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.

</details>


### [31] [Improving Table Understanding with LLMs and Entity-Oriented Search](https://arxiv.org/abs/2508.17028)
*Thi-Nhung Nguyen,Hoang Ngo,Dinh Phung,Thuy-Trang Vu,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: 本文提出一种面向实体的搜索方法和图查询语言，以提高大型语言模型对表格的理解能力，并在标准基准上取得新最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理表格内容的不可预测性时存在困难，依赖于预处理和关键词匹配，并且由于缺乏上下文信息而面临限制。

Method: 我们引入了一种面向实体的搜索方法，以利用问题和表格数据之间的语义相似性以及表格单元之间的隐式关系，同时使用图查询语言进行表格理解。

Result: 我们的方法有效减少了对数据预处理和关键词匹配的需求，同时增强了上下文清晰度。

Conclusion: 我们的方法在标准基准WikiTableQuestions和TabFact上实现了新的最先进性能。

Abstract: Our work addresses the challenges of understanding tables. Existing methods
often struggle with the unpredictable nature of table content, leading to a
reliance on preprocessing and keyword matching. They also face limitations due
to the lack of contextual information, which complicates the reasoning
processes of large language models (LLMs). To overcome these challenges, we
introduce an entity-oriented search method to improve table understanding with
LLMs. This approach effectively leverages the semantic similarities between
questions and table data, as well as the implicit relationships between table
cells, minimizing the need for data preprocessing and keyword matching.
Additionally, it focuses on table entities, ensuring that table cells are
semantically tightly bound, thereby enhancing contextual clarity. Furthermore,
we pioneer the use of a graph query language for table understanding,
establishing a new research direction. Experiments show that our approach
achieves new state-of-the-art performances on standard benchmarks
WikiTableQuestions and TabFact.

</details>


### [32] [GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection](https://arxiv.org/abs/2508.17057)
*Melissa Kazemi Rad,Alberto Purpura,Himanshu Kumar,Emily Chen,Mohammad Shahed Sorower*

Main category: cs.CL

TL;DR: GRAID是一种利用大型语言模型进行数据集增强的方法，能够提高有害文本分类的性能。


<details>
  <summary>Details</summary>
Motivation: 解决有害文本分类中的数据稀缺问题，以提高防护应用的效果。

Method: GRAID是一种利用大型语言模型进行数据集增强的新管道，包含两个阶段：(i) 使用受限的语言模型生成几何控制的示例，以及(ii) 通过多代理反思过程进行增强，促进风格多样性并发现边缘情况。

Result: 在两个基准数据集中，使用GRAID增强有害文本分类数据集显著提高了下游防护模型的性能。

Conclusion: 使用GRAID对有害文本分类数据集进行增强可以显著提高下游防护模型的性能。

Abstract: We address the problem of data scarcity in harmful text classification for
guardrailing applications and introduce GRAID (Geometric and Reflective
AI-Driven Data Augmentation), a novel pipeline that leverages Large Language
Models (LLMs) for dataset augmentation. GRAID consists of two stages: (i)
generation of geometrically controlled examples using a constrained LLM, and
(ii) augmentation through a multi-agentic reflective process that promotes
stylistic diversity and uncovers edge cases. This combination enables both
reliable coverage of the input space and nuanced exploration of harmful
content. Using two benchmark data sets, we demonstrate that augmenting a
harmful text classification dataset with GRAID leads to significant
improvements in downstream guardrail model performance.

</details>


### [33] [Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages](https://arxiv.org/abs/2508.17078)
*Yuemei Xu,Kexin Xu,Jian Zhou,Ling Hu,Lin Gui*

Main category: cs.CL

TL;DR: 本文提出了BridgeX-ICL，一种简单而有效的方法，用于改进低资源语言的零样本跨语言上下文学习。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在低资源语言上提升性能面临挑战，需要数据高效的无成本微调方法。

Method: 从真实MUSE双语词典构建神经元探测数据，定义了一组语言重叠神经元，并提出基于HSIC的度量来量化LLMs的内部语言谱。

Result: 在2个跨语言任务和15个语言对（来自7种不同的语言家族）上的实验验证了BridgeX-ICL的有效性。

Conclusion: 实验结果验证了BridgeX-ICL的有效性，并提供了关于LLMs多语言机制的实证见解。

Abstract: The current Large Language Models (LLMs) face significant challenges in
improving performance on low-resource languages and urgently need
data-efficient methods without costly fine-tuning. From the perspective of
language-bridge, we propose BridgeX-ICL, a simple yet effective method to
improve zero-shot Cross-lingual In-Context Learning (X-ICL) for low-resource
languages. Unlike existing works focusing on language-specific neurons,
BridgeX-ICL explores whether sharing neurons can improve cross-lingual
performance in LLMs or not. We construct neuron probe data from the
ground-truth MUSE bilingual dictionaries, and define a subset of language
overlap neurons accordingly, to ensure full activation of these anchored
neurons. Subsequently, we propose an HSIC-based metric to quantify LLMs'
internal linguistic spectrum based on overlap neurons, which guides optimal
bridge selection. The experiments conducted on 2 cross-lingual tasks and 15
language pairs from 7 diverse families (covering both high-low and moderate-low
pairs) validate the effectiveness of BridgeX-ICL and offer empirical insights
into the underlying multilingual mechanisms of LLMs.

</details>


### [34] [Token Homogenization under Positional Bias](https://arxiv.org/abs/2508.17126)
*Viacheslav Yusupov,Danil Maksimov,Ameliia Alaeva,Tatiana Zaitceva,Antipina Anna,Anna Vasileva,Chenlin Liu,Rayuth Chheng,Danil Sazanakov,Andrey Chetvergov,Alina Ermilova,Egor Shvetsov*

Main category: cs.CL

TL;DR: 本文研究了transformer模型中token表示的同质化现象，并发现其与位置偏差有关。


<details>
  <summary>Details</summary>
Motivation: 研究token同质化现象及其与位置偏差的关系，以理解大型语言模型中的表示学习机制。

Method: 通过逐层相似性分析和受控实验，研究了token表示在transformer层中的收敛现象及其与位置偏差的关系。

Result: 发现tokens在处理过程中系统性地失去独特性，尤其是在偏向极端位置时。

Conclusion: 本文确认了token同质化现象的存在，并表明其依赖于位置注意力机制。

Abstract: This paper investigates token homogenization - the convergence of token
representations toward uniformity across transformer layers and its
relationship to positional bias in large language models. We empirically
examine whether homogenization occurs and how positional bias amplifies this
effect. Through layer-wise similarity analysis and controlled experiments, we
demonstrate that tokens systematically lose distinctiveness during processing,
particularly when biased toward extremal positions. Our findings confirm both
the existence of homogenization and its dependence on positional attention
mechanisms.

</details>


### [35] [A Straightforward Pipeline for Targeted Entailment and Contradiction Detection](https://arxiv.org/abs/2508.17127)
*Antonin Sulc*

Main category: cs.CL

TL;DR: 本文提出了一种结合Transformer注意力机制和NLI模型的方法，以更有效地识别文本中句子之间的语义关系。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在Transformer注意力机制和自然语言推理（NLI）模型之间存在权衡，因此需要一种结合两者优势的方法来进行针对性分析。

Method: 我们的方法首先通过聚合token级别的注意力分数来识别与用户选择的目标句子上下文相关的候选句子，然后使用预训练的NLI模型对每个候选句子进行分类，以确定其作为前提（蕴含）或矛盾的关系。

Result: 通过使用基于注意力的显著性分数过滤NLI识别的关系，我们的方法能够高效地隔离文本中任何给定主张的最重要语义关系。

Conclusion: 我们的方法能够有效地隔离文本中任何给定主张的最重要语义关系。

Abstract: Finding the relationships between sentences in a document is crucial for
tasks like fact-checking, argument mining, and text summarization. A key
challenge is to identify which sentences act as premises or contradictions for
a specific claim. Existing methods often face a trade-off: transformer
attention mechanisms can identify salient textual connections but lack explicit
semantic labels, while Natural Language Inference (NLI) models can classify
relationships between sentence pairs but operate independently of contextual
saliency. In this work, we introduce a method that combines the strengths of
both approaches for a targeted analysis. Our pipeline first identifies
candidate sentences that are contextually relevant to a user-selected target
sentence by aggregating token-level attention scores. It then uses a pretrained
NLI model to classify each candidate as a premise (entailment) or
contradiction. By filtering NLI-identified relationships with attention-based
saliency scores, our method efficiently isolates the most significant semantic
relationships for any given claim in a text.

</details>


### [36] [The Power of Framing: How News Headlines Guide Search Behavior](https://arxiv.org/abs/2508.17131)
*Amrit Poudel,Maria Milkowski,Tim Weninger*

Main category: cs.CL

TL;DR: 研究显示，标题框架会影响用户的搜索行为，冲突和策略框架会破坏一致性，而情景框架会导致更具体的查询。


<details>
  <summary>Details</summary>
Motivation: 研究标题框架如何影响用户的搜索行为，因为框架效应在判断上的影响已被广泛记录，但其对后续搜索行为的影响尚不明确。

Method: 我们进行了一项受控实验，参与者发出查询并从由特定语言框架过滤的标题中进行选择。

Result: 标题框架显著影响了后续查询：冲突和策略框架破坏了与先前选择的一致性，而情景框架导致比主题框架更具体的查询。还观察到了适度的短期框架持续性，随着时间推移而减弱。

Conclusion: 这些结果表明，即使短暂地接触框架也可能会显著改变用户的信息寻求行为方向。

Abstract: Search engines play a central role in how people gather information, but
subtle cues like headline framing may influence not only what users believe but
also how they search. While framing effects on judgment are well documented,
their impact on subsequent search behavior is less understood. We conducted a
controlled experiment where participants issued queries and selected from
headlines filtered by specific linguistic frames. Headline framing
significantly shaped follow-up queries: conflict and strategy frames disrupted
alignment with prior selections, while episodic frames led to more concrete
queries than thematic ones. We also observed modest short-term frame
persistence that declined over time. These results suggest that even brief
exposure to framing can meaningfully alter the direction of users
information-seeking behavior.

</details>


### [37] [Geolocation-Aware Robust Spoken Language Identification](https://arxiv.org/abs/2508.17148)
*Qingzheng Wang,Hye-jin Shim,Jiancheng Sun,Shinji Watanabe*

Main category: cs.CL

TL;DR: 本文提出了一种地理定位感知的LID方法，通过引入地理定位预测作为辅助任务，提高了模型对方言和口音的统一表示能力，并在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的模型往往难以一致地将同一语言的方言和口音分类为一个统一的类别。

Method: 我们提出了地理定位感知的LID，这是一种将语言级地理定位信息纳入基于SSL的LID模型的新方法。具体来说，我们引入了地理定位预测作为辅助任务，并将预测向量注入中间表示作为条件信号。

Result: 在六个多语言数据集上的实验表明，我们的方法提高了对语言内变化和未见领域的鲁棒性。

Conclusion: 我们的方法在FLEURS上实现了新的最先进的准确率（97.7%），并在ML-SUPERB 2.0方言集上实现了9.7%的相对改进。

Abstract: While Self-supervised Learning (SSL) has significantly improved Spoken
Language Identification (LID), existing models often struggle to consistently
classify dialects and accents of the same language as a unified class. To
address this challenge, we propose geolocation-aware LID, a novel approach that
incorporates language-level geolocation information into the SSL-based LID
model. Specifically, we introduce geolocation prediction as an auxiliary task
and inject the predicted vectors into intermediate representations as
conditioning signals. This explicit conditioning encourages the model to learn
more unified representations for dialectal and accented variations. Experiments
across six multilingual datasets demonstrate that our approach improves
robustness to intra-language variations and unseen domains, achieving new
state-of-the-art accuracy on FLEURS (97.7%) and 9.7% relative improvement on
ML-SUPERB 2.0 dialect set.

</details>


### [38] [Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models](https://arxiv.org/abs/2508.17153)
*Tharindu Madusanka,Ian Pratt-Hartmann,Riza Batista-Navarro*

Main category: cs.CL

TL;DR: 本文研究了不同计算复杂度类和不同语法结构的问题实例对TLMs学习推理规则能力的影响，并进行了实证研究以探索可满足性问题的分布。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究已经探讨了自然语言可满足性问题，但尚未充分讨论不同计算复杂度类和不同语法结构对TLMs学习推理规则能力的影响。

Method: 本文通过实证研究探讨了不同计算复杂度类和不同语法结构的问题实例对TLMs学习推理规则能力的影响。

Result: 本文发现不同计算复杂度类和不同语法结构的问题实例会影响TLMs学习推理规则的能力，并且发现了可满足性问题的分布情况。

Conclusion: 本文研究了不同计算复杂度类和具有不同语法结构的问题实例对TLMs学习推理规则能力的影响，并进行了实证研究以探索可满足性问题的分布。

Abstract: Efforts to apply transformer-based language models (TLMs) to the problem of
reasoning in natural language have enjoyed ever-increasing success in recent
years. The most fundamental task in this area to which nearly all others can be
reduced is that of determining satisfiability. However, from a logical point of
view, satisfiability problems vary along various dimensions, which may affect
TLMs' ability to learn how to solve them. The problem instances of
satisfiability in natural language can belong to different computational
complexity classes depending on the language fragment in which they are
expressed. Although prior research has explored the problem of natural language
satisfiability, the above-mentioned point has not been discussed adequately.
Hence, we investigate how problem instances from varying computational
complexity classes and having different grammatical constructs impact TLMs'
ability to learn rules of inference. Furthermore, to faithfully evaluate TLMs,
we conduct an empirical study to explore the distribution of satisfiability
problems.

</details>


### [39] [SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization](https://arxiv.org/abs/2508.17157)
*Sebastian Martinez,Naman Ahuja,Fenil Bardoliya,Chris Bryan,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文介绍了一个名为SPORTSQL的系统，用于自然语言查询和可视化动态体育数据，特别是针对英格兰足球超级联赛。系统利用大型语言模型的能力进行查询解析和可视化选择，并通过一个新的基准测试DSQABENCH进行了评估。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供一个非专家用户可以轻松探索不断变化的体育统计数据的自然、对话式界面。

Method: 本文介绍了SPORTSQL系统，它利用大型语言模型（LLMs）的符号推理能力进行查询解析、模式链接和可视化选择。同时，为了评估系统性能，作者引入了Dynamic Sport Question Answering benchmark (DSQABENCH)。

Result: 本文展示了SPORTSQL系统如何通过自然语言查询和可视化来处理动态体育数据，并通过DSQABENCH基准测试验证了系统的有效性。

Conclusion: 本文提出了一个模块化、交互式的系统SPORTSQL，用于自然语言查询和可视化动态体育数据，特别是在英格兰足球超级联赛（EPL）方面。该系统通过将用户问题转换为可执行的SQL语句，实现了对实时、时间索引数据库的访问，并支持表格和可视化输出。

Abstract: We present a modular, interactive system, SPORTSQL, for natural language
querying and visualization of dynamic sports data, with a focus on the English
Premier League (EPL). The system translates user questions into executable SQL
over a live, temporally indexed database constructed from real-time Fantasy
Premier League (FPL) data. It supports both tabular and visual outputs,
leveraging the symbolic reasoning capabilities of Large Language Models (LLMs)
for query parsing, schema linking, and visualization selection. To evaluate
system performance, we introduce the Dynamic Sport Question Answering benchmark
(DSQABENCH), comprising 1,700+ queries annotated with SQL programs, gold
answers, and database snapshots. Our demo highlights how non-expert users can
seamlessly explore evolving sports statistics through a natural, conversational
interface.

</details>


### [40] [Quantifying Language Disparities in Multilingual Large Language Models](https://arxiv.org/abs/2508.17162)
*Songbo Hu,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文提出了一种框架，用于更准确地评估多语言模型的性能和语言间的差异，特别关注低资源语言的评估问题。


<details>
  <summary>Details</summary>
Motivation: 大型多语言评估的结果往往碎片化，并受到目标语言、实验设置差异和模型选择等因素的混淆。现有的方法在评估低资源语言时面临挑战。

Method: 我们提出了一种框架，可以分离这些混杂变量，并引入了三个可解释的指标——性能实现比率、其变异系数和语言潜力，以更细致和有洞察力地量化模型和语言之间的实际性能差异。

Result: 通过13种模型变体在11个多语言数据集上的案例研究，我们证明了我们的框架能够提供更可靠的模型性能和语言差异的测量，特别是在低资源语言方面。

Conclusion: 我们的框架提供了更可靠的模型性能和语言差异的测量，特别是对于资源较少的语言。此外，更高的整体模型性能并不一定意味着跨语言的更大公平性。

Abstract: Results reported in large-scale multilingual evaluations are often fragmented
and confounded by factors such as target languages, differences in experimental
setups, and model choices. We propose a framework that disentangles these
confounding variables and introduces three interpretable metrics--the
performance realisation ratio, its coefficient of variation, and language
potential--enabling a finer-grained and more insightful quantification of
actual performance disparities across both (i) models and (ii) languages.
Through a case study of 13 model variants on 11 multilingual datasets, we
demonstrate that our framework provides a more reliable measurement of model
performance and language disparities, particularly for low-resource languages,
which have so far proven challenging to evaluate. Importantly, our results
reveal that higher overall model performance does not necessarily imply greater
fairness across languages.

</details>


### [41] [The Impact of Annotator Personas on LLM Behavior Across the Perspectivism Spectrum](https://arxiv.org/abs/2508.17164)
*Olufunke O. Sarumi,Charles Welch,Daniel Braun,Jörg Schlötterer*

Main category: cs.CL

TL;DR: This paper explores how Large Language Models (LLMs) can annotate hate speech and abusiveness by considering predefined annotator personas. The results show that LLMs selectively use demographic attributes and perform well in weak data perspectivism scenarios, but do not surpass human annotators in strongly perspectivistic datasets.


<details>
  <summary>Details</summary>
Motivation: To explore the capability of Large Language Models (LLMs) to annotate hate speech and abusiveness while considering predefined annotator personas within the strong-to-weak data perspectivism spectra.

Method: We evaluated LLM-generated annotations against existing annotator modeling techniques for perspective modeling.

Result: LLMs selectively use demographic attributes from the personas. Prototypical annotators were identified, with persona features showing varying degrees of alignment with original human annotators. Annotator modeling techniques without explicit reliance on annotator information performed better under weak data perspectivism compared to both strong data perspectivism and human annotations.

Conclusion: LLM annotator modeling approaches human performance in strongly perspectivistic datasets but does not surpass it.

Abstract: In this work, we explore the capability of Large Language Models (LLMs) to
annotate hate speech and abusiveness while considering predefined annotator
personas within the strong-to-weak data perspectivism spectra. We evaluated
LLM-generated annotations against existing annotator modeling techniques for
perspective modeling. Our findings show that LLMs selectively use demographic
attributes from the personas. We identified prototypical annotators, with
persona features that show varying degrees of alignment with the original human
annotators. Within the data perspectivism paradigm, annotator modeling
techniques that do not explicitly rely on annotator information performed
better under weak data perspectivism compared to both strong data perspectivism
and human annotations, suggesting LLM-generated views tend towards aggregation
despite subjective prompting. However, for more personalized datasets tailored
to strong perspectivism, the performance of LLM annotator modeling approached,
but did not exceed, human annotators.

</details>


### [42] [Towards Alignment-Centric Paradigm: A Survey of Instruction Tuning in Large Language Models](https://arxiv.org/abs/2508.17184)
*Xudong Han,Junjie Yang,Tianyang Wang,Ziqian Bi,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: 本文对指令调优的完整流程进行了全面概述，包括数据收集方法、全参数和参数高效的微调策略以及评估协议。文章分类了数据构建的三种主要范式，并探讨了微调技术的范围，从传统的监督训练到轻量级方法如低秩适应（LoRA）和前缀调优。此外，还研究了在多语言和多模态场景中评估真实性、效用和安全性的挑战，并强调了医疗、法律和金融应用中的领域特定基准的出现。最后，文章讨论了自动化数据生成、自适应优化和稳健评估框架的前景，认为数据、算法和人类反馈的更紧密集成对于推进指令调优的LLMs至关重要。


<details>
  <summary>Details</summary>
Motivation: 指令调优是使大型语言模型（LLMs）与人类意图、安全约束和领域特定要求对齐的关键技术。本文旨在提供一个全面的综述，以帮助研究人员和从业者更好地理解和应用这一技术

Method: 本文对指令调优的完整流程进行了全面概述，包括数据收集方法、全参数和参数高效的微调策略以及评估协议

Result: 本文分类了数据构建的三种主要范式：专家注释、从更大模型中蒸馏以及自我改进机制，并探讨了微调技术的范围，从传统的监督训练到轻量级方法如低秩适应（LoRA）和前缀调优。此外，还研究了在多语言和多模态场景中评估真实性、效用和安全性的挑战，并强调了医疗、法律和金融应用中的领域特定基准的出现

Conclusion: 本文旨在为研究人员和从业者提供一个实用的参考，以设计既有效又可靠地与人类意图对齐的大型语言模型（LLMs）

Abstract: Instruction tuning is a pivotal technique for aligning large language models
(LLMs) with human intentions, safety constraints, and domain-specific
requirements. This survey provides a comprehensive overview of the full
pipeline, encompassing (i) data collection methodologies, (ii) full-parameter
and parameter-efficient fine-tuning strategies, and (iii) evaluation protocols.
We categorized data construction into three major paradigms: expert annotation,
distillation from larger models, and self-improvement mechanisms, each offering
distinct trade-offs between quality, scalability, and resource cost.
Fine-tuning techniques range from conventional supervised training to
lightweight approaches, such as low-rank adaptation (LoRA) and prefix tuning,
with a focus on computational efficiency and model reusability. We further
examine the challenges of evaluating faithfulness, utility, and safety across
multilingual and multimodal scenarios, highlighting the emergence of
domain-specific benchmarks in healthcare, legal, and financial applications.
Finally, we discuss promising directions for automated data generation,
adaptive optimization, and robust evaluation frameworks, arguing that a closer
integration of data, algorithms, and human feedback is essential for advancing
instruction-tuned LLMs. This survey aims to serve as a practical reference for
researchers and practitioners seeking to design LLMs that are both effective
and reliably aligned with human intentions.

</details>


### [43] [Active Domain Knowledge Acquisition with \$100 Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains](https://arxiv.org/abs/2508.17202)
*Yang Wu,Raha Moraffah,Rujing Yao,Jinhong Yu,Zhimin Tao,Xiaozhong Liu*

Main category: cs.CL

TL;DR: 本文提出PU-ADKA框架，通过与领域专家互动，在有限预算下提升LLM在专业领域的表现，并引入CKAD数据集促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法在高度专业化和成本敏感的领域（如药物发现和罕见病研究）中效果有限，因为缺乏专家知识。

Method: PU-ADKA框架通过主动与领域专家互动，在固定预算内选择最合适的专家进行查询，考虑了专家的可用性、知识边界和咨询成本。

Result: PU-ADKA在PubMed数据上的模拟训练和实际专家交互验证中表现出色，证明了其在受限预算下提升LLM性能的有效性。

Conclusion: PU-ADKA框架在受限预算下有效提升了专业领域的LLM性能，并通过真实世界部署验证了其有效性。此外，引入的CKAD数据集为该领域进一步研究提供了支持。

Abstract: Large Language Models (LLMs) have demonstrated an impressive level of general
knowledge. However, they often struggle in highly specialized and
cost-sensitive domains such as drug discovery and rare disease research due to
the lack of expert knowledge. In this paper, we propose a novel framework
(PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively
engaging domain experts within a fixed budget. Unlike traditional fine-tuning
approaches, PU-ADKA selectively identifies and queries the most appropriate
expert from a team, taking into account each expert's availability, knowledge
boundaries, and consultation costs. We train PU-ADKA using simulations on
PubMed data and validate it through both controlled expert interactions and
real-world deployment with a drug development team, demonstrating its
effectiveness in enhancing LLM performance in specialized domains under strict
budget constraints. In addition to outlining our methodological innovations and
experimental results, we introduce a new benchmark dataset, CKAD, for
cost-effective LLM domain knowledge acquisition to foster further research in
this challenging area.

</details>


### [44] [SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.17225)
*Xiaqiang Tang,Yi Wang,Keyu Hu,Rui Xu,Chuang Li,Weigao Sun,Jian Li,Sihong Xie*

Main category: cs.CL

TL;DR: SSFO是一种自监督对齐方法，用于增强RAG系统的忠实度，通过对比模型在有和没有上下文时的输出来构建偏好数据对，并利用DPO进行优化，无需标注成本或额外推理负担。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常需要昂贵的监督和后训练或显著的推理负担，而SSFO旨在克服这些限制，提供一种自监督对齐方法来增强RAG的忠实度。

Method: SSFO通过对比模型在有和没有上下文的情况下生成的输出来构建偏好数据对，并利用直接偏好优化（DPO）来对齐模型的忠实度，从而无需标注成本或额外的推理负担。

Result: SSFO在多个基于上下文的问答数据集上取得了最先进的忠实度表现，并且在跨语言忠实度和通用指令遵循能力方面表现出色。

Conclusion: SSFO在多个基于上下文的问答数据集上显著优于现有方法，达到了最先进的忠实度水平。SSFO表现出强大的泛化能力，提高了跨语言的忠实度并保持了通用的指令遵循能力。

Abstract: Retrieval-Augmented Generation (RAG) systems require Large Language Models
(LLMs) to generate responses that are faithful to the retrieved context.
However, faithfulness hallucination remains a critical challenge, as existing
methods often require costly supervision and post-training or significant
inference burdens. To overcome these limitations, we introduce Self-Supervised
Faithfulness Optimization (SSFO), the first self-supervised alignment approach
for enhancing RAG faithfulness. SSFO constructs preference data pairs by
contrasting the model's outputs generated with and without the context.
Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness
without incurring labeling costs or additional inference burden. We
theoretically and empirically demonstrate that SSFO leverages a benign form of
\emph{likelihood displacement}, transferring probability mass from
parametric-based tokens to context-aligned tokens. Based on this insight, we
propose a modified DPO loss function to encourage likelihood displacement.
Comprehensive evaluations show that SSFO significantly outperforms existing
methods, achieving state-of-the-art faithfulness on multiple context-based
question-answering datasets. Notably, SSFO exhibits strong generalization,
improving cross-lingual faithfulness and preserving general
instruction-following capabilities. We release our code and model at the
anonymous link: https://github.com/chkwy/SSFO

</details>


### [45] [ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation](https://arxiv.org/abs/2508.17234)
*Siying Zhou,Yiquan Wu,Hui Chen,Xavier Hu,Kun Kuang,Adam Jatowt,Ming Hu,Chunyan Zheng,Fei Wu*

Main category: cs.CL

TL;DR: 本文介绍了ClaimGen-CN数据集，设计了评估指标，并发现当前模型在事实精度和表达清晰度方面存在不足，需要进一步发展。


<details>
  <summary>Details</summary>
Motivation: 许多研究集中在提高法律专业人士的效率上，但对帮助非专业人士（如原告）的研究仍处于空白状态。本文探讨了基于给定案件事实的法律主张生成问题。

Method: 构建了ClaimGen-CN数据集，设计了一个针对生成声明的评估指标，包括事实性和清晰度两个关键维度，并进行了全面的零样本评估。

Result: 当前模型在事实精度和表达清晰度方面存在不足，表明需要更专门的发展。

Conclusion: 当前模型在事实精度和表达清晰度方面存在局限性，需要在该领域进行更有针对性的发展。为了鼓励对此重要任务的进一步探索，我们将公开数据集。

Abstract: Legal claims refer to the plaintiff's demands in a case and are essential to
guiding judicial reasoning and case resolution. While many works have focused
on improving the efficiency of legal professionals, the research on helping
non-professionals (e.g., plaintiffs) remains unexplored. This paper explores
the problem of legal claim generation based on the given case's facts. First,
we construct ClaimGen-CN, the first dataset for Chinese legal claim generation
task, from various real-world legal disputes. Additionally, we design an
evaluation metric tailored for assessing the generated claims, which
encompasses two essential dimensions: factuality and clarity. Building on this,
we conduct a comprehensive zero-shot evaluation of state-of-the-art general and
legal-domain large language models. Our findings highlight the limitations of
the current models in factual precision and expressive clarity, pointing to the
need for more targeted development in this domain. To encourage further
exploration of this important task, we will make the dataset publicly
available.

</details>


### [46] [Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language Model based Bundle Generation](https://arxiv.org/abs/2508.17250)
*Kaidong Feng,Zhu Sun,Hui Fang,Jie Yang,Wenyuan Liu,Yew-Soon Ong*

Main category: cs.CL

TL;DR: 本文提出了一种名为RouteDK的框架，用于通过LoRA专家架构路由蒸馏知识，以解决知识冲突问题并提高捆绑生成的性能。实验结果表明，该方法在保持高效计算的同时，实现了与教师LLM相当甚至更好的准确性，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管知识蒸馏为更高效的学生产生模型提供了途径，但初步研究表明，将来自教师LLM的多种蒸馏知识简单地集成到学生LLM中会导致知识冲突，从而影响捆绑生成的性能。因此，需要一种有效的框架来解决这个问题。

Method: 提出了一种通过LoRA专家架构路由蒸馏知识的框架RouteDK。首先，从教师LLM中蒸馏两种互补类型的捆绑生成知识：高层知识（可泛化的规则）和细粒度知识（会话特定的推理）。然后，为每种知识类型训练特定的LoRA专家，以及一个基础LoRA专家。为了有效整合，提出了一个动态融合模块，包含一个输入感知路由器，该路由器通过根据输入动态确定最佳权重来平衡专家贡献，从而有效缓解知识冲突。此外，设计了一个推理时增强模块以减少方差并减轻次优推理。

Result: 在三个公共数据集上的实验表明，RouteDK在保持强大计算效率的同时，实现了与教师LLM相当甚至更好的准确性，并优于最先进的捆绑生成方法。

Conclusion: 实验结果表明，RouteDK在保持强大计算效率的同时，实现了与教师LLM相当甚至更好的准确性，并优于最先进的捆绑生成方法。

Abstract: Large Language Models (LLMs) have shown potential in automatic bundle
generation but suffer from prohibitive computational costs. Although knowledge
distillation offers a pathway to more efficient student models, our preliminary
study reveals that naively integrating diverse types of distilled knowledge
from teacher LLMs into student LLMs leads to knowledge conflict, negatively
impacting the performance of bundle generation. To address this, we propose
RouteDK, a framework for routing distilled knowledge through a mixture of LoRA
expert architecture. Specifically, we first distill knowledge from the teacher
LLM for bundle generation in two complementary types: high-level knowledge
(generalizable rules) and fine-grained knowledge (session-specific reasoning).
We then train knowledge-specific LoRA experts for each type of knowledge
together with a base LoRA expert. For effective integration, we propose a
dynamic fusion module, featuring an input-aware router, where the router
balances expert contributions by dynamically determining optimal weights based
on input, thereby effectively mitigating knowledge conflicts. To further
improve inference reliability, we design an inference-time enhancement module
to reduce variance and mitigate suboptimal reasoning. Experiments on three
public datasets show that our RouteDK achieves accuracy comparable to or even
better than the teacher LLM, while maintaining strong computational efficiency.
In addition, it outperforms state-of-the-art approaches for bundle generation.

</details>


### [47] [Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis](https://arxiv.org/abs/2508.17258)
*Filippos Ventirozos,Peter Appleby,Matthew Shardlow*

Main category: cs.CL

TL;DR: 本文探讨了在标签稀缺的情况下，利用大型语言模型的零样本设置进行方面类别情感分析的可行性。


<details>
  <summary>Details</summary>
Motivation: 监督学习方法在领域数据稀缺和注释成本高的情况下效果不佳，且可能存在注释偏差，导致在新领域中的迁移能力差。

Method: 我们提出了结合多个思维链代理的新技术，利用大型语言模型的逐标记不确定性得分。

Result: 我们实验了Llama和Qwen模型的3B和70B+参数版本，展示了这些方法在实际需求中的应用。

Conclusion: 我们的工作展示了如何在标签稀缺的情况下满足实际需求，并引发了关于如何衡量准确性的讨论。

Abstract: Aspect-category sentiment analysis provides granular insights by identifying
specific themes within product reviews that are associated with particular
opinions. Supervised learning approaches dominate the field. However, data is
scarce and expensive to annotate for new domains. We argue that leveraging
large language models in a zero-shot setting is beneficial where the time and
resources required for dataset annotation are limited. Furthermore, annotation
bias may lead to strong results using supervised methods but transfer poorly to
new domains in contexts that lack annotations and demand reproducibility. In
our work, we propose novel techniques that combine multiple chain-of-thought
agents by leveraging large language models' token-level uncertainty scores. We
experiment with the 3B and 70B+ parameter size variants of Llama and Qwen
models, demonstrating how these approaches can fulfil practical needs and
opening a discussion on how to gauge accuracy in label-scarce conditions.

</details>


### [48] [From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users](https://arxiv.org/abs/2508.17281)
*Sadia Sultana Chowa,Riasad Alvi,Subhey Sadi Rahman,Md Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Mukhtar Hussain,Sami Azam*

Main category: cs.CL

TL;DR: 本文综述了LLM作为自主代理和工具用户的最新进展，分析了其架构设计、认知机制以及性能评估，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着人类水平的人工智能（AI）的发展，自主代理和大型语言模型（LLMs）的应用越来越广泛，因此需要对其最新进展进行系统回顾。

Method: 本文对2023年至2025年间发表在A*和A排名会议以及Q1期刊上的论文进行了系统分析，研究了LLM代理的架构设计原则、应用分类以及外部工具的集成策略。

Result: 本文分析了LLM代理的认知机制，包括推理、规划和记忆，并评估了当前的基准测试和评估协议，同时提供了68个公开数据集的分析以评估LLM代理在各种任务中的表现。

Conclusion: 本文总结了LLM作为自主代理和工具用户的最新进展，并讨论了未来的研究方向。

Abstract: The pursuit of human-level artificial intelligence (AI) has significantly
advanced the development of autonomous agents and Large Language Models (LLMs).
LLMs are now widely utilized as decision-making agents for their ability to
interpret instructions, manage sequential tasks, and adapt through feedback.
This review examines recent developments in employing LLMs as autonomous agents
and tool users and comprises seven research questions. We only used the papers
published between 2023 and 2025 in conferences of the A* and A rank and Q1
journals. A structured analysis of the LLM agents' architectural design
principles, dividing their applications into single-agent and multi-agent
systems, and strategies for integrating external tools is presented. In
addition, the cognitive mechanisms of LLM, including reasoning, planning, and
memory, and the impact of prompting methods and fine-tuning procedures on agent
performance are also investigated. Furthermore, we evaluated current benchmarks
and assessment protocols and have provided an analysis of 68 publicly available
datasets to assess the performance of LLM-based agents in various tasks. In
conducting this review, we have identified critical findings on verifiable
reasoning of LLMs, the capacity for self-improvement, and the personalization
of LLM-based agents. Finally, we have discussed ten future research directions
to overcome these gaps.

</details>


### [49] [Handling Students Dropouts in an LLM-driven Interactive Online Course Using Language Models](https://arxiv.org/abs/2508.17310)
*Yuanchun Wang,Yiyang Fu,Jifan Yu,Daniel Zhang-Li,Zheyuan Zhang,Joy Lim Jia Yin,Yucheng Wang,Peng Zhou,Jing Zhang,Huiqin Liu*

Main category: cs.CL

TL;DR: 本文研究了交互式在线课程中的辍学问题，提出了一个预测框架和个性化邮件召回代理，以提高学生参与度并减少辍学。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨交互式在线课程中的辍学问题，并提出解决方案以减少辍学率。

Method: 我们分析了交互日志以定义辍学并识别相关因素，并提出了一个课程进度自适应的辍学预测框架（CPADP）来预测辍学。此外，我们设计了一个个性化的电子邮件召回代理来重新吸引有风险的学生。

Result: 我们的研究结果揭示了辍学行为与文本交互模式之间的强烈联系。CPADP框架能够以最高95.4%的准确率预测辍学。

Conclusion: 我们的方法在具有不同背景的学生中得到了验证，证明了其可行性和有效性。

Abstract: Interactive online learning environments, represented by Massive AI-empowered
Courses (MAIC), leverage LLM-driven multi-agent systems to transform passive
MOOCs into dynamic, text-based platforms, enhancing interactivity through LLMs.
This paper conducts an empirical study on a specific MAIC course to explore
three research questions about dropouts in these interactive online courses:
(1) What factors might lead to dropouts? (2) Can we predict dropouts? (3) Can
we reduce dropouts? We analyze interaction logs to define dropouts and identify
contributing factors. Our findings reveal strong links between dropout
behaviors and textual interaction patterns. We then propose a
course-progress-adaptive dropout prediction framework (CPADP) to predict
dropouts with at most 95.4% accuracy. Based on this, we design a personalized
email recall agent to re-engage at-risk students. Applied in the deployed MAIC
system with over 3,000 students, the feasibility and effectiveness of our
approach have been validated on students with diverse backgrounds.

</details>


### [50] [CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation](https://arxiv.org/abs/2508.17324)
*Hunzalah Hassan Bhatti,Youssef Ahmed,Md Arid Hasan,Firoj Alam*

Main category: cs.CL

TL;DR: 本文介绍了CultranAI系统，该系统通过数据增强和LoRA微调大型语言模型（LLMs）来提升阿拉伯文化知识表示的性能。在PalmX文化评估共享任务中，我们的系统在盲测集上取得了70.50%的准确率，并在PalmX开发集上达到了84.1%的准确率。


<details>
  <summary>Details</summary>
Motivation: 本文报告了我们参与PalmX文化评估共享任务的情况。我们的目标是通过数据增强和LLMs的微调来提高阿拉伯文化知识表示的性能。

Method: 我们的系统CultranAI专注于数据增强和大型语言模型（LLMs）的LoRA微调，以进行阿拉伯文化知识表示。我们对多个LLMs进行了基准测试，以确定任务的最佳模型，并结合Palm数据集和自定义的22K个文化相关的多项选择题（MCQs）数据集进行实验。

Result: Fanar-1-9B-Instruct模型在任务中表现最佳，我们在包含22K+ MCQs的增强数据集上对其进行了微调。

Conclusion: 我们的系统在盲测集上排名第五，准确率为70.50%，在PalmX开发集上达到了84.1%的准确率。

Abstract: In this paper, we report our participation to the PalmX cultural evaluation
shared task. Our system, CultranAI, focused on data augmentation and LoRA
fine-tuning of large language models (LLMs) for Arabic cultural knowledge
representation. We benchmarked several LLMs to identify the best-performing
model for the task. In addition to utilizing the PalmX dataset, we augmented it
by incorporating the Palm dataset and curated a new dataset of over 22K
culturally grounded multiple-choice questions (MCQs). Our experiments showed
that the Fanar-1-9B-Instruct model achieved the highest performance. We
fine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the
blind test set, our submitted system ranked 5th with an accuracy of 70.50%,
while on the PalmX development set, it achieved an accuracy of 84.1%.

</details>


### [51] [Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering](https://arxiv.org/abs/2508.17330)
*Boyuan Liu,Feng Ji,Jiayan Nan,Han Zhao,Weiling Chen,Shihao Xu,Xing Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为Omne-R1的新方法，通过多阶段训练流程和自动生成QA对来提高多跳问答性能，结果表明该方法在复杂问题上效果显著。


<details>
  <summary>Details</summary>
Motivation: 本文旨在增强在无模式知识图谱上的多跳问答能力，通过整合先进的推理模型来解决现有方法的不足。

Method: 我们采用了一个多阶段的训练工作流程，包括两个强化学习阶段和一个监督微调阶段。为了应对有限的合适知识图谱和QA数据的挑战，我们构建了领域无关的知识图谱并自动生成QA对。

Result: 实验结果表明，在回答多跳问题上取得了显著的改进，特别是在更复杂的3+跳问题上表现突出。

Conclusion: 我们的方法在回答多跳问题上表现出显著的改进，并在更复杂的3+跳问题上显示出显著的性能提升。所提出的训练框架在各种知识领域中展示了强大的泛化能力。

Abstract: This paper introduces Omne-R1, a novel approach designed to enhance multi-hop
question answering capabilities on schema-free knowledge graphs by integrating
advanced reasoning models. Our method employs a multi-stage training workflow,
including two reinforcement learning phases and one supervised fine-tuning
phase. We address the challenge of limited suitable knowledge graphs and QA
data by constructing domain-independent knowledge graphs and auto-generating QA
pairs. Experimental results show significant improvements in answering
multi-hop questions, with notable performance gains on more complex 3+ hop
questions. Our proposed training framework demonstrates strong generalization
abilities across diverse knowledge domains.

</details>


### [52] [DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2508.17337)
*Haojie Zhang*

Main category: cs.CL

TL;DR: DropLoRA是一种改进的LoRA方法，通过动态低秩子空间学习提升性能，无需额外成本。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA方法在静态子空间中操作，存在性能瓶颈。为了克服这一限制，提出DropLoRA来提升性能。

Method: DropLoRA是一种基于剪枝的方法，它在LoRA的两个低秩矩阵之间集成一个剪枝模块，以模拟动态子空间学习。

Result: DropLoRA在LLaMA系列模型的微调中表现出色，特别是在常识推理、数学推理、代码生成和指令遵循等任务中。

Conclusion: DropLoRA通过动态低秩子空间学习显著提升了性能，且没有增加额外的训练或推理成本。实验结果表明，DropLoRA在多个大型语言模型生成任务中均优于LoRA。

Abstract: LoRA-based large model parameter-efficient fine-tuning (PEFT) methods use
low-rank de- composition to approximate updates to model parameters. However,
compared to full- parameter fine-tuning, low-rank updates often lead to a
performance gap in downstream tasks. To address this, we introduce DropLoRA, a
novel pruning-based approach that focuses on pruning the rank dimension. Unlike
conven- tional methods that attempt to overcome the low-rank bottleneck,
DropLoRA innovatively integrates a pruning module between the two low-rank
matrices in LoRA to simulate dy- namic subspace learning. This dynamic low-
rank subspace learning allows DropLoRA to overcome the limitations of
traditional LoRA, which operates within a static subspace. By continuously
adapting the learning subspace, DropLoRA significantly boosts performance
without incurring additional training or infer- ence costs. Our experimental
results demon- strate that DropLoRA consistently outperforms LoRA in
fine-tuning the LLaMA series across a wide range of large language model gener-
ation tasks, including commonsense reason- ing, mathematical reasoning, code
generation, and instruction-following. Our code is avail- able at
https://github.com/TayeeChang/DropLoRA.

</details>


### [53] [Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs](https://arxiv.org/abs/2508.17340)
*Ryoma Kondo,Riona Matsuoka,Takahiro Yoshida,Kazuyuki Yamasawa,Ryohei Hisano*

Main category: cs.CL

TL;DR: 本研究では、日本の行政裁判所の判決から法的知識グラフを構築し、司法判断の構造を明確にし、機械可読性を高める方法を提案した。


<details>
  <summary>Details</summary>
Motivation: 現存する自動的なアプローチでは、関連する法的文脈を特定できず、事実と法規範の関係を正確に追跡できず、司法判断の階層構造を誤って表現してしまうという限界がある。これらの制限は、裁判所が実際には法律をどのように適用しているかを捉える能力を妨げている。

Method: Promptベースの大規模言語モデルを使用して法的推論のコンポーネントを抽出し、法規範への参照を正規化し、事実、規範、法的適用を法的推論のオントロジーを通じてリンクする方法を採用した。

Result: 評価結果では、専門家が注釈をつけたデータを用いて、システムが大規模言語モデルのベースラインや検索増強手法よりも関連する法規範の取得においてより高い精度を達成したことが確認された。

Conclusion: 本文は、日本の行政裁判所の判決から構築された法的知識グラフを用いて、司法判断の構造を明確にし、機械可読性を高めることに成功した。

Abstract: Court judgments reveal how legal rules have been interpreted and applied to
facts, providing a foundation for understanding structured legal reasoning.
However, existing automated approaches for capturing legal reasoning, including
large language models, often fail to identify the relevant legal context, do
not accurately trace how facts relate to legal norms, and may misrepresent the
layered structure of judicial reasoning. These limitations hinder the ability
to capture how courts apply the law to facts in practice. In this paper, we
address these challenges by constructing a legal knowledge graph from 648
Japanese administrative court decisions. Our method extracts components of
legal reasoning using prompt-based large language models, normalizes references
to legal provisions, and links facts, norms, and legal applications through an
ontology of legal inference. The resulting graph captures the full structure of
legal reasoning as it appears in real court decisions, making implicit
reasoning explicit and machine-readable. We evaluate our system using expert
annotated data, and find that it achieves more accurate retrieval of relevant
legal provisions from facts than large language model baselines and
retrieval-augmented methods.

</details>


### [54] [The Arabic Generality Score: Another Dimension of Modeling Arabic Dialectness](https://arxiv.org/abs/2508.17347)
*Sanad Shaban,Nizar Habash*

Main category: cs.CL

TL;DR: 本文提出了一种新的衡量阿拉伯语方言通用性的方法AGS，通过结合词对齐、基于词源的编辑距离和平滑技术来标注平行语料库，并训练回归模型预测上下文中的AGS。该方法在多方言基准测试中优于现有的基线方法。


<details>
  <summary>Details</summary>
Motivation: Arabic dialects form a diverse continuum, yet NLP models often treat them as discrete categories. Recent work addresses this issue by modeling dialectness as a continuous variable, notably through the Arabic Level of Dialectness (ALDi). However, ALDi reduces complex variation to a single dimension.

Method: We introduce a pipeline that combines word alignment, etymology-aware edit distance, and smoothing to annotate a parallel corpus with word-level AGS. A regression model is then trained to predict AGS in context.

Result: Our approach outperforms strong baselines, including state-of-the-art dialect ID systems, on a multi-dialect benchmark.

Conclusion: AGS offers a scalable, linguistically grounded way to model lexical generality, enriching representations of Arabic dialectness.

Abstract: Arabic dialects form a diverse continuum, yet NLP models often treat them as
discrete categories. Recent work addresses this issue by modeling dialectness
as a continuous variable, notably through the Arabic Level of Dialectness
(ALDi). However, ALDi reduces complex variation to a single dimension. We
propose a complementary measure: the Arabic Generality Score (AGS), which
quantifies how widely a word is used across dialects. We introduce a pipeline
that combines word alignment, etymology-aware edit distance, and smoothing to
annotate a parallel corpus with word-level AGS. A regression model is then
trained to predict AGS in context. Our approach outperforms strong baselines,
including state-of-the-art dialect ID systems, on a multi-dialect benchmark.
AGS offers a scalable, linguistically grounded way to model lexical generality,
enriching representations of Arabic dialectness.

</details>


### [55] [UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat](https://arxiv.org/abs/2508.17378)
*Omer Nacar*

Main category: cs.CL

TL;DR: 本文评估了ALLaM-34B模型在多个方面的性能，包括生成、混合语言、现代标准阿拉伯语处理、推理、方言准确性和安全性。结果显示ALLaM-34B表现出色，具备实际部署的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）主要在英语语料库上训练，常常难以捕捉阿拉伯语的语言和文化细微差别。为了弥补这一差距，沙特数据和人工智能局（SDAIA）推出了专注于阿拉伯语的ALLaM系列模型。其中最强大的公开可用模型ALLaM-34B被HUMAIN采用，开发并部署了基于该模型的HUMAIN Chat封闭对话网络服务。本文介绍了对ALLaM-34B的扩展和改进的UI级评估。

Method: 使用跨越现代标准阿拉伯语、五个地区方言、混合语言、事实知识、算术和时间推理、创造性生成和对抗性安全的提示包，收集了115个输出，并用三个前沿的大语言模型裁判（GPT-5、Gemini 2.5 Pro、Claude Sonnet-4）进行评分。计算类别级均值和95%置信区间，分析分数分布，并可视化方言级指标热图。

Result: 更新后的分析显示，在生成和混合语言任务中表现一致优异（均为4.92/5），在现代标准阿拉伯语处理方面表现良好（4.74/5），推理能力较强（4.64/5），方言准确性有所提高（4.21/5）。与安全相关的提示显示出稳定可靠的表现（4.54/5）。

Conclusion: 这些结果将ALLaM-34B定位为一个强大且具有文化根基的阿拉伯语大语言模型，展示了技术实力和实际部署的准备程度。

Abstract: Large language models (LLMs) trained primarily on English corpora often
struggle to capture the linguistic and cultural nuances of Arabic. To address
this gap, the Saudi Data and AI Authority (SDAIA) introduced the $ALLaM$ family
of Arabic-focused models. The most capable of these available to the public,
$ALLaM-34B$, was subsequently adopted by HUMAIN, who developed and deployed
HUMAIN Chat, a closed conversational web service built on this model. This
paper presents an expanded and refined UI-level evaluation of $ALLaM-34B$.
Using a prompt pack spanning modern standard Arabic, five regional dialects,
code-switching, factual knowledge, arithmetic and temporal reasoning, creative
generation, and adversarial safety, we collected 115 outputs (23 prompts times
5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro,
Claude Sonnet-4). We compute category-level means with 95\% confidence
intervals, analyze score distributions, and visualize dialect-wise metric heat
maps. The updated analysis reveals consistently high performance on generation
and code-switching tasks (both averaging 4.92/5), alongside strong results in
MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect
fidelity (4.21/5). Safety-related prompts show stable, reliable performance of
(4.54/5). Taken together, these results position $ALLaM-34B$ as a robust and
culturally grounded Arabic LLM, demonstrating both technical strength and
practical readiness for real-world deployment.

</details>


### [56] [Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents](https://arxiv.org/abs/2508.17393)
*Sameer Komoravolu,Khalil Mrini*

Main category: cs.CL

TL;DR: ATA是一种用于评估LLM代理的新方法，通过多种技术结合和自适应测试生成，提高了评估的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM代理的评估仍然依赖于静态基准和小规模的人类研究，需要一种更有效和全面的评估方法。

Method: ATA是一种元代理，结合了静态代码分析、设计师询问、文献挖掘和基于角色的对抗性测试生成，并通过裁判反馈调整难度。

Result: ATA在旅行规划器和维基百科写作者上比专家标注者发现了更多多样且严重的失败，同时保持了严重性，并在20-30分钟内完成，而十名标注者的轮次需要几天时间。

Conclusion: ATA能够有效地检测LLM代理的缺陷，并提供定量指标和定性错误报告，有助于改进代理性能。

Abstract: LLM agents are increasingly deployed to plan, retrieve, and write with tools,
yet evaluation still leans on static benchmarks and small human studies. We
present the Agent-Testing Agent (ATA), a meta-agent that combines static code
analysis, designer interrogation, literature mining, and persona-driven
adversarial test generation whose difficulty adapts via judge feedback. Each
dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer
subsequent tests toward the agent's weakest capabilities. On a travel planner
and a Wikipedia writer, the ATA surfaces more diverse and severe failures than
expert annotators while matching severity, and finishes in 20--30 minutes
versus ten-annotator rounds that took days. Ablating code analysis and web
search increases variance and miscalibration, underscoring the value of
evidence-grounded test generation. The ATA outputs quantitative metrics and
qualitative bug reports for developers. We release the full methodology and
open-source implementation for reproducible agent testing:
https://github.com/KhalilMrini/Agent-Testing-Agent

</details>


### [57] [DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards](https://arxiv.org/abs/2508.17398)
*Aaryaman Kartha,Ahmed Masry,Mohammed Saidul Islam,Thinh Lang,Shadikur Rahman,Ridwan Mahbub,Mizanur Rahman,Mahir Ahmed,Md Rizwan Parvez,Enamul Hoque,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文介绍了DashboardQA，这是一个专门用于评估视觉-语言GUI代理理解和交互现实世界仪表板能力的基准。研究发现，即使最先进的代理在处理交互式仪表板任务时也面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的数据可视化问答基准主要关注静态图表，而忽略了交互性。这限制了它们评估现代多模态代理的能力。因此，我们需要一个专门针对交互式仪表板的基准来评估这些代理。

Method: 我们引入了DashboardQA，这是第一个专门设计用于评估视觉-语言GUI代理理解和交互现实世界仪表板能力的基准。该基准包括来自Tableau Public的112个交互式仪表板和405个问题-答案对，涵盖五个类别：多选、事实性、假设性、多仪表板和对话式。

Result: 通过评估各种领先的封闭和开源GUI代理，我们的分析揭示了它们的关键限制，特别是在定位仪表板元素、规划交互轨迹和进行推理方面。即使是最先进的代理，如基于Gemini-Pro-2.5的代理，也只能达到38.69%的准确率，而OpenAI CUA代理仅达到22.69%。

Conclusion: 我们的研究结果表明，交互式仪表板推理对于所有评估的VLM来说都是一个具有挑战性的任务。即使表现最好的代理也面临困难，这表明需要进一步的研究和改进。

Abstract: Dashboards are powerful visualization tools for data-driven decision-making,
integrating multiple interactive views that allow users to explore, filter, and
navigate data. Unlike static charts, dashboards support rich interactivity,
which is essential for uncovering insights in real-world analytical workflows.
However, existing question-answering benchmarks for data visualizations largely
overlook this interactivity, focusing instead on static charts. This limitation
severely constrains their ability to evaluate the capabilities of modern
multimodal agents designed for GUI-based reasoning. To address this gap, we
introduce DashboardQA, the first benchmark explicitly designed to assess how
vision-language GUI agents comprehend and interact with real-world dashboards.
The benchmark includes 112 interactive dashboards from Tableau Public and 405
question-answer pairs with interactive dashboards spanning five categories:
multiple-choice, factoid, hypothetical, multi-dashboard, and conversational. By
assessing a variety of leading closed- and open-source GUI agents, our analysis
reveals their key limitations, particularly in grounding dashboard elements,
planning interaction trajectories, and performing reasoning. Our findings
indicate that interactive dashboard reasoning is a challenging task overall for
all the VLMs evaluated. Even the top-performing agents struggle; for instance,
the best agent based on Gemini-Pro-2.5 achieves only 38.69% accuracy, while the
OpenAI CUA agent reaches just 22.69%, demonstrating the benchmark's significant
difficulty. We release DashboardQA at https://github.com/vis-nlp/DashboardQA

</details>


### [58] [DS@GT at CheckThat! 2025: A Simple Retrieval-First, LLM-Backed Framework for Claim Normalization](https://arxiv.org/abs/2508.17402)
*Aleksandar Pramov,Jiangqin Ma,Bina Patel*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级的声明归一化方法，在多数单语赛道中表现优异，但在零样本设置中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 声明归一化是任何自动事实核查系统的重要组成部分。它解析通常嘈杂的声明数据，如社交媒体帖子，将其转换为归一化的声明，然后输入下游真实性分类任务。

Method: 我们提出了一个轻量级的“检索优先，LLM支持”的管道，其中我们动态提示GPT-4o-mini使用上下文示例，或直接从训练数据集中检索最接近的归一化结果。

Result: 在官方测试集上，该系统在大多数单语赛道中排名靠前，在13种语言中获得了7个第一名。

Conclusion: 该系统在大多数单语赛道中排名靠前，但在零样本设置中表现不佳，突显了所提出解决方案的局限性。

Abstract: Claim normalization is an integral part of any automatic fact-check
verification system. It parses the typically noisy claim data, such as social
media posts into normalized claims, which are then fed into downstream veracity
classification tasks. The CheckThat! 2025 Task 2 focuses specifically on claim
normalization and spans 20 languages under monolingual and zero-shot
conditions. Our proposed solution consists of a lightweight
\emph{retrieval-first, LLM-backed} pipeline, in which we either dynamically
prompt a GPT-4o-mini with in-context examples, or retrieve the closest
normalization from the train dataset directly. On the official test set, the
system ranks near the top for most monolingual tracks, achieving first place in
7 out of of the 13 languages. In contrast, the system underperforms in the
zero-shot setting, highlighting the limitation of the proposed solution.

</details>


### [59] [MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models](https://arxiv.org/abs/2508.17444)
*Suramya Jadhav,Abhay Shanbhag,Amogh Thakurdesai,Ridhima Sinare,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本文提出了一种针对马拉地语的高质量并列语料库，并展示了基于BERT的模型在该数据集上的结果。


<details>
  <summary>Details</summary>
Motivation: 由于印地语语言在自然语言处理中的复杂性，如丰富的形态学和句法变化、多样的脚本以及标注数据的有限可用性，因此需要一个高质量的并列语料库来辅助语言理解任务。

Method: 本文提出了L3Cube-MahaParaphrase数据集，这是一个针对马拉地语的高质量并列语料库，并使用标准的基于变压器的BERT模型进行了实验。

Result: 本文展示了基于BERT的模型在L3Cube-MahaParaphrase数据集上的结果。

Conclusion: 本文介绍了L3Cube-MahaParaphrase数据集，并展示了基于BERT的模型在该数据集上的结果。数据集和模型已公开共享。

Abstract: Paraphrases are a vital tool to assist language understanding tasks such as
question answering, style transfer, semantic parsing, and data augmentation
tasks. Indic languages are complex in natural language processing (NLP) due to
their rich morphological and syntactic variations, diverse scripts, and limited
availability of annotated data. In this work, we present the
L3Cube-MahaParaphrase Dataset, a high-quality paraphrase corpus for Marathi, a
low resource Indic language, consisting of 8,000 sentence pairs, each annotated
by human experts as either Paraphrase (P) or Non-paraphrase (NP). We also
present the results of standard transformer-based BERT models on these
datasets. The dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [60] [Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD](https://arxiv.org/abs/2508.17450)
*Bryan Chen Zhengyu Tan,Daniel Wai Kit Chin,Zhengyuan Liu,Nancy F. Chen,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文提出了DuET-PD框架和Holistic DPO方法，以解决大型语言模型在说服对话中对虚假信息的轻信问题，并展示了显著的改进效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在说服对话中难以平衡对虚假信息的轻信和对有效纠正的抵抗，这是可靠部署的关键挑战。

Method: DuET-PD（双重视角的说服对话中的信任评估）框架，用于评估多轮立场变化动态，以及Holistic DPO训练方法，用于平衡正负说服示例。

Result: 即使最先进的模型如GPT-4o在持续误导性说服下，在MMLU-Pro上的准确率也只有27.32%。此外，结果揭示了新开源模型越来越顺从的趋势。通过Holistic DPO方法，Llama-3.1-8B-Instruct在安全语境下的准确率从4.21%提高到了76.54%。

Conclusion: 这些贡献为开发更可靠和适应性强的多轮对话大型语言模型提供了路径。

Abstract: Large Language Models (LLMs) can struggle to balance gullibility to
misinformation and resistance to valid corrections in persuasive dialogues, a
critical challenge for reliable deployment. We introduce DuET-PD (Dual
Evaluation for Trust in Persuasive Dialogues), a framework evaluating
multi-turn stance-change dynamics across dual dimensions: persuasion type
(corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via
SALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves
only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions.
Moreover, results reveal a concerning trend of increasing sycophancy in newer
open-source models. To address this, we introduce Holistic DPO, a training
approach balancing positive and negative persuasion examples. Unlike prompting
or resist-only training, Holistic DPO enhances both robustness to
misinformation and receptiveness to corrections, improving
Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts
from 4.21% to 76.54%. These contributions offer a pathway to developing more
reliable and adaptable LLMs for multi-turn dialogue. Code is available at
https://github.com/Social-AI-Studio/DuET-PD.

</details>


### [61] [Evaluating the Impact of Verbal Multiword Expressions on Machine Translation](https://arxiv.org/abs/2508.17458)
*Linfeng Liu,Saptarshi Ghosh,Tianyu Jiang*

Main category: cs.CL

TL;DR: This study examines the impact of verbal multiword expressions on machine translation quality and proposes an LLM-based approach to improve translation by replacing these expressions with their literal counterparts.


<details>
  <summary>Details</summary>
Motivation: VMWEs present significant challenges for natural language processing due to their complex and often non-compositional nature. Accurately translating these structures remains an open problem despite improvements in machine translation models.

Method: We analyzed the impact of three VMWE categories on machine translation quality using established datasets and sentences from machine translation datasets. We also proposed an LLM-based paraphrasing approach to replace these expressions with their literal counterparts.

Result: Our experimental results consistently show that VMWEs negatively affect translation quality. The LLM-based paraphrasing approach demonstrated significant improvement in translation quality for verbal idioms and verb-particle constructions.

Conclusion: VMWEs negatively affect translation quality, but an LLM-based paraphrasing approach can improve translation quality for verbal idioms and verb-particle constructions.

Abstract: Verbal multiword expressions (VMWEs) present significant challenges for
natural language processing due to their complex and often non-compositional
nature. While machine translation models have seen significant improvement with
the advent of language models in recent years, accurately translating these
complex linguistic structures remains an open problem. In this study, we
analyze the impact of three VMWE categories -- verbal idioms, verb-particle
constructions, and light verb constructions -- on machine translation quality
from English to multiple languages. Using both established multiword expression
datasets and sentences containing these language phenomena extracted from
machine translation datasets, we evaluate how state-of-the-art translation
systems handle these expressions. Our experimental results consistently show
that VMWEs negatively affect translation quality. We also propose an LLM-based
paraphrasing approach that replaces these expressions with their literal
counterparts, demonstrating significant improvement in translation quality for
verbal idioms and verb-particle constructions.

</details>


### [62] [Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking](https://arxiv.org/abs/2508.17490)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Mukta Takalikar,Raviraj Joshi*

Main category: cs.CL

TL;DR: 本文提出了一种高效的零样本长文档分类方法，通过句子排名来减少输入上下文，从而提高计算效率。


<details>
  <summary>Details</summary>
Motivation: Transformer-based模型如BERT在短文本分类中表现良好，但在长文档分类（LDC）中由于输入长度限制和计算效率低下而表现不佳。

Method: 我们提出了一种高效的零样本方法，利用句子排名来减少输入上下文而不改变模型架构。我们使用基于TF-IDF的排名策略选择最相关信息的句子。

Result: 保留仅前50%排名的句子可以保持与全文档推理相当的性能，同时将推理时间减少多达35%。

Conclusion: 我们的结果表明，句子排名是一种简单但有效的技术，可用于可扩展和高效的零样本长文档分类。

Abstract: Transformer-based models like BERT excel at short text classification but
struggle with long document classification (LDC) due to input length
limitations and computational inefficiencies. In this work, we propose an
efficient, zero-shot approach to LDC that leverages sentence ranking to reduce
input context without altering the model architecture. Our method enables the
adaptation of models trained on short texts, such as headlines, to long-form
documents by selecting the most informative sentences using a TF-IDF-based
ranking strategy. Using the MahaNews dataset of long Marathi news articles, we
evaluate three context reduction strategies that prioritize essential content
while preserving classification accuracy. Our results show that retaining only
the top 50\% ranked sentences maintains performance comparable to full-document
inference while reducing inference time by up to 35\%. This demonstrates that
sentence ranking is a simple yet effective technique for scalable and efficient
zero-shot LDC.

</details>


### [63] [Improving French Synthetic Speech Quality via SSML Prosody Control](https://arxiv.org/abs/2508.17494)
*Nassima Ould Ouali,Awais Hussain Sani,Ruben Bueno,Jonah Dauvet,Tim Luka Horstmann,Eric Moulines*

Main category: cs.CL

TL;DR: 本文介绍了一种新的端到端方法，通过在法语文本中插入SSML标签来控制合成语音的韵律，从而提高合成语音的表现力。


<details>
  <summary>Details</summary>
Motivation: 尽管最近有所进步，但由于商业文本到语音（TTS）系统中的韵律控制有限，合成语音通常缺乏表现力。

Method: 我们引入了第一个端到端的管道，将语音合成标记语言（SSML）标签插入法语文本以控制音调、语速、音量和暂停时间。我们采用了一个级联架构，使用两个QLoRA微调的Qwen 2.5-7B模型：一个预测短语断点位置，另一个对韵律目标进行回归，生成商业TTS兼容的SSML标记。

Result: 在14小时的法语播客语料库上评估，我们的方法在断点放置上的F1得分为99.2%，与仅提示大型语言模型（LLM）和BiLSTM基线相比，音调、速率和音量的平均绝对误差减少了25-40%。在涉及18名参与者超过9小时合成音频的感知评估中，我们的管道生成的SSML增强语音显著提高了自然度，平均意见分数从3.20提高到3.87（p < 0.005）。此外，15名听众更喜欢我们的增强合成。

Conclusion: 这些结果表明，在弥合合成语音和自然语音之间的表现力差距方面取得了重大进展。

Abstract: Despite recent advances, synthetic voices often lack expressiveness due to
limited prosody control in commercial text-to-speech (TTS) systems. We
introduce the first end-to-end pipeline that inserts Speech Synthesis Markup
Language (SSML) tags into French text to control pitch, speaking rate, volume,
and pause duration. We employ a cascaded architecture with two QLoRA-fine-tuned
Qwen 2.5-7B models: one predicts phrase-break positions and the other performs
regression on prosodic targets, generating commercial TTS-compatible SSML
markup. Evaluated on a 14-hour French podcast corpus, our method achieves 99.2%
F1 for break placement and reduces mean absolute error on pitch, rate, and
volume by 25-40% compared with prompting-only large language models (LLMs) and
a BiLSTM baseline. In perceptual evaluation involving 18 participants across
over 9 hours of synthesized audio, SSML-enhanced speech generated by our
pipeline significantly improves naturalness, with the mean opinion score
increasing from 3.20 to 3.87 (p < 0.005). Additionally, 15 of 18 listeners
preferred our enhanced synthesis. These results demonstrate substantial
progress in bridging the expressiveness gap between synthetic and natural
French speech. Our code is publicly available at
https://github.com/hi-paris/Prosody-Control-French-TTS.

</details>


### [64] [Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?](https://arxiv.org/abs/2508.17536)
*Hyeong Kyu Choi,Xiaojin Zhu,Yixuan Li*

Main category: cs.CL

TL;DR: 本文分析了Multi-Agent Debate (MAD) 的有效性，发现多数投票是性能提升的主要原因，而辩论本身并不提高预期正确性。通过理论框架和实验，我们展示了如何通过针对性干预来提高辩论效果。


<details>
  <summary>Details</summary>
Motivation: 尽管最近取得了进展，但驱动MAD有效性的关键因素仍然不清楚。我们希望通过分解MAD的组成部分来理解其有效性，并探索如何改进辩论的有效性。

Method: 我们将MAD分解为两个关键组件——多数投票和代理辩论，并通过广泛的实验评估了它们的贡献。我们提出了一个理论框架，将辩论建模为随机过程，并证明它在代理的信念轨迹上诱导了一个鞅，这意味着辩论本身不会提高预期正确性。

Result: 我们发现，仅多数投票就解释了通常归因于MAD的大部分性能提升。通过有针对性的干预，可以显著提高辩论的有效性。

Conclusion: 我们的研究结果表明，尽管MAD有潜力，但在许多实际情况下，简单的集成方法仍然是强大且更可靠的替代方案。

Abstract: Multi-Agent Debate~(MAD) has emerged as a promising paradigm for improving
the performance of large language models through collaborative reasoning.
Despite recent advances, the key factors driving MAD's effectiveness remain
unclear. In this work, we disentangle MAD into two key components--Majority
Voting and inter-agent Debate--and assess their respective contributions.
Through extensive experiments across seven NLP benchmarks, we find that
Majority Voting alone accounts for most of the performance gains typically
attributed to MAD. To explain this, we propose a theoretical framework that
models debate as a stochastic process. We prove that it induces a martingale
over agents' belief trajectories, implying that debate alone does not improve
expected correctness. Guided by these insights, we demonstrate that targeted
interventions, by biasing the belief update toward correction, can meaningfully
enhance debate effectiveness. Overall, our findings suggest that while MAD has
potential, simple ensembling methods remain strong and more reliable
alternatives in many practical settings. Code is released in
https://github.com/deeplearning-wisc/debate-or-vote.

</details>


### [65] [Humanizing Machines: Rethinking LLM Anthropomorphism Through a Multi-Level Framework of Design](https://arxiv.org/abs/2508.17573)
*Yunze Xiao,Lynnette Hui Xian Ng,Jiarui Liu,Mona T. Diab*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型中的拟人化特性，并提出将其视为设计概念，通过四个维度的线索分析，提供统一的分类法和可操作的杠杆。


<details>
  <summary>Details</summary>
Motivation: 当前关于拟人化的研究主要关注风险，强调过度信任和用户欺骗，而缺乏设计指导。本文旨在提供一种新的视角，将拟人化视为设计概念。

Method: 本文从多个学科出发，提出了LLM-based artifact的拟人化应反映设计师和解释者之间的交互，并通过分析每个线索的表现和有效性，提供了统一的分类法和可操作的杠杆。

Result: 本文提出了拟人化的四个维度：感知、语言、行为和认知，并通过分析这些线索的表现和有效性，提供了统一的分类法和可操作的杠杆。

Conclusion: 本文认为，应该将拟人化视为一种设计概念，可以有意地调整以支持用户目标，并倡导以功能为导向的拟人化设计评估。

Abstract: Large Language Models (LLMs) increasingly exhibit \textbf{anthropomorphism}
characteristics -- human-like qualities portrayed across their outlook,
language, behavior, and reasoning functions. Such characteristics enable more
intuitive and engaging human-AI interactions. However, current research on
anthropomorphism remains predominantly risk-focused, emphasizing over-trust and
user deception while offering limited design guidance. We argue that
anthropomorphism should instead be treated as a \emph{concept of design} that
can be intentionally tuned to support user goals. Drawing from multiple
disciplines, we propose that the anthropomorphism of an LLM-based artifact
should reflect the interaction between artifact designers and interpreters.
This interaction is facilitated by cues embedded in the artifact by the
designers and the (cognitive) responses of the interpreters to the cues. Cues
are categorized into four dimensions: \textit{perceptive, linguistic,
behavioral}, and \textit{cognitive}. By analyzing the manifestation and
effectiveness of each cue, we provide a unified taxonomy with actionable levers
for practitioners. Consequently, we advocate for function-oriented evaluations
of anthropomorphic design.

</details>


### [66] [CausalSent: Interpretable Sentiment Classification with RieszNet](https://arxiv.org/abs/2508.17576)
*Daniel Frees,Martin Pollack*

Main category: cs.CL

TL;DR: 本文提出了一种基于RieszNet的双头神经网络架构，用于提高文本分类器的可解释性，并在IMDB电影评论中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提高文本分类器的可解释性，我们复制并扩展了Bansal等人的工作，专注于模型的可解释性。

Method: 开发了一种基于RieszNet的双头神经网络架构，以提高处理效果估计的准确性。

Result: CausalSent在半合成IMDB电影评论中准确预测了处理效果，将效果估计的MAE降低了2-3倍。

Conclusion: CausalSent框架能够准确预测半合成IMDB电影评论中的处理效果，并通过集成验证模型进行观察性案例研究，发现'love'一词的存在导致正面情感概率增加2.9%。

Abstract: Despite the overwhelming performance improvements offered by recent natural
language procesing (NLP) models, the decisions made by these models are largely
a black box. Towards closing this gap, the field of causal NLP combines causal
inference literature with modern NLP models to elucidate causal effects of text
features. We replicate and extend Bansal et al's work on regularizing text
classifiers to adhere to estimated effects, focusing instead on model
interpretability. Specifically, we focus on developing a two-headed
RieszNet-based neural network architecture which achieves better treatment
effect estimation accuracy. Our framework, CausalSent, accurately predicts
treatment effects in semi-synthetic IMDB movie reviews, reducing MAE of effect
estimates by 2-3x compared to Bansal et al's MAE on synthetic Civil Comments
data. With an ensemble of validated models, we perform an observational case
study on the causal effect of the word "love" in IMDB movie reviews, finding
that the presence of the word "love" causes a +2.9% increase in the probability
of a positive sentiment.

</details>


### [67] [UQ: Assessing Language Models on Unsolved Questions](https://arxiv.org/abs/2508.17580)
*Fan Nie,Ken Ziyu Liu,Zihao Wang,Rui Sun,Wei Liu,Weijia Shi,Huaxiu Yao,Linjun Zhang,Andrew Y. Ng,James Zou,Sanmi Koyejo,Yejin Choi,Percy Liang,Niklas Muennighoff*

Main category: cs.CL

TL;DR: This paper introduces UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, designed to evaluate AI models on real-world, open-ended challenges.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks face a difficulty-realism tension: exam-style benchmarks are often artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems.

Method: We curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ-Dataset, UQ-Validators, and UQ-Platform.

Result: The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed.

Conclusion: UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge.

Abstract: Benchmarks shape progress in AI research. A useful benchmark should be both
difficult and realistic: questions should challenge frontier models while also
reflecting real-world usage. Yet, current paradigms face a difficulty-realism
tension: exam-style benchmarks are often made artificially difficult with
limited real-world value, while benchmarks based on real user interaction often
skew toward easy, high-frequency problems. In this work, we explore a radically
different paradigm: assessing models on unsolved questions. Rather than a
static benchmark scored once, we curate unsolved questions and evaluate models
asynchronously over time with validator-assisted screening and community
verification. We introduce UQ, a testbed of 500 challenging, diverse questions
sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi
and history, probing capabilities including reasoning, factuality, and
browsing. UQ is difficult and realistic by construction: unsolved questions are
often hard and naturally arise when humans seek answers, thus solving them
yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset
and its collection pipeline combining rule-based filters, LLM judges, and human
review to ensure question quality (e.g., well-defined and difficult); (2)
UQ-Validators, compound validation strategies that leverage the
generator-validator gap to provide evaluation signals and pre-screen candidate
solutions for human review; and (3) UQ-Platform, an open platform where experts
collectively verify questions and solutions. The top model passes UQ-validation
on only 15% of questions, and preliminary human verification has already
identified correct answers among those that passed. UQ charts a path for
evaluating frontier models on real-world, open-ended challenges, where success
pushes the frontier of human knowledge. We release UQ at
https://uq.stanford.edu.

</details>


### [68] [Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions](https://arxiv.org/abs/2508.17610)
*Nannan Huang,Haytham Fayek,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 本文探讨了训练后剪枝对LLM生成摘要公平性的影响，并提出了一种新的剪枝方法HGLA，该方法在保持或提高公平性方面表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 模型压缩通过训练后剪枝提供了一种减少模型大小和计算需求的方法，而不会显著影响模型性能。然而，剪枝对LLM生成摘要的公平性的影响尚未被探索，特别是在意见摘要中，有偏的输出可能会影响公众观点。

Method: 我们提出了High Gradient Low Activation (HGLA)剪枝，通过识别和移除对于输入处理冗余但对输出生成有影响的参数。

Result: 我们的系统分析表明，剪枝方法对公平性的影响大于校准集。HGLA可以更好地保持或甚至提高公平性，显示出在模型和任务中的潜力。

Conclusion: 我们的实验表明，HGLA可以在保持或甚至提高公平性方面优于现有方法，在传统方法有局限性的模型和任务中展现出前景。人类评估显示，HGLA生成的输出比现有的最先进剪枝方法更公平。

Abstract: Model compression through post-training pruning offers a way to reduce model
size and computational requirements without significantly impacting model
performance. However, the effect of pruning on the fairness of LLM-generated
summaries remains unexplored, particularly for opinion summarisation where
biased outputs could influence public views.In this paper, we present a
comprehensive empirical analysis of opinion summarisation, examining three
state-of-the-art pruning methods and various calibration sets across three
open-source LLMs using four fairness metrics. Our systematic analysis reveals
that pruning methods have a greater impact on fairness than calibration sets.
Building on these insights, we propose High Gradient Low Activation (HGLA)
pruning, which identifies and removes parameters that are redundant for input
processing but influential in output generation. Our experiments demonstrate
that HGLA can better maintain or even improve fairness compared to existing
methods, showing promise across models and tasks where traditional methods have
limitations. Our human evaluation shows HGLA-generated outputs are fairer than
existing state-of-the-art pruning methods. Code is available at:
https://github.com/amberhuang01/HGLA.

</details>


### [69] [Steering When Necessary: Flexible Steering Large Language Models with Backtracking](https://arxiv.org/abs/2508.17621)
*Jinwei Gan,Zifeng Cheng,Zhiwei Jiang,Cong Wang,Yafeng Yin,Xiang Luo,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: 本文提出了一种名为FASB的灵活激活转向框架，通过跟踪生成过程中的内部状态来动态确定干预的必要性和强度，并利用回溯机制纠正偏离的标记，从而更有效地对齐大型语言模型与期望行为。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常对所有生成进行无差别干预或仅依赖问题来决定干预，这限制了对干预强度的准确评估。有效对齐大型语言模型与期望行为仍然是一个重大挑战。

Method: 我们提出了灵活的激活转向框架（FASB），该框架通过跟踪生成过程中的内部状态来动态确定干预的必要性和强度，并考虑问题和生成内容。此外，我们提出了回溯机制以纠正偏离的标记并引导LLM朝着期望的行为发展。

Result: 我们的方法在TruthfulQA数据集和六个选择题数据集上的实验表明，它优于基线。

Conclusion: 我们的方法在TruthfulQA数据集和六个选择题数据集上的实验表明，它优于基线。

Abstract: Large language models (LLMs) have achieved remarkable performance across many
generation tasks. Nevertheless, effectively aligning them with desired
behaviors remains a significant challenge. Activation steering is an effective
and cost-efficient approach that directly modifies the activations of LLMs
during the inference stage, aligning their responses with the desired behaviors
and avoiding the high cost of fine-tuning. Existing methods typically
indiscriminately intervene to all generations or rely solely on the question to
determine intervention, which limits the accurate assessment of the
intervention strength. To this end, we propose the Flexible Activation Steering
with Backtracking (FASB) framework, which dynamically determines both the
necessity and strength of intervention by tracking the internal states of the
LLMs during generation, considering both the question and the generated
content. Since intervening after detecting a deviation from the desired
behavior is often too late, we further propose the backtracking mechanism to
correct the deviated tokens and steer the LLMs toward the desired behavior.
Extensive experiments on the TruthfulQA dataset and six multiple-choice
datasets demonstrate that our method outperforms baselines. Our code will be
released at https://github.com/gjw185/FASB.

</details>


### [70] [EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Spoken Dialogue Systems](https://arxiv.org/abs/2508.17623)
*Jingwen Liu,Kan Jen Cheng,Jiachen Lian,Akshay Anand,Rishi Jain,Faith Qiao,Robin Netzorg,Huang-Cheng Chou,Tingle Li,Guan-Ting Lin,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: 本文介绍了EMO-Reasoning，一个用于评估对话系统情感连贯性的基准，并提出了跨轮次情感推理评分，以克服情感语音数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个全面的系统来评估情感推理，尽管口语对话系统有了最近的进展。

Method: 引入EMO-Reasoning，一个用于评估对话系统中情感连贯性的基准，并提出跨轮次情感推理评分来评估多轮对话中的情感转换。

Result: 通过连续、分类和感知指标评估七个对话系统，结果显示我们的框架能够有效检测情感不一致，为改进当前对话系统提供见解。

Conclusion: 通过发布一个系统评估基准，我们旨在推动情感感知的口语对话建模向更自然和适应性的交互发展。

Abstract: Speech emotions play a crucial role in human-computer interaction, shaping
engagement and context-aware communication. Despite recent advances in spoken
dialogue systems, a holistic system for evaluating emotional reasoning is still
lacking. To address this, we introduce EMO-Reasoning, a benchmark for assessing
emotional coherence in dialogue systems. It leverages a curated dataset
generated via text-to-speech to simulate diverse emotional states, overcoming
the scarcity of emotional speech data. We further propose the Cross-turn
Emotion Reasoning Score to assess the emotion transitions in multi-turn
dialogues. Evaluating seven dialogue systems through continuous, categorical,
and perceptual metrics, we show that our framework effectively detects
emotional inconsistencies, providing insights for improving current dialogue
systems. By releasing a systematic evaluation benchmark, we aim to advance
emotion-aware spoken dialogue modeling toward more natural and adaptive
interactions.

</details>


### [71] [Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit](https://arxiv.org/abs/2508.17627)
*Zihao Wei,Liang Pang,Jiahao Liu,Jingcheng Deng,Shicheng Xu,Zenghao Duan,Jingang Wang,Fei Sun,Xunliang Cai,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出一种方法，通过检测推理完成点（RCP）来减轻大型语言模型的过度思考问题，实验表明该方法有效减少了资源消耗并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: LLM在补偿性推理阶段通常能产生正确答案，而推理收敛阶段常常导致过度思考，增加资源使用甚至无限循环。因此，缓解过度思考的关键是检测补偿性推理阶段的结束。

Method: 通过挖掘更敏感和一致的RCP模式，并基于启发式规则开发了一种轻量级阈值策略。

Result: 实验评估显示，所提出的方法在基准测试（AIME24、AIME25、GPQA-D）中减少了令牌消耗，同时保持或提高了推理准确性。

Conclusion: 实验结果表明，所提出的方法在减少令牌消耗的同时保持或提高了推理准确性。

Abstract: Large language models (LLMs) enhance complex reasoning tasks by scaling the
individual thinking process. However, prior work shows that overthinking can
degrade overall performance. Motivated by observed patterns in thinking length
and content length, we categorize reasoning into three stages: insufficient
exploration stage, compensatory reasoning stage, and reasoning convergence
stage. Typically, LLMs produce correct answers in the compensatory reasoning
stage, whereas reasoning convergence often triggers overthinking, causing
increased resource usage or even infinite loops. Therefore, mitigating
overthinking hinges on detecting the end of the compensatory reasoning stage,
defined as the Reasoning Completion Point (RCP). RCP typically appears at the
end of the first complete reasoning cycle and can be identified by querying the
LLM sentence by sentence or monitoring the probability of an end-of-thinking
token (e.g., \texttt{</think>}), though these methods lack an efficient and
precise balance. To improve this, we mine more sensitive and consistent RCP
patterns and develop a lightweight thresholding strategy based on heuristic
rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D)
demonstrate that the proposed method reduces token consumption while preserving
or enhancing reasoning accuracy.

</details>


### [72] [Weights-Rotated Preference Optimization for Large Language Models](https://arxiv.org/abs/2508.17637)
*Chenxu Yang,Ruipeng Jia,Mingyu Zheng,Naibin Gu,Zheng Lin,Siyuan Chen,Weichong Yin,Hua Wu,Weiping Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为RoPO的新算法，用于解决DPO中的奖励黑客问题，该算法通过KL散度和多粒度正交矩阵来约束模型，有效提升了性能并减少了参数使用。


<details>
  <summary>Details</summary>
Motivation: 解决DPO中的奖励黑客问题，即LLMs过度减少被拒绝完成的概率以获得高奖励，而没有真正满足其预期目标，导致生成内容过于冗长、缺乏多样性以及知识的灾难性遗忘。

Method: 我们提出了一个名为Weights-Rotated Preference Optimization (RoPO)的新算法，该算法通过KL散度隐式约束输出层logits，并通过在多粒度正交矩阵上微调显式约束中间隐藏状态。

Result: RoPO在AlpacaEval 2上实现了最高3.27点的提升，并且在MT-Bench上比最佳基线高出6.2到7.5点，仅使用0.015%的可训练参数。

Conclusion: RoPO在AlpacaEval 2上实现了最高3.27点的提升，并且在MT-Bench上比最佳基线高出6.2到7.5点，仅使用0.015%的可训练参数，证明了其在缓解DPO奖励黑客问题方面的有效性。

Abstract: Despite the efficacy of Direct Preference Optimization (DPO) in aligning
Large Language Models (LLMs), reward hacking remains a pivotal challenge. This
issue emerges when LLMs excessively reduce the probability of rejected
completions to achieve high rewards, without genuinely meeting their intended
goals. As a result, this leads to overly lengthy generation lacking diversity,
as well as catastrophic forgetting of knowledge. We investigate the underlying
reason behind this issue, which is representation redundancy caused by neuron
collapse in the parameter space. Hence, we propose a novel Weights-Rotated
Preference Optimization (RoPO) algorithm, which implicitly constrains the
output layer logits with the KL divergence inherited from DPO and explicitly
constrains the intermediate hidden states by fine-tuning on a multi-granularity
orthogonal matrix. This design prevents the policy model from deviating too far
from the reference model, thereby retaining the knowledge and expressive
capabilities acquired during pre-training and SFT stages. Our RoPO achieves up
to a 3.27-point improvement on AlpacaEval 2, and surpasses the best baseline by
6.2 to 7.5 points on MT-Bench with merely 0.015% of the trainable parameters,
demonstrating its effectiveness in alleviating the reward hacking problem of
DPO.

</details>


### [73] [SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models](https://arxiv.org/abs/2508.17647)
*Tong Bao,Mir Tafseer Nayeem,Davood Rafiei,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本文介绍了SurveyGen数据集和QUAL-SG框架，用于评估大型语言模型在自动调查生成中的表现。结果显示，虽然半自动方法可以取得部分成功，但完全自动的方法仍存在引用质量和批判性分析的问题。


<details>
  <summary>Details</summary>
Motivation: 自动调查生成已成为科学文档处理中的关键任务。尽管大型语言模型（LLMs）在生成调查文本方面显示出潜力，但缺乏标准化的评估数据集严重阻碍了它们与人工编写的调查进行严格评估。

Method: 构建了QUAL-SG框架，该框架通过在文献检索中引入质量意识指标来评估和选择高质量的源论文，以增强标准的检索增强生成（RAG）流程。

Result: 使用这个数据集和框架，我们系统地评估了最先进的LLMs在不同水平的人类参与下（从完全自动生成到人机协作写作）。实验结果和人类评估显示，半自动管道可以实现部分有竞争力的结果，但完全自动的调查生成仍然存在引用质量低和批判性分析有限的问题。

Conclusion: 虽然半自动管道可以实现部分有竞争力的结果，但完全自动的调查生成仍然存在引用质量低和批判性分析有限的问题。

Abstract: Automatic survey generation has emerged as a key task in scientific document
processing. While large language models (LLMs) have shown promise in generating
survey texts, the lack of standardized evaluation datasets critically hampers
rigorous assessment of their performance against human-written surveys. In this
work, we present SurveyGen, a large-scale dataset comprising over 4,200
human-written surveys across diverse scientific domains, along with 242,143
cited references and extensive quality-related metadata for both the surveys
and the cited papers. Leveraging this resource, we build QUAL-SG, a novel
quality-aware framework for survey generation that enhances the standard
Retrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware
indicators into literature retrieval to assess and select higher-quality source
papers. Using this dataset and framework, we systematically evaluate
state-of-the-art LLMs under varying levels of human involvement - from fully
automatic generation to human-guided writing. Experimental results and human
evaluations show that while semi-automatic pipelines can achieve partially
competitive outcomes, fully automatic survey generation still suffers from low
citation quality and limited critical analysis.

</details>


### [74] [CoCoA: Confidence- and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models](https://arxiv.org/abs/2508.17670)
*Anant Khandelwal,Manish Gupta,Puneet Agrawal*

Main category: cs.CL

TL;DR: CoCoA is a novel token-level algorithm for resolving conflicts in large language models, improving faithfulness and performance in various tasks.


<details>
  <summary>Details</summary>
Motivation: Faithful generation in large language models (LLMs) is challenged by knowledge conflicts between parametric memory and external context. Existing contrastive decoding methods tuned specifically to handle conflict often lack adaptability and can degrade performance in low conflict settings.

Method: CoCoA (Confidence- and Context-Aware Adaptive Decoding) is a novel token-level algorithm that resolves conflict by utilizing confidence-aware measures (entropy gap and contextual peakedness) and the generalized divergence between the parametric and contextual distributions.

Result: Extensive experiments across multiple LLMs on diverse Question Answering (QA), Summarization, and Long-Form Question Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance over strong baselines like AdaCAD. It yields significant gains in QA accuracy, up to 9.2 points on average compared to the strong baseline AdaCAD, and improves factuality in summarization and LFQA by up to 2.5 points on average across key benchmarks. Additionally, it demonstrates superior sensitivity to conflict variations.

Conclusion: CoCoA enables more informed, context-aware, and ultimately more faithful token generation.

Abstract: Faithful generation in large language models (LLMs) is challenged by
knowledge conflicts between parametric memory and external context. Existing
contrastive decoding methods tuned specifically to handle conflict often lack
adaptability and can degrade performance in low conflict settings. We introduce
CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level
algorithm for principled conflict resolution and enhanced faithfulness. CoCoA
resolves conflict by utilizing confidence-aware measures (entropy gap and
contextual peakedness) and the generalized divergence between the parametric
and contextual distributions. Crucially, CoCoA maintains strong performance
even in low conflict settings. Extensive experiments across multiple LLMs on
diverse Question Answering (QA), Summarization, and Long-Form Question
Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance
over strong baselines like AdaCAD. It yields significant gains in QA accuracy,
up to 9.2 points on average compared to the strong baseline AdaCAD, and
improves factuality in summarization and LFQA by up to 2.5 points on average
across key benchmarks. Additionally, it demonstrates superior sensitivity to
conflict variations. CoCoA enables more informed, context-aware, and ultimately
more faithful token generation.

</details>


### [75] [Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks](https://arxiv.org/abs/2508.17690)
*Danny Wang,Ruihong Qiu,Guangdong Bai,Zi Huang*

Main category: cs.CL

TL;DR: 本文提出了TextTopoOOD框架和TNT-OOD方法，以评估文本丰富的网络中的OOD检测，并展示了其在不同场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要解决标签偏移或简单的领域划分，忽略了文本和结构的复杂多样性。例如，在社交网络中，用户代表具有文本特征的节点，而边表示友谊状态，OOD可能源于机器人和正常用户之间的语言模式差异。

Method: 引入了TextTopoOOD框架来评估跨多种OOD场景的检测，并提出了TNT-OOD来建模文本和拓扑之间的复杂相互作用。

Result: 在四个OOD场景下的11个数据集上的实验表明了TextTopoOOD评估OOD检测的细微挑战性。

Conclusion: 实验表明，TextTopoOOD在评估文本丰富的网络中的OOD检测时具有细微的挑战性。

Abstract: Out-of-distribution (OOD) detection remains challenging in text-rich
networks, where textual features intertwine with topological structures.
Existing methods primarily address label shifts or rudimentary domain-based
splits, overlooking the intricate textual-structural diversity. For example, in
social networks, where users represent nodes with textual features (name, bio)
while edges indicate friendship status, OOD may stem from the distinct language
patterns between bot and normal users. To address this gap, we introduce the
TextTopoOOD framework for evaluating detection across diverse OOD scenarios:
(1) attribute-level shifts via text augmentations and embedding perturbations;
(2) structural shifts through edge rewiring and semantic connections; (3)
thematically-guided label shifts; and (4) domain-based divisions. Furthermore,
we propose TNT-OOD to model the complex interplay between Text aNd Topology
using: 1) a novel cross-attention module to fuse local structure into
node-level text representations, and 2) a HyperNetwork to generate
node-specific transformation parameters. This aligns topological and semantic
features of ID nodes, enhancing ID/OOD distinction across structural and
textual shifts. Experiments on 11 datasets across four OOD scenarios
demonstrate the nuanced challenge of TextTopoOOD for evaluating OOD detection
in text-rich networks.

</details>


### [76] [EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning](https://arxiv.org/abs/2508.17703)
*Yinda Chen,Yangfan He,Jing Yang,Dapeng Zhang,Zhenlong Yuan,Muhammad Attique Khan,Jamel Baili,Por Lip Yee*

Main category: cs.CL

TL;DR: 本文介绍了EMPOWER框架，用于提升医学提示质量，通过多种技术手段显著改善了事实准确性、领域特异性和临床偏好度。


<details>
  <summary>Details</summary>
Motivation: 当前优化方法未能充分解决特定领域的医学知识和安全要求。

Method: EMPOWER是一种新颖的进化框架，通过专门的表示学习、多维评估和结构保留算法来提高医学提示质量。方法包括：(1) 医疗术语注意力机制，(2) 综合评估架构，评估清晰度、特异性、临床相关性和事实准确性，(3) 保留临床推理完整性的组件级进化算法，以及(4) 确保遵循医学知识的语义验证模块。

Result: 在诊断、治疗和教育任务中的评估显示显著改进：事实错误内容减少了24.7%，领域特异性提高了19.6%，盲测中医生偏好度提高了15.3%。

Conclusion: 该框架解决了开发临床适当提示的关键挑战，促进了大型语言模型在医疗环境中的负责任整合。

Abstract: Prompt engineering significantly influences the reliability and clinical
utility of Large Language Models (LLMs) in medical applications. Current
optimization approaches inadequately address domain-specific medical knowledge
and safety requirements. This paper introduces EMPOWER, a novel evolutionary
framework that enhances medical prompt quality through specialized
representation learning, multi-dimensional evaluation, and structure-preserving
algorithms. Our methodology incorporates: (1) a medical terminology attention
mechanism, (2) a comprehensive assessment architecture evaluating clarity,
specificity, clinical relevance, and factual accuracy, (3) a component-level
evolutionary algorithm preserving clinical reasoning integrity, and (4) a
semantic verification module ensuring adherence to medical knowledge.
Evaluation across diagnostic, therapeutic, and educational tasks demonstrates
significant improvements: 24.7% reduction in factually incorrect content, 19.6%
enhancement in domain specificity, and 15.3% higher clinician preference in
blinded evaluations. The framework addresses critical challenges in developing
clinically appropriate prompts, facilitating more responsible integration of
LLMs into healthcare settings.

</details>


### [77] [Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based Language Models](https://arxiv.org/abs/2508.17734)
*Wataru Ikeda,Kazuki Yano,Ryosuke Takahashi,Jaesung Lee,Keigo Shibata,Jun Suzuki*

Main category: cs.CL

TL;DR: 研究显示，在预训练过程中，将FFN集中在连续中间层的70%中能提升多个下游任务的表现。


<details>
  <summary>Details</summary>
Motivation: 研究FFN在Transformer语言模型预训练中的重要性，并探讨其在不同层位置的重要性是否变化。

Method: 通过保持总参数数量不变，增加某些层的FFN维度并完全移除其他层的FFN，训练从头开始的模型以研究FFN在预训练中的重要性。

Result: 在不同大小和层数的模型上进行综合评估，发现将FFN集中在连续中间层的70%中表现优于标准配置。

Conclusion: 研究表明，将FFN集中在连续中间层的70%中可以提高多个下游任务的表现。

Abstract: This study investigates the layerwise importance of feed-forward networks
(FFNs) in Transformer-based language models during pretraining. We introduce an
experimental approach that, while maintaining the total parameter count,
increases the FFN dimensions in some layers and completely removes the FFNs
from other layers. Furthermore, since our focus is on the importance of FFNs
during pretraining, we train models from scratch to examine whether the
importance of FFNs varies depending on their layer positions, rather than using
publicly available pretrained models as is frequently done. Through
comprehensive evaluations of models with varying sizes (285M, 570M, and 1.2B
parameters) and layer counts (12, 24, and 40 layers), we demonstrate that
concentrating FFNs in 70% of the consecutive middle layers consistently
outperforms standard configurations for multiple downstream tasks.

</details>


### [78] [SMITE: Enhancing Fairness in LLMs through Optimal In-Context Example Selection via Dynamic Validation](https://arxiv.org/abs/2508.17735)
*Garima Chhikara,Kripabandhu Ghosh,Abhijnan Chakraborty*

Main category: cs.CL

TL;DR: 本研究通过动态验证集和SMITE算法提高大型语言模型的预测准确性和公平性，这是首次在上下文学习中应用动态验证的研究。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型在下游任务中的输出公平性对于包容性、平等代表和负责任的人工智能部署至关重要。

Method: 本研究引入了动态验证集的概念，该验证集随着测试集的变化而变化，取代了传统的静态验证方法，并提出了一种迭代算法SMITE来选择最优的上下文示例。

Result: 实验结果表明，本研究提出的技巧在四个不同的LLMs上显著提高了预测准确性和公平性。

Conclusion: 本研究提出的动态验证集和SMITE算法在提高预测准确性和公平性方面表现出显著优势，是首次在LLM的上下文学习中应用动态验证的研究。

Abstract: Large Language Models (LLMs) are widely used for downstream tasks such as
tabular classification, where ensuring fairness in their outputs is critical
for inclusivity, equal representation, and responsible AI deployment. This
study introduces a novel approach to enhancing LLM performance and fairness
through the concept of a dynamic validation set, which evolves alongside the
test set, replacing the traditional static validation approach. We also propose
an iterative algorithm, SMITE, to select optimal in-context examples, with each
example set validated against its corresponding dynamic validation set. The
in-context set with the lowest total error is used as the final demonstration
set. Our experiments across four different LLMs show that our proposed
techniques significantly improve both predictive accuracy and fairness compared
to baseline methods. To our knowledge, this is the first study to apply dynamic
validation in the context of in-context learning for LLMs.

</details>


### [79] [ISACL: Internal State Analyzer for Copyrighted Training Data Leakage](https://arxiv.org/abs/2508.17767)
*Guangwei Zhang,Qisheng Su,Jiateng Liu,Cheng Qian,Yanzhou Pan,Yanjie Fu,Denghui Zhang*

Main category: cs.CL

TL;DR: 本研究提出了一种主动方法，通过检查大型语言模型的内部状态来检测潜在的数据泄露，从而在文本生成之前防止敏感信息的暴露。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅在内容生成后解决这些泄漏问题，可能导致敏感信息的暴露。因此，需要一种主动的方法来检测和防止版权数据的泄露。

Method: 本研究使用了经过筛选的版权材料数据集，训练了一个神经网络分类器来识别风险，允许在生成过程中停止或修改输出以防止披露。该框架与检索增强生成（RAG）系统集成，以确保遵守版权和许可要求。

Result: 结果表明，分析内部状态可以有效降低版权数据泄露的风险，提供了一个可扩展的解决方案，能够顺利融入AI工作流程，确保符合版权法规同时保持高质量的文本生成。

Conclusion: 本研究提出了一种主动方法，通过检查大型语言模型的内部状态来检测潜在的数据泄露，从而在文本生成之前防止敏感信息的暴露。结果表明，分析内部状态可以有效降低版权数据泄露的风险，提供了一个可扩展的解决方案，能够顺利融入AI工作流程，确保符合版权法规同时保持高质量的文本生成。

Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but pose risks of inadvertently exposing copyrighted or proprietary data,
especially when such data is used for training but not intended for
distribution. Traditional methods address these leaks only after content is
generated, which can lead to the exposure of sensitive information. This study
introduces a proactive approach: examining LLMs' internal states before text
generation to detect potential leaks. By using a curated dataset of copyrighted
materials, we trained a neural network classifier to identify risks, allowing
for early intervention by stopping the generation process or altering outputs
to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG)
system, this framework ensures adherence to copyright and licensing
requirements while enhancing data privacy and ethical standards. Our results
show that analyzing internal states effectively mitigates the risk of
copyrighted data leakage, offering a scalable solution that fits smoothly into
AI workflows, ensuring compliance with copyright regulations while maintaining
high-quality text generation. The implementation is available on
GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}

</details>


### [80] [Speculating LLMs' Chinese Training Data Pollution from Their Tokens](https://arxiv.org/abs/2508.17771)
*Qingjie Zhang,Di Wang,Haoting Qian,Liu Yan,Tianwei Zhang,Ke Xu,Qi Li,Minlie Huang,Hewu Li,Han Qiu*

Main category: cs.CL

TL;DR: 该研究旨在定位大型语言模型中的受污染中文标记，并研究其与训练数据的关系。通过定义和分类PoC标记、构建检测方法以及分析PoC标记的出现情况，发现GPT的词汇表中存在大量受污染的中文标记，且推测GPT-4o的训练数据中与“Yui Hatano”相关的网页比例约为0.5%。


<details>
  <summary>Details</summary>
Motivation: 由于GPT等大型语言模型的词汇表中存在大量表示中文短语的标记，这些标记可能包含如色情或在线赌博等内容，因此需要定位这些受污染的中文标记，并研究它们与训练数据之间的关系。

Method: 首先，根据GPT的词汇表对受污染的中文标记（PoC tokens）进行了形式化定义和分类。其次，通过微调一个大型语言模型来检测PoC标记，考虑每个标记的语义及其相关搜索结果。最后，通过PoC标记的出现情况推测训练数据的污染情况，并在GPT和其他23个LLM上进行了实验验证。

Result: 实验表明，受污染的中文标记在各种大型语言模型中广泛存在，其中GPT的词汇表表现最差。此外，通过分析PoC标记的出现情况，验证了推测方法在C4和Pile等著名预训练数据集上的准确性，并推测GPT-4o的训练数据中与“Yui Hatano”相关的网页比例约为0.5%。

Conclusion: 研究发现，GPT的词汇表中存在大量受污染的中文标记，其中超过23%的长中文标记（即包含两个以上汉字的标记）要么是色情内容，要么是在线赌博相关内容。此外，通过分析GPT-4o的训练数据，推测与“Yui Hatano”相关的网页比例约为0.5%。

Abstract: Tokens are basic elements in the datasets for LLM training. It is well-known
that many tokens representing Chinese phrases in the vocabulary of GPT
(4o/4o-mini/o1/o3/4.5/4.1/o4-mini) are indicating contents like pornography or
online gambling. Based on this observation, our goal is to locate Polluted
Chinese (PoC) tokens in LLMs and study the relationship between PoC tokens'
existence and training data. (1) We give a formal definition and taxonomy of
PoC tokens based on the GPT's vocabulary. (2) We build a PoC token detector via
fine-tuning an LLM to label PoC tokens in vocabularies by considering each
token's both semantics and related contents from the search engines. (3) We
study the speculation on the training data pollution via PoC tokens'
appearances (token ID). Experiments on GPT and other 23 LLMs indicate that
tokens widely exist while GPT's vocabulary behaves the worst: more than 23%
long Chinese tokens (i.e., a token with more than two Chinese characters) are
either porn or online gambling. We validate the accuracy of our speculation
method on famous pre-training datasets like C4 and Pile. Then, considering
GPT-4o, we speculate that the ratio of "Yui Hatano" related webpages in
GPT-4o's training data is around 0.5%.

</details>


### [81] [Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation](https://arxiv.org/abs/2508.17796)
*Changsong Liu,Yizhou Peng,Eng Siong Chng*

Main category: cs.CL

TL;DR: 本文提出了一种合成驱动的多发音上下文偏差方法，用于在预训练的Whisper模型上进行零样本上下文自动语音识别（ASR）。通过利用文本到语音（TTS）系统合成包含每个目标罕见词的多样化语音样本，并使用预训练的Whisper模型提取多个预测发音变体，然后在beam-search解码中以浅融合方式为beam假设分配奖励，最终将识别的变体映射回原始罕见词。评估结果表明，该方法在Librispeech数据集上的测试集上显著降低了有偏词错误率，同时保持了无偏词错误率基本不变。


<details>
  <summary>Details</summary>
Motivation: 上下文自动语音识别（ASR）系统允许识别词汇外（OOV）单词，例如命名实体或罕见单词。然而，由于训练数据有限和发音模糊或不一致，这仍然具有挑战性。

Method: 我们提出了一种合成驱动的多发音上下文偏差方法，在预训练的Whisper模型上进行零样本上下文ASR。具体来说，我们利用文本到语音（TTS）系统合成包含每个目标罕见词的多样化语音样本，并使用预训练的Whisper模型提取多个预测发音变体。这些变体标记序列被编译成前缀-trie，在beam-search解码中以浅融合方式为beam假设分配奖励。之后，任何识别的变体都会在最终转录中映射回原始罕见词。

Result: 在Librispeech数据集上的评估结果表明，我们的方法在test-clean和test-other上分别将有偏词错误率（WER）降低了42%和43%，同时保持了无偏WER基本不变。

Conclusion: 我们的方法在Librispeech数据集上的评估结果表明，它在test-clean和test-other上分别将有偏词错误率（WER）降低了42%和43%，同时保持了无偏WER基本不变。

Abstract: Contextual automatic speech recognition (ASR) systems allow for recognizing
out-of-vocabulary (OOV) words, such as named entities or rare words. However,
it remains challenging due to limited training data and ambiguous or
inconsistent pronunciations. In this paper, we propose a synthesis-driven
multi-pronunciation contextual biasing method that performs zero-shot
contextual ASR on a pretrained Whisper model. Specifically, we leverage
text-to-speech (TTS) systems to synthesize diverse speech samples containing
each target rare word, and then use the pretrained Whisper model to extract
multiple predicted pronunciation variants. These variant token sequences are
compiled into a prefix-trie, which assigns rewards to beam hypotheses in a
shallow-fusion manner during beam-search decoding. After which, any recognized
variant is mapped back to the original rare word in the final transcription.
The evaluation results on the Librispeech dataset show that our method reduces
biased word error rate (WER) by 42% on test-clean and 43% on test-other while
maintaining unbiased WER essentially unchanged.

</details>


### [82] [DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models](https://arxiv.org/abs/2508.17803)
*Kaiwen Yan,Xuanqing Shi,Hongcheng Guo,Wenxuan Wang,Zhuosheng Zhang,Chengwei Qin*

Main category: cs.CL

TL;DR: DRQA is a novel method that transfers the benefits of resource competition from batch processing to single-question inference, enabling RLLMs to generate concise answers for simple questions while retaining sufficient reasoning depth for more challenging ones.


<details>
  <summary>Details</summary>
Motivation: RLLMs often suffer from overthinking, leading to excessive token consumption and computational inefficiency. However, when processing multiple questions in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically compressing reasoning steps for easier problems.

Method: DRQA leverages batch-generated preference data and reinforcement learning to train the model to allocate reasoning resources adaptively.

Result: Extensive experiments on a wide range of mathematical and scientific reasoning benchmarks demonstrate that DRQA significantly reduces token usage while maintaining, and in many cases improving, answer accuracy.

Conclusion: DRQA offers a promising direction for more efficient and scalable deployment of RLLMs, and it inspires further exploration into fine-grained control of reasoning behaviors.

Abstract: Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1,
have recently demonstrated remarkable capabilities by performing structured and
multi-step reasoning. However, recent studies reveal that RLLMs often suffer
from overthinking, i.e., producing unnecessarily lengthy reasoning chains even
for simple questions, leading to excessive token consumption and computational
inefficiency. Interestingly, we observe that when processing multiple questions
in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically
compressing reasoning steps for easier problems, due to implicit resource
competition. Inspired by this, we propose Dynamic Reasoning Quota Allocation
(DRQA), a novel method that transfers the benefits of resource competition from
batch processing to single-question inference. Specifically, DRQA leverages
batch-generated preference data and reinforcement learning to train the model
to allocate reasoning resources adaptively. By encouraging the model to
internalize a preference for responses that are both accurate and concise, DRQA
enables it to generate concise answers for simple questions while retaining
sufficient reasoning depth for more challenging ones. Extensive experiments on
a wide range of mathematical and scientific reasoning benchmarks demonstrate
that DRQA significantly reduces token usage while maintaining, and in many
cases improving, answer accuracy. By effectively mitigating the overthinking
problem, DRQA offers a promising direction for more efficient and scalable
deployment of RLLMs, and we hope it inspires further exploration into
fine-grained control of reasoning behaviors.

</details>


### [83] [Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning](https://arxiv.org/abs/2508.17855)
*Haijiang Liu,Qiyuan Li,Chao Gao,Yong Cao,Xiangyu Xu,Xun Wu,Daniel Hershcovich,Jinguang Gu*

Main category: cs.CL

TL;DR: MARK是一个多阶段推理框架，旨在提高大型语言模型在文化价值调查响应模拟任务中的准确性、可控性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型在文化价值调查响应模拟任务中的准确性、可控性和可解释性，引入了MARK框架。

Method: MARK框架受到MBTI心理框架中的类型动力学理论的启发，利用人类人口统计数据进行模拟，包括生活情境压力分析、群体级人格预测和自加权认知模仿。

Result: 在世界价值观调查上的实验表明，MARK比现有基线提高了10%的准确率，并减少了模型预测与人类偏好之间的差异。

Conclusion: MARK框架展示了其在提高零样本个性化和帮助社会科学家解释模型预测方面的潜力。

Abstract: Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value
survey response simulation, designed to enhance the accuracy, steerability, and
interpretability of large language models in this task. The system is inspired
by the type dynamics theory in the MBTI psychological framework for personality
research. It effectively predicts and utilizes human demographic information
for simulation: life-situational stress analysis, group-level personality
prediction, and self-weighted cognitive imitation. Experiments on the World
Values Survey show that MARK outperforms existing baselines by 10% accuracy and
reduces the divergence between model predictions and human preferences. This
highlights the potential of our framework to improve zero-shot personalization
and help social scientists interpret model predictions.

</details>


### [84] [Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs](https://arxiv.org/abs/2508.17863)
*Dingdong Wang,Junan Li,Mingyu Cui,Dongchao Yang,Xueyuan Chen,Helen Meng*

Main category: cs.CL

TL;DR: 本文对基于自监督学习的离散和连续语音特征进行了公平比较，并发现连续特征在多种任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 为了填补这两种范式之间的性能差距，我们进行了这项研究。

Method: 我们进行了基于自监督学习（SSL）的离散和连续特征的公平比较，并在相同的实验设置下评估了它们的性能。

Result: 连续特征在各种任务中通常优于离散标记。每种语音处理方法在学习和处理语音信息的方式上表现出不同的特征和模式。

Conclusion: 我们的研究结果希望为SpeechLLMs中的口语语言理解提供有价值的见解。

Abstract: With the rise of Speech Large Language Models (SpeechLLMs), two dominant
approaches have emerged for speech processing: discrete tokens and continuous
features. Each approach has demonstrated strong capabilities in audio-related
processing tasks. However, the performance gap between these two paradigms has
not been thoroughly explored. To address this gap, we present a fair comparison
of self-supervised learning (SSL)-based discrete and continuous features under
the same experimental settings. We evaluate their performance across six spoken
language understanding-related tasks using both small and large-scale LLMs
(Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including
efficient comparison, SSL layer analysis, LLM layer analysis, and robustness
comparison. Our findings reveal that continuous features generally outperform
discrete tokens in various tasks. Each speech processing method exhibits
distinct characteristics and patterns in how it learns and processes speech
information. We hope our results will provide valuable insights to advance
spoken language understanding in SpeechLLMs.

</details>


### [85] [ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models](https://arxiv.org/abs/2508.17892)
*Manlai Liang,Mandi Liu,Jiangzhou Ji,Huaijun Li,Haobo Yang,Yaohan He,Jinlong Li*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Intermediate Layer Retrieval (ILRe)的新上下文压缩管道，用于解决大型语言模型在长上下文场景中的问题。ILRe方法通过确定一个中间解码器层并利用注意力得分回忆标记，显著降低了预填充复杂度，并在长上下文场景中实现了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文场景中存在有效上下文长度短、计算复杂度高和内存开销大的问题，因此需要一种新的上下文压缩管道来解决这些问题。

Method: ILRe方法通过确定一个中间解码器层，仅流式传输到该层的分块预填充，并通过输入查询和该指定层中的完整键缓存之间的注意力得分来回忆标记。此外，还提出了一种多池化内核分配策略以保持语义的完整性。

Result: ILRe方法将预填充复杂度从O(L^2)降低到O(L)，并且在长上下文场景中实现了与完整上下文相当或更好的性能。它可以在不到半分钟内处理1M tokens的请求，并在RULER-1M基准测试中取得约79.8的分数。

Conclusion: ILRe方法在长上下文场景中表现出色，能够在不进行额外微调或开发的情况下处理1M tokens的请求，并在RULER-1M基准测试中取得了约79.8的分数。

Abstract: Large Language Models (LLMs) have demonstrated success across many
benchmarks. However, they still exhibit limitations in long-context scenarios,
primarily due to their short effective context length, quadratic computational
complexity, and high memory overhead when processing lengthy inputs. To
mitigate these issues, we introduce a novel context compression pipeline,
called Intermediate Layer Retrieval (ILRe), which determines one intermediate
decoder layer offline, encodes context by streaming chunked prefill only up to
that layer, and recalls tokens by the attention scores between the input query
and full key cache in that specified layer. In particular, we propose a
multi-pooling kernels allocating strategy in the token recalling process to
maintain the completeness of semantics. Our approach not only reduces the
prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance
comparable to or better than the full context in the long context scenarios.
Without additional post training or operator development, ILRe can process a
single $1M$ tokens request in less than half a minute (speedup $\approx
180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model
Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

</details>


### [86] [Pandora: Leveraging Code-driven Knowledge Transfer for Unified Structured Knowledge Reasoning](https://arxiv.org/abs/2508.17905)
*Yongrui Chen,Junhao He,Linbo Fu,Shenyu Zhang,Rihui Jin,Xinbang Dai,Jiaqi Li,Dehai Min,Nan Hu,Yuxin Zhang,Guilin Qi,Yi Huang,Tongtong Wu*

Main category: cs.CL

TL;DR: 本文介绍了Pandora，一种新的USKR框架，它通过两个关键创新解决了现有方法的局限性，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的USKR方法依赖于特定任务的策略或定制表示，这限制了它们在跨任务场景中的整体性能。

Method: Pandora框架利用了两个关键创新：基于Python的Pandas API的代码统一知识表示，以及通过自动构建跨任务记忆来增强LLM的统一推理过程。

Result: Pandora在六个广泛使用的基准测试中进行了广泛的实验，展示了其出色的统一推理能力。

Conclusion: 实验结果表明，Pandora在跨任务场景中表现出色，优于现有的统一推理框架，并能与特定任务方法竞争。

Abstract: Unified Structured Knowledge Reasoning (USKR) aims to answer natural language
questions by using structured sources such as tables, databases, and knowledge
graphs in a unified way. Existing USKR methods rely on task-specific strategies
or bespoke representations, which hinder their ability to dismantle barriers
between different SKR tasks, thereby constraining their overall performance in
cross-task scenarios. In this paper, we introduce \textsc{Pandora}, a novel
USKR framework that addresses the limitations of existing methods by leveraging
two key innovations. First, we propose a code-based unified knowledge
representation using \textsc{Python}'s \textsc{Pandas} API, which aligns
seamlessly with the pre-training of LLMs. This representation facilitates a
cohesive approach to handling different structured knowledge sources. Building
on this foundation, we employ knowledge transfer to bolster the unified
reasoning process of LLMs by automatically building cross-task memory. By
adaptively correcting reasoning using feedback from code execution,
\textsc{Pandora} showcases impressive unified reasoning capabilities. Extensive
experiments on six widely used benchmarks across three SKR tasks demonstrate
that \textsc{Pandora} outperforms existing unified reasoning frameworks and
competes effectively with task-specific methods.

</details>


### [87] [Evaluating the Representation of Vowels in Wav2Vec Feature Extractor: A Layer-Wise Analysis Using MFCCs](https://arxiv.org/abs/2508.17914)
*Domenico De Cristofaro,Vincenzo Norman Vitale,Alessandro Vietti*

Main category: cs.CL

TL;DR: 该研究比较了不同的特征提取方法在语音识别中的表现，以确定哪种方法在区分单韵母方面最有效。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同特征提取方法在语音识别中的表现，以确定哪种方法在区分单韵母方面最有效。

Method: 研究使用了TIMIT语料库，比较了MFCC、MFCC加共振峰和CNN激活特征，并通过训练SVM分类器来评估它们的分类准确性。

Result: 研究结果表明，不同特征提取方法在区分单韵母方面的分类准确性有所不同，从而为语音识别提供了有价值的见解。

Conclusion: 该研究评估了不同特征提取方法在语音识别中的表现，以确定哪种方法在区分单韵母方面最有效。

Abstract: Automatic Speech Recognition has advanced with self-supervised learning,
enabling feature extraction directly from raw audio. In Wav2Vec, a CNN first
transforms audio into feature vectors before the transformer processes them.
This study examines CNN-extracted information for monophthong vowels using the
TIMIT corpus. We compare MFCCs, MFCCs with formants, and CNN activations by
training SVM classifiers for front-back vowel identification, assessing their
classification accuracy to evaluate phonetic representation.

</details>


### [88] [Information availability in different languages and various technological constraints related to multilinguism on the Internet](https://arxiv.org/abs/2508.17918)
*Sonal Khosla,Haridasa Acharya*

Main category: cs.CL

TL;DR: 本文讨论了互联网上语言障碍的问题，分析了现有解决方案的不足，并强调了改进多语言支持的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着非英语用户数量的增加，需要解决语言障碍以提高互联网的可访问性。

Method: 本文分析了互联网上语言障碍的问题，并探讨了现有的解决方案和存在的差距。

Result: 本文指出了当前在多语言技术支持方面的不足，并强调了改进的必要性。

Conclusion: 本文旨在分析不同语言的信息可用性以及与互联网多语言相关的各种技术限制。

Abstract: The usage of Internet has grown exponentially over the last two decades. The
number of Internet users has grown from 16 Million to 1650 Million from 1995 to
2010. It has become a major repository of information catering almost every
area. Since the Internet has its origin in USA which is English speaking
country there is huge dominance of English on the World Wide Web. Although
English is a globally acceptable language, still there is a huge population in
the world which is not able to access the Internet due to language constraints.
It has been estimated that only 20-25% of the world population speaks English
as a native language. More and more people are accessing the Internet nowadays
removing the cultural and linguistic barriers and hence there is a high growth
in the number of non-English speaking users over the last few years on the
Internet. Although many solutions have been provided to remove the linguistic
barriers, still there is a huge gap to be filled. This paper attempts to
analyze the need of information availability in different languages and the
various technological constraints related to multi-linguism on the Internet.

</details>


### [89] [Feature-Refined Unsupervised Model for Loanword Detection](https://arxiv.org/abs/2508.17923)
*Promise Dodzi Kpoglu*

Main category: cs.CL

TL;DR: 本文提出了一种无监督方法，仅使用语言内部信息来检测借词，并在多个印欧语言数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 先前的工作主要依赖于语言外部信息来识别借词，但这种方法可能会在历史语言学流程中引入循环性和约束。

Method: 我们提出了一种无监督方法，仅依赖语言内部信息来处理单语和多语词表中的原生词和借词。通过提取相关的语言特征、对其进行评分并进行概率映射，我们迭代地优化初始结果，直到收敛。

Result: 实验结果表明，我们的模型在六个标准印欧语言的数据集上表现优异，特别是在跨语言数据扩展时取得了显著的性能提升。

Conclusion: 我们的模型在检测借词任务中优于基线方法，并且在跨语言数据扩展时表现出显著的性能提升。

Abstract: We propose an unsupervised method for detecting loanwords i.e., words
borrowed from one language into another. While prior work has primarily relied
on language-external information to identify loanwords, such approaches can
introduce circularity and constraints into the historical linguistics workflow.
In contrast, our model relies solely on language-internal information to
process both native and borrowed words in monolingual and multilingual
wordlists. By extracting pertinent linguistic features, scoring them, and
mapping them probabilistically, we iteratively refine initial results by
identifying and generalizing from emerging patterns until convergence. This
hybrid approach leverages both linguistic and statistical cues to guide the
discovery process. We evaluate our method on the task of isolating loanwords in
datasets from six standard Indo-European languages: English, German, French,
Italian, Spanish, and Portuguese. Experimental results demonstrate that our
model outperforms baseline methods, with strong performance gains observed when
scaling to cross-linguistic data.

</details>


### [90] [AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation](https://arxiv.org/abs/2508.17926)
*Henri Savigny,Bruno Yun*

Main category: cs.CL

TL;DR: 本文研究了如何利用一个大型语言模型来执行一个或多个论点挖掘任务，通过构建多任务数据集和探索不同的训练策略，结果表明任务特定微调和模型合并可以有效提高性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 本文动机是研究如何利用一个大型语言模型来执行一个或多个论点挖掘任务，以提高论点挖掘的效率和效果。

Method: 本文方法是构建一个跨19个知名论点挖掘数据集的多任务数据集，并探索使用Meta AI的Llama-3.1-8B-Instruct模型的各种训练策略：(1) 在单个任务上微调，(2) 在多个任务上联合微调，(3) 合并分别在单个任务上微调的模型。

Result: 实验结果表明，任务特定微调显著提高了所有任务的性能。此外，多任务微调在不降级的情况下保持了强大的性能，表明在相关任务之间有效的迁移学习。最后，模型合并提供了一种可行的折中方案：它产生了有竞争力的性能，同时减轻了全多任务微调相关的计算成本。

Conclusion: 本文结论是，任务特定微调显著提高了所有任务的性能，多任务微调在不降级的情况下保持了强大的性能，表明在相关任务之间有效的迁移学习。最后，模型合并提供了一种可行的折中方案：它产生了有竞争力的性能，同时减轻了全多任务微调相关的计算成本。

Abstract: Argument mining is a subfield of argumentation that aims to automatically
extract argumentative structures and their relations from natural language
texts. This paper investigates how a single large language model can be
leveraged to perform one or several argument mining tasks. Our contributions
are two-fold. First, we construct a multi-task dataset by surveying and
converting 19 well-known argument mining datasets from the literature into a
unified format. Second, we explore various training strategies using Meta AI's
Llama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2)
fine-tuning jointly on multiple tasks, and (3) merging models fine-tuned
separately on individual tasks. Our experiments show that task-specific
fine-tuning significantly improves individual performance across all tasks.
Moreover, multi-task fine-tuning maintains strong performance without
degradation, suggesting effective transfer learning across related tasks.
Finally, we demonstrate that model merging offers a viable compromise: it
yields competitive performance while mitigating the computational costs
associated with full multi-task fine-tuning.

</details>


### [91] [Debiasing Multilingual LLMs in Cross-lingual Latent Space](https://arxiv.org/abs/2508.17948)
*Qiwei Peng,Guimin Hu,Yekun Chai,Anders Søgaard*

Main category: cs.CL

TL;DR: 本文提出在联合潜在空间中进行去偏处理，而不是直接对LLM表示进行去偏，实验表明这种方法在跨语言场景下效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有的去偏技术在跨语言迁移方面效果有限，因此需要一种更有效的跨语言去偏方法。

Method: 使用在平行TED演讲脚本上训练的自编码器构建一个对齐良好的跨语言潜在空间，并在该空间中进行去偏处理。

Result: 自编码器有效地构建了一个对齐良好的跨语言潜在空间，并且在该空间中应用去偏技术显著提高了去偏性能和跨语言可迁移性。

Conclusion: 在学习的跨语言潜在空间中应用去偏技术显著提高了整体去偏性能和跨语言可迁移性。

Abstract: Debiasing techniques such as SentDebias aim to reduce bias in large language
models (LLMs). Previous studies have evaluated their cross-lingual
transferability by directly applying these methods to LLM representations,
revealing their limited effectiveness across languages. In this work, we
therefore propose to perform debiasing in a joint latent space rather than
directly on LLM representations. We construct a well-aligned cross-lingual
latent space using an autoencoder trained on parallel TED talk scripts. Our
experiments with Aya-expanse and two debiasing techniques across four languages
(English, French, German, Dutch) demonstrate that a) autoencoders effectively
construct a well-aligned cross-lingual latent space, and b) applying debiasing
techniques in the learned cross-lingual latent space significantly improves
both the overall debiasing performance and cross-lingual transferability.

</details>


### [92] [Understanding Subword Compositionality of Large Language Models](https://arxiv.org/abs/2508.17953)
*Qiwei Peng,Yekun Chai,Anders Søgaard*

Main category: cs.CL

TL;DR: 本文通过实验研究了大型语言模型如何组合子词信息，发现这些模型在结构相似性、语义分解性和形式保留方面表现出不同的模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）将子词序列作为输入，需要有效地将子词表示组合成有意义的词级表示。本文旨在探究LLMs如何组合子词信息。

Method: 我们进行了一组全面的实验，以探测LLMs如何组合子词信息，重点关注三个关键方面：结构相似性、语义可分解性和形式保留。

Result: 我们的实验分析表明，这五个LLM家族可以分为三个不同的组，可能反映了它们底层组合策略的差异。具体来说，我们观察到（i）在不同层中子词组合与整个词表示之间的结构相似性的演变有三种不同的模式；（ii）在逐层探测其对语义分解性的敏感性时表现出色；（iii）在探测对形式特征（如字符序列长度）的敏感性时有三种不同的模式。

Conclusion: 这些发现为LLMs的组合动态提供了有价值的见解，并突显了LLMs如何编码和整合子词信息的不同组合模式。

Abstract: Large language models (LLMs) take sequences of subwords as input, requiring
them to effective compose subword representations into meaningful word-level
representations. In this paper, we present a comprehensive set of experiments
to probe how LLMs compose subword information, focusing on three key aspects:
structural similarity, semantic decomposability, and form retention. Our
analysis of the experiments suggests that these five LLM families can be
classified into three distinct groups, likely reflecting difference in their
underlying composition strategies. Specifically, we observe (i) three distinct
patterns in the evolution of structural similarity between subword compositions
and whole-word representations across layers; (ii) great performance when
probing layer by layer their sensitivity to semantic decompositionality; and
(iii) three distinct patterns when probing sensitivity to formal features,
e.g., character sequence length. These findings provide valuable insights into
the compositional dynamics of LLMs and highlight different compositional
pattens in how LLMs encode and integrate subword information.

</details>


### [93] [German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German](https://arxiv.org/abs/2508.17973)
*Miriam Anschütz,Thanh Mai Pham,Eslam Nasrallah,Maximilian Müller,Cristian-George Craciun,Georg Groh*

Main category: cs.CL

TL;DR: 本文提出了一种用于德语文本简化的大规模数据集和模型，实现了最先进的性能，并开源以促进进一步研究。


<details>
  <summary>Details</summary>
Motivation: 为了创建针对不同读者群体的可访问文本，需要能够跨不同复杂程度进行改写。然而，目前缺乏大规模的德语数据集。

Method: 使用GPT-4自动生成数据集，并通过人工和LLM评估进行验证。然后基于German4All训练了一个可控制可读性的paraphrasing模型。

Result: German4All数据集包含超过25,000个样本，涵盖五个可读性级别。训练的模型在德语文本简化任务中表现优于现有方法。

Conclusion: 本文介绍了German4All数据集和一个可控制可读性的 paraphrasing 模型，该模型在德语文本简化任务中取得了最先进的性能，并且开源以促进进一步的研究。

Abstract: The ability to paraphrase texts across different complexity levels is
essential for creating accessible texts that can be tailored toward diverse
reader groups. Thus, we introduce German4All, the first large-scale German
dataset of aligned readability-controlled, paragraph-level paraphrases. It
spans five readability levels and comprises over 25,000 samples. The dataset is
automatically synthesized using GPT-4 and rigorously evaluated through both
human and LLM-based judgments. Using German4All, we train an open-source,
readability-controlled paraphrasing model that achieves state-of-the-art
performance in German text simplification, enabling more nuanced and
reader-specific adaptations. We opensource both the dataset and the model to
encourage further research on multi-level paraphrasing

</details>


### [94] [A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2508.17994)
*Oleg Silcenco,Marcos R. Machad,Wallace C. Ugulino,Daniel Braun*

Main category: cs.CL

TL;DR: 本研究引入了一个多语言客户评论数据集，并评估了GPT-4和LLaMA-3在基于方面的情感分析中的性能，结果显示两者均达到85%以上的准确率，GPT-4整体表现更优。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过引入一个手动标注的多语言客户评论数据集，为基于方面的情感分析提供一个新的基准，并评估当前最先进的模型如GPT-4和LLaMA-3的表现。

Method: 本研究引入了一个手动标注的多语言客户评论数据集，并使用该数据集评估了GPT-4和LLaMA-3在基于方面的情感分析中的性能。

Result: GPT-4和LLaMA-3在基于方面的情感分析中均达到了85%以上的准确率，其中GPT-4整体表现优于LLaMA-3。

Conclusion: 本研究引入了一个手动标注的多语言客户评论数据集，并评估了GPT-4和LLaMA-3在基于方面的情感分析中的性能，结果表明两者均达到了85%以上的准确率，其中GPT-4整体表现优于LLaMA-3。

Abstract: Aspect-based sentiment analysis enhances sentiment detection by associating
it with specific aspects, offering deeper insights than traditional sentiment
analysis. This study introduces a manually annotated dataset of 10,814
multilingual customer reviews covering brick-and-mortar retail stores, labeled
with eight aspect categories and their sentiment. Using this dataset, the
performance of GPT-4 and LLaMA-3 in aspect based sentiment analysis is
evaluated to establish a baseline for the newly introduced data. The results
show both models achieving over 85% accuracy, while GPT-4 outperforms LLaMA-3
overall with regard to all relevant metrics.

</details>


### [95] [Neither Valid nor Reliable? Investigating the Use of LLMs as Judges](https://arxiv.org/abs/2508.18076)
*Khaoula Chehbouni,Mohammed Haddou,Jackie Chi Kit Cheung,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 本文批评了当前对大型语言模型作为评估者（LLJs）的过度乐观态度，指出其有效性尚未得到充分验证，并呼吁更负责任的评估实践。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨LLJs在自然语言生成（NLG）系统评估中的有效性，并指出当前对LLJs的过度乐观可能带来的问题。

Method: 本文基于社会科学研究中的测量理论，识别并批判性地评估了LLJs使用的四个核心假设：它们作为人类判断的代理能力、作为评估者的能力、可扩展性和成本效益。

Result: 文章分析了LLJs在文本摘要、数据标注和安全对齐三个应用中的适用性，并指出LLJs的局限性可能影响其作为评估工具的可靠性。

Conclusion: 本文认为当前对LLJs的乐观态度可能为时过早，因为它们的采用已经超过了对其作为评估者的可靠性和有效性进行严格审查。文章强调了在LLJs评估中需要更负责任的评估实践，以确保它们在该领域的作用支持而非阻碍NLG的进步。

Abstract: Evaluating natural language generation (NLG) systems remains a core challenge
of natural language processing (NLP), further complicated by the rise of large
language models (LLMs) that aims to be general-purpose. Recently, large
language models as judges (LLJs) have emerged as a promising alternative to
traditional metrics, but their validity remains underexplored. This position
paper argues that the current enthusiasm around LLJs may be premature, as their
adoption has outpaced rigorous scrutiny of their reliability and validity as
evaluators. Drawing on measurement theory from the social sciences, we identify
and critically assess four core assumptions underlying the use of LLJs: their
ability to act as proxies for human judgment, their capabilities as evaluators,
their scalability, and their cost-effectiveness. We examine how each of these
assumptions may be challenged by the inherent limitations of LLMs, LLJs, or
current practices in NLG evaluation. To ground our analysis, we explore three
applications of LLJs: text summarization, data annotation, and safety
alignment. Finally, we highlight the need for more responsible evaluation
practices in LLJs evaluation, to ensure that their growing role in the field
supports, rather than undermines, progress in NLG.

</details>


### [96] [How Quantization Shapes Bias in Large Language Models](https://arxiv.org/abs/2508.18088)
*Federico Marcuzzi,Xuefei Ning,Roy Schwartz,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文全面评估了量化对模型偏差的影响，发现量化可以减少毒性，但可能增加刻板印象和不公平性，强调了在实际应用中平衡效率和伦理的重要性。


<details>
  <summary>Details</summary>
Motivation: 量化在模型压缩和效率提升方面具有重要意义，但其对模型偏差的影响尚未得到充分研究。本文旨在探讨量化对模型偏差的具体影响，特别是在不同人口统计子群体中的表现。

Method: 我们评估了量化对模型偏差的影响，特别关注其对个体人口统计子群体的影响。我们专注于权重和激活量化策略，并在广泛的偏差类型上进行了研究，包括刻板印象、毒性、情感和公平性。我们使用了概率和基于生成文本的指标，在九个基准测试中评估了不同架构家族和推理能力的模型。

Result: 量化对偏差的影响是复杂的：虽然它可以减少模型的毒性，并且不会显著影响情感，但在生成任务中，尤其是在激进压缩下，它往往会略微增加刻板印象和不公平性。这些趋势在人口统计类别和模型类型中总体一致，但其程度取决于具体设置。

Conclusion: 我们的结果突出了在实践中应用量化时仔细平衡效率和伦理考虑的重要性。

Abstract: This work presents a comprehensive evaluation of how quantization affects
model bias, with particular attention to its impact on individual demographic
subgroups. We focus on weight and activation quantization strategies and
examine their effects across a broad range of bias types, including
stereotypes, toxicity, sentiment, and fairness. We employ both probabilistic
and generated text-based metrics across nine benchmarks and evaluate models
varying in architecture family and reasoning ability. Our findings show that
quantization has a nuanced impact on bias: while it can reduce model toxicity
and does not significantly impact sentiment, it tends to slightly increase
stereotypes and unfairness in generative tasks, especially under aggressive
compression. These trends are generally consistent across demographic
categories and model types, although their magnitude depends on the specific
setting. Overall, our results highlight the importance of carefully balancing
efficiency and ethical considerations when applying quantization in practice.

</details>


### [97] [Speech-Based Depressive Mood Detection in the Presence of Multiple Sclerosis: A Cross-Corpus and Cross-Lingual Study](https://arxiv.org/abs/2508.18092)
*Monica Gonzalez-Machorro,Uwe Reichel,Pascal Hecker,Helly Hammer,Hesam Sagha,Florian Eyben,Robert Hoepner,Björn W. Schuller*

Main category: cs.CL

TL;DR: 本研究探讨了基于语音的抑郁检测方法在多发性硬化症患者中的适用性，并取得了中等的检测效果。


<details>
  <summary>Details</summary>
Motivation: 抑郁常与神经退行性疾病如多发性硬化症（MS）共存，但基于语音的人工智能在该情境下检测抑郁的潜力尚未被探索。

Method: 本研究通过跨语料库和跨语言分析，使用来自一般人群的英语数据和多发性硬化症患者（pwMS）的德语数据，评估了基于语音的抑郁检测方法在pwMS中的可迁移性。采用了监督机器学习模型，包括常规语音和语言特征、从语音情感识别（SER）模型中得出的情感维度以及探索性语音特征分析。

Result: 尽管数据有限，我们的模型在pwMS中检测抑郁情绪具有中等泛化能力，在二分类任务中达到了66%的未加权平均召回率（UAR）。特征选择进一步提高了性能，将UAR提升至74%。此外，研究还突显了情感变化在一般人群和pwMS中作为抑郁情绪指标的相关作用。

Conclusion: 本研究提供了对基于语音的抑郁检测方法在共病情况下（如多发性硬化症）进行泛化的初步探索。

Abstract: Depression commonly co-occurs with neurodegenerative disorders like Multiple
Sclerosis (MS), yet the potential of speech-based Artificial Intelligence for
detecting depression in such contexts remains unexplored. This study examines
the transferability of speech-based depression detection methods to people with
MS (pwMS) through cross-corpus and cross-lingual analysis using English data
from the general population and German data from pwMS. Our approach implements
supervised machine learning models using: 1) conventional speech and language
features commonly used in the field, 2) emotional dimensions derived from a
Speech Emotion Recognition (SER) model, and 3) exploratory speech feature
analysis. Despite limited data, our models detect depressive mood in pwMS with
moderate generalisability, achieving a 66% Unweighted Average Recall (UAR) on a
binary task. Feature selection further improved performance, boosting UAR to
74%. Our findings also highlight the relevant role emotional changes have as an
indicator of depressive mood in both the general population and within PwMS.
This study provides an initial exploration into generalising speech-based
depression detection, even in the presence of co-occurring conditions, such as
neurodegenerative diseases.

</details>


### [98] [Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering](https://arxiv.org/abs/2508.18093)
*Julius Gun,Timo Oksanen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在技术问答任务中的表现，发现混合RAG策略优于直接提示法，并展示了模型在不同语言中的高准确率。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估大型语言模型（LLMs）在技术问答任务中的表现，特别是在跨语言信息检索场景中。

Method: 本文使用直接提示法与三种检索增强生成（RAG）策略（关键词、语义、混合）比较了九种长上下文LLM，并使用LLM作为评委进行评估。

Result: 混合RAG策略在直接长上下文提示法之上表现最佳。像Gemini 2.5 Flash和较小的Qwen 2.5 7B这样的模型在使用RAG时在所有语言中都达到了85%以上的高准确率。

Conclusion: 本文贡献了一个专门工业领域的LLM性能详细分析和一个类似的评估开放框架，突出了实际的权衡和挑战。

Abstract: We present a case study evaluating large language models (LLMs) with
128K-token context windows on a technical question answering (QA) task. Our
benchmark is built on a user manual for an agricultural machine, available in
English, French, and German. It simulates a cross-lingual information retrieval
scenario where questions are posed in English against all three language
versions of the manual. The evaluation focuses on realistic
"needle-in-a-haystack" challenges and includes unanswerable questions to test
for hallucinations. We compare nine long-context LLMs using direct prompting
against three Retrieval-Augmented Generation (RAG) strategies (keyword,
semantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this
specific manual show that Hybrid RAG consistently outperforms direct
long-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5
7B achieve high accuracy (over 85%) across all languages with RAG. This paper
contributes a detailed analysis of LLM performance in a specialized industrial
domain and an open framework for similar evaluations, highlighting practical
trade-offs and challenges.

</details>


### [99] [Detecting and Characterizing Planning in Language Models](https://arxiv.org/abs/2508.18098)
*Jatin Nainani,Sankaran Vaidyanathan,Connor Watts,Andre N. Assis,Alice Rigg*

Main category: cs.CL

TL;DR: 本文提出了一种检测LLM中规划行为的方法，并发现规划并非普遍现象，不同模型和任务中存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设固定的规划范围，通常集中在单个提示或狭窄领域，因此需要一种区分规划与即兴创作的方法。

Method: 本文提出了形式化且基于因果关系的标准来检测规划，并将其操作化为半自动化注释流程。

Result: Gemma-2-2B模型在诗歌生成任务中通过即兴创作解决问题，而在MBPP任务中则在规划和即兴创作之间切换。指令微调改进了基础模型中的现有规划行为。

Conclusion: 本文的研究为LLM中规划行为的机制研究提供了可重复和可扩展的基础。

Abstract: Modern large language models (LLMs) have demonstrated impressive performance
across a wide range of multi-step reasoning tasks. Recent work suggests that
LLMs may perform planning - selecting a future target token in advance and
generating intermediate tokens that lead towards it - rather than merely
improvising one token at a time. However, existing studies assume fixed
planning horizons and often focus on single prompts or narrow domains. To
distinguish planning from improvisation across models and tasks, we present
formal and causally grounded criteria for detecting planning and operationalize
them as a semi-automated annotation pipeline. We apply this pipeline to both
base and instruction-tuned Gemma-2-2B models on the MBPP code generation
benchmark and a poem generation task where Claude 3.5 Haiku was previously
shown to plan. Our findings show that planning is not universal: unlike Haiku,
Gemma-2-2B solves the same poem generation task through improvisation, and on
MBPP it switches between planning and improvisation across similar tasks and
even successive token predictions. We further show that instruction tuning
refines existing planning behaviors in the base model rather than creating them
from scratch. Together, these studies provide a reproducible and scalable
foundation for mechanistic studies of planning in LLMs.

</details>


### [100] [SentiMM: A Multimodal Multi-Agent Framework for Sentiment Analysis in Social Media](https://arxiv.org/abs/2508.18108)
*Xilai Xu,Zilin Zhao,Chengye Song,Zining Wang,Jinhe Qiang,Jiongrui Yan,Yuhuai Lin*

Main category: cs.CL

TL;DR: 本文提出了一种名为SentiMM的多智能体框架，以解决社交媒体上多模态内容情感分析中的挑战。该框架通过专门代理处理文本和视觉输入，融合多模态特征，并通过知识检索丰富上下文，最终进行情感分类。实验表明SentiMM在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Existing methods often lack effective cross-modal fusion and external knowledge integration, leading to challenges in processing heterogeneous data and recognizing multi-label emotions.

Method: SentiMM, a novel multi-agent framework, processes text and visual inputs through specialized agents, fuses multimodal features, enriches context via knowledge retrieval, and aggregates results for final sentiment classification.

Result: Extensive experiments demonstrate that SentiMM achieves superior performance compared to state-of-the-art baselines.

Conclusion: SentiMM achieves superior performance compared to state-of-the-art baselines, validating the effectiveness of our structured approach.

Abstract: With the increasing prevalence of multimodal content on social media,
sentiment analysis faces significant challenges in effectively processing
heterogeneous data and recognizing multi-label emotions. Existing methods often
lack effective cross-modal fusion and external knowledge integration. We
propose SentiMM, a novel multi-agent framework designed to systematically
address these challenges. SentiMM processes text and visual inputs through
specialized agents, fuses multimodal features, enriches context via knowledge
retrieval, and aggregates results for final sentiment classification. We also
introduce SentiMMD, a large-scale multimodal dataset with seven fine-grained
sentiment categories. Extensive experiments demonstrate that SentiMM achieves
superior performance compared to state-of-the-art baselines, validating the
effectiveness of our structured approach.

</details>


### [101] [Toward a Better Localization of Princeton WordNet](https://arxiv.org/abs/2508.18134)
*Abed Alhakim Freihat*

Main category: cs.CL

TL;DR: 本文提出了一种结构化的普林斯顿WordNet本地化框架，并报告了将其应用于10,000个同义词集的成果。


<details>
  <summary>Details</summary>
Motivation: 随着普林斯顿WordNet在自然语言处理中的重要性不断增加，对其本地化以及确保这一过程的质量变得越来越关键。现有的努力在规模和严谨性方面仍然有限，而且缺乏研究来解决本地化的准确性及其与阿拉伯文化背景的一致性问题。

Method: 本文提出了一种结构化的框架，用于普林斯顿WordNet的本地化，详细描述了实现高质量结果所需的阶段和程序。

Result: 本文报告了将该框架应用于10,000个同义词集的成果。

Conclusion: 本文提出了一个结构化的普林斯顿WordNet本地化框架，详细描述了实现高质量结果所需的阶段和程序，并报告了将该框架应用于10,000个同义词集的成果。

Abstract: As Princeton WordNet continues to gain significance as a semantic lexicon in
Natural Language Processing, the need for its localization and for ensuring the
quality of this process has become increasingly critical. Existing efforts
remain limited in both scale and rigor, and there is a notable absence of
studies addressing the accuracy of localization or its alignment with the
cultural context of Arabic. This paper proposes a structured framework for the
localization of Princeton WordNet, detailing the stages and procedures required
to achieve high-quality results without compromising cultural authenticity. We
further present our experience in applying this framework, reporting outcomes
from the localization of 10,000 synsets.

</details>


### [102] [S2Sent: Nested Selectivity Aware Sentence Representation Learning](https://arxiv.org/abs/2508.18164)
*Jianxiang Zang,Nijia Mo,Yonda Wei,Meiling Ning,Hui Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer的句子表示选择机制S²Sent，通过空间选择和嵌套频率选择来优化跨块表示融合，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的编码器与对比学习的结合是句子表示学习的主流范式，但不同块表现出不同程度的语义感知能力。从可解释性的角度来看，知识神经元的语义感知潜力受刺激调节，因此合理的跨块表示融合是一个值得优化的方向。

Method: 提出了一种句子表示选择机制S²Sent，该机制在基于Transformer的编码器下游集成了一个参数化嵌套选择器。该选择器从模块化角度进行空间选择（SS）和嵌套频率选择（FS）。SS创新性地采用基于空间挤压的自门控机制来获得自适应权重，不仅实现了低信息冗余的融合，还捕捉了嵌入特征之间的依赖关系。嵌套FS用不同的DCT基函数替换GAP，以实现低语义损失的空间挤压。

Result: 广泛的实验表明，S²Sent在保持极低额外参数和推理延迟的同时，显著优于基线方法，并突出了高可集成性和可扩展性。

Conclusion: S²Sent在保持极低额外参数和推理延迟的同时，显著优于基线方法，并突出了高可集成性和可扩展性。

Abstract: The combination of Transformer-based encoders with contrastive learning
represents the current mainstream paradigm for sentence representation
learning. This paradigm is typically based on the hidden states of the last
Transformer block of the encoder. However, within Transformer-based encoders,
different blocks exhibit varying degrees of semantic perception ability. From
the perspective of interpretability, the semantic perception potential of
knowledge neurons is modulated by stimuli, thus rational cross-block
representation fusion is a direction worth optimizing. To balance the semantic
redundancy and loss across block fusion, we propose a sentence representation
selection mechanism S\textsuperscript{2}Sent, which integrates a parameterized
nested selector downstream of the Transformer-based encoder. This selector
performs spatial selection (SS) and nested frequency selection (FS) from a
modular perspective. The SS innovatively employs a spatial squeeze based
self-gating mechanism to obtain adaptive weights, which not only achieves
fusion with low information redundancy but also captures the dependencies
between embedding features. The nested FS replaces GAP with different DCT basis
functions to achieve spatial squeeze with low semantic loss. Extensive
experiments have demonstrated that S\textsuperscript{2}Sent achieves
significant improvements over baseline methods with negligible additional
parameters and inference latency, while highlighting high integrability and
scalability.

</details>


### [103] [DiscussLLM: Teaching Large Language Models When to Speak](https://arxiv.org/abs/2508.18167)
*Deep Anil Patel,Iain Melvin,Christopher Malon,Martin Renqiang Min*

Main category: cs.CL

TL;DR: 本文提出了一种名为DiscussLLM的框架，旨在通过训练模型主动决定何时发言来弥合大型语言模型在动态人类讨论中的“意识差距”。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在理解和生成类似人类的文本方面表现出色，但它们主要作为反应性代理，只有在直接提示时才会响应。这种被动性造成了“意识差距”，限制了它们作为真正协作伙伴的潜力。本文旨在解决这一问题，使模型能够主动决定何时发言。

Method: 本文提出了一种可扩展的两阶段数据生成管道，合成大规模的真实多轮人类讨论数据集。每个讨论都被标注了五种干预类型，并包含一个显式的对话触发点，其中AI干预可以增加价值。通过训练模型在不需要干预时预测一个特殊的静默标记，它们学会了在有帮助的贡献时才发言。还探索了两种架构基线：集成端到端模型和优化低延迟推理的解耦分类器-生成系统。

Result: 本文提出的框架能够准确地把握干预时机并生成有帮助的回应，为更情境感知和主动的对话AI铺平了道路。

Conclusion: 本文提出了DiscussLLM框架，旨在通过训练模型主动决定何时发言来弥合大型语言模型在动态人类讨论中的“意识差距”。实验结果表明，该框架能够准确地把握干预时机并生成有帮助的回应，为更情境感知和主动的对话AI铺平了道路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
understanding and generating human-like text, yet they largely operate as
reactive agents, responding only when directly prompted. This passivity creates
an "awareness gap," limiting their potential as truly collaborative partners in
dynamic human discussions. We introduce $\textit{DiscussLLM}$, a framework
designed to bridge this gap by training models to proactively decide not just
$\textit{what}$ to say, but critically, $\textit{when}$ to speak. Our primary
contribution is a scalable two-stage data generation pipeline that synthesizes
a large-scale dataset of realistic multi-turn human discussions. Each
discussion is annotated with one of five intervention types (e.g., Factual
Correction, Concept Definition) and contains an explicit conversational trigger
where an AI intervention adds value. By training models to predict a special
silent token when no intervention is needed, they learn to remain quiet until a
helpful contribution can be made. We explore two architectural baselines: an
integrated end-to-end model and a decoupled classifier-generator system
optimized for low-latency inference. We evaluate these models on their ability
to accurately time interventions and generate helpful responses, paving the way
for more situationally aware and proactive conversational AI.

</details>


### [104] [Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation](https://arxiv.org/abs/2508.18168)
*Hongyu Cao,Yuxuan Wu,Yucheng Cai,Xianyu Zhao,Zhijian Ou*

Main category: cs.CL

TL;DR: 本文提出了JSA-RAG，一种基于联合随机近似的端到端训练方法，用于改进RAG模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的方法在端到端优化RAG模型时存在偏差或高方差的梯度估计问题，因此需要一种更有效的训练方法。

Method: 提出了一种基于联合随机近似（JSA）的端到端训练方法，称为JSA-RAG，该方法是EM算法的随机扩展，特别适用于离散潜在变量模型的估计。

Result: 在五个数据集上的实验表明，JSA-RAG在开放域问答和知识基础对话任务中显著优于vanilla RAG和VRAG。

Conclusion: JSA-RAG显著优于传统的RAG和VRAG，并在生成、检索和低方差梯度估计方面表现出色。

Abstract: Retrieval-augmented generation (RAG) has become a widely recognized paradigm
to combine parametric memory with non-parametric memories. An RAG model
consists of two serial connecting components (retriever and generator). A major
challenge in end-to-end optimization of the RAG model is that marginalization
over relevant passages (modeled as discrete latent variables) from a knowledge
base is required. Traditional top-K marginalization and variational RAG (VRAG)
suffer from biased or high-variance gradient estimates. In this paper, we
propose and develop joint stochastic approximation (JSA) based end-to-end
training of RAG, which is referred to as JSA-RAG. The JSA algorithm is a
stochastic extension of the EM (expectation-maximization) algorithm and is
particularly powerful in estimating discrete latent variable models. Extensive
experiments are conducted on five datasets for two tasks (open-domain question
answering, knowledge-grounded dialogs) and show that JSA-RAG significantly
outperforms both vanilla RAG and VRAG. Further analysis shows the efficacy of
JSA-RAG from the perspectives of generation, retrieval, and low-variance
gradient estimate.

</details>


### [105] [Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios](https://arxiv.org/abs/2508.18183)
*Luana Bulla,Gabriele Tuccio,Misael Mongiovì,Aldo Gangemi*

Main category: cs.CL

TL;DR: 本文提出了一种新的手语翻译方法AulSign，利用大型语言模型通过动态提示和上下文学习进行翻译，并在低数据场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏平行语料库，自然语言到手语的翻译是一个高度复杂且研究不足的任务。现有的方法在数据稀缺环境中难以推广。

Method: 我们提出了AulSign，这是一种利用大型语言模型通过动态提示和上下文学习进行手语翻译的新方法。

Result: 我们在英语和意大利语上评估了我们的方法，并使用SignBank+和意大利LaCAM CNR-ISTC数据集进行了测试，结果优于最先进的模型。

Conclusion: 我们的研究展示了AulSign的有效性，具有增强边缘化语言社区的通信技术可访问性和包容性的潜力。

Abstract: Translating natural languages into sign languages is a highly complex and
underexplored task. Despite growing interest in accessibility and inclusivity,
the development of robust translation systems remains hindered by the limited
availability of parallel corpora which align natural language with sign
language data. Existing methods often struggle to generalize in these
data-scarce environments, as the few datasets available are typically
domain-specific, lack standardization, or fail to capture the full linguistic
richness of sign languages. To address this limitation, we propose Advanced Use
of LLMs for Sign Language Translation (AulSign), a novel method that leverages
Large Language Models via dynamic prompting and in-context learning with sample
selection and subsequent sign association. Despite their impressive abilities
in processing text, LLMs lack intrinsic knowledge of sign languages; therefore,
they are unable to natively perform this kind of translation. To overcome this
limitation, we associate the signs with compact descriptions in natural
language and instruct the model to use them. We evaluate our method on both
English and Italian languages using SignBank+, a recognized benchmark in the
field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior
performance compared to state-of-the-art models in low-data scenario. Our
findings demonstrate the effectiveness of AulSign, with the potential to
enhance accessibility and inclusivity in communication technologies for
underrepresented linguistic communities.

</details>


### [106] [Exploring the Interplay between Musical Preferences and Personality through the Lens of Language](https://arxiv.org/abs/2508.18208)
*Eliran Shem-Tov,Ella Rabinovich*

Main category: cs.CL

TL;DR: 本研究探讨了音乐偏好是否能在自发语言中通过大五人格特质进行识别，并发现了不同音乐流派粉丝的人格特征差异。


<details>
  <summary>Details</summary>
Motivation: 音乐偏好与人格特质之间存在关联，而语言分析可以检测人格。我们的研究旨在探讨音乐偏好是否能在自发语言中被识别。

Method: 我们使用了一个精心策划的超过50万条文本样本的数据集，来自近5000位具有可靠识别音乐偏好的作者，构建了先进的模型来评估人格特征。

Result: 我们的结果揭示了五种音乐流派的粉丝在人格特征上有显著差异。

Conclusion: 我们的研究揭示了不同音乐流派的粉丝在人格特征上有显著差异，并为计算语言学、音乐心理学和人格分析的交叉领域提供了资源。

Abstract: Music serves as a powerful reflection of individual identity, often aligning
with deeper psychological traits. Prior research has established correlations
between musical preferences and personality traits, while separate studies have
demonstrated that personality is detectable through linguistic analysis. Our
study bridges these two research domains by investigating whether individuals'
musical preferences are recognizable in their spontaneous language through the
lens of the Big Five personality traits (Openness, Conscientiousness,
Extroversion, Agreeableness, and Neuroticism). Using a carefully curated
dataset of over 500,000 text samples from nearly 5,000 authors with reliably
identified musical preferences, we build advanced models to assess personality
characteristics. Our results reveal significant personality differences across
fans of five musical genres. We release resources for future research at the
intersection of computational linguistics, music psychology and personality
analysis.

</details>


### [107] [Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation](https://arxiv.org/abs/2508.18210)
*Rishikesh Devanathan,Varun Nathan,Ayush Kumar*

Main category: cs.CL

TL;DR: 本文研究了客服领域中合成转录生成的问题，提出了一种诊断框架来评估合成对话的质量，并发现现有方法在多个方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 合成转录生成在客服领域至关重要，因为隐私和数据稀缺限制了模型训练和评估。客服对话是目标导向的、角色不对称的，并且行为复杂，具有不流畅、ASR噪声和合规驱动的代理行为。在没有转录文本的部署中，标准流程仍然会产生派生的电话属性，如意图摘要、主题流程和QA评估表。我们利用这些作为监督信号来指导生成。

Method: 我们引入了一个包含18个语言学和行为基础指标的诊断框架，用于比较真实和合成的转录文本。我们对四种语言无关的生成策略进行了基准测试，从简单的提示到特征感知的多阶段方法，以及无参考基线。

Result: 结果揭示了持续的挑战：没有一种方法在所有特性上都表现出色，特别是在不流畅、情感和行为现实性方面有显著的不足。

Conclusion: 我们的诊断工具暴露了这些差距，使合成对话在不同语言中的细粒度评估和压力测试成为可能。

Abstract: Synthetic transcript generation is critical in contact center domains, where
privacy and data scarcity limit model training and evaluation. Unlike prior
synthetic dialogue generation work on open-domain or medical dialogues, contact
center conversations are goal-oriented, role-asymmetric, and behaviorally
complex, featuring disfluencies, ASR noise, and compliance-driven agent
actions. In deployments where transcripts are unavailable, standard pipelines
still yield derived call attributes such as Intent Summaries, Topic Flow, and
QA Evaluation Forms. We leverage these as supervision signals to guide
generation. To assess the quality of such outputs, we introduce a diagnostic
framework of 18 linguistically and behaviorally grounded metrics for comparing
real and synthetic transcripts. We benchmark four language-agnostic generation
strategies, from simple prompting to characteristic-aware multi-stage
approaches, alongside reference-free baselines. Results reveal persistent
challenges: no method excels across all traits, with notable deficits in
disfluency, sentiment, and behavioral realism. Our diagnostic tool exposes
these gaps, enabling fine-grained evaluation and stress testing of synthetic
dialogue across languages.

</details>


### [108] [Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries](https://arxiv.org/abs/2508.18212)
*Meiling Ning,Zhongbao Zhang,Junda Ye,Jiabao Guo,Qingyuan Guan*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言模型的判断奖励模型ESFP-RM，通过利用基于解释的槽框架来提升奖励模型的性能，并在RLHF和OOD场景中展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 为了进一步推进基于语言模型的奖励建模范式，提出了一个核心见解：这种形式的奖励建模与自然语言推理（NLI）有根本的正式一致性。

Method: 提出了一种基于语言模型的判断奖励模型ESFP-RM，该模型利用基于解释的槽框架进行预测，以充分利用掩码语言模型（MLM）的优势。

Result: 实验表明，结合上下文解释的槽预测掩码语言模型（MLMs）在NLI任务上的表现显著优于主流的自回归模型。

Conclusion: ESFP-RM框架在从人类反馈中进行强化学习（RLHF）和分布外（OOD）场景中，相比生成式奖励模型提供了更稳定和通用的奖励信号。

Abstract: The emergence of LM-based judging reward modeling, represented by generative
reward models, has successfully made reinforcement learning from AI feedback
(RLAIF) efficient and scalable. To further advance this paradigm, we propose a
core insight: this form of reward modeling shares fundamental formal
consistency with natural language inference (NLI), a core task in natural
language understanding. This reframed perspective points to a key path for
building superior reward models: scaling the model's comprehension boundaries.
Pursuing this path, exploratory experiments on NLI tasks demonstrate that the
slot prediction masked language models (MLMs) incorporating contextual
explanations achieve significantly better performance compared to mainstream
autoregressive models. Based on this key finding, we propose ESFP-RM, a
two-stage LM-based judging reward model that utilizes an explanation based slot
framework for prediction to fully leverage the advantages of MLMs. Extensive
experiments demonstrate that in both reinforcement learning from human feedback
(RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers
more stable and generalizable reward signals compared to generative reward
models.

</details>


### [109] [MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues via Arena-style and Rubrics Protocols](https://arxiv.org/abs/2508.18240)
*Yuhao Du,Qianwei Huang,Guo Zhu,Zhanchen Dai,Sunian Chen,Qiming Zhu,Yuhao Zhang,Li Zhou,Benyou Wang*

Main category: cs.CL

TL;DR: 本文提出了MTalk-Bench，一个用于评估多轮语音到语音大型语言模型的基准测试，揭示了当前评估框架的局限性，并强调了需要更稳健、语音感知的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前评估框架无法充分评估复杂多轮对话中的性能，因此需要一个新的基准测试来解决这个问题。

Method: 引入MTalk-Bench，一个涵盖三个核心维度的多轮S2S基准测试，包括语义信息、副语言信息和环境声音。采用双方法评估框架，结合Arena风格评估（成对比较）和基于评分标准的评估（绝对评分）。

Result: S2S LLMs在语义信息处理方面表现出色，但在副语言信息和环境声音感知方面表现不佳；模型通常通过增加响应长度来恢复连贯性，但牺牲了多轮对话的效率；模态感知、任务特定设计优于盲目扩展。Arena和Rubrics产生一致且互补的排名，但只有在性能差距较大时才能可靠地区分；LLM作为评判者在清晰差距或明确标准时与人类一致，但存在位置和长度偏差，并且仅在有文本注释时在非言语评估上可靠。

Conclusion: 当前的评估框架在评估复杂多轮对话中的表现仍然不足，需要更稳健、语音感知的评估框架。

Abstract: The rapid advancement of speech-to-speech (S2S) large language models (LLMs)
has significantly improved real-time spoken interaction. However, current
evaluation frameworks remain inadequate for assessing performance in complex,
multi-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn
S2S benchmark covering three core dimensions: Semantic Information,
Paralinguistic Information, and Ambient Sound. Each dimension includes nine
realistic scenarios, along with targeted tasks to assess specific capabilities
such as reasoning. Our dual-method evaluation framework combines Arena-style
evaluation (pairwise comparison) and Rubrics-based evaluation (absolute
scoring) for relative and absolute assessment. The benchmark includes both
model and human outputs, evaluated by human evaluators and LLMs. Experimental
results reveal two sets of findings. Overall performance of S2S LLMs: (1)
models excel at semantic information processing yet underperform on
paralinguistic information and ambient sounds perception; (2) models typically
regain coherence by increasing response length, sacrificing efficiency in
multi-turn dialogues; (3) modality-aware, task-specific designs outperform
brute scaling. Evaluation framework and reliability: (1) Arena and Rubrics
yield consistent, complementary rankings, but reliable distinctions emerge only
when performance gaps are large; (2) LLM-as-a-judge aligns with humans when
gaps are clear or criteria explicit, but exhibits position and length biases
and is reliable on nonverbal evaluation only with text annotations. These
results highlight current limitations in S2S evaluation and the need for more
robust, speech-aware assessment frameworks.

</details>


### [110] [Demographic Biases and Gaps in the Perception of Sexism in Large Language Models](https://arxiv.org/abs/2508.18245)
*Judith Tavarez-Rodríguez,Fernando Sánchez-Vega,A. Pastor López-Monroy*

Main category: cs.CL

TL;DR: 本文探讨了不同大型语言模型在社交媒体文本中检测性别歧视的能力，并分析了模型中存在的性别和年龄等人口统计学偏差。研究结果表明，虽然大型语言模型可以在一定程度上检测性别歧视，但它们不能准确复制不同人口群体之间感知的多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有各种努力来提高性别歧视内容的检测，但由于其主观性和自动化模型中的偏见，这一任务仍然是一项重大挑战。我们希望通过研究不同大型语言模型在检测性别歧视方面的能力，为这一领域提供新的见解。

Method: 我们使用EXIST 2024推文数据集探索了不同大型语言模型在社交媒体文本中检测性别歧视的能力。该数据集为每条推文提供了六个不同的群体注释，使我们能够评估大型语言模型在性别歧视检测中模仿这些群体感知的程度。此外，我们分析了模型中存在的性别和年龄等人口统计学偏差，并进行了统计分析以确定哪些人口统计学特征最有效地完成这项任务。

Result: 我们的结果显示，虽然大型语言模型在考虑总体人口观点时可以在一定程度上检测性别歧视，但它们不能准确复制不同人口群体之间感知的多样性。

Conclusion: 我们的研究结果表明，尽管大型语言模型在考虑总体人口观点时可以在一定程度上检测性别歧视，但它们并不能准确复制不同人口群体之间感知的多样性。这突显了需要更精确校准的模型，以考虑不同人口群体之间的视角多样性。

Abstract: The use of Large Language Models (LLMs) has proven to be a tool that could
help in the automatic detection of sexism. Previous studies have shown that
these models contain biases that do not accurately reflect reality, especially
for minority groups. Despite various efforts to improve the detection of sexist
content, this task remains a significant challenge due to its subjective nature
and the biases present in automated models. We explore the capabilities of
different LLMs to detect sexism in social media text using the EXIST 2024 tweet
dataset. It includes annotations from six distinct profiles for each tweet,
allowing us to evaluate to what extent LLMs can mimic these groups' perceptions
in sexism detection. Additionally, we analyze the demographic biases present in
the models and conduct a statistical analysis to identify which demographic
characteristics (age, gender) contribute most effectively to this task. Our
results show that, while LLMs can to some extent detect sexism when considering
the overall opinion of populations, they do not accurately replicate the
diversity of perceptions among different demographic groups. This highlights
the need for better-calibrated models that account for the diversity of
perspectives across different populations.

</details>


### [111] [From BERT to LLMs: Comparing and Understanding Chinese Classifier Prediction in Language Models](https://arxiv.org/abs/2508.18253)
*ZiqiZhang,Jianfei Ma,Emmanuele Chersoni,Jieshun You,Zhaoxin Feng*

Main category: cs.CL

TL;DR: This paper evaluates the ability of Large Language Models (LLMs) to predict Chinese classifiers and finds that they perform worse than BERT, even with fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Classifiers are an important and defining feature of the Chinese language, and their correct prediction is key to numerous educational applications. However, it is unclear whether popular LLMs possess proper knowledge about Chinese classifiers.

Method: We employ various masking strategies to evaluate the LLMs' intrinsic ability, the contribution of different sentence elements, and the working of the attention mechanisms during prediction. We also explore fine-tuning for LLMs to enhance the classifier performance.

Result: LLMs perform worse than BERT, even with fine-tuning. The prediction greatly benefits from the information about the following noun, which explains the advantage of models with a bidirectional attention mechanism such as BERT.

Conclusion: LLMs perform worse than BERT, even with fine-tuning.

Abstract: Classifiers are an important and defining feature of the Chinese language,
and their correct prediction is key to numerous educational applications. Yet,
whether the most popular Large Language Models (LLMs) possess proper knowledge
the Chinese classifiers is an issue that has largely remain unexplored in the
Natural Language Processing (NLP) literature.
  To address such a question, we employ various masking strategies to evaluate
the LLMs' intrinsic ability, the contribution of different sentence elements,
and the working of the attention mechanisms during prediction. Besides, we
explore fine-tuning for LLMs to enhance the classifier performance.
  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning.
The prediction, as expected, greatly benefits from the information about the
following noun, which also explains the advantage of models with a
bidirectional attention mechanism such as BERT.

</details>


### [112] [MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains](https://arxiv.org/abs/2508.18260)
*Kaiwen Wei,Rui Shan,Dongsheng Zou,Jianzhong Yang,Bi Zhao,Junnan Zhu,Jiang Zhong*

Main category: cs.CL

TL;DR: MIRAGE is a new framework for medical QA tasks that uses structured knowledge graphs to perform dynamic multi-chain inference, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current approaches for medical QA tasks suffer from error accumulation due to single, linear reasoning chains and flat, context-agnostic information integration. MIRAGE addresses these challenges by introducing a dynamic multi-chain inference framework over structured medical knowledge graphs.

Method: MIRAGE decomposes complex queries into entity-grounded sub-questions, executes parallel inference chains, retrieves evidence adaptively via neighbor expansion and multi-hop traversal, and integrates answers using cross-chain verification.

Result: Experiments on three medical QA benchmarks show that MIRAGE outperforms GPT-4o, Tree-of-Thought variants, and other retrieval-augmented baselines in both automatic and human evaluations. It also improves interpretability by generating explicit reasoning chains.

Conclusion: MIRAGE consistently outperforms existing methods in medical QA tasks and improves interpretability by generating explicit reasoning chains.

Abstract: Large reasoning models (LRMs) have shown significant progress in test-time
scaling through chain-of-thought prompting. Current approaches like search-o1
integrate retrieval augmented generation (RAG) into multi-step reasoning
processes but rely on a single, linear reasoning chain while incorporating
unstructured textual information in a flat, context-agnostic manner. As a
result, these approaches can lead to error accumulation throughout the
reasoning chain, which significantly limits its effectiveness in medical
question-answering (QA) tasks where both accuracy and traceability are critical
requirements. To address these challenges, we propose MIRAGE (Multi-chain
Inference with Retrieval-Augmented Graph Exploration), a novel test-time
scalable reasoning framework that performs dynamic multi-chain inference over
structured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex
queries into entity-grounded sub-questions, 2) executes parallel inference
chains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop
traversal, and 4) integrates answers using cross-chain verification to resolve
contradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,
CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,
Tree-of-Thought variants, and other retrieval-augmented baselines in both
automatic and human evaluations. Additionally, MIRAGE improves interpretability
by generating explicit reasoning chains that trace each factual claim to
concrete chains within the knowledge graph, making it well-suited for complex
medical reasoning scenarios. The code will be available for further research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [113] [Humans Perceive Wrong Narratives from AI Reasoning Texts](https://arxiv.org/abs/2508.16599)
*Mosh Levy,Zohar Elyoseph,Yoav Goldberg*

Main category: cs.HC

TL;DR: 本研究发现人类在理解AI生成的推理文本中的因果关系方面存在显著差距，表明推理文本可能无法作为有效的可解释性工具。


<details>
  <summary>Details</summary>
Motivation: 本文旨在调查人类是否能够理解推理文本中的因果关系，这可能是人类理解模型计算过程的一个必要条件。

Method: 我们通过基于反事实测量的问题评估了人类识别推理文本中因果影响步骤的能力。

Result: 参与者的准确率仅为29.3%，仅略高于随机（25%），即使在评估高一致性的问题多数投票时，准确率也仅达到42%。

Conclusion: 我们的研究揭示了人类对推理文本的理解与模型实际使用方式之间的根本差距，挑战了将其作为简单可解释性工具的实用性。我们主张将推理文本视为需要研究的产物，而不是表面价值，理解这些模型使用语言的非人类方式是一个关键的研究方向。

Abstract: A new generation of AI models generates step-by-step reasoning text before
producing an answer. This text appears to offer a human-readable window into
their computation process, and is increasingly relied upon for transparency and
interpretability. However, it is unclear whether human understanding of this
text matches the model's actual computational process. In this paper, we
investigate a necessary condition for correspondence: the ability of humans to
identify which steps in a reasoning text causally influence later steps. We
evaluated humans on this ability by composing questions based on counterfactual
measurements and found a significant discrepancy: participant accuracy was only
29.3%, barely above chance (25%), and remained low (42%) even when evaluating
the majority vote on questions with high agreement. Our results reveal a
fundamental gap between how humans interpret reasoning texts and how models use
it, challenging its utility as a simple interpretability tool. We argue that
reasoning texts should be treated as an artifact to be investigated, not taken
at face value, and that understanding the non-human ways these models use
language is a critical research direction.

</details>


### [114] [Can AI Have a Personality? Prompt Engineering for AI Personality Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training](https://arxiv.org/abs/2508.18234)
*Tailon D. Jackson,Byunggu Yu*

Main category: cs.HC

TL;DR: 本研究探讨了通过提示工程使大型语言模型模拟一致人格的可能性，并在语音语言病理学学生培训的聊天机器人中进行了测试，结果表明这种方法有效。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨是否可以通过提示工程使大型语言模型模拟一致的人格特征，并应用于语音语言病理学学生培训的聊天机器人中。

Method: 本研究通过提示工程探索了大型语言模型（LLMs）能否模拟一致的人格特征，并在语音语言病理学（SLP）学生培训的聊天机器人中进行了测试。

Result: 研究发现，通过提示工程，聊天机器人能够保持可识别且一致的人格特征，并基于大五人格测试具有独特的人格特征。

Conclusion: 研究结果表明，通过提示工程可以模拟AI聊天机器人的稳定人格特征。

Abstract: This thesis investigates whether large language models (LLMs) can be guided
to simulate a consistent personality through prompt engineering. The study
explores this concept within the context of a chatbot designed for
Speech-Language Pathology (SLP) student training, specifically focused on
gender-affirming voice therapy. The chatbot, named Monae Jackson, was created
to represent a 32-year-old transgender woman and engage in conversations
simulating client-therapist interactions. Findings suggest that with prompt
engineering, the chatbot maintained a recognizable and consistent persona and
had a distinct personality based on the Big Five Personality test. These
results support the idea that prompt engineering can be used to simulate stable
personality characteristics in AI chatbots.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [115] [THEME : Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics](https://arxiv.org/abs/2508.16936)
*Hoyoung Lee,Wonbin Ahn,Suhwan Park,Jaehoon Lee,Minjae Kim,Sungdong Yoo,Taeyoon Lim,Woohyung Lim,Yongjae Lee*

Main category: q-fin.PM

TL;DR: THEME is a hierarchical contrastive learning framework that improves thematic investing by jointly modeling thematic relationships from text and market dynamics from returns.


<details>
  <summary>Details</summary>
Motivation: Thematic investing aims to construct portfolios aligned with structural trends, but selecting relevant stocks remains challenging due to overlapping sector boundaries and evolving market dynamics. The Thematic Representation Set (TRS) is constructed to overcome coverage limitations of thematic ETFs by incorporating industry classifications and financial news.

Method: THEME is a hierarchical contrastive learning framework that represents textual profiles of themes and stocks as embeddings. It leverages their hierarchical relationship to achieve semantic alignment and refines these embeddings through a temporal refinement stage that incorporates individual stock returns.

Result: Empirical results show that THEME outperforms strong baselines across multiple retrieval metrics and significantly improves performance in portfolio construction.

Conclusion: THEME provides a scalable and adaptive solution for navigating complex investment themes by jointly modeling thematic relationships from text and market dynamics from returns.

Abstract: Thematic investing aims to construct portfolios aligned with structural
trends, yet selecting relevant stocks remains challenging due to overlapping
sector boundaries and evolving market dynamics. To address this challenge, we
construct the Thematic Representation Set (TRS), an extended dataset that
begins with real-world thematic ETFs and expands upon them by incorporating
industry classifications and financial news to overcome their coverage
limitations. The final dataset contains both the explicit mapping of themes to
their constituent stocks and the rich textual profiles for each. Building on
this dataset, we introduce \textsc{THEME}, a hierarchical contrastive learning
framework. By representing the textual profiles of themes and stocks as
embeddings, \textsc{THEME} first leverages their hierarchical relationship to
achieve semantic alignment. Subsequently, it refines these semantic embeddings
through a temporal refinement stage that incorporates individual stock returns.
The final stock representations are designed for effective retrieval of
thematically aligned assets with strong return potential. Empirical results
show that \textsc{THEME} outperforms strong baselines across multiple retrieval
metrics and significantly improves performance in portfolio construction. By
jointly modeling thematic relationships from text and market dynamics from
returns, \textsc{THEME} provides a scalable and adaptive solution for
navigating complex investment themes.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [116] [Empirical Analysis of the Effect of Context in the Task of Automated Essay Scoring in Transformer-Based Models](https://arxiv.org/abs/2508.16638)
*Abhirup Chakravarty*

Main category: cs.CY

TL;DR: 本文提出了一种通过引入多个上下文维度来增强基于Transformer的自动作文评分模型的方法，在多个数据集中表现优异，且可适配到任何AES模型中。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构在其他任务中表现出色，但研究表明，替代的深度学习架构在自动作文评分（AES）中表现优于Transformer模型。因此，需要通过上下文增强来改进基于Transformer的AES模型。

Method: 本文使用ASAP-AES数据集分析了各种上下文因素对基于Transformer的模型性能的影响，并提出了一个通过引入多个上下文维度来增强模型的方法。

Result: 通过引入多个上下文维度，所提出的模型在整体作文数据集上实现了0.823的均方加权Kappa分数，在单独的作文集上达到了0.8697。该方法在平均性能上略逊于最先进的深度学习模型，但在八个集中的三个中表现更优。

Conclusion: 本文提出了一种通过引入多种上下文维度来增强基于Transformer的自动作文评分模型的方法。该方法在整体作文数据集上取得了0.823的均方加权Kappa分数，在单独的作文集上达到了0.8697。尽管该方法在平均性能上略逊于最先进的深度学习模型，但在八个集中的三个中表现更优。此外，这种增强方法与架构改进是正交的，可以无缝适配到任何AES模型中，为自动评分和评估的发展提供了灵活的技术。

Abstract: Automated Essay Scoring (AES) has emerged to prominence in response to the
growing demand for educational automation. Providing an objective and
cost-effective solution, AES standardises the assessment of extended responses.
Although substantial research has been conducted in this domain, recent
investigations reveal that alternative deep-learning architectures outperform
transformer-based models. Despite the successful dominance in the performance
of the transformer architectures across various other tasks, this discrepancy
has prompted a need to enrich transformer-based AES models through contextual
enrichment.
  This study delves into diverse contextual factors using the ASAP-AES dataset,
analysing their impact on transformer-based model performance. Our most
effective model, augmented with multiple contextual dimensions, achieves a mean
Quadratic Weighted Kappa score of 0.823 across the entire essay dataset and
0.8697 when trained on individual essay sets. Evidently surpassing prior
transformer-based models, this augmented approach only underperforms relative
to the state-of-the-art deep learning model trained essay-set-wise by an
average of 3.83\% while exhibiting superior performance in three of the eight
sets.
  Importantly, this enhancement is orthogonal to architecture-based
advancements and seamlessly adaptable to any AES model. Consequently, this
contextual augmentation methodology presents a versatile technique for refining
AES capabilities, contributing to automated grading and evaluation evolution in
educational settings.

</details>


### [117] [Leveraging Multi-Source Textural UGC for Neighbourhood Housing Quality Assessment: A GPT-Enhanced Framework](https://arxiv.org/abs/2508.16657)
*Qiyuan Hong,Huimin Zhao,Ying Long*

Main category: cs.CY

TL;DR: 该研究利用GPT-4o分析多源UGC，开发了一个改进的住房质量评估系统，展示了其在城市评估中的优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补客观-主观方法差距，并揭示平台特定的关注差异。

Method: 研究利用GPT-4o评估邻里住房质量，使用来自大众点评、微博和政府留言板的多源文本用户生成内容（UGC）。分析包括过滤相关文本、提取结构化评估单元和进行情感评分。

Result: 开发了一个包含11个类别共46个指标的改进住房质量评估系统。GPT-4o在微调设置中实现了92.5%的准确率，优于基于规则和BERT的模型。

Conclusion: 研究强调了整合UGC和GPT驱动分析在可扩展、以居民为中心的城市评估中的价值，为政策制定者和城市规划者提供了实用的见解。

Abstract: This study leverages GPT-4o to assess neighbourhood housing quality using
multi-source textural user-generated content (UGC) from Dianping, Weibo, and
the Government Message Board. The analysis involves filtering relevant texts,
extracting structured evaluation units, and conducting sentiment scoring. A
refined housing quality assessment system with 46 indicators across 11
categories was developed, highlighting an objective-subjective method gap and
platform-specific differences in focus. GPT-4o outperformed rule-based and BERT
models, achieving 92.5% accuracy in fine-tuned settings. The findings
underscore the value of integrating UGC and GPT-driven analysis for scalable,
resident-centric urban assessments, offering practical insights for
policymakers and urban planners.

</details>


### [118] [Invisible Filters: Cultural Bias in Hiring Evaluations Using Large Language Models](https://arxiv.org/abs/2508.16673)
*Pooja S. B. Rao,Laxminarayen Nagarajan Venkatesan,Mauro Cherubini,Dinesh Babu Jayagopi*

Main category: cs.CY

TL;DR: 本研究分析了大型语言模型在跨文化背景下的招聘评估，发现印度简历得分较低，这可能与语言特征有关，而基于姓名的偏见不明显。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在招聘中的日益应用，需要系统地研究LLM在不同文化背景下可能存在的偏见，以确保公平性和信任度。

Method: 我们使用两个面试转录本数据集（100个来自英国，100个来自印度）进行了系统分析，以检查LLM在跨文化和社会身份维度上的招聘评估。我们还进行了受控的身份替换（改变姓名的性别、种姓和区域）来测试基于姓名的偏见。

Result: 印度简历获得的LLM评分普遍低于英国简历，即使在匿名化后，差异与句法复杂性和词汇多样性等语言特征有关。在印度数据集中进行的身份替换没有产生统计学上显著的效果，表明单独的姓名可能不会影响LLM的评估。

Conclusion: 我们的研究强调了在LLM驱动的评估中评估语言和社会维度的重要性，并突出了在AI辅助招聘中进行文化敏感设计和问责的必要性。

Abstract: Artificial Intelligence (AI) is increasingly used in hiring, with large
language models (LLMs) having the potential to influence or even make hiring
decisions. However, this raises pressing concerns about bias, fairness, and
trust, particularly across diverse cultural contexts. Despite their growing
role, few studies have systematically examined the potential biases in
AI-driven hiring evaluation across cultures. In this study, we conduct a
systematic analysis of how LLMs assess job interviews across cultural and
identity dimensions. Using two datasets of interview transcripts, 100 from UK
and 100 from Indian job seekers, we first examine cross-cultural differences in
LLM-generated scores for hirability and related traits. Indian transcripts
receive consistently lower scores than UK transcripts, even when they were
anonymized, with disparities linked to linguistic features such as sentence
complexity and lexical diversity. We then perform controlled identity
substitutions (varying names by gender, caste, and region) within the Indian
dataset to test for name-based bias. These substitutions do not yield
statistically significant effects, indicating that names alone, when isolated
from other contextual signals, may not influence LLM evaluations. Our findings
underscore the importance of evaluating both linguistic and social dimensions
in LLM-driven evaluations and highlight the need for culturally sensitive
design and accountability in AI-assisted hiring.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [119] [Named Entity Recognition of Historical Text via Large Language Model](https://arxiv.org/abs/2508.18090)
*Shibingfeng Zhang,Giovanni Colavizza*

Main category: cs.DL

TL;DR: 研究探索了大型语言模型在历史文档中进行命名实体识别的可行性，结果显示它们在低资源情况下表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统监督机器学习方法需要大量标注数据，而历史文本由于标注数据稀缺或不存在，难以应用。此外，历史语言的变异性与噪声也增加了可靠NER系统的开发难度。

Method: 研究使用了零样本和少量样本提示策略，将大型语言模型应用于历史文档中的命名实体识别任务。

Result: 实验表明，大型语言模型在HIPE-2022数据集上的命名实体识别任务中表现出较强性能，尽管不如完全监督模型。

Conclusion: 研究结果表明，大型语言模型在低资源或历史重要语料库的信息提取中提供了一种可行且高效的替代方案，尽管其性能仍不及完全监督的模型。

Abstract: Large language models have demonstrated remarkable versatility across a wide
range of natural language processing tasks and domains. One such task is Named
Entity Recognition (NER), which involves identifying and classifying proper
names in text, such as people, organizations, locations, dates, and other
specific entities. NER plays a crucial role in extracting information from
unstructured textual data, enabling downstream applications such as information
retrieval from unstructured text.
  Traditionally, NER is addressed using supervised machine learning approaches,
which require large amounts of annotated training data. However, historical
texts present a unique challenge, as the annotated datasets are often scarce or
nonexistent, due to the high cost and expertise required for manual labeling.
In addition, the variability and noise inherent in historical language, such as
inconsistent spelling and archaic vocabulary, further complicate the
development of reliable NER systems for these sources.
  In this study, we explore the feasibility of applying LLMs to NER in
historical documents using zero-shot and few-shot prompting strategies, which
require little to no task-specific training data. Our experiments, conducted on
the HIPE-2022 (Identifying Historical People, Places and other Entities)
dataset, show that LLMs can achieve reasonably strong performance on NER tasks
in this setting. While their performance falls short of fully supervised models
trained on domain-specific annotations, the results are nevertheless promising.
These findings suggest that LLMs offer a viable and efficient alternative for
information extraction in low-resource or historically significant corpora,
where traditional supervised methods are infeasible.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [120] [How Do LLM-Generated Texts Impact Term-Based Retrieval Models?](https://arxiv.org/abs/2508.17715)
*Wei Huang,Keping Bi,Yinqiong Cai,Wei Chen,Jiafeng Guo,Xueqi Cheng*

Main category: cs.IR

TL;DR: 本研究分析了LLM生成内容对基于术语的检索模型的影响，发现这些模型优先选择术语分布与查询相似的文档，而不是表现出源偏见。


<details>
  <summary>Details</summary>
Motivation: 随着由大型语言模型生成的内容充斥互联网，信息检索系统面临区分和处理人类撰写和机器生成文本的挑战。需要了解基于术语的检索模型如何处理这些混合来源的内容。

Method: 进行了语言分析，以研究LLM生成文本的特征，并探讨了基于术语的检索模型是否表现出源偏见。

Result: LLM生成的文本表现出更平滑的高频和更陡峭的低频Zipf斜率、更高的术语特异性和更大的文档级多样性。基于术语的检索模型优先选择术语分布与查询相似的文档，而不是表现出固有的源偏见。

Conclusion: 本研究为理解并解决处理混合来源内容的基于术语的IR系统中的潜在偏差提供了基础。

Abstract: As more content generated by large language models (LLMs) floods into the
Internet, information retrieval (IR) systems now face the challenge of
distinguishing and handling a blend of human-authored and machine-generated
texts. Recent studies suggest that neural retrievers may exhibit a preferential
inclination toward LLM-generated content, while classic term-based retrievers
like BM25 tend to favor human-written documents. This paper investigates the
influence of LLM-generated content on term-based retrieval models, which are
valued for their efficiency and robust generalization across domains. Our
linguistic analysis reveals that LLM-generated texts exhibit smoother
high-frequency and steeper low-frequency Zipf slopes, higher term specificity,
and greater document-level diversity. These traits are aligned with LLMs being
trained to optimize reader experience through diverse and precise expressions.
Our study further explores whether term-based retrieval models demonstrate
source bias, concluding that these models prioritize documents whose term
distributions closely correspond to those of the queries, rather than
displaying an inherent source bias. This work provides a foundation for
understanding and addressing potential biases in term-based IR systems managing
mixed-source content.

</details>


### [121] [HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation](https://arxiv.org/abs/2508.18118)
*Junyi Chen,Lu Chi,Siliang Xu,Shiwei Ran,Bingyue Peng,Zehuan Yuan*

Main category: cs.IR

TL;DR: 本文提出了一种分层LLM框架HLLM-Creator，用于高效的用户兴趣建模和个性化内容生成。通过用户聚类和基于用户-广告匹配预测的剪枝策略提高生成效率，并设计了一个基于思维链推理的数据构建管道，生成高质量、用户特定的创意标题。实验表明，该方法在抖音搜索广告的个性化标题生成任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC系统主要依赖创作者的灵感，很少生成真正个性化的用户内容。在实际应用中，如在线广告，一个产品可能有多个卖点，不同用户关注不同的特性，这凸显了以用户为中心的创造性生成的重要价值。

Method: HLLM-Creator是一个分层的LLM框架，用于高效的用户兴趣建模和个性化内容生成。在推理过程中，采用了用户聚类和基于用户-广告匹配预测的剪枝策略，以提高生成效率并减少计算开销。此外，设计了一个基于思维链推理的数据构建管道，生成高质量、用户特定的创意标题，并确保事实一致性。

Result: HLLM-Creator在抖音搜索广告的个性化标题生成任务中表现出色，线上A/B测试显示广告点击率提高了0.476%。

Conclusion: HLLM-Creator在抖音搜索广告的个性化标题生成任务中表现出色，线上A/B测试显示广告点击率提高了0.476%，为工业场景中的更高效和有效的个性化生成铺平了道路。

Abstract: AI-generated content technologies are widely used in content creation.
However, current AIGC systems rely heavily on creators' inspiration, rarely
generating truly user-personalized content. In real-world applications such as
online advertising, a single product may have multiple selling points, with
different users focusing on different features. This underscores the
significant value of personalized, user-centric creative generation. Effective
personalized content generation faces two main challenges: (1) accurately
modeling user interests and integrating them into the content generation
process while adhering to factual constraints, and (2) ensuring high efficiency
and scalability to handle the massive user base in industrial scenarios.
Additionally, the scarcity of personalized creative data in practice
complicates model training, making data construction another key hurdle. We
propose HLLM-Creator, a hierarchical LLM framework for efficient user interest
modeling and personalized content generation. During inference, a combination
of user clustering and a user-ad-matching-prediction based pruning strategy is
employed to significantly enhance generation efficiency and reduce
computational overhead, making the approach suitable for large-scale
deployment. Moreover, we design a data construction pipeline based on
chain-of-thought reasoning, which generates high-quality, user-specific
creative titles and ensures factual consistency despite limited personalized
data. This pipeline serves as a critical foundation for the effectiveness of
our model. Extensive experiments on personalized title generation for Douyin
Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a
0.476% increase on Adss, paving the way for more effective and efficient
personalized generation in industrial scenarios. Codes for academic dataset are
available at https://github.com/bytedance/HLLM.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [122] [Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications](https://arxiv.org/abs/2508.17753)
*Theresa Pekarek Rosin,Julia Gachot,Henri-Leon Kordt,Matthias Kerzel,Stefan Wermter*

Main category: cs.RO

TL;DR: 本文评估了四种最先进的自动语音识别（ASR）系统，并发现它们在性能、幻觉倾向和固有偏差方面存在显著差异，这可能对人机交互（HRI）产生严重影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究在现实世界设置中，ASR系统如何处理不完美的音频，并适应不同的用户群体，特别是在人机交互（HRI）中的挑战性环境。

Method: 本文评估了四种最先进的自动语音识别（ASR）系统，并使用八个公开数据集分析了六个难度维度：领域特定、口音、噪声、年龄变化、障碍和自发性语音。

Result: 本文的分析表明，尽管在标准基准测试中表现相似，但四种最先进的ASR系统在性能、幻觉倾向和固有偏差方面存在显著差异。

Conclusion: 本文指出，尽管在标准基准测试中表现相似，但四种最先进的ASR系统在性能、幻觉倾向和固有偏差方面存在显著差异，这对人机交互（HRI）具有严重的影响。

Abstract: Automatic Speech Recognition (ASR) systems in real-world settings need to
handle imperfect audio, often degraded by hardware limitations or environmental
noise, while accommodating diverse user groups. In human-robot interaction
(HRI), these challenges intersect to create a uniquely challenging recognition
environment. We evaluate four state-of-the-art ASR systems on eight publicly
available datasets that capture six dimensions of difficulty: domain-specific,
accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis
demonstrates significant variations in performance, hallucination tendencies,
and inherent biases, despite similar scores on standard benchmarks. These
limitations have serious implications for HRI, where recognition errors can
interfere with task performance, user trust, and safety.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [123] [Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech with Adapters](https://arxiv.org/abs/2508.18006)
*Alessio Falai,Ziyao Zhang,Akos Gangoly*

Main category: eess.AS

TL;DR: 本文研究了通过适配器进行跨语言文本到语音合成，比较了未见过的说话人和语言适应的任务，以在目标语言中合成目标语音。结果表明，适配器在学习语言特定和说话人特定信息方面是有效的，同时避免了原始模型的说话人或语言信息的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 研究如何在没有目标语音录音的情况下，在目标语言中合成目标语音，并评估适配器在学习语言特定和说话人特定信息方面的效果。

Method: 通过适配器进行跨语言文本到语音合成的研究，比较了未见过的说话人和语言适应的任务，以在目标语言中合成目标语音，而目标语音在此语言中没有录音。

Result: 客观评估结果表明，适配器在学习语言特定和说话人特定信息方面是有效的，同时避免了原始模型的说话人或语言信息的灾难性遗忘。此外，还提出并验证了一个基于第二语言学习者误读检测技术的客观度量标准，用于衡量生成语音的自然程度。

Conclusion: 适配器在学习语言特定和说话人特定信息方面是有效的，使预训练模型能够学习未见过的说话人身份或语言，同时避免了原始模型的说话人或语言信息的灾难性遗忘。

Abstract: In this paper we investigate cross-lingual Text-To-Speech (TTS) synthesis
through the lens of adapters, in the context of lightweight TTS systems. In
particular, we compare the tasks of unseen speaker and language adaptation with
the goal of synthesising a target voice in a target language, in which the
target voice has no recordings therein. Results from objective evaluations
demonstrate the effectiveness of adapters in learning language-specific and
speaker-specific information, allowing pre-trained models to learn unseen
speaker identities or languages, while avoiding catastrophic forgetting of the
original model's speaker or language information. Additionally, to measure how
native the generated voices are in terms of accent, we propose and validate an
objective metric inspired by mispronunciation detection techniques in
second-language (L2) learners. The paper also provides insights into the impact
of adapter placement, configuration and the number of speakers used.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [124] [RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System](https://arxiv.org/abs/2508.17590)
*Zui Chen,Han Li,Xinhao Zhang,Xiaoyu Chen,Chunyin Dong,Yifeng Wang,Xin Cai,Su Zhang,Ziqi Li,Chi Ding,Jinxu Li,Shuai Wang,Dousheng Zhao,Sanhai Gao,Guangyi Liu*

Main category: cs.DB

TL;DR: RubikSQL 是一种新的 NL2SQL 系统，旨在解决现实世界企业级 NL2SQL 中的关键挑战，如隐含意图和领域特定术语。它将 NL2SQL 视为一个终身学习任务，通过数据库分析、结构化信息提取、代理规则挖掘和增强的思维链 SQL 分析来构建和优化知识库，并利用多代理工作流生成准确的 SQL。RubikSQL 在 KaggleDBQA 和 BIRD Mini-Dev 数据集上达到了最先进的性能，并发布了 RubikBench 基准测试，以捕捉工业 NL2SQL 场景的重要特征，为未来的研究提供宝贵的资源。


<details>
  <summary>Details</summary>
Motivation: We present RubikSQL, a novel NL2SQL system designed to address key challenges in real-world enterprise-level NL2SQL, such as implicit intents and domain-specific terminology.

Method: RubikSQL frames NL2SQL as a lifelong learning task, demanding both Knowledge Base (KB) maintenance and SQL generation. RubikSQL systematically builds and refines its KB through techniques including database profiling, structured information extraction, agentic rule mining, and Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a multi-agent workflow to leverage this curated KB, generating accurate SQLs.

Result: RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev datasets.

Conclusion: RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev datasets. Finally, we release the RubikBench benchmark, a new benchmark specifically designed to capture vital traits of industrial NL2SQL scenarios, providing a valuable resource for future research.

Abstract: We present RubikSQL, a novel NL2SQL system designed to address key challenges
in real-world enterprise-level NL2SQL, such as implicit intents and
domain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning
task, demanding both Knowledge Base (KB) maintenance and SQL generation.
RubikSQL systematically builds and refines its KB through techniques including
database profiling, structured information extraction, agentic rule mining, and
Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a
multi-agent workflow to leverage this curated KB, generating accurate SQLs.
RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev
datasets. Finally, we release the RubikBench benchmark, a new benchmark
specifically designed to capture vital traits of industrial NL2SQL scenarios,
providing a valuable resource for future research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [125] [MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation](https://arxiv.org/abs/2508.16674)
*Fangxin Shang,Yuan Xia,Dalu Yang,Yahui Wang,Binglin Yang*

Main category: cs.CV

TL;DR: 本文介绍了MedRepBench，一个全面的基准，用于评估医学报告的结构化解释质量。我们设计了一个奖励函数，并应用了Group Relative Policy Optimization (GRPO)来改进一个中等规模的VLM，实现了高达6%的召回率提升。我们观察到OCR+LLM管道虽然表现良好，但存在布局盲点和延迟问题，这促使进一步进步朝着稳健的全视觉报告理解方向发展。


<details>
  <summary>Details</summary>
Motivation: 医学报告解释在医疗保健中起着至关重要的作用，使患者面对的解释和有效的信息流 across 临床系统成为可能。尽管最近的视觉-语言模型(VLMs)和大型语言模型(LLMs)展示了通用文档理解能力，但仍缺乏标准化的基准来评估医学报告中的结构化解释质量。

Method: 我们设计了一个奖励函数，并应用了Group Relative Policy Optimization (GRPO)来改进一个中等规模的VLM，实现了高达6%的召回率提升。我们的评估框架支持两种互补的协议：(1) 一种客观评估，测量结构化临床项目的字段级召回率，以及(2) 一种自动主观评估，使用强大的LLM作为评分代理来评估事实性、可解释性和推理质量。

Result: 我们引入了MedRepBench，这是一个全面的基准，由1,900个去识别的真实世界中文医学报告组成，涵盖了不同的部门、患者人口统计和获取格式。该基准主要用于评估端到端的VLM用于结构化医学报告理解。为了实现受控比较，我们还包含了一个使用高质量OCR输出与LLMs结合的文本-only评估设置，使我们能够在字符识别错误最小化时估计上限性能。

Conclusion: 我们观察到OCR+LLM管道虽然表现良好，但存在布局盲点和延迟问题，这促使进一步进步朝着稳健的全视觉报告理解方向发展。

Abstract: Medical report interpretation plays a crucial role in healthcare, enabling
both patient-facing explanations and effective information flow across clinical
systems. While recent vision-language models (VLMs) and large language models
(LLMs) have demonstrated general document understanding capabilities, there
remains a lack of standardized benchmarks to assess structured interpretation
quality in medical reports. We introduce MedRepBench, a comprehensive benchmark
built from 1,900 de-identified real-world Chinese medical reports spanning
diverse departments, patient demographics, and acquisition formats. The
benchmark is designed primarily to evaluate end-to-end VLMs for structured
medical report understanding. To enable controlled comparisons, we also include
a text-only evaluation setting using high-quality OCR outputs combined with
LLMs, allowing us to estimate the upper-bound performance when character
recognition errors are minimized. Our evaluation framework supports two
complementary protocols: (1) an objective evaluation measuring field-level
recall of structured clinical items, and (2) an automated subjective evaluation
using a powerful LLM as a scoring agent to assess factuality, interpretability,
and reasoning quality. Based on the objective metric, we further design a
reward function and apply Group Relative Policy Optimization (GRPO) to improve
a mid-scale VLM, achieving up to 6% recall gain. We also observe that the
OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and
latency issues, motivating further progress toward robust, fully vision-based
report understanding.

</details>


### [126] [Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding](https://arxiv.org/abs/2508.17205)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于专家混合策略的多智能体框架，用于全面的高速公路场景理解。该框架利用大型通用视觉语言模型生成特定任务的思维链提示，以指导小型高效模型进行推理。它同时处理多个关键感知任务，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决高速公路场景理解中的多个关键感知任务，包括天气分类、路面湿滑评估和交通拥堵检测，同时平衡准确性和计算效率。

Method: 该论文引入了一个多智能体框架，围绕专家混合策略设计。一个大型通用视觉语言模型（VLM）如GPT-4o被赋予领域知识以生成特定任务的思维链（CoT）提示。这些细粒度的提示用于指导较小的高效VLM（例如Qwen2.5-VL-7B）在短视频上的推理，并结合互补模态。

Result: 实验结果表明，在多样化的交通和环境条件下，该框架表现出色。特别是路面湿滑数据集是多模态的，结合了视频流和道路天气传感器数据，突显了多模态推理的优势。

Conclusion: 该框架可以轻松集成到现有的交通摄像头系统中，并战略性地应用于高风险农村地区，如急转弯、易淹低地或结冰桥梁。通过持续监控目标地点，该系统提高了情境意识并提供了及时的警报，即使在资源受限的环境中也能发挥作用。

Abstract: This paper introduces a multi-agent framework for comprehensive highway scene
understanding, designed around a mixture-of-experts strategy. In this
framework, a large generic vision-language model (VLM), such as GPT-4o, is
contextualized with domain knowledge to generates task-specific
chain-of-thought (CoT) prompts. These fine-grained prompts are then used to
guide a smaller, efficient VLM (e.g., Qwen2.5-VL-7B) in reasoning over short
videos, along with complementary modalities as applicable. The framework
simultaneously addresses multiple critical perception tasks, including weather
classification, pavement wetness assessment, and traffic congestion detection,
achieving robust multi-task reasoning while balancing accuracy and
computational efficiency. To support empirical validation, we curated three
specialized datasets aligned with these tasks. Notably, the pavement wetness
dataset is multimodal, combining video streams with road weather sensor data,
highlighting the benefits of multimodal reasoning. Experimental results
demonstrate consistently strong performance across diverse traffic and
environmental conditions. From a deployment perspective, the framework can be
readily integrated with existing traffic camera systems and strategically
applied to high-risk rural locations, such as sharp curves, flood-prone
lowlands, or icy bridges. By continuously monitoring the targeted sites, the
system enhances situational awareness and delivers timely alerts, even in
resource-constrained environments.

</details>


### [127] [CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.17243)
*Zicong Tang,Ziyang Ma,Suqing Wang,Zuchao Li,Lefei Zhang,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 CoViPAL 的视觉标记剪枝方法，能够在不牺牲准确性的前提下提高大型视觉语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 由于单个图像可以生成数千个视觉标记，导致预填充阶段的高计算成本和解码阶段的显著内存开销，因此需要一种有效的剪枝方法。现有的方法在浅层中表现不佳，因为缺乏足够的上下文信息。

Method: 我们提出了 CoViPAL，这是一种逐层上下文感知的视觉标记剪枝方法，采用插件式剪枝模块 (PPM) 在视觉标记被 LVLM 处理之前预测并移除冗余的视觉标记。

Result: 在多个基准测试中进行的广泛实验表明，CoViPAL 在相等的标记预算下优于无训练剪枝方法，并且在具有可比监督的情况下超越了基于训练的方法。

Conclusion: CoViPAL 提供了一种可扩展且高效的解决方案，以在不牺牲准确性的情况下提高 LVLM 的推理效率。

Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of
text tokens and vision tokens extracted from images or videos. Due to the rich
visual information, a single image can generate thousands of vision tokens,
leading to high computational costs during the prefilling stage and significant
memory overhead during decoding. Existing methods attempt to prune redundant
vision tokens, revealing substantial redundancy in visual representations.
However, these methods often struggle in shallow layers due to the lack of
sufficient contextual information. We argue that many visual tokens are
inherently redundant even in shallow layers and can be safely and effectively
pruned with appropriate contextual signals. In this work, we propose CoViPAL, a
layer-wise contextualized visual token pruning method that employs a
Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision
tokens before they are processed by the LVLM. The PPM is lightweight,
model-agnostic, and operates independently of the LVLM architecture, ensuring
seamless integration with various models. Extensive experiments on multiple
benchmarks demonstrate that CoViPAL outperforms training-free pruning methods
under equal token budgets and surpasses training-based methods with comparable
supervision. CoViPAL offers a scalable and efficient solution to improve
inference efficiency in LVLMs without compromising accuracy.

</details>


### [128] [Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs](https://arxiv.org/abs/2508.17334)
*Somraj Gautam,Abhirama Subramanyam Penamakuri,Abhishek Bhandari,Gaurav Harit*

Main category: cs.CV

TL;DR: 介绍了一个新的基准测试MMCRICBENCH-3K，用于评估大型视觉-语言模型在复杂数值和跨语言推理方面的能力，并揭示了这些模型在结构感知、数值推理和跨语言泛化方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型视觉-语言模型在复杂数值和跨语言推理方面的能力，特别是针对板球得分卡的半结构化表格图像。

Method: 引入MMCRICBENCH-3K，一个用于评估大型视觉-语言模型（LVLMs）在复杂数值和跨语言推理方面的基准测试。

Result: 即使最先进的LVLMs，如GPT-4o和Qwen2.5VL，在英语子集上也表现不佳，而在印地语子集上的表现进一步下降。

Conclusion: 该数据集揭示了在结构感知的视觉文本理解、数值推理和跨语言泛化方面的关键局限性，并公开提供以促进相关研究。

Abstract: We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)
on cricket scorecards, designed to evaluate large vision-language models
(LVLMs) on complex numerical and cross-lingual reasoning over semi-structured
tabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated
scorecard images from ODI, T20, and Test formats, accompanied by 1,500 English
QA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English
scorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi
scorecards, with all questions and answers kept in English to enable controlled
cross-script evaluation. The task demands reasoning over structured numerical
data, multi-image context, and implicit domain knowledge. Empirical results
show that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle
on the English subset despite it being their primary training language and
exhibit a further drop in performance on the Hindi subset. This reveals key
limitations in structure-aware visual text understanding, numerical reasoning,
and cross-lingual generalization. The dataset is publicly available via Hugging
Face at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM
research in this direction.

</details>


### [129] [Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning](https://arxiv.org/abs/2508.17638)
*Xinyu Wei,Guoli Yang,Jialu Zhou,Mingyue Yang,Leqian Li,Kedi Zhang,Chunping Qiu*

Main category: cs.CV

TL;DR: DEHVF is a new method for vision-language fine-tuning that uses hierarchical visual features to improve accuracy without increasing computational overhead.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing methods that neglect the hierarchical semantic representations within the model and the fine-grained visual information available in the shallower visual encoding layers.

Method: DEHVF, an efficient vision-language fine-tuning method based on dynamic embedding and fusion of hierarchical visual features.

Result: DEHVF achieves higher accuracy than existing PEFT baselines while maintaining efficient training and inference.

Conclusion: DEHVF achieves higher accuracy than existing parameter-efficient fine-tuning (PEFT) baselines while maintaining efficient training and inference.

Abstract: Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects
visual features and then concatenates them with text tokens to form a unified
sequence input for Large Language Models (LLMs). However, this paradigm leads
to a significant increase in the length of the input sequence, resulting in
substantial computational overhead. Existing methods attempt to fuse visual
information into the intermediate layers of LLMs, which alleviate the sequence
length issue but often neglect the hierarchical semantic representations within
the model and the fine-grained visual information available in the shallower
visual encoding layers. To address this limitation, we propose DEHVF, an
efficient vision-language fine-tuning method based on dynamic embedding and
fusion of hierarchical visual features. Its core lies in leveraging the
inherent hierarchical representation characteristics of visual encoders and
language models. Through a lightweight hierarchical visual fuser, it
dynamically selects and fuses hierarchical features corresponding to semantic
granularity based on the internal representations of each layer in LLMs. The
fused layer-related visual features are then projected and aligned before being
directly embedded into the Feed-Forward Network (FFN) of the corresponding
layer in LLMs. This approach not only avoids sequence expansion but also
dynamically fuses multi-layer visual information. By fine-tuning only a small
number of parameters, DEHVF achieves precise alignment and complementarity of
cross-modal information at the same semantic granularity. We conducted
experiments across various VL benchmarks, including visual question answering
on ScienceQA and image captioning on COCO Captions. The results demonstrate
that DEHVF achieves higher accuracy than existing parameter-efficient
fine-tuning (PEFT) baselines while maintaining efficient training and
inference.

</details>


### [130] [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation](https://arxiv.org/abs/2508.17760)
*Mingyue Yang,Dianxi Shi,Jialu Zhou,Xinyu Wei,Leqian Li,Shaowu Yang,Chunping Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的图像生成方法CEIDM，通过双控机制实现对实体及其交互的有效控制，从而生成更高质量的图像。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像（T2I）生成中，实体的复杂性和它们的复杂交互对基于扩散模型的T2I方法构成了重大挑战：如何有效控制实体及其交互以生成高质量的图像。

Method: 我们提出了CEIDM，这是一种基于扩散模型的图像生成方法，具有实体和交互的双重控制。首先，我们提出了一种基于大型语言模型（LLMs）的实体交互关系挖掘方法，通过思维链提取合理且丰富的隐式交互关系，以指导扩散模型生成更接近现实逻辑且具有更多合理交互关系的高质量图像。此外，我们提出了一种交互动作聚类和偏移方法，对每个文本提示中包含的交互动作特征进行聚类和偏移。通过构建全局和局部双向偏移，我们增强了原始动作的语义理解和细节补充，使模型对交互“动作”概念的理解更加准确，并生成具有更准确交互动作的图像。最后，我们设计了一个实体控制网络，该网络在实体语义引导下生成掩码，然后利用多尺度卷积网络增强实体特征，并利用动态网络融合特征。这有效地控制了实体，并显著提高了图像质量。

Result: 实验表明，所提出的CEIDM方法在实体控制和其交互控制方面都优于现有的最具有代表性的方法。

Conclusion: 实验表明，所提出的CEIDM方法在实体控制和其交互控制方面都优于现有的最具有代表性的方法。

Abstract: In Text-to-Image (T2I) generation, the complexity of entities and their
intricate interactions pose a significant challenge for T2I method based on
diffusion model: how to effectively control entity and their interactions to
produce high-quality images. To address this, we propose CEIDM, a image
generation method based on diffusion model with dual controls for entity and
interaction. First, we propose an entity interactive relationships mining
approach based on Large Language Models (LLMs), extracting reasonable and rich
implicit interactive relationships through chain of thought to guide diffusion
models to generate high-quality images that are closer to realistic logic and
have more reasonable interactive relationships. Furthermore, We propose an
interactive action clustering and offset method to cluster and offset the
interactive action features contained in each text prompts. By constructing
global and local bidirectional offsets, we enhance semantic understanding and
detail supplementation of original actions, making the model's understanding of
the concept of interactive "actions" more accurate and generating images with
more accurate interactive actions. Finally, we design an entity control network
which generates masks with entity semantic guidance, then leveraging
multi-scale convolutional network to enhance entity feature and dynamic network
to fuse feature. It effectively controls entities and significantly improves
image quality. Experiments show that the proposed CEIDM method is better than
the most representative existing methods in both entity control and their
interaction control.

</details>


### [131] [Designing Practical Models for Isolated Word Visual Speech Recognition](https://arxiv.org/abs/2508.17894)
*Iason Ioannis Panagos,Giorgos Sfikas,Christophoros Nikou*

Main category: cs.CV

TL;DR: 本文旨在通过开发低硬件成本的视觉语音识别（VSR）架构来缓解现有系统计算成本高的问题，并展示了其在实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的VSR系统通常依赖于深度神经网络来提取输入序列中的有意义表示，但这些模型会带来显著的计算成本，导致硬件需求增加，在资源受限的实际场景中应用有限。

Method: 本文首先基准测试了来自图像分类文献中的高效模型，然后在时间卷积网络主干中采用了轻量级块设计，开发了轻量级端到端架构。

Result: 本文开发的模型具有低资源需求但强大的识别性能，并在英语单词的最大公共数据库上验证了其有效性。

Conclusion: 本文开发的低硬件成本的VSR架构在英语单词的最大公共数据库上进行了实验，证明了其有效性和实用性。代码和训练好的模型将公开可用。

Abstract: Visual speech recognition (VSR) systems decode spoken words from an input
sequence using only the video data. Practical applications of such systems
include medical assistance as well as human-machine interactions. A VSR system
is typically employed in a complementary role in cases where the audio is
corrupt or not available. In order to accurately predict the spoken words,
these architectures often rely on deep neural networks in order to extract
meaningful representations from the input sequence. While deep architectures
achieve impressive recognition performance, relying on such models incurs
significant computation costs which translates into increased resource demands
in terms of hardware requirements and results in limited applicability in
real-world scenarios where resources might be constrained. This factor prevents
wider adoption and deployment of speech recognition systems in more practical
applications. In this work, we aim to alleviate this issue by developing
architectures for VSR that have low hardware costs. Following the standard
two-network design paradigm, where one network handles visual feature
extraction and another one utilizes the extracted features to classify the
entire sequence, we develop lightweight end-to-end architectures by first
benchmarking efficient models from the image classification literature, and
then adopting lightweight block designs in a temporal convolution network
backbone. We create several unified models with low resource requirements but
strong recognition performance. Experiments on the largest public database for
English words demonstrate the effectiveness and practicality of our developed
models. Code and trained models will be made publicly available.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [132] [Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models](https://arxiv.org/abs/2508.16765)
*GodsGift Uzor,Hasan Al-Qudah,Ynes Ineza,Abdul Serwadda*

Main category: cs.CR

TL;DR: 本文提出了一个名为“LLM网关”的概念，这是一种轻量级、本地运行的模型，用于在用户查询发送到可能不可信但功能强大的基于云的LLM之前，过滤掉敏感信息。通过实验，我们证明了这种双模型方法引入的开销最小，同时显著增强了用户隐私，而不会损害LLM响应的质量。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的交互性质，用户以前所未有的方式分享个人和私人信息。即使用户选择不允许使用其数据进行训练，这些隐私设置在LLM提供商位于隐私法薄弱、政府监视或数据安全措施差的司法管辖区时，提供的保护有限。

Method: 提出了一种名为“LLM网关”的概念，这是一种轻量级、本地运行的模型，在用户查询发送到可能不可信但功能强大的基于云的LLM之前，过滤掉敏感信息。

Result: 通过人类受试者的实验，我们证明了这种双模型方法引入的开销最小，同时显著增强了用户隐私，而不会损害LLM响应的质量。

Conclusion: 通过实验，我们证明了这种双模型方法引入的开销最小，同时显著增强了用户隐私，而不会损害LLM响应的质量。

Abstract: The interactive nature of Large Language Models (LLMs), which closely track
user data and context, has prompted users to share personal and private
information in unprecedented ways. Even when users opt out of allowing their
data to be used for training, these privacy settings offer limited protection
when LLM providers operate in jurisdictions with weak privacy laws, invasive
government surveillance, or poor data security practices. In such cases, the
risk of sensitive information, including Personally Identifiable Information
(PII), being mishandled or exposed remains high. To address this, we propose
the concept of an "LLM gatekeeper", a lightweight, locally run model that
filters out sensitive information from user queries before they are sent to the
potentially untrustworthy, though highly capable, cloud-based LLM. Through
experiments with human subjects, we demonstrate that this dual-model approach
introduces minimal overhead while significantly enhancing user privacy, without
compromising the quality of LLM responses.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [133] [Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications](https://arxiv.org/abs/2508.16681)
*Eric Zhang*

Main category: cs.AI

TL;DR: 本文分析了基于规则的口吃检测系统，提出了一种增强的框架，并展示了其在临床应用中的优势。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析基于规则的口吃检测系统，并探索其在临床应用中的优势。

Method: 本文提出了一个增强的基于规则的框架，结合了语速归一化、多级声学特征分析和分层决策结构。

Result: 本文的方法在保持完全可解释性的同时实现了具有竞争力的性能，并在延长检测方面表现出色（97-99%的准确率）。

Conclusion: 本文表明，尽管神经方法在不受约束的环境中可能达到略高的准确性，但基于规则的方法在临床环境中具有独特的优势，包括决策可审计性、患者特定调整和实时反馈。

Abstract: Stuttering affects approximately 1% of the global population, impacting
communication and quality of life. While recent advances in deep learning have
pushed the boundaries of automatic speech dysfluency detection, rule-based
approaches remain crucial for clinical applications where interpretability and
transparency are paramount. This paper presents a comprehensive analysis of
rule-based stuttering detection systems, synthesizing insights from multiple
corpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced
rule-based framework that incorporates speaking-rate normalization, multi-level
acoustic feature analysis, and hierarchical decision structures. Our approach
achieves competitive performance while maintaining complete
interpretability-critical for clinical adoption. We demonstrate that rule-based
systems excel particularly in prolongation detection (97-99% accuracy) and
provide stable performance across varying speaking rates. Furthermore, we show
how these interpretable models can be integrated with modern machine learning
pipelines as proposal generators or constraint modules, bridging the gap
between traditional speech pathology practices and contemporary AI systems. Our
analysis reveals that while neural approaches may achieve marginally higher
accuracy in unconstrained settings, rule-based methods offer unique advantages
in clinical contexts where decision auditability, patient-specific tuning, and
real-time feedback are essential.

</details>


### [134] [Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs](https://arxiv.org/abs/2508.16846)
*Katherine Atwell,Pedram Heydari,Anthony Sicilia,Malihe Alikhani*

Main category: cs.AI

TL;DR: This paper explores sycophancy in large language models using a Bayesian framework, finding that LLMs are not Bayesian rational and that sycophancy can lead to increased Bayesian error.


<details>
  <summary>Details</summary>
Motivation: To understand sycophancy in LLMs and its impact on human/AI collaboration, as prior metrics fail to characterize shifts in rationality and accuracy measures can only be used in scenarios with a known ground truth.

Method: Utilizing a Bayesian framework to quantify sycophancy as deviations from rational behavior when presented with user perspectives, studying sycophancy for 3 different tasks, a combination of open-source and closed LLMs, and two different methods for probing sycophancy, and experimenting with multiple methods for eliciting probability judgments from LLMs.

Result: LLMs are not Bayesian rational, probing for sycophancy results in significant increases to the predicted posterior in favor of the steered outcome, sycophancy sometimes results in increased Bayesian error, and changes in Bayesian error due to sycophancy are not strongly correlated in Brier score.

Conclusion: LLMs are not Bayesian rational, probing for sycophancy results in significant increases to the predicted posterior in favor of the steered outcome, sycophancy sometimes results in increased Bayesian error, and changes in Bayesian error due to sycophancy are not strongly correlated in Brier score.

Abstract: Sycophancy, or overly agreeable or flattering behavior, is a documented issue
in large language models (LLMs), and is critical to understand in the context
of human/AI collaboration. Prior works typically quantify sycophancy by
measuring shifts in behavior or impacts on accuracy, but neither metric
characterizes shifts in rationality, and accuracy measures can only be used in
scenarios with a known ground truth. In this work, we utilize a Bayesian
framework to quantify sycophancy as deviations from rational behavior when
presented with user perspectives, thus distinguishing between rational and
irrational updates based on the introduction of user perspectives. In
comparison to other methods, this approach allows us to characterize excessive
behavioral shifts, even for tasks that involve inherent uncertainty or do not
have a ground truth. We study sycophancy for 3 different tasks, a combination
of open-source and closed LLMs, and two different methods for probing
sycophancy. We also experiment with multiple methods for eliciting probability
judgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause
deviations in LLMs' predicted posteriors that will lead to increased Bayesian
error. Our findings indicate that: 1) LLMs are not Bayesian rational, 2)
probing for sycophancy results in significant increases to the predicted
posterior in favor of the steered outcome, 3) sycophancy sometimes results in
increased Bayesian error, and in a small number of cases actually decreases
error, and 4) changes in Bayesian error due to sycophancy are not strongly
correlated in Brier score, suggesting that studying the impact of sycophancy on
ground truth alone does not fully capture errors in reasoning due to
sycophancy.

</details>


### [135] [Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets](https://arxiv.org/abs/2508.17391)
*Nikolaos Pavlidis,Vasilis Perifanis,Symeon Symeonidis,Pavlos S. Efraimidis*

Main category: cs.AI

TL;DR: This paper evaluates the performance of large language models (LLMs) on structured data tasks like classification, regression, and clustering. While LLMs excel in classification, they struggle with regression and clustering, highlighting their strengths and limitations as general-purpose predictive engines.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of LLMs in performing predictive tasks on structured data without explicit fine-tuning, and to understand their strengths and limitations compared to traditional ML models.

Method: We investigate the empirical function approximation capability of LLMs on small-scale structured datasets for classification, regression, and clustering tasks. We evaluate the performance of state-of-the-art LLMs under few-shot prompting and compare them against established ML baselines.

Result: LLMs achieve strong performance in classification tasks under limited data availability, establishing practical zero-training baselines. However, their performance in regression and clustering is poor compared to ML models.

Conclusion: LLMs can serve as general-purpose predictive engines for structured data, with clear strengths in classification and significant limitations in regression and clustering.

Abstract: Large Language Models (LLMs), originally developed for natural language
processing (NLP), have demonstrated the potential to generalize across
modalities and domains. With their in-context learning (ICL) capabilities, LLMs
can perform predictive tasks over structured inputs without explicit
fine-tuning on downstream tasks. In this work, we investigate the empirical
function approximation capability of LLMs on small-scale structured datasets
for classification, regression and clustering tasks. We evaluate the
performance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash,
DeepSeek-R1) under few-shot prompting and compare them against established
machine learning (ML) baselines, including linear models, ensemble methods and
tabular foundation models (TFMs). Our results show that LLMs achieve strong
performance in classification tasks under limited data availability,
establishing practical zero-training baselines. In contrast, the performance in
regression with continuous-valued outputs is poor compared to ML models, likely
because regression demands outputs in a large (often infinite) space, and
clustering results are similarly limited, which we attribute to the absence of
genuine ICL in this setting. Nonetheless, this approach enables rapid,
low-overhead data exploration and offers a viable alternative to traditional ML
pipelines in business intelligence and exploratory analytics contexts. We
further analyze the influence of context size and prompt structure on
approximation quality, identifying trade-offs that affect predictive
performance. Our findings suggest that LLMs can serve as general-purpose
predictive engines for structured data, with clear strengths in classification
and significant limitations in regression and clustering.

</details>


### [136] [LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios](https://arxiv.org/abs/2508.17692)
*Bingxi Zhao,Lin Geng Foo,Ping Hu,Christian Theobalt,Hossein Rahmani,Jun Liu*

Main category: cs.AI

TL;DR: 本文对基于大语言模型的代理推理框架进行了系统化的分类和分析，旨在为研究社区提供一个全面的视角，以理解不同框架的优势、适用场景和评估实践。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型的代理系统在各种自动化任务中表现出接近人类的性能，但不同的推理框架在组织推理过程方面有所不同。因此，需要一种系统化的分类法来分析这些框架，并帮助研究社区更好地理解它们的优缺点。

Method: 本文提出了一种系统化的分类法，将代理推理框架分解，并通过比较它们在不同场景中的应用来分析这些框架如何主导框架级别的推理。此外，还提出了一个统一的形式语言，进一步将代理推理系统分类为单代理方法、基于工具的方法和多代理方法。

Result: 本文提供了对代理推理框架在科学发现、医疗保健、软件工程、社会模拟和经济学等关键应用场景的全面回顾，并分析了每种框架的特征，总结了不同的评估策略。

Conclusion: 本文的结论是，通过系统地分类和分析代理推理框架，可以为研究社区提供一个全面的视角，以更好地理解不同代理推理框架的优势、适用场景和评估实践。

Abstract: Recent advances in the intrinsic reasoning capabilities of large language
models (LLMs) have given rise to LLM-based agent systems that exhibit
near-human performance on a variety of automated tasks. However, although these
systems share similarities in terms of their use of LLMs, different reasoning
frameworks of the agent system steer and organize the reasoning process in
different ways. In this survey, we propose a systematic taxonomy that
decomposes agentic reasoning frameworks and analyze how these frameworks
dominate framework-level reasoning by comparing their applications across
different scenarios. Specifically, we propose an unified formal language to
further classify agentic reasoning systems into single-agent methods,
tool-based methods, and multi-agent methods. After that, we provide a
comprehensive review of their key application scenarios in scientific
discovery, healthcare, software engineering, social simulation, and economics.
We also analyze the characteristic features of each framework and summarize
different evaluation strategies. Our survey aims to provide the research
community with a panoramic view to facilitate understanding of the strengths,
suitable scenarios, and evaluation practices of different agentic reasoning
frameworks.

</details>


### [137] [The AI Data Scientist](https://arxiv.org/abs/2508.18113)
*Farkhad Akimov,Munachiso Samuel Nwadike,Zangir Iklassov,Martin Takáč*

Main category: cs.AI

TL;DR: AI Data Scientist 是一个基于大型语言模型的自主代理，能够快速提供可操作的数据分析见解，使深度数据分析更加易于访问和行动。


<details>
  <summary>Details</summary>
Motivation: 传统的工作流程在提供数据分析结果方面速度较慢，而 AI Data Scientist 旨在通过自动化和高效的方法来弥补证据与行动之间的差距。

Method: AI Data Scientist 由多个专门的 LLM 子代理组成，每个子代理负责不同的任务，如数据清洗、统计测试、验证和通俗语言沟通。这些子代理可以自行编写代码、推理因果关系，并在需要时识别额外数据的需求。

Result: AI Data Scientist 能够在几分钟内完成原本可能需要几天或几周的任务，从而实现一种新的交互方式，使深度数据分析更加 accessible 和 actionable。

Conclusion: AI Data Scientist 是一种能够快速提供可操作见解的自主代理，它使深度数据分析既易于访问又具有行动性。

Abstract: Imagine decision-makers uploading data and, within minutes, receiving clear,
actionable insights delivered straight to their fingertips. That is the promise
of the AI Data Scientist, an autonomous Agent powered by large language models
(LLMs) that closes the gap between evidence and action. Rather than simply
writing code or responding to prompts, it reasons through questions, tests
ideas, and delivers end-to-end insights at a pace far beyond traditional
workflows. Guided by the scientific tenet of the hypothesis, this Agent
uncovers explanatory patterns in data, evaluates their statistical
significance, and uses them to inform predictive modeling. It then translates
these results into recommendations that are both rigorous and accessible. At
the core of the AI Data Scientist is a team of specialized LLM Subagents, each
responsible for a distinct task such as data cleaning, statistical testing,
validation, and plain-language communication. These Subagents write their own
code, reason about causality, and identify when additional data is needed to
support sound conclusions. Together, they achieve in minutes what might
otherwise take days or weeks, enabling a new kind of interaction that makes
deep data science both accessible and actionable.

</details>


### [138] [Unraveling the cognitive patterns of Large Language Models through module communities](https://arxiv.org/abs/2508.18192)
*Kushal Raj Bhandari,Pin-Yu Chen,Jianxi Gao*

Main category: cs.AI

TL;DR: 本文提出了一种基于网络的框架，将认知技能、LLM架构和数据集联系起来，以更好地理解大型语言模型的内部机制。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的内部架构和认知过程难以理解，因此需要一种新的方法来分析基础模型。

Method: 采用生物学中新兴认知的方法，开发了一个基于网络的框架，将认知技能、LLM架构和数据集联系起来。

Result: 模块社区中的技能分布表明，虽然LLM不严格遵循特定生物系统的焦点专业化，但它们表现出独特的模块社区，其涌现的技能模式部分反映了鸟类和小型哺乳动物大脑中的分布式且相互关联的认知组织。

Conclusion: 我们的框架为理解大型语言模型提供了新的见解，并建议有效的微调策略应利用分布式学习动力学而非刚性模块干预。

Abstract: Large Language Models (LLMs) have reshaped our world with significant
advancements in science, engineering, and society through applications ranging
from scientific discoveries and medical diagnostics to Chatbots. Despite their
ubiquity and utility, the underlying mechanisms of LLM remain concealed within
billions of parameters and complex structures, making their inner architecture
and cognitive processes challenging to comprehend. We address this gap by
adopting approaches to understanding emerging cognition in biology and
developing a network-based framework that links cognitive skills, LLM
architectures, and datasets, ushering in a paradigm shift in foundation model
analysis. The skill distribution in the module communities demonstrates that
while LLMs do not strictly parallel the focalized specialization observed in
specific biological systems, they exhibit unique communities of modules whose
emergent skill patterns partially mirror the distributed yet interconnected
cognitive organization seen in avian and small mammalian brains. Our numerical
results highlight a key divergence from biological systems to LLMs, where skill
acquisition benefits substantially from dynamic, cross-regional interactions
and neural plasticity. By integrating cognitive science principles with machine
learning, our framework provides new insights into LLM interpretability and
suggests that effective fine-tuning strategies should leverage distributed
learning dynamics rather than rigid modular interventions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [139] [Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework](https://arxiv.org/abs/2508.16629)
*Zeyu Zhang,Quanyu Dai,Rui Li,Xiaohe Bo,Xu Chen,Zhenhua Dong*

Main category: cs.LG

TL;DR: 本文提出了一种自适应和数据驱动的记忆框架，通过建模记忆周期来优化LLM代理。


<details>
  <summary>Details</summary>
Motivation: 之前的LLM代理的记忆机制是由人类专家手动预定义的，导致更高的劳动力成本和次优性能。此外，这些方法忽略了交互场景中的记忆周期效应，这对优化LLM代理以适应特定环境至关重要。

Method: 我们设计了一个MoE门函数来促进记忆检索，提出了一种可学习的聚合过程来提高记忆利用率，并开发了任务特定的反思以适应记忆存储。

Result: 我们进行了全面的实验，以评估所提出方法的有效性。

Conclusion: 我们的记忆框架使LLM代理能够在特定环境中有效地学习如何记忆信息，同时支持离策略和在策略优化。

Abstract: LLM-based agents have been extensively applied across various domains, where
memory stands out as one of their most essential capabilities. Previous memory
mechanisms of LLM-based agents are manually predefined by human experts,
leading to higher labor costs and suboptimal performance. In addition, these
methods overlook the memory cycle effect in interactive scenarios, which is
critical to optimizing LLM-based agents for specific environments. To address
these challenges, in this paper, we propose to optimize LLM-based agents with
an adaptive and data-driven memory framework by modeling memory cycles.
Specifically, we design an MoE gate function to facilitate memory retrieval,
propose a learnable aggregation process to improve memory utilization, and
develop task-specific reflection to adapt memory storage. Our memory framework
empowers LLM-based agents to learn how to memorize information effectively in
specific environments, with both off-policy and on-policy optimization. In
order to evaluate the effectiveness of our proposed methods, we conduct
comprehensive experiments across multiple aspects. To benefit the research
community in this area, we release our project at
https://github.com/nuster1128/learn_to_memorize.

</details>


### [140] [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Pingwei Sun,Feiye Huo,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Xiangyu Zhang,Maoxin He,Guangming Tan,Weile Jia,Tong Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为WISCA的权重缩放方法，通过改进神经网络的权重模式来提高训练效率和模型质量，无需改变网络结构。实验结果表明，该方法在多个架构上显著提升了收敛质量。


<details>
  <summary>Details</summary>
Motivation: 现有的训练优化方法主要集中在架构修改或优化器调整上，但缺乏对训练过程中权重模式的系统优化。

Method: 提出了一种称为WISCA的权重缩放方法，通过战略性地改进神经网络权重模式来提高训练效率和模型质量，而无需更改网络结构。

Result: 实验表明，WISCA在零样本验证任务上平均提高了5.6%，在多个架构上的训练困惑度平均降低了2.12%。

Conclusion: WISCA可以显著提高收敛质量，特别是在具有分组查询注意力（GQA）架构和LoRA微调任务的大型语言模型中。

Abstract: Transformer architecture gradually dominates the LLM field. Recent advances
in training optimization for Transformer-based large language models (LLMs)
primarily focus on architectural modifications or optimizer adjustments.
However, these approaches lack systematic optimization of weight patterns
during training. Weight pattern refers to the distribution and relative
magnitudes of weight parameters in a neural network. To address this issue, we
propose a Weight Scaling method called WISCA to enhance training efficiency and
model quality by strategically improving neural network weight patterns without
changing network structures. By rescaling weights while preserving model
outputs, WISCA indirectly optimizes the model's training trajectory.
Experiments demonstrate that WISCA significantly improves convergence quality
(measured by generalization capability and loss reduction), particularly in
LLMs with Grouped Query Attention (GQA) architectures and LoRA fine-tuning
tasks. Empirical results show 5.6% average improvement on zero-shot validation
tasks and 2.12% average reduction in training perplexity across multiple
architectures.

</details>


### [141] [Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration](https://arxiv.org/abs/2508.16677)
*Zhong Guan,Likang Wu,Hongke Zhao,Jiahui Wang,Le Wu*

Main category: cs.LG

TL;DR: 本文提出RED方法，通过受控探索和精炼的离线集成来增强小语言模型的推理能力。该方法平衡了离线蒸馏与在线强化学习，并针对离线数据插入问题进行了优化。通过熵变化比率调节模型权重，并设计了基于样本准确率的策略转移机制，有效解决了小模型探索空间不足和分布差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在通过强化学习与可验证奖励（RLVR）提高大型语言模型（LLMs）的推理能力，而对小型语言模型（SLMs）的推理能力提升尚未充分探索。结合大模型的蒸馏数据与SLMs自身的RLVR是一种自然的方法，但仍面临诸多挑战。因此，本文旨在解决这些挑战，提升SLMs的推理能力。

Method: 本文提出了一种名为RED的方法，通过受控探索和精炼的离线集成来增强小语言模型。该方法通过平衡离线蒸馏与在线强化学习，并设计了针对离线数据插入问题的优化方案，同时引入了基于熵变化比率的模型权重调节机制，以及基于样本准确率的策略转移机制。

Result: 本文提出的RED方法在增强小语言模型的推理能力方面取得了显著效果。通过平衡离线蒸馏与在线强化学习，解决了小模型探索空间不足和蒸馏过程中的冗余与复杂性问题。同时，基于样本准确率的策略转移机制有效缓解了离线数据与当前策略之间的分布差异。

Conclusion: 本文提出了一种名为RED的方法，通过受控探索和精炼的离线集成来增强小语言模型。该方法通过平衡离线蒸馏与在线强化学习，并设计了针对离线数据插入问题的优化方案，解决了小模型探索空间不足和蒸馏过程中的冗余与复杂性问题。此外，还设计了一个基于样本准确率的策略转移机制，以应对离线数据与当前策略之间的分布差异。

Abstract: Many existing studies have achieved significant improvements in the reasoning
capabilities of large language models (LLMs) through reinforcement learning
with verifiable rewards (RLVR), while the enhancement of reasoning abilities in
small language models (SLMs) has not yet been sufficiently explored. Combining
distilled data from larger models with RLVR on small models themselves is a
natural approach, but it still faces various challenges and issues. Therefore,
we propose \textit{\underline{R}}ecall-\textit{\underline{E}}xtend
\textit{\underline{D}}ynamics(RED): Enhancing Small Language Models through
Controlled Exploration and Refined Offline Integration. In this paper, we
explore the perspective of varying exploration spaces, balancing offline
distillation with online reinforcement learning. Simultaneously, we
specifically design and optimize for the insertion problem within offline data.
By monitoring the ratio of entropy changes in the model concerning offline and
online data, we regulate the weight of offline-SFT, thereby addressing the
issues of insufficient exploration space in small models and the redundancy and
complexity during the distillation process. Furthermore, to tackle the
distribution discrepancies between offline data and the current policy, we
design a sample-accuracy-based policy shift mechanism that dynamically chooses
between imitating offline distilled data and learning from its own policy.

</details>


### [142] [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
*ZeMing Gong,Chuanqi Tang,Xiaoliang Huo,Nicholas Pellegrino,Austin T. Wang,Graham W. Taylor,Angel X. Chang,Scott C. Lowe,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: 本文研究了双曲网络在生物多样性研究中的层次模型嵌入效果，并提出了一个基于双曲空间的多模态嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 我们研究双曲网络是否能为这种层次模型提供更好的嵌入空间。

Method: 我们的方法使用对比和基于堆叠蕴含的目标将多模态输入嵌入到共享的双曲空间中。

Result: 实验表明，双曲嵌入在与欧几里得基线竞争的同时，在使用DNA条形码的未见物种分类中优于所有其他模型。

Conclusion: 我们的框架为生物多样性建模提供了结构感知的基础，具有物种发现、生态监测和保护工作的潜力。

Abstract: Taxonomic classification in biodiversity research involves organizing
biological specimens into structured hierarchies based on evidence, which can
come from multiple modalities such as images and genetic information. We
investigate whether hyperbolic networks can provide a better embedding space
for such hierarchical models. Our method embeds multimodal inputs into a shared
hyperbolic space using contrastive and a novel stacked entailment-based
objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding
achieves competitive performance with Euclidean baselines, and outperforms all
other models on unseen species classification using DNA barcodes. However,
fine-grained classification and open-world generalization remain challenging.
Our framework offers a structure-aware foundation for biodiversity modelling,
with potential applications to species discovery, ecological monitoring, and
conservation efforts.

</details>


### [143] [Interpreting the Effects of Quantization on LLMs](https://arxiv.org/abs/2508.16785)
*Manpreet Singh,Hassan Sajjad*

Main category: cs.LG

TL;DR: 本研究通过多种可解释性技术分析了量化对大型语言模型中神经元行为的影响，发现量化对模型校准和神经元活性影响较小，且未观察到可能阻碍量化应用的剧烈变化。


<details>
  <summary>Details</summary>
Motivation: 量化为在资源受限环境中部署大型语言模型提供了一个实际的解决方案，但其对内部表示的影响尚未得到充分研究，这引发了对量化模型可靠性的疑问。

Method: 我们使用各种可解释性技术来研究量化如何影响模型和神经元行为，并分析了多个在4位和8位量化下的大型语言模型。

Result: 量化对模型校准的影响通常较小。神经元激活分析表明，无论是否量化，死亡神经元的数量保持一致。在神经元对预测的贡献方面，较小的全精度模型表现出更少的重要神经元，而较大的模型则更多，但Llama-2-7B除外。量化对神经元冗余的影响因模型而异。

Conclusion: 我们的研究结果表明，量化的影响可能因模型和任务而异，但没有观察到可能阻止使用量化的剧烈变化。

Abstract: Quantization offers a practical solution to deploy LLMs in
resource-constraint environments. However, its impact on internal
representations remains understudied, raising questions about the reliability
of quantized models. In this study, we employ a range of interpretability
techniques to investigate how quantization affects model and neuron behavior.
We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings
reveal that the impact of quantization on model calibration is generally minor.
Analysis of neuron activations indicates that the number of dead neurons, i.e.,
those with activation values close to 0 across the dataset, remains consistent
regardless of quantization. In terms of neuron contribution to predictions, we
observe that smaller full precision models exhibit fewer salient neurons,
whereas larger models tend to have more, with the exception of Llama-2-7B. The
effect of quantization on neuron redundancy varies across models. Overall, our
findings suggest that effect of quantization may vary by model and tasks,
however, we did not observe any drastic change which may discourage the use of
quantization as a reliable model compression technique.

</details>


### [144] [Attention Layers Add Into Low-Dimensional Residual Subspaces](https://arxiv.org/abs/2508.16929)
*Junxuan Wang,Xuyang Ge,Wentao Shu,Zhengfu He,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文揭示了注意力输出的低秩结构，并提出了一种子空间约束的训练方法，以减少稀疏字典学习中的死特征问题。


<details>
  <summary>Details</summary>
Motivation: 我们发现低秩结构是稀疏字典学习中普遍存在死特征问题的根本原因，这导致了随机初始化的特征与激活空间的内在几何之间的不匹配。

Method: 我们提出了一种子空间约束的训练方法，用于稀疏自编码器（SAEs），将特征方向初始化到激活的活动子空间中。

Result: 我们的方法将带有1M特征的Attention Output SAEs中的死特征从87%减少到1%以下，并可以进一步扩展到其他稀疏字典学习方法。

Conclusion: 我们的研究提供了对注意力几何的新见解，并为改进大型语言模型中的稀疏字典学习提供了实用工具。

Abstract: While transformer models are widely believed to operate in high-dimensional
hidden spaces, we show that attention outputs are confined to a surprisingly
low-dimensional subspace, where about 60\% of the directions account for 99\%
of the variance--a phenomenon that is induced by the attention output
projection matrix and consistently observed across diverse model families and
datasets. Critically, we find this low-rank structure as a fundamental cause of
the prevalent dead feature problem in sparse dictionary learning, where it
creates a mismatch between randomly initialized features and the intrinsic
geometry of the activation space. Building on this insight, we propose a
subspace-constrained training method for sparse autoencoders (SAEs),
initializing feature directions into the active subspace of activations. Our
approach reduces dead features from 87\% to below 1\% in Attention Output SAEs
with 1M features, and can further extend to other sparse dictionary learning
methods. Our findings provide both new insights into the geometry of attention
and practical tools for improving sparse dictionary learning in large language
models.

</details>


### [145] [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components](https://arxiv.org/abs/2508.17182)
*Hikaru Tsujimura,Arush Tagade*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型在高风险情况下表现出的过度自信行为，通过机制可解释性方法，发现其确定性表示可以分解为情感和逻辑两个部分，并指出可以通过调整这些部分来减轻过度自信的行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在高风险情况下往往表现出过度自信，提供未经证实的确定性信息。我们通过机制可解释性来研究这种行为的内部基础。

Method: 我们使用开源的Llama 3.2模型，这些模型在人类标注的确定性数据集上进行了微调，提取所有层的残差激活，并计算相似性度量以定位确定性表示。

Result: 我们的分析确定了对确定性对比最敏感的层，并揭示了高确定性表示分解为情感和逻辑簇的两个正交子组件，这与心理学中的双路径加工模型相呼应。从这些子组件中得出的引导向量显示出不同的因果效应：情感向量广泛影响预测准确性，而逻辑向量则产生更局部的影响。

Conclusion: 我们的研究提供了LLM自信行为多组件结构的机制证据，并指出了减轻过度自信行为的途径。

Abstract: Large Language Models (LLMs) often display overconfidence, presenting
information with unwarranted certainty in high-stakes contexts. We investigate
the internal basis of this behavior via mechanistic interpretability. Using
open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness
datasets, we extract residual activations across all layers, and compute
similarity metrics to localize assertive representations. Our analysis
identifies layers most sensitive to assertiveness contrasts and reveals that
high-assertive representations decompose into two orthogonal sub-components of
emotional and logical clusters-paralleling the dual-route Elaboration
Likelihood Model in Psychology. Steering vectors derived from these
sub-components show distinct causal effects: emotional vectors broadly
influence prediction accuracy, while logical vectors exert more localized
effects. These findings provide mechanistic evidence for the multi-component
structure of LLM assertiveness and highlight avenues for mitigating
overconfident behavior.

</details>


### [146] [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling](https://arxiv.org/abs/2508.17445)
*Yizhi Li,Qingshui Gu,Zhoufutu Wen,Ziniu Li,Tianshun Xing,Shuyue Guo,Tianyu Zheng,Xin Zhou,Xingwei Qu,Wangchunshu Zhou,Zheng Zhang,Wei Shen,Qian Liu,Chenghua Lin,Jian Yang,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: TreePO is a new approach for aligning large language models via reinforcement learning that improves inference efficiency and reduces computational costs while maintaining or enhancing exploration diversity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of recent advancements in aligning large language models via reinforcement learning, such as expensive on-policy rollouts and limited exploration of diverse reasoning paths.

Method: TreePO introduces a self-guided rollout algorithm that views sequence generation as a tree-structured searching process, utilizing dynamic tree sampling policy and fixed-length segment decoding. It also includes a segment-wise sampling algorithm, a tree-based segment-level advantage estimation, and analysis on dynamic divergence and fallback strategy.

Result: TreePO demonstrates performance gains on reasoning benchmarks and efficiency savings in GPU hours, with up to 43% reduction in sampling design for trained models, 40% reduction at trajectory-level, and 35% reduction at token-level sampling compute for existing models.

Conclusion: TreePO offers a practical path toward scaling RL-based post-training with fewer samples and less compute, while improving inference efficiency.

Abstract: Recent advancements in aligning large language models via reinforcement
learning have achieved remarkable gains in solving complex reasoning problems,
but at the cost of expensive on-policy rollouts and limited exploration of
diverse reasoning paths. In this work, we introduce TreePO, involving a
self-guided rollout algorithm that views sequence generation as a
tree-structured searching process. Composed of dynamic tree sampling policy and
fixed-length segment decoding, TreePO leverages local uncertainty to warrant
additional branches. By amortizing computation across common prefixes and
pruning low-value paths early, TreePO essentially reduces the per-update
compute burden while preserving or enhancing exploration diversity. Key
contributions include: (1) a segment-wise sampling algorithm that alleviates
the KV cache burden through contiguous segments and spawns new branches along
with an early-stop mechanism; (2) a tree-based segment-level advantage
estimation that considers both global and local proximal policy optimization.
and (3) analysis on the effectiveness of probability and quality-driven dynamic
divergence and fallback strategy. We empirically validate the performance gain
of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours
from 22\% up to 43\% of the sampling design for the trained models, meanwhile
showing up to 40\% reduction at trajectory-level and 35\% at token-level
sampling compute for the existing models. While offering a free lunch of
inference efficiency, TreePO reveals a practical path toward scaling RL-based
post-training with fewer samples and less compute. Home page locates at
https://m-a-p.ai/TreePO.

</details>


### [147] [Activation Transport Operators](https://arxiv.org/abs/2508.17540)
*Andrzej Szablewski,Marek Masiak*

Main category: cs.LG

TL;DR: 该研究提出了一种计算轻量级的方法，用于分析大语言模型中特征的线性传输机制，有助于提高模型的安全性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解特征如何在残差流中流动对于改进安全保护、早期检测和纠正模型错误具有重要意义。

Method: 研究提出了激活传输算子（ATO），这是一种从上游到下游残差的线性映射，通过下游SAE解码器投影在特征空间中进行评估。

Result: 研究实证证明了激活传输算子可以确定特征是否从先前层线性传输或由非线性层计算合成，并提供了传输效率的上限以及残差流子空间的大小估计。

Conclusion: 该研究提出了一种计算轻量级的方法，可以用于安全、调试以及更清晰地了解大语言模型中的线性计算位置。

Abstract: The residual stream mediates communication between transformer decoder layers
via linear reads and writes of non-linear computations. While sparse-dictionary
learning-based methods locate features in the residual stream, and activation
patching methods discover circuits within the model, the mechanism by which
features flow through the residual stream remains understudied. Understanding
this dynamic can better inform jailbreaking protections, enable early detection
of model mistakes, and their correction. In this work, we propose Activation
Transport Operators (ATO), linear maps from upstream to downstream residuals
$k$ layers later, evaluated in feature space using downstream SAE decoder
projections. We empirically demonstrate that these operators can determine
whether a feature has been linearly transported from a previous layer or
synthesised from non-linear layer computation. We develop the notion of
transport efficiency, for which we provide an upper bound, and use it to
estimate the size of the residual stream subspace that corresponds to linear
transport. We empirically demonstrate the linear transport, report transport
efficiency and the size of the residual stream's subspace involved in linear
transport. This compute-light (no finetuning, <50 GPU-h) method offers
practical tools for safety, debugging, and a clearer picture of where
computation in LLMs behaves linearly.

</details>


### [148] [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs](https://arxiv.org/abs/2508.17679)
*Trinayan Baruah,Kaustubh Shivdikar,Sara Prescott,David Kaeli*

Main category: cs.LG

TL;DR: 本文评估了基于Mamba的SSMs，并分析了它们在GPU上的行为，以探索可能的优化来继续扩展此类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer具有表达能力，但计算注意力的二次复杂度是增加序列长度时性能扩展的主要障碍。基于Mamba的SSMs提供了一种替代路径，通过为不同领域和字段（如视频、文本生成和图）的新颖模型架构减少了自注意的计算复杂度需求。因此，对这些新兴工作负载在GPU上的行为进行表征并理解其在GPU微架构设计中的需求非常重要。

Method: 我们构建了一个工作负载套件，涵盖了不同模型架构的代表性模型，并使用该套件分析了在GPU上运行基于Mamba的SSMs的架构影响。

Result: 我们评估了基于Mamba的SSMs，并表征了它们在GPU上的训练行为。

Conclusion: 我们的工作为继续扩展此类模型的性能提供了新的优化可能性。

Abstract: Mamba-based State Space Models (SSM) have emerged as a promising alternative
to the ubiquitous transformers. Despite the expressive power of transformers,
the quadratic complexity of computing attention is a major impediment to
scaling performance as we increase the sequence length. SSMs provide an
alternative path that addresses this problem, reducing the computational
complexity requirements of self-attention with novel model architectures for
different domains and fields such as video, text generation and graphs. Thus,
it is important to characterize the behavior of these emerging workloads on
GPUs and understand their requirements during GPU microarchitectural design. In
this work we evaluate Mamba-based SSMs and characterize their behavior during
training on GPUs. We construct a workload suite that offers representative
models that span different model architectures. We then use this suite to
analyze the architectural implications of running Mamba-based SSMs on GPUs. Our
work sheds new light on potential optimizations to continue scaling the
performance for such models.

</details>


### [149] [Proximal Supervised Fine-Tuning](https://arxiv.org/abs/2508.17784)
*Wenhong Zhu,Ruobing Xie,Rui Wang,Xingwu Sun,Di Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: PSFT is a fine-tuning method inspired by RL techniques that improves generalization and stability during SFT.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning (SFT) of foundation models often leads to poor generalization, where prior capabilities deteriorate after tuning on new tasks or domains.

Method: PSFT is inspired by TRPO and PPO in RL, and it incorporates trust-region benefits to constrain policy drift during SFT while maintaining competitive tuning.

Result: Experiments across mathematical and human-value domains show that PSFT matches SFT in-domain, outperforms it in out-of-domain generalization, remains stable under prolonged training without causing entropy collapse, and provides a stronger foundation for the subsequent optimization.

Conclusion: PSFT provides a stronger foundation for subsequent optimization and shows better out-of-domain generalization compared to SFT.

Abstract: Supervised fine-tuning (SFT) of foundation models often leads to poor
generalization, where prior capabilities deteriorate after tuning on new tasks
or domains. Inspired by trust-region policy optimization (TRPO) and proximal
policy optimization (PPO) in reinforcement learning (RL), we propose Proximal
SFT (PSFT). This fine-tuning objective incorporates the benefits of
trust-region, effectively constraining policy drift during SFT while
maintaining competitive tuning. By viewing SFT as a special case of policy
gradient methods with constant positive advantages, we derive PSFT that
stabilizes optimization and leads to generalization, while leaving room for
further optimization in subsequent post-training stages. Experiments across
mathematical and human-value domains show that PSFT matches SFT in-domain,
outperforms it in out-of-domain generalization, remains stable under prolonged
training without causing entropy collapse, and provides a stronger foundation
for the subsequent optimization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [150] [Anemoi: A Semi-Centralized Multi-agent Systems Based on Agent-to-Agent Communication MCP server from Coral Protocol](https://arxiv.org/abs/2508.17068)
*Xinxing Ren,Caelum Forder,Qianbo Zang,Ahsen Tahir,Roman J. Georgio,Suman Deb,Peter Carroll,Önder Gürcan,Zekun Guo*

Main category: cs.MA

TL;DR: Anemoi是一种基于Agent-to-Agent通信的半集中式多智能体系统，能够提高代理间的协作效率并减少对单一规划器的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统设计依赖于强大的规划器，导致性能下降，并且代理间通信有限，引入冗余和信息丢失。

Method: Anemoi基于Coral Protocol的Agent-to-Agent (A2A)通信MCP服务器构建，通过结构化和直接的代理间协作实现半集中式多智能体系统。

Result: Anemoi在GAIA基准测试中使用小型LLM（GPT-4.1-mini）作为规划器，达到了52.73%的准确率，超过了最强的开源基线OWL（43.63%）。

Conclusion: Anemoi在GAIA基准测试中表现出色，证明了其在使用小型LLM作为规划器时的高效性和可扩展性。

Abstract: Recent advances in generalist multi-agent systems (MAS) have largely followed
a context-engineering plus centralized paradigm, where a planner agent
coordinates multiple worker agents through unidirectional prompt passing. While
effective under strong planner models, this design suffers from two critical
limitations: (1) strong dependency on the planner's capability, which leads to
degraded performance when a smaller LLM powers the planner; and (2) limited
inter-agent communication, where collaboration relies on costly prompt
concatenation and context injection, introducing redundancy and information
loss. To address these challenges, we propose Anemoi, a semi-centralized MAS
built on the Agent-to-Agent (A2A) communication MCP server from Coral Protocol.
Unlike traditional designs, Anemoi enables structured and direct inter-agent
collaboration, allowing all agents to monitor progress, assess results,
identify bottlenecks, and propose refinements in real time. This paradigm
reduces reliance on a single planner, supports adaptive plan updates, and
minimizes redundant context passing, resulting in more scalable and
cost-efficient execution. Evaluated on the GAIA benchmark, Anemoi achieved
52.73\% accuracy with a small LLM (GPT-4.1-mini) as the planner, surpassing the
strongest open-source baseline OWL (43.63\%) by +9.09\% under identical LLM
settings. Our implementation is publicly available at
https://github.com/Coral-Protocol/Anemoi.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [151] [RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style Transfer](https://arxiv.org/abs/2508.17031)
*Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.SD

TL;DR: 本文提出了一种基于Transformer的非自回归方法，用于文本条件语音插入任务，能够动态确定语音插入的长度并保持语音特征。实验结果表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 更新语音音频当对应的文本转录被修改时，例如在文本转录中进行更正时。

Method: 我们提出了一种基于Transformer的非自回归方法，允许根据文本转录和可用部分输入的节奏动态确定语音插入的可变长度。

Result: 实验和用户研究结果表明，我们的方法优于现有的自适应文本到语音方法，并且提供了许多定性结果来评估所提方法输出的质量。

Conclusion: 我们的方法在LibriTTS上的实验和用户研究中表现优于基于现有自适应文本到语音方法的基线。

Abstract: We propose a method for the task of text-conditioned speech insertion, i.e.
inserting a speech sample in an input speech sample, conditioned on the
corresponding complete text transcript. An example use case of the task would
be to update the speech audio when corrections are done on the corresponding
text transcript. The proposed method follows a transformer-based
non-autoregressive approach that allows speech insertions of variable lengths,
which are dynamically determined during inference, based on the text transcript
and tempo of the available partial input. It is capable of maintaining the
speaker's voice characteristics, prosody and other spectral properties of the
available speech input. Results from our experiments and user study on LibriTTS
show that our method outperforms baselines based on an existing adaptive text
to speech method. We also provide numerous qualitative results to appreciate
the quality of the output from the proposed method.

</details>

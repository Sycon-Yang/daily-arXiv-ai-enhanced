<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本研究对意大利语性别中立重写任务中的先进大型语言模型进行了系统评估，并提出了一个衡量中立性和语义保真度的二维框架。结果表明，开放权重的大型语言模型表现优于现有模型，而微调后的模型在尺寸较小的情况下也能达到或超过最佳性能。


<details>
  <summary>Details</summary>
Motivation: 性别中立重写旨在消除不必要的性别规范，同时保持意义，这在语法性别语言如意大利语中尤其具有挑战性。

Method: 我们进行了系统评估，比较了多种大型语言模型的少样本提示，微调了选定模型，并应用了针对性的清理来提高任务相关性。

Result: 开放权重的大型语言模型在意大利语性别中立重写任务上表现优于现有专门模型，而微调后的模型在尺寸较小的情况下也能达到或超过最佳开放权重模型的性能。

Conclusion: 我们的研究发现，开放权重的大型语言模型在意大利语性别中立重写任务上表现优于现有专门模型，而微调后的模型在尺寸较小的情况下也能达到或超过最佳开放权重模型的性能。最后，我们讨论了优化训练数据以实现中立性和意义保留之间的权衡。

Abstract: Gender-neutral rewriting (GNR) aims to reformulate text to eliminate
unnecessary gender specifications while preserving meaning, a particularly
challenging task in grammatical-gender languages like Italian. In this work, we
conduct the first systematic evaluation of state-of-the-art large language
models (LLMs) for Italian GNR, introducing a two-dimensional framework that
measures both neutrality and semantic fidelity to the input. We compare
few-shot prompting across multiple LLMs, fine-tune selected models, and apply
targeted cleaning to boost task relevance. Our findings show that open-weight
LLMs outperform the only existing model dedicated to GNR in Italian, whereas
our fine-tuned models match or exceed the best open-weight LLM's performance at
a fraction of its size. Finally, we discuss the trade-off between optimizing
the training data for neutrality and meaning preservation.

</details>


### [2] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
*Alisa Kanganis,Katherine A. Keith*

Main category: cs.CL

TL;DR: 本文介绍了 Op-Fed 数据集，用于研究 FOMC 言论中的观点和立场，并探讨了其在模型训练和未来注释工作中的潜力。


<details>
  <summary>Details</summary>
Motivation: 创建一个高质量的标注数据集，以帮助研究货币政策言论中的观点和立场。

Method: 我们开发了一个五阶段的层次化方案来分离观点、货币政策和对货币政策的立场，并选择了使用主动学习的实例进行注释。

Result: 使用 Op-Fed 数据集，我们发现一个顶级的封闭权重 LLM 在观点分类中达到了 0.80 的零样本准确率，但在分类货币政策立场时仅达到 0.61，低于我们的 0.89 的人类基准。

Conclusion: Op-Fed 数据集对于未来的模型训练、置信度校准以及未来的注释工作具有潜在价值。

Abstract: The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets
monetary policy, affecting the borrowing and spending decisions of millions of
people. In this work, we release Op-Fed, a dataset of 1044 human-annotated
sentences and their contexts from FOMC transcripts. We faced two major
technical challenges in dataset creation: imbalanced classes -- we estimate
fewer than 8% of sentences express a non-neutral stance towards monetary policy
-- and inter-sentence dependence -- 65% of instances require context beyond the
sentence-level. To address these challenges, we developed a five-stage
hierarchical schema to isolate aspects of opinion, monetary policy, and stance
towards monetary policy as well as the level of context needed. Second, we
selected instances to annotate using active learning, roughly doubling the
number of positive instances across all schema aspects. Using Op-Fed, we found
a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion
classification but only 0.61 zero-shot accuracy classifying stance towards
monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be
useful for future model training, confidence calibration, and as a seed dataset
for future annotation efforts.

</details>


### [3] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
*John Mendonça,Lining Zhang,Rahul Mallidi,Alon Lavie,Isabel Trancoso,Luis Fernando D'Haro,João Sedoc*

Main category: cs.CL

TL;DR: 该论文介绍了DSTC12 Track 1的两个子任务，分析了现有基线的表现，并指出在文化相关的安全检测方面仍需改进。


<details>
  <summary>Details</summary>
Motivation: 由于传统指标往往不足，且安全考虑通常狭隘或文化偏见，因此需要对对话系统进行更全面的评估。

Method: 该论文分析了DSTC12 Track 1的两个子任务：(1) 对话级、多维自动评估指标，以及(2) 多语言和多文化安全检测。

Result: 在任务1中，Llama-3-8B基线在10个对话维度上达到了最高的平均Spearman相关性（0.1681）。在任务2中，参与团队在多语言安全子集上显著优于Llama-Guard-3-1B基线，但在文化子集上基线表现更好。

Conclusion: 该论文描述了为参与者提供的数据集和基线，以及两个子任务的提交评估结果。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for robust dialogue system evaluation, yet comprehensive assessment
remains challenging. Traditional metrics often prove insufficient, and safety
considerations are frequently narrowly defined or culturally biased. The DSTC12
Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and
Safety," is part of the ongoing effort to address these critical gaps. The
track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic
Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.
For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved
the highest average Spearman's correlation (0.1681), indicating substantial
room for improvement. In Task 2, while participating teams significantly
outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top
ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126
ROC-AUC), highlighting critical needs in culturally-aware safety. This paper
describes the datasets and baselines provided to participants, as well as
submission evaluation results for each of the two proposed subtasks.

</details>


### [4] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
*Shambhavi Krishna,Atharva Naik,Chaitali Agarwal,Sudharshan Govindan,Taesung Lee,Haw-Shiuan Chang*

Main category: cs.CL

TL;DR: 本文提出了一种迁移学习分析框架，揭示了影响模型性能的隐藏因素，为更有效的模型适应提供了见解。


<details>
  <summary>Details</summary>
Motivation: 由于在实际应用中，大型语言模型可能遇到训练期间未遇到的任务，因此需要依赖具有不同特征的数据集进行迁移学习，并预期分布外请求。

Method: 构建了一个迁移学习矩阵和降维分析框架，以剖析跨任务交互。训练并分析了10个模型，以识别潜在能力并发现迁移学习的副作用。

Result: 研究发现，性能提升往往无法用表面数据集相似性或源数据质量来解释，而是由源数据集的隐藏统计因素（如类别分布和生成长度倾向）以及特定语言特征更为重要。

Conclusion: 本文提供了对迁移学习复杂动态的见解，为更可预测和有效的大型语言模型适应铺平了道路。

Abstract: Large language models are increasingly deployed across diverse applications.
This often includes tasks LLMs have not encountered during training. This
implies that enumerating and obtaining the high-quality training data for all
tasks is infeasible. Thus, we often need to rely on transfer learning using
datasets with different characteristics, and anticipate out-of-distribution
requests. Motivated by this practical need, we propose an analysis framework,
building a transfer learning matrix and dimensionality reduction, to dissect
these cross-task interactions. We train and analyze 10 models to identify
latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)
and discover the side effects of the transfer learning. Our findings reveal
that performance improvements often defy explanations based on surface-level
dataset similarity or source data quality. Instead, hidden statistical factors
of the source dataset, such as class distribution and generation length
proclivities, alongside specific linguistic features, are actually more
influential. This work offers insights into the complex dynamics of transfer
learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [5] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
*Zhuoxuan Zhang,Jinhao Duan,Edward Kim,Kaidi Xu*

Main category: cs.CL

TL;DR: 本研究发现大型语言模型（LLMs）在内部表示中线性编码了问题模糊性，并且可以在神经元层面检测和控制模糊性。通过操纵特定神经元，可以控制模型的行为，从直接回答到回避。


<details>
  <summary>Details</summary>
Motivation: 现实世界的问题中存在普遍的模糊性，但大型语言模型（LLMs）往往给出自信的答案而不是寻求澄清。

Method: 在模型的预填充阶段，我们识别出少量神经元（少至一个）编码了问题模糊性信息。对这些模糊性编码神经元（AENs）进行训练的探测器在模糊性检测上表现良好，并在数据集之间具有泛化能力，优于基于提示和基于表示的基线方法。分层分析显示，AENs 从浅层出现，表明模糊性信号在模型处理流程中早期被编码。最后，我们展示了通过操纵 AENs，我们可以控制 LLM 的行为，从直接回答到回避。

Result: 探测器在模糊性检测上表现良好，并在数据集之间具有泛化能力，优于基于提示和基于表示的基线方法。分层分析显示，AENs 从浅层出现，表明模糊性信号在模型处理流程中早期被编码。通过操纵 AENs，可以控制 LLM 的行为，从直接回答到回避。

Conclusion: 我们的发现表明，LLMs 形成了问题模糊性的紧凑内部表示，从而实现了可解释和可控的行为。

Abstract: Ambiguity is pervasive in real-world questions, yet large language models
(LLMs) often respond with confident answers rather than seeking clarification.
In this work, we show that question ambiguity is linearly encoded in the
internal representations of LLMs and can be both detected and controlled at the
neuron level. During the model's pre-filling stage, we identify that a small
number of neurons, as few as one, encode question ambiguity information. Probes
trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance
on ambiguity detection and generalize across datasets, outperforming
prompting-based and representation-based baselines. Layerwise analysis reveals
that AENs emerge from shallow layers, suggesting early encoding of ambiguity
signals in the model's processing pipeline. Finally, we show that through
manipulating AENs, we can control LLM's behavior from direct answering to
abstention. Our findings reveal that LLMs form compact internal representations
of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [6] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
*Shang Qin,Jingheng Ye,Yinghui Li,Hai-Tao Zheng,Qi Li,Jinxiao Shan,Zhixing Li,Hong-Gee Kim*

Main category: cs.CL

TL;DR: 本文介绍了CL$^2$GEC，这是第一个针对中国文学语法错误修正的持续学习基准，用于评估在多个学术领域中的自适应CGEC。实验结果表明，基于正则化的方法比基于重放或简单顺序的方法更能有效缓解遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的CGEC研究缺乏针对多学科学术写作的专用基准，忽略了持续学习作为处理特定领域语言变化和防止灾难性遗忘的有前途的解决方案。

Method: 我们引入了CL$^2$GEC，这是第一个针对中国文学语法错误修正的持续学习基准，用于评估在多个学术领域中的自适应CGEC。

Result: 实验结果表明，基于正则化的方法比基于重放或简单顺序的方法更能有效缓解遗忘。

Conclusion: 我们的基准为跨不同学术领域的自适应语法错误修正研究提供了坚实的 foundation。

Abstract: The growing demand for automated writing assistance in diverse academic
domains highlights the need for robust Chinese Grammatical Error Correction
(CGEC) systems that can adapt across disciplines. However, existing CGEC
research largely lacks dedicated benchmarks for multi-disciplinary academic
writing, overlooking continual learning (CL) as a promising solution to handle
domain-specific linguistic variation and prevent catastrophic forgetting. To
fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning
benchmark for Chinese Literature Grammatical Error Correction, designed to
evaluate adaptive CGEC across multiple academic fields. Our benchmark includes
10,000 human-annotated sentences spanning 10 disciplines, each exhibiting
distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating
grammatical error correction in a continual learning setting, simulating
sequential exposure to diverse academic disciplines to reflect real-world
editorial dynamics. We evaluate large language models under sequential tuning,
parameter-efficient adaptation, and four representative CL algorithms, using
both standard GEC metrics and continual learning metrics adapted to task-level
variation. Experimental results reveal that regularization-based methods
mitigate forgetting more effectively than replay-based or naive sequential
approaches. Our benchmark provides a rigorous foundation for future research in
adaptive grammatical error correction across diverse academic domains.

</details>


### [7] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
*Xinxu Zhou,Jiaqi Bai,Zhenqi Sun,Fanxiang Zeng,Yue Liu*

Main category: cs.CL

TL;DR: This paper presents AgentCTG, a new framework for controlled text generation that improves precise and complex control over text generation using multi-agent workflows and an auto-prompt module, achieving state-of-the-art results and enhancing user experiences in practical applications.


<details>
  <summary>Details</summary>
Motivation: Controlled Text Generation (CTG) faces challenges in achieving fine-grained conditional control, especially in real-world scenarios where cost, scalability, domain knowledge learning, and precise control are required.

Method: The paper introduces a novel framework called AgentCTG, which simulates control and regulation mechanisms in multi-agent workflows. It explores various collaboration methods among agents and introduces an auto-prompt module to enhance generation effectiveness.

Result: AgentCTG achieves state-of-the-art results on multiple public datasets. It also demonstrates effectiveness in practical applications, such as improving the driving experience in online navigation and enabling more immersive interactions within online communities.

Conclusion: AgentCTG achieves state-of-the-art results on multiple public datasets and enhances the driving experience in online navigation through improved content delivery.

Abstract: Although significant progress has been made in many tasks within the field of
Natural Language Processing (NLP), Controlled Text Generation (CTG) continues
to face numerous challenges, particularly in achieving fine-grained conditional
control over generation. Additionally, in real scenario and online
applications, cost considerations, scalability, domain knowledge learning and
more precise control are required, presenting more challenge for CTG. This
paper introduces a novel and scalable framework, AgentCTG, which aims to
enhance precise and complex control over the text generation by simulating the
control and regulation mechanisms in multi-agent workflows. We explore various
collaboration methods among different agents and introduce an auto-prompt
module to further enhance the generation effectiveness. AgentCTG achieves
state-of-the-art results on multiple public datasets. To validate its
effectiveness in practical applications, we propose a new challenging
Character-Driven Rewriting task, which aims to convert the original text into
new text that conform to specific character profiles and simultaneously
preserve the domain knowledge. When applied to online navigation with
role-playing, our approach significantly enhances the driving experience
through improved content delivery. By optimizing the generation of contextually
relevant text, we enable a more immersive interaction within online
communities, fostering greater personalization and user engagement.

</details>


### [8] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
*Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为CARE的新框架，旨在解决大型语言模型在处理上下文信息时的不一致性问题。该方法通过模型自身的检索能力，在推理过程中显式整合上下文证据，从而提高了检索准确性和答案生成性能。实验结果表明，该方法在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）常常在上下文保真度方面遇到困难，当根据提供的信息回答问题时会产生不一致的答案。现有的方法要么依赖于昂贵的监督微调来生成答案后的证据，要么训练模型进行网络搜索，但不一定能提高对给定上下文的利用。

Method: 我们提出了CARE，这是一种新颖的原生检索增强推理框架，通过模型自身的检索能力教大型语言模型在推理过程中显式整合上下文证据。

Result: 在多个现实世界和反事实问答基准上的大量实验表明，我们的方法显著优于监督微调、传统的检索增强生成方法和外部检索解决方案。

Conclusion: 本研究代表了在使大型语言模型更准确、可靠和高效地执行知识密集型任务方面的根本性进展。

Abstract: Large language models (LLMs) often struggle with context fidelity, producing
inconsistent answers when responding to questions based on provided
information. Existing approaches either rely on expensive supervised
fine-tuning to generate evidence post-answer or train models to perform web
searches without necessarily improving utilization of the given context. We
propose CARE, a novel native retrieval-augmented reasoning framework that
teaches LLMs to explicitly integrate in-context evidence within their reasoning
process with the model's own retrieval capabilities. Our method requires
limited labeled evidence data while significantly enhancing both retrieval
accuracy and answer generation performance through strategically retrieved
in-context tokens in the reasoning chain. Extensive experiments on multiple
real-world and counterfactual QA benchmarks demonstrate that our approach
substantially outperforms supervised fine-tuning, traditional
retrieval-augmented generation methods, and external retrieval solutions. This
work represents a fundamental advancement in making LLMs more accurate,
reliable, and efficient for knowledge-intensive tasks.

</details>


### [9] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 研究构建了一个日语自然语言推理数据集，评估了大型语言模型在处理比较任务中的表现，发现提示格式和逻辑语义表示对模型性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补大型语言模型在处理涉及数值和逻辑表达的自然语言推理任务中的不足，特别是在非主流语言如日语中的表现。

Method: 构建了一个专注于比较的日语自然语言推理数据集，并在零样本和少样本设置下评估了多种大型语言模型。

Result: 模型的表现对零样本设置中的提示格式敏感，并且受少样本示例中的黄金标签影响。此外，模型在处理日语特有的语言现象时遇到困难。包含逻辑语义表示的提示有助于模型在即使有少样本示例的情况下也能正确预测标签。

Conclusion: 研究结果表明，大型语言模型在处理涉及比较的自然语言推理任务时仍然存在挑战，尤其是在日语等非主流语言中。然而，包含逻辑语义表示的提示可以提高模型的预测准确性。

Abstract: Large Language Models (LLMs) perform remarkably well in Natural Language
Inference (NLI). However, NLI involving numerical and logical expressions
remains challenging. Comparatives are a key linguistic phenomenon related to
such inference, but the robustness of LLMs in handling them, especially in
languages that are not dominant in the models' training data, such as Japanese,
has not been sufficiently explored. To address this gap, we construct a
Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in
zero-shot and few-shot settings. Our results show that the performance of the
models is sensitive to the prompt formats in the zero-shot setting and
influenced by the gold labels in the few-shot examples. The LLMs also struggle
to handle linguistic phenomena unique to Japanese. Furthermore, we observe that
prompts containing logical semantic representations help the models predict the
correct labels for inference problems that they struggle to solve even with
few-shot examples.

</details>


### [10] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
*Iyadh Ben Cheikh Larbi,Ajay Madhavan Ravichandran,Aljoscha Burchardt,Roland Roller*

Main category: cs.CL

TL;DR: 研究将指令调优的大语言模型与DSPy-based提示优化结合，用于处理临床笔记和结构化电子健康记录，结果表明其性能可与专用多模态系统媲美，且更具适应性和更低的复杂度。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在涉及结构化数据（如时间序列）的临床分类任务中的能力。

Method: 使用基于DSPy的提示优化调整指令调优的大语言模型，以联合处理临床笔记和结构化EHR输入。

Result: 该方法在性能上与专用多模态系统相当，同时具有更低的复杂性和更高的任务适应性。

Conclusion: 该方法在性能上与专用多模态系统相当，同时具有更低的复杂性和更高的任务适应性。

Abstract: Large language models (LLMs) excel at text generation, but their ability to
handle clinical classification tasks involving structured data, such as time
series, remains underexplored. In this work, we adapt instruction-tuned LLMs
using DSPy-based prompt optimization to process clinical notes and structured
EHR inputs jointly. Our results show that this approach achieves performance on
par with specialized multimodal systems while requiring less complexity and
offering greater adaptability across tasks.

</details>


### [11] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
*Xiao Zheng*

Main category: cs.CL

TL;DR: DSCC-HS is a proactive framework for suppressing LLM hallucination by using a compact proxy model to dynamically steer the target model during decoding.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM) hallucination is a significant barrier to their reliable deployment. Current methods like Retrieval-Augmented Generation (RAG) are often reactive.

Method: DSCC-HS is a novel, proactive framework that intervenes during autoregressive decoding. It uses a compact proxy model trained in adversarial roles as a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During inference, these proxies dynamically steer a large target model by injecting a real-time steering vector.

Result: Experiments on TruthfulQA and BioGEN show DSCC-HS achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2% Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained the highest FActScore of 46.50.

Conclusion: DSCC-HS is a principled and efficient solution for enhancing LLM factuality.

Abstract: Large Language Model (LLM) hallucination is a significant barrier to their
reliable deployment. Current methods like Retrieval-Augmented Generation (RAG)
are often reactive. We introduce **Dynamic Self-reinforcing Calibration for
Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that
intervenes during autoregressive decoding. Inspired by dual-process cognitive
theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a
Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During
inference, these proxies dynamically steer a large target model by injecting a
real-time steering vector, which is the difference between FAP and HDP logits,
at each decoding step. This plug-and-play approach requires no modification to
the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS
achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%
Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained
the highest FActScore of 46.50. These results validate DSCC-HS as a principled
and efficient solution for enhancing LLM factuality.

</details>


### [12] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
*Peter Beidler,Mark Nguyen,Kevin Lybarger,Ola Holmberg,Eric Ford,John Kang*

Main category: cs.CL

TL;DR: 本文介绍了一种自然语言处理（NLP）筛选工具，用于检测放射肿瘤学中的高严重性事件报告。我们使用两个数据集训练和评估了两种模型：支持向量机（SVM）和BlueBERT。结果表明，经过跨机构迁移学习优化的BlueBERT模型在检测高严重性报告方面表现出色，其性能接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 事件报告是医疗保健中安全和质量改进的重要工具，但手动审查耗时且需要专业知识。因此，我们需要一种自然语言处理（NLP）筛选工具来检测放射肿瘤学中的高严重性事件报告。

Method: 我们使用两个文本数据集来训练和评估我们的自然语言处理模型：来自我们机构的7,094份报告（Inst.）和来自IAEA SAFRON（SF）的571份报告，所有报告都由临床内容专家标注了严重程度评分。我们训练和评估了两种类型的模型：基线支持向量机（SVM）和BlueBERT，这是一种在PubMed摘要和住院患者数据上预训练的大语言模型。我们通过两种方式评估了模型的泛化能力。首先，我们评估了使用Inst.-train训练并在SF-test上测试的模型。其次，我们训练了一个BlueBERT_TRANSFER模型，该模型首先在Inst.-train上微调，然后在SF-train上微调，最后在SF-test集上测试。为了进一步分析模型性能，我们还检查了来自我们Inst.数据集的59份报告，这些报告已手动编辑以提高清晰度。

Result: 在Inst.测试上的分类性能使用SVM达到AUROC 0.82，使用BlueBERT达到0.81。在没有跨机构迁移学习的情况下，SF测试上的性能有限，使用SVM达到AUROC 0.42，使用BlueBERT达到0.56。经过在两个数据集上微调的BlueBERT_TRANSFER模型在SF测试上的性能提高到AUROC 0.78。SVM和BlueBERT_TRANSFER模型在手动清理的Inst.报告上的性能（AUROC 0.85和0.74）与人类表现（AUROC 0.81）相似。

Conclusion: 我们成功开发了跨机构的自然语言处理模型，用于放射肿瘤学中心的事件报告文本。这些模型在经过清理的数据集上检测高严重性报告的表现与人类相当。

Abstract: PURPOSE: Incident reports are an important tool for safety and quality
improvement in healthcare, but manual review is time-consuming and requires
subject matter expertise. Here we present a natural language processing (NLP)
screening tool to detect high-severity incident reports in radiation oncology
across two institutions.
  METHODS AND MATERIALS: We used two text datasets to train and evaluate our
NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA
SAFRON (SF), all of which had severity scores labeled by clinical content
experts. We trained and evaluated two types of models: baseline support vector
machines (SVM) and BlueBERT which is a large language model pretrained on
PubMed abstracts and hospitalized patient data. We assessed for
generalizability of our model in two ways. First, we evaluated models trained
using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that
was first fine-tuned on Inst.-train then on SF-train before testing on SF-test
set. To further analyze model performance, we also examined a subset of 59
reports from our Inst. dataset, which were manually edited for clarity.
  RESULTS Classification performance on the Inst. test achieved AUROC 0.82
using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,
performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56
using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,
improved the performance on SF test to AUROC 0.78. Performance of SVM, and
BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and
0.74) was similar to human performance (AUROC 0.81).
  CONCLUSION: In summary, we successfully developed cross-institution NLP
models on incident report text from radiation oncology centers. These models
were able to detect high-severity reports similarly to humans on a curated
dataset.

</details>


### [13] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
*Yaxin Gao,Yao Lu,Zongfei Zhang,Jiaqi Nie,Shanqing Yu,Qi Xuan*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的两阶段提示压缩方法DSPC，在保持语义的同时有效减少token数量，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了应对提示膨胀问题，现有方法需要训练一个小型辅助模型进行压缩，导致额外计算量较大。因此，提出一种无需训练的提示压缩方法。

Method: 提出了一种两阶段、无需训练的方法，称为Dual-Stage Progressive Compression (DSPC)。在粗粒度阶段，基于TF-IDF去除语义价值低的句子；在细粒度阶段，通过注意力贡献、跨模型损失差异和位置重要性评估token的重要性，实现低效用token的修剪同时保持语义。

Result: 在LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo上验证了DSPC的有效性，并在Longbench数据集的FewShot任务中取得了显著提升。例如，在使用仅3倍少的token的情况下，DSPC在FewShot任务中达到了49.17的性能，优于LongLLMLingua 7.76。

Conclusion: DSPC在受限的token预算下验证了其有效性，并在Longbench数据集的FewShot任务中表现出色，优于现有的最佳基线方法。

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language processing (NLP) tasks. To achieve more accurate output, the prompts
used to drive LLMs have become increasingly longer, which incurs higher
computational costs. To address this prompt inflation problem, prompt
compression has been proposed. However, most existing methods require training
a small auxiliary model for compression, incurring a significant amount of
additional computation. To avoid this, we propose a two-stage, training-free
approach, called Dual-Stage Progressive Compression (DSPC). In the
coarse-grained stage, semantic-related sentence filtering removes sentences
with low semantic value based on TF-IDF. In the fine-grained stage, token
importance is assessed using attention contribution, cross-model loss
difference, and positional importance, enabling the pruning of low-utility
tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct
and GPT-3.5-Turbo under a constrained token budget and observe consistent
improvements. For instance, in the FewShot task of the Longbench dataset, DSPC
achieves a performance of 49.17 by using only 3x fewer tokens, outperforming
the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [14] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
*Yosuke Mikami,Daiki Matsuoka,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 日本語の比較表現に対する論理的推論システムccg-jcompを提案し、その有効性を実証した。


<details>
  <summary>Details</summary>
Motivation: 日本語と英語には比較表現における形態的・意味的な違いがあるため、英語向けに開発された論理的推論システムを直接適用することが難しい。

Method: 構文論的意味論に基づいた論理的推論システムccg-jcompを提案し、日本語NLIデータセットで評価を行った。

Result: 提案したシステムは、既存のLLMと比較して高い精度を示した。

Conclusion: 本文は、日本語の比較表現に対する論理的推論システムccg-jcompの提案とその有効性の実証を示している。

Abstract: Natural Language Inference (NLI) involving comparatives is challenging
because it requires understanding quantities and comparative relations
expressed by sentences. While some approaches leverage Large Language Models
(LLMs), we focus on logic-based approaches grounded in compositional semantics,
which are promising for robust handling of numerical and logical expressions.
Previous studies along these lines have proposed logical inference systems for
English comparatives. However, it has been pointed out that there are several
morphological and semantic differences between Japanese and English
comparatives. These differences make it difficult to apply such systems
directly to Japanese comparatives. To address this gap, this study proposes
ccg-jcomp, a logical inference system for Japanese comparatives based on
compositional semantics. We evaluate the proposed system on a Japanese NLI
dataset containing comparative expressions. We demonstrate the effectiveness of
our system by comparing its accuracy with that of existing LLMs.

</details>


### [15] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
*Vani Kanjirangat,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: This paper investigates data-efficient and parameter-efficient methods for Arabic Dialect Identification, finding that LoRA-based models perform best in identifying dialects.


<details>
  <summary>Details</summary>
Motivation: To improve the effectiveness of Arabic Dialect Identification using Large Language Models (LLMs) through efficient methods that reduce the need for large amounts of data and parameters.

Method: The paper explores data-efficient and parameter-efficient approaches for Arabic Dialect Identification (ADI), including soft-prompting strategies like prefix-tuning, prompt-tuning, P-tuning, P-tuning V2, and LoRA reparameterizations. It also analyzes hard prompting with zero-shot and few-shot inferences and evaluates performance on various models.

Result: Soft-prompted encoder variants outperform other methods, while LoRA-based fine-tuned models achieve the best results, even surpassing full fine-tuning.

Conclusion: LLMs generally struggle to differentiate the dialectal nuances in few-shot or zero-shot setups, but soft-prompted encoder variants and LoRA-based fine-tuned models perform better.

Abstract: This paper discusses our exploration of different data-efficient and
parameter-efficient approaches to Arabic Dialect Identification (ADI). In
particular, we investigate various soft-prompting strategies, including
prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA
reparameterizations. For the data-efficient strategy, we analyze hard prompting
with zero-shot and few-shot inferences to analyze the dialect identification
capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT
approaches, we conducted our experiments using Arabic-specific encoder models
on several major datasets. We also analyzed the n-shot inferences on
open-source decoder-only models, a general multilingual model (Phi-3.5), and an
Arabic-specific one(SILMA). We observed that the LLMs generally struggle to
differentiate the dialectal nuances in the few-shot or zero-shot setups. The
soft-prompted encoder variants perform better, while the LoRA-based fine-tuned
models perform best, even surpassing full fine-tuning.

</details>


### [16] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
*Yangning Li,Tingwei Lu,Yinghui Li,Yankai Chen,Wei-Chieh Huang,Wenhao Jiang,Hui Wang,Hai-Tao Zheng,Philip S. Yu*

Main category: cs.CL

TL;DR: CAMPUS is a new framework for efficient instruction tuning that addresses the issue of curriculum rigidity by dynamically adapting to the model's evolving capabilities.


<details>
  <summary>Details</summary>
Motivation: Current curriculum tuning methods suffer from curriculum rigidity as they rely on static heuristic difficulty metrics and fail to adapt to the evolving capabilities of models during training.

Method: Competence-Aware Multi-Perspective curriculum instruction tuning framework (CAMPUS) is proposed, which includes dynamic selection for sub-curriculum, competency-aware adjustment to the curriculum schedule, and multiple difficulty-based scheduling.

Result: Extensive experiments prove the superior performance of CAMPUS compared to other state-of-the-art baselines for efficient instruction tuning.

Conclusion: CAMPUS demonstrates superior performance compared to other state-of-the-art baselines for efficient instruction tuning.

Abstract: Efficient instruction tuning aims to enhance the ultimate performance of
large language models (LLMs) trained on a given instruction dataset. Curriculum
learning as a typical data organization strategy has shown preliminary
effectiveness in instruction tuning. However, current curriculum tuning methods
suffer from the curriculum rigidity, since they rely solely on static heuristic
difficulty metrics. These methods fail to adapt to the evolving capabilities of
models during training, resulting in a fixed and potentially sub-optimal
learning trajectory. To address the issue, Competence-Aware Multi-Perspective
cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS
offers several advantages: (1) Dynamic selection for sub-curriculum. (2)
Competency-aware adjustment to the curriculum schedule. (3) Multiple
difficulty-based scheduling. Extensive experiments prove the superior
performance of CAMPUS, compared to other state-of-the-art baselines for
efficient instruction tuning.

</details>


### [17] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
*Laura García-Sardiña,Hermenegildo Fabregat,Daniel Deniz,Rabih Zbib*

Main category: cs.CL

TL;DR: 本研究评估了职位标题中语法性别分配对自动职位排名系统的影响，发现多语言模型存在性别偏见，并提出了使用RBO进行评估的方法。


<details>
  <summary>Details</summary>
Motivation: 研究显式语法性别分配在职位标题中如何影响自动职位排名系统的性能，从而评估和减少性别偏见。

Method: 使用RBO（Rank-Biased Overlap）作为评估指标，生成并分享了四个语法性别语言的测试集，用于评估职位标题匹配任务中的性别偏见。

Result: 所有测试的多语言模型均显示出不同程度的性别偏见，证明了当前系统存在性别偏见问题。

Conclusion: 所有测试的多语言模型都表现出不同程度的性别偏见，这表明需要改进自动职位排名系统以减少性别偏见。

Abstract: This work sets the ground for studying how explicit grammatical gender
assignment in job titles can affect the results of automatic job ranking
systems. We propose the usage of metrics for ranking comparison controlling for
gender to evaluate gender bias in job title ranking systems, in particular RBO
(Rank-Biased Overlap). We generate and share test sets for a job title matching
task in four grammatical gender languages, including occupations in masculine
and feminine form and annotated by gender and matching relevance. We use the
new test sets and the proposed methodology to evaluate the gender bias of
several out-of-the-box multilingual models to set as baselines, showing that
all of them exhibit varying degrees of gender bias.

</details>


### [18] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
*Edward Phillips,Sean Wu,Soheila Molaei,Danielle Belgrave,Anshul Thakur,David Clifton*

Main category: cs.CL

TL;DR: 本文提出了一种基于几何分析的框架，用于检测大型语言模型中的幻觉。该框架在多个数据集上表现出色，特别是在医疗数据集上。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒方法无法提供全局和局部不确定性估计。当前的局部方法通常依赖于对内部模型状态的白盒访问，而黑盒方法仅提供全局不确定性估计。本文旨在解决这一问题。

Method: 本文基于仅需黑盒模型访问的响应批次的原型分析，提出了几何体积和几何怀疑两种方法。几何体积用于衡量由响应嵌入得出的原型凸包体积，而几何怀疑则通过可靠性排序来减少幻觉。

Result: 实验表明，本文提出的框架在短格式问答数据集上的表现与现有方法相当或更好，并在医疗数据集上取得了更好的结果。此外，本文还通过证明凸包体积与熵之间的联系提供了理论支持。

Conclusion: 本文提出了一种几何框架，用于检测大型语言模型中的幻觉。实验表明，该框架在短格式问答数据集上的表现与现有方法相当或更好，并在医疗数据集上取得了更好的结果。

Abstract: Large language models demonstrate impressive results across diverse tasks but
are still known to hallucinate, generating linguistically plausible but
incorrect answers to questions. Uncertainty quantification has been proposed as
a strategy for hallucination detection, but no existing black-box approach
provides estimates for both global and local uncertainty. The former attributes
uncertainty to a batch of responses, while the latter attributes uncertainty to
individual responses. Current local methods typically rely on white-box access
to internal model states, whilst black-box methods only provide global
uncertainty estimates. We introduce a geometric framework to address this,
based on archetypal analysis of batches of responses sampled with only
black-box model access. At the global level, we propose Geometric Volume, which
measures the convex hull volume of archetypes derived from response embeddings.
At the local level, we propose Geometric Suspicion, which ranks responses by
reliability and enables hallucination reduction through preferential response
selection. Unlike prior dispersion methods which yield only a single global
score, our approach provides semantic boundary points which have utility for
attributing reliability to individual responses. Experiments show that our
framework performs comparably to or better than prior methods on short form
question-answering datasets, and achieves superior results on medical datasets
where hallucinations carry particularly critical risks. We also provide
theoretical justification by proving a link between convex hull volume and
entropy.

</details>


### [19] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
*Kartik Shinde,Laurent Besacier,Ondrej Bojar,Thibaut Thonet,Tirthankar Ghosal*

Main category: cs.CL

TL;DR: AutoMin 2025是一个关于自动会议摘要和基于会议转录本的问题回答的共享任务。尽管参与度较低，但通过引入多个基线系统，实现了对当前大型语言模型的全面评估。


<details>
  <summary>Details</summary>
Motivation: AutoMin 2025旨在评估当前大型语言模型在自动会议摘要和基于会议转录本的问题回答任务上的表现。由于参与度较低，组织者引入了多个基线系统以确保全面评估。

Method: AutoMin 2025包括两个任务：会议摘要和基于会议转录本的问题回答（QA）。会议摘要任务覆盖了英语和捷克语两种语言，以及项目会议和欧洲议会会议两个领域。QA任务专注于项目会议，并提供了单语QA和跨语言QA两种设置。

Result: 2025年的AutoMin任务中，只有1个团队参与会议摘要任务，2个团队参与QA任务。尽管参与度有限，但通过基线系统的引入，实现了对当前大型语言模型的全面评估。

Conclusion: 本文介绍了AutoMin的第三版，这是一个关于自动会议摘要的共享任务。虽然2025年的参与度较低，但通过包含多个基线系统，使当前大型语言模型在两个任务上的评估更加全面。

Abstract: This paper presents the third edition of AutoMin, a shared task on automatic
meeting summarization into minutes. In 2025, AutoMin featured the main task of
minuting, the creation of structured meeting minutes, as well as a new task:
question answering (QA) based on meeting transcripts.
  The minuting task covered two languages, English and Czech, and two domains:
project meetings and European Parliament sessions. The QA task focused solely
on project meetings and was available in two settings: monolingual QA in
English, and cross-lingual QA, where questions were asked and answered in Czech
based on English meetings.
  Participation in 2025 was more limited compared to previous years, with only
one team joining the minuting task and two teams participating in QA. However,
as organizers, we included multiple baseline systems to enable a comprehensive
evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [20] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
*Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense*

Main category: cs.CL

TL;DR: 研究发现，大语言模型对德国方言使用者存在显著的偏见，明确标记语言人口统计信息会放大这种偏见。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大语言模型是否反映了对使用方言的人的负面社会刻板印象，以及这些刻板印象如何在模型中体现。

Method: 研究基于社会语言学文献分析了与方言使用者相关的常见特质，并通过两个任务（关联任务和决策任务）评估了大语言模型的方言命名偏见和方言使用偏见。为了评估模型的方言使用偏见，构建了一个新的评估语料库，将七种地区德语方言（如阿勒曼尼语和巴伐利亚语）的句子与其标准德语对应句配对。

Result: 研究发现，所有评估的大语言模型都对德国方言使用者表现出显著的方言命名和使用偏见，这体现在负面形容词关联上，并且这些偏见在决策过程中被再现。此外，明确标记语言人口统计信息（如德国方言使用者）会比隐含线索（如方言使用）放大偏见。

Conclusion: 研究发现，所有评估的大语言模型都对德国方言使用者表现出显著的方言命名和使用偏见，这体现在负面形容词关联上，并且这些偏见在决策过程中被再现。此外，明确标记语言人口统计信息（如德国方言使用者）会比隐含线索（如方言使用）放大偏见。

Abstract: Dialects represent a significant component of human culture and are found
across all regions of the world. In Germany, more than 40% of the population
speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural
importance, individuals speaking dialects often face negative societal
stereotypes. We examine whether such stereotypes are mirrored by large language
models (LLMs). We draw on the sociolinguistic literature on dialect perception
to analyze traits commonly associated with dialect speakers. Based on these
traits, we assess the dialect naming bias and dialect usage bias expressed by
LLMs in two tasks: an association task and a decision task. To assess a model's
dialect usage bias, we construct a novel evaluation corpus that pairs sentences
from seven regional German dialects (e.g., Alemannic and Bavarian) with their
standard German counterparts. We find that: (1) in the association task, all
evaluated LLMs exhibit significant dialect naming and dialect usage bias
against German dialect speakers, reflected in negative adjective associations;
(2) all models reproduce these dialect naming and dialect usage biases in their
decision making; and (3) contrary to prior work showing minimal bias with
explicit demographic mentions, we find that explicitly labeling linguistic
demographics--German dialect speakers--amplifies bias more than implicit cues
like dialect usage.

</details>


### [21] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
*Yang Liu,Chenhui Chu*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型（LLMs）在不同类型的偏见场景中与人类价值观的对齐情况，发现LLMs在特定类型场景中表现出对齐偏好，并且同一模型家族的LLMs具有更高的判断一致性。此外，研究还发现LLMs在理解社会偏见方面没有显著差异，并且更倾向于使用自己生成的解释。通过微调较小的语言模型，它们也能够生成关于社会偏见的解释，但这些解释的可接受度相对较低。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型（LLMs）在不同类型的偏见场景中与人类价值观的对齐情况，以及它们在解释社会偏见方面的能力。

Method: 本研究分析了12个来自四个模型家族和四个数据集的LLMs，评估了它们在不同类型的偏见场景中的对齐情况。同时，研究还探讨了LLMs在解释社会偏见方面的理解能力和生成解释的偏好。

Result: 研究发现，大型语言模型（LLMs）在不同类型的偏见场景中表现出对特定类型场景的对齐偏好，并且同一模型家族的LLMs具有更高的判断一致性。此外，研究还发现LLMs在理解社会偏见方面没有显著差异，并且更倾向于使用自己生成的解释。通过微调较小的语言模型，它们也能够生成关于社会偏见的解释，但这些解释的可接受度相对较低。

Conclusion: 研究发现，大型语言模型（LLMs）在不同类型的偏见场景中表现出对特定类型场景的对齐偏好，并且同一模型家族的LLMs具有更高的判断一致性。此外，研究还发现LLMs在理解社会偏见方面没有显著差异，并且更倾向于使用自己生成的解释。通过微调较小的语言模型，它们也能够生成关于社会偏见的解释，但这些解释的可接受度相对较低。

Abstract: Large language models (LLMs) can lead to undesired consequences when
misaligned with human values, especially in scenarios involving complex and
sensitive social biases. Previous studies have revealed the misalignment of
LLMs with human values using expert-designed or agent-based emulated bias
scenarios. However, it remains unclear whether the alignment of LLMs with human
values differs across different types of scenarios (e.g., scenarios containing
negative vs. non-negative questions). In this study, we investigate the
alignment of LLMs with human values regarding social biases (HVSB) in different
types of bias scenarios. Through extensive analysis of 12 LLMs from four model
families and four datasets, we demonstrate that LLMs with large model parameter
scales do not necessarily have lower misalignment rate and attack success rate.
Moreover, LLMs show a certain degree of alignment preference for specific types
of scenarios and the LLMs from the same model family tend to have higher
judgment consistency. In addition, we study the understanding capacity of LLMs
with their explanations of HVSB. We find no significant differences in the
understanding of HVSB across LLMs. We also find LLMs prefer their own generated
explanations. Additionally, we endow smaller language models (LMs) with the
ability to explain HVSB. The generation results show that the explanations
generated by the fine-tuned smaller LMs are more readable, but have a
relatively lower model agreeability.

</details>


### [22] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 本文介绍了一种名为CER的新框架，用于生物医学事实核查，该框架结合了科学证据检索、大型语言模型的推理以及监督真实性预测，能够在多个数据集上取得优异的表现。


<details>
  <summary>Details</summary>
Motivation: 由于生物医学声明验证的复杂性，包括复杂的术语、领域专业知识的需求以及对科学证据的依赖，因此需要一种新的方法来解决这个问题。

Method: CER框架结合了科学证据检索、大型语言模型的推理以及监督真实性预测。

Result: 在HealthFC、BioASQ-7b和SciFact等专家标注的数据集上，CER框架展示了最先进的性能，并且在跨数据集的泛化方面表现良好。

Conclusion: CER框架在专家标注的数据集上表现出色，并且在跨数据集的泛化方面显示出良好的前景。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [23] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
*Mariano Barone,Antonio Romano,Giuseppe Riccio,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: CER 是一种新的生物医学事实核查框架，结合了科学证据检索、大型语言模型的推理和监督真伪预测，有效降低了生成内容的幻觉风险，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域的虚假信息对公共健康和医疗系统信任构成威胁，而现有的自动事实核查方法在处理复杂术语、领域专业知识和科学证据的依赖方面仍面临挑战。

Method: CER 结合了科学证据检索、大型语言模型的推理以及监督真伪预测，利用大语言模型的文本生成能力与先进的检索技术来确保生成内容基于可验证的证据。

Result: 在专家标注的数据集（如HealthFC、BioASQ-7b、SciFact）上的评估表明，CER 表现出最先进的性能，并具有良好的跨数据集泛化能力。

Conclusion: CER 是一种有效的生物医学事实核查框架，能够有效降低生成内容的幻觉风险，并在多个数据集上表现出色。

Abstract: Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [24] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
*Domenico Meconi,Simone Stirpe,Federico Martelli,Leonardo Lavalle,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在词义消歧和生成任务中的表现，发现它们在WSD任务中与专门系统相当，并在生成任务中表现出高准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管对大型语言模型进行了广泛的评估，但它们是否真正掌握词义的程度仍然研究不足。本文旨在填补这一空白，评估LLM在词义消歧和生成任务中的表现。

Method: 论文评估了指令调优的LLM在词义消歧（WSD）任务中的能力，并比较了它们与专门为此任务设计的最先进系统的性能。此外，还评估了两个表现最好的开源和闭源LLM在三种生成设置中的词义理解能力：定义生成、自由形式解释和示例生成。

Result: 在WSD任务中，GPT-4o和DeepSeek-V3等领先模型的表现与专门的WSD系统相当，同时在不同领域和难度下表现出更高的鲁棒性。在生成任务中，LLM能够以高达98%的准确率解释上下文中单词的含义，其中自由形式的解释任务表现最佳。

Conclusion: 论文指出，大型语言模型在词义消歧任务中表现出与专门的WSD系统相当的性能，并且在不同领域和难度下表现出更高的鲁棒性。在生成任务中，LLM能够以高达98%的准确率解释上下文中单词的含义，其中自由形式的解释任务表现最佳。

Abstract: Understanding the meaning of words in context is a fundamental capability for
Large Language Models (LLMs). Despite extensive evaluation efforts, the extent
to which LLMs show evidence that they truly grasp word senses remains
underexplored. In this paper, we address this gap by evaluating both i) the
Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,
comparing their performance to state-of-the-art systems specifically designed
for the task, and ii) the ability of two top-performing open- and closed-source
LLMs to understand word senses in three generative settings: definition
generation, free-form explanation, and example generation. Notably, we find
that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve
performance on par with specialized WSD systems, while also demonstrating
greater robustness across domains and levels of difficulty. In the generation
tasks, results reveal that LLMs can explain the meaning of words in context up
to 98\% accuracy, with the highest performance observed in the free-form
explanation task, which best aligns with their generative capabilities.

</details>


### [25] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
*Dayeon Ki,Marine Carpuat,Paul McNamee,Daniel Khashabi,Eugene Yang,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

TL;DR: 研究发现语言模型在引用时倾向于优先选择英语来源，这可能影响其引用行为。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探讨不同文档语言的混合是否以意想不到的方式影响生成和引用。

Method: 我们引入了一种控制方法，使用模型内部来测量语言偏好，同时保持其他因素（如文档相关性）不变。

Result: 我们发现，当查询为英语时，模型倾向于引用英语来源，这种偏差在低资源语言和中间上下文文档中更为明显。此外，模型有时会为了语言偏好而牺牲文档的相关性。

Conclusion: 我们的研究结果揭示了语言模型如何利用多语言上下文并影响引用行为。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enable language
models to answer knowledge-intensive queries with citation-supported responses
across languages. While such systems have been proposed, an open questions is
whether the mixture of different document languages impacts generation and
citation in unintended ways. To investigate, we introduce a controlled
methodology using model internals to measure language preference while holding
other factors such as document relevance constant. Across eight languages and
six open-weight models, we find that models preferentially cite English sources
when queries are in English, with this bias amplified for lower-resource
languages and for documents positioned mid-context. Crucially, we find that
models sometimes trade-off document relevance for language preference,
indicating that citation choices are not always driven by informativeness
alone. Our findings shed light on how language models leverage multilingual
context and influence citation behavior.

</details>


### [26] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
*Sami Ul Haq,Chinonso Cynthia Osuji,Sheila Castilho,Brian Davis*

Main category: cs.CL

TL;DR: 本文介绍了我们提交给WMT25机器翻译会议的自动翻译质量评估共享任务的系统。我们的系统基于COMET框架，利用增强的长上下文数据来预测段级错误跨度注释（ESA）得分。通过拼接领域内的人工标注句子并计算其得分的加权平均值来构建长上下文训练数据。我们通过归一化尺度整合多个人工判断数据集（MQM、SQM和DA），并训练多语言回归模型从源、假设和参考翻译中预测质量得分。实验结果表明，结合长上下文信息相比仅使用短片段的模型能更好地与人类判断相关联。


<details>
  <summary>Details</summary>
Motivation: 为了提高自动翻译质量评估的准确性，我们尝试利用长上下文信息来改进现有的模型。

Method: 我们的系统基于COMET框架，利用增强的长上下文数据来预测段级错误跨度注释（ESA）得分。通过拼接领域内的人工标注句子并计算其得分的加权平均值来构建长上下文训练数据。我们通过归一化尺度整合多个人工判断数据集（MQM、SQM和DA），并训练多语言回归模型从源、假设和参考翻译中预测质量得分。

Result: 实验结果表明，结合长上下文信息相比仅使用短片段的模型能更好地与人类判断相关联。

Conclusion: 本文展示了利用长上下文信息在自动翻译质量评估中的有效性，并提出了一个基于COMET框架的系统。

Abstract: In this paper, we present our submission to the Tenth Conference on Machine
Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.
  Our systems are built upon the COMET framework and trained to predict
segment-level Error Span Annotation (ESA) scores using augmented long-context
data.
  To construct long-context training data, we concatenate in-domain,
human-annotated sentences and compute a weighted average of their scores.
  We integrate multiple human judgment datasets (MQM, SQM, and DA) by
normalising their scales and train multilingual regression models to predict
quality scores from the source, hypothesis, and reference translations.
  Experimental results show that incorporating long-context information
improves correlations with human judgments compared to models trained only on
short segments.

</details>


### [27] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
*Colin Hong,Xu Guo,Anand Chaanan Singh,Esha Choukse,Dmitrii Ustiugov*

Main category: cs.CL

TL;DR: This paper introduces Slim-SC, a method to efficiently improve LLM reasoning performance by reducing computational overhead of Self-Consistency (SC) through pruning redundant reasoning chains.


<details>
  <summary>Details</summary>
Motivation: To address the computational overhead of Self-Consistency (SC) and provide an efficient TTS alternative.

Method: Slim-SC, a step-wise pruning strategy that identifies and removes redundant chains using inter-chain similarity at the thought level.

Result: Slim-SC reduces inference latency and KVC usage by up to 45% and 26%, respectively, with R1-Distill, while maintaining or improving accuracy.

Conclusion: Slim-SC offers a simple yet efficient TTS alternative for SC by reducing inference latency and KVC usage while maintaining or improving accuracy.

Abstract: Recently, Test-Time Scaling (TTS) has gained increasing attention for
improving LLM reasoning performance at test time without retraining the model.
A notable TTS technique is Self-Consistency (SC), which generates multiple
reasoning chains in parallel and selects the final answer via majority voting.
While effective, the order-of-magnitude computational overhead limits its broad
deployment. Prior attempts to accelerate SC mainly rely on model-based
confidence scores or heuristics with limited empirical support. For the first
time, we theoretically and empirically analyze the inefficiencies of SC and
reveal actionable opportunities for improvement. Building on these insights, we
propose Slim-SC, a step-wise pruning strategy that identifies and removes
redundant chains using inter-chain similarity at the thought level. Experiments
on three STEM reasoning datasets and two recent LLM architectures show that
Slim-SC reduces inference latency and KVC usage by up to 45% and 26%,
respectively, with R1-Distill, while maintaining or improving accuracy, thus
offering a simple yet efficient TTS alternative for SC.

</details>


### [28] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
*Minjia Mao,Bowen Yin,Yu Zhu,Xiao Fang*

Main category: cs.CL

TL;DR: ES-CoT 是一种在推理过程中通过检测答案收敛来缩短 CoT 生成的方法，能够在减少推理令牌数量的同时保持与标准 CoT 相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在解决复杂问题时表现出卓越的能力，但长链式思维（CoT）会带来高昂的推理成本。因此，需要一种方法来缩短 CoT 生成，同时保持性能损失最小。

Method: ES-CoT 是一种在推理过程中通过检测答案收敛来缩短 CoT 生成的方法。它在每个推理步骤的末尾提示 LLM 输出当前最终答案，并跟踪连续相同步骤答案的运行长度以衡量答案收敛。一旦运行长度显著增加并超过最小阈值，生成就会终止。

Result: 实验表明，ES-CoT 平均减少了约 41% 的推理令牌，同时保持了与标准 CoT 相当的准确性。此外，ES-CoT 与自一致性提示无缝集成，并在各种超参数选择下保持稳健。

Conclusion: ES-CoT 是一种实用且有效的高效推理方法，能够在减少推理令牌数量的同时保持与标准 CoT 相当的准确性。

Abstract: Reasoning large language models (LLMs) have demonstrated superior capacities
in solving complicated problems by generating long chain-of-thoughts (CoT), but
such a lengthy CoT incurs high inference costs. In this study, we introduce
ES-CoT, an inference-time method that shortens CoT generation by detecting
answer convergence and stopping early with minimal performance loss. At the end
of each reasoning step, we prompt the LLM to output its current final answer,
denoted as a step answer. We then track the run length of consecutive identical
step answers as a measure of answer convergence. Once the run length exhibits a
sharp increase and exceeds a minimum threshold, the generation is terminated.
We provide both empirical and theoretical support for this heuristic: step
answers steadily converge to the final answer, and large run-length jumps
reliably mark this convergence. Experiments on five reasoning datasets across
three LLMs show that ES-CoT reduces the number of inference tokens by about
41\% on average while maintaining accuracy comparable to standard CoT. Further,
ES-CoT integrates seamlessly with self-consistency prompting and remains robust
across hyperparameter choices, highlighting it as a practical and effective
approach for efficient reasoning.

</details>


### [29] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
*Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem*

Main category: cs.CL

TL;DR: Hala是一个基于翻译和调整管道构建的阿拉伯语中心指令和翻译模型家族，它在阿拉伯语相关的基准测试中取得了最先进的结果，并释放了模型、数据、评估和配方以加速阿拉伯语NLP的研究。


<details>
  <summary>Details</summary>
Motivation: 为了加速阿拉伯语自然语言处理的研究，需要更高效的阿拉伯语中心模型。

Method: Hala是通过我们的翻译和调整管道构建的阿拉伯语中心指令和翻译模型。首先，我们将一个强大的AR↔EN教师压缩到FP8（产生约2倍的吞吐量，没有质量损失），并用它来创建高保真双语监督。然后，轻量级语言模型LFM2-1.2B在这些数据上进行微调，并用于将高质量的英语指令集翻译成阿拉伯语，生成一个针对指令遵循的百万级语料库。Hala模型在350M、700M、1.2B和9B参数下进行训练，并应用slerp合并来平衡阿拉伯语专业化与基础模型的优势。

Result: Hala在阿拉伯语相关的基准测试中取得了最先进的结果，在“nano”（≤2B）和“small”（7-9B）类别中都优于其基础模型。

Conclusion: Hala在阿拉伯语相关的基准测试中取得了最先进的结果，并释放了模型、数据、评估和配方以加速阿拉伯语NLP的研究。

Abstract: We present Hala, a family of Arabic-centric instruction and translation
models built with our translate-and-tune pipeline. We first compress a strong
AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher
throughput with no quality loss) and use it to create high-fidelity bilingual
supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this
data and used to translate high-quality English instruction sets into Arabic,
producing a million-scale corpus tailored to instruction following. We train
Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to
balance Arabic specialization with base-model strengths. On Arabic-centric
benchmarks, Hala achieves state-of-the-art results within both the "nano"
($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release
models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [30] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
*Sami Ul Haq,Sheila Castilho,Yvette Graham*

Main category: cs.CL

TL;DR: 本文研究了基于语音的机器翻译质量评估方法，发现其与文本评估结果一致，但能识别出更多系统间的差异，建议将其纳入未来的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量评估主要依赖于文本，而许多实际应用涉及语音翻译，因此需要一种更自然的评估方式。

Method: 本文比较了文本和音频评估方法，使用来自WMT通用MT共享任务的10个MT系统的评估数据，并通过亚马逊Mechanical Turk收集了众包判断。此外，还进行了统计显著性测试和自我复制实验以验证音频方法的可靠性和一致性。

Result: 基于音频的评估结果与文本评估结果大致一致，但在某些情况下发现了显著差异，这归因于语音的丰富性和自然性。

Conclusion: 本文建议将基于语音的评估纳入未来的机器翻译评估框架中，以更好地反映实际应用中的翻译质量。

Abstract: Machine Translation (MT) has achieved remarkable performance, with growing
interest in speech translation and multimodal approaches. However, despite
these advancements, MT quality assessment remains largely text centric,
typically relying on human experts who read and compare texts. Since many
real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK
Translator) involve translation being spoken rather printed or read, a more
natural way to assess translation quality would be through speech as opposed
text-only evaluations. This study compares text-only and audio-based
evaluations of 10 MT systems from the WMT General MT Shared Task, using
crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,
performed statistical significance testing and self-replication experiments to
test reliability and consistency of audio-based approach. Crowd-sourced
assessments based on audio yield rankings largely consistent with text only
evaluations but, in some cases, identify significant differences between
translation systems. We attribute this to speech richer, more natural modality
and propose incorporating speech-based assessments into future MT evaluation
frameworks.

</details>


### [31] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
*Paweł Mąka,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本研究验证了训练数据稀疏性是上下文利用困难的关键瓶颈，并提出两种训练策略以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了验证标准训练数据中上下文丰富示例的稀疏性是否是上下文利用困难的原因，并探索如何提高模型对上下文的理解和利用能力。

Method: 通过构建具有受控比例的上下文相关示例的训练数据集，在单语和多语设置中系统地验证了这一假设。此外，还提出了两种训练策略来利用现有数据。

Result: 研究证实了训练数据稀疏性与模型性能之间的强关联，表明稀疏性是一个关键瓶颈。同时发现，一种上下文现象的改进并不推广到其他现象，跨语言转移效果有限。

Conclusion: 研究发现，训练数据的稀疏性是影响模型性能的关键瓶颈，并提出了两种训练策略来提高上下文利用效率，从而在单语和多语设置中分别提高了6和8个百分点的准确率。

Abstract: Achieving human-level translations requires leveraging context to ensure
coherence and handle complex phenomena like pronoun disambiguation. Sparsity of
contextually rich examples in the standard training data has been hypothesized
as the reason for the difficulty of context utilization. In this work, we
systematically validate this claim in both single- and multilingual settings by
constructing training datasets with a controlled proportions of contextually
relevant examples. We demonstrate a strong association between training data
sparsity and model performance confirming sparsity as a key bottleneck.
Importantly, we reveal that improvements in one contextual phenomenon do no
generalize to others. While we observe some cross-lingual transfer, it is not
significantly higher between languages within the same sub-family. Finally, we
propose and empirically evaluate two training strategies designed to leverage
the available data. These strategies improve context utilization, resulting in
accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in
single- and multilingual settings respectively.

</details>


### [32] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
*Zijie Lin,Bryan Hooi*

Main category: cs.CL

TL;DR: 本文提出了ConfMAD框架，通过引入置信度表达来提升MAD系统的性能和效果。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在辩论中难以清晰表达优势的问题，以及不当的置信度表达导致的代理坚持错误信念或过早收敛的问题。

Method: 提出将置信度表达融入MAD系统，以允许LLM明确传达其置信水平，并开发了ConfMAD框架来整合置信度表达。

Result: 实验结果证明了方法的有效性，并分析了置信度如何影响辩论动态。

Conclusion: 通过实验结果验证了方法的有效性，并提供了关于如何设计自信感知的MAD系统的见解。

Abstract: Generative Large Language Models (LLMs) have demonstrated remarkable
performance across a wide range of tasks. Recent research has introduced
Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate
human debate and thereby improve task performance. However, while some LLMs may
possess superior knowledge or reasoning capabilities for specific tasks, they
often struggle to clearly communicate this advantage during debates, in part
due to a lack of confidence expression. Moreover, inappropriate confidence
expression can cause agents in MAD systems to either stubbornly maintain
incorrect beliefs or converge prematurely on suboptimal answers, ultimately
reducing debate effectiveness and overall system performance. To address these
challenges, we propose incorporating confidence expression into MAD systems to
allow LLMs to explicitly communicate their confidence levels. To validate this
approach, we develop ConfMAD, a MAD framework that integrates confidence
expression throughout the debate process. Experimental results demonstrate the
effectiveness of our method, and we further analyze how confidence influences
debate dynamics, offering insights into the design of confidence-aware MAD
systems.

</details>


### [33] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
*Zekang Liu,Wei Feng,Fanhua Shang,Lianyu Hu,Jichao Feng,Liqing Gao*

Main category: cs.CL

TL;DR: 本文提出了一种基于问题的新型手语翻译任务QB-SLT，通过利用对话中的上下文信息来提高翻译效果。所提出的SSL-SSAW方法在两个新构建的数据集上取得了最先进的性能，并且对话辅助可以达到甚至超越词素辅助的效果。


<details>
  <summary>Details</summary>
Motivation: 手语翻译（SLT）可以弥合聋人和听力人士之间的沟通障碍，而对话提供了重要的上下文线索以帮助翻译。与词素注释相比，对话在交流中自然发生且更容易注释。因此，本文旨在探索对话的高效整合以提高手语翻译的效果。

Method: 本文提出了一种基于跨模态自监督学习和Sigmoid自注意力加权（SSL-SSAW）的融合方法。该方法首先采用对比学习对多模态特征进行对齐，然后引入Sigmoid自注意力加权模块从问题和手语序列中进行自适应特征提取。此外，还利用可用的问题文本通过自监督学习增强表示和翻译能力。

Result: 本文在新构建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上评估了所提出的方法，SSL-SSAW方法取得了最先进的性能。此外，可视化结果证明了结合对话在提高翻译质量方面的有效性。

Conclusion: 本文提出了基于问题的手语翻译(QB-SLT)任务，通过利用对话中的上下文信息来提高翻译效果。实验结果表明，所提出的SSL-SSAW方法在两个新构建的数据集上取得了最先进的性能，并且对话辅助可以达到甚至超越词素辅助的效果。

Abstract: Sign Language Translation (SLT) bridges the communication gap between deaf
people and hearing people, where dialogue provides crucial contextual cues to
aid in translation. Building on this foundational concept, this paper proposes
Question-based Sign Language Translation (QB-SLT), a novel task that explores
the efficient integration of dialogue. Unlike gloss (sign language
transcription) annotations, dialogue naturally occurs in communication and is
easier to annotate. The key challenge lies in aligning multimodality features
while leveraging the context of the question to improve translation. To address
this issue, we propose a cross-modality Self-supervised Learning with Sigmoid
Self-attention Weighting (SSL-SSAW) fusion method for sign language
translation. Specifically, we employ contrastive learning to align
multimodality features in QB-SLT, then introduce a Sigmoid Self-attention
Weighting (SSAW) module for adaptive feature extraction from question and sign
language sequences. Additionally, we leverage available question text through
self-supervised learning to enhance representation and translation
capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and
PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,
easily accessible question assistance can achieve or even surpass the
performance of gloss assistance. Furthermore, visualization results demonstrate
the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [34] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
*Monica Sekoyan,Nithin Rao Koluguri,Nune Tadevosyan,Piotr Zelasko,Travis Bartley,Nick Karpov,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: Canary-1B-v2是一个高效的多语言自动语音识别和语音到文本翻译模型，支持25种语言，表现出色且速度快。


<details>
  <summary>Details</summary>
Motivation: 开发一个快速、稳健且多语言的自动语音识别和语音到文本翻译模型，以提高性能并减少幻觉问题。

Method: Canary-1B-v2基于FastConformer编码器和Transformer解码器构建，采用两阶段预训练和微调过程，并使用动态数据平衡。同时，还实验了nGPT编码器和NeMo强制对齐器（NFA）以提供可靠的段级时间戳。

Result: Canary-1B-v2在英语ASR任务中优于Whisper-large-v3，并且在多语言ASR和AST任务中与更大的模型如Seamless-M4T-v2-large和基于LLM的系统相当。Parakeet-TDT-0.6B-v3也展示了其多语言ASR能力。

Conclusion: Canary-1B-v2展示了一个快速且稳健的多语言模型，能够有效进行自动语音识别和语音到文本翻译。此外，它在英语ASR任务中表现优于Whisper-large-v3，并且比其他大型模型具有竞争力。

Abstract: This report introduces Canary-1B-v2, a fast, robust multilingual model for
Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built
with a FastConformer encoder and Transformer decoder, it supports 25 languages
primarily European. The model was trained on 1.7M hours of total data samples,
including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce
hallucinations for ASR and AST. We describe its two-stage pre-training and
fine-tuning process with dynamic data balancing, as well as experiments with an
nGPT encoder. Results show nGPT scales well with massive data, while
FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the
NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable
segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2
outperforms Whisper-large-v3 on English ASR while being 10x faster, and
delivers competitive multilingual ASR and AST performance against larger models
like Seamless-M4T-v2-large and LLM-based systems. We also release
Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the
same 25 languages with just 600M parameters.

</details>


### [35] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
*Brian Yan,Injy Hamed,Shuichiro Shimizu,Vasista Lodagala,William Chen,Olga Iakovenko,Bashar Talafha,Amir Hussein,Alexander Polok,Kalvin Chang,Dominik Klement,Sara Althubaiti,Puyuan Peng,Matthew Wiesner,Thamar Solorio,Ahmed Ali,Sanjeev Khudanpur,Shinji Watanabe,Chih-Chen Chen,Zhen Wu,Karim Benharrak,Anuj Diwan,Samuele Cornell,Eunjung Yeo,Kwanghee Choi,Carlos Carvalho,Karen Rosero*

Main category: cs.CL

TL;DR: CS-FLEURS是一个新的数据集，用于开发和评估代码切换语音识别和翻译系统，涵盖多种语言对和语音生成方法。


<details>
  <summary>Details</summary>
Motivation: 开发和评估超越高资源语言的代码切换语音识别和翻译系统需要新的数据集。

Method: CS-FLEURS包含四个测试集和一个训练集，使用生成式文本到语音和合成语音数据来覆盖多种代码切换语言对。

Result: CS-FLEURS提供了113个独特的代码切换语言对的数据，包括不同类型的语音生成方法。

Conclusion: CS-FLEURS有助于拓宽未来代码切换语音研究的范围。

Abstract: We present CS-FLEURS, a new dataset for developing and evaluating
code-switched speech recognition and translation systems beyond high-resourced
languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique
code-switched language pairs across 52 languages: 1) a 14 X-English language
pair set with real voices reading synthetically generated code-switched
sentences, 2) a 16 X-English language pair set with generative text-to-speech
3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the
generative text-to-speech, and 4) a 45 X-English lower-resourced language pair
test set with concatenative text-to-speech. Besides the four test sets,
CS-FLEURS also provides a training set with 128 hours of generative
text-to-speech data across 16 X-English language pairs. Our hope is that
CS-FLEURS helps to broaden the scope of future code-switched speech research.
Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [36] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
*Yifan Liu,Wenkuan Zhao,Shanshan Zhong,Jinghui Qin,Mingfu Liang,Zhongzhan Huang,Wushao Wen*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准测试AssoCiAm，用于评估多模态大语言模型的关联能力，并通过混合计算方法解决评估中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架常常忽视关联任务中的固有模糊性，这源于关联的多样性，从而影响评估的可靠性。

Method: 我们将模糊性分为内部模糊性和外部模糊性，并引入AssoCiAm，这是一个基准测试，旨在通过混合计算方法绕过模糊性来评估关联能力。

Result: 我们在MLLM上进行了广泛的实验，揭示了认知与关联之间的强正相关关系。此外，我们观察到评估过程中的模糊性会导致MLLM的行为变得更加随机。

Conclusion: 我们验证了我们的方法在确保更准确和可靠的评估方面的有效性。

Abstract: Recent advancements in multimodal large language models (MLLMs) have garnered
significant attention, offering a promising pathway toward artificial general
intelligence (AGI). Among the essential capabilities required for AGI,
creativity has emerged as a critical trait for MLLMs, with association serving
as its foundation. Association reflects a model' s ability to think creatively,
making it vital to evaluate and understand. While several frameworks have been
proposed to assess associative ability, they often overlook the inherent
ambiguity in association tasks, which arises from the divergent nature of
associations and undermines the reliability of evaluations. To address this
issue, we decompose ambiguity into two types-internal ambiguity and external
ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative
ability while circumventing the ambiguity through a hybrid computational
method. We then conduct extensive experiments on MLLMs, revealing a strong
positive correlation between cognition and association. Additionally, we
observe that the presence of ambiguity in the evaluation process causes MLLMs'
behavior to become more random-like. Finally, we validate the effectiveness of
our method in ensuring more accurate and reliable evaluations. See Project Page
for the data and codes.

</details>


### [37] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
*Akhil Theerthala*

Main category: cs.CL

TL;DR: 本研究介绍了一个新颖且可重复的框架，将相关金融背景与行为金融学研究结合，构建监督数据用于端到端顾问。通过创建19k样本推理数据集并微调Qwen-3-8B模型，结果表明该模型在多个指标上表现与更大的基线相当，但成本降低了80%。


<details>
  <summary>Details</summary>
Motivation: 个性化财务建议需要考虑用户目标、约束、风险承受能力和司法管辖区。先前的LLM工作集中在为投资者和财务规划师提供支持系统上。同时，许多最近的研究通过高维护成本的代理管道检查了更广泛的个人理财任务，包括预算、债务管理、退休和遗产规划，其预期财务回报不足25%。

Method: 我们引入了一个新颖且可重复的框架，将相关的金融背景与行为金融学研究相结合，以构建端到端顾问的监督数据。使用这个框架，我们创建了一个19k样本的推理数据集，并对Qwen-3-8B模型进行了全面微调。

Result: 通过保留测试分割和盲目的LLM评审研究，我们证明了通过精心的数据整理和行为整合，我们的8B模型在事实准确性、流利度和个人化指标上达到了与显著更大的基线（14-32B参数）相当的性能，同时成本比更大的模型低80%。

Conclusion: 通过精心的数据整理和行为整合，我们的8B模型在事实准确性、流利度和个人化指标上达到了与显著更大的基线（14-32B参数）相当的性能，同时成本比更大的模型低80%。

Abstract: Personalized financial advice requires consideration of user goals,
constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on
support systems for investors and financial planners. Simultaneously, numerous
recent studies examine broader personal finance tasks, including budgeting,
debt management, retirement, and estate planning, through agentic pipelines
that incur high maintenance costs, yielding less than 25% of their expected
financial returns. In this study, we introduce a novel and reproducible
framework that integrates relevant financial context with behavioral finance
studies to construct supervision data for end-to-end advisors. Using this
framework, we create a 19k sample reasoning dataset and conduct a comprehensive
fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test
split and a blind LLM-jury study, we demonstrate that through careful data
curation and behavioral integration, our 8B model achieves performance
comparable to significantly larger baselines (14-32B parameters) across factual
accuracy, fluency, and personalization metrics while incurring 80% lower costs
than the larger counterparts.

</details>


### [38] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
*Vahid Ghafouri,Robert McNeil,Teodor Yankov,Madeleine Sumption,Luc Rocher,Scott A. Hale,Adam Mahdi*

Main category: cs.CL

TL;DR: 本研究通过大规模计算分析，比较了英国和美国议会辩论中与移民相关的言论，并利用LLM进行了细粒度分析，揭示了言论趋势的变化。


<details>
  <summary>Details</summary>
Motivation: 分析英国和美国议会辩论中与移民相关的言论，并比较其变化趋势。

Method: 我们使用开放权重的LLM对每条陈述进行标注，并跟踪时间和社会党派之间的总体语气。对于英国，我们还扩展了一个半自动框架来提取细粒度的叙述框架。

Result: 美国的言论日益两极分化，而英国议会的态度相对一致，但工党和保守党之间存在持续的意识形态差距，2025年达到最负面水平。英国议会声明中的叙述框架显示出向安全化叙述的转变，而长期的整合导向框架则有所下降。此外，关于移民的国家法律讨论逐渐被国际法和人权所取代。

Conclusion: 我们的研究展示了LLMs如何在政治和历史背景下支持可扩展的、细粒度的论述分析。

Abstract: We present a large-scale computational analysis of migration-related
discourse in UK parliamentary debates spanning over 75 years and compare it
with US congressional discourse. Using open-weight LLMs, we annotate each
statement with high-level stances toward migrants and track the net tone toward
migrants across time and political parties. For the UK, we extend this with a
semi-automated framework for extracting fine-grained narrative frames to
capture nuances of migration discourse. Our findings show that, while US
discourse has grown increasingly polarised, UK parliamentary attitudes remain
relatively aligned across parties, with a persistent ideological gap between
Labour and the Conservatives, reaching its most negative level in 2025. The
analysis of narrative frames in the UK parliamentary statements reveals a shift
toward securitised narratives such as border control and illegal immigration,
while longer-term integration-oriented frames such as social integration have
declined. Moreover, discussions of national law about immigration have been
replaced over time by international law and human rights, revealing nuances in
discourse trends. Taken together broadly, our findings demonstrate how LLMs can
support scalable, fine-grained discourse analysis in political and historical
contexts.

</details>


### [39] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
*Alejandro Hernández-Cano,Alexander Hägele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank Ďurech,Ido Hakimi,Juan García Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabolčec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour,Ilia Badanin,Harold Benoit,Emanuela Boros,Nicholas Browning,Fabian Bösch,Maximilian Böther,Niklas Canova,Camille Challier,Clement Charmillot,Jonathan Coles,Jan Deriu,Arnout Devos,Lukas Drescher,Daniil Dzenhaliou,Maud Ehrmann,Dongyang Fan,Simin Fan,Silin Gao,Miguel Gila,María Grandury,Diba Hashemi,Alexander Hoyle,Jiaming Jiang,Mark Klein,Andrei Kucharavy,Anastasiia Kucherenko,Frederike Lübeck,Roman Machacek,Theofilos Manitaras,Andreas Marfurt,Kyle Matoba,Simon Matrenok,Henrique Mendoncça,Fawzi Roberto Mohamed,Syrielle Montariol,Luca Mouchel,Sven Najem-Meyer,Jingwei Ni,Gennaro Oliva,Matteo Pagliardini,Elia Palme,Andrei Panferov,Léo Paoletti,Marco Passerini,Ivan Pavlov,Auguste Poiroux,Kaustubh Ponkshe,Nathan Ranchin,Javi Rando,Mathieu Sauser,Jakhongir Saydaliev,Muhammad Ali Sayfiddinov,Marian Schneider,Stefano Schuppli,Marco Scialanga,Andrei Semenov,Kumar Shridhar,Raghav Singhal,Anna Sotnikova,Alexander Sternfeld,Ayush Kumar Tarun,Paul Teiletche,Jannis Vamvas,Xiaozhe Yao,Hao Zhao Alexander Ilic,Ana Klimovic,Andreas Krause,Caglar Gulcehre,David Rosenthal,Elliott Ash,Florian Tramèr,Joost VandeVondele,Livio Veraldi,Martin Rajman,Thomas Schulthess,Torsten Hoefler,Antoine Bosselut,Martin Jaggi,Imanol Schlag*

Main category: cs.CL

TL;DR: Apertus is a fully open suite of large language models designed to address data compliance and multilingual representation issues. It uses openly available data, respects content-owner rights, and expands multilingual coverage. The models achieve state-of-the-art results and release all scientific artifacts with a permissive license.


<details>
  <summary>Details</summary>
Motivation: To address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation.

Method: Apertus models are pretrained exclusively on openly available data, retroactively respecting robots.txt exclusions and filtering for non-permissive, toxic, and personally identifiable content. The Goldfish objective is adopted during pretraining to strongly suppress verbatim recall of data while retaining downstream task performance.

Result: Apertus models expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. They achieve state-of-the-art results among fully open models on multilingual benchmarks.

Conclusion: Apertus models approach state-of-the-art results among fully open models on multilingual benchmarks, rivaling or surpassing open-weight counterparts. Additionally, all scientific artifacts from the development cycle are released with a permissive license, enabling transparent audit and extension.

Abstract: We present Apertus, a fully open suite of large language models (LLMs)
designed to address two systemic shortcomings in today's open model ecosystem:
data compliance and multilingual representation. Unlike many prior models that
release weights without reproducible data pipelines or regard for content-owner
rights, Apertus models are pretrained exclusively on openly available data,
retroactively respecting robots.txt exclusions and filtering for
non-permissive, toxic, and personally identifiable content. To mitigate risks
of memorization, we adopt the Goldfish objective during pretraining, strongly
suppressing verbatim recall of data while retaining downstream task
performance. The Apertus models also expand multilingual coverage, training on
15T tokens from over 1800 languages, with ~40% of pretraining data allocated to
non-English content. Released at 8B and 70B scales, Apertus approaches
state-of-the-art results among fully open models on multilingual benchmarks,
rivalling or surpassing open-weight counterparts. Beyond model weights, we
release all scientific artifacts from our development cycle with a permissive
license, including data preparation scripts, checkpoints, evaluation suites,
and training code, enabling transparent audit and extension.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [40] [Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection](https://arxiv.org/abs/2509.13853)
*Shun Huang,Zhihua Fang,Liang He*

Main category: cs.SD

TL;DR: 本文提出了一种新的训练技术OS-SCL和一种名为TFgram的时间-频率特征，以提高异常声音检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督方法有所进步，但在处理来自不同机器的同类型样本时，频繁的误报问题仍未解决。

Method: 本文引入了一种称为单阶段监督对比学习（OS-SCL）的新训练技术，并提出了一种名为TFgram的时间-频率特征。

Result: 在DCASE 2020挑战任务2中，使用Log-Mel特征实现了94.64% AUC、88.42% pAUC和89.24% mAUC。而使用TFgram特征实现了95.71% AUC、90.23% pAUC和91.23% mAUC。

Conclusion: 本文提出了一种新的训练技术，称为单阶段监督对比学习（OS-SCL），显著解决了这一问题。此外，还提出了一种名为TFgram的时间-频率特征，有效捕捉了异常声音检测的关键信息。

Abstract: Unsupervised anomalous sound detection aims to detect unknown anomalous
sounds by training a model using only normal audio data. Despite advancements
in self-supervised methods, the issue of frequent false alarms when handling
samples of the same type from different machines remains unresolved. This paper
introduces a novel training technique called one-stage supervised contrastive
learning (OS-SCL), which significantly addresses this problem by perturbing
features in the embedding space and employing a one-stage noisy supervised
contrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved
94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.
Additionally, a time-frequency feature named TFgram is proposed, which is
extracted from raw audio. This feature effectively captures critical
information for anomalous sound detection, ultimately achieving 95.71\% AUC,
90.23\% pAUC, and 91.23\% mAUC. The source code is available at:
\underline{www.github.com/huangswt/OS-SCL}.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [41] [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
*Sunkyung Lee,Seongmin Park,Jonghyo Kim,Mincheol Yoon,Jongwuk Lee*

Main category: cs.IR

TL;DR: 本文提出了一种名为GRUT的新模型，通过各种时间信号有效捕捉隐藏的用户偏好。引入了时间感知提示和趋势感知推理方法，实验表明GRUT在多个基准数据集上优于最先进的模型。


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on considering the sequential order of items and neglect to handle the temporal dynamics across items, which can imply evolving user preferences.

Method: We propose a novel model, Generative Recommender Using Time awareness (GRUT), which effectively captures hidden user preferences via various temporal signals. We introduce Time-aware Prompting, consisting of two key contexts: the user-level temporal context and the item-level transition context. We also devise Trend-aware Inference, a training-free method that enhances rankings by incorporating trend information about items with generation likelihood.

Result: Extensive experiments demonstrate that GRUT outperforms state-of-the-art models, with gains of up to 15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets.

Conclusion: GRUT outperforms state-of-the-art models, with gains of up to 15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets.

Abstract: Generative recommendation has emerged as a promising paradigm that formulates
the recommendations into a text-to-text generation task, harnessing the vast
knowledge of large language models. However, existing studies focus on
considering the sequential order of items and neglect to handle the temporal
dynamics across items, which can imply evolving user preferences. To address
this limitation, we propose a novel model, Generative Recommender Using Time
awareness (GRUT), effectively capturing hidden user preferences via various
temporal signals. We first introduce Time-aware Prompting, consisting of two
key contexts. The user-level temporal context models personalized temporal
patterns across timestamps and time intervals, while the item-level transition
context provides transition patterns across users. We also devise Trend-aware
Inference, a training-free method that enhances rankings by incorporating trend
information about items with generation likelihood. Extensive experiments
demonstrate that GRUT outperforms state-of-the-art models, with gains of up to
15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The
source code is available at https://github.com/skleee/GRUT.

</details>


### [42] [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
*Silan Hu,Shiqi Zhang,Yimin Shi,Xiaokui Xiao*

Main category: cs.IR

TL;DR: 本文介绍了GEM-Bench，这是第一个针对生成广告注入响应的基准测试，旨在促进GEM领域的研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试并未专门为此目的设计，这限制了未来的研究。

Method: 我们提出了GEM-Bench，这是第一个全面的基准测试，用于GEM中的广告注入响应生成。GEM-Bench包括三个精心策划的数据集，涵盖聊天机器人和搜索场景，一个度量本体，捕捉用户满意度和参与度的多个维度，并在可扩展的多智能体框架中实现了几种基线解决方案。

Result: 初步结果表明，虽然基于提示的方法实现了合理的参与度，如点击率，但它们通常会降低用户满意度。相比之下，基于预先生成的无广告响应插入广告的方法有助于缓解这个问题，但会引入额外的开销。

Conclusion: 这些发现突显了未来研究设计更有效和高效的生成广告注入响应解决方案的必要性。

Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing
generative engines, such as LLM-based chatbots, by seamlessly integrating
relevant advertisements into their responses. At the core of GEM lies the
generation and evaluation of ad-injected responses. However, existing
benchmarks are not specifically designed for this purpose, which limits future
research. To address this gap, we propose GEM-Bench, the first comprehensive
benchmark for ad-injected response generation in GEM. GEM-Bench includes three
curated datasets covering both chatbot and search scenarios, a metric ontology
that captures multiple dimensions of user satisfaction and engagement, and
several baseline solutions implemented within an extensible multi-agent
framework. Our preliminary results indicate that, while simple prompt-based
methods achieve reasonable engagement such as click-through rate, they often
reduce user satisfaction. In contrast, approaches that insert ads based on
pre-generated ad-free responses help mitigate this issue but introduce
additional overhead. These findings highlight the need for future research on
designing more effective and efficient solutions for generating ad-injected
responses in GEM.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [43] [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
*Haolong Zheng,Yekaterina Yegorova,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: 本文提出了一种名为TICL的方法，用于提升语音基础模型在语音上下文学习中的表现。该方法通过利用语义上下文来增强现有大型多模态模型的语音识别能力，无需微调。在多个具有挑战性的自动语音识别任务中，该方法显著提高了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 选择有效的上下文示例对于SICL性能至关重要，但选择方法仍缺乏研究。

Method: 我们提出了TICL，这是一种利用语义上下文来增强现成的大规模多模态模型语音识别能力的简单流程。

Result: 在包括口音英语、多语言语音和儿童语音在内的挑战性自动语音识别任务中，我们的方法使模型的性能超过了零样本性能，相对WER减少了高达84.7%。

Conclusion: 我们的方法在各种具有挑战性的自动语音识别任务中表现出色，能够显著提升模型的性能。

Abstract: Speech foundation models have recently demonstrated the ability to perform
Speech In-Context Learning (SICL). Selecting effective in-context examples is
crucial for SICL performance, yet selection methodologies remain underexplored.
In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline
that uses semantic context to enhance off-the-shelf large multimodal models'
speech recognition ability without fine-tuning. Across challenging automatic
speech recognition tasks, including accented English, multilingual speech, and
children's speech, our method enables models to surpass zero-shot performance
with up to 84.7% relative WER reduction. We conduct ablation studies to show
the robustness and efficiency of our method.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [44] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: TRRIP is a software-hardware co-design approach that optimizes instruction cache replacement by analyzing code temperature, reducing the L2 MPKI for instructions by 26.5% and achieving a geomean speedup of 3.9%.


<details>
  <summary>Details</summary>
Motivation: Modern mobile CPU software pose challenges for conventional instruction cache replacement policies due to their complex runtime behavior causing high reuse distance between executions of the same instruction. Mobile code commonly suffers from large amounts of stalls in the CPU frontend and thus starvation of the rest of the CPU resources. Complexity of these applications and their code footprint are projected to grow at a rate faster than available on-chip memory due to power and area constraints, making conventional hardware-centric methods for managing instruction caches to be inadequate.

Method: TRRIP (Temperature-based Re-Reference Interval Prediction) is a novel software-hardware co-design approach that enables the compiler to analyze, classify, and transform code based on 'temperature' (hot/cold), and to provide the hardware with a summary of code temperature information through a well-defined OS interface based on using code page attributes.

Result: TRRIP's lightweight hardware extension employs code temperature attributes to optimize the instruction cache replacement policy resulting in the eviction rate reduction of hot code. TRRIP is designed to be practical and adoptable in real mobile systems that have strict feature requirements on both the software and hardware components.

Conclusion: TRRIP can reduce the L2 MPKI for instructions by 26.5% resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running mobile code already optimized using PGO.

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [45] [When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training](https://arxiv.org/abs/2509.14132)
*Julia S. Dollis,Iago A. Brito,Fernanda B. Färber,Pedro S. F. B. Ribeiro,Rafael T. Sousa,Arlindo R. Galvão Filho*

Main category: cs.HC

TL;DR: 本文介绍了一个将大型语言模型整合到沉浸式VR中的框架，以创建医学上连贯的虚拟患者，并通过实验验证了其有效性，同时提出了关键的设计原则。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实（VR）在模拟物理环境方面表现出色，但其在训练复杂人际技能方面的有效性受到缺乏心理上可信的虚拟人类的限制。这是高风险领域（如医学教育）中的一个关键差距，其中沟通是一项核心能力。

Method: 本文介绍了一个框架，将大型语言模型（LLMs）整合到沉浸式VR中，以创建医学上连贯的虚拟患者，具有独特且一致的人格，基于模块化架构，将人格与临床数据分离。

Result: 结果表明，该方法不仅可行，而且被医生视为一种高度有益和有效的培训增强手段。此外，我们的分析揭示了关键的设计原则，包括“现实性-冗长性悖论”，即较少交流的代理可能显得更不真实，以及挑战必须被感知为真实的才能有指导意义。

Conclusion: 本文提供了一个经过验证的框架和关键见解，用于开发下一代社会智能VR培训环境。

Abstract: While virtual reality (VR) excels at simulating physical environments, its
effectiveness for training complex interpersonal skills is limited by a lack of
psychologically plausible virtual humans. This is a critical gap in high-stakes
domains like medical education, where communication is a core competency. This
paper introduces a framework that integrates large language models (LLMs) into
immersive VR to create medically coherent virtual patients with distinct,
consistent personalities, built on a modular architecture that decouples
personality from clinical data. We evaluated our system in a mixed-method,
within-subjects study with licensed physicians who engaged in simulated
consultations. Results demonstrate that the approach is not only feasible but
is also perceived by physicians as a highly rewarding and effective training
enhancement. Furthermore, our analysis uncovers critical design principles,
including a ``realism-verbosity paradox" where less communicative agents can
seem more artificial, and the need for challenges to be perceived as authentic
to be instructive. This work provides a validated framework and key insights
for developing the next generation of socially intelligent VR training
environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 本文介绍了一种新的隐私预测框架，利用差分隐私来生成高质量的合成文本，并在保持隐私的同时提高了实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言理解和生成方面取得了显著进展，但由于可能暴露敏感信息而引发了隐私问题。研究已经表明了信息泄露的风险，其中对手可以提取嵌入在提示中的敏感信息。因此，需要一种能够生成高质量合成文本并具有强大隐私保证的框架。

Method: 我们引入了一种新的隐私预测框架，利用差分隐私框架来确保信息泄露的最坏情况理论界限，而无需对底层模型进行微调。该方法对私有记录进行推理并聚合每个标记的输出分布，从而生成更长且连贯的合成文本。此外，我们提出了一种简单的混合操作，将私有和公共推理结合以进一步提高实用性。

Result: 实证评估表明，我们的方法在上下文学习任务中优于之前最先进的方法，能够在保持高实用性的同时实现隐私保护的文本生成。

Conclusion: 我们的方法在上下文学习任务中优于之前最先进的方法，为隐私保护的文本生成提供了有前景的方向，同时保持了高实用性。

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [47] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 研究显示语言模型的激活线性编码了信息的学习时间，这可能对模型处理冲突数据和知识修改有重要影响。


<details>
  <summary>Details</summary>
Motivation: 研究模型是否能区分信息的获取时间，以理解模型如何管理冲突数据和响应知识修改。

Method: 通过依次微调Llama-3.2-1B模型在六个不相交但类似的命名实体数据集上，研究模型激活是否线性编码训练时序信息。

Result: 模型的平均激活在测试样本中编码了训练顺序，线性探针可以准确区分“早期”和“晚期”实体，且能微调模型来报告未见过实体的训练阶段。

Conclusion: 模型能够区分信息的获取时间，这对理解模型如何处理冲突数据和响应知识修改具有重要意义。

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [48] [An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies](https://arxiv.org/abs/2509.12577)
*Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CY

TL;DR: 本文利用大语言模型分析参与式议会的讨论记录，揭示想法的演化和政策建议的形成过程，为理解参与式民主提供了新的实证视角。


<details>
  <summary>Details</summary>
Motivation: 尽管参与式议会被认为是解决复杂全球问题的有效民主论坛，但目前缺乏系统性地追踪想法如何在讨论中演化、优先排序或被舍弃的研究。本文旨在填补这一研究空白。

Method: 本文开发了基于大语言模型的方法，用于分析来自技术增强型线下参与式议会的讨论记录。该框架能够识别和可视化表达的建议空间，并重新构建每个代表在整个议会过程中不断变化的观点。

Result: 本文的方法为参与式民主过程提供了新的实证见解，并展示了大语言模型如何揭示传统会议输出中无法看到的高分辨率动态。

Conclusion: 本文提出了基于大语言模型的方法，用于分析参与式议会的讨论记录，从而揭示想法的演变和政策建议的形成过程。这种方法为理解参与式民主过程提供了新的实证见解，并展示了大语言模型在传统会议输出中无法捕捉的高分辨率动态。

Abstract: In an era of increasing societal fragmentation, political polarization, and
erosion of public trust in institutions, representative deliberative assemblies
are emerging as a promising democratic forum for developing effective policy
outcomes on complex global issues. Despite theoretical attention, there remains
limited empirical work that systematically traces how specific ideas evolve,
are prioritized, or are discarded during deliberation to form policy
recommendations. Addressing these gaps, this work poses two central questions:
(1) How might we trace the evolution and distillation of ideas into concrete
recommendations within deliberative assemblies? (2) How does the deliberative
process shape delegate perspectives and influence voting dynamics over the
course of the assembly? To address these questions, we develop LLM-based
methodologies for empirically analyzing transcripts from a tech-enhanced
in-person deliberative assembly. The framework identifies and visualizes the
space of expressed suggestions. We also empirically reconstruct each delegate's
evolving perspective throughout the assembly. Our methods contribute novel
empirical insights into deliberative processes and demonstrate how LLMs can
surface high-resolution dynamics otherwise invisible in traditional assembly
outputs.

</details>


### [49] [Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI](https://arxiv.org/abs/2509.13345)
*Zihao Li,Weiwei Yi,Jiahong Chen*

Main category: cs.CY

TL;DR: 本文探讨了大型语言模型中的幻觉问题，指出对准确性的过度依赖可能导致误判，并呼吁转向更全面的AI可信治理方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型渗透到日常决策中，其认识论和社会风险需要紧急审查。幻觉已成为重要的挑战，而目前对准确性的过度依赖可能误诊问题并产生反效果。

Method: 本文基于跨学科文献，开发了一种幻觉类型的分类法，并展示了在输出、个体和社会三个交织维度上的悖论。

Result: 本文揭示了准确性作为单一指标的局限性，并指出当前法规（如欧盟人工智能法案、GDPR和DSA）尚未具备解决这些认识论、关系性和系统性危害的能力。

Conclusion: 本文呼吁向多元化、情境意识强且能抵御操纵的AI可信治理方法转变。

Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their
epistemic and societal risks demand urgent scrutiny. Hallucinations, the
generation of fabricated, misleading, oversimplified or untrustworthy outputs,
has emerged as imperative challenges. While regulatory, academic, and technical
discourse position accuracy as the principal benchmark for mitigating such
harms, this article contends that overreliance on accuracy misdiagnoses the
problem and has counterproductive effect: the accuracy paradox. Drawing on
interdisciplinary literatures, this article develops a taxonomy of
hallucination types and shows the paradox along three intertwining dimensions:
outputs, individuals and society. First, accuracy functions as a superficial
proxy for reliability, incentivising the optimisation of rhetorical fluency and
surface-level correctness over epistemic trustworthiness. This encourages
passive user trust in outputs that appear accurate but epistemically untenable.
Second, accuracy as a singular metric fails to detect harms that are not
factually false but are nonetheless misleading, value-laden, or socially
distorting, including consensus illusions, sycophantic alignment, and subtle
manipulation. Third, regulatory overemphasis on accuracy obscures the wider
societal consequences of hallucination, including social sorting, privacy
violations, equity harms, epistemic convergence that marginalises dissent,
reduces pluralism, and causes social deskilling. By examining the EU AI Act,
GDPR, and DSA, the article argues that current regulations are not yet
structurally equipped to address these epistemic, relational, and systemic
harms and exacerbated by the overreliance on accuracy. By exposing such
conceptual and practical challenges, this article calls for a fundamental shift
towards pluralistic, context-aware, and manipulation-resilient approaches to AI
trustworthy governance.

</details>


### [50] [CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI](https://arxiv.org/abs/2509.13356)
*Hasin Jawad Ali,Ilhamul Azam,Ajwad Abrar,Md. Kamrul Hasan,Hasan Mahmud*

Main category: cs.CY

TL;DR: This paper introduces CogniAlign, a multi-agent framework that improves AI alignment with human values by using interdisciplinary deliberation and shows superior performance over GPT-4o in moral reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The challenge of aligning AI with human values persists due to the abstract and conflicting nature of moral principles and the opacity of existing approaches.

Method: CogniAlign is a multi-agent deliberation framework based on naturalistic moral realism, grounding moral reasoning in survivability and operationalizing it through structured deliberations among discipline-specific scientist agents.

Result: CogniAlign consistently outperforms GPT-4o across more than sixty moral questions, with average performance gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4 points in depth of explanation. In the Heinz dilemma, CogniAlign achieved an overall score of 89.2 compared to GPT-4o's 69.2.

Conclusion: CogniAlign demonstrates the potential of interdisciplinary deliberation as a scalable pathway for safe and transparent AI alignment by reducing black-box reasoning and avoiding deceptive alignment.

Abstract: The challenge of aligning artificial intelligence (AI) with human values
persists due to the abstract and often conflicting nature of moral principles
and the opacity of existing approaches. This paper introduces CogniAlign, a
multi-agent deliberation framework based on naturalistic moral realism, that
grounds moral reasoning in survivability, defined across individual and
collective dimensions, and operationalizes it through structured deliberations
among discipline-specific scientist agents. Each agent, representing
neuroscience, psychology, sociology, and evolutionary biology, provides
arguments and rebuttals that are synthesized by an arbiter into transparent and
empirically anchored judgments. We evaluate CogniAlign on classic and novel
moral questions and compare its outputs against GPT-4o using a five-part
ethical audit framework. Results show that CogniAlign consistently outperforms
the baseline across more than sixty moral questions, with average performance
gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4
points in depth of explanation. In the Heinz dilemma, for example, CogniAlign
achieved an overall score of 89.2 compared to GPT-4o's 69.2, demonstrating a
decisive advantage in handling moral reasoning. By reducing black-box reasoning
and avoiding deceptive alignment, CogniAlign highlights the potential of
interdisciplinary deliberation as a scalable pathway for safe and transparent
AI alignment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [51] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 本文分析了自动化问题解决工具的性能，并提出了一个协作的专家-执行器框架，以提高问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 当前的评估主要报告总体的问题解决率，这掩盖了成功和失败的根本原因，使得诊断模型弱点或指导有针对性的改进变得困难。

Method: 我们首先分析了三种最先进的工具在SWE-Bench-Verified中的性能和效率，然后对150个失败实例进行了系统的手动分析，以开发一个全面的失败模式分类法，并提出了一个协作的专家-执行器框架。

Result: 我们的框架解决了之前难以解决的22.2%的问题，结果揭示了两种架构范式的不同失败特征，大多数代理失败源于错误的推理和认知死锁。

Conclusion: 这些发现为通过诊断评估和协作设计构建更强大的代理铺平了道路。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [52] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: 研究发现较长的 Chain-of-Thought (CoT) 推理并不总是更好，提出 SEER 自适应框架来压缩 CoT 以提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 研究 CoT 在软件工程任务中的权衡，发现较长的 CoT 不总是有益的，需要自适应的 CoT 控制。

Method: SEER 结合了 Best-of-N 抽样和任务感知的自适应过滤，根据预推理输出动态调整阈值以减少冗余和计算开销。

Result: SEER 平均将 CoT 缩短 42.1%，提高准确性，减少截断，并消除大多数无限循环。

Conclusion: SEER 是一种实用的方法，使增强 CoT 的 LLM 更高效和稳健，即使在资源受限的情况下也是如此。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 本文提出了一种使用地球观测卫星图像对检测亚马逊雨林砍伐的方法。我们的方法利用深度学习技术比较同一区域在不同日期的图像，并识别森林覆盖的变化。我们还提出了一种视觉语义模型，可自动用相关关键词标注检测到的变化。我们在亚马逊图像对的数据集上评估了我们的方法，并展示了其在检测砍伐和生成相关注释方面的有效性。我们的方法为监测和研究亚马逊雨林的砍伐影响提供了有用的工具。虽然我们专注于环境应用，但该方法具有通用性，可以应用于其他领域。


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林是一个关键的生态系统，对调节地球气候和提供无数物种的栖息地至关重要。亚马逊的砍伐是一个重大问题，因为它对全球碳排放和生物多样性有显著影响。

Method: 我们提出了一种使用地球观测卫星图像对检测亚马逊雨林砍伐的方法。我们的方法利用深度学习技术比较同一区域在不同日期的图像，并识别森林覆盖的变化。我们还提出了一种视觉语义模型，可自动用相关关键词标注检测到的变化。

Result: 我们在亚马逊图像对的数据集上评估了我们的方法，并展示了其在检测砍伐和生成相关注释方面的有效性。

Conclusion: 我们的方法为监测和研究亚马逊雨林的砍伐影响提供了有用的工具。虽然我们专注于环境应用，但该方法具有通用性，可以应用于其他领域。

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [54] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: 本文研究了大型视觉语言模型中的对象幻觉问题，并提出了一个名为VisionWeaver的新方法来减少幻觉并提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 对象幻觉在大型视觉语言模型中显著阻碍了它们的实际应用。不同的视觉编码器采用的多样化训练范式赋予它们不同的归纳偏差，这导致了它们不同的幻觉表现。现有的基准测试通常集中在粗粒度的幻觉检测上，未能捕捉到我们假设中详述的多样化幻觉。

Method: 我们提出了VisionWeaver，这是一种新的上下文感知路由网络，它利用全局视觉特征生成路由信号，动态聚合来自多个专业专家的视觉特征。

Result: 我们的评估确认了编码器表现出独特的幻觉特征。通过这些见解以及简单特征融合的次优性，我们提出了VisionWeaver。

Conclusion: 我们的实验确认了VisionWeaver在显著减少幻觉和提高整体模型性能方面的有效性。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [55] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 本文介绍了Dense Video Understanding (DVU)，通过减少标记化时间和标记开销实现了高帧率视频理解。我们提出了DIVE基准测试，用于密集时间推理。此外，我们提出了Gated Residual Tokenization (GRT)框架，该框架通过运动补偿的跨门控标记化和语义场景内标记化合并来减少冗余并提高效率。实验结果表明，GRT在DIVE上表现优于更大的VLLM基线，并且随着FPS的增加而积极扩展。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型（VLLMs）和基准测试主要依赖低帧率采样，如均匀采样或关键帧选择，这会丢弃密集的时间信息。这种权衡避免了对每一帧进行标记化的高成本，否则会导致冗余计算和随着视频长度增加的线性标记增长。然而，对于像讲座理解这样的任务，信息几乎出现在每一帧中，需要精确的时间对齐。因此，我们需要一种方法来解决这一差距。

Method: 我们提出了Gated Residual Tokenization (GRT)，这是一种两阶段框架：(1) 运动补偿的跨门控标记化利用像素级运动估计在标记化过程中跳过静态区域，实现标记数和计算的次线性增长。(2) 语义场景内标记化合并融合同一场景中的静态区域的标记，进一步减少冗余同时保留动态语义。

Result: 在DIVE上的实验表明，GRT优于更大的VLLM基线，并且随着FPS的增加而积极扩展。

Conclusion: 这些结果强调了密集时间信息的重要性，并表明GRT使高效、可扩展的高帧率视频理解成为可能。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 本文比较了LLM作为评判者范式中的“思考”和“非思考”模型，发现思考模型在准确率、效率和鲁棒性方面都有显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地被用作自动评判者进行基准测试和奖励建模，确保其可靠性、效率和鲁棒性变得至关重要。

Method: 我们使用开源的Qwen 3模型（0.6B、1.7B和4B参数）对“思考”和“非思考”LLM在LLM作为评判者范式中的表现进行了系统比较。我们评估了准确性和计算效率（FLOPs），并进一步研究了非思考模型的增强策略，包括上下文学习、评分标准引导的评判、基于参考的评估和n-best聚合。

Result: 结果表明，尽管进行了这些增强，非思考模型通常仍不如其思考 counterparts。思考模型在几乎没有额外开销（低于2倍）的情况下，准确率大约高出10个百分点。相比之下，像少样本学习这样的增强策略带来的收益较小，但成本更高（超过8倍）。偏见和鲁棒性分析进一步表明，思考模型在各种偏见条件下（如位置、从众、身份、多样性、随机偏见）表现出更高的稳定性（平均高出6%）。此外，我们将实验扩展到多语言设置，结果证实了显式推理的好处不仅限于英语。

Conclusion: 我们的工作得出了几个重要的发现，这些发现提供了系统性的证据，表明在LLM作为评判者的范式中，显式的推理不仅在准确性、效率方面有明显的优势，而且在鲁棒性方面也有优势。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [57] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 本文提出了一种新的指令调优框架PDDL-Instruct，旨在通过逻辑链式思维推理来增强大型语言模型的符号规划能力。实验结果表明，该方法在规划任务中表现优异，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多样化任务中表现出色，但在需要形式化表示的领域（如规划域定义语言PDDL）中，其执行结构化符号规划的能力仍然有限。

Method: 本文提出了一种新的指令调优框架PDDL-Instruct，旨在通过逻辑链式思维推理来增强大型语言模型的符号规划能力。我们的方法专注于通过显式的逻辑推理步骤来教授模型关于动作适用性、状态转换和计划有效性的严格推理。

Result: 实验结果表明，基于链式思维推理的指令调优模型在规划方面表现显著更好，在标准基准测试中实现了高达94%的规划准确性，比基线模型提高了66%的绝对值。

Conclusion: 本工作弥合了大型语言模型的一般推理能力与自动化规划所需的逻辑精度之间的差距，为开发更好的AI规划系统提供了有前景的方向。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [58] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: 我们引入了SteeringControl，一个用于评估表示控制方法在核心对齐目标（偏差、有害生成和幻觉）及其对次级行为（如奉承和常识道德）影响的基准。我们的结果表明，强大的控制性能依赖于特定的控制方法、模型和目标行为的组合，并且这些三者之间的不良组合可能导致严重的概念纠缠。


<details>
  <summary>Details</summary>
Motivation: 先前的对齐工作通常强调真实性或推理能力来展示表示控制的副作用，但我们发现有许多未探索的权衡尚未以系统的方式理解。

Method: 我们引入了SteeringControl，这是一个用于评估跨核心对齐目标（偏差、有害生成和幻觉）的表示控制方法的基准，并研究它们对次级行为（如奉承和常识道德）的影响。我们收集了一个与安全相关的主要和次级行为的数据集来评估控制效果和行为纠缠，并基于独特的组件构建了一个模块化的控制框架。

Result: 我们在Qwen-2.5-7B和Llama-3.1-8B上的结果表明，强大的控制性能依赖于特定的控制方法、模型和目标行为的组合，并且这些三者之间的不良组合可能导致严重的概念纠缠。

Conclusion: 我们的结论是，强大的控制性能依赖于特定的控制方法、模型和目标行为的组合，并且这些三者之间的不良组合可能导致严重的概念纠缠。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [59] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: This paper proposes StaR, a training method that improves the ability of multimodal agents to execute toggle instructions accurately, with significant improvements in both specific tasks and general performance.


<details>
  <summary>Details</summary>
Motivation: The inability of multimodal agents to reliably execute toggle control instructions remains a key bottleneck in ubiquitous GUI control.

Method: We propose State-aware Reasoning (StaR), a training method that teaches agents to perceive the current toggle state, analyze the desired state from the instruction, and act accordingly.

Result: Experiments on three multimodal agents demonstrate that StaR can improve toggle instruction execution accuracy by over 30%. Further evaluations on three public benchmarks show that StaR also enhances general task performance. Evaluations on a dynamic environment highlight the potential of StaR for real-world applications.

Conclusion: StaR can improve toggle instruction execution accuracy by over 30%, and also enhances general task performance. It has potential for real-world applications.

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [60] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: 本文提出THOR，通过工具集成的分层优化提升大型语言模型在数学推理中的表现，解决了数据构建、优化和推理增强的问题，并在多个基准测试中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有方法在构建工具集成推理数据、执行细粒度优化和增强推理方面面临三个关键挑战。

Method: 我们提出了THOR（通过强化学习进行工具集成的分层优化），包括TIRGen数据集构建管道、联合优化轨迹级问题解决和步骤级代码生成的强化学习策略，以及利用即时工具反馈动态修正错误推理路径的自我纠正机制。

Result: 我们的方法在多种模型上表现出强大的泛化能力，在推理和非推理模型中均表现良好。它在相似规模的模型上实现了最先进的性能，并在代码基准测试中持续改进。

Conclusion: 我们的方法在多种数学基准测试中表现出色，并在代码基准测试中实现了稳定的改进。

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [61] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: 研究显示，信息流的变化可能导致认知表现的转变，递归网络在处理复杂语法时表现更好，但并非所有网络拓扑变化都能带来性能优势。


<details>
  <summary>Details</summary>
Motivation: 我们想了解认知是否可能通过一系列主要过渡来进化，这些过渡会改变生物神经网络的结构，从而根本改变信息流。

Method: 我们使用了理想化的信息流模型和人工神经网络（ANNs）来评估网络中的信息流变化是否能产生认知表现的过渡变化。

Result: 我们发现递归网络在处理输入类型方面有定性扩展，并且在学习最复杂的语法时表现出性能的定性提高。同时，训练递归网络的难度形成了一种过渡障碍和条件不可逆性。

Conclusion: 我们的研究结果表明，信息流的变化可以导致认知表现的转变。

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>

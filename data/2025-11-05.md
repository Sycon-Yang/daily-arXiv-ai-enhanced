<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.CV](#cs.CV) [Total: 6]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Multi-Personality Generation of LLMs at Decoding-time](https://arxiv.org/abs/2511.01891)
*Rongxin Chen,Yunfan Li,Yige Yuan,Bingbing Xu,Huawei Shen*

Main category: cs.CL

TL;DR: 本文提出了一种新的多个性生成框架MPG，通过解码时间组合范式实现了高效的多个性生成，并在多个任务中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重新训练的方法成本高且可扩展性差，而解码时间的方法通常依赖外部模型或启发式方法，限制了灵活性和鲁棒性。因此，需要一种更高效、灵活的方法来实现多个性生成。

Method: 本文提出了一种基于解码时间组合范式的多个性生成（MPG）框架。为了高效实现MPG，设计了基于推测块级拒绝采样的方法（SCR），通过估计阈值在滑动窗口内并行验证生成的响应。

Result: 实验结果显示，MPG在MBTI人格和角色扮演任务中取得了显著效果，性能提升了16%-18%。

Conclusion: 实验结果表明，MPG在MBTI人格和角色扮演任务中表现出色，提升了16%-18%。代码和数据已发布在GitHub上。

Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of
multiple personalization attributes, is a fundamental challenge. Existing
retraining-based approaches are costly and poorly scalable, while decoding-time
methods often rely on external models or heuristics, limiting flexibility and
robustness. In this paper, we propose a novel Multi-Personality Generation
(MPG) framework under the decoding-time combination paradigm. It flexibly
controls multi-personality without relying on scarce multi-dimensional models
or extra training, leveraging implicit density ratios in single-dimensional
models as a "free lunch" to reformulate the task as sampling from a target
strategy aggregating these ratios. To implement MPG efficiently, we design
Speculative Chunk-level based Rejection sampling (SCR), which generates
responses in chunks and parallelly validates them via estimated thresholds
within a sliding window. This significantly reduces computational overhead
while maintaining high-quality generation. Experiments on MBTI personality and
Role-Playing demonstrate the effectiveness of MPG, showing improvements up to
16%-18%. Code and data are available at https://github.com/Libra117/MPG .

</details>


### [2] [Rethinking LLM Human Simulation: When a Graph is What You Need](https://arxiv.org/abs/2511.02135)
*Joseph Suh,Suhong Moon,Serina Chang*

Main category: cs.CL

TL;DR: 研究提出了一种基于图的模型GEMS，用于模拟人类行为，其性能与大型语言模型相当，但更高效、可解释性更强。


<details>
  <summary>Details</summary>
Motivation: 探讨是否需要使用大型语言模型（LLMs）来模拟人类行为，或者较小的、领域特定的模型是否足够。

Method: 将离散选择模拟任务转化为图上的链接预测问题，利用关系知识并在需要时结合语言表示。

Result: GEMS在三个关键设置的三个模拟数据集上实现了与LLMs相当或更好的准确性，同时具有更高的效率、可解释性和透明度。

Conclusion: 图-based建模作为LLMs的人类模拟轻量级替代方案显示出潜力。

Abstract: Large language models (LLMs) are increasingly used to simulate humans, with
applications ranging from survey prediction to decision-making. However, are
LLMs strictly necessary, or can smaller, domain-grounded models suffice? We
identify a large class of simulation problems in which individuals make choices
among discrete options, where a graph neural network (GNN) can match or surpass
strong LLM baselines despite being three orders of magnitude smaller. We
introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete
choice simulation tasks as a link prediction problem on graphs, leveraging
relational knowledge while incorporating language representations only when
needed. Evaluations across three key settings on three simulation datasets show
that GEMS achieves comparable or better accuracy than LLMs, with far greater
efficiency, interpretability, and transparency, highlighting the promise of
graph-based modeling as a lightweight alternative to LLMs for human simulation.
Our code is available at https://github.com/schang-lab/gems.

</details>


### [3] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: 本文提出了一种新的输入感知块级剪枝方法IG-Pruning，能够在推理时动态选择层掩码，从而提高大语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）计算需求的增长，高效推理对于实际部署变得越来越重要。深度剪枝已成为减少大语言模型计算成本的有前途的方法，但现有方法通常依赖于固定的块掩码，这可能导致不同任务和输入下的次优性能。

Method: IG-Pruning是一种输入感知的块级剪枝方法，在推理时动态选择层掩码。该方法包括两个阶段：(1) 通过语义聚类和L0优化发现多样化的掩码候选；(2) 在不需要大量训练的情况下实现高效的动态剪枝。

Result: 实验结果表明，我们的方法在静态深度剪枝方法上表现更优，特别适合资源受限的部署场景。

Conclusion: 实验结果表明，我们的方法在静态深度剪枝方法上表现更优，特别适合资源受限的部署场景。

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [4] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 研究开发了一个基础设施，用于探测LLM并评估其答案，发现LLM标注者的协议度较低，建议使用多个LLM作为评估者以避免统计上显著但不具有普遍性的结果。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人在涉及非医疗因素的情况下必须提供一致的建议，例如当存在人口统计数据时。了解医疗聊天机器人未能按预期执行的条件非常重要。

Method: 研究开发了一个基础设施，用于自动生成查询来探测LLM，并使用多个LLM-as-a-judge设置和提示来评估这些查询的答案。

Result: LLM标注者的协议度得分较低（平均Cohen's Kappa κ=0.118），只有特定的（回答，评估）LLM对在不同写作风格、性别和种族中显示出统计学上的显著差异。

Conclusion: 研究建议在使用LLM评估时使用多个LLM作为评估者，以避免得出统计上显著但不具有普遍性的结果，特别是在缺乏真实数据的情况下。还建议公开LLM之间的协议度指标以提高透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [5] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: LTD-Bench is a new benchmark that evaluates LLMs by requiring them to generate visual outputs, revealing their spatial reasoning limitations and providing a more intuitive assessment of their capabilities.


<details>
  <summary>Details</summary>
Motivation: Current evaluation paradigms for large language models (LLMs) represent a critical blind spot in AI research, as they rely on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities.

Method: LTD-Bench transforms LLM evaluation from abstract scores to directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code. It implements a comprehensive methodology with complementary generation tasks and recognition tasks across three progressively challenging difficulty levels.

Result: Extensive experiments with state-of-the-art models expose an alarming capability gap: even LLMs achieving impressive results on traditional benchmarks demonstrate profound deficiencies in establishing bidirectional mappings between language and spatial concepts.

Conclusion: LTD-Bench's visual outputs enable powerful diagnostic analysis, offering a potential approach to investigate model similarity.

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [6] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: M-Solomon is a multimodal embedder that adaptively determines when to augment queries, leading to improved performance and faster embedding latency.


<details>
  <summary>Details</summary>
Motivation: Current studies have shown that query augmentation can be detrimental to performance for some queries and leads to substantial embedding latency. Additionally, previous methods have not been explored in multimodal environments.

Method: M-Solomon is a universal multimodal embedder that can adaptively determine when to augment queries. It divides queries into two groups at the dataset level, introduces a synthesis process for generating appropriate augmentations, and presents adaptive query augmentation.

Result: Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.

Conclusion: M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [7] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li,Zhongliang Yang,Kejiang Chen,Wenxuan Wang,Tianxin Zhang,Sifang Wan,Kecheng Wang,Haitian Li,Xu Wang,Lefan Cheng,Youdan Yang,Baocheng Chen,Ziyu Liu,Yufei Sun,Liyan Wu,Wenya Wen,Xingchi Gu,Peiru Yang*

Main category: cs.CL

TL;DR: LiveSecBench 是一个针对中文语言大模型的安全基准，涵盖多个关键维度，能够动态更新以适应新的威胁，已评估18个大模型并提供公开排行榜。


<details>
  <summary>Details</summary>
Motivation: 为了应对中文语言大模型在实际应用中可能面临的安全挑战，需要一个专门的基准来评估和提升大模型的安全性能。

Method: LiveSecBench 通过六个关键维度（合法性、伦理、事实性、隐私、对抗鲁棒性和推理安全）对模型进行评估，并通过动态更新计划保持其相关性，以纳入新的威胁向量。

Result: LiveSecBench (v251030) 已经评估了18个大模型，提供了中文语境下AI安全的全景图，并且排行榜是公开可访问的。

Conclusion: LiveSecBench 是一个动态且持续更新的安全基准，专门针对中文语言大模型应用环境，有助于评估大模型在法律、伦理、事实性、隐私、对抗鲁棒性和推理安全等方面的表现。

Abstract: In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.

</details>


### [8] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: AyurParam-2.9B 是一个经过微调的领域专业化双语语言模型，使用了专家精心策划的阿育吠陀数据集，在特定基准测试中表现出色，显示出在专业医学知识中进行领域适应和高质量监督的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理需要深厚文化、语言和专业知识的特殊领域时表现不佳，而传统医学系统如阿育吠陀蕴含着数世纪的细致文本和临床知识，主流 LLM 无法准确解释或应用这些知识。

Method: AyurParam-2.9B 是一个经过微调的领域专业化双语语言模型，使用了一个广泛且由专家精心策划的阿育吠陀数据集，该数据集涵盖了经典文本和临床指南，并包含了上下文感知、推理和客观风格的问答。

Result: AyurParam 在 BhashaBench-Ayur 上不仅超越了其规模类别（1.5-3B 参数）的所有开源指令调优模型，还展示了与更大模型竞争或优越的性能。

Conclusion: AyurParam 的结果突显了在提供可靠、文化相符的专门医学知识人工智能时，进行真实领域适应和高质量监督的必要性。

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [9] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: 研究提出了一种自动化多轮越狱攻击框架AutoAdv，展示了多轮攻击在多个模型上的高成功率，并强调了需要多轮意识防御的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估主要集中在单次交互上，而现实世界中的攻击是通过自适应的多轮对话进行的，因此需要一种新的方法来评估和防御多轮攻击。

Method: AutoAdv框架结合了三种自适应机制：模式管理器、温度管理器和两阶段重写策略，以实现自动化的多轮越狱攻击。

Result: AutoAdv在Llama-3.1-8B上实现了高达95%的攻击成功率，比单次基线提高了24%。此外，在商业和开源模型上的广泛评估揭示了当前安全机制的持续脆弱性。

Conclusion: 研究结果表明，针对单次交互优化的对齐策略在长时间对话中无法保持鲁棒性，突显了需要多轮意识防御的紧迫性。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [10] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: 本文研究了CPT模型合并，提出了一种三阶段评估方法，并在金融领域测试了三种合并方法。结果表明，合并专家可以提高性能并产生跨领域技能，其中TIES方法更为稳健。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在一般任务中表现出色，但在像金融这样的专业领域中却表现不佳，需要多样化的领域知识、数学推理和多语言处理技能。合并领域特定的持续预训练（CPT）“专家”提供了一种比昂贵且不稳定的多技能训练更实用的替代方案。然而，与已建立的基于监督微调（SFT）的模型合并不同，CPT模型合并仍 largely 未被探索。

Method: 我们创建了金融LLM，从金融、数学和日语专家中进行合并，并提出了一个三阶段评估，重点关注知识恢复、互补性和涌现性。评估了三种合并方法（Task Arithmetic、TIES和DARE-TIES）在一个由18个任务组成的综合金融基准测试集上。

Result: 结果表明，将专家与其基础模型合并可以恢复CPT期间丢失的一般知识，而合并专家可以提高性能并可能产生跨领域的技能。在这些方法中，Task Arithmetic表现强劲但对超参数敏感，而TIES更稳健。我们的研究还表明，虽然模型相似性与合并成功率相关，但涌现技能取决于更复杂的因素。

Conclusion: 本文提出了对CPT模型合并的第一个基础分析，建立了一个有原则的框架，并为从现有资源中构建多技能LLM提供了明确的指导。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [11] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: 研究评估了基于角色的提示是否能提高大型语言模型在宏观经济预测任务中的表现。结果显示，GPT-4o的预测准确性与人类专家相当，且在提供相关上下文数据的情况下，能够保持具有竞争力的预测性能。此外，研究发现角色描述对预测没有显著影响，可以省略以减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 我们评估基于角色的提示是否能提高大型语言模型（LLM）在宏观经济预测任务中的表现。

Method: 我们使用来自PersonaHub语料库的2,368个与经济学相关的角色，让GPT-4o复制ECB专业预测者调查，跨越50个季度轮次（2013-2025）。我们将基于角色的提示预测与人类专家小组进行比较，并对四个目标变量（HICP、核心HICP、GDP增长、失业率）和四个预测时间范围进行比较。我们还与100个没有角色描述的基线预测进行比较，以隔离其影响。

Result: 首先，GPT-4o和人类预测者的准确性水平非常相似，差异在统计学上显著但实际影响较小。我们在2024-2025年的数据上进行了样本外评估，证明GPT-4o可以在未见过的事件中保持具有竞争力的预测性能，尽管与样本内期间有明显差异。其次，我们的消融实验显示，角色描述没有可测量的预测优势，这表明可以省略这些提示组件以减少计算成本而不牺牲准确性。

Conclusion: 我们的结果表明，GPT-4o即使在提供相关上下文数据的情况下，也能在宏观经济事件上实现具有竞争力的预测准确性，同时揭示了多样化的提示会产生与人类小组相比非常一致的预测。

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [12] [Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching](https://arxiv.org/abs/2511.02537)
*Kenza Khelkhal,Dihia Lanasri*

Main category: cs.CL

TL;DR: 本文提出了一种基于自然语言处理的智能招聘系统，用于自动提取简历信息并匹配候选人与职位描述，提高了招聘效率并减少了人为偏见。


<details>
  <summary>Details</summary>
Motivation: 招聘过程通常涉及手动筛选大量简历，这一任务耗时且容易出错，并且容易受到人类偏见的影响。因此，需要一种自动化的方法来提高效率和减少偏差。

Method: Smart-Hiring结合了文档解析、命名实体识别和上下文文本嵌入技术，以捕获技能、经验和资格。通过先进的自然语言处理技术，将简历和职位描述编码到共享向量空间中，计算候选人与职位发布之间的相似性得分。

Result: 在涵盖多个专业领域的实际数据集上进行了实验，证明了所提出方法的鲁棒性和可行性。系统在保持高度可解释性和透明度的同时，实现了竞争性的匹配准确性。

Conclusion: 本文介绍了Smart-Hiring系统，这是一个基于自然语言处理的端到端管道，能够自动从非结构化简历中提取结构化信息，并对候选人和职位描述进行语义匹配。实验表明该方法具有鲁棒性和可行性，同时在决策过程中保持了高度的可解释性和透明性。

Abstract: Hiring processes often involve the manual screening of hundreds of resumes
for each job, a task that is time and effort consuming, error-prone, and
subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural
Language Processing (NLP) pipeline de- signed to automatically extract
structured information from unstructured resumes and to semantically match
candidates with job descriptions. The proposed system combines document
parsing, named-entity recognition, and contextual text embedding techniques to
capture skills, experience, and qualifications. Using advanced NLP technics,
Smart-Hiring encodes both resumes and job descriptions in a shared vector space
to compute similarity scores between candidates and job postings. The pipeline
is modular and explainable, allowing users to inspect extracted entities and
matching rationales. Experiments were conducted on a real-world dataset of
resumes and job descriptions spanning multiple professional domains,
demonstrating the robustness and feasibility of the proposed approach. The
system achieves competitive matching accuracy while preserving a high degree of
interpretability and transparency in its decision process. This work introduces
a scalable and practical NLP frame- work for recruitment analytics and outlines
promising directions for bias mitigation, fairness-aware modeling, and
large-scale deployment of data-driven hiring solutions.

</details>


### [13] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 该研究分析了Google Translate在将英文官方信息（如与新冠相关的文本）翻译成罗马尼亚语时的词汇错误，并试图提高机器翻译的质量。


<details>
  <summary>Details</summary>
Motivation: 研究目的是改进Google Translate，以提高机器翻译的质量。

Method: 该研究对230篇从英语翻译成罗马尼亚语的文本进行了全面分析。

Result: 研究发现了一些在翻译官方信息时出现的词汇错误。

Conclusion: 该研究旨在通过改善词法选择和减少错误来提高机器翻译的整体质量。

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [14] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: 本文提出了一种新的知识追踪方法NTKT，通过利用预训练的语言模型，将学生历史和问题内容表示为文本序列，从而提高预测性能并更好地适应冷启动问题和用户。


<details>
  <summary>Details</summary>
Motivation: 现有的知识追踪模型通常只使用响应正确性以及元数据，而忽略了问题文本，这可能是一个被忽视的重要教学见解来源，导致预测性能受限。

Method: NTKT将知识追踪任务重新定义为一个下一个标记预测任务，使用预训练的大语言模型（LLMs），将学生历史和问题内容表示为文本序列，使LLMs能够学习行为和语言中的模式。

Result: 实验结果表明，NTKT在最先进的神经知识追踪模型上取得了显著的性能提升，并且在冷启动问题和用户上具有更好的泛化能力。

Conclusion: NTKT方法通过利用预训练的语言模型，显著提高了知识追踪的性能，并且在冷启动问题和用户上表现更好，证明了问题内容在知识追踪中的重要性。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [15] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: CGES是一种基于置信度的早期停止方法，用于减少大型语言模型在测试时的调用次数，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的自一致性策略需要固定数量的调用，并且当正确答案罕见时可能会失败。

Method: CGES是一种贝叶斯框架，通过从标记概率或奖励模型中得出的标量置信度信号形成候选答案的后验分布，并在候选答案的后验质量超过阈值时自适应地停止采样。

Result: CGES在五个推理基准测试中减少了平均模型调用次数约69%（例如，从16.0到4.9），同时保持了与自一致性相当的准确性。

Conclusion: CGES在五个推理基准测试中减少了平均模型调用次数约69%，同时保持了与自一致性的准确性相当。

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [16] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: 本文提出了一种名为TRACE的框架，用于解决大型语言模型与人类价值观之间的对齐问题。该框架通过程序化政策应用的方式，实现了精确的重新对齐，从而为可持续和负责任的人工智能部署提供了基础。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐实践导致静态、脆弱且维护成本高的模型，无法跟上不断变化的规范和政策。这种对齐-现实差距成为长期可靠使用的主要挑战。现有的解决方案不足：大规模重新标注在经济上不可行，而标准的遗忘方法会损害实用性而不是实现精确的政策更新。

Method: TRACE框架通过程序化地对现有偏好数据进行分类，识别高影响冲突，并应用混合优化方法来干净地反转、丢弃或保留偏好，同时保护模型性能。

Result: 实证结果表明，TRACE在多种模型家族中实现了稳健的重新对齐。在合成基准和PKU-SafeRLHF数据集上的实验表明，TRACE在不降低通用能力的情况下强制执行新原则。

Conclusion: 本文提出了TRACE框架，以解决大型语言模型（LLMs）与人类价值观之间的对齐问题。该框架通过程序化政策应用的方式，实现了精确的重新对齐，从而为可持续和负责任的人工智能部署提供了基础。

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [17] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 本文通过设计一个受控数据集并进行细粒度分析，发现特定知识类型的高不熟悉度是导致幻觉的主要原因，并提出KnownPatch方法来减轻新知识引起的幻觉。


<details>
  <summary>Details</summary>
Motivation: 先前的研究没有深入研究这些幻觉的具体表现和潜在机制，因此我们设计了一个受控数据集Biography-Reasoning，并进行了细粒度分析。

Method: 我们提出了KnownPatch，在训练后期插入少量已知知识样本，以减轻新知识引起的事实幻觉。

Result: 当微调的数据集中特定知识类型完全由新知识组成时，LLMs表现出显著增加的幻觉倾向。这表明特定知识类型的高不熟悉度是幻觉的更强驱动因素，这些倾向甚至会影响QA任务中的其他知识类型。

Conclusion: 我们的方法有效缓解了新知识学习对模型关注关键实体的干扰，同时提高了性能。

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [18] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.CL

TL;DR: 本文提出了一种高效存储微调模型参数更新的方法，通过结合低秩近似和稀疏化，提高了存储效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于微调模型的存储挑战，需要更高效的存储方法来保存参数更新。

Method: 本文利用微调更新的低秩和稀疏特性，提出了一种基于奇异向量重要性的选择性稀疏化方法，以确保保留最具影响力的组件。

Result: 实验表明，本文的方法在相同的内存预算下，比单独使用低秩近似或稀疏化方法具有更高的存储效率和更好的准确性。

Conclusion: 本文提出了一种名为最优奇异损伤的方法，通过选择性地对低秩近似更新进行稀疏化，实现了更高效的存储和更高的准确性。

Abstract: Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [19] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: 本文介绍了PragExTra，这是第一个多语言语料库和检测框架，用于研究语用显化现象。语料库涵盖八个语言对，并通过空对齐和主动学习进行细化。结果表明，实体和系统级别的显化最为常见，主动学习提高了分类器的准确性。PragExTra将语用显化确立为可测量的跨语言现象，并朝着构建文化意识机器翻译迈出了重要一步。


<details>
  <summary>Details</summary>
Motivation: Translators often enrich texts with background details that make implicit cultural meanings explicit for new audiences. This phenomenon, known as pragmatic explicitation, has been widely discussed in translation theory but rarely modeled computationally.

Method: We introduce PragExTra, the first multilingual corpus and detection framework for pragmatic explicitation. The corpus covers eight language pairs from TED-Multi and Europarl and includes additions such as entity descriptions, measurement conversions, and translator remarks. We identify candidate explicitation cases through null alignments and refined using active learning with human annotation.

Result: Our results show that entity and system-level explicitations are most frequent, and that active learning improves classifier accuracy by 7-8 percentage points, achieving up to 0.88 accuracy and 0.82 F1 across languages.

Conclusion: PragExTra establishes pragmatic explicitation as a measurable, cross-linguistic phenomenon and takes a step towards building culturally aware machine translation.

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [20] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: 研究发现，由于语言可及性问题，低资源语言国家的AI用户比例比预期低20%，表明语言可及性是公平人工智能扩散的重要障碍。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能在全球范围内的扩散速度不均的问题，特别是由于数据稀缺导致前沿大型语言模型在低资源语言上的表现不佳，从而影响了这些国家的AI采用率。

Method: 使用加权回归模型，将语言效应与社会经济和人口因素分开，以测试低资源语言国家（LRLCs）的AI用户比例是否较低。

Result: 发现LRLCs的AI用户比例大约比其基准值低20%。

Conclusion: 语言可及性是公平人工智能扩散的重要独立障碍。

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [21] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: 本文提出了一种集中式多LLM框架CoRL，通过强化学习优化性能与成本的权衡，在不同预算条件下实现高效的多代理LLM系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在不同领域表现出互补的优势，并且具有不同的推理成本，这促使设计多代理LLM系统，其中专门模型可以高效协作。现有方法主要依赖于去中心化框架，这为每个输入调用多个LLM，从而导致大量且不受控制的推理成本。

Method: 我们将协调问题形式化为具有双重目标的强化学习：最大化任务性能同时最小化总体推理成本。此外，我们期望多代理系统在推理过程中在不同预算条件下表现出适应性行为。为此，我们提出了CoRL，这是一个在可控多预算设置中优化性能成本权衡的强化学习框架。

Result: 实验在四个不同的基准上进行，结果表明CoRL使单一系统在高预算环境下超越最佳专家LLM，同时在更经济的低预算模式下保持强大的性能。

Conclusion: 实验表明，CoRL使单一系统在高预算环境下超越最佳专家LLM，同时在更经济的低预算模式下保持强大性能，突显了集中协调在可扩展和成本效益高的多代理LLM系统中的有效性。

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [22] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索器架构AMER，通过生成多个查询向量来提高检索性能，并在合成和真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的检索器在目标文档嵌入之间的距离增大时表现较差，而相关文档的条件分布可能是多模态的，例如表示查询的不同解释。

Method: 本文提出了一种新的检索器架构，称为AMER（Autoregressive Multi-Embedding Retriever），该模型自回归地生成多个查询向量，并使用所有预测的查询向量从语料库中检索文档。

Result: 在合成向量化数据上，所提出的方法能够完美捕捉多个目标分布，性能比单嵌入模型提高了4倍。在真实世界的多答案检索数据集上微调后，AMER在两个评估数据集上相对于单嵌入基线分别获得了4%和21%的相对提升。

Conclusion: 本文展示了多查询向量检索器的潜力，并为未来的研究开辟了新方向。

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [23] [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.02805)
*Qianhao Yuan,Jie Lou,Zichao Li,Jiawei Chen,Yaojie Lu,Hongyu Lin,Le Sun,Debing Zhang,Xianpei Han*

Main category: cs.CL

TL;DR: MemSearcher是一种新的搜索代理工作流，通过平衡信息完整性和效率，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的搜索代理在处理多轮交互时面临计算和内存成本高的问题，而仅使用当前回合则会丢失关键信息，因此需要找到一种方法来平衡信息完整性和效率。

Method: 提出了一种名为MemSearcher的代理工作流，该工作流迭代地维护一个紧凑的内存，并将当前回合与之结合。同时引入了多上下文GRPO，这是一种端到端的强化学习框架，用于优化MemSearcher代理的推理、搜索策略和内存管理。

Result: MemSearcher在七个公共基准测试中取得了显著改进，特别是在Qwen2.5-3B-Instruct和Qwen2.5-7B-Instruct上分别提高了11%和12%。3B版本的MemSearcher甚至超过了7B版本的基线。

Conclusion: MemSearcher通过平衡信息完整性和效率，实现了更高的准确性和更低的计算开销。

Abstract: Typical search agents concatenate the entire interaction history into the LLM
context, preserving information integrity but producing long, noisy contexts,
resulting in high computation and memory costs. In contrast, using only the
current turn avoids this overhead but discards essential information. This
trade-off limits the scalability of search agents. To address this challenge,
we propose MemSearcher, an agent workflow that iteratively maintains a compact
memory and combines the current turn with it. At each turn, MemSearcher fuses
the user's question with the memory to generate reasoning traces, perform
search actions, and update memory to retain only information essential for
solving the task. This design stabilizes context length across multi-turn
interactions, improving efficiency without sacrificing accuracy. To optimize
this workflow, we introduce multi-context GRPO, an end-to-end RL framework that
jointly optimize reasoning, search strategies, and memory management of
MemSearcher Agents. Specifically, multi-context GRPO samples groups of
trajectories under different contexts and propagates trajectory-level
advantages across all conversations within them. Trained on the same dataset as
Search-R1, MemSearcher achieves significant improvements over strong baselines
on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on
Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher
even outperforms 7B-based baselines, demonstrating that striking a balance
between information integrity and efficiency yields both higher accuracy and
lower computational overhead. The code and models will be publicly available at
https://github.com/icip-cas/MemSearcher

</details>


### [24] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: Oolong是一个新的基准测试，用于评估模型在处理大量文本时的推理能力。尽管最先进的模型在该测试中表现不佳，但作者发布了数据和评估工具，以促进相关模型的发展。


<details>
  <summary>Details</summary>
Motivation: 随着模型上下文长度的增长，人们开始关注模型是否能有效利用整个上下文长度。现有的长上下文评估通常依赖于从上下文中检索信息，这使得大部分上下文标记被视为噪声。Oolong旨在提供一种更全面的评估方式，以测试模型在处理大量文本时的能力。

Method: Oolong是一个长期上下文推理任务的基准测试，分为两个任务集：Oolong-synth（自然主义合成任务）和Oolong-real（现实对话数据任务）。它要求模型对大量示例进行分类和计数，并对时间关系和用户关系进行推理。

Result: 即使是最先进的模型，如GPT-5、Claude-Sonnet-4和Gemini-2.5-Pro，在Oolong上的准确率也低于50%。这表明当前模型在处理大量文本推理任务时仍存在困难。

Conclusion: Oolong是一个需要模型对大量文本进行推理的基准测试，即使最先进的模型在Oolong上也表现不佳。作者发布了数据和评估工具，以促进能够处理大量文本推理的模型的发展。

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [25] [SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](https://arxiv.org/abs/2511.02280)
*Fangxun Shu,Yongjie Ye,Yue Liao,Zijian Kang,Weijie Yin,Jiacong Wang,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-RL 是一种用于提升多模态大语言模型推理能力的强化学习后训练框架，通过双奖励系统改进模型的推理质量和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于仅基于结果的监督，无法确保良好的推理过程，并且使用统一的思考策略，导致简单任务过度思考和复杂任务思考不足。

Method: SAIL-RL 采用双奖励系统：思考奖励通过事实依据、逻辑连贯性和答案一致性评估推理质量；判断奖励自适应地确定是否需要深度推理或直接回答。

Result: 实验表明，SAIL-RL 在 SAIL-VL2 上提升了推理和多模态理解基准测试的表现，在 4B 和 8B 规模下均表现出色，性能可与 GPT-4o 等商业闭源模型相媲美，并显著减少了幻觉现象。

Conclusion: SAIL-RL 是一种用于增强多模态大语言模型推理能力的强化学习后训练框架，通过教它们何时以及如何思考来提高模型的可靠性与适应性。

Abstract: We introduce SAIL-RL, a reinforcement learning (RL) post-training framework
that enhances the reasoning capabilities of multimodal large language models
(MLLMs) by teaching them when and how to think. Existing approaches are limited
by outcome-only supervision, which rewards correct answers without ensuring
sound reasoning, and by uniform thinking strategies, which often lead to
overthinking on simple tasks and underthinking on complex ones. SAIL-RL
addresses these challenges with a dual reward system: the Thinking Reward,
which evaluates reasoning quality through factual grounding, logical coherence,
and answer consistency, and the Judging Reward, which adaptively determines
whether deep reasoning or direct answering is appropriate. Experiments on the
state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal
understanding benchmarks at both 4B and 8B scales, achieving competitive
performance against commercial closed-source models such as GPT-4o, and
substantially reduces hallucinations, establishing it as a principled framework
for building more reliable and adaptive MLLMs. The code will be available at
https://github.com/BytedanceDouyinContent/SAIL-RL.

</details>


### [26] [Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions](https://arxiv.org/abs/2511.02288)
*Cuong Tuan Nguyen,Ngoc Tuan Nguyen,Triet Hoang Minh Dao,Huy Minh Nhat,Huy Truong Dinh*

Main category: cs.CV

TL;DR: 本文提出了一种基于图神经网络的方法用于手写数学表达式的结构识别，通过构建符号之间的空间关系图，并利用深度BLSTM和GNN技术优化结构，取得了良好的实验结果。


<details>
  <summary>Details</summary>
Motivation: 为了提高手写数学表达式结构识别的准确性，我们提出了一种基于图神经网络的方法，以更好地捕捉符号之间的空间关系。

Method: 我们提出了一种基于图神经网络（GNN）的方法，将手写数学表达式建模为图，其中节点代表符号，边捕捉空间依赖关系。使用深度BLSTM网络进行符号分割、识别和空间关系分类，形成初始的原始图。然后，2D-CFG解析器生成所有可能的空间关系，而基于GNN的链接预测模型通过移除不必要的连接来优化结构，最终形成符号标签图。

Result: 实验结果表明，我们的方法在手写数学表达式的结构识别中表现出色，具有良好的性能。

Conclusion: 实验结果表明，我们的方法在手写数学表达式的结构识别中表现出色，具有良好的性能。

Abstract: We propose a Graph Neural Network (GNN)-based approach for Handwritten
Mathematical Expression (HME) recognition by modeling HMEs as graphs, where
nodes represent symbols and edges capture spatial dependencies. A deep BLSTM
network is used for symbol segmentation, recognition, and spatial relation
classification, forming an initial primitive graph. A 2D-CFG parser then
generates all possible spatial relations, while the GNN-based link prediction
model refines the structure by removing unnecessary connections, ultimately
forming the Symbol Label Graph. Experimental results demonstrate the
effectiveness of our approach, showing promising performance in HME structure
recognition.

</details>


### [27] [CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](https://arxiv.org/abs/2511.02360)
*Jizheng Ma,Xiaofei Zhou,Yanlong Song,Han Yan*

Main category: cs.CV

TL;DR: CoCoVa is a new framework for vision-language models that uses continuous cross-modal reasoning to improve performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Contemporary Vision-Language Models (VLMs) are constrained to reasoning within the discrete and rigid space of linguistic tokens, which bottlenecks the rich, high-dimensional nature of visual perception.

Method: CoCoVa is a novel framework for vision-language models that leverages continuous cross-modal reasoning. It uses an iterative reasoning cycle with a Latent Q-Former (LQ-Former) as a dynamic reasoning engine, along with a token selection mechanism and multi-task training.

Result: Evaluations show CoCoVa improves accuracy and token efficiency over strong baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B models on almost all benchmarks. When scaled to 7B LLM backbones, it remains competitive with state-of-the-art models.

Conclusion: CoCoVa can bridge the representational gap between discrete language processing and the continuous nature of visual understanding.

Abstract: In human cognition, there exist numerous thought processes that are tacit and
beyond verbal expression, enabling us to understand and interact with the world
in multiple ways. However, contemporary Vision-Language Models (VLMs) remain
constrained to reasoning within the discrete and rigid space of linguistic
tokens, thereby bottlenecking the rich, high-dimensional nature of visual
perception. To bridge this gap, we propose CoCoVa (Chain of Continuous
Vision-Language Thought), a novel framework for vision-language model that
leverages continuous cross-modal reasoning for diverse vision-language tasks.
The core of CoCoVa is an iterative reasoning cycle, where a novel Latent
Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a
chain of latent thought vectors through cross-modal fusion. To focus this
process, a token selection mechanism dynamically identifies salient visual
regions, mimicking attentional focus. To ensure these latent thoughts remain
grounded, we train the model with a multi-task objective that combines
contrastive learning and diffusion-based reconstruction, enforcing alignment
between latent representations and both visual and textual modalities.
Evaluations show CoCoVa improves accuracy and token efficiency over strong
baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B
models on almost all benchmarks. When scaled to 7B LLM backbones, it remains
competitive with state-of-the-art models. Qualitative analysis validates that
learned latent space captures interpretable and structured reasoning patterns,
highlighting the potential of CoCoVa to bridge the representational gap between
discrete language processing and the continuous nature of visual understanding.

</details>


### [28] [DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding](https://arxiv.org/abs/2511.02495)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang*

Main category: cs.CV

TL;DR: 本文介绍了DetectiumFire，一个大规模多模态数据集，用于火灾相关研究，旨在推动智能安全系统的发展。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的火灾领域注释的公开数据集，将这些模型应用于火灾领域仍然具有挑战性。

Method: 引入了DetectiumFire，这是一个大规模多模态数据集，包含22.5k高分辨率火灾相关图像和2.5k真实世界火灾相关视频，并进行了传统计算机视觉标签和详细文本提示的标注。

Result: DetectiumFire在规模、多样性和数据质量方面优于现有基准，显著减少了冗余并增强了现实场景的覆盖范围。我们在多个任务中验证了DetectiumFire的实用性，包括目标检测、基于扩散的图像生成和视觉-语言推理。

Conclusion: DetectiumFire具有显著的优势，可以推动火灾相关研究并支持智能安全系统的开发。我们发布了DetectiumFire以促进AI社区对火灾理解的更广泛探索。

Abstract: Recent advances in multi-modal models have demonstrated strong performance in
tasks such as image generation and reasoning. However, applying these models to
the fire domain remains challenging due to the lack of publicly available
datasets with high-quality fire domain annotations. To address this gap, we
introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k
high-resolution fire-related images and 2.5k real-world fire-related videos
covering a wide range of fire types, environments, and risk levels. The data
are annotated with both traditional computer vision labels (e.g., bounding
boxes) and detailed textual prompts describing the scene, enabling applications
such as synthetic data generation and fire risk reasoning. DetectiumFire offers
clear advantages over existing benchmarks in scale, diversity, and data
quality, significantly reducing redundancy and enhancing coverage of real-world
scenarios. We validate the utility of DetectiumFire across multiple tasks,
including object detection, diffusion-based image generation, and
vision-language reasoning. Our results highlight the potential of this dataset
to advance fire-related research and support the development of intelligent
safety systems. We release DetectiumFire to promote broader exploration of fire
understanding in the AI community. The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890

</details>


### [29] [UniChange: Unifying Change Detection with Multimodal Large Language Model](https://arxiv.org/abs/2511.02607)
*Xu Zhang,Danyang Li,Xiaohang Dong,Tianhao Wu,Hualong Yu,Jianye Wang,Qicheng Li,Xiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型的统一变化检测模型 UniChange，成功地将二进制变化检测和语义变化检测任务统一起来，并在多个基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前模型通常只能从单类型注释数据中获取有限的知识，无法同时利用多种二进制变化检测 (BCD) 和语义变化检测 (SCD) 数据集，导致泛化能力和多样性受限。

Method: UniChange 利用多模态大语言模型的语言先验和统一能力，结合生成语言能力和专门的 CD 功能。通过引入三种特殊标记，并利用文本提示引导变化类别的识别，从而消除对预定义分类头的依赖。

Result: UniChange 在四个公共基准 (WHU-CD、S2Looking、LEVIR-CD+ 和 SECOND) 上实现了最先进的性能，分别达到了 90.41、53.04、78.87 和 57.62 的 IoU 分数。

Conclusion: UniChange 是一个统一的变化检测模型，通过引入三种特殊标记 [T1]、[T2] 和 [CHANGE] 成功地将 BCD 和 SCD 任务统一起来。实验表明，该模型在四个公开基准上取得了最先进的性能。

Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land
cover dynamics. While recent high performance models and high quality datasets
have significantly advanced the field, a critical limitation persists. Current
models typically acquire limited knowledge from single-type annotated data and
cannot concurrently leverage diverse binary change detection (BCD) and semantic
change detection (SCD) datasets. This constraint leads to poor generalization
and limited versatility. The recent advancements in Multimodal Large Language
Models (MLLMs) introduce new possibilities for a unified CD framework. We
leverage the language priors and unification capabilities of MLLMs to develop
UniChange, the first MLLM-based unified change detection model. UniChange
integrates generative language abilities with specialized CD functionalities.
Our model successfully unifies both BCD and SCD tasks through the introduction
of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange
utilizes text prompts to guide the identification of change categories,
eliminating the reliance on predefined classification heads. This design allows
UniChange to effectively acquire knowledge from multi-source datasets, even
when their class definitions conflict. Experiments on four public benchmarks
(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,
surpassing all previous methods. The code is available at
https://github.com/Erxucomeon/UniChange.

</details>


### [30] [VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation](https://arxiv.org/abs/2511.02778)
*Kevin Qinghong Lin,Yuhao Zheng,Hangyu Ran,Dantong Zhu,Dongxing Mao,Linjie Li,Philip Torr,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出了VCode和VCoder，以解决视觉中心编码中的问题，并展示了其在提升视觉中心编码性能方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前的研究主要集中在语言相关的任务上，而视觉相关的编码研究较少，因此需要一种更紧凑、可解释和可执行的视觉表示方法。

Method: VCode通过将多模态理解重新定义为代码生成任务，评估了符号保真度，并引入了VCoder框架来改进视觉中心编码。

Result: VCoder在多个基准测试中取得了12.3点的整体提升，表明其在视觉中心编码方面的有效性。

Conclusion: VCode和VCoder的提出为视觉中心编码提供了新的方向，同时揭示了语言中心和视觉中心编码之间的差距。

Abstract: Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [31] [Complete asymptotic type-token relationship for growing complex systems with inverse power-law count rankings](https://arxiv.org/abs/2511.02069)
*Pablo Rosillo-Rodes,Laurent Hébert-Dufresne,Peter Sheridan Dodds*

Main category: physics.soc-ph

TL;DR: 本文提出了一种理想化的增长系统模型，能够确定地产生逆幂律类型计数排名，并证明了Zipf定律可以单独导致一般类型的-令牌关系。


<details>
  <summary>Details</summary>
Motivation: 研究复杂系统的增长动力学中的统计规律性，特别是Zipf定律和Heaps定律之间的关系。

Method: 本文提出了一个理想化的增长系统模型，该模型可以确定地产生任意的逆幂律类型计数排名，并允许确定类型-令牌关系的精确渐近行为。

Result: 本文获得了所有α值的统一渐近表达式，修正了α=1和α≫1的特殊情况。

Conclusion: 本文证明了Zipf定律可以单独导致一般类型的-令牌关系，而无需任何随机机制或抽样过程。

Abstract: The growth dynamics of complex systems often exhibit statistical regularities
involving power-law relationships. For real finite complex systems formed by
countable tokens (animals, words) as instances of distinct types (species,
dictionary entries), an inverse power-law scaling $S \sim r^{-\alpha}$ between
type count $S$ and type rank $r$, widely known as Zipf's law, is widely
observed to varying degrees of fidelity. A secondary, summary relationship is
Heaps' law, which states that the number of types scales sublinearly with the
total number of observed tokens present in a growing system. Here, we propose
an idealized model of a growing system that (1) deterministically produces
arbitrary inverse power-law count rankings for types, and (2) allows us to
determine the exact asymptotics of the type-token relationship. Our argument
improves upon and remedies earlier work. We obtain a unified asymptotic
expression for all values of $\alpha$, which corrects the special cases of
$\alpha = 1$ and $\alpha \gg 1$. Our approach relies solely on the form of
count rankings, avoids unnecessary approximations, and does not involve any
stochastic mechanisms or sampling processes. We thereby demonstrate that a
general type-token relationship arises solely as a consequence of Zipf's law.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [32] [SciDaSynth: Interactive Structured Data Extraction from Scientific Literature with Large Language Model](https://arxiv.org/abs/2404.13765)
*Xingbo Wang,Samantha L. Huey,Rui Sheng,Saurabh Mehta,Fei Wang*

Main category: cs.HC

TL;DR: SciDaSynth 是一个基于大型语言模型的交互式系统，用于自动从多种来源生成结构化数据表，并解决跨文档数据不一致性问题，实验证明其在生成高质量数据方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于科学文献的爆炸性增长，高效且准确地提取结构化数据变得至关重要，但现有工具在处理多模态、多样性和不一致信息时面临挑战。

Method: SciDaSynth 利用大型语言模型 (LLMs) 自动根据用户的查询生成结构化数据表，整合来自文本、表格和图表等多种来源的信息，并提供多维视觉摘要和语义分组功能以进行数据验证和优化。

Result: 通过一项针对营养学和自然语言处理研究人员的实验研究，证明了 SciDaSynth 在生成高质量结构化数据方面的有效性，比基线方法更高效。

Conclusion: SciDaSynth 是一种有效的交互式系统，能够高效地生成高质量的结构化数据，并支持跨文档数据不一致性的解决。

Abstract: The explosion of scientific literature has made the efficient and accurate
extraction of structured data a critical component for advancing scientific
knowledge and supporting evidence-based decision-making. However, existing
tools often struggle to extract and structure multimodal, varied, and
inconsistent information across documents into standardized formats. We
introduce SciDaSynth, a novel interactive system powered by large language
models (LLMs) that automatically generates structured data tables according to
users' queries by integrating information from diverse sources, including text,
tables, and figures. Furthermore, SciDaSynth supports efficient table data
validation and refinement, featuring multi-faceted visual summaries and
semantic grouping capabilities to resolve cross-document data inconsistencies.
A within-subjects study with nutrition and NLP researchers demonstrates
SciDaSynth's effectiveness in producing high-quality structured data more
efficiently than baseline methods. We discuss design implications for human-AI
collaborative systems supporting data extraction tasks. The system code is
available at https://github.com/xingbow/SciDaEx

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [33] [Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.02304)
*Beyazit Yalcinkaya,Marcell Vazquez-Chanlatte,Ameesh Shah,Hanna Krasowski,Sanjit A. Seshia*

Main category: cs.MA

TL;DR: 本文提出了一种新的多任务多智能体强化学习框架，能够实现高效的协作和任务分配。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在样本效率和多任务情况下存在限制，因此需要一种更有效的框架来解决这个问题。

Method: 本文提出了一种名为Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL) 的框架，用于学习任务条件下的去中心化团队策略。

Result: 实验表明，代理之间出现了任务感知的多步骤协调，例如按按钮解锁门、保持门以及短路任务。

Conclusion: 本文提出了一个框架，用于学习任务条件下的去中心化团队策略，并展示了所学策略的价值函数可以在测试时最优地分配任务。

Abstract: We study the problem of learning multi-task, multi-agent policies for
cooperative, temporal objectives, under centralized training, decentralized
execution. In this setting, using automata to represent tasks enables the
decomposition of complex tasks into simpler sub-tasks that can be assigned to
agents. However, existing approaches remain sample-inefficient and are limited
to the single-task case. In this work, we present Automata-Conditioned
Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for
learning task-conditioned, decentralized team policies. We identify the main
challenges to ACC-MARL's feasibility in practice, propose solutions, and prove
the correctness of our approach. We further show that the value functions of
learned policies can be used to assign tasks optimally at test time.
Experiments show emergent task-aware, multi-step coordination among agents,
e.g., pressing a button to unlock a door, holding the door, and
short-circuiting tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: 研究引入了Deep Value Benchmark (DVB)，以评估大型语言模型是否学习了基本的人类价值观而非表面偏好。实验表明，大多数模型在深度价值泛化方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 为了确保AI系统能够稳健地概括人类意图，而不是仅仅学习偏好数据中的表面模式，需要区分模型是学习了深层价值还是表面偏好。

Method: DVB使用一种新颖的实验设计，通过控制深度价值和浅层特征之间的混杂因素来测试大型语言模型是否学习了基本的人类价值观。

Result: 在9种不同的模型中，平均DVGR仅为0.30，所有模型的深度价值泛化率都低于随机水平。更大的模型的DVGR略低于较小的模型。

Conclusion: DVB提供了一个可解释的指标，用于衡量对齐的核心特征。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [35] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: This paper proposes InsurAgent, an LLM-empowered agent for modeling insurance decision-making. It addresses the limitations of LLMs in estimating quantitative probabilities by incorporating modules for retrieval, reasoning, and memory.


<details>
  <summary>Details</summary>
Motivation: Flood insurance is an effective strategy for individuals to mitigate disaster-related losses. However, participation rates among at-risk populations in the United States remain strikingly low. This gap underscores the need to understand and model the behavioral mechanisms underlying insurance decisions.

Method: This study constructs a benchmark dataset to capture insurance purchase probabilities across factors. Using this dataset, the capacity of LLMs is evaluated. To address the limitation, InsurAgent, an LLM-empowered agent comprising five modules including perception, retrieval, reasoning, action, and memory, is proposed.

Result: LLMs exhibit a qualitative understanding of factors, but they fall short in estimating quantitative probabilities. The retrieval module leverages retrieval-augmented generation (RAG) to ground decisions in empirical survey data, achieving accurate estimation of marginal and bivariate probabilities. The reasoning module leverages LLM common sense to extrapolate beyond survey data, capturing contextual information that is intractable for traditional models. The memory module supports the simulation of temporal decision evolutions.

Conclusion: InsurAgent provides a valuable tool for behavioral modeling and policy analysis.

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [36] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: ATHENA is a framework that integrates symbolic utility modeling and semantic adaptation to improve human-centric decision-making models, showing significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The gap between individual decision-making models and population optimal predictions arises from the uniqueness of the individual decision-making process, shaped by numerical attributes and linguistic influences. The paper aims to address this gap by proposing a framework that integrates symbolic utility modeling and semantic adaptation.

Method: ATHENA integrates two stages: first, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices.

Result: ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5% over the strongest cutting-edge models. Ablation studies confirm that both stages of ATHENA are critical and complementary.

Conclusion: ATHENA provides a new scheme for modeling human-centric decisions by organically integrating symbolic utility modeling and semantic adaptation.

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [37] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun,Xuhui Zhou,Weihua Du,Xingyao Wang,Sean Welleck,Graham Neubig,Maarten Sap,Yiming Yang*

Main category: cs.AI

TL;DR: 本文提出了一种多目标强化学习方法PPP，通过优化生产力、主动性和个性化三个维度，提升了AI代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注任务成功，但有效的现实世界代理需要优化三个维度：生产力（任务完成）、主动性（提出关键问题）和个性化（适应多样化的用户偏好）。

Method: 本文引入了UserVille，一个具有LLM基础用户模拟器的交互环境，以及PPP，一种多目标强化学习方法，联合优化生产力、主动性和个性化三个维度。

Result: 在软件工程和深度研究任务上的实验表明，使用PPP训练的代理相比强大的基线（如GPT-5）有显著提升，表现出能够提出战略澄清问题、适应未见过的用户偏好并提高任务成功的互动能力。

Conclusion: 本文表明，明确优化以用户为中心的交互对于构建实用且有效的AI代理至关重要。

Abstract: While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [38] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文研究了多代理推理中的懒惰代理行为问题，并提出了一个框架来解决这一问题，从而提高多代理系统的协作效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多代理推理设置表现出有希望的性能，但存在一个关键限制：懒惰代理行为，其中一个代理主导而另一个贡献很少，破坏了协作并使设置退化为无效的单代理。

Method: 本文首先提供了理论分析，说明为什么懒惰行为在多代理推理中自然出现，然后引入了一种稳定且高效的因果影响测量方法，以减轻这个问题。最后，提出了一种可验证的奖励机制，通过允许推理代理丢弃噪声输出、整合指令并在必要时重新开始其推理过程来鼓励深思熟虑。

Result: 广泛的实验表明，我们的框架缓解了懒惰代理行为，并解锁了多代理框架在复杂推理任务中的全部潜力。

Conclusion: 本文提出的框架能够缓解懒惰代理行为，并释放多代理框架在复杂推理任务中的全部潜力。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [39] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 本文提出了一种协作迷宫解决基准，用于评估AI模型在协作任务中的表现。研究发现，单独表现良好的模型在协作时可能会表现不佳，这表明需要改进评估方法和训练策略以提高协作能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI的发展，越来越多的系统将依赖于由独立开发的代理组成的系统，这些系统需要有效的协作，即使在部分可观察的情况下也是如此。然而，目前缺乏大规模的实证研究来评估这种协作。

Method: 我们提出了一个协作迷宫解决基准，用于评估不同模型在单独、同质和异质配对中的协作能力。

Result: 我们的结果显示了一个“协作差距”：单独表现良好的模型在需要协作时可能会显著下降。协作可能会崩溃；例如，单独解决迷宫很好的小型蒸馏模型在某些配对中可能几乎完全失败。

Conclusion: 我们的研究结果表明，需要开发新的评估方法、训练策略和交互设计，以提高AI系统在协作任务中的表现。这些发现适用于AI-AI和人机协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [40] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: 本文介绍了CostBench，这是一个用于评估代理经济推理和重规划能力的成本导向基准。通过在旅行规划领域进行实验，发现当前的大型语言模型代理在成本意识规划方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments.

Method: Introduce CostBench, a scalable, cost-centric benchmark to evaluate agents' economic reasoning and replanning abilities.

Result: Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions.

Conclusion: CostBench lays the groundwork for developing future agents that are both economically rational and robust.

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [41] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin,Yunzhi Shi,Tong Geng,Weijie Zhao,Wei Wang,Ravender Pal Singh*

Main category: cs.AI

TL;DR: 本文提出Agent-Omni框架，通过代理系统协调现有模型，实现灵活多模态推理，具备高性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）受限于固定的模态对，需要昂贵的微调，并且缺乏强大的推理支持。构建完全全能的模型在实际中仍不现实。

Method: 提出了一种基于代理的框架Agent-Omni，通过主代理系统协调现有基础模型，实现多模态推理。

Result: Agent-Omni在文本、图像、音频、视频和全模态基准测试中表现出色，特别是在需要复杂跨模态推理的任务中达到了最先进的性能。

Conclusion: Agent-Omni框架通过代理系统协调现有的基础模型，实现了灵活的多模态推理，具有模块化和可扩展性，能够适应多种输入并保持透明性和可解释性。

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [42] [An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM](https://arxiv.org/abs/2511.02234)
*Jiawei Liu,Enis Berk Çoban,Zarina Schevchenko,Hao Tang,Zhigang Zhu,Michael I Mandel,Johanna Devaney*

Main category: cs.MM

TL;DR: 本研究探讨了在音频MLLM中交错指令调整的影响，发现交错提示能提高推理任务的性能，但微调会牺牲音频标记能力。


<details>
  <summary>Details</summary>
Motivation: 标准的多模态大型语言模型（MLLM）训练涉及将非文本信息（如视觉或音频）与文本提示连接起来。这种方法可能不会鼓励模态的深度整合，限制了模型利用核心语言模型推理能力的能力。因此，本研究探讨了在音频MLLM中交错指令调整的影响。

Method: 研究使用了LTU模型作为测试平台，并利用SHARD数据集进行了实验，该数据集是一个新的用于音频基础语义推理的基准测试，专注于同义词和上位词识别。

Result: 研究结果表明，即使在零样本情况下，交错提示也能提高推理任务的性能，但使用交错训练提示进行微调可以进一步提高结果，然而这会牺牲MLLM的音频标记能力。

Conclusion: 研究发现，即使是在零样本情况下，交错提示也能提高推理任务的性能，但使用交错训练提示进行微调可以进一步提高结果，然而这会牺牲MLLM的音频标记能力。

Abstract: Standard training for Multi-modal Large Language Models (MLLMs) involves
concatenating non-textual information, like vision or audio, with a text
prompt. This approach may not encourage deep integration of modalities,
limiting the model's ability to leverage the core language model's reasoning
capabilities. This work examined the impact of interleaved instruction tuning
in an audio MLLM, where audio tokens are interleaved within the prompt. Using
the Listen, Think, and Understand (LTU) model as a testbed, we conduct an
experiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our
newly created reasoning benchmark for audio-based semantic reasoning focusing
on synonym and hypernym recognition. Our findings show that while even
zero-shot interleaved prompting improves performance on our reasoning tasks, a
small amount of fine-tuning using interleaved training prompts improves the
results further, however, at the expense of the MLLM's audio labeling ability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)
*Zijian Zhang,Rong Wang,Shiyang Li,Yuebo Luo,Mingyi Hong,Caiwen Ding*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Developing efficient CUDA kernels is increasingly critical for AI
applications such as large-scale LLM training. However, manual kernel design is
both costly and time-consuming, motivating automatic approaches that leverage
LLMs for code generation. Existing methods for automatic kernel generation,
however, often produce low-efficiency kernels, incur high computational
overhead, and fail to generalize across settings. In this work, we propose
CudaForge, a training-free multi-agent workflow for CUDA kernel generation and
optimization. Our workflow is inspired by the iterative workflow of human
experts, which contains steps such as developing initial kernels, testing
correctness, analyzing hardware feedback, and iterative improvement. More
specifically, CudaForge employs two LLM agents: a Coder and a Judge, that
iteratively generate, correct, and optimize CUDA kernels, while integrating
hardware feedback such as Nsight Compute (NCU) metrics. In extensive
evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3,
achieves 97.6\% correctness of generated kernels and an average 1.68$\times$
speedup over PyTorch baselines, substantially surpassing state-of-the-art
models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed,
CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090,
3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4,
QwQ-32B), while maintaining high efficiency. In particular, generating an
optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$
0.3 API cost, which is significantly cheaper than existing agentic work that
costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that
multi-agent, training-free workflows can enable cost-effective, generalizable,
and high-performance CUDA kernel optimization. Code available at
https://github.com/OptimAI-Lab/CudaForge

</details>


### [44] [Retrieval-Augmented Multimodal Depression Detection](https://arxiv.org/abs/2511.01892)
*Ruibo Hou,Shiyu Teng,Jiaqing Liu,Shurong Chai,Yinhao Li,Lanfen Lin,Yen-Wei Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的检索增强生成框架，用于抑郁症检测，通过整合情感内容和大型语言模型提高情感表示和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有工作在情感分析方面存在高计算成本、领域不匹配和静态知识限制的问题。

Method: 我们提出了一个新颖的检索增强生成（RAG）框架。给定与抑郁相关的文本，我们的方法从情感数据集中检索语义相关的情感内容，并使用大型语言模型（LLM）生成情感提示作为辅助模态。

Result: 实验表明，我们的方法在AVEC 2019数据集上实现了最先进的性能，CCC为0.593，MAE为3.95。

Conclusion: 我们的方法在AVEC 2019数据集上实现了最先进的性能，CCC为0.593，MAE为3.95，超过了之前的迁移学习和多任务学习基线。

Abstract: Multimodal deep learning has shown promise in depression detection by
integrating text, audio, and video signals. Recent work leverages sentiment
analysis to enhance emotional understanding, yet suffers from high
computational cost, domain mismatch, and static knowledge limitations. To
address these issues, we propose a novel Retrieval-Augmented Generation (RAG)
framework. Given a depression-related text, our method retrieves semantically
relevant emotional content from a sentiment dataset and uses a Large Language
Model (LLM) to generate an Emotion Prompt as an auxiliary modality. This prompt
enriches emotional representation and improves interpretability. Experiments on
the AVEC 2019 dataset show our approach achieves state-of-the-art performance
with CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning and
multi-task learning baselines.

</details>


### [45] [TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding](https://arxiv.org/abs/2511.02017)
*Aditya Sridhar,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: TapOut是一种无需训练的动态推测策略选择算法，利用多臂老虎机实现高效的推测加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于手动调整的敏感阈值（例如，token熵），这些阈值设置成本高且在不同模型和领域中泛化能力差。

Method: TapOut是一种在线、无需训练、即插即用的算法，利用多臂老虎机进行动态推测策略选择。该方法采用元算法，根据过去的奖励和探索选择多个无参数的动态推测策略。

Result: 通过在多种模型对和数据集上的广泛实验，TapOut在不需要任何超参数调整的情况下，实现了与现有动态推测基线方法相当或更优的速度提升。

Conclusion: TapOut在不需要任何超参数调整的情况下，相比现有的动态推测基线方法，在速度提升方面表现出色或更具竞争力。

Abstract: Speculative decoding accelerates LLMs by using a lightweight draft model to
generate tokens autoregressively before verifying them in parallel with a
larger target model. However, determining the optimal number of tokens to draft
remains a key challenge limiting the approach's effectiveness. Dynamic
speculative decoding aims to intelligently decide how many tokens to draft to
achieve maximum speedups. Existing methods often rely on hand-tuned, sensitive
thresholds (e.g., token entropy), which are costly to set and generalize poorly
across models and domains. We propose TapOut, an online, training-free,
plug-and-play algorithm for dynamic speculation policy selection using
multi-armed bandits. Our approach employs a meta-algorithm that selects among
multiple parameter-free dynamic speculation strategies based on past reward and
exploration. We conduct extensive experiments across diverse model pairs and
datasets, showing that TapOut achieves competitive or superior speedups
compared to well-established dynamic speculation baselines without any
hyperparameter tuning.

</details>


### [46] [Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning](https://arxiv.org/abs/2511.02044)
*Vivswan Shah,Randy Cogill,Hanwei Yue,Gopinath Chennupati,Rinat Khaziev*

Main category: cs.LG

TL;DR: 研究显示，在微调过程中附加简短解释可以提高大型语言模型的分类性能，即使使用随机标记也能产生积极效果。


<details>
  <summary>Details</summary>
Motivation: 研究在微调过程中附加简短解释是否能获得更好的模型。

Method: 通过在微调过程中附加简短的解释来评估对话响应质量，并使用多模型生成的数据对7B参数模型进行微调。

Result: 在18个数据集、任务设置中，带有标签和解释的训练优于仅标签基线。随机标记也表现出改善准确性的效果。

Conclusion: 解释增强的微调，无论是使用真实的推理还是精心构造的随机标记序列，都能提高大型语言模型分类的准确性并可靠性，并阐明了标记级支架如何在推理过程中塑造计算。

Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels.
We ask whether attaching brief explanations to each label during fine-tuning
yields better models. We evaluate conversational response quality along three
axes: naturalness, comprehensiveness, and on-topic adherence, each rated on
5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune
a 7B-parameter model and test across six diverse conversational datasets.
Across 18 dataset, task settings, label-plus-explanation training outperforms
label-only baselines.
  A central and unexpected result concerns random tokens. We replace
human-written explanations with text that is syntactically incoherent yet
vocabulary-aligned with the originals (e.g., shuffled or bag-of-words
variants). Despite lacking semantics, these pseudo-explanations still improve
accuracy over label-only training and often narrow much of the gap to true
explanations. The effect persists across datasets and training seeds,
indicating that gains arise less from meaning than from structure: the extra
token budget encourages richer intermediate computation and acts as a
regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit
higher activation entropy in intermediate layers alongside sharper predictive
mass at the output layer, consistent with increased deliberation before
decision. Overall, explanation-augmented fine-tuning, whether with genuine
rationales or carefully constructed random token sequences, improves accuracy
and reliability for LLM classification while clarifying how token-level
scaffolding shapes computation during inference.

</details>


### [47] [LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS](https://arxiv.org/abs/2511.02089)
*Stefan F. Schouten,Peter Bloem*

Main category: cs.LG

TL;DR: 本文重新审视了Contrast-Consistent Search (CCS)，提出相对对比一致性作为优化目标，并将其重新表述为特征问题，从而得到闭式解和多变量扩展。评估显示这些方法在性能上与CCS相当，同时解决了初始化敏感性问题。


<details>
  <summary>Details</summary>
Motivation: CCS是一种无监督探测方法，能够测试大型语言模型是否在其内部激活中表示二进制特征，如句子真实性。然而，其两术语目标仅部分被理解。

Method: 我们重新审视CCS，以澄清其机制并扩展其适用性。我们提出应该优化相对对比一致性，并将CCS重新表述为一个特征问题，从而得到具有可解释特征值的闭式解，并自然扩展到多个变量。

Result: 我们在一系列数据集上评估了这些方法，发现它们恢复了与CCS相似的性能，同时避免了对随机初始化的敏感性问题。

Conclusion: 我们的结果表明，相对对比一致性不仅提高了对CCS的理解，还为更广泛的探测和机制可解释性方法打开了路径。

Abstract: Contrast-Consistent Search (CCS) is an unsupervised probing method able to
test whether large language models represent binary features, such as sentence
truth, in their internal activations. While CCS has shown promise, its two-term
objective has been only partially understood. In this work, we revisit CCS with
the aim of clarifying its mechanisms and extending its applicability. We argue
that what should be optimized for, is relative contrast consistency. Building
on this insight, we reformulate CCS as an eigenproblem, yielding closed-form
solutions with interpretable eigenvalues and natural extensions to multiple
variables. We evaluate these approaches across a range of datasets, finding
that they recover similar performance to CCS, while avoiding problems around
sensitivity to random initialization. Our results suggest that relativizing
contrast consistency not only improves our understanding of CCS but also opens
pathways for broader probing and mechanistic interpretability methods.

</details>


### [48] [Can LLMs subtract numbers?](https://arxiv.org/abs/2511.02795)
*Mayank Jobanputra,Nils Philipp Walter,Maitrey Mehta,Blerta Veseli,Evan Parker Kelly Chapple,Yifan Wang,Sneha Chetani,Ellie Pavlick,Antonio Vergari,Vera Demberg*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型在减法运算中的表现，发现其准确性远低于加法，并提出了通过指令调优来改善这一问题的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管减法在结构上与加法和乘法不同，但之前的研究主要关注加法和乘法，而减法却较少受到关注。

Method: 本文对八个预训练的大型语言模型进行了系统研究，评估了它们在加法和减法问题上的表现，并测试了少量示例学习和指令调优等技术的效果。

Result: 实验结果显示，减法的准确性远低于加法，特别是在a < b的情况下，模型经常忽略负号。然而，指令调优的模型在生成负号方面取得了接近完美的准确性。

Conclusion: 本文的发现为大型语言模型在减法运算中的局限性和可恢复性提供了更清晰的描述。

Abstract: We present a systematic study of subtraction in large language models (LLMs).
While prior benchmarks emphasize addition and multiplication, subtraction has
received comparatively little attention despite being structurally distinct as
a non-commutative operation. We evaluate eight pretrained LLMs spanning four
families on addition and subtraction problems. Our experiments reveal that
subtraction accuracy lags behind addition by a wide margin. We find that the
errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs
frequently produce the correct magnitude but omit the negative sign. Probing
analyses show that LLMs internally encode whether results should be negative,
yet this information is often not reflected in generated outputs. We further
test well-known techniques such as few-shot learning and instruction-tuning to
see if they can improve the LLMs' performance. Our results suggest that while
few-shot prompting yields modest gains, the instruction-tuned models achieve
near-perfect accuracies in generating the negative sign. Together, these
findings provide a clearer characterization of the limitations and
recoverability of LLMs' arithmetic capabilities in subtraction.

</details>


### [49] [In Good GRACEs: Principled Teacher Selection for Knowledge Distillation](https://arxiv.org/abs/2511.02833)
*Abhishek Panigrahi,Bingbin Liu,Sadhika Malladi,Sham Kakade,Surbhi Goel*

Main category: cs.LG

TL;DR: GRACE是一种轻量级评分，用于评估教师对微调学生模型的有效性，并能提供关于蒸馏设计选择的指导。


<details>
  <summary>Details</summary>
Motivation: 选择最佳教师对于特定学生-任务组合需要昂贵的试错过程。

Method: 提出了一种轻量级评分GRACE，用于量化教师对微调学生模型的有效性。GRACE测量学生梯度的分布特性，而无需访问验证器、教师logits、教师内部或测试数据。

Result: 在GSM8K和MATH上，GRACE与蒸馏LLaMA和OLMo学生的性能有很强的相关性（高达86%的Spearman相关性）。使用GRACE选择的教师训练学生可以比盲目使用最佳教师提高性能达7.4%。

Conclusion: GRACE可以高效且有效地识别与给定学生高度兼容的教师，并为如何进行蒸馏提供细粒度指导。

Abstract: Knowledge distillation is an efficient strategy to use data generated by
large "teacher" language models to train smaller capable "student" models, but
selecting the optimal teacher for a specific student-task combination requires
expensive trial-and-error. We propose a lightweight score called GRACE to
quantify how effective a teacher will be for post-training a student model.
GRACE measures distributional properties of the student's gradients without
access to a verifier, teacher logits, teacher internals, or test data. From an
information-theoretic perspective, GRACE connects to leave-one-out stability of
gradient-based algorithms, which controls the generalization performance of the
distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86%
Spearman correlation) with the performance of the distilled LLaMA and OLMo
students. In particular, training a student using the GRACE-selected teacher
can improve the performance by up to 7.4% over naively using the
best-performing teacher. Further, GRACE can provide guidance on crucial design
choices in distillation, including (1) the best temperature to use when
generating from the teacher, (2) the best teacher to use given a size
constraint, and (3) the best teacher to use within a specific model family.
Altogether, our findings demonstrate that GRACE can efficiently and effectively
identify a strongly compatible teacher for a given student and provide
fine-grained guidance on how to perform distillation.

</details>

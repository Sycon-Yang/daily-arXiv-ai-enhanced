<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)
*Mesay Gemeda Yigezu,Girma Yohannis Bade,Atnafu Lambebo Tonja,Olga Kolesnikova,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 本文研究了埃塞俄比亚南部地区Wolaita和Gofa语言的双语语言识别问题，并采用基于BERT的预训练语言模型和LSTM方法取得了较好的结果。


<details>
  <summary>Details</summary>
Motivation: 在埃塞俄比亚南部地区，Wolaita和Gofa语言之间存在词义相似性和差异性，使得语言识别任务具有挑战性。

Method: 采用了各种实验，结合基于BERT的预训练语言模型和LSTM方法。

Result: 基于BERT的预训练语言模型和LSTM方法的组合在测试集上取得了0.72的F1分数。

Conclusion: 该工作在处理社交媒体中的问题和为该领域的进一步研究提供基础方面是有效的。

Abstract: Language identification is the task of determining the languages for a given
text. In many real world scenarios, text may contain more than one language,
particularly in multilingual communities. Bilingual Language Identification
(BLID) is the task of identifying and distinguishing between two languages in a
given text. This paper presents BLID for languages spoken in the southern part
of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and
differences between the two languages makes the language identification task
challenging. To overcome this challenge, we employed various experiments on
various approaches. Then, the combination of the BERT based pretrained language
model and LSTM approach performed better, with an F1 score of 0.72 on the test
set. As a result, the work will be effective in tackling unwanted social media
issues and providing a foundation for further research in this area.

</details>


### [2] [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)
*Debdeep Sanyal,Manodeep Ray,Murari Mandal*

Main category: cs.CL

TL;DR: 本文提出了一种名为AntiDote的方法，用于训练大型语言模型以抵抗恶意微调，从而在保持模型性能的同时提高安全性。


<details>
  <summary>Details</summary>
Motivation: 开放权重大型语言模型的发布在推动可访问研究和防止滥用之间造成了紧张关系，例如恶意微调以引发有害内容。当前的安全措施难以在保留LLM的一般能力的同时抵抗具有完整模型权重和架构访问权限的坚定对手，这些对手可以使用全参数微调来消除现有的防护措施。

Method: 我们引入了AntiDote，这是一种双层优化过程，用于训练大型语言模型以抵抗此类篡改。AntiDote涉及一个辅助对手超网络，该网络学习生成恶意低秩适应（LoRA）权重，这些权重基于防御者模型的内部激活条件。然后，防御者LLM通过一个目标来消除这些对抗性权重添加的影响，迫使它保持其安全对齐。

Result: 我们在52种多样化的红队攻击套件上验证了这种方法，包括越狱提示、潜在空间操作和直接权重空间攻击。AntiDote在对抗攻击上的鲁棒性比篡改抵抗和遗忘基线高出最多27.4%。至关重要的是，这种鲁棒性是以最小的效用损失实现的，在MMLU、HellaSwag和GSM8K等能力基准测试中，性能下降不超过0.5%。

Conclusion: 我们的工作提供了一种实用且计算高效的构建开放权重模型的方法，其中安全性是一个更内在和有弹性的属性。

Abstract: The release of open-weight large language models (LLMs) creates a tension
between advancing accessible research and preventing misuse, such as malicious
fine-tuning to elicit harmful content. Current safety measures struggle to
preserve the general capabilities of the LLM while resisting a determined
adversary with full access to the model's weights and architecture, who can use
full-parameter fine-tuning to erase existing safeguards. To address this, we
introduce AntiDote, a bi-level optimization procedure for training LLMs to be
resistant to such tampering. AntiDote involves an auxiliary adversary
hypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)
weights conditioned on the defender model's internal activations. The defender
LLM is then trained with an objective to nullify the effect of these
adversarial weight additions, forcing it to maintain its safety alignment. We
validate this approach against a diverse suite of 52 red-teaming attacks,
including jailbreak prompting, latent space manipulation, and direct
weight-space attacks. AntiDote is upto 27.4\% more robust against adversarial
attacks compared to both tamper-resistance and unlearning baselines. Crucially,
this robustness is achieved with a minimal trade-off in utility, incurring a
performance degradation of upto less than 0.5\% across capability benchmarks
including MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute
efficient methodology for building open-weight models where safety is a more
integral and resilient property.

</details>


### [3] [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)
*Yao Liang,Dongcheng Zhao,Feifei Zhao,Guobin Shen,Yuwei Wang,Dongqi Liang,Yi Zeng*

Main category: cs.CL

TL;DR: 本文介绍了一个新的基准测试MVPBench，用于评估大型语言模型在75个国家中与多维人类价值偏好的对齐情况。研究发现，大型语言模型在不同地区和人口背景下的对齐性能存在差异，并且轻量级微调方法可以显著提高价值对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试常常忽视文化和社会多样性，导致对价值对齐在全球范围内的泛化理解有限。因此，我们需要一个更全面的基准测试来评估大型语言模型在不同文化和人口背景下的价值对齐情况。

Method: 我们引入了MVPBench，这是一个新的基准测试，系统地评估大型语言模型在75个国家中与多维人类价值偏好的对齐情况。此外，我们使用MVPBench对几种最先进的大型语言模型进行了深入分析，并展示了轻量级微调方法可以显著提高价值对齐效果。

Result: 我们的研究表明，大型语言模型在地理和人口方面的对齐性能存在显著差异。同时，我们发现轻量级微调方法如LoRA和DPO可以显著提高模型在域内和域外设置中的价值对齐效果。

Conclusion: 我们的研究强调了在评估大型语言模型的价值对齐时考虑人口和文化因素的必要性，并为构建文化适应性和价值敏感的模型提供了可行的见解。MVPBench为未来关于全球对齐、个性化价值建模和公平人工智能发展的研究提供了一个实用的基础。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe and effective deployment across diverse user populations.
However, existing benchmarks often neglect cultural and demographic diversity,
leading to limited understanding of how value alignment generalizes globally.
In this work, we introduce MVPBench, a novel benchmark that systematically
evaluates LLMs' alignment with multi-dimensional human value preferences across
75 countries. MVPBench contains 24,020 high-quality instances annotated with
fine-grained value labels, personalized questions, and rich demographic
metadata, making it the most comprehensive resource of its kind to date. Using
MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,
revealing substantial disparities in alignment performance across geographic
and demographic lines. We further demonstrate that lightweight fine-tuning
methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization
(DPO), can significantly enhance value alignment in both in-domain and
out-of-domain settings. Our findings underscore the necessity for
population-aware alignment evaluation and provide actionable insights for
building culturally adaptive and value-sensitive LLMs. MVPBench serves as a
practical foundation for future research on global alignment, personalized
value modeling, and equitable AI development.

</details>


### [4] [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)
*Hoang-Trung Nguyen,Tan-Minh Nguyen,Xuan-Bach Le,Tuan-Kiet Le,Khanh-Huyen Nguyen,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 本文介绍了NOWJ团队在COLIEE 2025竞赛中所有五个任务的方法和结果，特别是在法律案例蕴含任务中取得了优异成绩，并展示了传统信息检索技术与现代生成模型结合的潜力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在展示NOWJ团队在COLIEE 2025竞赛中所有五个任务中的方法和结果，特别是法律案例蕴含任务（Task 2）的进展。

Method: 我们系统地整合了预排序模型（BM25、BERT、monoT5）、基于嵌入的语义表示（BGE-m3、LLM2Vec）和先进的大型语言模型（Qwen-2、QwQ-32B、DeepSeek-V3）用于摘要、相关性评分和上下文化重新排序。

Result: 在Task 2中，我们的两阶段检索系统结合了词法语义过滤和基于上下文的LLM分析，以0.3195的F1分数获得第一名。在其他任务中，我们通过精心设计的集成和有效的基于提示的推理策略展示了强大的性能。

Conclusion: 我们的研究展示了传统信息检索技术与现代生成模型结合的潜力，为法律信息处理的未来发展提供了有价值的参考。

Abstract: This paper presents the methodologies and results of the NOWJ team's
participation across all five tasks at the COLIEE 2025 competition, emphasizing
advancements in the Legal Case Entailment task (Task 2). Our comprehensive
approach systematically integrates pre-ranking models (BM25, BERT, monoT5),
embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large
Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance
scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage
retrieval system combined lexical-semantic filtering with contextualized LLM
analysis, achieving first place with an F1 score of 0.3195. Additionally, in
other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal
Textual Entailment, and Legal Judgment Prediction--we demonstrated robust
performance through carefully engineered ensembles and effective prompt-based
reasoning strategies. Our findings highlight the potential of hybrid models
integrating traditional IR techniques with contemporary generative models,
providing a valuable reference for future advancements in legal information
processing.

</details>


### [5] [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)
*Fengyu She,Nan Wang,Hongfei Wu,Ziyi Wan,Jingmian Wang,Chang Wang*

Main category: cs.CL

TL;DR: This paper presents SciGPT, a domain-adapted foundation model for scientific literature understanding, and ScienceBench, an open source benchmark to evaluate scientific LLMs. SciGPT outperforms GPT-4o in core scientific tasks and shows strong robustness in unseen scientific tasks.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of scientific literature creates a bottleneck for researchers to efficiently synthesize knowledge. General-purpose LLMs often fail to capture scientific domain-specific nuances and struggle with complex scientific tasks, limiting their utility for interdisciplinary research.

Method: SciGPT is built on the Qwen3 architecture and incorporates three key innovations: low-cost domain distillation via a two-stage pipeline, a Sparse Mixture-of-Experts (SMoE) attention mechanism, and knowledge-aware adaptation integrating domain ontologies.

Result: Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in core scientific tasks including sequence labeling, generation, and inference. It also exhibits strong robustness in unseen scientific tasks.

Conclusion: SciGPT demonstrates potential to facilitate AI-augmented scientific discovery by outperforming GPT-4o in core scientific tasks and showing strong robustness in unseen scientific tasks.

Abstract: Scientific literature is growing exponentially, creating a critical
bottleneck for researchers to efficiently synthesize knowledge. While
general-purpose Large Language Models (LLMs) show potential in text processing,
they often fail to capture scientific domain-specific nuances (e.g., technical
jargon, methodological rigor) and struggle with complex scientific tasks,
limiting their utility for interdisciplinary research. To address these gaps,
this paper presents SciGPT, a domain-adapted foundation model for scientific
literature understanding and ScienceBench, an open source benchmark tailored to
evaluate scientific LLMs.
  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:
(1) low-cost domain distillation via a two-stage pipeline to balance
performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention
mechanism that cuts memory consumption by 55\% for 32,000-token long-document
reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to
bridge interdisciplinary knowledge gaps.
  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in
core scientific tasks including sequence labeling, generation, and inference.
It also exhibits strong robustness in unseen scientific tasks, validating its
potential to facilitate AI-augmented scientific discovery.

</details>


### [6] [No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models](https://arxiv.org/abs/2509.08075)
*Flor Miriam Plaza-del-Arco,Paul Röttger,Nino Scherrer,Emanuele Borgonovo,Elmar Plischke,Dirk Hovy*

Main category: cs.CL

TL;DR: 本研究分析了15种社会人口学角色对大型语言模型错误拒绝的影响，并提出了一种基于蒙特卡洛的方法来量化这一问题。结果显示，随着模型能力的提高，角色对拒绝率的影响逐渐减小。


<details>
  <summary>Details</summary>
Motivation: 最近的工作表明，角色提示可能导致模型错误地拒绝用户请求。然而，没有工作完全量化了这个问题的范围。

Method: 我们提出了一种基于蒙特卡洛的方法，以样本高效的方式量化这个问题。

Result: 随着模型变得越来越强大，人格对拒绝率的影响越来越小。某些社会人口学角色在某些模型中增加了错误拒绝，这表明对齐策略或安全机制中存在潜在的偏见。然而，我们发现模型选择和任务显著影响错误拒绝，尤其是在敏感内容任务中。

Conclusion: 我们的研究结果表明，人格效应可能被高估了，可能是由于其他因素造成的。

Abstract: Large language models (LLMs) are increasingly integrated into our daily lives
and personalized. However, LLM personalization might also increase unintended
side effects. Recent work suggests that persona prompting can lead models to
falsely refuse user requests. However, no work has fully quantified the extent
of this issue. To address this gap, we measure the impact of 15
sociodemographic personas (based on gender, race, religion, and disability) on
false refusal. To control for other factors, we also test 16 different models,
3 tasks (Natural Language Inference, politeness, and offensiveness
classification), and nine prompt paraphrases. We propose a Monte Carlo-based
method to quantify this issue in a sample-efficient manner. Our results show
that as models become more capable, personas impact the refusal rate less and
less. Certain sociodemographic personas increase false refusal in some models,
which suggests underlying biases in the alignment strategies or safety
mechanisms. However, we find that the model choice and task significantly
influence false refusals, especially in sensitive content tasks. Our findings
suggest that persona effects have been overestimated, and might be due to other
factors.

</details>


### [7] [Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression](https://arxiv.org/abs/2509.08093)
*Nathaniel Imel,Noga Zaslavsky*

Main category: cs.CL

TL;DR: This paper explores whether large language models can evolve human-like semantic systems, finding that they can do so through mechanisms similar to those found in human languages.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs can evolve efficient human-like semantic systems, given that they are not trained for the objective of achieving near-optimal compression via the Information Bottleneck (IB) complexity-accuracy principle.

Method: The study focused on the domain of color as a testbed for cognitive theories of categorization and replicated two influential human behavioral studies with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct). It also simulated cultural evolution of pseudo color-naming systems in LLMs via iterated in-context language learning.

Result: Gemini aligns well with the naming patterns of native English speakers and achieves a significantly high IB-efficiency score, while Llama exhibits an efficient but lower complexity system compared to English. LLMs restructure initially random systems towards greater IB-efficiency and increased alignment with patterns observed across the world's languages.

Conclusion: LLMs are capable of evolving perceptually grounded, human-like semantic systems, driven by the same fundamental principle that governs semantic efficiency across human languages.

Abstract: Converging evidence suggests that systems of semantic categories across human
languages achieve near-optimal compression via the Information Bottleneck (IB)
complexity-accuracy principle. Large language models (LLMs) are not trained for
this objective, which raises the question: are LLMs capable of evolving
efficient human-like semantic systems? To address this question, we focus on
the domain of color as a key testbed of cognitive theories of categorization
and replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two
influential human behavioral studies. First, we conduct an English color-naming
study, showing that Gemini aligns well with the naming patterns of native
English speakers and achieves a significantly high IB-efficiency score, while
Llama exhibits an efficient but lower complexity system compared to English.
Second, to test whether LLMs simply mimic patterns in their training data or
actually exhibit a human-like inductive bias toward IB-efficiency, we simulate
cultural evolution of pseudo color-naming systems in LLMs via iterated
in-context language learning. We find that akin to humans, LLMs iteratively
restructure initially random systems towards greater IB-efficiency and
increased alignment with patterns observed across the world's languages. These
findings demonstrate that LLMs are capable of evolving perceptually grounded,
human-like semantic systems, driven by the same fundamental principle that
governs semantic efficiency across human languages.

</details>


### [8] [MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion](https://arxiv.org/abs/2509.08105)
*Kosei Uemura,David Guzmán,Quang Phuoc Nguyen,Jesujoba Oluwadara Alabi,En-shiun Annie Lee,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: MERLIN是一个两阶段的模型堆叠框架，通过课程学习策略和少量DoRA权重的适应，在低资源和高资源设置中都表现出了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语中表现出色，但在许多低资源语言（LRLs）中仍难以进行复杂推理。现有的编码器-解码器方法如LangBridge和MindMerger在中等和高资源语言中提高了准确性，但它们在LRLs上仍有很大差距。

Method: MERLIN是一个两阶段的模型堆叠框架，应用了从通用双语平行文本到任务特定数据的课程学习策略，并仅适应了一小部分DoRA权重。

Result: MERLIN在AfriMGSM基准测试中将精确匹配准确率提高了12.9个百分点，并超过了GPT-4o-mini。它在MGSM和MSVAMP上也分别提高了0.9和2.8个百分点。

Conclusion: MERLIN在AfriMGSM基准测试中比MindMerger提高了12.9个百分点，并且优于GPT-4o-mini。它还在MGSM和MSVAMP上取得了稳定的增益，证明了其在低资源和高资源设置中的有效性。

Abstract: Large language models excel in English but still struggle with complex
reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder
methods such as LangBridge and MindMerger raise accuracy on mid and
high-resource languages, yet they leave a large gap on LRLs. We present MERLIN,
a two-stage model-stacking framework that applies a curriculum learning
strategy -- from general bilingual bitext to task-specific data -- and adapts
only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves
exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.
It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),
demonstrating effectiveness across both low and high-resource settings.

</details>


### [9] [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)
*Nivedha Sivakumar,Natalie Mackraz,Samira Khorshidi,Krishna Patel,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 我们的研究显示，基于提示的去偏策略并不能有效防止偏见的传递，这表明在内在模型中纠正偏见可能有助于防止其传播到下游任务。


<details>
  <summary>Details</summary>
Motivation: 先前的工作假设从预训练的大语言模型（LLMs）到适应模型的偏见不会转移。我们想验证这个假设是否成立。

Method: 我们研究了因果模型在提示适应下的偏见转移假设（BTH），并评估了几种基于提示的去偏策略。

Result: 我们发现偏见可以通过提示传递，并且流行的基于提示的缓解方法并不总是能防止偏见的传递。此外，我们发现当改变少样本组成参数时，偏见仍然高度相关。

Conclusion: 我们的研究结果表明，在内在模型中纠正偏差，可能会防止偏差传播到下游任务。

Abstract: A dangerous assumption that can be made from prior work on the bias transfer
hypothesis (BTH) is that biases do not transfer from pre-trained large language
models (LLMs) to adapted models. We invalidate this assumption by studying the
BTH in causal models under prompt adaptations, as prompting is an extremely
popular and accessible adaptation strategy used in real-world applications. In
contrast to prior work, we find that biases can transfer through prompting and
that popular prompt-based mitigation methods do not consistently prevent biases
from transferring. Specifically, the correlation between intrinsic biases and
those after prompt adaptation remain moderate to strong across demographics and
tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age
(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we
find that biases remain strongly correlated when varying few-shot composition
parameters, such as sample size, stereotypical content, occupational
distribution and representational balance (rho >= 0.90). We evaluate several
prompt-based debiasing strategies and find that different approaches have
distinct strengths, but none consistently reduce bias transfer across models,
tasks or demographics. These results demonstrate that correcting bias, and
potentially improving reasoning ability, in intrinsic models may prevent
propagation of biases to downstream tasks.

</details>


### [10] [Verbalized Algorithms](https://arxiv.org/abs/2509.08150)
*Supriya Lall,Christian Farrell,Hari Pathanjaly,Marko Pavic,Sarvesh Chezhian,Masataro Asai*

Main category: cs.CL

TL;DR: 本文提出了一种称为'口头算法'（VAs）的范式，利用经典算法与LLM结合，以提高在排序和聚类任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的一次性查询LLM的方法可能无法获得正确的答案，特别是在需要推理的任务中。因此，需要一种更可靠的方法来利用LLM的能力。

Method: 提出了一种称为'口头算法'（VAs）的范式，利用具有已建立理论理解的经典算法。VAs将任务分解为对自然语言字符串的简单基本操作，这些操作应该能够可靠地回答，并将LLM的作用限制在这些简单任务上。例如，对于对自然语言字符串进行排序的任务，'口头排序'使用LLM作为二进制比较Oracle，在已知且经过充分分析的排序算法（如位序排序网络）中使用。

Result: 该方法在排序和聚类任务中表现出有效性。

Conclusion: 该方法在排序和聚类任务中表现出有效性。

Abstract: Instead of querying LLMs in a one-shot manner and hoping to get the right
answer for a reasoning task, we propose a paradigm we call \emph{verbalized
algorithms} (VAs), which leverage classical algorithms with established
theoretical understanding. VAs decompose a task into simple elementary
operations on natural language strings that they should be able to answer
reliably, and limit the scope of LLMs to only those simple tasks. For example,
for sorting a series of natural language strings, \emph{verbalized sorting}
uses an LLM as a binary comparison oracle in a known and well-analyzed sorting
algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of
this approach on sorting and clustering tasks.

</details>


### [11] [Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions](https://arxiv.org/abs/2509.08217)
*Eve Fleisig,Matthias Orlikowski,Philipp Cimiano,Dan Klein*

Main category: cs.CL

TL;DR: 本文研究了标注者过滤方法对保留数据标签变化的影响，发现现有方法可能误删有价值的标注者，建议采用更保守的设置并考虑标签多样性。


<details>
  <summary>Details</summary>
Motivation: 机器学习数据集必须保留数据标签的变化，同时过滤掉垃圾或低质量的回答，但如何平衡标注者的可靠性和代表性是一个问题。

Method: 我们评估了一系列用于标注者过滤的启发式方法如何影响主观任务中变化的保留。

Result: 发现这些方法在保留变化方面效果不佳，保守的标注者移除设置（<5%）最好，之后所有测试的方法都会增加与真实平均标签的平均绝对误差。

Conclusion: 这些结果突显了需要考虑标签多样性的垃圾信息移除方法。

Abstract: For machine learning datasets to accurately represent diverse opinions in a
population, they must preserve variation in data labels while filtering out
spam or low-quality responses. How can we balance annotator reliability and
representation? We empirically evaluate how a range of heuristics for annotator
filtering affect the preservation of variation on subjective tasks. We find
that these methods, designed for contexts in which variation from a single
ground-truth label is considered noise, often remove annotators who disagree
instead of spam annotators, introducing suboptimal tradeoffs between accuracy
and label diversity. We find that conservative settings for annotator removal
(<5%) are best, after which all tested methods increase the mean absolute error
from the true average label. We analyze performance on synthetic spam to
observe that these methods often assume spam annotators are less random than
real spammers tend to be: most spammers are distributionally indistinguishable
from real annotators, and the minority that are distinguishable tend to give
fixed answers, not random ones. Thus, tasks requiring the preservation of
variation reverse the intuition of existing spam filtering methods: spammers
tend to be less random than non-spammers, so metrics that assume variation is
spam fare worse. These results highlight the need for spam removal methods that
account for label diversity.

</details>


### [12] [Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection](https://arxiv.org/abs/2509.08304)
*Yehudit Aperstein,Alon Gottlib,Gal Benita,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架来建模语义覆盖关系（SCR），通过基于问答的方法评估文档之间的语义关系，并构建了一个合成数据集用于基准测试和训练。研究结果显示，判别模型在SCR预测中表现优于生成方法。


<details>
  <summary>Details</summary>
Motivation: 理解信息如何在不同格式的文档之间共享对于信息检索、摘要和内容对齐等任务至关重要。现有的方法可能无法充分捕捉文档之间的语义关系，因此需要一种新的框架来建模语义覆盖关系（SCR）。

Method: 我们采用基于问答（QA）的方法，使用跨文档的共享问题的答案性作为语义覆盖的指标。我们构建了一个从SQuAD语料库派生的合成数据集，通过改写源段落并选择性地省略信息，实现了对内容重叠的精确控制。

Result: 我们的研究结果表明，判别模型在SCR预测中显著优于生成方法，RoBERTa-base模型取得了最高的准确率61.4%，而基于随机森林的模型在宏F1分数上表现最佳，为52.9%。QA提供了一种有效的视角来评估风格多样的文本之间的语义关系，为当前模型推理信息的能力提供了见解。

Conclusion: 我们的研究结果表明，判别模型在SCR预测中显著优于生成方法，RoBERTa-base模型取得了最高的准确率61.4%，而基于随机森林的模型在宏F1分数上表现最佳，为52.9%。QA提供了一种有效的视角来评估风格多样的文本之间的语义关系，为当前模型推理信息的能力提供了见解。

Abstract: Understanding how information is shared across documents, regardless of the
format in which it is expressed, is critical for tasks such as information
retrieval, summarization, and content alignment. In this work, we introduce a
novel framework for modelling Semantic Coverage Relations (SCR), which
classifies document pairs based on how their informational content aligns. We
define three core relation types: equivalence, where both texts convey the same
information using different textual forms or styles; inclusion, where one
document fully contains the information of another and adds more; and semantic
overlap, where each document presents partially overlapping content. To capture
these relations, we adopt a question answering (QA)-based approach, using the
answerability of shared questions across documents as an indicator of semantic
coverage. We construct a synthetic dataset derived from the SQuAD corpus by
paraphrasing source passages and selectively omitting information, enabling
precise control over content overlap. This dataset allows us to benchmark
generative language models and train transformer-based classifiers for SCR
prediction. Our findings demonstrate that discriminative models significantly
outperform generative approaches, with the RoBERTa-base model achieving the
highest accuracy of 61.4% and the Random Forest-based model showing the best
balance with a macro-F1 score of 52.9%. The results show that QA provides an
effective lens for assessing semantic relations across stylistically diverse
texts, offering insights into the capacity of current models to reason about
information beyond surface similarity. The dataset and code developed in this
study are publicly available to support reproducibility.

</details>


### [13] [Toward Subtrait-Level Model Explainability in Automated Writing Evaluation](https://arxiv.org/abs/2509.08345)
*Alejandro Andrade-Lotero,Lee Becker,Joshua Southerland,Scott Hellman*

Main category: cs.CL

TL;DR: 本文通过生成式语言模型进行可解释性和子特质评分原型设计，展示了人类和自动化子特质评分之间的适度相关性，为教育工作者和学生提供了解密分数的细节。


<details>
  <summary>Details</summary>
Motivation: 子特质（潜在特质成分）评估为提高自动化写作评分的透明度提供了有前景的路径。

Method: 我们使用生成式语言模型进行可解释性和子特质评分原型设计。

Result: 我们展示了人类子特质和特质评分之间，以及自动化和人类子特质评分之间的适度相关性。

Conclusion: 我们的方法为教育工作者和学生解密分数提供了细节。

Abstract: Subtrait (latent-trait components) assessment presents a promising path
toward enhancing transparency of automated writing scores. We prototype
explainability and subtrait scoring with generative language models and show
modest correlation between human subtrait and trait scores, and between
automated and human subtrait scores. Our approach provides details to demystify
scores for educators and students.

</details>


### [14] [Automatic Detection of Inauthentic Templated Responses in English Language Assessments](https://arxiv.org/abs/2509.08355)
*Yashad Samant,Lee Becker,Scott Hellman,Bradley Behan,Sarah Hughes,Joshua Southerland*

Main category: cs.CL

TL;DR: 本文介绍了一种基于机器学习的方法来检测高风险英语评估中的非真实、模板化回答，并强调了定期更新模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 在高风险的英语语言评估中，低技能的考生可能会使用称为“模板”的记忆材料来欺骗自动评分系统。

Method: 本文描述了一种基于机器学习的方法来解决AuDITR任务。

Result: 本文介绍了AuDITR任务，并展示了定期更新这些模型的重要性。

Conclusion: 本文介绍了AuDITR任务，并展示了定期更新这些模型的重要性。

Abstract: In high-stakes English Language Assessments, low-skill test takers may employ
memorized materials called ``templates'' on essay questions to ``game'' or fool
the automated scoring system. In this study, we introduce the automated
detection of inauthentic, templated responses (AuDITR) task, describe a machine
learning-based approach to this task and illustrate the importance of regularly
updating these models in production.

</details>


### [15] [<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)
*Sergey Pletenev,Daniil Moskovskiy,Alexander Panchenko*

Main category: cs.CL

TL;DR: 研究显示，尽管大型语言模型能生成合成数据，但在文本净化任务中，其性能远不如人类生成的数据。


<details>
  <summary>Details</summary>
Motivation: 研究现代大型语言模型在敏感领域如文本净化中的表现，并探索使用合成数据作为人类数据的替代方案。

Method: 使用Llama 3和Qwen激活修补模型生成合成有毒文本，与人类生成的数据进行比较。

Result: 微调合成数据的模型表现明显低于人类数据训练的模型，联合指标下降高达30%。

Conclusion: 当前大型语言模型在文本净化领域存在局限性，强调了多样化的人工标注数据对于构建稳健净化系统的重要性。

Abstract: Modern Large Language Models (LLMs) are excellent at generating synthetic
data. However, their performance in sensitive domains such as text
detoxification has not received proper attention from the scientific community.
This paper explores the possibility of using LLM-generated synthetic toxic data
as an alternative to human-generated data for training models for
detoxification. Using Llama 3 and Qwen activation-patched models, we generated
synthetic toxic counterparts for neutral texts from ParaDetox and SST-2
datasets. Our experiments show that models fine-tuned on synthetic data
consistently perform worse than those trained on human data, with a drop in
performance of up to 30% in joint metrics. The root cause is identified as a
critical lexical diversity gap: LLMs generate toxic content using a small,
repetitive vocabulary of insults that fails to capture the nuances and variety
of human toxicity. These findings highlight the limitations of current LLMs in
this domain and emphasize the continued importance of diverse, human-annotated
data for building robust detoxification systems.

</details>


### [16] [Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model](https://arxiv.org/abs/2509.08381)
*Yu Cheng Chih,Yong Hao Hou*

Main category: cs.CL

TL;DR: 本研究提出了一种基于LLaMA的十亿参数模型ETLCH，通过低秩适应在少量样本下进行微调，实现了在JSON提取、知识图谱提取和命名实体识别任务中的高性能，证明了小型模型在资源受限环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在金融合规报告、法律文档分析和多语言知识库构建等领域，部署大型语言模型（LLMs）进行结构化数据提取对于较小的团队来说往往不切实际，因为运行大型架构的成本高昂，而且准备高质量的大规模数据集困难。大多数最近的指令微调研究集中在七十亿参数或更大的模型上，而关于较小模型在低资源、多任务条件下的可靠性证据有限。

Method: ETLCH是一种基于LLaMA的十亿参数模型，通过低秩适应在每个任务仅使用数百到一千个样本进行微调，用于JSON提取、知识图谱提取和命名实体识别。

Result: 尽管规模较小，ETLCH在大多数评估指标上都优于强大的基线模型，在最低数据规模下也观察到了显著的提升。

Conclusion: 研究表明，经过良好微调的小型模型可以在计算成本较低的情况下提供稳定且准确的结构化输出，从而在资源受限的环境中实现成本效益高且可靠的信息化提取流程。

Abstract: Deploying large language models (LLMs) for structured data extraction in
domains such as financial compliance reporting, legal document analytics, and
multilingual knowledge base construction is often impractical for smaller teams
due to the high cost of running large architectures and the difficulty of
preparing large, high-quality datasets. Most recent instruction-tuning studies
focus on seven-billion-parameter or larger models, leaving limited evidence on
whether much smaller models can work reliably under low-resource, multi-task
conditions. This work presents ETLCH, a billion-parameter LLaMA-based model
fine-tuned with low-rank adaptation on only a few hundred to one thousand
samples per task for JSON extraction, knowledge graph extraction, and named
entity recognition. Despite its small scale, ETLCH outperforms strong baselines
across most evaluation metrics, with substantial gains observed even at the
lowest data scale. These findings demonstrate that well-tuned small models can
deliver stable and accurate structured outputs at a fraction of the
computational cost, enabling cost-effective and reliable information extraction
pipelines in resource-constrained environments.

</details>


### [17] [CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework](https://arxiv.org/abs/2509.08438)
*Jinzhong Ning,Paerhati Tulajiang,Yingying Le,Yijia Zhang,Yuanyuan Sun,Hongfei Lin,Haifeng Liu*

Main category: cs.CL

TL;DR: 本文引入了一个大规模的真实人类语音数据集CommonVoice-SpeechRE，并提出了一种新的框架RPG-MoGe，以解决SpeechRE研究中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的SpeechRE研究依赖于合成数据，缺乏足够的真实人类语音数据，并且现有模型存在刚性的单序生成模板和弱语义对齐的问题，这限制了它们的性能。

Method: 本文提出了Relation Prompt-Guided Multi-Order Generative Ensemble (RPG-MoGe) 框架，该框架包含多阶三元组生成集成策略和基于CNN的潜在关系预测头，用于指导跨模态对齐和准确的三元组生成。

Result: 实验结果表明，本文提出的RPG-MoGe框架在SpeechRE任务上优于最先进的方法，并提供了基准数据集和有效的解决方案。

Conclusion: 本文提出了一个大规模的真实人类语音数据集CommonVoice-SpeechRE和一种新的框架RPG-MoGe，以解决现有SpeechRE研究中的问题。实验表明，该方法优于最先进的方法，并为实际应用提供了基准数据集和有效的解决方案。

Abstract: Speech Relation Extraction (SpeechRE) aims to extract relation triplets
directly from speech. However, existing benchmark datasets rely heavily on
synthetic data, lacking sufficient quantity and diversity of real human speech.
Moreover, existing models also suffer from rigid single-order generation
templates and weak semantic alignment, substantially limiting their
performance. To address these challenges, we introduce CommonVoice-SpeechRE, a
large-scale dataset comprising nearly 20,000 real-human speech samples from
diverse speakers, establishing a new benchmark for SpeechRE research.
Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative
Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet
generation ensemble strategy, leveraging data diversity through diverse element
orders during both training and inference, and (2) CNN-based latent relation
prediction heads that generate explicit relation prompts to guide cross-modal
alignment and accurate triplet generation. Experiments show our approach
outperforms state-of-the-art methods, providing both a benchmark dataset and an
effective solution for real-world SpeechRE. The source code and dataset are
publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.

</details>


### [18] [Adversarial Attacks Against Automated Fact-Checking: A Survey](https://arxiv.org/abs/2509.08463)
*Fanzhen Liu,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Jia Wu,Jian Yang,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文综述了针对事实核查系统的对抗性攻击，分析了现有攻击方法及其对自动事实核查系统的影响，并探讨了对抗意识防御措施和开放研究问题，强调了构建稳健事实核查框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 在信息虚假传播的时代，事实核查在验证声明和促进可靠信息方面发挥着关键作用。尽管自动化事实核查已经取得了显著进展，但现有的系统仍然容易受到对抗性攻击的干扰，这些攻击可以操纵或生成声明、证据或声明-证据对。这些攻击可能会扭曲真相，误导决策者，并最终削弱事实核查模型的可靠性。尽管对抗性攻击在自动事实核查系统中的研究兴趣日益增长，但缺乏全面的概述。

Method: 本文提供了对针对事实核查系统的对抗性攻击的首次深入综述，对现有攻击方法进行了分类，并评估了它们对自动事实核查系统的影响。此外，还探讨了最近的对抗意识防御措施，并指出了需要进一步探索的开放研究问题。

Result: 本文提供了对针对事实核查系统的对抗性攻击的首次深入综述，对现有攻击方法进行了分类，并评估了它们对自动事实核查系统的影响。此外，还探讨了最近的对抗意识防御措施，并指出了需要进一步探索的开放研究问题。

Conclusion: 本文强调了构建能够抵御对抗性操纵的稳健事实核查框架的紧迫性，以保持高验证准确性。

Abstract: In an era where misinformation spreads freely, fact-checking (FC) plays a
crucial role in verifying claims and promoting reliable information. While
automated fact-checking (AFC) has advanced significantly, existing systems
remain vulnerable to adversarial attacks that manipulate or generate claims,
evidence, or claim-evidence pairs. These attacks can distort the truth, mislead
decision-makers, and ultimately undermine the reliability of FC models. Despite
growing research interest in adversarial attacks against AFC systems, a
comprehensive, holistic overview of key challenges remains lacking. These
challenges include understanding attack strategies, assessing the resilience of
current models, and identifying ways to enhance robustness. This survey
provides the first in-depth review of adversarial attacks targeting FC,
categorizing existing attack methodologies and evaluating their impact on AFC
systems. Additionally, we examine recent advancements in adversary-aware
defenses and highlight open research questions that require further
exploration. Our findings underscore the urgent need for resilient FC
frameworks capable of withstanding adversarial manipulations in pursuit of
preserving high verification accuracy.

</details>


### [19] [Acquiescence Bias in Large Language Models](https://arxiv.org/abs/2509.08480)
*Daniel Braun*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型是否存在顺从偏差，发现它们倾向于回答'否'，而不是像人类那样倾向于同意。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型很容易受到输入中相对较小的变化的影响，并且是基于人类生成的数据训练的，因此合理地假设它们可能会表现出类似的倾向。

Method: 我们进行了一项研究，调查了在不同模型、任务和语言（英语、德语和波兰语）中大型语言模型是否存在顺从偏差。

Result: 我们的结果表明，与人类不同，大型语言模型表现出一种倾向于回答'否'的偏见，无论这是否表示同意或不同意。

Conclusion: 我们的研究结果表明，与人类不同，大型语言模型表现出一种倾向于回答'否'的偏见，无论这是否表示同意或不同意。

Abstract: Acquiescence bias, i.e. the tendency of humans to agree with statements in
surveys, independent of their actual beliefs, is well researched and
documented. Since Large Language Models (LLMs) have been shown to be very
influenceable by relatively small changes in input and are trained on
human-generated data, it is reasonable to assume that they could show a similar
tendency. We present a study investigating the presence of acquiescence bias in
LLMs across different models, tasks, and languages (English, German, and
Polish). Our results indicate that, contrary to humans, LLMs display a bias
towards answering no, regardless of whether it indicates agreement or
disagreement.

</details>


### [20] [Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text](https://arxiv.org/abs/2509.08484)
*Pia Sommerauer,Giulia Rambelli,Tommaso Caselli*

Main category: cs.CL

TL;DR: 本文研究了Persona-prompting对语言抽象性的影响，发现其在调节语言抽象性方面存在局限性，并引发了关于传播刻板印象的风险的担忧。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨Persona-prompting是否会导致生成短文本时的语言抽象性不同，这是一项已知的刻板印象标志。

Method: 我们分析了六种开放权重的LLM在三种提示条件下的输出，比较了11种基于角色的响应与通用AI助手的响应。我们引入了Self-Stereo数据集，用于分析自我报告的刻板印象。

Result: 我们的结果表明，Persona-prompting在调节语言抽象性方面存在局限性，确认了关于人格生态作为社会人口群体代表的批评，并引发了关于即使似乎在唤起边缘化群体的声音时传播刻板印象的风险的担忧。

Conclusion: 我们的研究结果突显了Persona-prompting在调节语言抽象性方面的局限性，确认了关于人格生态作为社会人口群体代表的批评，并引发了关于即使似乎在唤起边缘化群体的声音时传播刻板印象的风险的担忧。

Abstract: Persona-prompting is a growing strategy to steer LLMs toward simulating
particular perspectives or linguistic styles through the lens of a specified
identity. While this method is often used to personalize outputs, its impact on
how LLMs represent social groups remains underexplored. In this paper, we
investigate whether persona-prompting leads to different levels of linguistic
abstraction - an established marker of stereotyping - when generating short
texts linking socio-demographic categories with stereotypical or
non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias
framework, we analyze outputs from six open-weight LLMs under three prompting
conditions, comparing 11 persona-driven responses to those of a generic AI
assistant. To support this analysis, we introduce Self-Stereo, a new dataset of
self-reported stereotypes from Reddit. We measure abstraction through three
metrics: concreteness, specificity, and negation. Our results highlight the
limits of persona-prompting in modulating abstraction in language, confirming
criticisms about the ecology of personas as representative of socio-demographic
groups and raising concerns about the risk of propagating stereotypes even when
seemingly evoking the voice of a marginalized group.

</details>


### [21] [Too Helpful, Too Harmless, Too Honest or Just Right?](https://arxiv.org/abs/2509.08486)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: TrinityX 是一种基于校准专家混合的模块化对齐框架，显著提升了 LLM 在 HHH 方面的性能，并优化了计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法通常单独优化每个维度，导致权衡和不一致的行为，而 MoE 架构由于路由不准确限制了其在对齐任务中的效果。

Method: TrinityX 采用了一种基于校准路由机制的混合专家架构，将每个 HHH 维度的专家输出整合为一个统一的对齐感知表示。

Result: TrinityX 在三个标准对齐基准测试中表现出色，相对提升分别为 32.5%、33.9% 和 28.4%，同时减少了超过 40% 的内存使用和推理延迟。

Conclusion: TrinityX 是一种模块化的对齐框架，通过在 Transformer 架构中引入校准专家混合（MoCaE），有效提升了 LLM 在 Helpfulness、Harmlessness 和 Honesty 方面的性能，并且在内存使用和推理延迟方面也有所优化。

Abstract: Large Language Models (LLMs) exhibit strong performance across a wide range
of NLP tasks, yet aligning their outputs with the principles of Helpfulness,
Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing
methods often optimize for individual alignment dimensions in isolation,
leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)
architectures offer modularity, they suffer from poorly calibrated routing,
limiting their effectiveness in alignment tasks. We propose TrinityX, a modular
alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)
within the Transformer architecture. TrinityX leverages separately trained
experts for each HHH dimension, integrating their outputs through a calibrated,
task-adaptive routing mechanism that combines expert signals into a unified,
alignment-aware representation. Extensive experiments on three standard
alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and
TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,
achieving relative improvements of 32.5% in win rate, 33.9% in safety score,
and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and
inference latency by over 40% compared to prior MoE-based approaches. Ablation
studies highlight the importance of calibrated routing, and cross-model
evaluations confirm TrinityX's generalization across diverse LLM backbones.

</details>


### [22] [CM-Align: Consistency-based Multilingual Alignment for Large Language Models](https://arxiv.org/abs/2509.08541)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种基于一致性的数据选择方法（CM-Align），以构建高质量的多语言偏好数据，从而提高多语言对齐性能。实验结果证明了该方法的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 当前的方法存在两个局限性，导致多语言偏好数据嘈杂，进而限制了对齐性能：1）并非所有英文响应都是高质量的，使用低质量的响应可能会误导其他语言的对齐。2）当前方法通常使用有偏见或启发式的方法来构建多语言偏好对。

Method: 我们设计了一种基于一致性的数据选择方法，以构建高质量的多语言偏好数据来改善多语言对齐（CM-Align）。具体来说，我们的方法包括两部分：一致性引导的英文参考选择和跨语言一致性为基础的多语言偏好数据构建。

Result: 实验结果表明，我们的方法在三个大型语言模型和三个常见任务上都有效且优越。

Conclusion: 实验结果表明，我们的方法在三个大型语言模型和三个常见任务上都有效且优越，这进一步表明了构建高质量偏好数据的必要性。

Abstract: Current large language models (LLMs) generally show a significant performance
gap in alignment between English and other languages. To bridge this gap,
existing research typically leverages the model's responses in English as a
reference to select the best/worst responses in other languages, which are then
used for Direct Preference Optimization (DPO) training. However, we argue that
there are two limitations in the current methods that result in noisy
multilingual preference data and further limited alignment performance: 1) Not
all English responses are of high quality, and using a response with low
quality may mislead the alignment for other languages. 2) Current methods
usually use biased or heuristic approaches to construct multilingual preference
pairs. To address these limitations, we design a consistency-based data
selection method to construct high-quality multilingual preference data for
improving multilingual alignment (CM-Align). Specifically, our method includes
two parts: consistency-guided English reference selection and cross-lingual
consistency-based multilingual preference data construction. Experimental
results on three LLMs and three common tasks demonstrate the effectiveness and
superiority of our method, which further indicates the necessity of
constructing high-quality preference data.

</details>


### [23] [LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge](https://arxiv.org/abs/2509.08596)
*Dima Galat,Diego Molla-Aliod*

Main category: cs.CL

TL;DR: 本文探讨了如何利用大型语言模型进行信息检索，并通过集成零样本模型在生物医学问答任务中实现最先进的性能。结果表明，基于集成的零样本方法结合有效的RAG流程可以作为领域调优系统的实用且可扩展的替代方案。


<details>
  <summary>Details</summary>
Motivation: 生物医学问答（QA）由于需要从庞大、复杂且迅速发展的语料库中精确解释专业知识而面临重大挑战。我们旨在探索如何利用大型语言模型进行信息检索，并通过集成零样本模型实现高性能的领域特定QA任务。

Method: 我们探索了大型语言模型（LLMs）在信息检索（IR）中的使用，并利用零样本模型的集成在特定领域的Yes/No QA任务上实现了最先进的性能。我们的方法聚合了多个LLM变体的输出，包括来自Anthropic和Google的模型，以合成更准确和稳健的答案。

Result: 我们在BioASQ挑战任务上评估了我们的方法，结果显示集成可以超越单个LLMs，在某些情况下可以与领域调优系统相媲美或超越它们，同时保持通用性并避免昂贵的微调或标记数据的需求。此外，我们的研究揭示了上下文长度和性能之间的关系：虽然扩展的上下文旨在提供有价值的证据，但它们同时可能带来信息稀释和模型迷失的风险。

Conclusion: 我们的结果表明，基于集成的零样本方法，当与有效的RAG流程结合时，可以作为生物医学问答领域调优系统的实用且可扩展的替代方案。

Abstract: Biomedical question answering (QA) poses significant challenges due to the
need for precise interpretation of specialized knowledge drawn from a vast,
complex, and rapidly evolving corpus. In this work, we explore how large
language models (LLMs) can be used for information retrieval (IR), and an
ensemble of zero-shot models can accomplish state-of-the-art performance on a
domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge
tasks, we show that ensembles can outperform individual LLMs and in some cases
rival or surpass domain-tuned systems - all while preserving generalizability
and avoiding the need for costly fine-tuning or labeled data. Our method
aggregates outputs from multiple LLM variants, including models from Anthropic
and Google, to synthesize more accurate and robust answers. Moreover, our
investigation highlights a relationship between context length and performance:
while expanded contexts are meant to provide valuable evidence, they
simultaneously risk information dilution and model disorientation. These
findings emphasize IR as a critical foundation in Retrieval-Augmented
Generation (RAG) approaches for biomedical QA systems. Precise, focused
retrieval remains essential for ensuring LLMs operate within relevant
information boundaries when generating answers from retrieved documents. Our
results establish that ensemble-based zero-shot approaches, when paired with
effective RAG pipelines, constitute a practical and scalable alternative to
domain-tuned systems for biomedical question answering.

</details>


### [24] [Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications](https://arxiv.org/abs/2509.08604)
*Anran Li,Lingfei Qian,Mengmeng Du,Yu Yin,Yan Hu,Zihao Sun,Yihang Fu,Erica Stutz,Xuguang Ai,Qianqian Xie,Rui Zhu,Jimin Huang,Yifan Yang,Siru Liu,Yih-Chung Tham,Lucila Ohno-Machado,Hyunghoon Cho,Zhiyong Lu,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 本研究评估了大规模语言模型在医学领域中的记忆情况，发现记忆普遍存在，并提出了实用的建议以优化模型的表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估大规模语言模型在医学领域中的记忆程度，以了解其对医学应用的影响，并提供实用的建议来优化模型的表现。

Method: 研究对大规模语言模型在医学领域的记忆进行了全面评估，分析了其普遍性、特征、数量和潜在的下游影响。研究系统地分析了常见的适应场景：(1) 在医学语料库上继续预训练，(2) 在标准医学基准上微调，(3) 在真实临床数据上微调，包括来自耶鲁纽黑文健康系统的超过13,000个独特的住院记录。

Result: 研究结果表明，记忆在所有适应场景中都很普遍，并且比一般领域报告的要高。记忆影响了大规模语言模型在医学领域的发展和采用，并可以分为三种类型：有益（如准确回忆临床指南和生物医学参考文献）、无信息（如重复的免责声明或模板化的医疗文档语言）和有害（如重新生成数据集特定或敏感的临床内容）。

Conclusion: 研究发现，记忆在医学领域的大规模语言模型中普遍存在，并且比一般领域报告的要高。记忆可以分为有益、无信息和有害三种类型。基于这些发现，研究提供了实用的建议，以促进有益的记忆，减少无信息的记忆，并减轻有害的记忆，以防止敏感或可识别的患者信息泄露。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
medicine. To date, LLMs have been widely applied to tasks such as diagnostic
assistance, medical question answering, and clinical information synthesis.
However, a key open question remains: to what extent do LLMs memorize medical
training data. In this study, we present the first comprehensive evaluation of
memorization of LLMs in medicine, assessing its prevalence (how frequently it
occurs), characteristics (what is memorized), volume (how much content is
memorized), and potential downstream impacts (how memorization may affect
medical applications). We systematically analyze common adaptation scenarios:
(1) continued pretraining on medical corpora, (2) fine-tuning on standard
medical benchmarks, and (3) fine-tuning on real-world clinical data, including
over 13,000 unique inpatient records from Yale New Haven Health System. The
results demonstrate that memorization is prevalent across all adaptation
scenarios and significantly higher than reported in the general domain.
Memorization affects both the development and adoption of LLMs in medicine and
can be categorized into three types: beneficial (e.g., accurate recall of
clinical guidelines and biomedical references), uninformative (e.g., repeated
disclaimers or templated medical document language), and harmful (e.g.,
regeneration of dataset-specific or sensitive clinical content). Based on these
findings, we offer practical recommendations to facilitate beneficial
memorization that enhances domain-specific reasoning and factual accuracy,
minimize uninformative memorization to promote deeper learning beyond
surface-level patterns, and mitigate harmful memorization to prevent the
leakage of sensitive or identifiable patient information.

</details>


### [25] [OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2509.08612)
*Xinfeng Liao,Xuanqi Chen,Lianxi Wang,Jiahuan Yang,Zhuowei Chen,Ziying Rong*

Main category: cs.CL

TL;DR: 本文提出了OTE-SGN，通过句法-语义协作注意力机制和自适应注意力融合模块，在ABSA任务中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于语法树和方面感知注意力，难以建模复杂的语义关系，且依赖线性点积特征无法捕捉非线性关联，导致无关词语的噪声掩盖关键意见词。

Method: 提出了一种基于最优传输的句法-语义图网络（OTESGN），引入了句法-语义协作注意力机制，包括句法图感知注意力和语义最优传输注意力，并采用自适应注意力融合模块和对比正则化来提高鲁棒性。

Result: OTE-SGN在Twitter和Laptop14基准测试中取得了最先进的结果，分别提高了+1.01% F1和+1.30% F1。消融研究和可视化分析证实了其在精确定位意见词和抗噪声方面的有效性。

Conclusion: OTE-SGN在Twitter和Laptop14基准测试中优于之前的最佳模型，分别提高了+1.01% F1和+1.30% F1。消融研究和可视化分析证实了其在精确定位意见词和抗噪声方面的有效性。

Abstract: Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and
determine their sentiment polarity. While dependency trees combined with
contextual semantics effectively identify aspect sentiment, existing methods
relying on syntax trees and aspect-aware attention struggle to model complex
semantic relationships. Their dependence on linear dot-product features fails
to capture nonlinear associations, allowing noisy similarity from irrelevant
words to obscure key opinion terms. Motivated by Differentiable Optimal
Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph
Network (OTESGN), which introduces a Syntactic-Semantic Collaborative
Attention. It comprises a Syntactic Graph-Aware Attention for mining latent
syntactic dependencies and modeling global syntactic topology, as well as a
Semantic Optimal Transport Attention designed to uncover fine-grained semantic
alignments amidst textual noise, thereby accurately capturing sentiment signals
obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates
these heterogeneous features, and contrastive regularization further improves
robustness. Experiments demonstrate that OTESGN achieves state-of-the-art
results, outperforming previous best models by +1.01% F1 on Twitter and +1.30%
F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its
efficacy in precise localization of opinion words and noise resistance.

</details>


### [26] [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)
*Hyunjun Kim,Junwoo Ha,Sangyoon Yu,Haon Park*

Main category: cs.CL

TL;DR: 我们提出了一个自动化框架X-Teaming Evolutionary M2S，用于发现和优化多轮到单轮（M2S）模板。通过语言模型引导进化，我们在GPT-4.1上取得了44.8%的成功率，并发现了提示长度与分数之间的正相关关系。


<details>
  <summary>Details</summary>
Motivation: 多轮到单轮（M2S）将迭代红队作战压缩为一个结构化提示，但之前的工作依赖于少量手动编写的模板。我们希望提供一个自动化框架来发现和优化M2S模板。

Method: 我们提出了X-Teaming Evolutionary M2S，这是一个通过语言模型引导进化发现和优化M2S模板的自动化框架。系统结合了来自12个来源的智能采样和受StrongREJECT启发的LLM作为评判者，并记录了完全可审计的日志。

Result: 通过将成功阈值设置为θ=0.70，我们获得了五代进化、两个新的模板家族以及在GPT-4.1上的44.8%总体成功率（103/230）。平衡的跨模型小组2,500次试验显示结构性增益可以转移，但因目标而异；两个模型在同一阈值下得分为零。我们还发现提示长度和分数之间存在正耦合，这促使了长度感知的判断。

Conclusion: 我们的结果表明，结构级搜索是获得更强单次探测的可重复途径，并强调了阈值校准和跨模型评估的重要性。

Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one
structured prompt, but prior work relied on a handful of manually written
templates. We present X-Teaming Evolutionary M2S, an automated framework that
discovers and optimizes M2S templates through language-model-guided evolution.
The system pairs smart sampling from 12 sources with an LLM-as-judge inspired
by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta =
0.70$, we obtain five evolutionary generations, two new template families, and
44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of
2,500 trials (judge fixed) shows that structural gains transfer but vary by
target; two models score zero at the same threshold. We also find a positive
coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route
to stronger single-turn probes and underscore the importance of threshold
calibration and cross-model evaluation. Code, configurations, and artifacts are
available at https://github.com/hyunjun1121/M2S-x-teaming.

</details>


### [27] [Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](https://arxiv.org/abs/2509.08753)
*Neil Zeghidour,Eugene Kharitonov,Manu Orsini,Václav Volhejn,Gabriel de Marmiesse,Edouard Grave,Patrick Pérez,Laurent Mazaré,Alexandre Défossez*

Main category: cs.CL

TL;DR: DSM是一种新的流式多模态序列到序列学习方法，能够在保持高性能的同时支持任意长度的序列。


<details>
  <summary>Details</summary>
Motivation: 传统的方法要么是离线的，要么需要学习输入或输出流的推进策略，而DSM提供了一种更灵活的流式多模态序列到序列学习方法。

Method: DSM通过将时间对齐移到预处理步骤，并在流之间引入适当的延迟，使用仅解码器的语言模型来建模已对齐的流。

Result: DSM在自动语音识别（ASR）和文本到语音（TTS）等主要任务上进行了广泛实验，结果表明其性能和延迟都达到了最先进的水平。

Conclusion: DSM在多个序列到序列任务中表现出色，具有状态领先性能和低延迟，并且能够处理任意长的序列。

Abstract: We introduce Delayed Streams Modeling (DSM), a flexible formulation for
streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence
generation is often cast in an offline manner, where the model consumes the
complete input sequence before generating the first output timestep.
Alternatively, streaming sequence-to-sequence rely on learning a policy for
choosing when to advance on the input stream, or write to the output stream.
DSM instead models already time-aligned streams with a decoder-only language
model. By moving the alignment to a pre-processing step,and introducing
appropriate delays between streams, DSM provides streaming inference of
arbitrary output sequences, from any input combination, making it applicable to
many sequence-to-sequence problems. In particular, given text and audio
streams, automatic speech recognition (ASR) corresponds to the text stream
being delayed, while the opposite gives a text-to-speech (TTS) model. We
perform extensive experiments for these two major sequence-to-sequence tasks,
showing that DSM provides state-of-the-art performance and latency while
supporting arbitrary long sequences, being even competitive with offline
baselines. Code, samples and demos are available at
https://github.com/kyutai-labs/delayed-streams-modeling

</details>


### [28] [Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms](https://arxiv.org/abs/2509.08778)
*Minyeong Choe,Haehyun Cho,Changho Seo,Hyunil Kim*

Main category: cs.CL

TL;DR: 研究发现，Qwen基础模型在早期层中，注意力模块比MLP模块更有利于事实回忆，这表明自回归Transformer家族中的架构差异可能导致不同的事实回忆机制。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer基于语言模型如何存储和检索事实关联对于提高可解释性和实现有针对性的模型编辑至关重要。然而，这些发现是否适用于不同的自回归架构仍然不清楚。

Method: 我们对几个模型（包括GPT、LLaMA、Qwen和DeepSeek）进行了全面评估，分析了事实信息是如何编码和访问的。

Result: 我们发现Qwen基础模型与之前模式不同：早期层中的注意力模块比MLP模块对事实回忆的贡献更大。

Conclusion: 我们的研究结果表明，即使在自回归Transformer家族中，架构差异也可能导致事实回忆的基本不同机制。

Abstract: Understanding how Transformer-based language models store and retrieve
factual associations is critical for improving interpretability and enabling
targeted model editing. Prior work, primarily on GPT-style models, has
identified MLP modules in early layers as key contributors to factual recall.
However, it remains unclear whether these findings generalize across different
autoregressive architectures. To address this, we conduct a comprehensive
evaluation of factual recall across several models -- including GPT, LLaMA,
Qwen, and DeepSeek -- analyzing where and how factual information is encoded
and accessed. Consequently, we find that Qwen-based models behave differently
from previous patterns: attention modules in the earliest layers contribute
more to factual recall than MLP modules. Our findings suggest that even within
the autoregressive Transformer family, architectural variations can lead to
fundamentally different mechanisms of factual recall.

</details>


### [29] [Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals](https://arxiv.org/abs/2509.08809)
*Cheng Chen,Haiyan Yin,Ivor Tsang*

Main category: cs.CL

TL;DR: This paper proposes a new method for evaluating LLM annotations in unsupervised environments using a student model and the CAI Ratio, which shows strong correlation with LLM accuracy.


<details>
  <summary>Details</summary>
Motivation: Evaluating the quality of LLM annotations is challenging in dynamic, unsupervised environments where oracle feedback is scarce and conventional methods fail.

Method: Proposed a novel agentic annotation paradigm where a student model collaborates with a noisy teacher (LLM) to assess and refine annotation quality without relying on oracle feedback. Introduced the Consistent and Inconsistent (CAI) Ratio as a novel unsupervised evaluation metric.

Result: Applied to ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a strong positive correlation with LLM accuracy.

Conclusion: CAI Ratio is an essential tool for unsupervised evaluation and model selection in real-world settings.

Abstract: Large Language Models (LLMs), when paired with prompt-based tasks, have
significantly reduced data annotation costs and reliance on human annotators.
However, evaluating the quality of their annotations remains challenging in
dynamic, unsupervised environments where oracle feedback is scarce and
conventional methods fail. To address this challenge, we propose a novel
agentic annotation paradigm, where a student model collaborates with a noisy
teacher (the LLM) to assess and refine annotation quality without relying on
oracle feedback. The student model, acting as an unsupervised feedback
mechanism, employs a user preference-based majority voting strategy to evaluate
the consistency of the LLM outputs. To systematically measure the reliability
of LLM-generated annotations, we introduce the Consistent and Inconsistent
(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only
quantifies the annotation quality of the noisy teacher under limited user
preferences but also plays a critical role in model selection, enabling the
identification of robust LLMs in dynamic, unsupervised environments. Applied to
ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a
strong positive correlation with LLM accuracy, establishing it as an essential
tool for unsupervised evaluation and model selection in real-world settings.

</details>


### [30] [MoVoC: Morphology-Aware Subword Construction for Geez Script Languages](https://arxiv.org/abs/2509.08812)
*Hailay Kidu Teklehaymanot,Dren Fazlija,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 本文提出了一种形态感知的子词分词方法MoVoC，用于处理低资源、形态复杂的语言，如盖兹文字书写的语言。该方法结合了形态和BPE分词，提高了语言准确性和标记效率，并提供了形态标注的数据集和分词器以供进一步研究。


<details>
  <summary>Details</summary>
Motivation: 子词分词方法常常无法保留形态边界，在低资源、形态复杂的语言中尤其明显，例如使用盖兹文字书写的语言。

Method: 我们提出了MoVoC（形态感知子词词汇构建），并训练了MoVoC-Tok，这是一种将监督形态分析整合到子词词汇中的分词器。这种混合分割方法结合了基于形态的和字节对编码（BPE）的标记，以保持形态完整性同时保持词汇意义。

Result: 虽然所提出的分词方法没有显著提高自动翻译质量，但我们观察到内在指标、MorphoScore和边界精度的一致改进，这突显了形态感知分割在提高语言准确性和标记效率方面的价值。

Conclusion: 我们的形态标注数据集和分词器将公开以支持低资源、形态丰富的语言的进一步研究。

Abstract: Subword-based tokenization methods often fail to preserve morphological
boundaries, a limitation especially pronounced in low-resource, morphologically
complex languages such as those written in the Geez script. To address this, we
present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train
MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into
the subword vocabulary. This hybrid segmentation approach combines
morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological
integrity while maintaining lexical meaning. To tackle resource scarcity, we
curate and release manually annotated morpheme data for four Geez script
languages and a morpheme-aware vocabulary for two of them. While the proposed
tokenization method does not lead to significant gains in automatic translation
quality, we observe consistent improvements in intrinsic metrics, MorphoScore,
and Boundary Precision, highlighting the value of morphology-aware segmentation
in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated
datasets and tokenizer will be publicly available to support further research
in low-resource, morphologically rich languages. Our code and data are
available on GitHub: https://github.com/hailaykidu/MoVoC

</details>


### [31] [Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora](https://arxiv.org/abs/2509.08824)
*Thales Sales Almeida,Rodrigo Nogueira,Helio Pedrini*

Main category: cs.CL

TL;DR: 本文研究了构建多语言LLM语料库的方法，并展示了语言特定过滤管道的价值。我们的方法适用于其他语言，为多语言LLM开发提供了见解。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的工作主要集中在英语上，但仍然缺乏对如何为其他语言构建有效训练语料库的理解。我们需要更好地理解如何构建高质量的多语言语料库，以提高LLM的性能。

Method: 我们探索了构建基于网络的LLM语料库的可扩展方法，并应用这些方法构建了一个新的120B标记的葡萄牙语语料库。我们使用持续预训练设置，研究了不同的数据选择和预处理策略如何影响模型在从英语转移到其他语言时的性能。

Result: 我们构建了一个新的120B标记的葡萄牙语语料库，其结果与工业级语料库相当。我们的研究结果表明，语言特定的过滤管道对于提高LLM性能非常重要。

Conclusion: 我们的研究展示了语言特定过滤管道的价值，包括教育、科学、技术、工程和数学（STEM）以及有害内容的分类器。我们表明，将模型适应到目标语言可以提高性能，这强化了高质量、语言特定数据的重要性。虽然我们的案例研究集中在葡萄牙语上，但我们的方法适用于其他语言，为多语言LLM开发提供了见解。

Abstract: The performance of large language models (LLMs) is deeply influenced by the
quality and composition of their training data. While much of the existing work
has centered on English, there remains a gap in understanding how to construct
effective training corpora for other languages. We explore scalable methods for
building web-based corpora for LLMs. We apply them to build a new 120B token
corpus in Portuguese that achieves competitive results to an industrial-grade
corpus. Using a continual pretraining setup, we study how different data
selection and preprocessing strategies affect LLM performance when
transitioning a model originally trained in English to another language. Our
findings demonstrate the value of language-specific filtering pipelines,
including classifiers for education, science, technology, engineering, and
mathematics (STEM), as well as toxic content. We show that adapting a model to
the target language leads to performance improvements, reinforcing the
importance of high-quality, language-specific data. While our case study
focuses on Portuguese, our methods are applicable to other languages, offering
insights for multilingual LLM development.

</details>


### [32] [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
*Joachim Baumann,Paul Röttger,Aleksandra Urman,Albert Wendsjö,Flor Miriam Plaza-del-Arco,Johannes B. Gruber,Dirk Hovy*

Main category: cs.CL

TL;DR: This paper explores the risks of LLM hacking in social science research, showing that implementation choices can introduce biases and errors, leading to incorrect conclusions. Even advanced models are not immune, and human annotations are essential for mitigating these risks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the risks associated with using large language models (LLMs) in social science research, particularly the potential for systematic biases and errors introduced by implementation choices such as model selection, prompting strategies, or temperature settings.

Method: The study replicated 37 data annotation tasks from 21 published social science research studies using 18 different models. It analyzed 13 million LLM labels to test 2,361 realistic hypotheses about how researcher choices affect statistical conclusions.

Result: The study found incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models and in half the hypotheses for small language models. The risk of LLM hacking decreases as effect sizes increase, highlighting the need for more rigorous verification near significance thresholds.

Conclusion: LLM hacking poses significant risks to the validity of social science research, and even highly accurate models do not completely eliminate this risk. Human annotations are crucial for reducing false positives and improving model selection, while common regression estimator correction techniques are largely ineffective.

Abstract: Large language models (LLMs) are rapidly transforming social science research
by enabling the automation of labor-intensive tasks like data annotation and
text analysis. However, LLM outputs vary significantly depending on the
implementation choices made by researchers (e.g., model selection, prompting
strategy, or temperature settings). Such variation can introduce systematic
biases and random errors, which propagate to downstream analyses and cause Type
I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks
from 21 published social science research studies with 18 different models.
Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure
how plausible researcher choices affect statistical conclusions. We find
incorrect conclusions based on LLM-annotated data in approximately one in three
hypotheses for state-of-the-art models, and in half the hypotheses for small
language models. While our findings show that higher task performance and
better general model capabilities reduce LLM hacking risk, even highly accurate
models do not completely eliminate it. The risk of LLM hacking decreases as
effect sizes increase, indicating the need for more rigorous verification of
findings near significance thresholds. Our extensive analysis of LLM hacking
mitigation techniques emphasizes the importance of human annotations in
reducing false positive findings and improving model selection. Surprisingly,
common regression estimator correction techniques are largely ineffective in
reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is
unacceptably simple. With few LLMs and just a handful of prompt paraphrases,
anything can be presented as statistically significant.

</details>


### [33] [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
*Kaiyan Zhang,Yuxin Zuo,Bingxiang He,Youbang Sun,Runze Liu,Che Jiang,Yuchen Fan,Kai Tian,Guoli Jia,Pengfei Li,Yu Fu,Xingtai Lv,Yuchen Zhang,Sihang Zeng,Shang Qu,Haozhan Li,Shijie Wang,Yuru Wang,Xinwei Long,Fangfu Liu,Xiang Xu,Jiaze Ma,Xuekai Zhu,Ermo Hua,Yihao Liu,Zonglin Li,Huayu Chen,Xiaoye Qu,Yafu Li,Weize Chen,Zhenzhao Yuan,Junqi Gao,Dong Li,Zhiyuan Ma,Ganqu Cui,Zhiyuan Liu,Biqing Qi,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文综述了强化学习在大型语言模型推理中的应用，分析了当前挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，强化学习已成为提升其推理能力的关键方法。然而，进一步扩展RL以实现人工超级智能面临诸多挑战，因此有必要重新审视该领域的发展并探索优化策略。

Method: 本文通过回顾和分析强化学习在大型语言模型和推理模型中的应用，包括基础组件、核心问题、训练资源和下游应用，来探讨RL在推理能力提升中的作用。

Result: 本文总结了强化学习在大型语言模型和推理模型中的研究现状，并识别了未来的研究机会和方向。

Conclusion: 本文综述了强化学习（RL）在大型语言模型（LLMs）推理中的最新进展，并探讨了如何通过RL将LLMs转化为推理模型（LRMs）。同时，文章指出了当前RL在扩展至人工超级智能（ASI）过程中面临的挑战，并提出了未来的研究方向。

Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilities, particularly in addressing
complex logical tasks such as mathematics and coding. As a result, RL has
emerged as a foundational methodology for transforming LLMs into LRMs. With the
rapid progress of the field, further scaling of RL for LRMs now faces
foundational challenges not only in computational resources but also in
algorithm design, training data, and infrastructure. To this end, it is timely
to revisit the development of this domain, reassess its trajectory, and explore
strategies to enhance the scalability of RL toward Artificial SuperIntelligence
(ASI). In particular, we examine research applying RL to LLMs and LRMs for
reasoning abilities, especially since the release of DeepSeek-R1, including
foundational components, core problems, training resources, and downstream
applications, to identify future opportunities and directions for this rapidly
evolving area. We hope this review will promote future research on RL for
broader reasoning models. Github:
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [34] [Measuring and mitigating overreliance is necessary for building human-compatible AI](https://arxiv.org/abs/2509.08010)
*Lujain Ibrahim,Katherine M. Collins,Sunnie S. Y. Kim,Anka Reuel,Max Lamparth,Kevin Feng,Lama Ahmad,Prajna Soni,Alia El Kattan,Merlin Stein,Siddharth Swaroop,Ilia Sucholutsky,Andrew Strait,Q. Vera Liao,Umang Bhatt*

Main category: cs.CY

TL;DR: 本文讨论了大型语言模型的过度依赖问题，并提出了解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在医疗保健和个人建议等各个领域越来越影响重要决策，过度依赖的风险也在增加。

Method: 我们综合了个体和社会层面的过度依赖风险，探讨了大型语言模型的特性、系统设计特征和用户认知偏差，并检查了历史上的过度依赖测量方法，提出了改进的方向。

Result: 我们提出了AI研究界可以采取的缓解策略，以确保大型语言模型增强而不是削弱人类能力。

Conclusion: 测量和减轻过度依赖必须成为大型语言模型研究和部署的核心。

Abstract: Large language models (LLMs) distinguish themselves from previous
technologies by functioning as collaborative "thought partners," capable of
engaging more fluidly in natural language. As LLMs increasingly influence
consequential decisions across diverse domains from healthcare to personal
advice, the risk of overreliance - relying on LLMs beyond their capabilities -
grows. This position paper argues that measuring and mitigating overreliance
must become central to LLM research and deployment. First, we consolidate risks
from overreliance at both the individual and societal levels, including
high-stakes errors, governance challenges, and cognitive deskilling. Then, we
explore LLM characteristics, system design features, and user cognitive biases
that - together - raise serious and unique concerns about overreliance in
practice. We also examine historical approaches for measuring overreliance,
identifying three important gaps and proposing three promising directions to
improve measurement. Finally, we propose mitigation strategies that the AI
research community can pursue to ensure LLMs augment rather than undermine
human capabilities.

</details>


### [35] [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants](https://arxiv.org/abs/2509.08494)
*Benjamin Sturgeon,Daniel Samuelson,Jacob Haimes,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: 本文探讨了人工智能系统在维护人类代理方面的表现，并提出了一个评估基准测试，以衡量AI在不同维度上的支持程度。


<details>
  <summary>Details</summary>
Motivation: 随着人类越来越多地将任务和决策委托给人工智能，我们可能失去对个人和集体未来的控制。因此，需要评估AI系统在维护人类代理方面的表现。

Method: 本文通过整合哲学和科学的代理理论与AI辅助评估方法，使用大型语言模型（LLMs）来模拟和验证用户查询以及评估AI响应，开发了HumanAgencyBench（HAB）基准测试。

Result: 研究发现，当代基于大型语言模型的助手在人类代理方面支持较低至中等，并且在不同系统开发者和维度之间存在显著差异。例如，Anthropic LLMs整体上最支持人类代理，但在避免价值操纵方面是最不支持的。

Conclusion: 本文认为，当前基于大型语言模型的助手在人类代理方面支持较低至中等，并且在不同系统开发者和维度之间存在显著差异。作者呼吁转向更稳健的安全和对齐目标。

Abstract: As humans delegate more tasks and decisions to artificial intelligence (AI),
we risk losing control of our individual and collective futures. Relatively
simple algorithmic systems already steer human decision-making, such as social
media feed algorithms that lead people to unintentionally and absent-mindedly
scroll through engagement-optimized content. In this paper, we develop the idea
of human agency by integrating philosophical and scientific theories of agency
with AI-assisted evaluation methods: using large language models (LLMs) to
simulate and validate user queries and to evaluate AI responses. We develop
HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions
of human agency based on typical AI use cases. HAB measures the tendency of an
AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,
Correct Misinformation, Defer Important Decisions, Encourage Learning, and
Maintain Social Boundaries. We find low-to-moderate agency support in
contemporary LLM-based assistants and substantial variation across system
developers and dimensions. For example, while Anthropic LLMs most support human
agency overall, they are the least supportive LLMs in terms of Avoid Value
Manipulation. Agency support does not appear to consistently result from
increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and
we encourage a shift towards more robust safety and alignment targets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
*Bohan Yu,Yekun Chai*

Main category: cs.LG

TL;DR: EvolKV是一种自适应的、面向任务的KV缓存压缩框架，通过多目标优化和进化搜索实现内存效率和任务性能的联合优化，在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法通常依赖于启发式方法，如均匀分配缓存或静态驱逐策略，但忽略了层特定特征模式和任务性能之间的关键相互作用，这可能导致泛化能力下降。

Method: EvolKV将缓存分配重新表述为一个多目标优化问题，并利用进化搜索动态配置层预算，同时直接最大化下游性能。

Result: 在11个任务上的广泛实验表明，我们的方法在长上下文任务上优于所有基线方法，并且在GSM8K上比启发式基线高出高达7个百分点。EvolKV在代码完成任务中表现优于完整KV缓存设置，仅使用了原始预算的1.5%。

Conclusion: EvolKV展示了在KV缓存压缩中的巨大潜力，尤其是在代码完成任务中，仅使用原始预算的1.5%就能超越完整的KV缓存设置。

Abstract: Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

</details>


### [37] [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
*Minqi Jiang,João G. M. Araújo,Will Ellsworth,Sian Gooding,Edward Grefenstette*

Main category: cs.LG

TL;DR: 本文提出了一种名为生成数据精炼（GDR）的框架，利用预训练的生成模型将包含不良内容的数据集转换为更适合训练的精炼数据集。实验表明，GDR在数据集匿名化方面优于行业级解决方案，并能直接净化高度不安全的数据集。GDR的简单性和有效性使其成为扩大前沿模型训练数据总量的强大工具。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据的质量和数量决定了大型模型的能力，而训练数据的增长速度超过了网络上新数据的索引速度，因此预计未来十年会出现数据耗尽的情况。用户生成的内容虽然存在，但未被公开索引，将其纳入数据集中存在泄露隐私信息和其他不良内容的风险。

Method: 我们引入了一个框架，称为生成数据精炼（GDR），用于使用预训练的生成模型将包含不良内容的数据集转换为更适用于训练的精炼数据集。

Result: 实验表明，GDR在数据集匿名化方面可以优于行业级解决方案，并且可以直接净化高度不安全的数据集。此外，通过生成与真实数据集中的每个示例条件相关的合成数据，GDR的精炼输出自然地匹配了网络规模数据集的多样性，从而避免了通过模型提示生成多样化合成数据的通常具有挑战性的任务。

Conclusion: GDR的简单性和有效性使其成为扩大前沿模型训练数据总量的强大工具。

Abstract: For a fixed parameter size, the capabilities of large models are primarily
determined by the quality and quantity of its training data. Consequently,
training datasets now grow faster than the rate at which new data is indexed on
the web, leading to projected data exhaustion over the next decade. Much more
data exists as user-generated content that is not publicly indexed, but
incorporating such data comes with considerable risks, such as leaking private
information and other undesirable content. We introduce a framework, Generative
Data Refinement (GDR), for using pretrained generative models to transform a
dataset with undesirable content into a refined dataset that is more suitable
for training. Our experiments show that GDR can outperform industry-grade
solutions for dataset anonymization, as well as enable direct detoxification of
highly unsafe datasets. Moreover, we show that by generating synthetic data
that is conditioned on each example in the real dataset, GDR's refined outputs
naturally match the diversity of web scale datasets, and thereby avoid the
often challenging task of generating diverse synthetic data via model
prompting. The simplicity and effectiveness of GDR make it a powerful tool for
scaling up the total stock of training data for frontier models.

</details>


### [38] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 本文介绍了AgentGym-RL框架和ScalingInter-RL方法，用于训练LLM代理进行多轮交互决策，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个统一的、交互式的强化学习框架，可以在不依赖监督微调的情况下，在多样化的现实环境中从零开始训练这样的代理。

Method: 我们引入了AgentGym-RL框架，通过强化学习训练LLM代理进行多轮交互决策，并提出了ScalingInter-RL训练方法来平衡探索与利用，确保稳定优化。

Result: 我们在广泛的实验中验证了AgentGym-RL框架和ScalingInter-RL方法的稳定性和有效性，代理在27个任务中表现优异。

Conclusion: 我们的代理在27个任务中达到了或超过了商业模型的表现，并且我们提供了关键见解和完整的AgentGym-RL框架以供开源，以推动智能代理的发展。

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [39] [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
*Zhanming Shen,Zeyu Qin,Zenan Huang,Hao Chen,Jiaqi Hu,Yihong Zhuang,Guoshan Lu,Gang Chen,Junbo Zhao*

Main category: cs.LG

TL;DR: MoT 是一种轻量级框架，通过教师特定的监督微调和权重空间合并来统一多个教师的推理能力。在竞赛数学基准测试中，使用少量高质量 CoT 样本，MoT 显著提升了模型性能，并展现出良好的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的推理蒸馏方法受限于单一教师假设，而实践中存在多个候选教师和不断增长的 CoT 数据集。不同学生可能有不同的“最佳教师”，并且对于同一学生，最佳教师可能因数据集而异。

Method: 提出了一种轻量级框架 Merge-of-Thought Distillation (MoT)，它在教师特定的监督微调分支和学生变体的权重空间合并之间交替进行。

Result: 在竞赛数学基准测试中，使用仅约 200 个高质量 CoT 样本，应用 MoT 的 Qwen3-14B 学生超越了包括 DEEPSEEK-R1、QWEN3-30B-A3B、QWEN3-32B 和 OPENAI-O1 在内的强大模型，展示了显著的提升。此外，MoT 一致优于最佳单教师蒸馏和朴素多教师联合，提高了性能上限并减轻了过拟合，对分布偏移和同级教师表现出鲁棒性。

Conclusion: MoT 是一种简单且可扩展的方法，可以有效地将长链式思维能力从多样化的教师中蒸馏到紧凑的学生模型中。

Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [40] [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://arxiv.org/abs/2509.08777)
*Eric Slyman,Mehrab Tanjim,Kushal Kafle,Stefan Lee*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态感知方法MMB，通过贝叶斯提示集成和图像聚类来改善文本到图像生成系统的评估效果。MMB在多个基准测试中表现出色，提高了准确性并增强了校准度。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）被越来越多地用于评估文本到图像（TTI）生成系统，但这些“裁判”模型常常存在偏见、过度自信和在不同图像领域中表现不一致的问题。虽然提示集成在单模态文本设置中显示出缓解这些问题的潜力，但我们的实验表明，标准的集成方法在TTI任务中无法有效推广。

Method: 我们提出了一种新的多模态感知方法，称为多模态贝叶斯提示集成（MMB），该方法使用增强图像聚类的贝叶斯提示集成方法，使裁判能够根据每个样本的视觉特征动态分配提示权重。

Result: MMB在成对偏好判断中的准确性得到了提高，并大大增强了校准度，使更容易衡量裁判的真实不确定性。在两个TTI基准测试HPSv2和MJBench上的评估中，MMB在与人类注释的一致性和校准度方面优于现有的基线。

Conclusion: 我们的研究强调了针对裁判校准的多模态特定策略的重要性，并为可靠的大规模文本到图像评估指明了有希望的前进方向。

Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate
text-to-image (TTI) generation systems, providing automated judgments based on
visual and textual context. However, these "judge" models often suffer from
biases, overconfidence, and inconsistent performance across diverse image
domains. While prompt ensembling has shown promise for mitigating these issues
in unimodal, text-only settings, our experiments reveal that standard
ensembling methods fail to generalize effectively for TTI tasks. To address
these limitations, we propose a new multimodal-aware method called Multimodal
Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt
ensemble approach augmented by image clustering, allowing the judge to
dynamically assign prompt weights based on the visual characteristics of each
sample. We show that MMB improves accuracy in pairwise preference judgments and
greatly enhances calibration, making it easier to gauge the judge's true
uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB
outperforms existing baselines in alignment with human annotations and
calibration across varied image content. Our findings highlight the importance
of multimodal-specific strategies for judge calibration and suggest a promising
path forward for reliable large-scale TTI evaluation.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [41] [Scaling Truth: The Confidence Paradox in AI Fact-Checking](https://arxiv.org/abs/2509.08803)
*Ihsan A. Qazi,Zohaib Khan,Abdullah Ghani,Agha A. Raza,Zafar A. Qazi,Wassay Sajjad,Ayesha Ali,Asher Javaid,Muhammad Abdullah Sohail,Abdul H. Azeemi*

Main category: cs.SI

TL;DR: 研究评估了九个大型语言模型在事实核查任务中的表现，发现较小的模型虽然自信但准确性较低，而较大的模型更准确但信心较低，这可能加剧信息不平等。


<details>
  <summary>Details</summary>
Motivation: 由于虚假信息的上升，需要可扩展和可靠的事实核查解决方案。大型语言模型（LLMs）在自动化事实验证方面有潜力，但它们在全球背景下的效果仍不确定。

Method: 系统评估了九个已建立的LLM，使用了5000个之前由174个专业事实核查组织在47种语言中评估过的声明，并测试了模型在训练截止日期后的声明上的泛化能力以及四种提示策略。

Result: 发现了一个令人担忧的模式，类似于Dunning-Kruger效应：较小的、易于访问的模型表现出高信心但较低的准确性，而较大的模型表现出较高的准确性但较低的信心。性能差距在非英语语言和来自全球南方的声明中最明显。

Conclusion: 研究结果建立了多语言基准，为未来的研究提供了证据基础，并为确保公平获取可信的AI辅助事实核查的政策提供了依据。

Abstract: The rise of misinformation underscores the need for scalable and reliable
fact-checking solutions. Large language models (LLMs) hold promise in
automating fact verification, yet their effectiveness across global contexts
remains uncertain. We systematically evaluate nine established LLMs across
multiple categories (open/closed-source, multiple sizes, diverse architectures,
reasoning-based) using 5,000 claims previously assessed by 174 professional
fact-checking organizations across 47 languages. Our methodology tests model
generalizability on claims postdating training cutoffs and four prompting
strategies mirroring both citizen and professional fact-checker interactions,
with over 240,000 human annotations as ground truth. Findings reveal a
concerning pattern resembling the Dunning-Kruger effect: smaller, accessible
models show high confidence despite lower accuracy, while larger models
demonstrate higher accuracy but lower confidence. This risks systemic bias in
information verification, as resource-constrained organizations typically use
smaller models. Performance gaps are most pronounced for non-English languages
and claims originating from the Global South, threatening to widen existing
information inequalities. These results establish a multilingual benchmark for
future research and provide an evidence base for policy aimed at ensuring
equitable access to trustworthy, AI-assisted fact-checking.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [42] [XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols](https://arxiv.org/abs/2509.08182)
*Faruk Alpay,Taylan Alpay*

Main category: cs.PL

TL;DR: 本文提出了一种逻辑优先的XML提示方法，用于引导大型语言模型生成符合结构的输出。通过形式化XML树的完整格结构，证明了单调提示到提示运算符的最小固定点，并展示了如何通过语法约束解码保证XML结构的正确性。


<details>
  <summary>Details</summary>
Motivation: 为了在现实系统中引导大型语言模型（LLMs）生成可解析且符合模式的输出，本文提出了逻辑优先的XML提示方法，以统一语法约束解码、固定点语义和收敛的人机交互循环。

Method: 本文采用逻辑优先的方法处理XML提示，结合了语法约束解码、层次化提示的固定点语义以及收敛的人机交互循环。同时，形式化了XML树的完整格结构，并证明了单调提示到提示运算符的最小固定点。

Result: 本文的形式化方法证明了单调提示到提示运算符的最小固定点，这些固定点表征了稳态协议，并在任务感知收缩度量下证明了迭代引导的Banach风格收敛性。此外，还展示了如何通过语法约束解码保证XML结构的正确性并保持任务性能。

Conclusion: 本文提出了一种逻辑优先的XML提示方法，该方法统一了语法约束解码、层次化提示的固定点语义以及收敛的人机交互循环。通过形式化XML树的完整格结构，证明了单调提示到提示运算符的最小固定点，这些固定点表征了稳态协议，并在任务感知收缩度量下证明了迭代引导的Banach风格收敛性。

Abstract: Structured prompting with XML tags has emerged as an effective way to steer
large language models (LLMs) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained decoding guarantees
well-formedness while preserving task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned decoding, chain-of-verification, and programmatic
prompting.

</details>

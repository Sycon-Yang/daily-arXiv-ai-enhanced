<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 6]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
*Yingtai Xiao,Yuqing Zhu,Sirat Samyoun,Wanrong Zhang,Jiachen T. Wang,Jian Du*

Main category: cs.CL

TL;DR: 提出了一种新的token级归因方法TokenShapley，通过结合Shapley值和KNN检索技术，实现了更精确的归因效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在用户寻求响应中特定关键词（如数字、年份或名称）的归因时效果不佳，因此需要一种更细粒度的归因方法。

Method: 提出了一种新的token级归因方法TokenShapley，结合了基于Shapley值的数据归因和受KNN增强LLM最新进展启发的KNN检索技术。

Result: TokenShapley在四个基准测试中表现优于最先进的基线，在token级归因方面实现了11-23%的准确率提升。

Conclusion: TokenShapley在四个基准测试中表现出色，优于最先进的基线，在token级归因方面实现了11-23%的准确率提升。

Abstract: Large language models (LLMs) demonstrate strong capabilities in in-context
learning, but verifying the correctness of their generated responses remains a
challenge. Prior work has explored attribution at the sentence level, but these
methods fall short when users seek attribution for specific keywords within the
response, such as numbers, years, or names. To address this limitation, we
propose TokenShapley, a novel token-level attribution method that combines
Shapley value-based data attribution with KNN-based retrieval techniques
inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed
datastore for contextual retrieval and computing Shapley values to quantify
token importance, TokenShapley provides a fine-grained data attribution
approach. Extensive evaluations on four benchmarks show that TokenShapley
outperforms state-of-the-art baselines in token-level attribution, achieving an
11-23% improvement in accuracy.

</details>


### [2] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
*Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 研究探讨了使用用户行为预测作为衡量大型语言模型泛化能力的替代方法，并在电影和音乐推荐数据集上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 测量大型语言模型的泛化能力具有挑战性，因为数据污染。随着模型的增长和计算成本的降低，确保任务和测试用例在训练阶段未被看到将变得几乎不可能。

Method: 我们引入了一个新的框架来测试用户行为预测。

Result: 结果与我们框架的预测一致，显示GPT-4o的表现优于GPT-4o-mini和Llama。

Conclusion: 所有模型都有很大的改进空间，尤其是Llama。

Abstract: Measuring the generalization ability of Large Language Models (LLMs) is
challenging due to data contamination. As models grow and computation becomes
cheaper, ensuring tasks and test cases are unseen during training phases will
become nearly impossible. We argue that knowledge-retrieval and reasoning tasks
are not ideal for measuring generalization, as LLMs are not trained for
specific tasks. Instead, we propose user behavior prediction, also a key aspect
of personalization, as a theoretically sound, scalable, and robust alternative.
We introduce a novel framework for this approach and test it on movie and music
recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.
Results align with our framework's predictions, showing GPT-4o outperforms
GPT-4o-mini and Llama, though all models have much room for improvement,
especially Llama.

</details>


### [3] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
*Mohammad Zia Ur Rehman,Aditya Shah,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种名为ASCEND的自适应监督对比学习框架，用于检测隐性性别歧视。该方法通过基于阈值的对比学习机制优化嵌入空间，结合交叉熵损失进行分类，并利用文本特征增强模型性能。实验结果表明，ASCEND在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法往往忽略隐性性别歧视，而社交媒体的全球影响力加剧了仇恨内容的传播，因此需要更有效的检测方法。

Method: 引入了自适应监督对比学习框架ASCEND，通过基于阈值的对比学习机制，选择性地将相似度超过可学习阈值的样本对作为正例，从而优化嵌入空间，并结合交叉熵损失进行最终分类。

Result: 在EXIST2021和MLSC数据集上的评估表明，ASCEND在多个任务中显著优于现有方法，提升了隐性性别歧视语言的检测效果。

Conclusion: ASCEND显著优于现有方法，平均宏F1提升9.86%、29.63%和32.51%，表明其在捕捉隐性性别歧视语言的细微线索方面的有效性。

Abstract: The global reach of social media has amplified the spread of hateful content,
including implicit sexism, which is often overlooked by conventional detection
methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning
framework for implicit sexism detectioN (ASCEND). A key innovation of our
method is the incorporation of threshold-based contrastive learning: by
computing cosine similarities between embeddings, we selectively treat only
those sample pairs as positive if their similarity exceeds a learnable
threshold. This mechanism refines the embedding space by robustly pulling
together representations of semantically similar texts while pushing apart
dissimilar ones, thus reducing false positives and negatives. The final
classification is achieved by jointly optimizing a contrastive loss with a
cross-entropy loss. Textual features are enhanced through a word-level
attention module. Additionally, we employ sentiment, emotion, and toxicity
features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that
ASCEND significantly outperforms existing methods, with average Macro F1
improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting
its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [4] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
*Miloud Mihoubi,Meriem Zerkouk,Belkacem Chikhaoui*

Main category: cs.CL

TL;DR: 本文提出了一种基于AI的框架，通过情感分析、提示工程和跨模态注意力融合来提高辍学预测的准确性，并提供了可操作的干预措施，以减少远程学习中的辍学风险。


<details>
  <summary>Details</summary>
Motivation: 学生在远程学习中的辍学仍然是一个关键挑战，具有深远的社会和经济后果。虽然传统的机器学习模型利用结构化的社会人口统计和行为数据，但它们往往无法捕捉到非结构化学生互动中的细微情感和上下文因素。

Method: 本文引入了三种协同创新：检索增强生成（RAG）用于领域特定的情感分析，提示工程用于解码学术压力因素，以及跨模态注意力融合以动态对齐文本、行为和社会人口统计数据。

Result: 该框架在4423名学生的纵向数据集上实现了89%的准确率和0.88的F1分数，比传统模型提高了7%，并将假阴性减少了21%。此外，该系统还能生成可解释的干预措施，例如为孤立学习者提供导师计划。

Conclusion: 本文提出了一种变革性的AI框架，通过三个协同创新来重新定义辍学预测，该框架在4423名学生的纵向数据集上实现了89%的准确率和0.88的F1分数，优于传统模型，并提供了可操作的干预措施，有助于减轻全球教育系统中的辍学风险。

Abstract: Student dropout in distance learning remains a critical challenge, with
profound societal and economic consequences. While classical machine learning
models leverage structured socio-demographic and behavioral data, they often
fail to capture the nuanced emotional and contextual factors embedded in
unstructured student interactions. This paper introduces a transformative AI
framework that redefines dropout prediction through three synergistic
innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment
analysis, prompt engineering to decode academic stressors, and cross-modal
attention fusion to dynamically align textual, behavioral, and
socio-demographic insights. By grounding sentiment analysis in a curated
knowledge base of pedagogical content, our RAG-enhanced BERT model interprets
student comments with unprecedented contextual relevance, while optimized
prompts isolate indicators of academic distress (e.g., "isolation," "workload
anxiety"). A cross-modal attention layer then fuses these insights with
temporal engagement patterns, creating holistic risk profiles. Evaluated on a
longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and
an F1-score of 0.88, outperforming conventional models by 7% and reducing false
negatives by 21%. Beyond prediction, the system generates interpretable
interventions by retrieving contextually aligned strategies (e.g., mentorship
programs for isolated learners). This work bridges the gap between predictive
analytics and actionable pedagogy, offering a scalable solution to mitigate
dropout risks in global education systems

</details>


### [5] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
*Cheng Yuan,Xinkai Rui,Yongqi Fan,Yawei Fan,Boyang Zhong,Jiacheng Wang,Weiyan Zhang,Tong Ruan*

Main category: cs.CL

TL;DR: LCDS is a system that generates reliable discharge summaries by using logical rules and source mapping, and supports source attribution for error correction and LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from hallucination issues when generating discharge summaries, and EMRs consist of long-form data that makes it difficult for LLMs to attribute generated content to sources.

Method: LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries, and incorporates a comprehensive set of logical rules to generate reliable discharge summaries tailored to different clinical fields. It also supports source attribution for generated content.

Result: LCDS generates more reliable discharge summaries with source attribution, allowing experts to review and correct errors. The resulting summaries can be used for incremental fine-tuning of LLMs.

Conclusion: LCDS can generate more reliable discharge summaries and support source attribution, which helps experts review and correct errors, and the resulting summaries can be used for incremental fine-tuning of LLMs.

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in
automated discharge summary generation, they still suffer from hallucination
issues, such as generating inaccurate content or fabricating information
without valid sources. In addition, electronic medical records (EMRs) typically
consist of long-form data, making it challenging for LLMs to attribute the
generated content to the sources. To address these challenges, we propose LCDS,
a Logic-Controlled Discharge Summary generation system. LCDS constructs a
source mapping table by calculating textual similarity between EMRs and
discharge summaries to constrain the scope of summarized content. Moreover,
LCDS incorporates a comprehensive set of logical rules, enabling it to generate
more reliable silver discharge summaries tailored to different clinical fields.
Furthermore, LCDS supports source attribution for generated content, allowing
experts to efficiently review, provide feedback, and rectify errors. The
resulting golden discharge summaries are subsequently recorded for incremental
fine-tuning of LLMs. Our project and demo video are in the GitHub repository
https://github.com/ycycyc02/LCDS.

</details>


### [6] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
*Ming Gong,Xucheng Huang,Chenghan Yang,Xianhan Peng,Haoxin Wang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 本文介绍了MindFlow，这是一个针对电子商务的首个开源多模态LLM代理，通过整合记忆、决策和动作模块，并采用模块化的'MLLM-as-Tool'策略，实现了在处理复杂查询、提高用户满意度和降低运营成本方面的显著改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在电子商务客服中有了新的应用，但它们在复杂、多模态场景中的能力仍然受到限制。

Method: 基于CoALA框架，整合了记忆、决策和动作模块，并采用模块化的'MLLM-as-Tool'策略进行有效的视觉文本推理。

Result: 通过在线A/B测试和基于模拟的消融实验评估，MindFlow在处理复杂查询、提高用户满意度和降低运营成本方面表现出显著优势。

Conclusion: MindFlow在处理复杂查询、提高用户满意度和降低运营成本方面表现出显著优势，在实际部署中观察到了93.53%的相对改进。

Abstract: Recent advances in large language models (LLMs) have enabled new applications
in e-commerce customer service. However, their capabilities remain constrained
in complex, multimodal scenarios. We present MindFlow, the first open-source
multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it
integrates memory, decision-making, and action modules, and adopts a modular
"MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via
online A/B testing and simulation-based ablation, MindFlow demonstrates
substantial gains in handling complex queries, improving user satisfaction, and
reducing operational costs, with a 93.53% relative improvement observed in
real-world deployments.

</details>


### [7] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种名为LAG的方法，用于高效选择和组合知识库中的专家，无需额外训练或数据访问，适用于各种需要知识的任务，并且与RAG等其他解决方案兼容。


<details>
  <summary>Details</summary>
Motivation: 随着针对特定任务和领域的微调语言模型专家的普及，需要高效的选取和组合方法。

Method: 提出了一种名为LAG的方法，利用大型知识库和任务特定的LoRA适配器，无需额外训练或数据访问，即可在每个标记和层的基础上过滤、检索和应用专家。

Result: 在各种需要知识的任务上评估LAG，结果优于现有的无数据方法。此外，还探索了有额外数据可用的情况，证明了LAG与RAG等其他解决方案的兼容性。

Conclusion: LAG可以高效地选择和组合知识库中的专家，适用于各种需要知识的任务，并且与RAG等其他解决方案兼容。

Abstract: The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

</details>


### [8] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
*Riccardo Alberghi,Elizaveta Demyanenko,Luca Biggio,Luca Saglietti*

Main category: cs.CL

TL;DR: 研究显示，在相同训练令牌预算下，使用低效追踪训练的模型在未见过的图上泛化能力更好，这与模型对下一个令牌预测的信心有关。


<details>
  <summary>Details</summary>
Motivation: 研究两个关键因素：(i) 分配更多测试时计算有助于解决更难的问题，但通常会引入推理轨迹中的冗余；(ii) 计算在系统和渐进推理中最为有效，形成类似于人类问题解决的结构化思维链（CoTs）。

Method: 在基于分层图的最短路径任务中引入一个受控设置，训练解码器仅变压器模型，使用自定义分词器，比较在最优自底向上动态规划追踪和包含回溯的较长有效追踪上训练的模型。

Result: 使用低效追踪训练的模型在未见过的图上表现更好，这与模型对下一个令牌预测的信心有关。

Conclusion: 模型在相同训练令牌预算下，使用低效追踪训练的模型在未见过的图上泛化能力更好。这种优势不是由于长度本身，而是模型对下一个令牌预测的信心与泛化能力相关。

Abstract: Recent advances in natural language processing highlight two key factors for
improving reasoning in large language models (LLMs): (i) allocating more
test-time compute tends to help on harder problems but often introduces
redundancy in the reasoning trace, and (ii) compute is most effective when
reasoning is systematic and incremental, forming structured chains of thought
(CoTs) akin to human problem-solving. To study these factors in isolation, we
introduce a controlled setting based on shortest-path tasks in layered graphs.
We train decoder-only transformers on question-trace-answer triples using a
custom tokenizer, comparing models trained on optimal bottom-up dynamic
programming traces with those trained on longer, valid traces involving
backtracking. Surprisingly, with the same training-token budget, models trained
on inefficient traces generalize better to unseen graphs. This benefit is not
due to length alone-injecting arbitrary redundancy into reasoning traces fails
to help and can even hurt performance. Instead, we find that generalization
correlates with the model's confidence in next-token prediction, suggesting
that long, coherent, and locally incremental traces make the training signal
easier to optimize.

</details>


### [9] [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
*Guanzhong Pan,Mei Tan,Hyunji Nam,Lucía Langlois,James Malamut,Liliana Deonizio,Dorottya Demszky*

Main category: cs.CL

TL;DR: EduCoder 是一个开源工具，旨在解决教育对话编码的复杂性，支持协作定义代码本、分类和开放式编码，并通过多注释者对比提高数据可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有通用文本注释工具难以应对教育对话中教师-学生和同伴互动的复杂性，包括定义代码本、支持开放式和分类编码以及将话语与外部特征关联等问题。

Method: EduCoder 通过提供一个平台，让研究人员和领域专家基于观察到的数据协作定义复杂的代码本，并结合分类和开放式注释类型以及上下文材料，同时提供多注释者响应的对比功能。

Result: EduCoder 解决了教育对话编码的挑战，提供了协作定义代码本的平台，支持多种注释类型，并通过多注释者对比提高数据可靠性。

Conclusion: EduCoder 是一个开源工具，旨在解决教育对话转录本编码的复杂性，通过提供协作定义复杂代码本的平台，支持分类和开放式编码，并提供多注释者响应的对比，以提高数据可靠性。

Abstract: We introduce EduCoder, a domain-specialized tool designed to support
utterance-level annotation of educational dialogue. While general-purpose text
annotation tools for NLP and qualitative research abound, few address the
complexities of coding education dialogue transcripts -- with diverse
teacher-student and peer interactions. Common challenges include defining
codebooks for complex pedagogical features, supporting both open-ended and
categorical coding, and contextualizing utterances with external features, such
as the lesson's purpose and the pedagogical value of the instruction. EduCoder
is designed to address these challenges by providing a platform for researchers
and domain experts to collaboratively define complex codebooks based on
observed data. It incorporates both categorical and open-ended annotation types
along with contextual materials. Additionally, it offers a side-by-side
comparison of multiple annotators' responses, allowing comparison and
calibration of annotations with others to improve data reliability. The system
is open-source, with a demo video available.

</details>


### [10] [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)
*Ruidi Chang,Chunyuan Deng,Hanjie Chen*

Main category: cs.CL

TL;DR: 研究提出了InfoRidge框架，揭示了变压器中预测信息的变化趋势，并通过残差缩放系数分析了中间层在泛化中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 为了填补对变压器内部机制理解不足的空白，研究了中间层和最终层在训练过程中泛化能力的演变和传播。

Method: 提出了一种基于信息论的框架InfoRidge，用于表征预测信息如何随深度变化，并引入了残差缩放系数作为评估单个Transformer层相对重要性的功能探针。

Result: 实验结果显示预测信息在上层中间层达到峰值，形成一个泛化脊，然后在最终层下降，反映了从泛化到记忆的转变。残差缩放系数表明，在分布变化下，模型会降低最终层的权重并越来越多地依赖脊层。

Conclusion: 这些发现为变压器的内部机制提供了新的见解，并强调了中间层在支持泛化中的关键作用。

Abstract: Transformer-based language models have achieved state-of-the-art performance
in natural language generation (NLG) tasks, yet their internal mechanisms for
synthesizing task-relevant information remain insufficiently understood. While
prior studies suggest that intermediate layers often yield more generalizable
representations than final layers, how this generalization ability emerges and
propagates across layers during training remains unclear. To address this gap,
we propose InfoRidge, an information-theoretic framework, to characterize how
predictive information-the mutual information between hidden representations
and target outputs-varies across depth. Estimating this quantity enables us to
trace the flow of task-relevant information throughout the model during
training. Our experiments across various models and datasets reveal a
consistent non-monotonic trend: predictive information peaks in upper-middle
layers-forming a generalization ridge-before declining in final layers,
reflecting a transition between generalization and memorization. To further
investigate this phenomenon, we introduce residual scaling
coefficients-trainable scalar parameters applied to each residual block-which
serve as functional probes for assessing the relative importance of individual
transformer layers. These coefficients reveal that, under distribution shift,
models downweight final layers and increasingly rely on ridge layers,
highlighting their role in generalization. Together, these findings offer new
insights into the internal mechanisms of transformers and underscore the
critical role of intermediate layers in supporting generalization.

</details>


### [11] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
*Guillem Ramírez,Alexandra Birch,Ivan Titov*

Main category: cs.CL

TL;DR: 本文提出了一种通过隐私配置文件保护用户数据的方法，并引入了一个多语言数据集PEEP来支持研究，但实验表明轻量级语言模型在遵循这些指令方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 用户希望在使用大型语言模型时保持对数据的控制，而目前主要依赖商业API，这可能导致数据暴露给服务提供商。因此，需要一种方法让用户能够定义隐私偏好并保护数据。

Method: 本文构建了一个框架，其中本地模型使用隐私配置文件来重写查询，仅隐藏用户认为敏感的细节，然后将查询发送到外部模型。此外，还引入了PEEP数据集，用于支持研究。

Result: 实验表明，轻量级语言模型可以部分遵循隐私配置文件的指令，但仍然面临一致性的挑战。

Conclusion: 本文提出了一种通过隐私配置文件来保护用户数据的方法，但实验表明轻量级语言模型在遵循这些指令方面仍存在挑战，需要更先进的模型来更好地理解和遵守用户的隐私偏好。

Abstract: Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

</details>


### [12] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
*Jaedong Hwang,Kumar Tanmay,Seok-Jin Lee,Ayush Agrawal,Hamid Palangi,Kumar Ayush,Ila Fiete,Paul Pu Liang*

Main category: cs.CL

TL;DR: 本文介绍了GeoFact-X，一个基于地理的多语言事实推理基准，并提出了BRIDGE训练方法，以提高多语言推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言基准仅关注最终答案，忽视了模型是否真的在目标语言中进行推理。此外，对于低资源语言，如斯瓦希里语或泰语，LLM经常误解提示或默认用英语进行推理，这影响了事实准确性、可解释性和信任度。

Method: 我们提出了BRIDGE，一种新的训练方法，通过语言一致性奖励引导监督微调和测试时的强化学习，以使推理与输入语言对齐。

Result: BRIDGE显著提高了多语言推理的准确性，证明了关注推理的多语言强化学习对于稳健的跨语言泛化至关重要。

Conclusion: 我们的结果表明，BRIDGE显著提高了多语言推理的准确性，证明了关注推理的多语言强化学习对于稳健的跨语言泛化至关重要。

Abstract: Large Language Models (LLMs) have achieved strong performance in domains like
mathematics, factual QA, and code generation, yet their multilingual reasoning
capabilities in these tasks remain underdeveloped. Especially for low-resource
languages such as Swahili or Thai, LLMs can often misinterpret prompts or
default to reasoning in English. This implicit bias toward high-resource
languages undermines factual accuracy, interpretability, and trust. Current
multilingual benchmarks focus only on final answers, overlooking whether models
actually reason in the target language. To address this gap, we introduce
GeoFact-X, a geography-based multilingual factual reasoning benchmark with
annotated reasoning traces in five languages: English, Hindi, Japanese,
Swahili, and Thai. We further propose BRIDGE, a novel training method that
guides supervised fine-tuning and test-time reinforcement learning with a
language-consistency reward to align reasoning with the input language.
Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to
assess answer correctness and the quality and language consistency of reasoning
traces, enabling nuanced and scalable analysis beyond surface-level metrics.
Our results show that BRIDGE significantly enhances multilingual reasoning
fidelity, demonstrating that reasoning-aware multilingual reinforcement
learning is crucial for robust cross-lingual generalization.
https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [13] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
*Yufei Tao,Adam Hiatt,Rahul Seetharaman,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文介绍了CoPE评估框架，用于分析大型语言模型如何处理上下文和参数知识。研究发现模型对后续信息有位置偏差，并提出了基于提示的方法来改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型能够利用上下文和参数知识，但它们如何优先和整合这些来源仍不明确。我们需要一个系统的方法来评估和理解这一点。

Method: 我们引入了CoPE评估框架，用于系统地测量不同模型和语言中的上下文知识（CK）和参数知识（PK）。我们使用MultiWikiAtomic数据集分析了大型语言模型如何整合上下文、优先处理信息以及在开放性问题回答中融入PK。

Result: 我们的分析发现了一个称为“lost-in-the-later”的现象，即大型语言模型倾向于忽略或低估后续信息。此外，推理模型和使用链式思维（CoT）提示的非推理模型使用上下文更少，并未能缓解这一现象。CoT提示导致召回率降低和响应变短，从而影响了上下文的基础。

Conclusion: 我们的研究揭示了大型语言模型在处理上下文信息时存在位置偏差，即更倾向于忽略或低估后续信息。我们设计了基于提示的方法来有效利用输入上下文，并通过案例研究证明了这种方法可以提高事实依据并减少幻觉。

Abstract: Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

</details>


### [14] [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)
*Ashwin Rao,Sze Yuh Nina Wang,Kristina Lerman*

Main category: cs.CL

TL;DR: 该研究分析了近1000万条在X上的堕胎相关帖子，发现性别显著调节堕胎态度和情感表达，特别是在保守地区，并且独立于意识形态。达布斯草案意见的泄露进一步加剧了在线参与度，不成比例地动员了处于威胁中的地区内的支持堕胎女性。这些发现表明，堕胎言论不仅在意识形态上两极分化，而且深深受到性别和地点的结构影响，突显了身份在制度中断时期塑造政治表达中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 了解性别和当地社会政治背景如何相互作用以塑造公众话语。

Method: 我们分析了近1000万条在X（前身为Twitter）上的堕胎相关帖子，这些帖子来自具有推断出的性别、意识形态和位置的用户。

Result: 性别显著调节堕胎态度和情感表达，特别是在保守地区，并且独立于意识形态。这导致了堕胎态度的性别差距，在保守地区更加明显。达布斯草案意见的泄露进一步加剧了在线参与度，不成比例地动员了处于威胁中的地区内的支持堕胎女性。

Conclusion: 这些发现表明，堕胎言论不仅在意识形态上两极分化，而且深深受到性别和地点的结构影响，突显了身份在制度中断时期塑造政治表达中的核心作用。

Abstract: The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health
Organization marked a turning point in the national debate over reproductive
rights. While the ideological divide over abortion is well documented, less is
known about how gender and local sociopolitical contexts interact to shape
public discourse. Drawing on nearly 10 million abortion-related posts on X
(formerly Twitter) from users with inferred gender, ideology and location, we
show that gender significantly moderates abortion attitudes and emotional
expression, particularly in conservative regions, and independently of
ideology. This creates a gender gap in abortion attitudes that grows more
pronounced in conservative regions. The leak of the Dobbs draft opinion further
intensified online engagement, disproportionately mobilizing pro-abortion women
in areas where access was under threat. These findings reveal that abortion
discourse is not only ideologically polarized but also deeply structured by
gender and place, highlighting the central role of identity in shaping
political expression during moments of institutional disruption.

</details>


### [15] [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
*Sana Kang,Myeongseok Gwon,Su Young Kwon,Jaewook Lee,Andrew Lan,Bhiksha Raj,Rita Singh*

Main category: cs.CL

TL;DR: PhoniTale is a new system that generates mnemonics for L2 vocabulary by leveraging phonological similarities between L1 and L2, and it shows promising results compared to human-generated mnemonics.


<details>
  <summary>Details</summary>
Motivation: Vocabulary acquisition is challenging for L2 learners, especially when learning typologically distant languages. Previous research has focused on native English speakers, but this paper addresses the reverse scenario.

Method: PhoniTale is a cross-lingual mnemonic generation system that retrieves L1 keyword sequences based on phonological similarity and uses LLMs to generate mnemonics.

Result: PhoniTale was evaluated using automated metrics and human evaluations, and it performed comparably to human-authored mnemonics. A short-term recall test also showed its effectiveness.

Conclusion: PhoniTale performs comparably to human-authored mnemonics, but there are key areas for future improvement in mnemonic quality and methodology.

Abstract: Vocabulary acquisition poses a significant challenge for second-language (L2)
learners, especially when learning typologically distant languages such as
English and Korean, where phonological and structural mismatches complicate
vocabulary learning. Recently, large language models (LLMs) have been used to
generate keyword mnemonics by leveraging similar keywords from a learner's
first language (L1) to aid in acquiring L2 vocabulary. However, most of this
research has focused on native English speakers learning other languages,
rather than the reverse. In this paper, we present PhoniTale, a novel
cross-lingual mnemonic generation system that retrieves L1 keyword sequence
based on phonological similarity and uses LLMs to generate mnemonics. We
evaluate PhoniTale using both automated metrics and human evaluations,
comparing its output to mnemonics created by humans and by previous automated
approaches. To assess practical effectiveness, we also conduct a short-term
recall test measuring mnemonic helpfulness. Our findings show that PhoniTale
performs comparably to human-authored mnemonics. We also highlight key areas
for future improvement in mnemonic quality and methodology.

</details>


### [16] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
*Martin Schuele*

Main category: cs.CL

TL;DR: The paper explores the semantic capabilities of Large Language Models (LLMs) by analyzing their inner workings and drawing on classical semantic theories.


<details>
  <summary>Details</summary>
Motivation: To understand the extent to which LLMs truly understand language, focusing on semantics at the word and sentence level.

Method: The paper examines the inner workings of LLMs and their generated representation of language, drawing on classical semantic theories by Frege and Russell.

Result: A more nuanced picture of the potential semantic capabilities of LLMs is obtained.

Conclusion: LLMs have some semantic capabilities, but their understanding is limited compared to human understanding.

Abstract: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to
replicate human language abilities through technology, ranging from text
generation to engaging in conversations. However, it remains controversial to
what extent these systems truly understand language. We examine this issue by
narrowing the question down to the semantics of LLMs at the word and sentence
level. By examining the inner workings of LLMs and their generated
representation of language and by drawing on classical semantic theories by
Frege and Russell, we get a more nuanced picture of the potential semantic
capabilities of LLMs.

</details>


### [17] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
*Ashima Suvarna,Christina Chance,Hamid Palangi,Sophie Hao,Thomas Hartvigsen,Saadia Gabriel*

Main category: cs.CL

TL;DR: 本文介绍了MODELCITIZENS数据集，用于研究社交媒体上的毒性语言检测，并提出了基于LLaMA和Gemma的模型，以提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的毒性检测模型通常在注释中将多样化的注释者观点合并为一个单一的地面真实值，这会抹去重要的上下文特定的毒性概念，如被重新使用的语言。为了应对这一问题，我们需要一个更全面的数据集来捕捉社区规范和生活经验对毒性感知的影响。

Method: 我们引入了MODELCITIZENS数据集，其中包括6.8K条社交媒体帖子和40K条跨不同身份群体的毒性注释，并通过LLM生成的对话场景增强了这些帖子。此外，我们发布了基于LLaMA和Gemma的模型LLAMACITIZEN-8B和GEMMACITIZEN-12B，这些模型在MODELCITIZENS上进行了微调。

Result: 最先进的毒性检测工具（例如OpenAI Moderation API，GPT-o4-mini）在MODELCITIZENS上的表现不佳，特别是在增强上下文的帖子上表现更差。然而，我们发布的LLAMACITIZEN-8B和GEMMACITIZEN-12B模型在分布内评估中比GPT-o4-mini高出5.5%。

Conclusion: 我们的研究强调了社区知情的注释和建模在包容性内容审核中的重要性。

Abstract: Automatic toxic language detection is critical for creating safe, inclusive
online spaces. However, it is a highly subjective task, with perceptions of
toxic language shaped by community norms and lived experience. Existing
toxicity detection models are typically trained on annotations that collapse
diverse annotator perspectives into a single ground truth, erasing important
context-specific notions of toxicity such as reclaimed language. To address
this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K
toxicity annotations across diverse identity groups. To capture the role of
conversational context on toxicity, typical of social media posts, we augment
MODELCITIZENS posts with LLM-generated conversational scenarios.
State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,
GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on
context-augmented posts. Finally, we release LLAMACITIZEN-8B and
GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,
which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our
findings highlight the importance of community-informed annotation and modeling
for inclusive content moderation.

</details>


### [18] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
*Jean-Philippe Corbeil,Asma Ben Abacha,George Michalopoulos,Phillip Swazinna,Miguel Del-Agua,Jerome Tremblay,Akila Jeeson Daniel,Cari Bader,Kevin Cho,Pooja Krishnan,Nathan Bodenstab,Thomas Lin,Wenxuan Teng,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文研究了使用大型语言模型解决临床自然语言处理任务的方法，提出了生成真实护士口述记录的代理流程，并发布了两个开源数据集以促进进一步研究。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺性和敏感性，结构化表格报告和医疗订单提取任务仍然未被充分探索，但这些任务的实际解决方案可以显著减轻医疗保健提供者的文档负担。

Method: 本文使用私有和开源临床数据集评估了开放权重和封闭权重大型语言模型在结构化表格报告和医疗订单提取任务中的性能，并提出了一个代理流程来生成真实的护士口述记录。

Result: 本文评估了不同大型语言模型在这些任务中的表现，并分析了它们的优缺点，同时发布了两个开源数据集以支持进一步研究。

Conclusion: 本文探讨了使用大型语言模型解决临床自然语言处理任务的潜力，并提出了生成真实且不敏感的护士口述记录的代理流程，同时发布了两个开源数据集以促进进一步研究。

Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong
performance on clinical natural language processing (NLP) tasks across multiple
medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular
reporting from nurse dictations and medical order extraction from
doctor-patient consultations - remain underexplored due to data scarcity and
sensitivity, despite active industry efforts. Practical solutions to these
real-world clinical tasks can significantly reduce the documentation burden on
healthcare providers, allowing greater focus on patient care. In this paper, we
investigate these two challenging tasks using private and open-source clinical
datasets, evaluating the performance of both open- and closed-weight LLMs, and
analyzing their respective strengths and limitations. Furthermore, we propose
an agentic pipeline for generating realistic, non-sensitive nurse dictations,
enabling structured extraction of clinical observations. To support further
research in both areas, we release SYNUR and SIMORD, the first open-source
datasets for nurse observation extraction and medical order extraction.

</details>


### [19] [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)
*Alex ZH Dou,Zhongwei Wan,Dongfei Cui,Xin Wang,Jing Xiong,Haokun Lin,Chaofan Tao,Shen Yan,Mi Zhang*

Main category: cs.CL

TL;DR: 本文提出了R2-LLMs，这是一种分层检索增强推理框架，旨在在不依赖蒸馏的情况下提升大型语言模型的测试时间扩展。通过双级基于检索的上下文学习，R2-LLMs在复杂推理任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: Test-time scaling在语言建模中已成为一种有前途的范式，利用额外的计算资源在推理时提高模型性能。然而，现有的方法通常需要从更先进的模型中蒸馏链式思维（CoT）训练数据，这限制了其适用性。因此，本文提出了一种无需蒸馏即可提升测试时间扩展的方法。

Method: R2-LLMs是一种分层检索增强推理框架，结合了双级基于检索的上下文学习：(1) 在粗粒度级别，从复杂推理问题中提取抽象模板并检索相似的问题-答案对以促进高层次的上下文学习；(2) 在细粒度级别，通过蒙特卡洛树搜索（MCTS）高效地从参考数学问题数据集中检索类似中间解决方案步骤，并利用过程奖励模型（PRM）进行评分以细化逐步推理。

Result: 在MATH500、GSM8K和OlympiadBench-TO数据集上的实证评估显示，使用LLaMA-3.1-8B相比基线实现了显著的相对改进，最高提升了16%。

Conclusion: R2-LLMs在复杂推理任务中展示了有效性，通过使用LLaMA-3.1-8B相比基线提升了高达16%的相对改进。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.

</details>


### [20] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
*Sihyun Park*

Main category: cs.CL

TL;DR: Re5 是一种自评估和修订框架，旨在提高指令遵循性能同时保持生成内容的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据点和修订迭代增加时成本显著上升，而使用高性能评估工具的方法可能导致输出质量下降。

Method: Re5 提取用户指令中的任务和约束组件，进行结构评估以防止错误积累，并应用细粒度的特定约束内容评估，随后进行选择性修订。

Result: 实验结果表明，Re5 在少量数据的情况下实现了与 GPT-4o-mini 生成数据训练的模型相当的指令遵循性能，并且在非修订初始响应上保持了 64.24% 的胜率。

Conclusion: Re5 是一种高效的解决方案，可以在最小的外部监督下增强指令遵循能力。

Abstract: Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

</details>


### [21] [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)
*Mingzhe Li,Jing Xiang,Qishen Zhang,Kaiyang Wan,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文提出了一种翻转的知识蒸馏范式，使大型语言模型能够从较小的语言模型中学习，从而提高性能。


<details>
  <summary>Details</summary>
Motivation: 在文本匹配任务中，微调的较小模型通常能产生更有效的领域特定表示，因为它们专注于优化输入对的相似性。为了利用小型模型的专业优势和大型语言模型的丰富语义理解，我们提出了这种翻转的知识蒸馏范式。

Method: 我们引入了一种翻转的知识蒸馏范式，其中大型语言模型（LLM）从较小的语言模型（SLM）学习。我们通过使用LoRA将LLM重新解释为编码器-解码器方式来解决架构差距。编码器生成压缩表示，而解码器将其映射到输出空间。在训练过程中，编码器生成表示及其相似性，并使用我们提出的Margin-aware Contrastive Learning (MCL)方法与教师产生的相似性分数对齐。

Result: 实验在金融和医疗保健基准以及现实应用中确认了该范式的有效性，模型已在在线环境中完全部署。

Conclusion: 我们的范式只需要一个表现合理的较小模型，就能让大型语言模型取得更好的性能。实验在金融和医疗保健基准以及现实应用中确认了其有效性，并且该模型已在在线环境中完全部署。

Abstract: Knowledge distillation typically involves transferring knowledge from a Large
Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such
as text matching, fine-tuned smaller models often yield more effective
domain-specific representations, as they focus on optimizing the similarity of
input pairs. To leverage both the specialized strengths of small models and the
rich semantic understanding of LLMs, we introduce a flipped knowledge
distillation paradigm, where LLM learns from SLM. Specifically, we address the
architectural gap between decoder-only LLMs and smaller encoder-based models by
reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder
generates compressed representations, while the decoder maps them to the output
space. During training, the encoder produces representations and their
similarities, which are then aligned with the similarity scores produced by the
teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.
The MCL ensures accurate similarity for both positive and negative pairs, and
adaptively handles the internal differences within positive and negative
samples. Our paradigm requires only a reasonably good-performing SLM, allowing
the LLM to achieve improved performance. Experiments on financial and
healthcare benchmarks, as well as real-world applications, confirm its
effectiveness, and the model has been fully deployed in an online environment.

</details>


### [22] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
*Yiqiao Jin,Kartik Sharma,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.CL

TL;DR: SARA is a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets by combining natural-language text snippets with semantic compression vectors.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented Generation (RAG) extends large language models with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches often discard fine-grained details essential for factual accuracy.

Method: SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context efficiency and answer correctness. It represents contexts at two complementary levels: fine-grained natural-language spans and compact, interpretable vectors. An iterative evidence-selection module employs the compression vectors for dynamic reranking of contexts.

Result: SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53) across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma).

Conclusion: SARA consistently improves answer relevance, answer correctness, and semantic similarity across multiple datasets and LLMs, demonstrating the importance of integrating textual and compressed representations for robust, context-efficient RAG.

Abstract: Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

</details>


### [23] [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)
*Haoxin Wang,Xianhan Peng,Xucheng Huang,Yizhe Huang,Ming Gong,Chenghan Yang,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: 本文介绍了ECom-Bench，这是第一个用于评估电子商务客户支持领域中具有多模态能力的LLM代理的基准框架。


<details>
  <summary>Details</summary>
Motivation: 为了评估具有多模态能力的LLM代理在电子商务客户支持领域的表现，需要一个能够反映现实世界复杂性的基准框架。

Method: ECom-Bench通过基于真实电子商务客户互动收集的个人资料信息进行动态用户模拟，并利用来自真实电子商务对话的真实任务数据集。

Result: 即使先进的模型如GPT-4o在我们的基准测试中也只能达到10-20%的pass^3指标，这表明复杂的电子商务场景带来了显著的挑战。

Conclusion: ECom-Bench是一个具有挑战性的基准框架，能够促进电子商务领域中LLM代理的研究和开发。

Abstract: In this paper, we introduce ECom-Bench, the first benchmark framework for
evaluating LLM agent with multimodal capabilities in the e-commerce customer
support domain. ECom-Bench features dynamic user simulation based on persona
information collected from real e-commerce customer interactions and a
realistic task dataset derived from authentic e-commerce dialogues. These
tasks, covering a wide range of business scenarios, are designed to reflect
real-world complexities, making ECom-Bench highly challenging. For instance,
even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our
benchmark, highlighting the substantial difficulties posed by complex
e-commerce scenarios. Upon publication, the code and data will be open-sourced
to facilitate further research and development in this domain.

</details>


### [24] [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)
*SeungWon Ji,Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为Smoothie-Qwen的轻量级后处理方法，用于解决多语言大型语言模型中的语言混淆问题。该方法通过调整token级别的输出概率来抑制不希望的语言生成，从而提高模型的语言可控性。实验结果表明，该方法在减少无意中产生的中文输出的同时，保持了多语言任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLMs）常常表现出语言混淆的问题，即倾向于用主导语言生成响应，而不管提示的语言是什么。为了解决这个问题，本文提出了一个有效的解决方案。

Method: 本文提出了一种轻量级的后处理方法Smoothie-Qwen，通过选择性地调整token级别的输出概率，以有效抑制不希望的语言生成。

Result: 将该方法应用于Qwen模型后，无意中产生的中文输出减少了95%以上，同时保持了多语言基准任务的准确性。

Conclusion: 本文提出了一种轻量级的后处理方法Smoothie-Qwen，以解决多语言大型语言模型（LLMs）中出现的语言混淆问题。该方法在不重新训练模型的情况下，有效抑制了不希望的语言生成，提高了模型的语言可控性，使其在全球应用中更加可靠。

Abstract: Multilingual large language models (LLMs) often exhibit language confusion, a
tendency to generate responses in a dominant language irrespective of the
prompt's language. To address this, we propose Smoothie-Qwen, a lightweight,
post-hoc method that mitigates language bias without retraining. This technique
selectively adjusts token-level output probabilities to effectively suppress
undesired language generation. Applied to the Qwen model, our method reduces
unintended Chinese output by over 95% while preserving task accuracy on
multilingual benchmarks. This work provides a practical and efficient solution
for enhancing the language controllability of LLMs, making them more reliable
for global applications.

</details>


### [25] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
*Weihua Du,Pranjal Aggarwal,Sean Welleck,Yiming Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为DualDistill的微调框架，用于从多个教师模型中蒸馏互补的推理策略到一个统一的学生模型中。通过这种方法，我们训练了Agentic-R1，它能够为每个查询动态选择最优策略，对于算术和算法问题调用工具，而对于抽象问题则使用基于文本的推理。实验结果表明，该方法在各种任务中提高了准确性，包括计算密集型和标准基准测试，证明了多策略蒸馏在实现稳健和高效推理方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的长链式思维（long-CoT）模型在数学推理方面表现出色，但依赖于缓慢且容易出错的自然语言轨迹。工具增强的代理通过代码执行来处理算术问题，但在复杂的逻辑任务上往往表现不佳。

Method: 我们引入了一个微调框架DualDistill，该框架从多个教师模型中蒸馏互补的推理策略到一个统一的学生模型中。使用这种方法，我们训练了Agentic-R1，它能够为每个查询动态选择最优策略，对于算术和算法问题调用工具，而对于抽象问题则使用基于文本的推理。

Result: 我们的方法在各种任务中提高了准确性，包括计算密集型和标准基准测试，证明了多策略蒸馏在实现稳健和高效推理方面的有效性。

Conclusion: 我们的方法在各种任务中提高了准确性，包括计算密集型和标准基准测试，证明了多策略蒸馏在实现稳健和高效推理方面的有效性。

Abstract: Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [26] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
*Fedor Chernogorskii,Sergei Averkiev,Liliya Kudraleeva,Zaven Martirosian,Maria Tikhonova,Valentin Malykh,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文介绍了DRAGON，一个用于评估俄语中RAG系统的动态基准测试，解决了现有评估资源不足和静态的问题，并提供了完整的评估框架和公共排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG基准测试主要针对英语，而其他语言如俄语的评估资源稀缺且静态，无法捕捉实际部署中的动态特性。

Method: 本文构建了一个定期更新的俄语新闻和公共文档语料库，并利用知识图谱自动生成问题，以评估检索器和生成器组件。

Result: 本文提出了DRAGON，这是第一个用于评估俄语中RAG系统的动态基准测试，并发布了完整的评估框架和公共排行榜。

Conclusion: 本文提出了DRAGON，这是一个用于评估俄语中RAG系统的动态基准测试，旨在解决现有评估资源不足和静态的问题。同时，本文还发布了完整的评估框架和公共排行榜，以促进社区参与和比较。

Abstract: Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

</details>


### [27] [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
*YiHan Jiao,ZheHao Tan,Dan Yang,DuoLin Sun,Jie Feng,Jian Wang,Peng Wei*

Main category: cs.CL

TL;DR: 本文提出了一种新的RAG指令微调方法HIRAG，通过引入分层思维链来提升模型处理外部知识的能力，并在多个数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统主要依赖大型语言模型本身的上下文学习能力，但对RAG生成模型所需的具体能力的研究不足，导致文档质量不一致和检索系统不完善的问题。此外，有限的研究在微调RAG生成模型时往往缺乏对RAG任务的细致关注或对思维链过程的深入利用。

Method: 提出了一种新的RAG指令微调方法，称为分层思维指令微调检索增强生成（HIRAG），结合了“先思考后回答”的策略，利用多级渐进式思维链来增强模型的开卷考试能力。

Result: HIRAG训练策略显著提高了模型在多个数据集上的性能，证明了其有效性。

Conclusion: 实验表明，HIRAG训练策略显著提高了模型在RGB、PopQA、MuSiQue、HotpotQA和PubmedQA等数据集上的性能。

Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for
addressing the challenges faced by large language models in handling real-time
information and domain-specific problems. Traditional RAG systems primarily
rely on the in-context learning (ICL) capabilities of the large language model
itself. Still, in-depth research on the specific capabilities needed by the RAG
generation model is lacking, leading to challenges with inconsistent document
quality and retrieval system imperfections. Even the limited studies that
fine-tune RAG generative models often \textit{lack a granular focus on RAG
task} or \textit{a deeper utilization of chain-of-thought processes}. To
address this, we propose that RAG models should possess three progressively
hierarchical abilities (1) Filtering: the ability to select relevant
information; (2) Combination: the ability to combine semantic information
across paragraphs; and (3) RAG-specific reasoning: the ability to further
process external knowledge using internal knowledge. Thus, we introduce our new
RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning
Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering"
strategy. This method enhances the model's open-book examination capability by
utilizing multi-level progressive chain-of-thought. Experiments show that the
HIRAG training strategy significantly improves the model's performance on
datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

</details>


### [28] [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
*Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: Omni-router Transformer是一种改进的MoE架构，通过共享路由器提高不同层之间专家的合作和专业化，从而在ASR任务中取得更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法在每层中独立路由专家，而大多数层的路由器做出的专家选择与其他层的路由器选择不强相关，因此需要一种方法来提高不同层之间专家的合作和专业化。

Method: 使用共享路由器跨不同MoE层，以增加不同层之间专家的合作并鼓励更大的专业化。

Result: 在大规模伪标记数据集上的实验和10个不同的、域外ASR基准的评估表明，Omni-router Transformer能够降低训练损失并优于密集模型和Switch Transformer模型，平均词错误率分别减少了11.2%和8.2%。

Conclusion: Omni-router Transformer能够实现更低的训练损失，并且在多个ASR基准上表现优于密集模型和Switch Transformer模型，同时提供结构化的专家使用和改进的鲁棒性。

Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling
to automatic speech recognition (ASR). Traditional MoE methods, such as the
Switch Transformer, route experts independently within each layer. Our analysis
reveals that routers in most layers make expert choices that are not strongly
correlated with the choices of the routers in other layers. To increase the
cooperation between experts in different layers and encourage greater
specialization, we use a shared router across different MoE layers. We call
this model \emph{Omni-router Transformer}. Extensive experiments on a
large-scale pseudo-labeled dataset and evaluations across 10 diverse,
out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is
able to achieve lower training loss and consistently outperform dense and
Switch Transformer models, reducing average word error rates by 11.2% and 8.2%,
respectively, while providing structured expert usage and improved robustness
to diverse data.

</details>


### [29] [GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge](https://arxiv.org/abs/2507.05740)
*Yujia Hu,Tuan-Phong Nguyen,Shrestha Ghosh,Moritz Müller,Simon Razniewski*

Main category: cs.CL

TL;DR: GPTKB v1.5 是一个由 GPT-4.1 构建的 1 亿三元组知识库，用于探索和分析语言模型的知识，并提供了三种使用案例。


<details>
  <summary>Details</summary>
Motivation: 目前对语言模型的事实知识仍不清晰，且难以进行随意浏览和可扩展的统计分析。

Method: GPTKB v1.5 是通过使用 GPTKB 方法进行大规模递归 LLM 知识材料化构建的，成本为 14,000 美元。

Result: GPTKB v1.5 是一个包含 1 亿个三元组的密集互连知识库，展示了三种使用案例：基于链接遍历的 LLM 知识探索、基于 SPARQL 的结构化 LLM 知识查询以及 LLM 知识优缺点的比较探索。

Conclusion: GPTKB v1.5 提供了一种全新的方式来探索和分析语言模型的知识，为系统性研究和自动化知识库构建开辟了新途径。

Abstract: Language models are powerful tools, yet their factual knowledge is still
poorly understood, and inaccessible to ad-hoc browsing and scalable statistical
analysis. This demonstration introduces GPTKB v1.5, a densely interlinked
100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using
the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu
et al., ACL 2025). The demonstration experience focuses on three use cases: (1)
link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM
knowledge querying, (3) comparative exploration of the strengths and weaknesses
of LLM knowledge. Massive-recursive LLM knowledge materialization is a
groundbreaking opportunity both for the research area of systematic analysis of
LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator
is accessible at https://gptkb.org.

</details>


### [30] [DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities](https://arxiv.org/abs/2507.05750)
*Jing Yang Lee,Hamed Bonab,Nasser Zalmout,Ming Zeng,Sanket Lokegaonkar,Colin Lockard,Binxuan Huang,Ritesh Sarkhel,Haodong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过从现有文本语料库中合成对话数据来解决大型语言模型在多轮对话任务中的能力与训练范式的不匹配问题。通过将DocTalk纳入预训练，可以显著提升大型语言模型的多轮对话能力，如上下文记忆和理解，同时不会影响基础性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话任务中越来越被使用，但它们的预训练数据主要由连续的散文组成，这可能导致所需能力和训练范式之间的不匹配。

Method: 提出了一种新颖的方法，通过从现有文本语料库中合成对话数据来解决这一差异。展示了一个将多个相关文档转换为扩展的多轮、多主题信息寻求对话的流程。

Result: 将DocTalk纳入预训练可以带来高达40%的上下文记忆和理解能力的提升，而不会损害基础性能。

Conclusion: 通过将DocTalk纳入预训练，可以显著提升大型语言模型的多轮对话能力，如上下文记忆和理解，同时不会影响基础性能。

Abstract: Large Language Models (LLMs) are increasingly employed in multi-turn
conversational tasks, yet their pre-training data predominantly consists of
continuous prose, creating a potential mismatch between required capabilities
and training paradigms. We introduce a novel approach to address this
discrepancy by synthesizing conversational data from existing text corpora. We
present a pipeline that transforms a cluster of multiple related documents into
an extended multi-turn, multi-topic information-seeking dialogue. Applying our
pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training
dialogue corpus consisting of over 730k long conversations. We hypothesize that
exposure to such synthesized conversational structures during pre-training can
enhance the fundamental multi-turn capabilities of LLMs, such as context memory
and understanding. Empirically, we show that incorporating DocTalk during
pre-training results in up to 40% gain in context memory and understanding,
without compromising base performance. DocTalk is available at
https://huggingface.co/datasets/AmazonScience/DocTalk.

</details>


### [31] [Flippi: End To End GenAI Assistant for E-Commerce](https://arxiv.org/abs/2507.05788)
*Anand A. Rajasekar,Praveen Tangarajan,Anjali Nainani,Amogh Batwal,Vinay Rao Dandin,Anusua Trivedi,Ozan Ersoy*

Main category: cs.CL

TL;DR: This paper introduces Flippi, an end-to-end conversational assistant powered by large language models for e-commerce, which enhances product discovery through natural language dialogue and provides personalized shopping experiences.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges posed by the vast and often overwhelming product landscape in e-commerce, enabling customers to discover products more efficiently through natural language dialogue.

Method: Flippi uses advanced NLP techniques such as Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG), Named Entity Recognition (NER), and Context Reduction to interpret customer queries and provide precise product information. It also includes comparative analysis features to help users make informed choices.

Result: Flippi demonstrates its ability to identify and present the most attractive offers on an e-commerce site, empowering users to make cost-effective decisions. It also outlines a robust architecture for integration across various e-commerce platforms and presents a comprehensive evaluation framework covering performance metrics, user satisfaction, and the impact on customer engagement and conversion rates.

Conclusion: Flippi sets a new standard for customer satisfaction and engagement in the digital marketplace by bridging the convenience of online shopping with personalized assistance traditionally found in physical stores.

Abstract: The emergence of conversational assistants has fundamentally reshaped user
interactions with digital platforms. This paper introduces Flippi-a
cutting-edge, end-to-end conversational assistant powered by large language
models (LLMs) and tailored for the e-commerce sector. Flippi addresses the
challenges posed by the vast and often overwhelming product landscape, enabling
customers to discover products more efficiently through natural language
dialogue. By accommodating both objective and subjective user requirements,
Flippi delivers a personalized shopping experience that surpasses traditional
search methods. This paper details how Flippi interprets customer queries to
provide precise product information, leveraging advanced NLP techniques such as
Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),
Named Entity Recognition (NER), and Context Reduction. Flippi's unique
capability to identify and present the most attractive offers on an e-commerce
site is also explored, demonstrating how it empowers users to make
cost-effective decisions. Additionally, the paper discusses Flippi's
comparative analysis features, which help users make informed choices by
contrasting product features, prices, and other relevant attributes. The
system's robust architecture is outlined, emphasizing its adaptability for
integration across various e-commerce platforms and the technological choices
underpinning its performance and accuracy. Finally, a comprehensive evaluation
framework is presented, covering performance metrics, user satisfaction, and
the impact on customer engagement and conversion rates. By bridging the
convenience of online shopping with the personalized assistance traditionally
found in physical stores, Flippi sets a new standard for customer satisfaction
and engagement in the digital marketplace.

</details>


### [32] [Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports](https://arxiv.org/abs/2507.05799)
*Amane Watahiki,Tomoki Doi,Taiga Shinozaki,Satoshi Nishida,Takuya Niikawa,Katsunori Miyahara,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文研究了大型视觉语言模型（LVLM）在处理amodal completion任务中的表现，发现它们在某些情况下表现不佳，特别是在日语提示下。


<details>
  <summary>Details</summary>
Motivation: 研究LVLM在与amodal completion相关的文本上的推理能力，以了解它们在处理隐藏部分物体时的表现。

Method: 为了填补这一空白，我们构建了一个基于基本形式本体论的基准，以实现对amodal completion的系统分类。

Result: 研究结果表明，虽然许多LVLM在整体表现上达到人类水平，但在某些物体类型的完成上准确性存在差异。值得注意的是，在某些类别中，一些LLaVA-NeXT变体和Claude 3.5 Sonnet在原始图像上的准确性低于缺乏视觉内容的空白刺激。这种差异仅在日语提示下出现，表明这些模型在日语特定的语言能力上存在不足。

Conclusion: 研究结果表明，尽管许多LVLM在整体表现上达到人类水平，但在某些物体类型的完成上准确性存在差异。值得注意的是，在某些类别中，一些LLaVA-NeXT变体和Claude 3.5 Sonnet在原始图像上的准确性低于缺乏视觉内容的空白刺激。这种差异仅在日语提示下出现，表明这些模型在日语特定的语言能力上存在不足。

Abstract: One of the main objectives in developing large vision-language models (LVLMs)
is to engineer systems that can assist humans with multimodal tasks, including
interpreting descriptions of perceptual experiences. A central phenomenon in
this context is amodal completion, in which people perceive objects even when
parts of those objects are hidden. Although numerous studies have assessed
whether computer-vision algorithms can detect or reconstruct occluded regions,
the inferential abilities of LVLMs on texts related to amodal completion remain
unexplored. To address this gap, we constructed a benchmark grounded in Basic
Formal Ontology to achieve a systematic classification of amodal completion.
Our results indicate that while many LVLMs achieve human-comparable performance
overall, their accuracy diverges for certain types of objects being completed.
Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet
exhibit lower accuracy on original images compared to blank stimuli lacking
visual content. Intriguingly, this disparity emerges only under Japanese
prompting, suggesting a deficiency in Japanese-specific linguistic competence
among these models.

</details>


### [33] [How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures](https://arxiv.org/abs/2507.05885)
*Tanvina Patel,Wiebke Hutiri,Aaron Yi Ding,Odette Scharenborg*

Main category: cs.CL

TL;DR: 本文评估了ASR系统的性能和偏见测量方法，并提出了改进报告的建议。


<details>
  <summary>Details</summary>
Motivation: 目前的研究主要集中在检测和量化ASR中的偏见，以及开发缓解方法，但缺乏有效的测量性能和偏见的方法。

Method: 本文比较了文献中和提出的不同性能和偏见测量方法，以评估最先进的端到端ASR系统。

Result: 实验结果表明，平均错误率不足以全面评估系统性能，需要结合其他测量方法。

Conclusion: 本文提出了对ASR系统性能和偏见测量方法的评估，并提出了改进报告的建议。

Abstract: There is increasingly more evidence that automatic speech recognition (ASR)
systems are biased against different speakers and speaker groups, e.g., due to
gender, age, or accent. Research on bias in ASR has so far primarily focused on
detecting and quantifying bias, and developing mitigation approaches. Despite
this progress, the open question is how to measure the performance and bias of
a system. In this study, we compare different performance and bias measures,
from literature and proposed, to evaluate state-of-the-art end-to-end ASR
systems for Dutch. Our experiments use several bias mitigation strategies to
address bias against different speaker groups. The findings reveal that
averaged error rates, a standard in ASR research, alone is not sufficient and
should be supplemented by other measures. The paper ends with recommendations
for reporting ASR performance and bias to better represent a system's
performance for diverse speaker groups, and overall system bias.

</details>


### [34] [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
*Sungjib Lim,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出了一种使用大型语言模型进行虚拟被试模拟的框架，以高效生成具有结构效度的调查项目。


<details>
  <summary>Details</summary>
Motivation: 随着心理计量调查越来越多地用于评估大型语言模型（LLMs）的特质，需要一种适合LLMs的可扩展调查项目生成方法。一个关键挑战是确保生成项目的结构效度，即它们是否真正测量了预期的特质。传统上，这需要耗费大量人力的数据收集。

Method: 我们提出了一种使用LLMs进行虚拟被试模拟的框架。我们的核心思想是考虑中介因素：通过这些因素，相同的特质可以导致对调查项目的不同反应。通过模拟具有不同中介因素的被试，我们确定了能够稳健测量目标特质的调查项目。

Result: 在三种心理学特质理论（大五、施瓦茨、VIA）上的实验表明，我们的中介生成方法和模拟框架有效识别了高效度的项目。LLMs展示了从特质定义生成合理中介因素的能力，并模拟被试行为以验证项目。

Conclusion: 我们的问题定义、指标、方法和数据集为成本效益高的调查开发和更深入理解LLMs如何复制人类行为开辟了新方向。我们将公开发布我们的数据集和代码以支持未来的工作。

Abstract: As psychometric surveys are increasingly used to assess the traits of large
language models (LLMs), the need for scalable survey item generation suited for
LLMs has also grown. A critical challenge here is ensuring the construct
validity of generated items, i.e., whether they truly measure the intended
trait. Traditionally, this requires costly, large-scale human data collection.
To make it efficient, we present a framework for virtual respondent simulation
using LLMs. Our central idea is to account for mediators: factors through which
the same trait can give rise to varying responses to a survey item. By
simulating respondents with diverse mediators, we identify survey items that
robustly measure intended traits. Experiments on three psychological trait
theories (Big5, Schwartz, VIA) show that our mediator generation methods and
simulation framework effectively identify high-validity items. LLMs demonstrate
the ability to generate plausible mediators from trait definitions and to
simulate respondent behavior for item validation. Our problem formulation,
metrics, methodology, and dataset open a new direction for cost-effective
survey development and a deeper understanding of how LLMs replicate human-like
behavior. We will publicly release our dataset and code to support future work.

</details>


### [35] [Few-shot text-based emotion detection](https://arxiv.org/abs/2507.05918)
*Teodor-George Marchitan,Claudiu Creanga,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 该论文介绍了Unibuc-NLP团队在SemEval 2025 Workshop任务11中的方法，主要集中在使用大型语言模型进行实验，并取得了较好的结果。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决基于文本的情感检测中的差距问题，并通过实验验证其方法的有效性。

Method: 该论文主要采用了大型语言模型（如Gemini、Qwen、DeepSeek）进行少样本提示或微调实验。

Result: 该论文在多标签情感检测赛道中，对于英语子集的F1宏得分达到了0.7546（96支队伍中的第26名），对于葡萄牙语（莫桑比克）子集的F1宏得分为0.1727（36支队伍中的第35名），对于Emakhuwa子集的F1宏得分为0.325（31支队伍中的第1名）。

Conclusion: 该论文介绍了Unibuc-NLP团队在SemEval 2025 Workshop任务11中的方法，主要集中在使用大型语言模型进行实验，并取得了较好的结果。

Abstract: This paper describes the approach of the Unibuc - NLP team in tackling the
SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion
Detection. We mainly focused on experiments using large language models
(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With
our final system, for the multi-label emotion detection track (track A), we got
an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36
teams) for the Portuguese (Mozambican) subset and $0.325$ (\textbf{1}/31 teams)
for the Emakhuwa subset.

</details>


### [36] [Towards a Principled Evaluation of Knowledge Editors](https://arxiv.org/abs/2507.05937)
*Sebastian Pohl,Max Ploner,Alan Akbik*

Main category: cs.CL

TL;DR: 本文研究了知识编辑评估方法的稳健性和公平性，发现不同的评估指标和方法以及不同的编辑批次大小会影响知识编辑器的排名，并且基于字符串匹配的评估方法存在假阳性问题。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是研究现有的知识编辑评估方法是否稳健，并且是否公平地评价不同的编辑器。此外，还希望了解这些编辑器对模型整体能力的影响。

Method: 本文通过分析不同的评估指标和方法以及不同的编辑批次大小对知识编辑器排名的影响，来解决评估方法的稳健性和公平性问题。此外，还进行了手动评估以检查基于字符串匹配的评估方法的可靠性。

Result: 结果表明，不同的评估指标和方法以及不同的编辑批次大小会导致知识编辑器排名不同。此外，在通用语言理解任务中也观察到了这种影响。手动评估显示基于字符串匹配的评估方法倾向于产生假阳性结果。

Conclusion: 本文结论是，选择不同的评估指标和方法以及不同的编辑批次大小会导致知识编辑器排名不同，并且在通用语言理解任务中也观察到了这种影响。此外，手动评估显示基于字符串匹配的评估方法倾向于产生假阳性结果。

Abstract: Model editing has been gaining increasing attention over the past few years.
For Knowledge Editing in particular, more challenging evaluation datasets have
recently been released. These datasets use different methodologies to score the
success of editors. Yet, it remains under-explored how robust these
methodologies are and whether they unfairly favor some editors. Moreover, the
disruptive impact of these editors on overall model capabilities remains a
constant blind spot.
  We address both of these problems and show that choosing different metrics
and evaluation methodologies as well as different edit batch sizes can lead to
a different ranking of knowledge editors. Crucially we demonstrate this effect
also on general language understanding tasks evaluated alongside the knowledge
editing tasks. Further we include a manual assessment of the string matching
based evaluation method for knowledge editing that is favored by recently
released datasets, revealing a tendency to produce false positive matches.

</details>


### [37] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
*Bing Wang,Ximing Li,Mengzhe Ye,Changchun Li,Bo Fu,Jianfeng Qu,Lin Yuanbo Wu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.

</details>


### [38] [Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems](https://arxiv.org/abs/2507.05940)
*Sandeep Mishra,Anubhab Mandal,Bishal Santra,Tushar Abhishek,Pawan Goyal,Manish Gupta*

Main category: cs.CL

TL;DR: 本文研究了聊天系统中的ghosting问题，通过四个公开的数据集进行实验，比较了不同的自动补全方法，并提出了一种新的动态提前停止策略。结果显示，统计方法在已见前缀上表现更好，而神经网络模型在未见查询上更优。添加对话上下文显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: Ghosting是现代搜索引擎和聊天界面中一个重要的功能，可以极大地增强用户体验。然而，尽管聊天系统越来越多地使用ghosting，但这一问题在NLP/ML研究社区中却很少受到关注。缺乏标准化的基准和相对性能分析。

Method: 我们使用四个公开的对话数据集（两个人类-人类：DailyDialog和DSTC7-Ubuntu，两个人类-机器人：Open Assistant和ShareGPT）对这个问题进行了开放和全面的研究。我们实验了各种现有的查询自动补全方法（使用tries）、n-gram方法和深度学习方法，有无对话上下文。我们还提出了一种基于熵的动态提前停止策略。

Result: 统计n-gram模型和tries在已见前缀的模型性能和推理效率方面优于基于深度学习的模型。对于未见查询，神经网络模型如T5和Phi-2表现更好。添加对话上下文显著提高了ghosting的质量，特别是在Open-Assistant和ShareGPT上。

Conclusion: 我们的分析发现，统计n-gram模型和tries在已见前缀的模型性能和推理效率方面优于基于深度学习的模型。对于未见查询，像T5和Phi-2这样的神经网络模型表现更好。添加对话上下文显著提高了ghosting的质量，尤其是在Open-Assistant和ShareGPT上。我们公开了代码和数据。

Abstract: Ghosting, the ability to predict a user's intended text input for inline
query auto-completion, is an invaluable feature for modern search engines and
chat interfaces, greatly enhancing user experience. By suggesting completions
to incomplete queries (or prefixes), ghosting aids users with slow typing
speeds, disabilities, or limited language proficiency. Ghosting is a
challenging problem and has become more important with the ubiquitousness of
chat-based systems like ChatGPT, Copilot, etc. Despite the increasing
prominence of chat-based systems utilizing ghosting, this challenging problem
of Chat-Ghosting has received little attention from the NLP/ML research
community. There is a lack of standardized benchmarks and relative performance
analysis of deep learning and non-deep learning methods. We address this
through an open and thorough study of this problem using four publicly
available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and
two human-bot (Open Assistant and ShareGPT). We experiment with various
existing query auto-completion methods (using tries), n-gram methods and deep
learning methods, with and without dialog context. We also propose a novel
entropy-based dynamic early stopping strategy. Our analysis finds that
statistical n-gram models and tries outperform deep learning based models in
terms of both model performance and inference efficiency for seen prefixes. For
unseen queries, neural models like T5 and Phi-2 lead to better results. Adding
conversational context leads to significant improvements in ghosting quality,
especially for Open-Assistant and ShareGPT. We make code and data publicly
available

</details>


### [39] [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)
*Lucas Fonseca Lage,Simon Ostermann*

Main category: cs.CL

TL;DR: OpenFActScore is an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models. It enables the use of any Hugging Face-compatible model for both AFG and AFV and shows that open models can approximate the performance of closed-source systems.


<details>
  <summary>Details</summary>
Motivation: The original FActScore relies on closed-source and commercial models, which limits transparency and reproducibility. OpenFActScore aims to enable the use of any Hugging Face-compatible model for both AFG and AFV.

Method: OpenFActScore is an open-source implementation of the FActScore framework that uses Atomic Fact Generation (AFG) and Atomic Fact Validation (AFV) to evaluate the factual accuracy of text generated by large language models (LLMs).

Result: OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. The results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments.

Conclusion: OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation.

Abstract: We introduce OpenFActScore, an open-source implementation of the FActScore
framework for evaluating the factuality of text generated by large language
models (LLMs). FActScore evaluates the factual accuracy of long-form text by
using Atomic Fact Generation (AFG) to extract individual factual claims and
Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge
source. While the original FActScore relies on closed-source and commercial
models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any
Hugging Face-compatible model for both AFG and AFV. We provide a detailed
technical overview of our implementation, highlighting design choices and
modifications made to support open models. We evaluate multiple open-source
LLMs on both AFG and AFV using the original FActScore benchmark, reporting
BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our
results show that open models can approximate the performance of closed-source
systems, with Gemma achieving the best overall performance, and our final setup
obtains a 0.99 Pearson correlation with the original FActScore experiments.
OpenFActScore promotes transparency, reproducibility, and cost-effective
evaluation, and is available at: https://github.com/lflage/OpenFActScore.

</details>


### [40] [We Should Evaluate Real-World Impact](https://arxiv.org/abs/2507.05973)
*Ehud Reiter*

Main category: cs.CL

TL;DR: The ACL community rarely evaluates the real-world impact of NLP systems, and this needs to change for NLP technology to be more useful and adopted.


<details>
  <summary>Details</summary>
Motivation: The ACL community has very little interest in evaluating the real-world impact of NLP systems.

Method: A structured survey of the ACL Anthology.

Result: Perhaps 0.1% of the papers in the ACL Anthology contain such evaluations; furthermore, most papers which include impact evaluations present them very sketchily and instead focus on metric evaluations.

Conclusion: NLP technology would be more useful and more quickly adopted if we seriously tried to understand and evaluate its real-world impact.

Abstract: The ACL community has very little interest in evaluating the real-world
impact of NLP systems. A structured survey of the ACL Anthology shows that
perhaps 0.1% of its papers contain such evaluations; furthermore most papers
which include impact evaluations present them very sketchily and instead focus
on metric evaluations. NLP technology would be more useful and more quickly
adopted if we seriously tried to understand and evaluate its real-world impact.

</details>


### [41] [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)
*Gabriel Chua,Leanne Tan,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本文介绍了RabakBench，这是一个针对新加坡独特语言环境的多语言安全基准测试，涵盖了Singlish、中文、马来语和泰米尔语。RabakBench通过一个可扩展的三阶段流程构建：(i) 生成 - 通过使用LLM驱动的红队技术增强真实的Singlish网络内容来生成对抗性示例；(ii) 标注 - 使用多数投票的LLM标注器进行半自动化多标签安全标注，与人类判断一致；(iii) 翻译 - 保持语言细微差别和毒性的同时进行高保真翻译。最终数据集包含四个语言和六个细粒度安全类别中的5000多个安全标记示例。评估11个流行的开源和闭源防护分类器显示性能显著下降。RabakBench不仅在东南亚多语言环境中实现了稳健的安全评估，还为在低资源环境中构建本地化安全数据集提供了一个可复制的框架。该基准数据集，包括人工验证的翻译和评估代码，都是公开的。


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks.

Method: RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages.

Result: The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation.

Conclusion: RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available.

Abstract: Large language models (LLMs) and their safety classifiers often perform
poorly on low-resource languages due to limited training data and evaluation
benchmarks. This paper introduces RabakBench, a new multilingual safety
benchmark localized to Singapore's unique linguistic context, covering
Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a
scalable three-stage pipeline: (i) Generate - adversarial example generation by
augmenting real Singlish web content with LLM-driven red teaming; (ii) Label -
semi-automated multi-label safety annotation using majority-voted LLM labelers
aligned with human judgments; and (iii) Translate - high-fidelity translation
preserving linguistic nuance and toxicity across languages. The final dataset
comprises over 5,000 safety-labeled examples across four languages and six
fine-grained safety categories with severity levels. Evaluations of 11 popular
open-source and closed-source guardrail classifiers reveal significant
performance degradation. RabakBench not only enables robust safety evaluation
in Southeast Asian multilingual settings but also offers a reproducible
framework for building localized safety datasets in low-resource environments.
The benchmark dataset, including the human-verified translations, and
evaluation code are publicly available.

</details>


### [42] [Evolution without Large Models: Training Language Model with Task Principles](https://arxiv.org/abs/2507.05991)
*Minghang Zhu,Shen Gao,Zhengliang Shi,Jiabao Fang,Pengjie Ren,Zhaochun Ren,Zhumin Chen,Shuo Shang*

Main category: cs.CL

TL;DR: 本文提出了一种自我进化方法，通过多级原理生成和基于原理的实例生成来提高语言模型性能，同时减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型训练方法虽然减少了训练成本，但面临数据增强过程中的高碳排放和使用封闭源代码LLM时的数据泄露风险。

Method: 我们提出了一个自我进化方法，包括多级原理生成和基于原理的实例生成。多级原理生成使大规模模型能够根据少量任务数据总结任务完成原则，然后基于原理的实例生成使用这些任务原则生成大量数据用于模型训练。

Result: 实验结果表明，与直接使用较小规模的语言模型生成数据相比，我们的方法显著提高了模型性能。此外，由于仅使用大规模语言模型生成任务完成原则，训练模型相关的碳排放量大大减少。

Conclusion: 我们的方法在模型性能上显著优于直接使用较小规模的语言模型生成数据的方法，并且由于只使用大规模语言模型生成任务完成原则，训练模型相关的碳排放量大大减少。

Abstract: A common training approach for language models involves using a large-scale
language model to expand a human-provided dataset, which is subsequently used
for model training.This method significantly reduces training costs by
eliminating the need for extensive human data annotation. However, it still
faces challenges such as high carbon emissions during data augmentation and the
risk of data leakage when we use closed-source LLMs. To address these issues,
we propose a self-evolution method for language models. First, we introduce the
Multi-level Principle Generation, which enables a large-scale model to
summarize task-completion principles based on a small amount of task data.
Then, we propose the Principle-based Instance Generation, in which a
smaller-scale language model uses these task principles to generate a large
amount of data. This data is then used for model training. Experimental results
show that our proposed method significantly improves model performance compared
to directly using a smaller-scale language model to generate data.
Additionally, since we only use the large-scale language model to generate the
task-completion principles, the carbon emissions associated with training the
model are greatly reduced.

</details>


### [43] [DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations](https://arxiv.org/abs/2507.05997)
*Nicholas Popovič,Ashish Kangen,Tim Schopf,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的合成数据生成和上下文学习的管道，以解决零样本或少样本设置下的文档级别实体和关系提取问题。


<details>
  <summary>Details</summary>
Motivation: 在零样本或少样本设置中，高质量的注释语料库仍然稀缺。现有的方法依赖于手动注释的示例或直接的零样本推理，而本文旨在通过合成数据生成和检索-based上下文学习来解决这一问题。

Method: 本文提出了一种完全自动化的基于大语言模型的合成数据生成和上下文学习的管道。该方法结合了合成数据生成与基于检索的上下文学习，使用了一个优化推理的语言模型。

Result: 基于我们的方法，我们生成了一个包含超过5k个维基百科摘要的合成数据集，其中大约有59k个实体和30k个关系三元组。此外，在DocIE共享任务上评估了上下文学习的表现。

Conclusion: 在零样本设置下，文档级别的联合实体和关系提取仍然是一个具有挑战性的任务，即使对于最先进的大型语言模型也是如此。

Abstract: Large, high-quality annotated corpora remain scarce in document-level entity
and relation extraction in zero-shot or few-shot settings. In this paper, we
present a fully automatic, LLM-based pipeline for synthetic data generation and
in-context learning for document-level entity and relation extraction. In
contrast to existing approaches that rely on manually annotated demonstrations
or direct zero-shot inference, our method combines synthetic data generation
with retrieval-based in-context learning, using a reasoning-optimized language
model. This allows us to build a high-quality demonstration database without
manual annotation and to dynamically retrieve relevant examples at inference
time. Based on our approach we produce a synthetic dataset of over $5k$
Wikipedia abstracts with approximately $59k$ entities and $30k$ relation
triples. Finally, we evaluate in-context learning performance on the DocIE
shared task, extracting entities and relations from long documents in a
zero-shot setting. We find that in-context joint entity and relation extraction
at document-level remains a challenging task, even for state-of-the-art large
language models.

</details>


### [44] [Conditional Multi-Stage Failure Recovery for Embodied Agents](https://arxiv.org/abs/2507.06016)
*Youmna Farag,Svetlana Stoyanchev,Mohan Li,Simon Keizer,Rama Doddipatla*

Main category: cs.CL

TL;DR: 本文提出了一种基于零次链提示的条件多阶段故障恢复框架，通过利用LLM的推理能力来分析执行挑战并制定解决方案，在TEACH数据集的TfD基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 具身代理执行复杂任务容易出现执行失败，这促使需要有效的故障恢复机制。

Method: 我们引入了一个条件多阶段故障恢复框架，该框架采用零次链提示。框架分为四个错误处理阶段，其中三个在任务执行期间运行，一个作为执行后的反思阶段。我们的方法利用LLM的推理能力分析环境中的执行挑战并制定战略解决方案。

Result: 我们在TEACH数据集的TfD基准上评估了我们的方法，并实现了最先进的性能，优于没有错误恢复的基线11.5%，并且超过了现有最强模型19%。

Conclusion: 我们的方法在TEACH数据集的TfD基准上实现了最先进的性能，优于没有错误恢复的基线11.5%，并且超过了现有最强模型19%。

Abstract: Embodied agents performing complex tasks are susceptible to execution
failures, motivating the need for effective failure recovery mechanisms. In
this work, we introduce a conditional multistage failure recovery framework
that employs zero-shot chain prompting. The framework is structured into four
error-handling stages, with three operating during task execution and one
functioning as a post-execution reflection phase. Our approach utilises the
reasoning capabilities of LLMs to analyse execution challenges within their
environmental context and devise strategic solutions. We evaluate our method on
the TfD benchmark of the TEACH dataset and achieve state-of-the-art
performance, outperforming a baseline without error recovery by 11.5% and
surpassing the strongest existing model by 19%.

</details>


### [45] [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
*Yizhan Huang,Zhe Yang,Meifang Chen,Jianping Zhang,Michael R. Lyu*

Main category: cs.CL

TL;DR: 本文研究了大语言模型中训练数据的记忆难度，提出了熵-记忆定律，并开发了一种用于区分训练和测试数据的方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何表征训练数据在大语言模型中的记忆难度，这是一个尚未深入探索的问题。

Method: 通过在OLMo系列开源模型上的实证实验，提出了熵-记忆定律，并采用相同策略发现了数据集推断方法。

Result: 数据熵与记忆分数呈线性相关，高度随机的字符串（即无意义字符串）表现出比更广泛的训练语料库更低的实证熵。

Conclusion: 本文提出了一个简单的有效方法来区分训练数据和测试数据，即数据集推断（DI）。

Abstract: Large Language Models (LLMs) are known to memorize portions of their training
data, sometimes reproducing content verbatim when prompted appropriately. In
this work, we investigate a fundamental yet under-explored question in the
domain of memorization: How to characterize memorization difficulty of training
data in LLMs? Through empirical experiments on OLMo, a family of open models,
we present the Entropy-Memorization Law. It suggests that data entropy is
linearly correlated with memorization score. Moreover, in a case study of
memorizing highly randomized strings, or "gibberish", we observe that such
sequences, despite their apparent randomness, exhibit unexpectedly low
empirical entropy compared to the broader training corpus. Adopting the same
strategy to discover Entropy-Memorization Law, we derive a simple yet effective
approach to distinguish training and testing data, enabling Dataset Inference
(DI).

</details>


### [46] [A Survey on Prompt Tuning](https://arxiv.org/abs/2507.06085)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: 本文综述了提示调优方法，将其分为直接提示学习和迁移学习两类，并对每种方法进行了分析。


<details>
  <summary>Details</summary>
Motivation: 本文旨在综述提示调优方法，以帮助研究者更好地理解和应用这一参数高效的方法。

Method: 本文将现有的提示调优方法分为直接提示学习和迁移学习两类，并对每种方法进行了分析。

Result: 本文对不同方法进行了分析，并提供了比较不同框架的可视化示例。

Conclusion: 本文综述了提示调优方法，分析了其设计、创新、见解、优势和劣势，并指出了计算效率和训练稳定性方面的挑战，讨论了未来改进训练鲁棒性和扩展应用范围的方向。

Abstract: This survey reviews prompt tuning, a parameter-efficient approach for
adapting language models by prepending trainable continuous vectors while
keeping the model frozen. We classify existing approaches into two categories:
direct prompt learning and transfer learning. Direct prompt learning methods
include: general optimization approaches, encoder-based methods, decomposition
strategies, and mixture-of-experts frameworks. Transfer learning methods
consist of: general transfer approaches, encoder-based methods, and
decomposition strategies. For each method, we analyze method designs,
innovations, insights, advantages, and disadvantages, with illustrative
visualizations comparing different frameworks. We identify challenges in
computational efficiency and training stability, and discuss future directions
in improving training robustness and broadening application scope.

</details>


### [47] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
*Mohammad Mahdi Derakhshani,Dheeraj Varghese,Marzieh Fadaee,Cees G. M. Snoek*

Main category: cs.CL

TL;DR: 本文介绍了NeoBabel，一个新颖的多语言图像生成框架，它在性能、效率和包容性方面设定了新的帕累托前沿，支持六种语言。该模型通过大规模多语言预训练和高分辨率指令调优进行训练。评估结果显示，NeoBabel在多语言性能上达到最先进的水平，同时保持了强大的英语能力。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成的进步主要以英语为中心，这给非英语使用者带来了障碍，并加剧了数字不平等。现有的系统依赖于翻译管道，这些管道引入了语义漂移、计算开销和文化偏差。

Method: NeoBabel框架结合了大规模多语言预训练和高分辨率指令调优进行训练。

Result: NeoBabel在多语言性能上达到了最先进的水平，同时保持了强大的英语能力，在m-GenEval上得分为0.75，在m-DPG上得分为0.68。它在英语任务上的表现与领先模型相当，而在多语言基准测试中分别优于它们+0.11和+0.09。

Conclusion: 我们的工作表明，多语言能力不是一种权衡，而是改进生成式AI的稳健性、效率和文化精确性的催化剂。

Abstract: Text-to-image generation advancements have been predominantly
English-centric, creating barriers for non-English speakers and perpetuating
digital inequities. While existing systems rely on translation pipelines, these
introduce semantic drift, computational overhead, and cultural misalignment. We
introduce NeoBabel, a novel multilingual image generation framework that sets a
new Pareto frontier in performance, efficiency and inclusivity, supporting six
languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is
trained using a combination of large-scale multilingual pretraining and
high-resolution instruction tuning. To evaluate its capabilities, we expand two
English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.
NeoBabel achieves state-of-the-art multilingual performance while retaining
strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.
Notably, it performs on par with leading models on English tasks while
outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though
these models are built on multilingual base LLMs. This demonstrates the
effectiveness of our targeted alignment training for preserving and extending
crosslingual generalization. We further introduce two new metrics to rigorously
assess multilingual alignment and robustness to code-mixed prompts. Notably,
NeoBabel matches or exceeds English-only models while being 2-4x smaller. We
release an open toolkit, including all code, model checkpoints, a curated
dataset of 124M multilingual text-image pairs, and standardized multilingual
evaluation protocols, to advance inclusive AI research. Our work demonstrates
that multilingual capability is not a trade-off but a catalyst for improved
robustness, efficiency, and cultural fidelity in generative AI.

</details>


### [48] [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)
*Taolin Zhang,Zihan Ma,Maosong Cao,Junnan Liu,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文介绍了Code Triangle框架，用于评估大型语言模型在代码生成方面的表现。研究发现，虽然LLMs能够形成自洽的系统，但其解决方案缺乏多样性与鲁棒性。通过结合人类生成的内容和模型混合，可以显著提升LLMs的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面取得了显著进展，但它们真正的编程能力仍未得到充分探索。我们需要一种系统的方法来评估LLMs的编程能力，以发现其局限性并寻找改进方法。

Method: 我们引入了Code Triangle框架，系统地从三个基本维度评估LLMs：编辑分析、代码实现和测试用例生成。通过在竞赛编程基准上的广泛实验，我们分析了LLMs的表现。

Result: 实验结果表明，LLMs可以在这些维度上形成自洽的系统，但其解决方案往往缺乏人类程序员的多样性和鲁棒性。模型错误倾向于因训练数据偏差和有限的推理转移而聚集。

Conclusion: 我们的研究展示了将人类生成的编辑器文章、解决方案和多样的测试用例相结合，以及利用模型混合可以显著提高LLMs的性能和鲁棒性。此外，我们揭示了LLMs在认知上的一致性和不一致性，这可能有助于自我反思和自我改进，为开发更强大的编码模型提供了潜在方向。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation, yet their true programming competence remains underexplored. We
introduce the Code Triangle framework, which systematically evaluates LLMs
across three fundamental dimensions: editorial analysis, code implementation,
and test case generation. Through extensive experiments on competitive
programming benchmarks, we reveal that while LLMs can form a self-consistent
system across these dimensions, their solutions often lack the diversity and
robustness of human programmers. We identify a significant distribution shift
between model cognition and human expertise, with model errors tending to
cluster due to training data biases and limited reasoning transfer. Our study
demonstrates that incorporating human-generated editorials, solutions, and
diverse test cases, as well as leveraging model mixtures, can substantially
enhance both the performance and robustness of LLMs. Furthermore, we reveal
both the consistency and inconsistency in the cognition of LLMs that may
facilitate self-reflection and self-improvement, providing a potential
direction for developing more powerful coding models.

</details>


### [49] [Skywork-R1V3 Technical Report](https://arxiv.org/abs/2507.06167)
*Wei Shen,Jiangbo Pei,Yi Peng,Xuchen Song,Yang Liu,Jian Peng,Haofeng Sun,Yunzhuo Hao,Peiyu Wang,Yahui Zhou*

Main category: cs.CL

TL;DR: Skywork-R1V3 is an advanced, open-source vision-language model that transfers reasoning skills from text-only LLMs to visual tasks using a post-training RL framework. It achieves state-of-the-art results on MMMU and showcases RL as a powerful engine for advancing open-source VLM capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to transfer reasoning skills from text-only Large Language Models (LLMs) to visual tasks and to improve the performance of multimodal reasoning models.

Method: The paper introduces a post-training RL framework that effectively activates and enhances the model's reasoning ability without additional continue pre-training. It also introduces a unique indicator of reasoning capability, the entropy of critical reasoning tokens, and analyzes curriculum learning and reinforcement finetuning strategies.

Result: Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%, which matches entry-level human capabilities. The RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs.

Conclusion: Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities.

Abstract: We introduce Skywork-R1V3, an advanced, open-source vision-language model
(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies
in effectively transferring reasoning skills from text-only Large Language
Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily
stems from our elaborate post-training RL framework, which effectively
activates and enhances the model's reasoning ability, without the need for
additional continue pre-training. Through this framework, we further uncover
the fundamental role of the connector module in achieving robust cross-modal
alignment for multimodal reasoning models. In addition, we introduce a unique
indicator of reasoning capability, the entropy of critical reasoning tokens,
which has proven highly effective for checkpoint selection during RL training.
Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving
from 64.3% to 76.0%. This performance matches entry-level human capabilities.
Remarkably, our RL-powered post-training approach enables even the 38B
parameter model to rival top closed-source VLMs. The implementation
successfully transfers mathematical reasoning to other subject-related
reasoning tasks. We also include an analysis of curriculum learning and
reinforcement finetuning strategies, along with a broader discussion on
multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal
reasoning, showcasing RL as a powerful engine for advancing open-source VLM
capabilities.

</details>


### [50] [CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization](https://arxiv.org/abs/2507.06181)
*Zhongyuan Peng,Yifan Yao,Kaijing Ma,Shuyue Guo,Yizhe Li,Yichi Zhang,Chenchen Zhang,Yifan Zhang,Zhouliang Yu,Luming Li,Minghao Liu,Yihang Xia,Jiawei Shen,Yuchen Wu,Yixin Cao,Zhaoxiang Zhang,Wenhao Huang,Jiaheng Liu,Ge Zhang*

Main category: cs.CL

TL;DR: 本文提出了 CriticLean 框架，通过强化学习提升评判阶段的作用，显著提高了形式化语义一致性的评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注生成和编译的成功，而较少关注评判阶段，即评估生成的形式化是否真正捕捉原始问题的语义意图。

Method: 引入了 CriticLean 框架，包括 CriticLeanGPT 和 CriticLeanBench，用于评估形式化语义一致性和构建高质量数据集 FineLeanCorpus。

Result: CriticLeanGPT 在区分语义正确和错误的形式化方面显著优于现有基线模型，并构建了一个高质量的数据集 FineLeanCorpus。

Conclusion: 优化评判阶段对于生成可靠的正式化至关重要，CriticLean 为未来的形式化数学推理提供了有价值的见解。

Abstract: Translating natural language mathematical statements into formal, executable
code is a fundamental challenge in automated theorem proving. While prior work
has focused on generation and compilation success, little attention has been
paid to the critic phase-the evaluation of whether generated formalizations
truly capture the semantic intent of the original problem. In this paper, we
introduce CriticLean, a novel critic-guided reinforcement learning framework
that elevates the role of the critic from a passive validator to an active
learning component. Specifically, first, we propose the CriticLeanGPT, trained
via supervised fine-tuning and reinforcement learning, to rigorously assess the
semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,
a benchmark designed to measure models' ability to distinguish semantically
correct from incorrect formalizations, and demonstrate that our trained
CriticLeanGPT models can significantly outperform strong open- and
closed-source baselines. Building on the CriticLean framework, we construct
FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich
domain diversity, broad difficulty coverage, and high correctness based on
human evaluation. Overall, our findings highlight that optimizing the critic
phase is essential for producing reliable formalizations, and we hope our
CriticLean will provide valuable insights for future advances in formal
mathematical reasoning.

</details>


### [51] [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)
*Maximilian Heil,Dionne Bang*

Main category: cs.CL

TL;DR: 本文研究了迁移学习和风格数据增强在改善英语新闻文本中主观和客观句子分类方面的有效性，并发现指定编码器的迁移学习优于通用编码器的微调，同时精心策划的增强显著提高了模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提高英语新闻文本中主观和客观句子的分类效果，通过迁移学习和风格数据增强的方法来实现这一目标。

Method: 我们研究了迁移学习和风格数据增强在改善英语新闻文本中主观和客观句子分类方面的有效性。我们的方法对比了预训练编码器的微调和迁移学习的微调变压器在相关任务上的表现。我们还引入了一个使用GPT-4o生成预定主观风格的改写语句的受控增强管道，并采用相同模型来纠正和优化生成的样本以确保标签和风格的一致性。

Result: 实验结果表明，指定编码器的迁移学习优于通用编码器的微调，并且经过精心策划的增强显著提高了模型的鲁棒性，特别是在检测主观内容方面。我们的官方提交排名为24名中的第16名。

Conclusion: 我们的研究结果表明，指定编码器的迁移学习优于通用编码器的微调，并且经过精心策划的增强显著提高了模型的鲁棒性，特别是在检测主观内容方面。结合编码器专业化与标签一致增强的价值得到了证实。

Abstract: This paper presents our submission to Task 1, Subjectivity Detection, of the
CheckThat! Lab at CLEF 2025. We investigate the effectiveness of
transfer-learning and stylistic data augmentation to improve classification of
subjective and objective sentences in English news text. Our approach contrasts
fine-tuning of pre-trained encoders and transfer-learning of fine-tuned
transformer on related tasks. We also introduce a controlled augmentation
pipeline using GPT-4o to generate paraphrases in predefined subjectivity
styles. To ensure label and style consistency, we employ the same model to
correct and refine the generated samples. Results show that transfer-learning
of specified encoders outperforms fine-tuning general-purpose ones, and that
carefully curated augmentation significantly enhances model robustness,
especially in detecting subjective content. Our official submission placed us
$16^{th}$ of 24 participants. Overall, our findings underscore the value of
combining encoder specialization with label-consistent augmentation for
improved subjectivity detection. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-subject.

</details>


### [52] [DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification](https://arxiv.org/abs/2507.06195)
*Maximilian Heil,Aleksandar Pramov*

Main category: cs.CL

TL;DR: 本研究评估了用于验证数值和时间声明真伪的建模策略，发现证据质量和标记化方式对性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 数值声明、涉及数量的陈述、比较和时间参考对于自动事实检查系统来说具有独特的挑战性。

Method: 我们使用QuanTemp数据集并构建自己的证据检索管道来评估数值和时间声明的真伪预测建模策略。我们研究了三个关键因素：(1) 使用ModernBERT的更多证据和更长的输入上下文窗口的影响，(2) 右到左（R2L）标记化的影响，以及(3) 它们对分类性能的综合影响。

Result: 与算术推理任务中的先前发现相反，R2L标记化并未提升数值任务的自然语言推理（NLI）。更长的上下文窗口也没有提高真伪性能，这突显了证据质量是主要瓶颈。

Conclusion: 我们的最佳系统在Task 3 of CheckThat! 2025中取得了竞争性的宏平均F1分数0.57，并位列前四名。

Abstract: Numerical claims, statements involving quantities, comparisons, and temporal
references, pose unique challenges for automated fact-checking systems. In this
study, we evaluate modeling strategies for veracity prediction of such claims
using the QuanTemp dataset and building our own evidence retrieval pipeline. We
investigate three key factors: (1) the impact of more evidences with longer
input context windows using ModernBERT, (2) the effect of right-to-left (R2L)
tokenization, and (3) their combined influence on classification performance.
Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does
not boost natural language inference (NLI) of numerical tasks. A longer context
window does also not enhance veracity performance either, highlighting evidence
quality as the dominant bottleneck. Our best-performing system achieves
competitive macro-average F1 score of 0.57 and places us among the Top-4
submissions in Task 3 of CheckThat! 2025. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-numerical.

</details>


### [53] [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)
*Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Ho-Kyeong Ra,Viren Bajaj,Zeya Ahmad*

Main category: cs.CL

TL;DR: UQLM 是一个 Python 包，用于使用不确定性量化技术检测 LLM 的幻觉，提供了一种现成的解决方案，可以提高 LLM 输出的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM 生成虚假或误导性内容的问题影响了下游应用的安全性和可信度，因此需要一种有效的检测方法。

Method: UQLM 使用最先进的不确定性量化 (UQ) 技术来检测 LLM 的幻觉。

Result: UQLM 提供了一套基于 UQ 的评分器，可以计算从 0 到 1 的响应级置信度分数。

Conclusion: UQLM 提供了一种现成的解决方案，可用于基于不确定性的幻觉检测，可以轻松集成以提高 LLM 输出的可靠性。

Abstract: Hallucinations, defined as instances where Large Language Models (LLMs)
generate false or misleading content, pose a significant challenge that impacts
the safety and trust of downstream applications. We introduce UQLM, a Python
package for LLM hallucination detection using state-of-the-art uncertainty
quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers
that compute response-level confidence scores ranging from 0 to 1. This library
provides an off-the-shelf solution for UQ-based hallucination detection that
can be easily integrated to enhance the reliability of LLM outputs.

</details>


### [54] [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)
*Rui-Jie Zhu,Tianhao Peng,Tianhao Cheng,Xingwei Qu,Jinfa Huang,Dawei Zhu,Hao Wang,Kaiwen Xue,Xuanliang Zhang,Yong Shan,Tianle Cai,Taylor Kergan,Assel Kembay,Andrew Smith,Chenghua Lin,Binh Nguyen,Yuqi Pan,Yuhong Chou,Zefan Cai,Zhenhe Wu,Yongchi Zhao,Tianyu Liu,Jian Yang,Wangchunshu Zhou,Chujie Zheng,Chongxuan Li,Yuyin Zhou,Zhoujun Li,Zhaoxiang Zhang,Jiaheng Liu,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文综述了隐式推理领域，探讨了其基础神经网络层的作用、多种隐式推理方法以及高级范式，旨在推动大语言模型认知前沿的研究。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是推进隐式推理研究，以解决显式链式思维（CoT）在自然语言推理中的表达带宽限制问题。

Method: 本文通过 examining the foundational role of neural network layers, exploring diverse latent reasoning methodologies, and discussing advanced paradigms such as infinite-depth latent reasoning via masked diffusion models来推进隐式推理研究。

Result: 本文提供了隐式推理新兴领域的全面概述，并讨论了包括基于激活的循环、隐藏状态传播和微调策略在内的多种隐式推理方法。

Conclusion: 本文通过统一这些观点，旨在澄清隐式推理的概念格局，并勾勒出LLM认知前沿的研究未来方向。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities, especially when guided by explicit chain-of-thought (CoT)
reasoning that verbalizes intermediate steps. While CoT improves both
interpretability and accuracy, its dependence on natural language reasoning
limits the model's expressive bandwidth. Latent reasoning tackles this
bottleneck by performing multi-step inference entirely in the model's
continuous hidden state, eliminating token-level supervision. To advance latent
reasoning research, this survey provides a comprehensive overview of the
emerging field of latent reasoning. We begin by examining the foundational role
of neural network layers as the computational substrate for reasoning,
highlighting how hierarchical representations support complex transformations.
Next, we explore diverse latent reasoning methodologies, including
activation-based recurrence, hidden state propagation, and fine-tuning
strategies that compress or internalize explicit reasoning traces. Finally, we
discuss advanced paradigms such as infinite-depth latent reasoning via masked
diffusion models, which enable globally consistent and reversible reasoning
processes. By unifying these perspectives, we aim to clarify the conceptual
landscape of latent reasoning and chart future directions for research at the
frontier of LLM cognition. An associated GitHub repository collecting the
latest papers and repos is available at:
https://github.com/multimodal-art-projection/LatentCoT-Horizon/.

</details>


### [55] [DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media](https://arxiv.org/abs/2507.06205)
*Ayush Parikh,Hoang Thanh Thanh Truong,Jeanette Schofield,Maximilian Heil*

Main category: cs.CL

TL;DR: 本文介绍了DS@GT团队在CLEF 2025 CheckThat! Task 4a科学网络话语检测任务中探索的方法，包括三种建模方法，并取得了较好的成绩。


<details>
  <summary>Details</summary>
Motivation: 我们旨在确定推文是否包含科学声明、对科学研究或出版物的引用以及/或提及科学实体，如大学或科学家。

Method: 我们探索了三种建模方法：transformer微调、LLM的少量提示和一个结合模型，其设计基于早期实验。

Result: 我们的团队在比赛中取得了第7名，宏平均F1得分为0.8611，比DeBERTaV3基线提高了0.8375。

Conclusion: 我们的团队在比赛中取得了第7名，宏平均F1得分为0.8611，比DeBERTaV3基线提高了0.8375。

Abstract: In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a
Scientific Web Discourse Detection, present the methods we explored for this
task. For this multiclass classification task, we determined if a tweet
contained a scientific claim, a reference to a scientific study or publication,
and/or mentions of scientific entities, such as a university or a scientist. We
present 3 modeling approaches for this task: transformer finetuning, few-shot
prompting of LLMs, and a combined ensemble model whose design was informed by
earlier experiments. Our team placed 7th in the competition, achieving a
macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline
of 0.8375. Our code is available on Github at
https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.

</details>


### [56] [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
*Zhiyuan Peng,Ting-ruen Wei,Tingyu Song,Yilun Zhao,Yi Fang*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估LLM-based rerankers的效率-效果权衡的方法，即E²R-FLOPs，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法依赖于硬件和运行时选择，难以准确评估效率-效果权衡。因此，需要一种更通用、更准确的评估方法。

Method: 本文提出了E²R-FLOPs指标，包括每PetaFLOP的相关性排名（RPP）和每PetaFLOP的查询数（QPP），并构建了一个可解释的FLOPs估算器来估计LLM-based rerankers的FLOPs。

Result: 通过广泛的实验，本文评估了不同架构的LLM-based rerankers，并揭示了效率-效果权衡问题。

Conclusion: 本文提出了一种新的评估LLM-based rerankers的效率-效果权衡的方法，即E²R-FLOPs，并通过实验验证了其有效性。

Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in
information retrieval, achieving strong performance. However, their high
computational demands often hinder practical deployment. Existing studies
evaluate the efficiency of LLM-based rerankers using proxy metrics such as
latency, the number of forward passes, input tokens, and output tokens.
However, these metrics depend on hardware and running-time choices (\eg
parallel or not, batch size, etc), and often fail to account for model size,
making it difficult to interpret and obscuring the evaluation of the
efficiency-effectiveness tradeoff. To address this issue, we propose
E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per
PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for
hardware-agnostic throughput. Companied with the new metrics, an interpretable
FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even
without running any experiments. Based on the proposed metrics, we conduct
comprehensive experiments to evaluate a wide range of LLM-based rerankers with
different architecture, studying the efficiency-effectiveness trade-off and
bringing this issue to the attention of the research community.

</details>


### [57] [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
*Xiangru Tang,Tianrui Qin,Tianhao Peng,Ziyang Zhou,Daniel Shao,Tingting Du,Xinming Wei,Peng Xia,Fang Wu,He Zhu,Ge Zhang,Jiaheng Liu,Xingyao Wang,Sirui Hong,Chenglin Wu,Hao Cheng,Chi Wang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: Agent KB is a hierarchical experience framework that enables complex agentic problem solving through a novel Reason-Retrieve-Refine pipeline, improving success rates in various tasks.


<details>
  <summary>Details</summary>
Motivation: Language agents struggle with effective error correction and experience reuse across domains. Agents traditionally cannot learn from each other's experiences.

Method: Agent KB is a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline.

Result: Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%.

Conclusion: Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks.

Abstract: As language agents tackle increasingly complex tasks, they struggle with
effective error correction and experience reuse across domains. We introduce
Agent KB, a hierarchical experience framework that enables complex agentic
problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses
a core limitation: agents traditionally cannot learn from each other's
experiences. By capturing both high-level strategies and detailed execution
logs, Agent KB creates a shared knowledge base that enables cross-agent
knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success
rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3
improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on
intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to
improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a
modular, framework-agnostic infrastructure for enabling agents to learn from
past experiences and generalize successful strategies to new tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [58] [Evaluation of Habitat Robotics using Large Language Models](https://arxiv.org/abs/2507.06157)
*William Li,Lei Hamilton,Kaise Al-natour,Sanjeev Mohindra*

Main category: cs.RO

TL;DR: 本研究评估了大型语言模型在具身机器人任务中的有效性，发现推理模型在各种配置下表现更优，为具身机器人开发提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估大型语言模型在具身机器人任务中的有效性，并探索其在不同配置下的表现。

Method: 本研究使用Meta PARTNER基准评估大型语言模型在解决具身机器人任务中的有效性，通过在随机化室内厨房场景中进行实验，分析不同模型的表现。

Result: 结果表明，像OpenAI o3-mini这样的推理模型在Meta PARTNER的具身机器人环境中表现优于非推理模型如OpenAI GPT-4o和Llama 3。

Conclusion: 该研究为具身机器人开发提供了一个有前景的研究方向，表明推理模型在具身机器人环境中表现优于非推理模型。

Abstract: This paper focuses on evaluating the effectiveness of Large Language Models
at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR
provides simplified environments and robotic interactions within randomized
indoor kitchen scenes. Each randomized kitchen scene is given a task where two
robotic agents cooperatively work together to solve the task. We evaluated
multiple frontier models on Meta PARTNER environments. Our results indicate
that reasoning models like OpenAI o3-mini outperform non-reasoning models like
OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied
environments. o3-mini displayed outperform across centralized, decentralized,
full observability, and partial observability configurations. This provides a
promising avenue of research for embodied robotic development.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: 本文介绍了一种工具，用于提高大型语言模型在代码开发和回答复杂问题方面的能力，通过引入外部知识来减少幻觉并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在提高大型语言模型在代码开发和回答复杂问题方面的能力，特别是在Reservoir Computing领域。

Method: 通过检索增强生成（RAG）和知识图谱引入外部知识，以减少幻觉并提高生成响应的准确性。

Result: 系统提供了类似ChatGPT的交互体验，专门针对ReservoirPy，使用户能够编写、调试和理解Python代码，并获取可靠的领域特定见解。

Conclusion: 我们的模型在编码任务上表现优于ChatGPT-4o和NotebookLM，并且相比其基础模型Codestral-22B有显著提升。

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [60] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: 本文提出了一种名为CorePipe的自动化管道和一个名为CoreCodeBench的可配置多场景仓库级基准，用于评估大型语言模型在真实工程项目中的适用性。


<details>
  <summary>Details</summary>
Motivation: Existing repository-level benchmarks primarily focus on single scenarios, such as code generation or bug fixing, without adequately capturing the diversity and complexity of real-world software or project engineering workflows. Furthermore, these benchmarks suffer from limited controllability in question positioning and reliability issues in their generated test cases.

Method: We present CorePipe, a fully automated pipeline that converts repositories into comprehensive test cases, and introduce CoreCodeBench, a configurable multi-scenario repository-level benchmark.

Result: Experiments with 16 LLMs across diverse scenarios reveal varying capabilities and offer multi-dimensional insights into LLM performance in engineering contexts.

Conclusion: CoreCodeBench provides a comprehensive and extensive repository-level benchmark to investigate the applicability of LLMs in real-world engineering projects.

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai,Haohan Zhao,Rong Feng,Changyi Ma,Wenzhuo Liu,Hongbo Zhao,Xi Lin,Dong Yi,Min Xie,Qingfu Zhang,Hongbin Liu,Gaofeng Meng,Fei Zhu*

Main category: cs.LG

TL;DR: 本文比较了监督微调和强化学习微调在持续后训练中的表现，发现RFT在保持先验知识和提升模型通用能力方面优于SFT，并提出了一种基于滚动的实例过滤算法来提高RFT的稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在数据回放、模型扩展或参数正则化等方法上，而CPT中的学习范式的基本作用尚未得到充分探索。本文旨在比较SFT和RFT在知识保留方面的效果。

Method: 本文对监督微调（SFT）和强化学习微调（RFT）两种核心后训练范式进行了比较分析，并提出了基于滚动的实例过滤算法以提高RFT的稳定性和效率。

Result: 实验结果显示，SFT在连续学习下游任务时会导致先前任务的灾难性遗忘，而RFT能够自然保留先验知识并达到与多任务训练相当的性能。此外，RFT成功保护甚至增强了模型在标准基准上的通用知识，而SFT严重损害了模型的通用能力。

Conclusion: 本文全面研究了RFT作为持续后训练的稳健范式的优越性。

Abstract: Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [62] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong,Xuandong Zhao,Aneesh Pappu,Dawn Song*

Main category: cs.LG

TL;DR: 本文综述了大型语言模型的记忆现象，探讨了其影响因素、检测方法、缓解策略及其法律和伦理影响，并指出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决大型语言模型在训练数据记忆方面的行为问题，以及由此引发的隐私风险和学习与记忆之间的界限问题。

Method: 本文综合了近期的研究，探讨了记忆现象的影响因素、检测和缓解方法，以及相关的法律和伦理问题。

Result: 本文分析了影响记忆的关键因素，评估了检测和测量记忆内容的方法，并讨论了缓解策略，同时指出了在最小化有害记忆与保持模型效用之间平衡的开放挑战。

Conclusion: 本文提供了对大型语言模型记忆现象的全面概述，并指出了未来研究的关键方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [63] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li,Zefan Wang,Ye He,Yuxuan Li,Qi Shi,Jianling Li,Yonggang Hu,Wanxiang Che,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: AutoTriton是第一个基于强化学习的Triton编程模型，能够自动生成高性能内核，提升了AI系统的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习内核开发需要在硬件上优化计算单元，同时平衡内存管理、并行性和硬件特定优化，但手动调整关键参数如块大小和内存访问模式存在较大障碍。

Method: AutoTriton通过监督微调（SFT）和基于规则的奖励与执行奖励结合的强化学习（RL）算法进行优化，以提升Triton编程能力。

Result: AutoTriton在TritonBench和KernelBench的五个评估通道中表现出与主流大型模型相当的性能，例如Claude-4-Sonnet和DeepSeek-R1-0528。

Conclusion: AutoTriton展示了强化学习在自动生成高性能内核方面的潜力，为构建更高效的AI系统奠定了重要基础。

Abstract: Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [64] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi,Wenhao Yu,Zaitang Li,Yonglin Wang,Hongming Zhang,Ninghao Liu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 本文提出了一种名为MobileGUI-RL的可扩展框架，用于在在线环境中训练GUI代理，通过自我探索和过滤合成课程任务，并适应GRPO以平衡任务成功和执行效率。实验结果表明该方法在三个在线移动代理基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数方法在离线环境中使用预收集的轨迹训练GUI代理，这限制了可扩展性，导致对特定UI模板的过拟合，并在面对未见过的环境时导致脆弱的策略。

Method: MobileGUI-RL，一个在在线环境中训练GUI代理的可扩展框架，包含两个关键组件：通过自我探索和过滤合成可学习任务的课程，以及通过轨迹感知优势和复合奖励适应GRPO以平衡任务成功和执行效率。

Result: 在三个在线移动代理基准测试中，我们的方法表现出了持续的提升。

Conclusion: 我们的方法在三个在线移动代理基准测试中表现出一致的提升，验证了其有效性。

Abstract: Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [65] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider,Itamar Zimerman,Eliya Nachmani*

Main category: cs.LG

TL;DR: 本文研究了如何将Transformer的微分设计应用到Mamba中，并提出了一种新的微分机制，有效解决了Mamba中的注意力过度分配问题。


<details>
  <summary>Details</summary>
Motivation: 序列模型如Transformer和RNNs经常过度关注不相关的内容，导致中间表示噪声大，影响LLM的能力。而Mamba虽然效率高，但同样存在这个问题。

Method: 本文探索了适用于Transformer的微分设计是否可以应用于Mamba架构，并提出了针对Mamba的新微分机制。

Result: 实验表明，本文提出的微分机制在语言建模任务中表现出色，提升了Mamba的检索能力和性能。

Conclusion: 本文通过引入一种新的微分机制，有效缓解了Mamba模型中过度分配注意力的问题，并在语言建模基准上验证了其优越性。

Abstract: Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [66] [SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads](https://arxiv.org/abs/2507.06192)
*Jiale Lao,Immanuel Trummer*

Main category: cs.DB

TL;DR: SQLBarber 是一个基于大型语言模型的系统，用于生成定制化和现实的 SQL 工作负载，相比现有方法在生成时间和与目标成本分布的对齐度上有显著改进。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，获取真实世界的 SQL 查询很困难，而现有的 SQL 生成方法在自定义和满足现实约束方面存在局限。因此，需要一种更有效的 SQL 生成方法。

Method: SQLBarber 使用了声明式接口、LLM 驱动的管道以及贝叶斯优化器来生成 SQL 模板，并利用 Amazon Redshift 和 Snowflake 的执行统计信息来推导 SQL 模板规范和查询成本分布。

Result: SQLBarber 在生成时间上减少了 1 到 3 个数量级，并显著提高了与目标成本分布的对齐度。它是唯一能够生成定制化 SQL 模板的系统。

Conclusion: SQLBarber 是一个基于大型语言模型的系统，可以生成定制化和现实的 SQL 工作负载。它在生成时间、与目标成本分布的对齐度等方面优于现有方法。

Abstract: Database research and development often require a large number of SQL queries
for benchmarking purposes. However, acquiring real-world SQL queries is
challenging due to privacy concerns, and existing SQL generation methods are
limited in customization and in satisfying realistic constraints. To address
this issue, we present SQLBarber, a system based on Large Language Models
(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)
eliminates the need for users to manually craft SQL templates in advance, while
providing the flexibility to accept natural language specifications to
constrain SQL templates, (ii) scales efficiently to generate large volumes of
queries matching any user-defined cost distribution (e.g., cardinality and
execution plan cost), and (iii) uses execution statistics from Amazon Redshift
and Snowflake to derive SQL template specifications and query cost
distributions that reflect real-world query characteristics. SQLBarber
introduces (i) a declarative interface for users to effortlessly generate
customized SQL templates, (ii) an LLM-powered pipeline augmented with a
self-correction module that profiles, refines, and prunes SQL templates based
on query costs, and (iii) a Bayesian Optimizer to efficiently explore different
predicate values and identify a set of queries that satisfy the target cost
distribution. We construct and open-source ten benchmarks of varying difficulty
levels and target query cost distributions based on real-world statistics from
Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show
that SQLBarber is the only system that can generate customized SQL templates.
It reduces query generation time by one to three orders of magnitude, and
significantly improves alignment with the target cost distribution, compared
with existing methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models](https://arxiv.org/abs/2507.05288)
*Shuliang Liu,Hongyi Liu,Aiwei Liu,Bingchen Duan,Qi Zheng,Yibo Yan,He Geng,Peijie Jiang,Jia Liu,Xuming Hu*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The widespread deployment of large language models (LLMs) across critical
domains has amplified the societal risks posed by algorithmically generated
misinformation. Unlike traditional false content, LLM-generated misinformation
can be self-reinforcing, highly plausible, and capable of rapid propagation
across multiple languages, which traditional detection methods fail to mitigate
effectively. This paper introduces a proactive defense paradigm, shifting from
passive post hoc detection to anticipatory mitigation strategies. We propose a
Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of
training and deployed data; (2) Inference Reliability, embedding
self-corrective mechanisms during reasoning; and (3) Input Robustness,
enhancing the resilience of model interfaces against adversarial attacks.
Through a comprehensive survey of existing techniques and a comparative
meta-analysis, we demonstrate that proactive defense strategies offer up to
63\% improvement over conventional methods in misinformation prevention,
despite non-trivial computational overhead and generalization challenges. We
argue that future research should focus on co-designing robust knowledge
foundations, reasoning certification, and attack-resistant interfaces to ensure
LLMs can effectively counter misinformation across varied domains.

</details>


### [68] [News Source Citing Patterns in AI Search Systems](https://arxiv.org/abs/2507.05301)
*Kai-Cheng Yang*

Main category: cs.IR

TL;DR: 研究分析了AI搜索系统在引用新闻源方面的行为，发现它们引用的新闻源集中且偏向自由派，但低可信度的来源较少被引用，同时用户满意度不受引用新闻源的政治倾向或质量的影响。


<details>
  <summary>Details</summary>
Motivation: 由于AI搜索系统在信息获取中的影响力日益增强，但其引用模式仍不明确，因此需要进行研究以填补这一空白。

Method: 通过分析AI Search Arena的数据，包括来自三大供应商（OpenAI、Perplexity和Google）的24,000多条对话和65,000多条响应，以及其中嵌入的366,000多个引用。

Result: 模型引用不同的新闻来源，但在引用行为上表现出共同的模式。新闻引用集中在少数几家媒体，并显示出明显的自由派偏见，但低可信度的来源很少被引用。用户偏好分析表明，引用新闻来源的政治倾向或质量对用户满意度没有显著影响。

Conclusion: 这些发现揭示了当前AI搜索系统的重要挑战，并对其设计和治理具有重要意义。

Abstract: AI-powered search systems are emerging as new information gatekeepers,
fundamentally transforming how users access news and information. Despite their
growing influence, the citation patterns of these systems remain poorly
understood. We address this gap by analyzing data from the AI Search Arena, a
head-to-head evaluation platform for AI search systems. The dataset comprises
over 24,000 conversations and 65,000 responses from models across three major
providers: OpenAI, Perplexity, and Google. Among the over 366,000 citations
embedded in these responses, 9% reference news sources. We find that while
models from different providers cite distinct news sources, they exhibit shared
patterns in citation behavior. News citations concentrate heavily among a small
number of outlets and display a pronounced liberal bias, though low-credibility
sources are rarely cited. User preference analysis reveals that neither the
political leaning nor the quality of cited news sources significantly
influences user satisfaction. These findings reveal significant challenges in
current AI search systems and have important implications for their design and
governance.

</details>


### [69] [Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA](https://arxiv.org/abs/2507.05577)
*Shashank Verma,Fengyi Jiang,Xiangning Xue*

Main category: cs.IR

TL;DR: 本文介绍了我们在BioASQ 2025 Task13b挑战中的方法和结果，构建了一个基于检索增强生成（RAG）的系统，通过检索相关的PubMed文档和片段来生成答案。我们的系统在检索任务中获得了MAP@10为0.1581，排名第十。在答案生成方面，我们的系统在yes/no问题上获得了0.95的宏F1分数（排名第12），在事实性问题上获得了0.64的平均倒数排名（MRR）（排名第1），在列表问题上获得了0.63的平均F1分数（排名第5），在理想答案上获得了0.29的ROUGE-SU4 F1分数（排名第11）。


<details>
  <summary>Details</summary>
Motivation: 生物医学语义问答在信息检索基础上可以发挥重要作用，帮助研究人员、医疗专业人员和普通用户获取基于证据的相关知识。BioASQ 2025 Task13b挑战为这一领域的发展提供了一个重要的基准平台。

Method: 我们构建了一个基于检索增强生成（RAG）的系统，通过检索相关的PubMed文档和片段来生成答案。在检索任务中，我们使用密集嵌入进行初始检索，并应用了微调的交叉编码器和大型语言模型（LLMs）进行重新排序。在答案生成方面，我们采用了指令调优的LLMs的少样本提示。

Result: 我们的系统在检索任务中获得了MAP@10为0.1581，排名第十。在答案生成方面，我们的系统在yes/no问题上获得了0.95的宏F1分数（排名第12），在事实性问题上获得了0.64的平均倒数排名（MRR）（排名第1），在列表问题上获得了0.63的平均F1分数（排名第5），在理想答案上获得了0.29的ROUGE-SU4 F1分数（排名第11）。

Conclusion: 我们的系统在BioASQ 2025 Task13b挑战中取得了不错的成绩，特别是在事实性问题和列表问题上表现优异。

Abstract: Biomedical semantic question answering rooted in information retrieval can
play a crucial role in keeping up to date with vast, rapidly evolving and
ever-growing biomedical literature. A robust system can help researchers,
healthcare professionals and even layman users access relevant knowledge
grounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important
benchmark, offering a competitive platform for advancement of this space. This
paper presents the methodologies and results from our participation in this
challenge where we built a Retrieval-Augmented Generation (RAG) system that can
answer biomedical questions by retrieving relevant PubMed documents and
snippets to generate answers. For the retrieval task, we generated dense
embeddings from biomedical articles for initial retrieval, and applied an
ensemble of finetuned cross-encoders and large language models (LLMs) for
re-ranking to identify top relevant documents. Our solution achieved an MAP@10
of 0.1581, placing 10th on the leaderboard for the retrieval task. For answer
generation, we employed few-shot prompting of instruction-tuned LLMs. Our
system achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean
Reciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of
0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal
answers (rank 11).

</details>


### [70] [Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation](https://arxiv.org/abs/2507.05933)
*Y. Du*

Main category: cs.IR

TL;DR: 本文提出了一种轻量级框架，用于预测查询级别的检索性能，通过结合量化鲁棒性和邻域密度度量，实现了在多个数据集上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 我们观察到高质量的嵌入占据嵌入空间中几何稳定的区域并表现出一致的邻域结构，这促使我们提出了这种方法。

Method: 我们提出了一种轻量级框架，通过结合量化鲁棒性和邻域密度度量来预测查询级别的检索性能。

Result: 我们在4个标准检索数据集上评估了我们的方法，在Recall@10上相对于竞争基线取得了9.4±1.2%的一致改进。

Conclusion: 我们的分析揭示了不同查询类型中嵌入质量的系统模式，为有针对性的数据增强提供了见解。

Abstract: Vector retrieval systems exhibit significant performance variance across
queries due to heterogeneous embedding quality. We propose a lightweight
framework for predicting retrieval performance at the query level by combining
quantization robustness and neighborhood density metrics. Our approach is
motivated by the observation that high-quality embeddings occupy geometrically
stable regions in the embedding space and exhibit consistent neighborhood
structures. We evaluate our method on 4 standard retrieval datasets, showing
consistent improvements of 9.4$\pm$1.2\% in Recall@10 over competitive
baselines. The framework requires minimal computational overhead (less than 5\%
of retrieval time) and enables adaptive retrieval strategies. Our analysis
reveals systematic patterns in embedding quality across different query types,
providing insights for targeted training data augmentation.

</details>


### [71] [Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India](https://arxiv.org/abs/2507.06090)
*Swapnil Bhattacharyya,Shrey Ganatra,Harshvivek Kashid,Spandan Anaokar,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.IR

TL;DR: 本文介绍了Nyay-Darpan，一个用于消费者案件文件总结和相似案件判决检索的框架，旨在填补消费者法AI工具的空白，并通过高准确率展示其有效性。


<details>
  <summary>Details</summary>
Motivation: AI驱动的司法辅助和案件预测在刑事和民事领域得到了广泛研究，但在消费者法领域，尤其是在印度，仍鲜有探索。

Method: 我们提出了Nyay-Darpan，这是一个创新的双功能框架，(i)总结消费者案件文件，(ii)检索相似的案件判决以辅助消费者纠纷解决中的决策。我们的方法不仅填补了消费者法AI工具的空白，还引入了一种创新的方法来评估摘要的质量。

Result: 我们的系统在相似案件预测中达到了75%以上的准确率，并在材料摘要评估指标中达到了约70%的准确率，展示了其实际有效性。

Conclusion: 我们的系统在相似案件预测中达到了75%以上的准确率，并在材料摘要评估指标中达到了约70%的准确率，展示了其实际有效性。我们将公开发布Nyay-Darpan框架和数据集，以促进可重复性和推动这一未被充分研究但具有影响力的领域进一步研究。

Abstract: AI-based judicial assistance and case prediction have been extensively
studied in criminal and civil domains, but remain largely unexplored in
consumer law, especially in India. In this paper, we present Nyay-Darpan, a
novel two-in-one framework that (i) summarizes consumer case files and (ii)
retrieves similar case judgements to aid decision-making in consumer dispute
resolution. Our methodology not only addresses the gap in consumer law AI tools
but also introduces an innovative approach to evaluate the quality of the
summary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',
symbolizing the ability of our tool to reflect the core of consumer disputes
through precise summarization and intelligent case retrieval. Our system
achieves over 75 percent accuracy in similar case prediction and approximately
70 percent accuracy across material summary evaluation metrics, demonstrating
its practical effectiveness. We will publicly release the Nyay-Darpan framework
and dataset to promote reproducibility and facilitate further research in this
underexplored yet impactful domain.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [72] [Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools](https://arxiv.org/abs/2507.05305)
*Lorenzo Lee Solano,Charles Koutcheme,Juho Leinonen,Alexandra Vassar,Jake Renzella*

Main category: cs.CY

TL;DR: 本文研究了通过微调小型语言模型来解决编程编译器错误的可行性，并展示了其在教育中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前的前沿大型语言模型虽然能帮助初学者理解编译器错误，但其计算规模、成本和过度辅助的问题限制了其在教育领域的广泛应用。

Method: 本文使用了一个包含40,000个C编译器错误解释的新数据集，对三个开源模型进行了微调，并进行了专家人类评审和大规模自动化分析。

Result: 实验结果表明，微调显著提升了小型模型的教育质量，使其性能达到与大型模型相当的水平。

Conclusion: 本文证明了通过监督微调的小型专用语言模型可以作为教育工具的可行替代方案，且在性能上可与大型模型相媲美。

Abstract: Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher
cryptic compiler errors for novice programmers, but their computational scale,
cost, and tendency to over-assist make them problematic for widespread
pedagogical adoption. This work demonstrates that smaller, specialised language
models, enhanced via Supervised Fine-Tuning (SFT), present a more viable
alternative for educational tools. We utilise a new dataset of 40,000 C
compiler error explanations, derived from real introductory programming (CS1/2)
student-generated programming errors, which we used to fine-tune three
open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual
evaluation, combining expert human reviews with a large-scale automated
analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our
results show that SFT significantly boosts the pedagogical quality of smaller
models, achieving performance comparable to much larger models. We analyse the
trade-offs between model size and quality, confirming that fine-tuning compact,
efficient models on high-quality, domain-specific data is a potent strategy for
creating specialised models to drive educational tools. We provide a replicable
methodology to foster broader access to generative AI capabilities in
educational contexts.

</details>


### [73] [Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review](https://arxiv.org/abs/2507.06185)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 本文分析了在学术论文中发现的隐藏指令问题，并强调了需要统一管理生成式AI在学术评估中的使用政策。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨隐藏指令在学术同行评审中的潜在影响，并识别系统性漏洞。

Method: 本文通过分析隐藏指令在大型语言模型中的注入技术，揭示了四种类型的隐藏提示，并对出版商的政策进行了比较。

Result: 本文发现隐藏指令可以操纵AI辅助的同行评审，并指出出版商在政策上的不一致。

Conclusion: 本文分析了在学术论文预印本网站arXiv上发现的隐藏指令问题，并强调了需要在提交门户进行协调的技术筛查以及统一管理生成式AI在学术评估中的使用政策。

Abstract: In July 2025, 18 academic manuscripts on the preprint website arXiv were
found to contain hidden instructions known as prompts designed to manipulate
AI-assisted peer review. Instructions such as "GIVE A POSITIVE REVIEW ONLY"
were concealed using techniques like white-colored text. Author responses
varied: one planned to withdraw the affected paper, while another defended the
practice as legitimate testing of reviewer compliance. This commentary analyzes
this practice as a novel form of research misconduct. We examine the technique
of prompt injection in large language models (LLMs), revealing four types of
hidden prompts, ranging from simple positive review commands to detailed
evaluation frameworks. The defense that prompts served as "honeypots" to detect
reviewers improperly using AI fails under examination--the consistently
self-serving nature of prompt instructions indicates intent to manipulate.
Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer
review entirely, while Springer Nature permits limited use with disclosure
requirements. The incident exposes systematic vulnerabilities extending beyond
peer review to any automated system processing scholarly texts, including
plagiarism detection and citation indexing. Our analysis underscores the need
for coordinated technical screening at submission portals and harmonized
policies governing generative AI (GenAI) use in academic evaluation.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [74] [AI-Reporter: A Path to a New Genre of Scientific Communication](https://arxiv.org/abs/2507.05903)
*Gerd Graßhoff*

Main category: cs.DL

TL;DR: AI-Reporter 通过技术手段将学术演示快速转化为科学出版物。


<details>
  <summary>Details</summary>
Motivation: 旨在展示技术创新如何弥合短暂演示与永久科学文献之间的差距。

Method: 通过一个具体的案例研究，展示了系统如何将学术演示转化为出版准备好的章节。

Result: 系统能够在不到三分钟的时间内将学术演示转化为出版准备好的章节。

Conclusion: AI-Reporter 是科学出版实践的范式转变，展示了如何将学术演示转化为出版准备好的章节。

Abstract: The AI-Reporter represents a paradigmatic shift in scientific publication
practice. This document demonstrates through a concrete case study how our
system transforms academic presentations into publication-ready chapters -- in
less than three minutes. Using Arno Simons' lecture on Large Language Models
from the ``Large Language Models for the History, Philosophy, and Sociology of
Science'' workshop (NEPI) as an example, we show how technological innovation
bridges the gap between ephemeral presentation and permanent scientific
documentation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data](https://arxiv.org/abs/2507.05660)
*Aravind Cheruvu,Shravya Kanchi,Sifat Muhammad Abdullah,Nicholas Kong,Daphne Yao,Murtuza Jadliwala,Bimal Viswanath*

Main category: cs.CR

TL;DR: TuneShield is a defense framework that mitigates toxicity during chatbot fine-tuning while preserving conversational quality. It uses LLM-based toxicity classification and generates synthetic conversation samples to reinforce desirable behavior.


<details>
  <summary>Details</summary>
Motivation: Mitigating toxicity during chatbot customization, especially when dealing with untrusted training data, remains a significant challenge. Existing solutions may not be effective against adaptive adversarial and jailbreak attacks.

Method: TuneShield leverages LLM-based toxicity classification to identify toxic samples and generates synthetic conversation samples, termed 'healing data,' to mitigate toxicity while reinforcing desirable behavior during fine-tuning. It also performs an alignment process to nudge the chatbot towards producing desired responses.

Result: TuneShield outperforms industry API services in identifying toxic samples. It effectively mitigates toxicity injection attacks while preserving conversational quality, even with imperfect or biased toxicity classifiers. It is resilient against adaptive adversarial and jailbreak attacks and shows effectiveness in mitigating adaptive toxicity injection attacks during dialog-based learning.

Conclusion: TuneShield effectively mitigates toxicity injection attacks while preserving conversational quality, even when the toxicity classifiers are imperfect or biased. It is resilient against adaptive adversarial and jailbreak attacks and demonstrates effectiveness in mitigating adaptive toxicity injection attacks during dialog-based learning.

Abstract: Recent advances in foundation models, such as LLMs, have revolutionized
conversational AI. Chatbots are increasingly being developed by customizing
LLMs on specific conversational datasets. However, mitigating toxicity during
this customization, especially when dealing with untrusted training data,
remains a significant challenge. To address this, we introduce TuneShield, a
defense framework designed to mitigate toxicity during chatbot fine-tuning
while preserving conversational quality. TuneShield leverages LLM-based
toxicity classification, utilizing the instruction-following capabilities and
safety alignment of LLMs to effectively identify toxic samples, outperforming
industry API services. TuneShield generates synthetic conversation samples,
termed 'healing data', based on the identified toxic samples, using them to
mitigate toxicity while reinforcing desirable behavior during fine-tuning. It
performs an alignment process to further nudge the chatbot towards producing
desired responses. Our findings show that TuneShield effectively mitigates
toxicity injection attacks while preserving conversational quality, even when
the toxicity classifiers are imperfect or biased. TuneShield proves to be
resilient against adaptive adversarial and jailbreak attacks. Additionally,
TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection
attacks during dialog-based learning (DBL).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [76] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: 本研究提出通过在训练中使用结构化标题来提高生成式文本到图像模型的可控性和对齐度，并引入了一个高质量的数据集Re-LAION-Caption 19M。


<details>
  <summary>Details</summary>
Motivation: 生成式文本到图像模型常常因大规模数据集（如LAION-5B）的噪声和非结构化特性而难以遵循提示，这迫使用户依赖提示工程来获得期望的输出。

Method: 引入了Re-LAION-Caption 19M数据集，该数据集包含1900万张遵循四部分模板（主体、环境、美学和相机细节）的图像，并使用结构化和随机打乱的标题对PixArt-Σ和Stable Diffusion 2进行了微调。

Result: 结构化的标题版本在使用视觉问答（VQA）模型时表现出更高的文本-图像对齐分数。

Conclusion: 通过在训练中强制执行一致的标题结构，可以显著提高模型的可控性和对齐度。

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [77] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: 本文提出了一种数据整理流程，构建了一个合成文化数据集CulTwin，并在该数据集上微调CLIP以创建CultureCLIP，从而更好地区分文化差异。实验结果表明，CultureCLIP在细粒度概念识别任务中表现优于基础CLIP。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型（如CLIP）在多模态理解方面表现出色，但在区分视觉相似但文化不同的概念时存在困难。这种限制源于高质量文化特定数据集的稀缺、缺乏集成的上下文知识以及缺少强调细微差别的硬负样本。

Method: 首先设计了一个数据整理流程，利用开源的VLM和文本到图像扩散模型构建CulTwin，一个合成文化数据集。然后在CulTwin上微调CLIP以创建CultureCLIP，通过定制的对比学习将文化概念与上下文增强的标题和合成图像对齐，从而实现更精细的文化区分，同时保持泛化能力。

Result: CultureCLIP在文化相关的基准测试中表现优于基础CLIP，在某些任务中细粒度概念识别提升了显著的5.49%，同时保留了CLIP原有的泛化能力。

Conclusion: 实验表明，CultureCLIP在文化相关基准测试中优于基础CLIP，某些任务的细粒度概念识别提升了显著的5.49%，同时保留了CLIP原有的泛化能力，验证了我们的数据合成和VLM骨干训练范式在捕捉细微文化差异方面的有效性。

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
*Yue Wang,Miao Zhou,Guijing Huang,Rui Zhuo,Chao Yi,Zhenliang Ma*

Main category: cs.AI

TL;DR: Chat2SPaT is a method that converts users' semi-structured and ambiguous descriptions on the signal control plan to exact signal phase and timing (SPaT) results, which could be transformed into structured stage-based or ring-based plans to interact with intelligent transportation system (ITS) software and traffic signal controllers.


<details>
  <summary>Details</summary>
Motivation: Pre-timed traffic signal control requires tedious manual work for signaling plan creating and updating. When the time-of-day or day-of-week plans are utilized, one intersection is often associated with multiple plans, leading to further repetitive manual plan parameter inputting.

Method: Chat2SPaT leverages large language models' (LLMs) capability of understanding users' plan descriptions and reformulates the plan as a combination of phase sequence and phase attribute results in the json format. Python scripts are designed to locate phases in a cycle, address nuances of traffic signal control, and assemble the complete traffic signal control plan.

Result: Experiments show that Chat2SPaT can generate plans with an accuracy of over 94% for both English and Chinese cases, using a test dataset with over 300 plan descriptions. It is the first benchmark for evaluating LLMs' capability of understanding traffic signal control plan descriptions.

Conclusion: Chat2SPaT provides an easy-to-use plan management pipeline for traffic practitioners and researchers, serving as a potential new building block for a more accurate and versatile application of LLMs in the field of ITS.

Abstract: Pre-timed traffic signal control, commonly used for operating signalized
intersections and coordinated arterials, requires tedious manual work for
signaling plan creating and updating. When the time-of-day or day-of-week plans
are utilized, one intersection is often associated with multiple plans, leading
to further repetitive manual plan parameter inputting. To enable a
user-friendly traffic signal control plan management process, this study
proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous
descriptions on the signal control plan to exact signal phase and timing (SPaT)
results, which could further be transformed into structured stage-based or
ring-based plans to interact with intelligent transportation system (ITS)
software and traffic signal controllers. With curated prompts, Chat2SPaT first
leverages large language models' (LLMs) capability of understanding users' plan
descriptions and reformulate the plan as a combination of phase sequence and
phase attribute results in the json format. Based on LLM outputs, python
scripts are designed to locate phases in a cycle, address nuances of traffic
signal control, and finally assemble the complete traffic signal control plan.
Within a chat, the pipeline can be utilized iteratively to conduct further plan
editing. Experiments show that Chat2SPaT can generate plans with an accuracy of
over 94% for both English and Chinese cases, using a test dataset with over 300
plan descriptions. As the first benchmark for evaluating LLMs' capability of
understanding traffic signal control plan descriptions, Chat2SPaT provides an
easy-to-use plan management pipeline for traffic practitioners and researchers,
serving as a potential new building block for a more accurate and versatile
application of LLMs in the field of ITS. The source codes, prompts and test
dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.

</details>


### [79] [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
*Haochen Huang,Jiahuan Pei,Mohammad Aliannejadi,Xin Sun,Moonisa Ahsan,Pablo Cesar,Chuang Yu,Zhaochun Ren,Junxiao Wang*

Main category: cs.AI

TL;DR: 本文介绍了一个用于AR训练的综合性数据集，并评估了九种最先进的视觉-语言模型。结果显示，这些模型在细粒度任务中表现不佳，强调了需要改进的数据集、基准测试和进一步研究以提高细粒度视觉-语言对齐。此外，这项工作还具有社会影响，旨在为视障和视力受损用户提供平等的AI驱动学习机会。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）对于使AI驱动的智能助手在多模态环境中进行解释和推理至关重要，但它们在增强现实（AR）培训中的应用仍鲜有探索。因此，我们需要一个专门的数据集来推动这一领域的发展。

Method: 我们引入了一个针对AR训练的综合性数据集，包含系统化的视觉-语言任务，并在该数据集上评估了九种最先进的VLMs。

Result: 我们的结果表明，即使是先进的模型，如GPT-4o，在细粒度装配任务中也表现不佳，仅在状态检测中达到40.54%的最大F1分数。

Conclusion: 我们的研究揭示了即使先进的模型在细粒度装配任务中也存在困难，这表明需要改进的数据集、基准测试和进一步的研究来提高细粒度视觉-语言对齐。此外，我们的工作在社会层面具有更广泛的影响，特别是在为视障和视力受损用户提供平等的AI驱动学习机会方面。

Abstract: Vision-language models (VLMs) are essential for enabling AI-powered smart
assistants to interpret and reason in multimodal environments. However, their
application in augmented reality (AR) training remains largely unexplored. In
this work, we introduce a comprehensive dataset tailored for AR training,
featuring systematized vision-language tasks, and evaluate nine
state-of-the-art VLMs on it. Our results reveal that even advanced models,
including GPT-4o, struggle with fine-grained assembly tasks, achieving a
maximum F1 score of just 40.54% on state detection. These findings highlight
the demand for enhanced datasets, benchmarks, and further research to improve
fine-grained vision-language alignment. Beyond technical contributions, our
work has broader social implications, particularly in empowering blind and
visually impaired users with equitable access to AI-driven learning
opportunities. We provide all related resources, including the dataset, source
code, and evaluation results, to support the research community.

</details>


### [80] [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
*Jiahuan Pei,Fanghua Ye,Xin Sun,Wentao Deng,Koen Hindriks,Junxiao Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为WikiHowAgent的多智能体工作流，用于模拟互动教学-学习对话，并通过一个包含114,296个教师-学习者对话的数据集进行评估。结果显示该工作流在不同设置中有效，提供了关于LLM在各个领域能力的见解。


<details>
  <summary>Details</summary>
Motivation: 现有工作往往缺乏可扩展性，并且未能利用多样化的大型课程内容，缺乏评估教学质量的框架。

Method: 我们提出了WikiHowAgent，这是一个多智能体工作流，利用LLM来模拟互动教学-学习对话。它集成了教师和学习者代理、一个交互管理器和一个评估器，以促进程序性学习并评估教学质量。

Result: 结果表明该工作流在不同设置中有效，提供了关于LLM在各个领域能力的见解。

Conclusion: 我们的工作展示了WikiHowAgent在不同设置中的有效性，并提供了关于LLM在各个领域能力的见解。我们的数据集和实现是完全开源的。

Abstract: Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

</details>


### [81] [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
*Shuai Zhao,Yulin Zhang,Luwei Xiao,Xinyi Wu,Yanhao Jia,Zhongliang Guo,Xiaobao Wu,Cong-Duy Nguyen,Guoming Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 本文介绍了CROP数据集和Affective-ROPTester框架，用于评估大型语言模型在预测早产儿视网膜病变风险时的表现和情感偏差。结果显示，结合外部知识可以提高模型性能，而正面情绪框架有助于减少预测偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在各个领域取得了显著进展，但它们在预测早产儿视网膜病变（ROP）风险方面的能力仍鲜有探索。为了弥补这一差距，我们引入了一个新的中文基准数据集CROP，包含993个带有低、中、高风险标签的入院记录。

Method: 我们提出了Affective-ROPTester，这是一个自动评估框架，结合了三种提示策略：基于指令、思维链（CoT）和上下文学习（ICL）。此外，我们在提示级别集成了情感元素，以研究不同情感框架如何影响模型预测ROP及其偏差模式的能力。

Result: 实证结果表明，当仅依赖内在知识时，LLMs在ROP风险预测方面的效果有限，但当结合结构化的外部输入时，性能显著提升。此外，模型输出中存在情感偏差，倾向于高估中、高风险病例。与负面情绪相比，正面情绪框架有助于减轻模型输出中的预测偏差。

Conclusion: 这些发现突显了情感敏感提示工程在提高诊断可靠性中的关键作用，并强调了Affective-ROPTester作为评估和减轻临床语言建模系统中情感偏差的框架的实用性。

Abstract: Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

</details>


### [82] [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
*Fathinah Izzati,Xinyue Li,Yuxuan Wu,Gus Xia*

Main category: cs.AI

TL;DR: 本文提出了一种新的音乐标题模型MusiScene，能够生成与音乐相配的场景描述，并展示了其在生成上下文相关标题方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的音乐标题模型仅关注音乐元素，而本文旨在通过引入跨模态信息来改进音乐场景想象任务。

Method: 本文构建了一个包含3,371对数据的大规模视频-音频标题数据集，并微调了Music Understanding LLaMA以创建MusiScene模型。

Result: MusiScene在生成上下文相关标题方面比MU-LLaMA更有效。

Conclusion: 本文提出了MusiScene模型，该模型能够生成与音乐相配的场景描述，并证明了其在生成上下文相关标题方面的优越性。此外，利用生成的MSI标题可以增强从文本生成视频背景音乐的能力。

Abstract: Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

</details>


### [83] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: 开发了一个基于大型语言模型的聊天机器人HopeBot，用于实施PHQ-9，并在一项研究中评估其效果。结果显示，该聊天机器人具有良好的用户信任度和满意度，可能作为常规抑郁症筛查的可行辅助工具。


<details>
  <summary>Details</summary>
Motivation: 静态工具如PHQ-9虽然能有效筛查抑郁症，但缺乏互动性和适应性。

Method: 开发了HopeBot，这是一个由大型语言模型（LLM）驱动的聊天机器人，使用检索增强生成和实时澄清来实施PHQ-9。进行了一项受试者内研究，132名英国和中国的成年人完成了自我管理版本和聊天机器人版本。

Result: 得分显示了强烈的协议（ICC = 0.91；45%相同）。75名提供比较反馈的参与者中，71%表示对聊天机器人更有信任感，强调了更清晰的结构、解释性指导和支持性语气。平均评分（0-10）分别为8.4（舒适度）、7.7（语音清晰度）、7.6（处理敏感话题）和7.4（建议有用性）；后者根据就业状况和之前的心理健康服务使用情况显著变化（p < 0.05）。总体而言，87.1%的人表示愿意再次使用或推荐HopeBot。

Conclusion: 这些发现表明，基于语音的LLM聊天机器人可以作为可行的、低负担的辅助工具，用于常规抑郁症筛查。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [84] [ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark](https://arxiv.org/abs/2507.05727)
*He Wang,Linhan Ma,Dake Guo,Xiong Wang,Lei Xie,Jin Xu,Junyang Lin*

Main category: eess.AS

TL;DR: 本文提出了一个名为ContextASR-Bench的基准，用于评估上下文语音识别。该基准涵盖了多个领域的大量数据，并分析了模型在识别命名实体方面的表现。实验结果表明，LALMs在性能上优于传统ASR模型。


<details>
  <summary>Details</summary>
Motivation: 现有的自动语音识别（ASR）评估主要集中在无上下文的范式上，这限制了对模型在上下文建模和基于世界知识的记忆与推理能力的评估。因此，需要一个能评估ASR系统通用性和智能性的基准。

Method: 本文提出了一种新的基准ContextASR-Bench，用于评估上下文语音识别。该基准包括多个领域的大规模数据集，并分析了模型在识别命名实体方面的表现。

Result: 实验结果表明，具有强大世界知识和上下文学习能力的大型音频语言模型（LALMs）在性能上显著优于传统的ASR模型。

Conclusion: 本文提出了一个名为ContextASR-Bench的全面、大规模基准，用于评估上下文语音识别。该基准涵盖了超过10个领域的40,000个数据条目，能够对模型在不同上下文信息情况下的性能进行深入评估。此外，该基准还分析了模型在识别听觉输入中提到的命名实体方面的有效性。实验结果表明，具有强大世界知识和上下文学习能力的LALMs在性能上显著优于传统的ASR模型。

Abstract: Automatic Speech Recognition (ASR) has been extensively investigated, yet
prior evaluative efforts have largely been restricted to contextless paradigms.
This constraint stems from the limited proficiency of conventional ASR models
in context modeling and their deficiency in memory and reasoning based on world
knowledge. Recent breakthroughs in the development of Large Language Models
(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly
enhanced the visibility of general artificial intelligence capabilities.
Consequently, there exists a compelling need for a benchmark that can evaluate
both the generality and intelligence of ASR systems. To address this gap, we
propose ContextASR-Bench: a comprehensive, large-scale benchmark designed to
assess contextual speech recognition. This benchmark encompasses up to 40,000
data entries across over 10 domains, enabling a thorough evaluation of model
performance in scenarios that omit or incorporate coarse-grained or
fine-grained contextual information. Moreover, diverging from conventional ASR
evaluations, our benchmark includes an analysis of model efficacy in
recognizing named entities mentioned within the auditory input. Our extensive
evaluation highlights that LALMs, with strong world knowledge and context
learning capabilities, outperform conventional ASR models by a large margin.
The dataset and evaluation code have been released at
https://github.com/MrSupW/ContextASR-Bench.

</details>

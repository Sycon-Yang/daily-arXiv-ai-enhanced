<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 68]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 9]
- [eess.AS](#eess.AS) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
*Yanis Labrak,Richard Dufour,Mickaël Rouvier*

Main category: cs.CL

TL;DR: 本文研究了语音语言模型中离散单位表示的优化，探讨了模型架构、数据表示和训练鲁棒性的影响，揭示了离散词汇的有效使用及领域匹配的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在优化语音建模在持续预训练中的表现，通过研究离散单位表示来提升语音语言模型的性能。

Method: 本文系统地研究了模型架构、数据表示和训练鲁棒性对持续预训练阶段的影响，同时分析了聚类分布和音素对齐以探索离散词汇的有效使用。

Result: 实验表明，最优离散化策略随着模型容量的变化而变化，并揭示了语言和副语言模式。此外，聚类数据选择对模型鲁棒性有显著影响。

Conclusion: 本文通过研究语音编码器和聚类粒度，揭示了离散词汇在语音建模中的有效使用，并强调了领域匹配在模型鲁棒性中的重要性。

Abstract: This paper investigates discrete unit representations in Speech Language
Models (SLMs), focusing on optimizing speech modeling during continual
pre-training. In this paper, we systematically examine how model architecture,
data representation, and training robustness influence the pre-training stage
in which we adapt existing pre-trained language models to the speech modality.
Our experiments highlight the role of speech encoders and clustering
granularity across different model scales, showing how optimal discretization
strategies vary with model capacity. By examining cluster distribution and
phonemic alignments, we investigate the effective use of discrete vocabulary,
uncovering both linguistic and paralinguistic patterns. Additionally, we
explore the impact of clustering data selection on model robustness,
highlighting the importance of domain matching between discretization training
and target applications.

</details>


### [2] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
*Jerry Li,Evangelos Papalexakis*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，利用N-Gram频率张量和张量分解技术来检测LLM中的幻觉，结果表明该方法在HaluEval数据集上优于传统基线，并且与最先进的LLM法官表现相当。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种涉及自然语言的任务中表现出有效性，但幻觉问题仍然困扰着这些模型，限制了它们在生成一致、真实信息时的可信度。检测幻觉已成为一个重要话题，各种方法如不确定性估计、LLM法官、检索增强生成（RAG）和一致性检查显示出前景。然而，许多这些方法建立在基础指标之上，如ROUGE、BERTScore或困惑度，这些指标往往缺乏足够的语义深度来有效检测幻觉。

Method: 我们提出了一种受ROUGE启发的新方法，从LLM生成的文本中构建N-Gram频率张量。该张量通过编码共现模式来捕捉更丰富的语义结构，从而更好地区分事实内容和幻觉内容。我们应用张量分解方法提取每个模式的奇异值，并将其作为输入特征训练一个多层感知器（MLP）二分类器进行幻觉检测。

Result: 我们的方法在HaluEval数据集上表现出显著优于传统基线的性能，并且与最先进的LLM法官表现相当。

Conclusion: 我们的方法在HaluEval数据集上表现出色，优于传统基线，并且与最先进的LLM法官表现相当。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness across a wide
variety of tasks involving natural language, however, a fundamental problem of
hallucinations still plagues these models, limiting their trustworthiness in
generating consistent, truthful information. Detecting hallucinations has
quickly become an important topic, with various methods such as uncertainty
estimation, LLM Judges, retrieval augmented generation (RAG), and consistency
checks showing promise. Many of these methods build upon foundational metrics,
such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth
necessary to detect hallucinations effectively. In this work, we propose a
novel approach inspired by ROUGE that constructs an N-Gram frequency tensor
from LLM-generated text. This tensor captures richer semantic structure by
encoding co-occurrence patterns, enabling better differentiation between
factual and hallucinated content. We demonstrate this by applying tensor
decomposition methods to extract singular values from each mode and use these
as input features to train a multi-layer perceptron (MLP) binary classifier for
hallucinations. Our method is evaluated on the HaluEval dataset and
demonstrates significant improvements over traditional baselines, as well as
competitive performance against state-of-the-art LLM judges.

</details>


### [3] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
*Jiacheng Wei,Faguo Wu,Xiao Zhang*

Main category: cs.CL

TL;DR: SAGE is a framework that enables large language models to adaptively update their knowledge during reasoning at inference time by decomposing tasks into atomic subtasks and using a trigger-guided approach.


<details>
  <summary>Details</summary>
Motivation: Large language models are unable to continuously adapt and learn from new data during reasoning at inference time.

Method: SAGE is a trigger-guided dynamic fine-tuning framework that decomposes complex reasoning tasks into atomic subtasks. It consists of three key components: a Trigger module, a Trigger Buffer module, and a Lora Store module.

Result: Evaluation results show that SAGE demonstrates excellent accuracy, robustness, and stability on the atomic reasoning subtask through dynamic knowledge updating during test time.

Conclusion: SAGE demonstrates excellent accuracy, robustness, and stability on the atomic reasoning subtask through dynamic knowledge updating during test time.

Abstract: Large language models are unable to continuously adapt and learn from new
data during reasoning at inference time. To address this limitation, we propose
that complex reasoning tasks be decomposed into atomic subtasks and introduce
SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive
updates during reasoning at inference time. SAGE consists of three key
components: (1) a Trigger module that detects reasoning failures through
multiple evaluation metrics in real time; (2) a Trigger Buffer module that
clusters anomaly samples using a streaming clustering process with HDBSCAN,
followed by stability checks and similarity-based merging; and (3) a Lora Store
module that dynamically optimizes parameter updates with an adapter pool for
knowledge retention. Evaluation results show that SAGE demonstrates excellent
accuracy, robustness, and stability on the atomic reasoning subtask through
dynamic knowledge updating during test time.

</details>


### [4] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
*Andrea Wynn,Harsh Satija,Gillian Hadfield*

Main category: cs.CL

TL;DR: 我们的研究发现，多智能体辩论可能有害，因为模型倾向于同意而不是挑战错误的推理，这可能导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 之前的工作主要集中在同质代理群体内的辩论，而我们研究了模型能力的多样性如何影响多智能体互动的动态和结果。

Method: 我们通过一系列实验探索了模型能力多样性对多智能体交互动态和结果的影响。

Result: 我们发现辩论有时会降低准确性，即使在更强的模型数量超过较弱模型的情况下也是如此。

Conclusion: 我们的研究结果表明，当代理既没有被激励也没有充分的能力来抵抗有说服力但错误的推理时，简单应用辩论可能导致性能下降。

Abstract: While multi-agent debate has been proposed as a promising strategy for
improving AI reasoning ability, we find that debate can sometimes be harmful
rather than helpful. The prior work has exclusively focused on debates within
homogeneous groups of agents, whereas we explore how diversity in model
capabilities influences the dynamics and outcomes of multi-agent interactions.
Through a series of experiments, we demonstrate that debate can lead to a
decrease in accuracy over time -- even in settings where stronger (i.e., more
capable) models outnumber their weaker counterparts. Our analysis reveals that
models frequently shift from correct to incorrect answers in response to peer
reasoning, favoring agreement over challenging flawed reasoning. These results
highlight important failure modes in the exchange of reasons during multi-agent
debate, suggesting that naive applications of debate may cause performance
degradation when agents are neither incentivized nor adequately equipped to
resist persuasive but incorrect reasoning.

</details>


### [5] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
*Jessica M. Lundin,Ada Zhang,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 研究展示了如何通过少量特征准确预测翻译质量，揭示了语言类型学和token繁殖率对翻译质量的影响。


<details>
  <summary>Details</summary>
Motivation: 旨在无需运行翻译系统即可准确预测翻译质量。

Method: 使用梯度提升模型，基于少量特征（如token繁殖率、token计数和基本的语言学元数据）预测翻译质量。

Result: 梯度提升模型在XX→英语和英语→XX的FLORES-200基准测试中分别达到了R²=0.66和R²=0.72的性能。

Conclusion: 这些发现表明，翻译质量由token级别的繁殖率和更广泛的语言类型学共同塑造，为多语言评估和质量估计提供了新的见解。

Abstract: We show that translation quality can be predicted with surprising accuracy
\textit{without ever running the translation system itself}. Using only a
handful of features, token fertility ratios, token counts, and basic linguistic
metadata (language family, script, and region), we can forecast ChrF scores for
GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient
boosting models achieve favorable performance ($R^{2}=0.66$ for
XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature
importance analyses reveal that typological factors dominate predictions into
English, while fertility plays a larger role for translations into diverse
target languages. These findings suggest that translation quality is shaped by
both token-level fertility and broader linguistic typology, offering new
insights for multilingual evaluation and quality estimation.

</details>


### [6] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
*Logan Lawrence,Ashton Williamson,Alexander Shelton*

Main category: cs.CL

TL;DR: 本文提出了一种直接评分方法，利用合成摘要在测试时充当成对的机器排名，并在多个基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的成对比较方法在样本级性能上表现良好，但无法为单个摘要分配绝对分数，这对于需要阈值化的应用场景至关重要。

Method: 本文提出了一种直接评分方法，利用合成摘要在测试时充当成对的机器排名。

Result: 本文的方法在SummEval、TopicalChat和HANNA等元评估基准上的轴平均样本级相关性方面表现与最先进的成对评估器相当。

Conclusion: 本文提出了一种直接评分方法，该方法使用合成摘要在测试时充当成对的机器排名。结果显示，该方法在多个元评估基准上的样本级相关性方面表现与最先进的成对评估器相当。

Abstract: As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

</details>


### [7] [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)
*Hajar Sakai,Yi-En Tseng,Mohammadsadegh Mikaeili,Joshua Bosire,Franziska Jovin*

Main category: cs.CL

TL;DR: 本文介绍了一种基于大型语言模型的框架，用于分析医院呼叫中心的工作人员消息，提升数据利用效率并支持改进患者护理。


<details>
  <summary>Details</summary>
Motivation: 医院呼叫中心生成大量工作人员消息，传统监督学习方法需要注释数据和大量训练，而大型语言模型提供了更计算高效的医疗分析方法。

Method: 本文提出了一种多阶段的基于大型语言模型（LLM）的框架，用于识别工作人员消息的主题并以多类方式进行分类。评估了多种LLM类型，包括推理、通用和轻量级模型。

Result: 最佳性能模型是o3，取得了78.4%加权F1分数和79.2%的准确率，其次是gpt-5（75.3%加权F1分数和76.2%准确率）。

Conclusion: 本文提出的基于大型语言模型的框架能够有效地处理医院呼叫中心的工作人员消息，提高数据利用效率，并支持改进患者体验和护理质量。

Abstract: Hospital call centers serve as the primary contact point for patients within
a hospital system. They also generate substantial volumes of staff messages as
navigators process patient requests and communicate with the hospital offices
following the established protocol restrictions and guidelines. This
continuously accumulated large amount of text data can be mined and processed
to retrieve insights; however, traditional supervised learning approaches
require annotated data, extensive training, and model tuning. Large Language
Models (LLMs) offer a paradigm shift toward more computationally efficient
methodologies for healthcare analytics. This paper presents a multi-stage
LLM-based framework that identifies staff message topics and classifies
messages by their reasons in a multi-class fashion. In the process, multiple
LLM types, including reasoning, general-purpose, and lightweight models, were
evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score
and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and
76.2% accuracy). The proposed methodology incorporates data security measures
and HIPAA compliance requirements essential for healthcare environments. The
processed LLM outputs are integrated into a visualization decision support tool
that transforms the staff messages into actionable insights accessible to
healthcare professionals. This approach enables more efficient utilization of
the collected staff messaging data, identifies navigator training
opportunities, and supports improved patient experience and care quality.

</details>


### [8] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
*Jessica M. Lundin,Ada Zhang,Nihal Karim,Hamza Louzan,Victor Wei,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 研究发现分词效率影响低资源语言的准确性，推理模型表现更好，分词膨胀增加了成本，建议改进分词方法和公平定价。


<details>
  <summary>Details</summary>
Motivation: 解决形态复杂的低资源语言因分词效率低下而面临的结构劣势问题

Method: 评估10个大型语言模型（LLMs）在AfriMMLU数据集上的表现

Result: 高分词率（tokens/word）与较低的准确性相关，推理模型在高资源和低资源语言中表现优于非推理模型，分词膨胀导致训练成本和时间增加

Conclusion: 这些结果促使人们关注形态意识的分词、公平定价和多语言基准，以实现更公平的自然语言处理（NLP）

Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically
complex, low-resource languages, inflating compute resources and depressing
accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA
items; 5 subjects; 16 African languages) and show that fertility (tokens/word)
reliably predicts accuracy. Higher fertility consistently predicts lower
accuracy across all models and subjects. We further find that reasoning models
(DeepSeek, o1) consistently outperform non-reasoning peers across high and low
resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in
prior generations. Finally, translating token inflation to economics, a
doubling in tokens results in quadrupled training cost and time, underscoring
the token tax faced by many languages. These results motivate morphologically
aware tokenization, fair pricing, and multilingual benchmarks for equitable
natural language processing (NLP).

</details>


### [9] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
*Mansi Garg,Lee-Chi Wang,Bhavesh Ghanchi,Sanjana Dumpala,Shreyash Kakde,Yen Chih Chen*

Main category: cs.CL

TL;DR: 本研究提出了一种基于RAG架构的生物医学文献问答系统，以提高准确、基于证据的医学信息的可访问性。


<details>
  <summary>Details</summary>
Motivation: 传统健康搜索引擎存在不足，且公众获取生物医学研究的滞后，因此需要一种更有效的系统来提供准确、基于证据的医学信息。

Method: 该系统基于检索增强生成（RAG）架构，结合了PubMed文章、精心策划的问答数据集和医学百科全书等多种来源的信息，使用MiniLM语义嵌入和FAISS向量搜索进行检索，并通过微调的Mistral-7B-v0.3语言模型生成答案。

Result: 实验结果表明，与基线模型相比，该系统在事实一致性和语义相关性方面有显著提升。

Conclusion: 研究结果表明，基于RAG的系统能够有效提高生物医学文献问答的准确性和相关性，为未来多语言适应、隐私保护推理和个性化医疗AI系统铺平了道路。

Abstract: This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

</details>


### [10] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
*Serge Lionel Nikiema,Jordan Samhi,Micheline Bénédicte Moumoula,Albérick Euraste Djiré,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型是否真正理解概念，并提出了双向推理作为测试真正理解的方法。通过对比微调（CFT）训练模型，实验结果显示CFT能够实现双向推理，从而提升模型的理解能力。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决一个基本问题：大型语言模型是否真正理解概念，还是仅仅识别模式。

Method: 研究人员开发了对比微调（CFT），通过三种类型的例子对模型进行训练：保持语义意义的正例、语义不同的负例以及前向混淆例。

Result: 实验表明，CFT成功实现了双向推理，能够在保持前向任务能力的同时实现强大的反向性能。

Conclusion: 作者得出结论，双向推理既作为评估真正理解的理论框架，又作为开发更强大AI系统的实用训练方法。

Abstract: This research addresses a fundamental question in AI: whether large language
models truly understand concepts or simply recognize patterns. The authors
propose bidirectional reasoning,the ability to apply transformations in both
directions without being explicitly trained on the reverse direction, as a test
for genuine understanding. They argue that true comprehension should naturally
allow reversibility. For example, a model that can change a variable name like
userIndex to i should also be able to infer that i represents a user index
without reverse training. The researchers tested current language models and
discovered what they term cognitive specialization: when models are fine-tuned
on forward tasks, their performance on those tasks improves, but their ability
to reason bidirectionally becomes significantly worse. To address this issue,
they developed Contrastive Fine-Tuning (CFT), which trains models using three
types of examples: positive examples that maintain semantic meaning, negative
examples with different semantics, and forward-direction obfuscation examples.
This approach aims to develop deeper understanding rather than surface-level
pattern recognition and allows reverse capabilities to develop naturally
without explicit reverse training. Their experiments demonstrated that CFT
successfully achieved bidirectional reasoning, enabling strong reverse
performance while maintaining forward task capabilities. The authors conclude
that bidirectional reasoning serves both as a theoretical framework for
assessing genuine understanding and as a practical training approach for
developing more capable AI systems.

</details>


### [11] [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)
*Anya Ji,Claire Augusta Bergey,Ron Eliav,Yoav Artzi,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 这项研究探讨了人们如何通过交流建立新的共享命名系统，并发现这些临时约定反映了真正的概念协调。


<details>
  <summary>Details</summary>
Motivation: 人们如何谈论他们之前从未谈论过的事情？一种观点认为新的共享命名系统建立了一个任意的链接到特定目标，另一种观点则提出形成共享的描述对象的方式涉及更广泛的的概念对齐。

Method: 在一项双人交流研究中（N=302），利用最近发布的KiloGram数据集，其中包含超过1000张抽象的 tangram 图像。

Result: 我们发现了强有力的证据表明，合作伙伴相对于他们的预测试标签显示出更高的对齐度。这种泛化也随着视觉相似性非线性衰减（与Shepard定律一致），并且在图像的可命名性水平上是稳健的。

Conclusion: 这些发现表明，临时约定不是任意的标签，而是反映真正的概念协调，这对参考理论和设计更适应的语言代理有影响。

Abstract: How do people talk about things they've never talked about before? One view
suggests that a new shared naming system establishes an arbitrary link to a
specific target, like proper names that cannot extend beyond their bearers. An
alternative view proposes that forming a shared way of describing objects
involves broader conceptual alignment, reshaping each individual's semantic
space in ways that should generalize to new referents. We test these competing
accounts in a dyadic communication study (N=302) leveraging the
recently-released KiloGram dataset containing over 1,000 abstract tangram
images. After pairs of participants coordinated on referential conventions for
one set of images through repeated communication, we measured the extent to
which their descriptions aligned for undiscussed images. We found strong
evidence for generalization: partners showed increased alignment relative to
their pre-test labels. Generalization also decayed nonlinearly with visual
similarity (consistent with Shepard's law) and was robust across levels of the
images' nameability. These findings suggest that ad hoc conventions are not
arbitrary labels but reflect genuine conceptual coordination, with implications
for theories of reference and the design of more adaptive language agents.

</details>


### [12] [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)
*Hongyan Xie,Yitong Yao,Yikun Ban,Zixuan Huang,Deqing Wang,Zhenhe Wu,Haoxiang Su,Chao Wang,Shuangyong Song,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为Chain-of-Thought Correctness Perception Distillation (CoPeD)的方法，旨在从任务设置和数据利用的角度提高学生模型的推理质量。首先，引入了一个正确性感知的任务设置，鼓励学生模型根据正确的推理过程预测答案并在错误时进行修正。然后，提出了一种正确性感知加权损失函数，根据推理和答案的综合损失动态调整每个训练实例的贡献。实验表明CoPeD在分布内和分布外基准推理数据集上都有效。


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) excel at reasoning tasks but are expensive to deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated by LLMs to copy LLMs' abilities. However, these CoT data may include noisy rationales that either fail to substantiate the answers or contribute no additional information to support answer prediction, which leads SLMs to capture spurious correlations between questions and answers and compromise the quality of reasoning.

Method: Chain-of-Thought Correctness Perception Distillation (CoPeD), which aims to improve the reasoning quality of the student model from the perspectives of task setting and data utilization. Firstly, we introduce a correctness-aware task setting that encourages the student model to predict answers based on correct rationales and revise them when they are incorrect. Then, we propose a Correctness-Aware Weighted loss, which dynamically adjusts the contribution of each training instance based on the combined loss of the rationale and the answer.

Result: Experiments have shown that CoPeD is effective on both in-distribution (IND) and out-of-distribution (OOD) benchmark reasoning datasets.

Conclusion: CoPeD is effective on both in-distribution (IND) and out-of-distribution (OOD) benchmark reasoning datasets.

Abstract: Large language models (LLMs) excel at reasoning tasks but are expensive to
deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated
by LLMs to copy LLMs' abilities. However, these CoT data may include noisy
rationales that either fail to substantiate the answers or contribute no
additional information to support answer prediction, which leads SLMs to
capture spurious correlations between questions and answers and compromise the
quality of reasoning. In this work, we propose Chain-of-Thought Correctness
Perception Distillation (CoPeD), which aims to improve the reasoning quality of
the student model from the perspectives of task setting and data utilization.
Firstly, we introduce a correctness-aware task setting that encourages the
student model to predict answers based on correct rationales and revise them
when they are incorrect. This setting improves the faithfulness of reasoning
and allows the model to learn from its mistakes. Then, we propose a
Correctness-Aware Weighted loss, which dynamically adjusts the contribution of
each training instance based on the combined loss of the rationale and the
answer. This strategy encourages the model to focus more on samples where the
rationale offers stronger support for the correct answer. Experiments have
shown that CoPeD is effective on both in-distribution (IND) and
out-of-distribution (OOD) benchmark reasoning datasets.

</details>


### [13] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
*Qiyuan Chen,Hongsen Huang,Qian Shao,Jiahe Chen,Jintai Chen,Hongxia Xu,Renjie Hua,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Icon$^{2}$的新方法，通过利用大型语言模型（LLMs）的表示空间固有调节，实现高效且定制化的偏好数据集构建。实验结果显示，该方法在对齐和效率方面均取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统构建偏好数据集的方法面临显著挑战：依赖于预先收集的指令可能导致与目标模型的分布不匹配，而需要采样多个随机响应则引入了大量计算开销。

Method: Icon$^{2}$通过提取分层方向向量来编码复杂的人类偏好，并利用这些向量根据内在一致性过滤自我合成的指令。在解码过程中，应用双向内在控制来引导标记表示，从而精确生成具有清晰对齐差异的响应对。

Result: Icon$^{2}$在对齐和效率方面都有显著提升。Llama3-8B和Qwen2-7B在AlpacaEval 2.0和Arena-Hard上的平均胜率分别提高了13.89%和13.45%，同时计算成本减少了高达48.1%。

Conclusion: 实验结果表明，Icon$^{2}$在对齐和效率方面都有显著提升。Llama3-8B和Qwen2-7B在AlpacaEval 2.0和Arena-Hard上的平均胜率分别提高了13.89%和13.45%，同时计算成本减少了高达48.1%。

Abstract: Large Language Models (LLMs) require high quality preference datasets to
align with human preferences. However, conventional methods for constructing
such datasets face significant challenges: reliance on pre-collected
instructions often leads to distribution mismatches with target models, while
the need for sampling multiple stochastic responses introduces substantial
computational overhead. In this work, we explore a paradigm shift by leveraging
inherent regulation of LLMs' representation space for efficient and tailored
preference dataset construction, named Icon$^{2}$. Specifically, it first
extracts layer-wise direction vectors to encode sophisticated human preferences
and then uses these vectors to filter self-synthesized instructions based on
their inherent consistency. During decoding, bidirectional inherent control is
applied to steer token representations, enabling the precise generation of
response pairs with clear alignment distinctions. Experimental results
demonstrate significant improvements in both alignment and efficiency.
Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on
AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by
up to 48.1%.

</details>


### [14] [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)
*Qiyuan Chen,Jiahe Chen,Hongsen Huang,Qian Shao,Jintai Chen,Renjie Hua,Hongxia Xu,Ruijia Wu,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: 本文介绍了一种用于生成式搜索引擎优化的框架，通过构建基准和设计多代理系统，揭示了内容影响的动力学，并为未来研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 传统基于排名的搜索范式转向生成式搜索引擎，使得传统的SEO指标过时，因此需要理解、衡量和优化内容对合成答案的影响。

Method: 本文构建了一个大规模的内容中心基准CC-GSEO-Bench，并设计了一个多代理系统，通过协作分析-修改-评估的工作流程来操作化该框架。

Result: 通过使用该框架进行实证分析，揭示了内容影响的动力学，并为创作者提供了可操作的策略。

Conclusion: 本文提出了一个全面的端到端框架，用于生成式搜索引擎优化（GSEO），并展示了该框架在理解内容影响力方面的新兴见解，为未来的研究奠定了基础。

Abstract: The paradigm shift from traditional ranked-based search to Generative Search
Engines has rendered conventional SEO metrics obsolete, creating an urgent need
to understand, measure, and optimize for content influence on synthesized
answers. This paper introduces a comprehensive, end-to-end framework for
Generative Search Engine Optimization (GSEO) to address this challenge. We make
two primary contributions. First, we construct CC-GSEO-Bench, a large-scale,
content-centric benchmark, and propose a multi-dimensional evaluation framework
that systematically quantifies influence, moving beyond surface-level
attribution to assess substantive semantic impact. Second, we design a novel
multi-agent system that operationalizes this framework, automating the
strategic refinement of content through a collaborative analyze-revise-evaluate
workflow. Our empirical analysis using this framework reveals novel insights
into the dynamics of content influence, offering actionable strategies for
creators and establishing a principled foundation for future GSEO research.

</details>


### [15] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: cs.CL

TL;DR: 本文提出了一种基于不平衡最优传输的对齐模型，用于解决声学和语言表示之间的对齐问题，以提高自动语音识别的性能。


<details>
  <summary>Details</summary>
Motivation: 对齐声学和语言表示是桥接预训练模型在自动语音识别（ASR）中的知识转移的核心挑战。这种对齐本质上是结构化且不对称的，因此需要一种新的方法来处理这些复杂性。

Method: 我们提出了一种基于不平衡最优传输的对齐模型，该模型明确处理分布不匹配和结构不对称性，并在声学和语言模态之间进行软部分匹配。

Result: 实验结果表明，我们的方法在灵活控制匹配程度方面有效，从而提高了ASR性能。

Conclusion: 我们的方法在灵活控制匹配程度方面表现出色，从而提高了ASR性能。

Abstract: Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

</details>


### [16] [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)
*Shay Dahary,Avi Edana,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型进行歌曲歌词情感属性分析的可行性，并提出了一个手动标注的数据集。


<details>
  <summary>Details</summary>
Motivation: 歌曲歌词的情感内容对听众体验和音乐偏好有重要影响，因此需要准确的情感属性分析。

Method: 本研究构建了一个手动标注的数据集，并评估了多个公开可用的大型语言模型在零样本场景下的表现，同时微调了一个基于BERT的模型以预测多标签情感分数。

Result: 实验结果揭示了零样本和微调模型在捕捉歌词情感内容方面的相对优势和局限性。

Conclusion: 研究结果表明，大型语言模型在情感识别任务中具有潜力，但需要根据具体应用场景选择合适的模型。

Abstract: The emotional content of song lyrics plays a pivotal role in shaping listener
experiences and influencing musical preferences. This paper investigates the
task of multi-label emotional attribution of song lyrics by predicting six
emotional intensity scores corresponding to six fundamental emotions. A
manually labeled dataset is constructed using a mean opinion score (MOS)
approach, which aggregates annotations from multiple human raters to ensure
reliable ground-truth labels. Leveraging this dataset, we conduct a
comprehensive evaluation of several publicly available large language models
(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model
specifically for predicting multi-label emotion scores. Experimental results
reveal the relative strengths and limitations of zero-shot and fine-tuned
models in capturing the nuanced emotional content of lyrics. Our findings
highlight the potential of LLMs for emotion recognition in creative texts,
providing insights into model selection strategies for emotion-based music
information retrieval applications. The labeled dataset is available at
https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

</details>


### [17] [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)
*Liang Zhang,Yuan Li,Shijie Zhang,Zheng Zhang,Xitong Li*

Main category: cs.CL

TL;DR: SAID is a new framework that combines text and structural information to improve intent detection in conversational systems.


<details>
  <summary>Details</summary>
Motivation: Existing methods have predominantly focused on textual data, neglecting to effectively capture the crucial structural information inherent in conversational systems.

Method: SAID, a novel framework that integrates both textual and relational structure information in a unified manner for model pretraining, and QueryAdapt, a query-adaptive attention network that generates intent-specific relation tokens.

Result: Extensive experimental results on two real-world datasets demonstrate that SAID significantly outperforms state-of-the-art methods.

Conclusion: SAID significantly outperforms state-of-the-art methods.

Abstract: Intent detection is a crucial component of modern conversational systems,
since accurately identifying user intent at the beginning of a conversation is
essential for generating effective responses. Recent efforts have focused on
studying this problem under a challenging few-shot scenario. These approaches
primarily leverage large-scale unlabeled dialogue text corpora to pretrain
language models through various pretext tasks, followed by fine-tuning for
intent detection with very limited annotations. Despite the improvements
achieved, existing methods have predominantly focused on textual data,
neglecting to effectively capture the crucial structural information inherent
in conversational systems, such as the query-query relation and query-answer
relation. To address this gap, we propose SAID, a novel framework that
integrates both textual and relational structure information in a unified
manner for model pretraining for the first time. Building on this framework, we
further propose a novel mechanism, the query-adaptive attention network
(QueryAdapt), which operates at the relation token level by generating
intent-specific relation tokens from well-learned query-query and query-answer
relations explicitly, enabling more fine-grained knowledge transfer. Extensive
experimental results on two real-world datasets demonstrate that SAID
significantly outperforms state-of-the-art methods.

</details>


### [18] [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
*Yuxuan Hu,Jihao Liu,Ke Wang,Jinliang Zhen,Weikang Shi,Manyuan Zhang,Qi Dou,Rui Liu,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了LM-Searcher框架，通过NCode和排名任务的方法实现跨领域神经架构优化，具有良好的性能和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的NAS方法依赖于提示工程和领域特定调整，限制了其在不同任务中的实用性和可扩展性。

Method: 提出了一种新的框架LM-Searcher，利用LLM进行跨领域神经架构优化，引入了NCode作为通用数值字符串表示，并将NAS问题重新表述为排名任务。

Result: LM-Searcher在域内（如图像分类的CNN）和域外（如分割和生成的LoRA配置）任务中都表现出色。

Conclusion: LM-Searcher在多种任务中表现出色，展示了基于LLM的架构搜索的新范式。

Abstract: Recent progress in Large Language Models (LLMs) has opened new avenues for
solving complex optimization problems, including Neural Architecture Search
(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt
engineering and domain-specific tuning, limiting their practicality and
scalability across diverse tasks. In this work, we propose LM-Searcher, a novel
framework that leverages LLMs for cross-domain neural architecture optimization
without the need for extensive domain-specific adaptation. Central to our
approach is NCode, a universal numerical string representation for neural
architectures, which enables cross-domain architecture encoding and search. We
also reformulate the NAS problem as a ranking task, training LLMs to select
high-performing architectures from candidate pools using instruction-tuning
samples derived from a novel pruning-based subspace sampling strategy. Our
curated dataset, encompassing a wide range of architecture-performance pairs,
encourages robust and transferable learning. Comprehensive experiments
demonstrate that LM-Searcher achieves competitive performance in both in-domain
(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA
configurations for segmentation and generation) tasks, establishing a new
paradigm for flexible and generalizable LLM-based architecture search. The
datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

</details>


### [19] [Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning](https://arxiv.org/abs/2509.05660)
*Hong Su*

Main category: cs.CL

TL;DR: 本文提出了一种扩展方法重用范围的方法，以解决相似性较低或隐藏相似性的问题。通过将问题和解决方案分开，并引导LLM适应新的相关问题，实现了更有效的跨问题方法重用。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常要求问题高度相似，但本文旨在扩展方法重用的范围，以处理相似性较低或隐藏相似性的问题。

Method: 本文提出了一种方法，将问题和解决方案分开，而不是直接将配对输入到LLM中。然后引导LLM适应新的但相关的问题，使其专注于解决方案的转移而不是问题识别。此外，还扩展了这种方法，以处理仅共享部分特征或隐藏特征的情况。

Result: 实验验证表明，本文提出的范围扩展方法提高了过滤可重用解决方案的概率，从而提高了跨问题方法重用的有效性。

Conclusion: 本文提出了一种扩展方法重用范围的方法，以解决相似性较低或隐藏相似性的问题。实验验证表明，该方法提高了过滤可重用解决方案的概率，从而提高了跨问题方法重用的有效性。

Abstract: Large language models (LLMs) have been widely applied to assist in finding
solutions for diverse questions. Prior work has proposed representing a method
as a pair of a question and its corresponding solution, enabling method reuse.
However, existing approaches typically require the questions to be highly
similar. In this paper, we extend the scope of method reuse to address
questions with low similarity or with hidden similarities that are not
explicitly observable. For questions that are similar in a general-specific
sense (i.e., broader or narrower in scope), we propose to first separate the
question and solution, rather than directly feeding the pair to the LLM. The
LLM is then guided to adapt the solution to new but related questions, allowing
it to focus on solution transfer rather than question recognition. Furthermore,
we extend this approach to cases where questions only share partial features or
hidden characteristics. This enables cross-question method reuse beyond
conventional similarity constraints. Experimental verification shows that our
scope-extension approach increases the probability of filtering out reusable
solutions, thereby improving the effectiveness of cross-question method reuse.

</details>


### [20] [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)
*Michael Hoffmann,Jophin John,Stefan Schweter,Gokul Ramakrishnan,Hoi-Fong Mak,Alice Zhang,Dmitry Gaynullin,Nicolay J. Hammer*

Main category: cs.CL

TL;DR: Llama-GENBA-10B is a trilingual foundation model designed to reduce English-centric bias by balancing resources across English, German, and Bavarian. It addresses challenges in multilingual training and evaluation, achieving strong cross-lingual performance and promoting low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address English-centric bias in large language models and promote low-resource languages like Bavarian, particularly for the German NLP community.

Method: Llama-GENBA-10B is built on Llama 3.1-8B and scaled to 10B parameters, continuously pretrained on a multilingual corpus with balanced resources. It addresses four challenges: curating a multilingual corpus, creating a unified tokenizer, optimizing architecture and language-ratio hyperparameters, and establishing a standardized trilingual evaluation suite.

Result: Llama-GENBA-10B achieves strong cross-lingual performance, surpassing existing models in Bavarian and outperforming EuroLLM in English while matching its results in German. Training on Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining.

Conclusion: Llama-GENBA-10B demonstrates strong cross-lingual performance and offers a blueprint for inclusive foundation models that integrate low-resource languages.

Abstract: We present Llama-GENBA-10B, a trilingual foundation model addressing
English-centric bias in large language models. Built on Llama 3.1-8B and scaled
to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens
(82B English, 82B German, and 80M Bavarian), balancing resources while
preventing English dominance. Targeted at the German NLP community, the model
also promotes Bavarian as a low-resource language. Development tackled four
challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)
creating a unified tokenizer for English, German, and Bavarian, (3) optimizing
architecture and language-ratio hyperparameters for cross-lingual transfer, and
(4) establishing the first standardized trilingual evaluation suite by
translating German benchmarks into Bavarian. Evaluations show that
Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned
variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing
itself as the best model in its class for this language, while also
outperforming EuroLLM in English and matching its results in German. Training
on the Cerebras CS-2 demonstrated efficient large-scale multilingual
pretraining with documented energy use, offering a blueprint for inclusive
foundation models that integrate low-resource languages.

</details>


### [21] [Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models](https://arxiv.org/abs/2509.05691)
*Ningyuan Deng,Hanyu Duan,Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 本研究评估了13种文本嵌入模型在捕捉数值细节方面的能力，发现它们存在困难，并提供了关于嵌入数值能力的深入见解。


<details>
  <summary>Details</summary>
Motivation: 由于文本嵌入模型在需要理解文本中细微数值信息的领域（如金融和医疗）中的应用日益增加，因此需要了解它们是否能准确编码数值内容。

Method: 本研究使用金融情境的合成数据，评估了13种广泛使用的文本嵌入模型。

Result: 研究发现，这些模型通常难以准确捕捉数值细节。

Conclusion: 本研究发现，当前的文本嵌入模型在捕捉数值细节方面存在困难，并为未来的研究提供了关于嵌入数值能力的深入见解。

Abstract: Text embedding models are widely used in natural language processing
applications. However, their capability is often benchmarked on tasks that do
not require understanding nuanced numerical information in text. As a result,
it remains unclear whether current embedding models can precisely encode
numerical content, such as numbers, into embeddings. This question is critical
because embedding models are increasingly applied in domains where numbers
matter, such as finance and healthcare. For example, Company X's market share
grew by 2\% should be interpreted very differently from Company X's market
share grew by 20\%, even though both indicate growth in market share. This
study aims to examine whether text embedding models can capture such nuances.
Using synthetic data in a financial context, we evaluate 13 widely used text
embedding models and find that they generally struggle to capture numerical
details accurately. Our further analyses provide deeper insights into embedding
numeracy, informing future research to strengthen embedding model-based NLP
systems with improved capacity for handling numerical content.

</details>


### [22] [A Survey of the State-of-the-Art in Conversational Question Answering Systems](https://arxiv.org/abs/2509.05716)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Fahmida Islam,Maryam Tahermazandarani,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文综述了对话问答系统的研究现状，探讨了其核心组件、机器学习方法和大型语言模型的作用，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着对话问答系统在客户支持、教育、法律和医疗等领域的广泛应用，保持对话的连贯性和相关性变得至关重要。因此，需要对当前的ConvQA技术进行深入分析，以推动其进一步发展。

Method: 本文通过分析ConvQA系统的组成部分，如历史选择、问题理解和答案预测，以及使用先进的机器学习技术（如强化学习、对比学习和迁移学习）来提高ConvQA的准确性与效率，进行了全面的研究。

Result: 本文详细分析了ConvQA的关键组件、先进机器学习技术的应用以及大型语言模型的影响，并对关键的数据集进行了全面分析。

Conclusion: 本文提供了对对话问答（ConvQA）领域的全面概述，并为该领域未来的发展提供了有价值的见解。

Abstract: Conversational Question Answering (ConvQA) systems have emerged as a pivotal
area within Natural Language Processing (NLP) by driving advancements that
enable machines to engage in dynamic and context-aware conversations. These
capabilities are increasingly being applied across various domains, i.e.,
customer support, education, legal, and healthcare where maintaining a coherent
and relevant conversation is essential. Building on recent advancements, this
survey provides a comprehensive analysis of the state-of-the-art in ConvQA.
This survey begins by examining the core components of ConvQA systems, i.e.,
history selection, question understanding, and answer prediction, highlighting
their interplay in ensuring coherence and relevance in multi-turn
conversations. It further investigates the use of advanced machine learning
techniques, including but not limited to, reinforcement learning, contrastive
learning, and transfer learning to improve ConvQA accuracy and efficiency. The
pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,
Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact
through data scalability and architectural advancements. Additionally, this
survey presents a comprehensive analysis of key ConvQA datasets and concludes
by outlining open research directions. Overall, this work offers a
comprehensive overview of the ConvQA landscape and provides valuable insights
to guide future advancements in the field.

</details>


### [23] [Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models](https://arxiv.org/abs/2509.05719)
*Donya Rooein,Flor Miriam Plaza-del-Arco,Debora Nozza,Dirk Hovy*

Main category: cs.CL

TL;DR: 尽管波斯语有大量使用者和数字文本，但在主观任务中仍面临数据可用性和质量的挑战，现有数据集缺乏关键人口统计因素，导致预测模型结果不稳定。


<details>
  <summary>Details</summary>
Motivation: 由于波斯语有超过1.27亿的使用者和越来越多的数字文本，它被认为是一种中等资源语言，但我们在主观任务中发现数据可用性和质量存在重大挑战。

Method: 我们审查了110篇关于波斯语主观任务的出版物，并评估了可用数据集的可用性和质量。

Result: 我们发现现有数据集往往缺乏关键的人口统计因素，如年龄和性别，这会影响语言主观性建模的准确性。此外，使用可用数据集评估预测模型的结果在数据集和模型之间高度不稳定。

Conclusion: 我们的研究结果表明，数据量不足以显著改善一种语言在自然语言处理中的前景。

Abstract: Given Farsi's speaker base of over 127 million people and the growing
availability of digital text, including more than 1.3 million articles on
Wikipedia, it is considered a middle-resource language. However, this label
quickly crumbles when the situation is examined more closely. We focus on three
subjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)
and find significant challenges in data availability and quality, despite the
overall increase in data availability. We review 110 publications on subjective
tasks in Farsi and observe a lack of publicly available datasets. Furthermore,
existing datasets often lack essential demographic factors, such as age and
gender, that are crucial for accurately modeling subjectivity in language. When
evaluating prediction models using the few available datasets, the results are
highly unstable across both datasets and models. Our findings indicate that the
volume of data is insufficient to significantly improve a language's prospects
in NLP.

</details>


### [24] [QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing](https://arxiv.org/abs/2509.05729)
*Charles M. Varmantchaonala,Niclas GÖtting,Nils-Erik SchÜtte,Jean Louis E. K. Fendji,Christopher Gies*

Main category: cs.CL

TL;DR: 本文介绍了一种名为QCSE的预训练量子上下文敏感嵌入模型，该模型利用量子计算的优势来提升自然语言处理的效果，并在低资源语言中展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过量子计算的力量，提供一种新颖的方法来编码和理解自然语言的复杂性，并解决低资源语言数据不足的问题。

Method: 本文提出了一种预训练的量子上下文敏感嵌入模型QCSE，利用量子系统的独特性质来学习语言中的上下文关系。提出了五种不同的方法来计算上下文矩阵，包括指数衰减、正弦调制、相位偏移和基于哈希的转换。

Result: 实验结果表明，QCSE不仅能够捕捉上下文敏感性，还能够利用量子系统的表达能力来表示丰富的上下文感知语言信息。此外，使用Fulani语言进一步突显了QNLP在解决此类语言数据不足问题上的潜力。

Conclusion: 本文强调了量子计算在自然语言处理（NLP）中的潜力，并为QNLP在各种任务和领域中的实际应用开辟了新的途径。

Abstract: Quantum Natural Language Processing (QNLP) offers a novel approach to
encoding and understanding the complexity of natural languages through the
power of quantum computation. This paper presents a pretrained quantum
context-sensitive embedding model, called QCSE, that captures context-sensitive
word embeddings, leveraging the unique properties of quantum systems to learn
contextual relationships in languages. The model introduces quantum-native
context learning, enabling the utilization of quantum computers for linguistic
tasks. Central to the proposed approach are innovative context matrix
computation methods, designed to create unique, representations of words based
on their surrounding linguistic context. Five distinct methods are proposed and
tested for computing the context matrices, incorporating techniques such as
exponential decay, sinusoidal modulation, phase shifts, and hash-based
transformations. These methods ensure that the quantum embeddings retain
context sensitivity, thereby making them suitable for downstream language tasks
where the expressibility and properties of quantum systems are valuable
resources. To evaluate the effectiveness of the model and the associated
context matrix methods, evaluations are conducted on both a Fulani corpus, a
low-resource African language, dataset of small size and an English corpus of
slightly larger size. The results demonstrate that QCSE not only captures
context sensitivity but also leverages the expressibility of quantum systems
for representing rich, context-aware language information. The use of Fulani
further highlights the potential of QNLP to mitigate the problem of lack of
data for this category of languages. This work underscores the power of quantum
computation in natural language processing (NLP) and opens new avenues for
applying QNLP to real-world linguistic challenges across various tasks and
domains.

</details>


### [25] [Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification](https://arxiv.org/abs/2509.05741)
*Fernando Gabriela García,Qiyang Shi,Zilin Feng*

Main category: cs.CL

TL;DR: VeriFact-CoT 是一种新的方法，通过多阶段机制提高大型语言模型生成内容的准确性和可信度，解决幻觉和缺乏引用的问题。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决大型语言模型在生成复杂、事实敏感内容时普遍存在的幻觉问题和缺乏可信引用来源的问题。

Method: VeriFact-CoT 采用了一种多阶段机制，包括‘事实验证-反思-引用整合’，使大型语言模型能够批判性地自我检查并修改其中间推理步骤和最终答案。

Result: VeriFact-CoT 显著提高了生成内容的准确性和可信度，使其更适合用于科学研宄、新闻报道和法律咨询等高保真度需求的应用场景。

Conclusion: VeriFact-CoT 提高了大型语言模型生成内容的客观准确性、可信度和可追溯性，使其在需要高保真度的应用中更加可靠。

Abstract: This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a
novel method designed to address the pervasive issues of hallucination and the
absence of credible citation sources in Large Language Models (LLMs) when
generating complex, fact-sensitive content. By incorporating a multi-stage
mechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT
empowers LLMs to critically self-examine and revise their intermediate
reasoning steps and final answers. This process significantly enhances the
objective accuracy, trustworthiness, and traceability of the generated outputs,
making LLMs more reliable for applications demanding high fidelity such as
scientific research, news reporting, and legal consultation.

</details>


### [26] [LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization](https://arxiv.org/abs/2509.05863)
*Luis Felipe Chary,Miguel Arjona Ramirez*

Main category: cs.CL

TL;DR: LatinX是一种多语言文本到语音模型，通过三阶段训练方法在跨语言保留说话人身份方面表现出色，并在客观和主观评估中优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言文本到语音(TTS)模型在跨语言保留源说话人身份方面存在不足，因此需要一种新的方法来改进这一问题。

Method: LatinX是一个12层的仅解码器Transformer模型，分为三个阶段进行训练：(i) 文本到音频映射的预训练，(ii) 零样本语音克隆的监督微调，(iii) 使用基于词错误率(WER)和说话人相似性度量的自动标记对进行直接偏好优化(DPO)。

Result: LatinX在英语和罗曼语系语言（特别是葡萄牙语）上训练，使用DPO方法 consistently 减少了WER并提高了客观相似性。人类评估表明其在感知说话人相似性方面优于XTTSv2基线模型。

Conclusion: LatinX通过DPO方法在减少WER和提高客观相似性方面表现出色，同时人类评估显示其在感知说话人相似性方面优于基线模型。未来工作包括平衡偏好信号和低延迟架构。

Abstract: We present LatinX, a multilingual text-to-speech (TTS) model for cascaded
speech-to-speech translation that preserves the source speaker's identity
across languages. LatinX is a 12-layer decoder-only Transformer trained in
three stages: (i) pre-training for text-to-audio mapping, (ii) supervised
fine-tuning for zero-shot voice cloning, and (iii) alignment with Direct
Preference Optimization (DPO) using automatically labeled pairs based on Word
Error Rate (WER) and speaker-similarity metrics. Trained on English and Romance
languages with emphasis on Portuguese, LatinX with DPO consistently reduces WER
and improves objective similarity over the fine-tuned baseline. Human
evaluations further indicate stronger perceived speaker similarity than a
strong baseline (XTTSv2), revealing gaps between objective and subjective
measures. We provide cross-lingual analyses and discuss balanced preference
signals and lower-latency architectures as future work.

</details>


### [27] [ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula](https://arxiv.org/abs/2509.05867)
*ZiXuan Zhang,Bowen Hao,Yingjie Li,Hongzhi Yin*

Main category: cs.CL

TL;DR: ZhiFangDanTai is a framework that enhances TCM formula generation by combining GraphRAG with LLM fine-tuning, leading to improved performance and comprehensive results.


<details>
  <summary>Details</summary>
Motivation: Existing models for TCM lack comprehensive results and sufficient details, such as the roles of formula components, efficacy, contraindications, and diagnosis information.

Method: ZhiFangDanTai combines Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning to retrieve and synthesize structured TCM knowledge and construct an enhanced instruction dataset.

Result: ZhiFangDanTai demonstrates significant improvements over state-of-the-art models on both collected and clinical datasets.

Conclusion: ZhiFangDanTai achieves significant improvements over state-of-the-art models and is open-sourced.

Abstract: Traditional Chinese Medicine (TCM) formulas play a significant role in
treating epidemics and complex diseases. Existing models for TCM utilize
traditional algorithms or deep learning techniques to analyze formula
relationships, yet lack comprehensive results, such as complete formula
compositions and detailed explanations. Although recent efforts have used TCM
instruction datasets to fine-tune Large Language Models (LLMs) for explainable
formula generation, existing datasets lack sufficient details, such as the
roles of the formula's sovereign, minister, assistant, courier; efficacy;
contraindications; tongue and pulse diagnosis-limiting the depth of model
outputs. To address these challenges, we propose ZhiFangDanTai, a framework
combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM
fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured
TCM knowledge into concise summaries, while also constructing an enhanced
instruction dataset to improve LLMs' ability to integrate retrieved
information. Furthermore, we provide novel theoretical proofs demonstrating
that integrating GraphRAG with fine-tuning techniques can reduce generalization
error and hallucination rates in the TCM formula task. Experimental results on
both collected and clinical datasets demonstrate that ZhiFangDanTai achieves
significant improvements over state-of-the-art models. Our model is
open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

</details>


### [28] [MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries](https://arxiv.org/abs/2509.05878)
*François Grolleau,Emily Alsentzer,Timothy Keyes,Philip Chung,Akshay Swaminathan,Asad Aali,Jason Hom,Tridu Huynh,Thomas Lew,April S. Liang,Weihan Chu,Natasha Z. Steele,Christina F. Lin,Jingkun Yang,Kameron C. Black,Stephen P. Ma,Fateme N. Haredasht,Nigam H. Shah,Kevin Schulman,Jonathan H. Chen*

Main category: cs.CL

TL;DR: 本文提出了MedFactEval和MedAgentBrief，用于评估和生成临床文本中的事实准确性，以推动生成式AI在临床工作流程中的负责任部署。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）生成的临床文本的事实准确性是采用该技术的关键障碍，因为专家评审对于这些系统所需的持续质量保证来说是不可扩展的。

Method: 我们引入了MedFactEval，这是一个可扩展的、基于事实的评估框架，其中临床医生定义高显著性的关键事实，并通过一个“LLM陪审团”——多LLM多数投票来评估这些事实是否包含在生成的摘要中。此外，我们提出了MedAgentBrief，这是一种模型无关的多步骤工作流，旨在生成高质量、事实准确的出院摘要。

Result: 我们建立了黄金标准参考，通过对住院病例中临床医生定义的关键事实进行七位医生的多数投票。MedFactEval LLM陪审团与该小组几乎达到完美的一致性（Cohen's kappa=81%），其性能在统计上不劣于单个专家（kappa=67%，P < 0.001）。

Conclusion: 我们的工作提供了稳健的评估框架（MedFactEval）和高性能的生成流程（MedAgentBrief），为在临床工作流程中负责任地部署生成式AI提供全面的方法。

Abstract: Evaluating factual accuracy in Large Language Model (LLM)-generated clinical
text is a critical barrier to adoption, as expert review is unscalable for the
continuous quality assurance these systems require. We address this challenge
with two complementary contributions. First, we introduce MedFactEval, a
framework for scalable, fact-grounded evaluation where clinicians define
high-salience key facts and an "LLM Jury"--a multi-LLM majority vote--assesses
their inclusion in generated summaries. Second, we present MedAgentBrief, a
model-agnostic, multi-step workflow designed to generate high-quality, factual
discharge summaries. To validate our evaluation framework, we established a
gold-standard reference using a seven-physician majority vote on
clinician-defined key facts from inpatient cases. The MedFactEval LLM Jury
achieved almost perfect agreement with this panel (Cohen's kappa=81%), a
performance statistically non-inferior to that of a single human expert
(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework
(MedFactEval) and a high-performing generation workflow (MedAgentBrief),
offering a comprehensive approach to advance the responsible deployment of
generative AI in clinical workflows.

</details>


### [29] [Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues](https://arxiv.org/abs/2509.05882)
*Abhijnan Nath,Carine Graff,Nikhil Krishnaswamy*

Main category: cs.CL

TL;DR: 本文研究了不同对齐方法如何影响LLM代理在多轮、多边协作中的有效性，并提出了一种新的反事实评估框架。结果表明，考虑摩擦的方法在帮助达成共识和提高任务结果的正确性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）融入各种工作流程，它们越来越被视为与人类的“合作者”。为了使这些AI合作者可靠，它们在多轮交互中的行为必须是可预测、验证和验证的。然而，常见的对齐技术通常是在简化的单用户设置下开发的，没有考虑到长视野多边互动的动力学。因此，本文研究了不同的对齐方法如何影响LLM代理在多轮、多边协作中的有效性。

Method: 本文通过角色扮演方法评估了不同训练的摩擦代理在协作任务对话中的干预效果，并提出了一种新的反事实评估框架来量化摩擦干预如何改变群体协作和信念对齐的轨迹。

Result: 实验结果表明，一种考虑摩擦的方法显著优于常见的对齐基线，在帮助达成共识和任务结果的正确性方面表现更好。

Conclusion: 本文结论是，一种考虑摩擦的方法显著优于常见的对齐基线，有助于达成共识和任务结果的正确性。

Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are
increasingly being considered "collaborators" with humans. If such AI
collaborators are to be reliable, their behavior over multiturn interactions
must be predictable, validated and verified before deployment. Common alignment
techniques are typically developed under simplified single-user settings and do
not account for the dynamics of long-horizon multiparty interactions. This
paper examines how different alignment methods affect LLM agents' effectiveness
as partners in multiturn, multiparty collaborations. We study this question
through the lens of friction agents that intervene in group dialogues to
encourage the collaborative group to slow down and reflect upon their reasoning
for deliberative decision-making. Using a roleplay methodology, we evaluate
interventions from differently-trained friction agents in collaborative task
conversations. We propose a novel counterfactual evaluation framework that
quantifies how friction interventions change the trajectory of group
collaboration and belief alignment. Our results show that a friction-aware
approach significantly outperforms common alignment baselines in helping both
convergence to a common ground, or agreed-upon task-relevant propositions, and
correctness of task outcomes.

</details>


### [30] [Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling](https://arxiv.org/abs/2509.05908)
*Yue Gu,Zhihao Du,Ying Shi,Shiliang Zhang,Qian Chen,Jiqing Han*

Main category: cs.CL

TL;DR: 本文提出了一种名为PSC-Joint的方法，通过计算和联合建模三种语义相关性来提高上下文ASR的性能，并通过净化机制减少计算成本。该方法在不同长度的偏置列表上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于交叉注意力的上下文自动语音识别（ASR）模型在识别个性化偏置短语方面取得了显著进展，但交叉注意力的有效性受到偏置信息量变化的影响，尤其是在偏置列表长度显著增加时。我们发现，无论偏置列表的长度如何，只有一小部分偏置信息对特定的ASR中间表示最为相关。因此，通过识别和整合最相关的偏置信息而不是整个偏置列表，可以缓解偏置信息量变化对上下文ASR的影响。

Method: 我们提出了纯化语义相关联合建模（PSC-Joint）方法，通过定义和计算从粗到细的三种语义相关性（列表级、短语级和标记级），并联合建模以产生它们的交集，从而突出和整合最相关的偏置信息。此外，我们还提出了一种基于分组和竞争策略的净化机制，以减少计算成本。

Result: 与基线方法相比，我们的PSC-Joint方法在AISHELL-1和KeSpeech数据集上的平均相对F1分数分别提高了21.34%和28.46%。

Conclusion: 我们的PSC-Joint方法在不同长度的偏置列表上实现了平均相对F1分数的显著提升，表明该方法能够有效缓解偏置信息量变化对上下文ASR的影响。

Abstract: Recently, cross-attention-based contextual automatic speech recognition (ASR)
models have made notable advancements in recognizing personalized biasing
phrases. However, the effectiveness of cross-attention is affected by
variations in biasing information volume, especially when the length of the
biasing list increases significantly. We find that, regardless of the length of
the biasing list, only a limited amount of biasing information is most relevant
to a specific ASR intermediate representation. Therefore, by identifying and
integrating the most relevant biasing information rather than the entire
biasing list, we can alleviate the effects of variations in biasing information
volume for contextual ASR. To this end, we propose a purified semantic
correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and
calculate three semantic correlations between the ASR intermediate
representations and biasing information from coarse to fine: list-level,
phrase-level, and token-level. Then, the three correlations are jointly modeled
to produce their intersection, so that the most relevant biasing information
across various granularities is highlighted and integrated for contextual
recognition. In addition, to reduce the computational cost introduced by the
joint modeling of three semantic correlations, we also propose a purification
mechanism based on a grouped-and-competitive strategy to filter out irrelevant
biasing phrases. Compared with baselines, our PSC-Joint approach achieves
average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%
on KeSpeech, across biasing lists of varying lengths.

</details>


### [31] [Accelerating Large Language Model Inference via Early-Exiting Algorithms](https://arxiv.org/abs/2509.05915)
*Sangmin Bae*

Main category: cs.CL

TL;DR: 本文通过共同设计自适应算法和模型架构，解决了动态计算与效率之间的冲突，提出了一个统一的框架，实现了效率和性能的新帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然取得了显著的能力，但其实际部署受到显著计算成本的阻碍。虽然自适应计算方法如早退出有望减少这些成本，但它们引入了一个基本矛盾：旨在节省计算的每个标记动态性常常在批量推理中造成系统级瓶颈，反而降低了吞吐量。

Method: 本文首先提出了一种高效的并行解码机制，以解决传统早退出中的关键开销问题；然后展示了深度参数共享作为架构基础，不仅产生了紧凑、参数高效的模型，还固有地缓解了影响动态推理的关键同步问题；最后，本文提出了一种轻量级路由器预训练框架，以动态分配每个标记的最佳递归深度。

Result: 本文提出的高效并行解码机制有效减少了传统早退出中的开销；深度参数共享不仅产生了紧凑、参数高效的模型，还缓解了动态推理中的同步问题；轻量级路由器预训练框架实现了自适应计算和参数效率的优化，建立了效率和性能的新帕累托前沿。

Conclusion: 本文通过共同设计自适应算法和模型架构，解决了动态计算与效率之间的冲突，提出了一个统一的框架，实现了效率和性能的新帕累托前沿。

Abstract: Large language models have achieved remarkable capabilities, but their
practical deployment is hindered by significant computational costs. While
adaptive computation methods like early-exiting promise to reduce these costs,
they introduce a fundamental conflict: the per-token dynamism intended to save
computation often creates system-level bottlenecks that can paradoxically
reduce throughput in batched inference. This dissertation resolves this
conflict by co-designing adaptive algorithms and model architectures to strike
an optimal balance between dynamism and efficiency. To this end, our work first
addresses critical sources of overhead in conventional early-exiting by
proposing an efficient parallel decoding mechanism. We then show that deep
parameter sharing provides an architectural foundation that not only yields
compact, parameter-efficient models but also inherently mitigates the critical
synchronization issues affecting dynamic inference. Finally, this work presents
a unified framework where lightweight routers are pretrained to dynamically
assign an optimal recursion depth for each token. This approach establishes a
new Pareto frontier between efficiency and performance by effectively
optimizing for both adaptive computation and parameter efficiency within a
single model.

</details>


### [32] [KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino](https://arxiv.org/abs/2509.06065)
*Lorenzo Alfred Nery,Ronald Dawson Catignas,Thomas James Tiam-Lee*

Main category: cs.CL

TL;DR: 研究提出了一个他加禄语的TruthfulQA基准，并评估了多个模型在多语言环境下的表现，发现存在性能差距和问题类型的差异，强调了需要更广泛的多语言评估。


<details>
  <summary>Details</summary>
Motivation: 为了填补在低资源语言中评估LLM的空白，我们提出了KatotohananQA，这是TruthfulQA基准的他加禄语翻译。

Method: 使用二元选择框架评估了七个免费级别的专有模型。

Result: 研究发现英语和他加禄语真实性之间存在显著性能差距，较新的OpenAI模型（GPT-5和GPT-5 mini）表现出强大的多语言鲁棒性。此外，还发现了不同问题特征之间的差异，这表明某些问题类型、类别和主题对多语言迁移的鲁棒性较低。

Conclusion: 研究结果表明，需要更广泛的多语言评估以确保LLM使用的公平性和可靠性。

Abstract: Large Language Models (LLMs) achieve remarkable performance across various
tasks, but their tendency to produce hallucinations limits reliable adoption.
Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet
they are primarily available in English, leaving a gap in evaluating LLMs in
low-resource languages. To address this, we present KatotohananQA, a Filipino
translation of the TruthfulQA benchmark. Seven free-tier proprietary models
were assessed using a binary-choice framework. Findings show a significant
performance gap between English and Filipino truthfulness, with newer OpenAI
models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.
Results also reveal disparities across question characteristics, suggesting
that some question types, categories, and topics are less robust to
multilingual transfer which highlight the need for broader multilingual
evaluation to ensure fairness and reliability in LLM usage.

</details>


### [33] [Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis](https://arxiv.org/abs/2509.06074)
*Zhenqi Jia,Rui Liu,Berrak Sisman,Haizhou Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的多模态细粒度上下文交互图-based的对话语音合成系统MFCIG-CSS，通过构建语义和韵律交互图来提升语音自然度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了MDH中细粒度语义和韵律交互建模的不足。

Method: 提出了一种基于多模态细粒度上下文交互图的CSS系统MFCIG-CSS，构建了两个专门的多模态细粒度对话交互图：语义交互图和韵律交互图。

Result: MFCIG-CSS在DailyTalk数据集上表现出优越的语音表现力。

Conclusion: MFCIG-CSS在DailyTalk数据集上的实验表明，它在语音表现力方面优于所有基线模型。

Abstract: Conversational Speech Synthesis (CSS) aims to generate speech with natural
prosody by understanding the multimodal dialogue history (MDH). The latest work
predicts the accurate prosody expression of the target utterance by modeling
the utterance-level interaction characteristics of MDH and the target
utterance. However, MDH contains fine-grained semantic and prosody knowledge at
the word level. Existing methods overlook the fine-grained semantic and
prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a
novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our
approach constructs two specialized multimodal fine-grained dialogue
interaction graphs: a semantic interaction graph and a prosody interaction
graph. These two interaction graphs effectively encode interactions between
word-level semantics, prosody, and their influence on subsequent utterances in
MDH. The encoded interaction features are then leveraged to enhance synthesized
speech with natural conversational prosody. Experiments on the DailyTalk
dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of
prosodic expressiveness. Code and speech samples are available at
https://github.com/AI-S2-Lab/MFCIG-CSS.

</details>


### [34] [Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge](https://arxiv.org/abs/2509.06079)
*Hao Liang,Ruitao Wu,Bohan Zeng,Junbo Niu,Wentao Zhang,Bin Dong*

Main category: cs.CL

TL;DR: 本文提出了一种基于标题的推理框架，用于解决多模态推理问题，并在多个基准测试中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 尽管在基于文本的推理方面取得了显著进展，但最先进的模型如GPT-o3在多模态场景中仍然表现不佳，因此需要一种更有效的解决方案。

Method: 本文引入了一种基于标题的推理框架，以解决多模态推理中的挑战。

Result: 该方法在ICML 2025 AI for Math Workshop & Challenge 2: SeePhys中获得了第一名，并在MathVerse基准测试中验证了其泛化能力。

Conclusion: 本文提出了一种基于标题的推理框架，能够有效地连接视觉和文本模态，并在多个基准测试中展示了其有效性和通用性。

Abstract: Multimodal reasoning remains a fundamental challenge in artificial
intelligence. Despite substantial advances in text-based reasoning, even
state-of-the-art models such as GPT-o3 struggle to maintain strong performance
in multimodal scenarios. To address this gap, we introduce a caption-assisted
reasoning framework that effectively bridges visual and textual modalities. Our
approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge
2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we
validate its generalization on the MathVerse benchmark for geometric reasoning,
demonstrating the versatility of our method. Our code is publicly available at
https://github.com/OpenDCAI/SciReasoner.

</details>


### [35] [Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models](https://arxiv.org/abs/2509.06100)
*Kefan Cao,Shuaicheng Wu*

Main category: cs.CL

TL;DR: 本文提出OLieRA方法，通过引入李群理论来保持大型语言模型参数的几何结构，从而有效缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在顺序多任务设置中容易发生灾难性遗忘，传统参数正则化方法忽略了添加微调会破坏模型参数的内在几何结构的问题。

Method: 提出了一种基于李群理论的正交低秩适应方法（OLieRA），利用乘法更新来保持参数几何结构，同时对任务子空间应用正交性约束。

Result: 实验表明，OLieRA在标准CL基准测试中表现优异，并在大量任务设置中保持了高性能。

Conclusion: OLieRA在标准CL基准测试中达到了最先进的结果，并在大量任务设置中保持了顶级性能。

Abstract: Large language models (LLMs) are prone to catastrophic forgetting in
sequential multi-task settings. Parameter regularization methods such as O-LoRA
and N-LoRA alleviate task interference by enforcing low-rank subspace
orthogonality, but they overlook the fact that conventional additive
fine-tuning disrupts the intrinsic geometric structure of LLM parameters,
limiting performance. Our key insight is that the parameter space of LLMs
possesses a geometric structure, which must be preserved in addition to
enforcing orthogonality. Based on this, we propose Orthogonal Low-rank
Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM
fine-tuning: leveraging multiplicative updates to preserve parameter geometry
while applying orthogonality constraints to task subspaces. Experiments
demonstrate that OLieRA achieves state-of-the-art results on the Standard CL
benchmark and remains among the top-performing methods in the Large Number of
Tasks setting.

</details>


### [36] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)
*Jinrui Yang,Xudong Han,Timothy Baldwin*

Main category: cs.CL

TL;DR: 本文介绍了EuroParlVote，一个用于评估大型语言模型在政治敏感情境下的新基准，揭示了模型在性别分类和投票预测中的偏见问题，并提出了改进的方向。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）在政治敏感情境下的表现，特别是关注公平性和偏见问题。

Method: 我们引入了EuroParlVote，这是一个用于评估大型语言模型（LLM）在政治敏感情境下的新基准。它将欧洲议会辩论演讲与投票结果联系起来，并为每位欧洲议会议员（MEP）提供了丰富的人口统计数据，如性别、年龄、国家和政治团体。使用EuroParlVote，我们在两个任务上评估了最先进的LLM——性别分类和投票预测，揭示了一致的偏见模式。

Result: 我们发现LLM经常将女性MEP错误分类为男性，并且在模拟女性发言人的投票时准确性降低。政治上，LLM倾向于支持中间派团体，而在极左派和极右派团体上的表现较差。专有模型如GPT-4o在鲁棒性和公平性方面优于开放权重的替代方案。

Conclusion: 我们释放了EuroParlVote数据集、代码和演示，以支持未来在政治语境下NLP公平性和问责制的研究。

Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language
models (LLMs) in politically sensitive contexts. It links European Parliament
debate speeches to roll-call vote outcomes and includes rich demographic
metadata for each Member of the European Parliament (MEP), such as gender, age,
country, and political group. Using EuroParlVote, we evaluate state-of-the-art
LLMs on two tasks -- gender classification and vote prediction -- revealing
consistent patterns of bias. We find that LLMs frequently misclassify female
MEPs as male and demonstrate reduced accuracy when simulating votes for female
speakers. Politically, LLMs tend to favor centrist groups while underperforming
on both far-left and far-right ones. Proprietary models like GPT-4o outperform
open-weight alternatives in terms of both robustness and fairness. We release
the EuroParlVote dataset, code, and demo to support future research on fairness
and accountability in NLP within political contexts.

</details>


### [37] [Understanding the Influence of Synthetic Data for Text Embedders](https://arxiv.org/abs/2509.06184)
*Jacob Mitchell Springer,Vaibhav Adlakha,Siva Reddy,Aditi Raghunathan,Marius Mosbach*

Main category: cs.CL

TL;DR: 本文重现并公开了合成数据，发现合成数据对模型泛化能力的提升有限，并且在不同任务之间存在性能权衡。


<details>
  <summary>Details</summary>
Motivation: 由于没有公开可用的合成数据集，这阻碍了研究其在泛化中的作用。因此，我们希望通过重现和发布合成数据来解决这个问题。

Method: 我们首先重现并公开发布了Wang等人提出的合成数据(Mistral-E5)。然后，我们仔细检查了合成数据在哪些地方提高了模型的泛化能力。

Result: 我们的合成数据质量高，并且在性能上带来了持续的提升。然而，合成数据带来的好处是稀疏的，主要集中在个别数据集上。此外，我们在不同类别之间的性能之间观察到了权衡。

Conclusion: 当前合成数据方法在构建通用文本嵌入器方面存在局限性，并挑战了在合成数据上训练能提高模型鲁棒性的观点。

Abstract: Recent progress in developing general purpose text embedders has been driven
by training on ever-growing corpora of synthetic LLM-generated data.
Nonetheless, no publicly available synthetic dataset exists, posing a barrier
to studying its role for generalization. To address this issue, we first
reproduce and publicly release the synthetic data proposed by Wang et al.
(Mistral-E5). Our synthetic data is high quality and leads to consistent
improvements in performance. Next, we critically examine where exactly
synthetic data improves model generalization. Our analysis reveals that
benefits from synthetic data are sparse and highly localized to individual
datasets. Moreover, we observe trade-offs between the performance on different
categories and data that benefits one task, degrades performance on another.
Our findings highlight the limitations of current synthetic data approaches for
building general-purpose embedders and challenge the notion that training on
synthetic data leads to more robust embedding models across tasks.

</details>


### [38] [Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation](https://arxiv.org/abs/2509.06196)
*Mohamed T. Younes,Omar Walid,Khaled Shaban,Ali Hamdi,Mai Hassan*

Main category: cs.CL

TL;DR: 本文提出了一种用于招聘自动化的新型方法，通过微调大型语言模型并构建合成数据集，提高了招聘任务的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有通用大型语言模型在招聘任务中存在局限性，因此需要一种专门针对招聘任务的微调方法，以提高性能和准确性。

Method: 本文提出了一种新的方法，通过微调大型语言模型（LLMs）来提高准确性与效率，并构建了一个标准化JSON格式的合成数据集以确保一致性与可扩展性。此外，还使用DeepSeek解析简历并将其放入训练集中以提高数据多样性和真实性。

Result: 实验表明，微调后的Phi-4模型在F1分数上达到了90.62%，显著优于基础模型和其他最先进的LLMs，显示出卓越的精确度和召回率。

Conclusion: 本研究突显了微调大型语言模型的潜力，并将通过提供更准确的候选人-职位匹配来彻底改变招聘流程。

Abstract: This paper presents a novel approach to recruitment automation. Large
Language Models (LLMs) were fine-tuned to improve accuracy and efficiency.
Building upon our previous work on the Multilayer Large Language Model-Based
Robotic Process Automation Applicant Tracking (MLAR) system . This work
introduces a novel methodology. Training fine-tuned LLMs specifically tuned for
recruitment tasks. The proposed framework addresses the limitations of generic
LLMs by creating a synthetic dataset that uses a standardized JSON format. This
helps ensure consistency and scalability. In addition to the synthetic data
set, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes
were parsed into the same structured JSON format and placed in the training
set. This will help improve data diversity and realism. Through
experimentation, we demonstrate significant improvements in performance
metrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall
similarity compared to base models and other state-of-the-art LLMs. In
particular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,
indicating exceptional precision and recall in recruitment tasks. This study
highlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize
recruitment workflows by providing more accurate candidate-job matching.

</details>


### [39] [MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment](https://arxiv.org/abs/2509.06200)
*Omar Walid,Mohamed T. Younes,Khaled Shaban,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: 本文提出了 MSLEF，这是一个多段集成框架，通过微调 LLM 提高简历解析的准确性，其分段感知设计增强了对不同简历布局的适应能力，并在多个指标上优于单模型系统。


<details>
  <summary>Details</summary>
Motivation: 现有的单模型系统在处理不同格式和结构的简历时存在局限性，需要一种更灵活和准确的方法来提升简历解析的性能。

Method: MSLEF 是一个采用 LLM 微调的多段集成框架，通过加权投票整合微调的大型语言模型，每个模型专门处理特定的简历部分以提高准确性。它引入了一个分段感知架构，利用针对每个简历部分的字段特定权重，克服了单模型系统的局限性。

Result: MSLEF 在 Exact Match (EM)、F1 分数、BLEU、ROUGE 和 Recruitment Similarity (RS) 指标上取得了显著改进，在 RS 上比最佳单模型提升了高达 +7%。

Conclusion: MSLEF 的分段感知设计提高了在各种简历布局上的泛化能力，使其在现实招聘场景中高度适应，并确保了候选人的精确和可靠表示。

Abstract: This paper presents MSLEF, a multi-segment ensemble framework that employs
LLM fine-tuning to enhance resume parsing in recruitment automation. It
integrates fine-tuned Large Language Models (LLMs) using weighted voting, with
each model specializing in a specific resume segment to boost accuracy.
Building on MLAR , MSLEF introduces a segment-aware architecture that leverages
field-specific weighting tailored to each resume part, effectively overcoming
the limitations of single-model systems by adapting to diverse formats and
structures. The framework incorporates Gemini-2.5-Flash LLM as a high-level
aggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4
14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,
BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best
single model by up to +7% in RS. Its segment-aware design enhances
generalization across varied resume layouts, making it highly adaptable to
real-world hiring scenarios while ensuring precise and reliable candidate
representation.

</details>


### [40] [No Encore: Unlearning as Opt-Out in Music Generation](https://arxiv.org/abs/2509.06277)
*Jinju Kim,Taehan Kim,Abdul Waheed,Rita Singh*

Main category: cs.CL

TL;DR: 本文研究了在音乐生成模型中应用机器遗忘技术的可行性，并分析了其有效性及挑战。


<details>
  <summary>Details</summary>
Motivation: AI音乐生成在创意产业中迅速发展，但这些系统可能涉及对受版权保护作品的滥用，引发伦理和法律问题。因此，需要研究如何防止无意中使用创作内容。

Method: 本文探索了现有的机器遗忘方法，并将其应用于预训练的文本到音乐（TTM）基线模型，以分析其在不损害模型性能的情况下遗忘预训练数据集的有效性。

Result: 通过实验，本文提供了在音乐生成中应用遗忘技术的挑战见解，并为未来的研究提供了基础分析。

Conclusion: 本文提供了在音乐生成中应用机器遗忘技术的初步结果，并为未来的研究提供了基础分析。

Abstract: AI music generation is rapidly emerging in the creative industries, enabling
intuitive music generation from textual descriptions. However, these systems
pose risks in exploitation of copyrighted creations, raising ethical and legal
concerns. In this paper, we present preliminary results on the first
application of machine unlearning techniques from an ongoing research to
prevent inadvertent usage of creative content. Particularly, we explore
existing methods in machine unlearning to a pre-trained Text-to-Music (TTM)
baseline and analyze their efficacy in unlearning pre-trained datasets without
harming model performance. Through our experiments, we provide insights into
the challenges of applying unlearning in music generation, offering a
foundational analysis for future works on the application of unlearning for
music generative models.

</details>


### [41] [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350)
*Junjie Mu,Zonghao Ying,Zhekui Fan,Zonglei Jing,Yaoyuan Zhang,Zhengmin Yu,Wenxin Zhang,Quanchen Zou,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Mask-GCG的方法，通过可学习的令牌掩码来减少LLM提示中的冗余，提高攻击效率。


<details>
  <summary>Details</summary>
Motivation: 现有的GCG变体依赖于固定长度的后缀，但其中的潜在冗余尚未被探索。

Method: 我们提出了Mask-GCG，这是一种插件方法，利用可学习的令牌掩码来识别后缀中的关键令牌。

Result: 实验结果表明，后缀中的大多数令牌对攻击成功有显著贡献，而修剪少数低影响的令牌不会影响损失值或损害攻击成功率。

Conclusion: 我们的研究揭示了LLM提示中的令牌冗余，并为开发高效且可解释的LLM提供了见解。

Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various
successful methods whereby attackers manipulate models into generating harmful
responses that they are designed to avoid. Among these, Greedy Coordinate
Gradient (GCG) has emerged as a general and effective approach that optimizes
the tokens in a suffix to generate jailbreakable prompts. While several
improved variants of GCG have been proposed, they all rely on fixed-length
suffixes. However, the potential redundancy within these suffixes remains
unexplored. In this work, we propose Mask-GCG, a plug-and-play method that
employs learnable token masking to identify impactful tokens within the suffix.
Our approach increases the update probability for tokens at high-impact
positions while pruning those at low-impact positions. This pruning not only
reduces redundancy but also decreases the size of the gradient space, thereby
lowering computational overhead and shortening the time required to achieve
successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the
original GCG and several improved variants. Experimental results show that most
tokens in the suffix contribute significantly to attack success, and pruning a
minority of low-impact tokens does not affect the loss values or compromise the
attack success rate (ASR), thereby revealing token redundancy in LLM prompts.
Our findings provide insights for developing efficient and interpretable LLMs
from the perspective of jailbreak attacks.

</details>


### [42] [PL-CA: A Parametric Legal Case Augmentation Framework](https://arxiv.org/abs/2509.06356)
*Ao Chang,Yubo Chen,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法PL-CA，通过参数化RAG框架和构建多任务法律数据集，解决了传统RAG方法在处理法律领域任务时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法仅将检索到的文档直接注入模型的上下文中，这严重限制了模型，因为它们的上下文窗口有限，并且通过过长的上下文引入了额外的计算开销，从而干扰了模型的注意力并降低了下游任务的性能。此外，许多现有的基准测试缺乏专家注释，只关注单一的下游任务，而现实世界的法律场景包含多个混合的法律任务，表明传统基准测试不足以反映模型的真实能力。

Method: 我们提出了PL-CA，引入了一个参数化RAG（P-RAG）框架，对语料库知识进行数据增强，并将这种法律知识编码为参数向量，然后通过LoRA将其整合到LLM的前馈网络（FFN）中，从而减轻模型的上下文压力。

Result: 我们在自己的数据集上进行了实验，实验结果表明，我们的方法减少了与过长上下文相关的开销，同时在下游任务上保持了与传统RAG相当的性能。

Conclusion: 我们的方法在保持下游任务竞争力的同时，减少了与过长上下文相关的开销。

Abstract: Conventional RAG is considered one of the most effective methods for
addressing model knowledge insufficiency and hallucination, particularly in the
judicial domain that requires high levels of knowledge rigor, logical
consistency, and content integrity. However, the conventional RAG method only
injects retrieved documents directly into the model's context, which severely
constrains models due to their limited context windows and introduces
additional computational overhead through excessively long contexts, thereby
disrupting models' attention and degrading performance on downstream tasks.
Moreover, many existing benchmarks lack expert annotation and focus solely on
individual downstream tasks while real-world legal scenarios consist of
multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for
reflecting models' true capabilities. To address these limitations, we propose
PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data
augmentation on corpus knowledge and encode this legal knowledge into
parametric vectors, and then integrates this parametric knowledge into the
LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context
pressure. Additionally, we also construct a multi-task legal dataset comprising
more than 2000 training and test instances, which are all expert-annotated and
manually verified. We conduct our experiments on our dataset, and the
experimental results demonstrate that our method reduces the overhead
associated with excessively long contexts while maintaining competitive
performance on downstream tasks compared to conventional RAG. Our code and
dataset are provided in the appendix.

</details>


### [43] [Do LLMs exhibit the same commonsense capabilities across languages?](https://arxiv.org/abs/2509.06401)
*Ivan Martínez-Murillo,Elena Lloret,Paloma Moreda,Albert Gatt*

Main category: cs.CL

TL;DR: 本文介绍了MULTICOM基准，用于评估大型语言模型在多语言常识生成任务中的表现，并发现英语表现最佳，而资源较少的语言表现较差。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型的多语言常识生成能力，并提供一个评估基准。

Method: 引入了MULTICOM基准，扩展了COCOTEROS数据集到四种语言，并评估了多种开源大型语言模型的表现。

Result: 英语表现优于其他语言，特别是在资源较少的语言中，上下文支持的效果不一。

Conclusion: 当前大型语言模型在多语言常识生成方面仍存在局限性。

Abstract: This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.

</details>


### [44] [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)
*Junteng Liu,Yunji Li,Chi Zhang,Jingyang Li,Aili Chen,Ke Ji,Weiyu Cheng,Zijia Wu,Chengyu Du,Qidi Xu,Jiayuan Song,Zhengmao Zhu,Wenhu Chen,Pengyu Zhao,Junxian He*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据生成方法WebExplorer，用于训练更强大的网络代理，该代理在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有开源网络代理在复杂任务中表现出有限的信息获取能力或缺乏透明实现。

Method: 我们引入了WebExplorer：一种使用基于模型的探索和迭代、长短查询演化的系统数据生成方法。

Result: WebExplorer-8B在各种信息获取基准测试中达到了其规模下的最先进性能，并且在某些任务上优于更大的模型。

Conclusion: 我们的方法为构建长视野网络代理提供了一条实用的路径。

Abstract: The paradigm of Large Language Models (LLMs) has increasingly shifted toward
agentic applications, where web browsing capabilities are fundamental for
retrieving information from diverse online sources. However, existing
open-source web agents either demonstrate limited information-seeking abilities
on complex tasks or lack transparent implementations. In this work, we identify
that the key challenge lies in the scarcity of challenging data for information
seeking. To address this limitation, we introduce WebExplorer: a systematic
data generation approach using model-based exploration and iterative,
long-to-short query evolution. This method creates challenging query-answer
pairs that require multi-step reasoning and complex web navigation. By
leveraging our curated high-quality dataset, we successfully develop advanced
web agent WebExplorer-8B through supervised fine-tuning followed by
reinforcement learning. Our model supports 128K context length and up to 100
tool calling turns, enabling long-horizon problem solving. Across diverse
information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art
performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able
to effectively search over an average of 16 turns after RL training, achieving
higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best
performance among models up to 100B parameters on WebWalkerQA and FRAMES.
Beyond these information-seeking tasks, our model also achieves strong
generalization on the HLE benchmark even though it is only trained on
knowledge-intensive QA data. These results highlight our approach as a
practical path toward long-horizon web agents.

</details>


### [45] [Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training](https://arxiv.org/abs/2509.06518)
*Andrei Baroian,Kasper Notebomer*

Main category: cs.CL

TL;DR: 本文提出了三种新的LWS变体，通过预训练阶段的线性插值重新分配FFN宽度和注意力头，在固定参数预算下实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的语言模型使用统一的层大小，忽略了不同深度的不同功能角色及其计算能力需求。

Method: 引入了三种新的LWS变体——Framed、Reverse和Crown，通过在预训练阶段进行两或三点线性插值来重新分配FFN宽度和注意力头。

Result: 所有模型收敛到相似的损失，并且相比等成本的各向同性基线取得了更好的性能，而训练吞吐量没有显著下降。

Conclusion: 这项工作是预训练层结构设计空间的初步探索，但未来的工作应将实验扩展到数量级更多的标记和参数，以全面评估其潜力。

Abstract: Transformer-based language models traditionally use uniform (isotropic) layer
sizes, yet they ignore the diverse functional roles that different depths can
play and their computational capacity needs. Building on Layer-Wise Scaling
(LWS) and pruning literature, we introduce three new LWS variants - Framed,
Reverse, and Crown - that redistribute FFN widths and attention heads via two
or three-point linear interpolation in the pre-training stage. We present the
first systematic ablation of LWS and its variants, on a fixed budget of 180M
parameters, trained on 5B tokens. All models converge to similar losses and
achieve better performance compared to an equal-cost isotropic baseline,
without a substantial decrease in training throughput. This work represents an
initial step into the design space of layer-wise architectures for
pre-training, but future work should scale experiments to orders of magnitude
more tokens and parameters to fully assess their potential.

</details>


### [46] [LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection](https://arxiv.org/abs/2509.06524)
*Jian Wu,Hang Yu,Bingchang Liu,Wenjie Yang,Peng Di,Jianguo Li,Yue Zhang*

Main category: cs.CL

TL;DR: LAMDAS is a novel approach that uses pre-trained LLMs as implicit classifiers for domain-specific data selection, achieving better performance and efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Adapting large language models (LLMs) to specific domains often faces a critical bottleneck: the scarcity of high-quality, human-curated data.

Method: LAMDAS leverages the pre-trained LLM itself as an implicit classifier, reframing data selection as a one-class classification problem.

Result: LAMDAS not only exceeds the performance of full-data training using a fraction of the data but also outperforms nine state-of-the-art (SOTA) baselines under various scenarios.

Conclusion: LAMDAS achieves the most compelling balance between performance gains and computational efficiency compared to all evaluated baselines.

Abstract: Adapting large language models (LLMs) to specific domains often faces a
critical bottleneck: the scarcity of high-quality, human-curated data. While
large volumes of unchecked data are readily available, indiscriminately using
them for fine-tuning risks introducing noise and degrading performance.
Strategic data selection is thus crucial, requiring a method that is both
accurate and efficient. Existing approaches, categorized as similarity-based
and direct optimization methods, struggle to simultaneously achieve these
goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for
domain-specific DAta Selection), a novel approach that leverages the
pre-trained LLM itself as an implicit classifier, thereby bypassing explicit
feature engineering and computationally intensive optimization process. LAMDAS
reframes data selection as a one-class classification problem, identifying
candidate data that "belongs" to the target domain defined by a small reference
dataset. Extensive experimental results demonstrate that LAMDAS not only
exceeds the performance of full-data training using a fraction of the data but
also outperforms nine state-of-the-art (SOTA) baselines under various
scenarios. Furthermore, LAMDAS achieves the most compelling balance between
performance gains and computational efficiency compared to all evaluated
baselines.

</details>


### [47] [SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion](https://arxiv.org/abs/2509.06531)
*Mengxue Yang,Chun Yang,Jiaqi Zhu,Jiafan Li,Jingqi Zhang,Yuyang Li,Ying Li*

Main category: cs.CL

TL;DR: 本文提出了SLiNT框架，通过注入结构信息和对比学习来解决知识图谱中链接预测的结构性稀疏和语义模糊问题。


<details>
  <summary>Details</summary>
Motivation: To address the challenges of structural sparsity and semantic ambiguity in link prediction in knowledge graphs, especially under incomplete or zero-shot settings.

Method: SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a modular framework that injects knowledge-graph-derived structural context into a frozen LLM backbone with lightweight LoRA-based adaptation for robust link prediction. Specifically, Structure-Guided Neighborhood Enhancement (SGNE), Dynamic Hard Contrastive Learning (DHCL), and Gradient-Decoupled Dual Injection (GDDI).

Result: Experiments on WN18RR and FB15k-237 show that SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines.

Conclusion: SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines, demonstrating the effectiveness of structure-aware representation learning for scalable knowledge graph completion.

Abstract: Link prediction in knowledge graphs requires integrating structural
information and semantic context to infer missing entities. While large
language models offer strong generative reasoning capabilities, their limited
exploitation of structural signals often results in structural sparsity and
semantic ambiguity, especially under incomplete or zero-shot settings. To
address these challenges, we propose SLiNT (Structure-aware Language model with
Injection and coNtrastive Training), a modular framework that injects
knowledge-graph-derived structural context into a frozen LLM backbone with
lightweight LoRA-based adaptation for robust link prediction. Specifically,
Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to
enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive
Learning (DHCL) introduces fine-grained supervision by interpolating hard
positives and negatives to resolve entity-level ambiguity; and
Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware
intervention while preserving the core LLM parameters. Experiments on WN18RR
and FB15k-237 show that SLiNT achieves superior or competitive performance
compared with both embedding-based and generation-based baselines,
demonstrating the effectiveness of structure-aware representation learning for
scalable knowledge graph completion.

</details>


### [48] [HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2509.06596)
*Xin Tong,Zhi Lin,Jingya Wang,Bo Jin*

Main category: cs.CL

TL;DR: HAVE是一个无需微调的参数无关解码框架，通过head-adaptive gating和value calibration解决大型语言模型中的幻觉问题，具有高效、广泛适用、透明和可重复的优点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在检索增强或长上下文生成中常常产生幻觉，即使存在相关证据，这源于两个问题：头部重要性被当作输入无关，原始注意力权重不能准确反映每个标记的真实贡献。

Method: HAVE框架引入了head-adaptive gating和value calibration两个模块，通过实例级别的软重新加权注意力头和增强注意力的值向量幅度来近似写回贡献，从而构建与模型更新对齐的token级证据，并通过轻量级的不确定性缩放策略将其与语言模型分布融合。

Result: 实验结果表明，HAVE框架在多个QA基准测试和LLM家族中一致减少了幻觉，并且优于强基线方法，如DAGCD，仅需适度的开销。

Conclusion: HAVE框架能够有效减少大型语言模型中的幻觉现象，并在多个问答基准测试中表现出色，同时具有透明性、可重复性和易于集成的特点，有助于在现实场景中实现更可信的生成。

Abstract: Large Language Models (LLMs) often produce hallucinations in
retrieval-augmented or long-context generation, even when relevant evidence is
present. This stems from two issues: head importance is treated as
input-agnostic, and raw attention weights poorly reflect each token's true
contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a
parameter-free decoding framework that directly addresses both challenges. HAVE
introduces head-adaptive gating, which performs instance-level soft reweighing
of attention heads, and value calibration, which augments attention with the
magnitude of value vectors to approximate write-back contribution. Together,
these modules construct token-level evidence aligned with model updates and
fuse it with the LM distribution through a lightweight uncertainty-scaled
policy. HAVE requires no finetuning and operates in a single forward pass,
making it efficient and broadly applicable. Experiments across multiple QA
benchmarks and LLM families demonstrate that HAVE consistently reduces
hallucinations and outperforms strong baselines, including DAGCD, with modest
overhead. The framework is transparent, reproducible, and readily integrates
with off-the-shelf LLMs, advancing trustworthy generation in real-world
settings.

</details>


### [49] [Guided Decoding and Its Critical Role in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06631)
*Özgür Uğur,Musa Yılmaz,Esra Şavirdi,Özay Ezerceli,Mahmut El Huseyni,Selva Taş,Reyhan Bayraktar*

Main category: cs.CL

TL;DR: 本研究探讨了引导解码在RAG系统中的作用，比较了三种方法在不同多轮提示设置下的表现，发现多轮交互对引导解码有显著影响，并提供了理论和实践指导。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLMs）集成到各种应用中，需要结构化且可靠的结果。在RAG系统中，确保输出与预期格式一致并减少幻觉是一个关键挑战。

Method: 本研究考察了引导解码在RAG系统中的作用，并比较了三种方法（Outlines、XGrammar和LM Format Enforcer）在不同多轮提示设置（0轮、1轮和2轮）下的表现。

Result: 研究结果揭示了多轮交互如何影响引导解码，并发现了意外的表现差异，这些差异可以为特定用例的方法选择提供信息。

Conclusion: 本研究加深了对RAG系统中结构化输出生成的理解，为LLM的部署提供了理论见解和实践指导。

Abstract: The integration of Large Language Models (LLMs) into various applications has
driven the need for structured and reliable responses. A key challenge in
Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align
with expected formats while minimizing hallucinations. This study examines the
role of guided decoding in RAG systems, comparing three methods, Outlines,
XGrammar, and LM Format Enforcer, across different multi-turn prompting setups
(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,
and output quality, we provide insights into their performance and
applicability. Our findings reveal how multi-turn interactions influence guided
decoding, uncovering unexpected performance variations that can inform method
selection for specific use cases. This work advances the understanding of
structured output generation in RAG systems, offering both theoretical insights
and practical guidance for LLM deployment.

</details>


### [50] [Modelling Intertextuality with N-gram Embeddings](https://arxiv.org/abs/2509.06637)
*Yi Xing*

Main category: cs.CL

TL;DR: 本文提出了一种新的定量模型来分析互文性，通过成对比较n-gram嵌入并平均结果，验证和测试显示该方法有效且高效，能够捕捉和量化互文关系。


<details>
  <summary>Details</summary>
Motivation: 为了实现可扩展的分析和基于网络的见解，需要一种新的定量模型来研究互文性。

Method: 提出了一种新的定量模型，通过成对比较两个文本的n-gram嵌入并平均结果来计算整体互文性。

Result: 验证和可扩展性测试表明该方法有效且高效，网络分析进一步揭示了中心性和社区结构。

Conclusion: 该方法在验证和可扩展性测试中表现出有效性，能够成功捕捉和量化互文关系。

Abstract: Intertextuality is a central tenet in literary studies. It refers to the
intricate links between literary texts that are created by various types of
references. This paper proposes a new quantitative model of intertextuality to
enable scalable analysis and network-based insights: perform pairwise
comparisons of the embeddings of n-grams from two texts and average their
results as the overall intertextuality. Validation on four texts with known
degrees of intertextuality, alongside a scalability test on 267 diverse texts,
demonstrates the method's effectiveness and efficiency. Network analysis
further reveals centrality and community structures, affirming the approach's
success in capturing and quantifying intertextual relationships.

</details>


### [51] [Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval](https://arxiv.org/abs/2509.06650)
*Hao Lin,Peitong Xie,Jingxue Chen,Jie Lin,Qingkun Tang,Qianchun Lu*

Main category: cs.CL

TL;DR: MoLER是用于检索增强生成（RAG）系统的领域感知方法，通过使用MoL增强的强化学习优化检索。它包含两个阶段：使用混合损失（MoL）进行持续预训练（CPT）以平衡领域特定知识和通用语言能力，以及利用组相对策略优化（GRPO）进行强化学习（RL）阶段，以优化查询和段落生成以最大化文档召回率。关键创新是多查询单段落后期融合（MSLF）策略，这在RL训练期间减少了计算开销，同时通过多查询多段落后期融合（MMLF）保持可扩展推理。在基准数据集上的实验表明，MoLER实现了最先进的性能，显著优于基线方法。MoLER弥合了RAG系统中的知识差距，使专业领域的稳健和可扩展检索成为可能。


<details>
  <summary>Details</summary>
Motivation: Existing coarse-ranking optimization approaches often struggle to balance domain-specific knowledge learning with query enhancement, resulting in suboptimal retrieval performance.

Method: MoLER is a domain-aware RAG method that uses MoL-Enhanced Reinforcement Learning to optimize retrieval. It has a two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of Losses (MoL) to balance domain-specific knowledge with general language capabilities, and a reinforcement learning (RL) phase leveraging Group Relative Policy Optimization (GRPO) to optimize query and passage generation for maximizing document recall. A key innovation is our Multi-query Single-passage Late Fusion (MSLF) strategy, which reduces computational overhead during RL training while maintaining scalable inference via Multi-query Multi-passage Late Fusion (MMLF).

Result: Extensive experiments on benchmark datasets show that MoLER achieves state-of-the-art performance, significantly outperforming baseline methods.

Conclusion: MoLER bridges the knowledge gap in RAG systems, enabling robust and scalable retrieval in specialized domains.

Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval
stage, particularly the coarse-ranking process. Existing coarse-ranking
optimization approaches often struggle to balance domain-specific knowledge
learning with query enhencement, resulting in suboptimal retrieval performance.
To address this challenge, we propose MoLER, a domain-aware RAG method that
uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a
two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of
Losses (MoL) to balance domain-specific knowledge with general language
capabilities, and a reinforcement learning (RL) phase leveraging Group Relative
Policy Optimization (GRPO) to optimize query and passage generation for
maximizing document recall. A key innovation is our Multi-query Single-passage
Late Fusion (MSLF) strategy, which reduces computational overhead during RL
training while maintaining scalable inference via Multi-query Multi-passage
Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER
achieves state-of-the-art performance, significantly outperforming baseline
methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and
scalable retrieval in specialized domains.

</details>


### [52] [IntrEx: A Dataset for Modeling Engagement in Educational Conversations](https://arxiv.org/abs/2509.06652)
*Xingwei Tan,Mahathi Parvatham,Chiara Gambi,Gabriele Pergola*

Main category: cs.CL

TL;DR: 研究引入了IntrEx数据集，用于分析教师-学生互动中的有趣性，并发现经过有趣性评分微调的大规模语言模型在预测人类有趣性判断方面优于更大的专有模型，同时分析了语言和认知因素如何影响教育对话中的参与度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补关于教育对话中驱动参与度的语言特征的知识空白。

Method: 研究引入了IntrEx数据集，这是第一个对教师-学生互动中的有趣性和预期有趣性进行标注的大规模数据集。通过比较基于评分的方法，结合强化学习从人类反馈中学习的思路，进行了严格的标注过程，并使用大规模语言模型分析了语言和认知因素如何影响教育对话中的参与度。

Result: 研究发现，经过有趣性评分微调的大规模语言模型（7B/8B参数）在预测人类有趣性判断方面优于更大的专有模型如GPT-4o。此外，研究还分析了语言和认知因素（如具体性、可理解性、回应）如何影响教育对话中的参与度。

Conclusion: 研究发现，经过有趣性评分微调的大规模语言模型（7B/8B参数）在预测人类有趣性判断方面优于更大的专有模型如GPT-4o，表明专门数据集在教育环境中建模参与度的潜力。

Abstract: Engagement and motivation are crucial for second-language acquisition, yet
maintaining learner interest in educational conversations remains a challenge.
While prior research has explored what makes educational texts interesting,
still little is known about the linguistic features that drive engagement in
conversations. To address this gap, we introduce IntrEx, the first large
dataset annotated for interestingness and expected interestingness in
teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus
(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,
allowing for the study of engagement beyond isolated turns to capture how
interest evolves over extended dialogues. We employ a rigorous annotation
process with over 100 second-language learners, using a comparison-based rating
approach inspired by reinforcement learning from human feedback (RLHF) to
improve agreement. We investigate whether large language models (LLMs) can
predict human interestingness judgments. We find that LLMs (7B/8B parameters)
fine-tuned on interestingness ratings outperform larger proprietary models like
GPT-4o, demonstrating the potential for specialised datasets to model
engagement in educational settings. Finally, we analyze how linguistic and
cognitive factors, such as concreteness, comprehensibility (readability), and
uptake, influence engagement in educational dialogues.

</details>


### [53] [ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data](https://arxiv.org/abs/2509.06675)
*Vladislav Stankov,Matyáš Kopp,Ondřej Bojar*

Main category: cs.CL

TL;DR: ParCzech4Speech 1.0 是一个处理过的捷克议会演讲数据集，包含三种变体，适用于语音建模任务，并在 LINDAT 仓库中发布。


<details>
  <summary>Details</summary>
Motivation: 改进 ParCzech 3.0 的语音识别版本，以提取更多数据并提高对齐可靠性。

Method: 将捷克议会演讲的音频记录与官方转录文本结合，并使用 WhisperX 和 Wav2Vec 2.0 提取自动的音频-文本对齐。

Result: 提供了三种变体：(1) 句子分段用于自动语音识别和语音合成任务；(2) 未分段保留原始话语流；(3) 原始对齐用于进一步自定义优化。所有变体都保持原始元数据。

Conclusion: ParCzech4Speech 1.0 是一个针对语音建模任务的处理版本，提供了三种灵活的变体，并在 LINDAT 仓库中发布。

Abstract: We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0
corpus, targeted at speech modeling tasks with the largest variant containing
2,695 hours. We combined the sound recordings of the Czech parliamentary
speeches with the official transcripts. The recordings were processed with
WhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our
processing pipeline improves upon the ParCzech 3.0 speech recognition version
by extracting more data with higher alignment reliability. The dataset is
offered in three flexible variants: (1) sentence-segmented for automatic speech
recognition and speech synthesis tasks with clean boundaries, (2) unsegmented
preserving original utterance flow across sentences, and (3) a raw-alignment
for further custom refinement for other possible tasks. All variants maintain
the original metadata and are released under a permissive CC-BY license. The
dataset is available in the LINDAT repository, with the sentence-segmented and
unsegmented variants additionally available on Hugging Face.

</details>


### [54] [Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments](https://arxiv.org/abs/2509.06704)
*Amir Homayounirad,Enrico Liscio,Tong Wang,Catholijn M. Jonker,Luciano C. Siebert*

Main category: cs.CL

TL;DR: 本文探讨了通过直接识别主观性来提高模型性能的方法，并发现结合对比损失和二元交叉熵损失并未提升性能。


<details>
  <summary>Details</summary>
Motivation: 聚合多个注释到一个真实标签可能会隐藏关于注释者分歧的有价值见解，特别是在涉及主观性的任务中。

Method: 我们探索了两种主要方法：通过价值预测推断主观性与直接识别主观性。

Result: 直接主观性识别显著提高了标记主观论点的模型性能。此外，将对比损失与二元交叉熵损失结合并没有提高性能，但减少了对每标签主观性的依赖。

Conclusion: 我们的方法可以帮助识别个体可能不同解释的论点，促进更细致的标注过程。

Abstract: Aggregating multiple annotations into a single ground truth label may hide
valuable insights into annotator disagreement, particularly in tasks where
subjectivity plays a crucial role. In this work, we explore methods for
identifying subjectivity in recognizing the human values that motivate
arguments. We evaluate two main approaches: inferring subjectivity through
value prediction vs. directly identifying subjectivity. Our experiments show
that direct subjectivity identification significantly improves the model
performance of flagging subjective arguments. Furthermore, combining
contrastive loss with binary cross-entropy loss does not improve performance
but reduces the dependency on per-label subjectivity. Our proposed methods can
help identify arguments that individuals may interpret differently, fostering a
more nuanced annotation process.

</details>


### [55] [Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint](https://arxiv.org/abs/2509.06795)
*Yanrui Du,Fenglei Fan,Sendong Zhao,Jiawei Cao,Qika Lin,Kai He,Ting Liu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 本文提出ProCon方法，通过投影约束损失项和预热策略来减轻IFT导致的安全风险，同时保持任务性能，并稳定r-direction。


<details>
  <summary>Details</summary>
Motivation: Instruction Fine-Tuning (IFT) 虽然有效，但会显著影响LLMs的安全性，特别是拒绝恶意指令的能力。研究发现r-direction在隐藏状态中起着关键作用，但在训练过程中容易漂移，导致安全风险。

Method: 我们提出了ProCon方法，引入了投影约束损失项，以规范每个训练样本的隐藏状态在r-direction上的投影幅度。此外，我们还引入了预热策略，强调早期阶段的强约束并扩展数据分布以增强约束信号。

Result: 实验结果表明，我们的方法在各种数据集、场景和LLMs上都能显著减轻由IFT引起的安全风险，同时保持任务性能的提升。与强基线相比，我们的方法始终表现出更优的整体性能。

Conclusion: 我们的方法可以显著减轻IFT带来的安全风险，同时保持任务性能的提升。ProCon有助于在训练过程中稳定r-direction，为未来的安全研究奠定了坚实的基础。

Abstract: Instruction Fine-Tuning (IFT) has been widely adopted as an effective
post-training strategy to enhance various abilities of Large Language Models
(LLMs). However, prior studies have shown that IFT can significantly compromise
LLMs' safety, particularly their ability to refuse malicious instructions,
raising significant concerns. Recent research into the internal mechanisms of
LLMs has identified the refusal direction (r-direction) in the hidden states,
which plays a pivotal role in governing refusal behavior. Building on this
insight, our study reveals that the r-direction tends to drift during training,
which we identify as one of the causes of the associated safety risks. To
mitigate such drift, our proposed ProCon method introduces a
projection-constrained loss term that regularizes the projection magnitude of
each training sample's hidden state onto the r-direction. Our initial analysis
shows that applying an appropriate constraint can effectively mitigate the
refusal direction drift and associated safety risks, but remains limited by
overall performance barriers. To overcome this barrier, informed by our
observation of early-stage sharp drift and a data-driven perspective, we
introduce a warm-up strategy that emphasizes early-stage strong constraints and
broaden the data distribution to strengthen constraint signals, leading to an
enhanced ProCon method. Experimental results under various datasets, scenarios,
and LLMs demonstrate that our method can significantly mitigate safety risks
posed by IFT while preserving task performance gains. Even compared with strong
baselines, our method consistently delivers superior overall performance.
Crucially, our analysis indicates that ProCon can contribute to stabilizing the
r-direction during training, while such an interpretability-driven exploration
of LLMs' internal mechanisms lays a solid foundation for future safety
research.

</details>


### [56] [MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML](https://arxiv.org/abs/2509.06806)
*Haoyu Dong,Pengkun Zhang,Mingzhe Lu,Yanzhen Shen,Guolin Ke*

Main category: cs.CL

TL;DR: MachineLearningLM是一种便携式继续预训练框架，使通用大型语言模型具备强大的上下文学习能力，同时保持其广泛的知识和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在标准机器学习任务中难以通过上下文学习（ICL）利用许多示例，因此需要一种能够增强其上下文学习能力的框架。

Method: 通过从数百万个结构性因果模型中合成机器学习任务，使用随机森林教师将基于树的决策策略提炼到LLM中，并采用高效的提示序列化方法。

Result: MachineLearningLM在金融、物理、生物和医疗领域等分布外表格分类任务上平均比强基线模型（如GPT-5-mini）高出约15%。

Conclusion: MachineLearningLM在不进行任务特定训练的情况下，达到了随机森林级别的准确性，并且保留了通用聊天能力。

Abstract: Large language models (LLMs) possess broad world knowledge and strong
general-purpose reasoning ability, yet they struggle to learn from many
in-context examples on standard machine learning (ML) tasks, that is, to
leverage many-shot demonstrations purely via in-context learning (ICL) without
gradient descent. We introduce MachineLearningLM, a portable
continued-pretraining framework that equips a general-purpose LLM with robust
in-context ML capability while preserving its general knowledge and reasoning
for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural
causal models (SCMs), spanning shot counts up to 1,024. We begin with a
random-forest teacher, distilling tree-based decision strategies into the LLM
to strengthen robustness in numerical modeling. All tasks are serialized with a
token-efficient prompt, enabling 3x to 6x more examples per context window and
delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),
MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an
average of about 15% on out-of-distribution tabular classification across
finance, physics, biology, and healthcare domains. It exhibits a striking
many-shot scaling law: accuracy increases monotonically as in-context
demonstrations grow from 8 to 1,024. Without any task-specific training, it
attains random-forest-level accuracy across hundreds of shots. General chat
capabilities, including knowledge and reasoning, are preserved: it achieves
75.4% on MMLU.

</details>


### [57] [MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security](https://arxiv.org/abs/2509.06807)
*Yanrui Du,Fenglei Fan,Sendong Zhao,Jiawei Cao,Ting Liu,Bing Qin*

Main category: cs.CL

TL;DR: 本文提出了 MoGU_v2 框架，以提高 LLMs 的安全性和可用性，同时避免权衡。


<details>
  <summary>Details</summary>
Motivation: 解决 LLMs 在安全性和可用性之间的权衡问题，提高实际可用性。

Method: 提出 MoGU 框架和改进的 MoGU_v2 框架，通过动态分配权重和更紧密的路由器与隐藏状态耦合来平衡安全性和可用性。

Result: MoGU_v2 在各种 LLMs 上表现出强大的适应性和稳定的提升，并能通过简单的数据混合策略恢复安全性而不影响任务性能。

Conclusion: MoGU_V2 是一种强大且通用的解决方案，可以减轻现实应用中的安全风险。

Abstract: As Large Language Models (LLMs) increasingly permeate human life, their
security has emerged as a critical concern, particularly their ability to
maintain harmless responses to malicious instructions. Although extensive
methods have improved LLMs' security, they often lead to conservative,
rejection-oriented responses that compromise practical usability. This presents
a key challenge: how to advance the Pareto frontier between LLMs' usability and
security, rather than necessitate a trade-off between them. To address this, we
propose the MoGU framework, in which the intra-layer router dynamically
allocates weights by sensing hidden states, thereby balancing the contributions
of security-optimized and usability-optimized variants. Despite its initial
potential, the MoGU framework faces limitations such as parameter redundancy
and performance bottlenecks. To overcome these, we further propose an improved
MoGU_v2 framework that establishes a tighter coupling between the routers and
hidden states. In MoGU_v2, routers are embedded only in layers encoding highly
classifiable security features, and backbone modules are activated during
router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong
adaptability and stable improvements across various series of LLMs, including
mainstream LLMs serving as brains in various applications, on-device LLMs
optimized for resource-constrained scenarios, and reasoning LLMs tailored for
user interpretability. Meanwhile, even facing risks introduced by Instruction
Fine-tuning, MoGU_v2 can easily restore security without compromising the task
performance gains via a simple data-mix strategy. These comprehensive
improvements highlight MoGU_V2 as a robust and versatile solution for
mitigating security risks in real-world applications.

</details>


### [58] [Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem](https://arxiv.org/abs/2509.06809)
*Valentin Quesnel,Damien Sileo*

Main category: cs.CL

TL;DR: 本文提出了一种利用自动定理证明研究生成高质量数学推理数据的方法，以解决大型语言模型在深度结构推理任务中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 高质、逻辑正确的数据稀缺是推动大型语言模型（LLM）数学推理的关键瓶颈。我们的工作通过将数十年的自动定理证明研究转化为可扩展的数据引擎来应对这一挑战。

Method: 我们的框架利用E-prover的饱和能力在庞大的TPTP公理库上推导出一个大规模、保证有效的定理语料库。我们的流程是原则性的和简单的：饱和公理，过滤出“有趣”的定理，并生成任务。

Result: 我们的零样本实验显示，前沿模型在需要深度结构推理的任务上性能急剧下降。

Conclusion: 我们的框架提供了一种诊断工具来衡量这一差距，并且提供了一个可扩展的符号训练数据源来解决它。

Abstract: The scarcity of high-quality, logically sound data is a critical bottleneck
for advancing the mathematical reasoning of Large Language Models (LLMs). Our
work confronts this challenge by turning decades of automated theorem proving
research into a scalable data engine. Rather than relying on error-prone LLMs
or complex proof-assistant syntax like Lean and Isabelle, our framework
leverages E-prover's saturation capabilities on the vast TPTP axiom library to
derive a massive, guaranteed-valid corpus of theorems. Our pipeline is
principled and simple: saturate axioms, filter for "interesting" theorems, and
generate tasks. With no LLMs in the loop, we eliminate factual errors by
construction. This purely symbolic data is then transformed into three
difficulty-controlled challenges: entailment verification, premise selection,
and proof reconstruction. Our zero-shot experiments on frontier models reveal a
clear weakness: performance collapses on tasks requiring deep, structural
reasoning. Our framework provides both the diagnostic tool to measure this gap
and a scalable source of symbolic training data to address it. We make the code
and data publicly available.
  https://github.com/sileod/reasoning_core
https://hf.co/datasets/reasoning-core/rc1

</details>


### [59] [A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs](https://arxiv.org/abs/2509.06813)
*Max Malyi,Jonathan Shek,Alasdair McDonald,Andre Biscaya*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，用于评估大型语言模型在分类风力涡轮机维护日志方面的表现，并发现人机协同系统是当前最有效的应用方式。


<details>
  <summary>Details</summary>
Motivation: 有效的运行和维护对于降低风能的平准化能源成本至关重要，但风力涡轮机维护日志的非结构化、自由文本性质给自动化分析带来了重大障碍。

Method: 我们提出了一个新颖且可重复的框架，用于在分类这些复杂的工业记录任务上对大型语言模型进行基准测试，并系统评估了一系列最先进的专有和开源大型语言模型。

Result: 我们的结果量化了一个明确的性能层次结构，确定了与基准标准高度一致且可信、校准良好的置信度分数的顶级模型。我们还表明，分类性能高度依赖于任务的语义模糊性，所有模型在客观组件识别上的共识高于解释性维护操作。

Conclusion: 我们得出结论，近期最有效和负责任的应用是人机协同系统，其中大型语言模型作为强大的助手，加速并标准化数据标注，从而提高运维数据质量和下游可靠性分析。

Abstract: Effective Operation and Maintenance (O&M) is critical to reducing the
Levelised Cost of Energy (LCOE) from wind power, yet the unstructured,
free-text nature of turbine maintenance logs presents a significant barrier to
automated analysis. Our paper addresses this by presenting a novel and
reproducible framework for benchmarking Large Language Models (LLMs) on the
task of classifying these complex industrial records. To promote transparency
and encourage further research, this framework has been made publicly available
as an open-source tool. We systematically evaluate a diverse suite of
state-of-the-art proprietary and open-source LLMs, providing a foundational
assessment of their trade-offs in reliability, operational efficiency, and
model calibration. Our results quantify a clear performance hierarchy,
identifying top models that exhibit high alignment with a benchmark standard
and trustworthy, well-calibrated confidence scores. We also demonstrate that
classification performance is highly dependent on the task's semantic
ambiguity, with all models showing higher consensus on objective component
identification than on interpretive maintenance actions. Given that no model
achieves perfect accuracy and that calibration varies dramatically, we conclude
that the most effective and responsible near-term application is a
Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate
and standardise data labelling for human experts, thereby enhancing O&M data
quality and downstream reliability analysis.

</details>


### [60] [COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens](https://arxiv.org/abs/2509.06836)
*Eugene Kwek,Wenpeng Yin*

Main category: cs.CL

TL;DR: COMPACT是一种新的剪枝方法，通过修剪稀有词汇和FFN中间通道，在保持标准Transformer架构的同时，提高了模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 提高LLM在内存、延迟和服务成本方面的效率对于边缘部署、交互应用和大规模可持续推理至关重要。现有剪枝方法存在局限性，如宽度剪枝破坏标准Transformer布局或需要自定义推理代码，深度剪枝移除整个层可能导致准确率骤降。

Method: COMPACT联合修剪稀有词汇以缩小嵌入/解码器，并使用常见标记加权激活修剪FFN中间通道，使重要性与修剪后的标记分布对齐。

Result: 在Qwen、LLaMA和Gemma系列（0.5B-70B）上的实验显示，COMPACT在相似或更高的剪枝比例下实现了最先进的下游任务性能，显著减少了参数、GPU内存和端到端延迟。

Conclusion: COMPACT在减少参数、GPU内存和端到端延迟方面表现出色，同时保持了标准的Transformer架构，适用于各种规模的模型。

Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial
for edge deployment, interactive applications, and sustainable inference at
scale. Pruning is a key technique toward this goal. However, prior pruning
methods are limited: width pruning often breaks the standard transformer layout
or requires custom inference code, while depth pruning removes entire layers
and can cause abrupt accuracy drops. In this work, we propose COMPACT, which
jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)
prunes FFN intermediate channels using common-token-weighted activations,
aligning importance with the post-pruning token distribution. COMPACT enjoys
merits of both depth and width pruning, such as: deployment-friendliness (keeps
a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN
pruning), training-free operation with competitive pruning time, and strong
memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and
Gemma families (0.5B-70B) show state-of-the-art downstream task performance at
similar or higher pruning ratios, with substantial reductions in parameters,
GPU memory, and end-to-end latency.

</details>


### [61] [EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models](https://arxiv.org/abs/2509.06838)
*Mohammad Reza Mirbagheri,Mohammad Mahdi Mirkamali,Zahra Motoshaker Arani,Ali Javeri,Amir Mahdi Sadeghzadeh,Rasool Jalili*

Main category: cs.CL

TL;DR: 本研究引入了EPT（波斯可信度评估）指标，用于评估大型语言模型在六个关键方面的可信度。研究发现模型在安全维度上存在显著不足，并提供了关于模型与波斯伦理文化价值观对齐的见解。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型的可信度是一个关键挑战，因为可靠性不仅对于准确性能至关重要，而且对于维护伦理、文化和社会价值也很重要。需要仔细调整训练数据和基于文化的评估标准，以开发负责任的人工智能系统。

Method: 引入了EPT（波斯可信度评估）指标，这是一个文化相关的基准，专门用于评估LLMs在六个关键方面（真实性、安全性、公平性、鲁棒性、隐私和道德对齐）的可信度。我们整理了一个标记数据集，并使用自动化的LLM和人工评估方法评估了几种领先的模型。

Result: 研究结果揭示了模型在安全维度上的显著不足，强调了对此关键方面的关注的紧迫性。此外，研究结果提供了这些模型与波斯伦理文化价值观对齐的有价值见解，并突出了推进值得信赖和文化负责任的人工智能的关键差距和机会。

Conclusion: 研究结果揭示了模型在安全维度上的显著不足，强调了对此关键方面的关注的紧迫性。此外，研究结果提供了这些模型与波斯伦理文化价值观对齐的有价值见解，并突出了推进值得信赖和文化负责任的人工智能的关键差距和机会。

Abstract: Large Language Models (LLMs), trained on extensive datasets using advanced
deep learning architectures, have demonstrated remarkable performance across a
wide range of language tasks, becoming a cornerstone of modern AI technologies.
However, ensuring their trustworthiness remains a critical challenge, as
reliability is essential not only for accurate performance but also for
upholding ethical, cultural, and social values. Careful alignment of training
data and culturally grounded evaluation criteria are vital for developing
responsible AI systems. In this study, we introduce the EPT (Evaluation of
Persian Trustworthiness) metric, a culturally informed benchmark specifically
designed to assess the trustworthiness of LLMs across six key aspects:
truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We
curated a labeled dataset and evaluated the performance of several leading
models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and
Qwen - using both automated LLM-based and human assessments. Our results reveal
significant deficiencies in the safety dimension, underscoring the urgent need
for focused attention on this critical aspect of model behavior. Furthermore,
our findings offer valuable insights into the alignment of these models with
Persian ethical-cultural values and highlight critical gaps and opportunities
for advancing trustworthy and culturally responsible AI. The dataset is
publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.

</details>


### [62] [The Majority is not always right: RL training for solution aggregation](https://arxiv.org/abs/2509.06870)
*Wenting Zhao,Pranjal Aggarwal,Swarnadeep Saha,Asli Celikyilmaz,Jason Weston,Ilia Kulikov*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习学习聚合的方法AggLM，能够在多个基准测试中超越现有方法，并有效泛化到不同模型的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的聚合方法（如多数投票或奖励模型排序）可能只能带来有限的好处，因此需要一种更有效的聚合方法。

Method: 本文提出了一种通过强化学习从可验证奖励中学习聚合的方法，训练一个聚合模型来审查、协调并合成最终的正确答案。

Result: 实验结果表明，AggLM在多个基准测试中表现优于强规则基线和奖励模型基线，并且能够有效泛化到不同模型的解决方案。

Conclusion: 本文提出了一种新的聚合方法AggLM，它在多个基准测试中表现优于基于规则和奖励模型的基线方法，并且能够有效地泛化到不同模型的解决方案。

Abstract: Scaling up test-time compute, by generating multiple independent solutions
and selecting or aggregating among them, has become a central paradigm for
improving large language models (LLMs) on challenging reasoning tasks. While
most prior work relies on simple majority voting or reward model ranking to
aggregate solutions, these approaches may only yield limited benefits. In this
work, we propose to learn aggregation as an explicit reasoning skill: given a
set of candidate solutions, we train an aggregator model to review, reconcile,
and synthesize a final, correct answer using reinforcement learning from
verifiable rewards. A key ingredient is careful balancing of easy and hard
training examples, allowing the model to learn both to recover
minority-but-correct answers as well as easy majority-correct answers.
Empirically, we find our method, AggLM, outperforms both strong rule-based and
reward-model baselines, across multiple benchmarks. Furthermore, it generalizes
effectively to solutions from differing models, including stronger ones than
contained in the training data, all while requiring substantially fewer tokens
than majority voting with larger numbers of solutions.

</details>


### [63] [UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction](https://arxiv.org/abs/2509.06883)
*Joe Wilder,Nikhil Kadapala,Benji Xu,Mohammed Alsaadi,Aiden Parsons,Mitchell Rogers,Palash Agarwal,Adam Hassick,Laura Dietz*

Main category: cs.CL

TL;DR: 我们参与了CheckThat! Task 2 English，并探索了各种提示和上下文学习的方法，包括少样本提示和不同LLM家族的微调，以从社交媒体段落中提取值得检查的声明。我们的最佳METEOR分数是通过微调FLAN-T5模型获得的，但有时其他方法可以提取更高品质的声明，即使它们的METEOR分数较低。


<details>
  <summary>Details</summary>
Motivation: 我们的目标是从社交媒体段落中提取值得检查的声明。

Method: 我们探索了各种提示和上下文学习的方法，包括少样本提示和不同LLM家族的微调，以从社交媒体段落中提取值得检查的声明。

Result: 我们的最佳METEOR分数是通过微调FLAN-T5模型获得的，但有时其他方法可以提取更高品质的声明，即使它们的METEOR分数较低。

Conclusion: 我们的最佳METEOR分数是通过微调FLAN-T5模型获得的，但有时其他方法可以提取更高品质的声明，即使它们的METEOR分数较低。

Abstract: We participate in CheckThat! Task 2 English and explore various methods of
prompting and in-context learning, including few-shot prompting and fine-tuning
with different LLM families, with the goal of extracting check-worthy claims
from social media passages. Our best METEOR score is achieved by fine-tuning a
FLAN-T5 model. However, we observe that higher-quality claims can sometimes be
extracted using other methods, even when their METEOR scores are lower.

</details>


### [64] [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)
*Marc Marone,Orion Weller,William Fleshman,Eugene Yang,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: mmBERT是一个预训练于1800多种语言的编码器模型，通过创新方法提升了低资源语言的性能。


<details>
  <summary>Details</summary>
Motivation: 缺乏针对多语言编码器模型的最新研究，尤其是低资源语言的处理。

Method: 引入了反向掩码比率调度和反向温度采样比率，并在衰减阶段加入超过1700种低资源语言的数据。

Result: 在仅包含低资源语言的短期衰减阶段，mmBERT的表现与OpenAI的o3和Google的Gemini 2.5 Pro相当。

Conclusion: mmBERT在分类和检索任务上显著优于前一代模型，无论语言资源丰富与否。

Abstract: Encoder-only languages models are frequently used for a variety of standard
machine learning tasks, including classification and retrieval. However, there
has been a lack of recent research for encoder models, especially with respect
to multilingual models. We introduce mmBERT, an encoder-only language model
pretrained on 3T tokens of multilingual text in over 1800 languages. To build
mmBERT we introduce several novel elements, including an inverse mask ratio
schedule and an inverse temperature sampling ratio. We add over 1700
low-resource languages to the data mix only during the decay phase, showing
that it boosts performance dramatically and maximizes the gains from the
relatively small amount of training data. Despite only including these
low-resource languages in the short decay phase we achieve similar
classification performance to models like OpenAI's o3 and Google's Gemini 2.5
Pro. Overall, we show that mmBERT significantly outperforms the previous
generation of models on classification and retrieval tasks -- on both high and
low-resource languages.

</details>


### [65] [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)
*Aivin V. Solatorio*

Main category: cs.CL

TL;DR: 本文提出了一种名为 Proof-Carrying Numbers (PCN) 的协议，用于确保数值的准确性。PCN 通过机械验证来防止数值幻觉，将数值作为与结构化声明相关的“声明绑定令牌”发出，并由验证器根据声明的策略进行检查。验证被放置在渲染器中，而不是模型中，从而防止欺骗并保证失败关闭行为。PCN 是一种轻量级且与模型无关的协议，可以无缝集成到现有应用程序中，并可以通过加密承诺进行扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的保护措施（如检索增强生成、引用和不确定性估计）虽然提高了透明度，但不能保证准确性：伪造或错误引用的数值可能仍会被显示为正确。因此，需要一种能够通过机械验证来确保数值准确性的方法。

Method: PCN 通过机械验证来确保数值的准确性，将数值段作为与结构化声明相关的“声明绑定令牌”发出，并由验证器根据声明的策略（例如精确相等、舍入、别名或带有限定符的容差）检查每个令牌。验证被放置在渲染器中，而不是模型中：只有经过检查的数值才会被标记为已验证，其他所有数值默认为未验证。

Result: PCN 被形式化并证明了其合理性、在诚实令牌下的完备性、失败关闭行为以及在策略细化下的单调性。PCN 是一种轻量级且与模型无关的协议，可以无缝集成到现有应用程序中，并可以通过加密承诺进行扩展。

Conclusion: PCN 是一种轻量级且与模型无关的协议，可以无缝集成到现有应用程序中，并可以通过加密承诺进行扩展。通过在显示前强制验证步骤，PCN 在数值敏感场景中建立了一个简单的契约：信任只能通过证明获得，而缺乏标记则传达不确定性。

Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that
deviate from available data, a failure known as \emph{numeric hallucination}.
Existing safeguards -- retrieval-augmented generation, citations, and
uncertainty estimation -- improve transparency but cannot guarantee fidelity:
fabricated or misquoted values may still be displayed as if correct. We propose
\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that
enforces numeric fidelity through mechanical verification. Under PCN, numeric
spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a
verifier checks each token under a declared policy (e.g., exact equality,
rounding, aliases, or tolerance with qualifiers). Crucially, PCN places
verification in the \emph{renderer}, not the model: only claim-checked numbers
are marked as verified, and all others default to unverified. This separation
prevents spoofing and guarantees fail-closed behavior. We formalize PCN and
prove soundness, completeness under honest tokens, fail-closed behavior, and
monotonicity under policy refinement. PCN is lightweight and model-agnostic,
integrates seamlessly into existing applications, and can be extended with
cryptographic commitments. By enforcing verification as a mandatory step before
display, PCN establishes a simple contract for numerically sensitive settings:
\emph{trust is earned only by proof}, while the absence of a mark communicates
uncertainty.

</details>


### [66] [Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning](https://arxiv.org/abs/2509.06948)
*Liang Chen,Xueting Han,Li Shen,Jing Bai,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过双层优化来改善监督微调和强化学习之间的合作，从而提高推理模型的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在训练大型语言模型时存在严重的效率问题，而常见的两阶段方法限制了SFT和RL之间的交互，从而限制了整体效果。

Method: 本文提出了一种新的学习推理模型的方法，采用双层优化来促进这两种训练范式的更好合作。

Result: 实验评估表明，我们的方法优于基线，并实现了更好的效果和效率平衡。

Conclusion: 我们的方法在五个推理基准测试中表现出色，实现了效果和效率之间的更好平衡。

Abstract: Reinforcement learning (RL) has proven effective in incentivizing the
reasoning abilities of large language models (LLMs), but suffers from severe
efficiency challenges due to its trial-and-error nature. While the common
practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this
decoupled two-stage approach limits interaction between SFT and RL, thereby
constraining overall effectiveness. This study introduces a novel method for
learning reasoning models that employs bilevel optimization to facilitate
better cooperation between these training paradigms. By conditioning the SFT
objective on the optimal RL policy, our approach enables SFT to meta-learn how
to guide RL's optimization process. During training, the lower level performs
RL updates while simultaneously receiving SFT supervision, and the upper level
explicitly maximizes the cooperative gain-the performance advantage of joint
SFT-RL training over RL alone. Empirical evaluations on five reasoning
benchmarks demonstrate that our method consistently outperforms baselines and
achieves a better balance between effectiveness and efficiency.

</details>


### [67] [Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models](https://arxiv.org/abs/2509.06949)
*Yinjie Wang,Ling Yang,Bowen Li,Ye Tian,Ke Shen,Mengdi Wang*

Main category: cs.CL

TL;DR: 研究人员提出了一种名为TraceRL的轨迹感知强化学习框架，用于改进扩散语言模型的推理能力。他们开发了多个先进的扩散语言模型（TraDo），并在数学和编程任务中表现出色。此外，他们还推出了首个长CoT DLM，并发布了开源框架以促进研究和应用。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在复杂数学和编程任务中的表现有限，需要一种更有效的后训练方法来提高其推理能力。同时，需要一个灵活的框架来适应不同规模的模型和任务。

Method: TraceRL是一种轨迹感知的强化学习框架，将首选推理轨迹纳入后训练，并适用于不同架构。该框架配备了基于扩散的价值模型，以提高训练稳定性。此外，还采用了课程学习方法来构建长CoT DLM。

Result: TraDo-4B-Instruct在复杂数学推理任务中优于7B规模的自回归模型。TraDo-8B-Instruct在数学推理基准上相对于Qwen2.5-7B-Instruct和Llama3.1-8B-Instruct分别提高了6.1%和51.3%。长CoT DLM在MATH500基准上相对于Qwen2.5-7B-Instruct提高了18.1%的相对准确率。

Conclusion: 通过TraceRL框架，研究人员开发了一系列最先进的扩散语言模型（TraDo），并在复杂数学推理任务中表现出色。此外，他们还推出了首个长CoT DLM，并在MATH500基准上取得了显著的准确率提升。为了促进可重复研究和实际应用，他们发布了全面的开源框架。

Abstract: We propose TraceRL, a trajectory-aware reinforcement learning framework for
diffusion language models (DLMs) that incorporates preferred inference
trajectory into post-training, and is applicable across different
architectures. Equipped with a diffusion-based value model that enhances
training stability, we demonstrate improved reasoning performance on complex
math and coding tasks. Besides, it can also be applied to adapt block-specific
models to larger blocks, which improves sampling flexibility. Employing
TraceRL, we derive a series of state-of-the-art diffusion language models,
namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still
consistently outperforms them across complex math reasoning tasks.
TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over
Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical
reasoning benchmarks. Through curriculum learning, we also derive the first
long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%
relative accuracy gain. To facilitate reproducible research and practical
applications, we release a comprehensive open-source framework for building,
training, and deploying diffusion LLMs across diverse architectures. The
framework integrates accelerated KV-cache techniques and inference engines for
both inference and reinforcement learning, and includes implementations of
various supervised fine-tuning and RL methods for mathematics, coding, and
general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL

</details>


### [68] [On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts](https://arxiv.org/abs/2509.06952)
*Linlu Qiu,Cedegao E. Zhang,Joshua B. Tenenbaum,Yoon Kim,Roger P. Levy*

Main category: cs.CL

TL;DR: 本文研究了语言模型在语用推理能力上的表现，并提出了一种评估框架。结果表明，最先进的语言模型在语言理解方面表现良好，而使用RSA方法可以进一步提高语言生成的表现。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地被用作对话代理，了解它们的语用推理能力变得越来越重要。

Method: 我们提出了一个评估框架，该框架源自一种流行的通信游戏Wavelength，在这种游戏中，说话者和听者以细致的方式就各种概念进行交流。我们研究了各种语言模型在语言理解和语言生成方面的表现，使用直接提示和链式思维（CoT）提示，并进一步探索了将贝叶斯语用推理纳入语言模型推理的理性言语行为（RSA）方法。

Result: 最先进的语言模型在语言理解方面表现出色，其准确性接近人类，并且即使没有CoT提示或RSA，也与人类判断有很高的相关性。在语言生成方面，CoT可以优于直接提示，使用RSA比这两种方法都有显著的改进。

Conclusion: 我们的研究有助于识别语言模型在语用推理能力上的优势和局限性，并展示了使用RSA改进这些能力的潜力，为理解语言模型和人类的概念表征、语言理解和社交推理打开了未来的研究方向。

Abstract: Language use is shaped by pragmatics -- i.e., reasoning about communicative
goals and norms in context. As language models (LMs) are increasingly used as
conversational agents, it becomes ever more important to understand their
pragmatic reasoning abilities. We propose an evaluation framework derived from
Wavelength, a popular communication game where a speaker and a listener
communicate about a broad range of concepts in a granular manner. We study a
range of LMs on both language comprehension and language production using
direct and Chain-of-Thought (CoT) prompting, and further explore a Rational
Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM
inference. We find that state-of-the-art LMs, but not smaller ones, achieve
strong performance on language comprehension, obtaining similar-to-human
accuracy and exhibiting high correlations with human judgments even without CoT
prompting or RSA. On language production, CoT can outperform direct prompting,
and using RSA provides significant improvements over both approaches. Our study
helps identify the strengths and limitations in LMs' pragmatic reasoning
abilities and demonstrates the potential for improving them with RSA, opening
up future avenues for understanding conceptual representation, language
understanding, and social reasoning in LMs and humans.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [69] [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
*Yuze Liu,Zhaoyuan Zhang,Xiangsheng Zeng,Yihe Zhang,Leping Yu,Lejia Wang,Xi Yu*

Main category: cs.DB

TL;DR: 该研究创建了一个语言原生的数据库，用于BNNS聚合物热导复合材料，能够有效组织和检索相关信息，并通过RAG和工具增强代理生成可操作的指导。


<details>
  <summary>Details</summary>
Motivation: 传统化学和材料研究主要依赖于基于语言的描述，而不是表格，这限制了传统数据库和机器学习所能利用的信息。因此，需要一种能够捕获和组织这些信息的方法，以便更好地支持材料发现。

Method: 该研究构建了一个异构数据库，记录了从制备、表征、理论计算和机制推理中获取的信息，并通过语义、关键词和值过滤器进行查询。此外，还利用检索增强生成（RAG）和工具增强代理来实现检索与推理的结合，以提供可操作的SOP。

Result: 该研究成功构建了一个语言原生的数据库，能够有效地组织和检索关于BNNS聚合物热导复合材料的信息，并通过RAG和工具增强代理实现了高效的文献整合和指导生成。

Conclusion: 该研究提供了一个语言原生的数据库，用于硼氮烷纳米片（BNNS）聚合物热导复合材料，能够从论文中捕获轻度结构化的信息，并通过语义、关键词和值过滤器进行复合检索。该系统可以将文献合成准确、可验证和专家风格的指导，为LLM驱动的材料发现提供了语言丰富的基础。

Abstract: Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [70] [TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition](https://arxiv.org/abs/2509.05983)
*Minh N. H. Nguyen,Anh Nguyen Tran,Dung Truong Dinh,Nam Van Vo*

Main category: cs.SD

TL;DR: 本文提出了一种新的越南语-英语代码切换自动语音识别模型（TSPC），该模型通过音素中心的方法提高了性能，取得了比现有方法更好的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR系统难以捕捉代码切换场景中的细微音系变化，尤其是在越南语和英语这样的语言对中，存在独特的音系特征和相似声音识别的模糊性。

Method: 提出了一种基于音素的两阶段模型（TSPC），利用扩展的越南语音素集作为中间表示，以促进混合语言建模。

Result: 实验结果表明，TSPC在越南语-英语代码切换自动语音识别中优于现有基线，如PhoWhisper-base，实现了20.8%的显著更低词错误率，并且使用了更少的训练资源。

Conclusion: TSPC在越南语-英语代码切换自动语音识别中表现出色，能够显著降低词错误率，并通过语音特征的两阶段架构提高性能。

Abstract: Code-switching (CS) presents a significant challenge for general Auto-Speech
Recognition (ASR) systems. Existing methods often fail to capture the subtle
phonological shifts inherent in CS scenarios. The challenge is particularly
difficult for language pairs like Vietnamese and English, where both distinct
phonological features and the ambiguity arising from similar sound recognition
are present. In this paper, we propose a novel architecture for
Vietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC
employs a phoneme-centric approach, built upon an extended Vietnamese phoneme
set as an intermediate representation to facilitate mixed-lingual modeling.
Experimental results demonstrate that TSPC consistently outperforms existing
baselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a
significantly lower word error rate of 20.8\% with reduced training resources.
Furthermore, the phonetic-based two-stage architecture enables phoneme
adaptation and language conversion to enhance ASR performance in complex CS
Vietnamese-English ASR scenarios.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [ForensicsData: A Digital Forensics Dataset for Large Language Models](https://arxiv.org/abs/2509.05331)
*Youssef Chakir,Iyad Lahsen-Cherif*

Main category: cs.CR

TL;DR: 本文介绍了ForensicsData，这是一个从实际恶意软件分析报告中获取的大量问题-上下文-答案（Q-C-A）数据集，旨在解决数字取证研究中的资源限制问题，并通过使用大型语言模型和专门的评估过程来提高数据质量。


<details>
  <summary>Details</summary>
Motivation: The growing complexity of cyber incidents presents significant challenges for digital forensic investigators, especially in evidence collection and analysis. Public resources are still limited because of ethical, legal, and privacy concerns, even though realistic datasets are necessary to support research and tool developments.

Method: A unique workflow was used to create the dataset, which extracts structured data, uses large language models (LLMs) to transform it into Q-C-A format, and then uses a specialized evaluation process to confirm its quality.

Result: Among the models evaluated, Gemini 2 Flash demonstrated the best performance in aligning generated content with forensic terminology.

Conclusion: ForensicsData aims to advance digital forensics by enabling reproducible experiments and fostering collaboration within the research community.

Abstract: The growing complexity of cyber incidents presents significant challenges for
digital forensic investigators, especially in evidence collection and analysis.
Public resources are still limited because of ethical, legal, and privacy
concerns, even though realistic datasets are necessary to support research and
tool developments. To address this gap, we introduce ForensicsData, an
extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware
analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique
workflow was used to create the dataset, which extracts structured data, uses
large language models (LLMs) to transform it into Q-C-A format, and then uses a
specialized evaluation process to confirm its quality. Among the models
evaluated, Gemini 2 Flash demonstrated the best performance in aligning
generated content with forensic terminology. ForensicsData aims to advance
digital forensics by enabling reproducible experiments and fostering
collaboration within the research community.

</details>


### [72] [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608)
*Waris Gill,Natalie Isak,Matthew Dressman*

Main category: cs.CR

TL;DR: BinaryShield 是一种隐私保护的威胁情报系统，能够安全地共享攻击指纹，同时保持高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于隐私法规限制，企业中的 LLM 服务无法共享关于提示注入攻击的威胁情报，这导致攻击可能在多个服务中长期存在而未被检测到。

Method: BinaryShield 通过结合 PII 去标识化、语义嵌入、二值量化和随机响应机制，将可疑提示转换为非可逆的指纹，以保留攻击模式并提供隐私保护。

Result: BinaryShield 在 F1 分数上达到了 0.94，显著优于 SimHash（0.77），并且在存储和相似性搜索方面分别实现了 64 倍和 38 倍的改进。

Conclusion: BinaryShield 是一种隐私保护的威胁情报系统，能够在遵守监管要求的前提下，安全地共享攻击指纹。它在 F1 分数上显著优于 SimHash，并且在存储和相似性搜索方面有显著改进。

Abstract: The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.

</details>


### [73] [An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection](https://arxiv.org/abs/2509.06920)
*Haywood Gelman,John D. Hastings,David Kenley*

Main category: cs.CR

TL;DR: 本研究提出了一种基于大型语言模型的新方法，用于动态合成syslog消息并检测内部威胁，结果表明该方法在检测性能上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 由于现有数据集静态且访问受限，限制了自适应检测模型的发展，因此需要一种新的方法来解决这个问题。

Method: 本研究引入了一种基于伦理的新型方法，利用大型语言模型Claude Sonnet 3.7动态合成syslog消息，并分析这些消息以检测内部威胁。

Result: Claude Sonnet 3.7在几乎所有指标上都优于GPT-4o，特别是在减少误报和提高检测准确性方面表现突出。

Conclusion: 研究结果表明，大型语言模型在合成数据集生成和内部威胁检测方面具有良好的前景。

Abstract: Insider threats are a growing organizational problem due to the complexity of
identifying their technical and behavioral elements. A large research body is
dedicated to the study of insider threats from technological, psychological,
and educational perspectives. However, research in this domain has been
generally dependent on datasets that are static and limited access which
restricts the development of adaptive detection models. This study introduces a
novel, ethically grounded approach that uses the large language model (LLM)
Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which
contain indicators of insider threat scenarios. The messages reflect real-world
data distributions by being highly imbalanced (1% insider threats). The syslogs
were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with
their performance evaluated through statistical metrics including precision,
recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across
nearly all metrics, particularly in reducing false alarms and improving
detection accuracy. The results show strong promise for the use of LLMs in
synthetic dataset generation and insider threat detection.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [74] [Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods](https://arxiv.org/abs/2509.06195)
*Jinrui Yang,Fan Jiang,Timothy Baldwin*

Main category: cs.IR

TL;DR: 本文研究了多语言信息检索系统中的语言公平性问题，评估了传统方法和神经排序器的公平性，并提出了一种新的损失函数LaKDA来减少语言偏差。


<details>
  <summary>Details</summary>
Motivation: 确保多语言信息检索系统中不同语言用户能够公平地访问信息至关重要。

Method: 本文基于假设，即不同语言但语义相同的查询应产生等效的排名列表，评估了传统检索方法和基于mBERT和XLM-R的DPR神经排序器的公平性。此外，引入了一种名为LaKDA的新损失函数，以减轻神经MLIR方法中的语言偏差。

Result: 分析揭示了当前MLIR技术中的内在语言偏差，不同检索方法之间存在显著差异，而LaKDA在提高语言公平性方面表现出色。

Conclusion: 本文分析了当前MLIR技术中的内在语言偏差，并展示了LaKDA在提高语言公平性方面的有效性。

Abstract: Language fairness in multilingual information retrieval (MLIR) systems is
crucial for ensuring equitable access to information across diverse languages.
This paper sheds light on the issue, based on the assumption that queries in
different languages, but with identical semantics, should yield equivalent
ranking lists when retrieving on the same multilingual documents. We evaluate
the degree of fairness using both traditional retrieval methods, and a DPR
neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a
novel loss designed to mitigate language biases in neural MLIR approaches. Our
analysis exposes intrinsic language biases in current MLIR technologies, with
notable disparities across the retrieval methods, and the effectiveness of
LaKDA in enhancing language fairness.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [75] [ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders](https://arxiv.org/abs/2509.05309)
*Xiangyu Liu,Haodi Lei,Yi Liu,Yang Liu,Wei Hu*

Main category: q-bio.QM

TL;DR: 本文提出了ProtSAE，一种语义引导的稀疏自编码器，用于改善蛋白质语言模型中的语义纠缠问题，并展示了其在可解释性和下游生成任务中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏自编码器（SAE）在蛋白质语言模型（PLMs）中存在语义纠缠问题，导致难以可靠地解释或操控模型行为。

Method: 提出了一种语义引导的稀疏自编码器（ProtSAE），通过使用标注数据集和领域知识来指导语义解缠，以减轻纠缠属性的影响。

Result: ProtSAE在可解释性实验中表现出比之前方法更生物相关和可解释的隐藏特征，并在可解释的探测任务中取得了更好的结果。

Conclusion: ProtSAE能够学习到更生物相关且可解释的隐藏特征，并在保持高重建保真度的同时，在可解释的探测任务中取得更好的结果，展示了其在引导PLMs进行下游生成任务的潜力。

Abstract: Sparse Autoencoder (SAE) has emerged as a powerful tool for mechanistic
interpretability of large language models. Recent works apply SAE to protein
language models (PLMs), aiming to extract and analyze biologically meaningful
features from their latent spaces. However, SAE suffers from semantic
entanglement, where individual neurons often mix multiple nonlinear concepts,
making it difficult to reliably interpret or manipulate model behaviors. In
this paper, we propose a semantically-guided SAE, called ProtSAE. Unlike
existing SAE which requires annotation datasets to filter and interpret
activations, we guide semantic disentanglement during training using both
annotation datasets and domain knowledge to mitigate the effects of entangled
attributes. We design interpretability experiments showing that ProtSAE learns
more biologically relevant and interpretable hidden features compared to
previous methods. Performance analyses further demonstrate that ProtSAE
maintains high reconstruction fidelity while achieving better results in
interpretable probing. We also show the potential of ProtSAE in steering PLMs
for downstream generation tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [76] [Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance](https://arxiv.org/abs/2509.05978)
*Mohamed Mohamed,Brennan Nichyporuk,Douglas L. Arnold,Tal Arbel*

Main category: eess.IV

TL;DR: 本研究提出了一种能够根据自由形式的语言提示生成高分辨率3D反事实医学图像的框架，这是首次将语言引导的原生3D扩散模型应用于神经影像数据。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可比较的预训练基础模型，3D领域的进展受到显著限制。因此，视觉-语言模型在仅依靠自然语言描述的情况下生成高分辨率3D反事实医学图像的潜力尚未被探索。

Method: 我们适应了最先进的3D扩散模型，并从Simple Diffusion中获得了增强，并结合了增强的条件以提高文本对齐和图像质量。

Result: 我们的框架成功地在两个不同的神经MRI数据集上模拟了多发性硬化症（MS）的不同反事实病变负荷和阿尔茨海默病的认知状态，生成了高质量的图像，同时在合成生成的医学图像中保持了受试者的真实性。

Conclusion: 我们的研究为基于提示的3D医学影像中的疾病进展分析奠定了基础。

Abstract: Vision-language models have demonstrated impressive capabilities in
generating 2D images under various conditions; however the impressive
performance of these models in 2D is largely enabled by extensive, readily
available pretrained foundation models. Critically, comparable pretrained
foundation models do not exist for 3D, significantly limiting progress in this
domain. As a result, the potential of vision-language models to produce
high-resolution 3D counterfactual medical images conditioned solely on natural
language descriptions remains completely unexplored. Addressing this gap would
enable powerful clinical and research applications, such as personalized
counterfactual explanations, simulation of disease progression scenarios, and
enhanced medical training by visualizing hypothetical medical conditions in
realistic detail. Our work takes a meaningful step toward addressing this
challenge by introducing a framework capable of generating high-resolution 3D
counterfactual medical images of synthesized patients guided by free-form
language prompts. We adapt state-of-the-art 3D diffusion models with
enhancements from Simple Diffusion and incorporate augmented conditioning to
improve text alignment and image quality. To our knowledge, this represents the
first demonstration of a language-guided native-3D diffusion model applied
specifically to neurological imaging data, where faithful three-dimensional
modeling is essential to represent the brain's three-dimensional structure.
Through results on two distinct neurological MRI datasets, our framework
successfully simulates varying counterfactual lesion loads in Multiple
Sclerosis (MS), and cognitive states in Alzheimer's disease, generating
high-quality images while preserving subject fidelity in synthetically
generated medical images. Our results lay the groundwork for prompt-driven
disease progression analysis within 3D medical imaging.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [77] [Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models](https://arxiv.org/abs/2509.06415)
*Jaemin Son,Sujin Choi,Inyong Yun*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的token剪枝框架，用于降低视觉-语言模型在文档理解任务中的计算成本，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 最近在视觉-语言模型（VLMs）方面的进展在文档理解任务中取得了令人印象深刻的成果，但它们的高计算需求仍然是一个挑战。为了减轻计算负担，我们提出了这种方法。

Method: 我们提出了一种轻量级的token剪枝框架，在VLM处理之前从文档图像中过滤掉非信息性的背景区域。一个二进制的patch级分类器移除了非文本区域，一个最大池化细化步骤恢复了碎片化的文本区域以增强空间一致性。

Result: 在真实世界文档数据集上的实验表明，我们的方法显著降低了计算成本，同时保持了可比的准确性。

Conclusion: 我们的方法显著降低了计算成本，同时保持了可比的准确性。

Abstract: Recent progress in vision-language models (VLMs) has led to impressive
results in document understanding tasks, but their high computational demands
remain a challenge. To mitigate the compute burdens, we propose a lightweight
token pruning framework that filters out non-informative background regions
from document images prior to VLM processing. A binary patch-level classifier
removes non-text areas, and a max-pooling refinement step recovers fragmented
text regions to enhance spatial coherence. Experiments on real-world document
datasets demonstrate that our approach substantially lowers computational
costs, while maintaining comparable accuracy.

</details>


### [78] [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)
*Wenxuan Huang,Shuang Chen,Zheyong Xie,Shaosheng Cao,Shixiang Tang,Yufan Shen,Qingyu Yin,Wenbo Hu,Xiaoman Wang,Yuntian Tang,Junbo Qiao,Yue Guo,Yao Hu,Zhenfei Yin,Philip Torr,Yu Cheng,Wanli Ouyang,Shaohui Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为IRG的框架，通过交替进行基于文本的思考和图像合成来提高文本到图像生成的质量。实验结果表明，IRG在多个基准测试中表现优异，并在视觉质量和细节保真度方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管统一的多模态理解和生成模型在图像生成能力上取得了显著进步，但与那些紧密耦合理解与生成的系统（如GPT-4o）相比，指令遵循和细节保留方面仍存在较大差距。受最近在交错推理方面的进展启发，我们探索这种推理是否能进一步改善文本到图像（T2I）生成。

Method: 引入了Interleaving Reasoning Generation (IRG)框架，该框架交替进行基于文本的思考和图像合成。提出了Interleaving Reasoning Generation Learning (IRGL)，旨在实现两个子目标：(1) 加强初始的思考和生成阶段以建立核心内容和基础质量，(2) 实现高质量的文本反思并在后续图像中忠实实施这些改进。

Result: 实验结果显示，IRG在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN等基准测试中取得了绝对5-10分的提升，并在视觉质量和细粒度保真度方面有显著改进。

Conclusion: 实验结果表明，IRG在多个基准测试中取得了SOTA性能，并在视觉质量和细节保真度方面有显著提升。代码、模型权重和数据集将被发布。

Abstract: Unified multimodal understanding and generation models recently have achieve
significant improvement in image generation capability, yet a large gap remains
in instruction following and detail preservation compared to systems that
tightly couple comprehension with generation such as GPT-4o. Motivated by
recent advances in interleaving reasoning, we explore whether such reasoning
can further improve Text-to-Image (T2I) generation. We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics. To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image. We curate IRGL-300K, a dataset organized into six decomposed
learning modes that jointly cover learning text-based thinking, and full
thinking-image trajectories. Starting from a unified foundation model that
natively emits interleaved text-image outputs, our two-stage training first
builds robust thinking and reflection, then efficiently tunes the IRG pipeline
in the full thinking-image trajectory data. Extensive experiments show SoTA
performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,
GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality
and fine-grained fidelity. The code, model weights and datasets will be
released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [79] [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
*Yuda Song,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 本文研究了基于结果的强化学习方法在提高大语言模型推理能力时导致生成多样性下降的问题，并提出了两种新的探索方法来解决这一问题。实验和理论分析表明，这些方法在提高准确性的同时保持了生成多样性。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的强化学习方法虽然提高了准确性，但导致了生成多样性的系统性损失。这在实际应用中是一个关键问题，因为多样性对于测试时的扩展至关重要。因此，需要一种方法来在提高准确性的同时保持生成多样性。

Method: 本文提出了两种互补算法：历史探索和批量探索。历史探索通过UCB风格的奖励鼓励很少观察到的答案，而批量探索通过惩罚批次内的重复来促进测试时的多样性。此外，还引入了一个新的基于结果的老虎机模型来形式化基于结果的探索的好处。

Result: 实验表明，这两种方法在标准竞赛数学任务上提高了准确性，同时缓解了多样性崩溃问题。理论分析进一步证明了基于结果的探索的优势。

Conclusion: 本文提出了基于结果的探索方法，以在增强推理能力的同时保持生成多样性。实验表明，这两种方法在提高准确性的同时缓解了多样性崩溃问题。理论分析进一步证明了基于结果的探索的优势，为RL方法提供了实用路径，使其在增强推理能力的同时不牺牲多样性。

Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [80] [Authorship Without Writing: Large Language Models and the Senior Author Analogy](https://arxiv.org/abs/2509.05390)
*Clint Hurshman,Sebastian Porsdam Mann,Julian Savulescu,Brian D. Earp*

Main category: cs.CY

TL;DR: 本文讨论了大型语言模型在科研论文写作中的使用是否应被视为合法的作者资格，并认为在特定条件下，这种使用可以被视为合法的作者资格。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨大型语言模型在生物伦理、科学和医学写作中的使用是否应被视为合法的作者资格。

Method: 本文通过类比高级作者角色来分析大型语言模型（LLMs）在科研论文写作中的作用。

Result: 研究结果表明，在特定条件下，使用LLMs可以被视为一种合法的作者资格形式。

Conclusion: 我们得出结论，要么应承认这种使用是合法的，要么当前的作者资格标准需要根本性的修订。

Abstract: The use of large language models (LLMs) in bioethical, scientific, and
medical writing remains controversial. While there is broad agreement in some
circles that LLMs cannot count as authors, there is no consensus about whether
and how humans using LLMs can count as authors. In many fields, authorship is
distributed among large teams of researchers, some of whom, including
paradigmatic senior authors who guide and determine the scope of a project and
ultimately vouch for its integrity, may not write a single word. In this paper,
we argue that LLM use (under specific conditions) is analogous to a form of
senior authorship. On this view, the use of LLMs, even to generate complete
drafts of research papers, can be considered a legitimate form of authorship
according to the accepted criteria in many fields. We conclude that either such
use should be recognized as legitimate, or current criteria for authorship
require fundamental revision. AI use declaration: GPT-5 was used to help format
Box 1. AI was not used for any other part of the preparation or writing of this
manuscript.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [81] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型（LLMs）作为人类受试者研究中真实参与者的替代品的可行性。结果显示，尽管LLMs可以生成与人类参与者相似的响应，但它们在内部一致性方面存在显著问题，这表明它们在准确替代真实参与者进行人类受试者研究方面存在关键差距。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究人员主要关注LLM生成的调查数据是否与被提示代表的人类对应物相符。相比之下，我们解决了一个更根本的问题：代理在不同的实验设置下是否保持内部一致性？

Method: 我们设计了一项研究，旨在揭示代理的内部状态并检查代理在基本对话设置中的行为。这种设计使我们能够探索一组行为假设，以评估代理的对话行为是否与其揭示的内部状态一致。

Result: 我们的研究结果表明，LLMs在不同模型家族和不同模型规模下存在显著的内部不一致性。

Conclusion: 我们的研究发现，尽管代理可以生成与人类参与者相匹配的响应，但它们在内部一致性方面存在显著问题，这表明它们在准确替代真实参与者进行人类受试者研究方面存在关键差距。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [82] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: 本文提出了一种新的推理方法REER，通过从已知良好解决方案反向推导出深度推理过程，构建了一个大规模数据集并训练了一个高性能模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法如强化学习和指令蒸馏在开放性、创造性生成任务中表现不佳，因为缺乏明确的奖励信号或受教师模型能力限制。

Method: REER（反向工程推理）方法，从已知的良好解决方案反向推导出潜在的深度推理过程。

Result: 构建了DeepWriting-20K数据集，并训练了DeepWriter-8B模型，该模型在性能上超过了开源基线，并且在某些方面优于GPT-4o和Claude 3.5等专有模型。

Conclusion: REER方法通过从已知的良好解决方案反向推导出潜在的深度推理过程，克服了传统方法的局限性。使用这种方法，研究者构建了一个大规模的数据集，并训练了一个性能优越的模型。

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [83] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: 本文提出了一种名为EDIT的测试时缩放方法，以提高大型推理模型的推理效率，减少冗长复杂的推理轨迹，从而提升可读性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型容易过度思考的问题，即倾向于将简单问题复杂化，导致过多的策略切换和冗长复杂的推理轨迹，从而影响其可解释性。

Method: 提出了一种测试时缩放方法EDIT（Efficient Dynamic Inference Trimming），该方法在测试时有效地引导大型推理模型识别最短的正确推理路径。

Result: 通过广泛的实验，EDIT在各种模型和数据集上显著提高了推理效率，产生了简洁但信息丰富的输出。

Conclusion: EDIT显著提高了推理效率，产生了简洁且信息丰富的输出，改善了可读性和用户体验。

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [84] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文提出了一种基于合成数据的强化学习方法，用于提升自主单智能体的代理技能，并在基准测试中取得了良好结果。


<details>
  <summary>Details</summary>
Motivation: 为了提高自主单智能体的代理技能，同时保持推理能力，我们需要一种有效的持续强化学习方法。

Method: 我们提出了一种完全基于合成数据的简单强化学习方法，并将其应用于各种开源大语言模型。

Result: 我们的最佳变体SFR-DR-20B在Humanity's Last Exam基准测试中达到了28.7%的准确率。

Conclusion: 我们的工作展示了通过持续强化学习提高自主单智能体的代理技能的潜力，并在Humanity's Last Exam基准测试中取得了显著成果。

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [85] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 本文是对深度研究系统中强化学习基础的首次系统性综述，涵盖了数据合成与整理、代理研究的强化学习方法、代理强化学习训练系统和框架等方面，并提供了实用的指导。


<details>
  <summary>Details</summary>
Motivation: 目前，深度研究系统在协调推理、搜索和工具使用方面取得了进展，但训练整个堆栈端到端仍然不切实际。因此，本文旨在探讨强化学习在深度研究系统中的应用，以解决现有方法的局限性。

Method: 本文对深度研究系统中的强化学习方法进行了系统性的综述，涵盖了数据合成与整理、代理研究的强化学习方法、代理强化学习训练系统和框架等方面。

Result: 本文系统地综述了深度研究系统中强化学习的各个方面，包括数据合成与整理、代理研究的强化学习方法、代理强化学习训练系统和框架等，并提供了实用的指导。

Conclusion: 本文总结了深度研究系统中强化学习的基础，提出了训练强大、透明的深度研究代理的实用指导。

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [86] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 本文介绍了VehicleWorld，这是第一个全面的汽车领域环境，具有30个模块、250个API和680个属性，提供实时状态信息。我们提出了基于状态的功能调用（SFC），实验结果表明SFC显著优于传统FC方法。


<details>
  <summary>Details</summary>
Motivation: 智能车辆驾驶舱对API代理提出了独特的挑战，需要协调紧密耦合的子系统，这超出了典型任务环境的复杂性。传统功能调用（FC）方法是无状态的，需要多次探索性调用来构建环境意识，导致效率低下和有限的错误恢复。

Method: 我们提出了基于状态的功能调用（SFC），这是一种新的方法，保持显式的系统状态意识，并实现直接的状态转换以达到目标条件。

Result: 通过系统分析，我们发现直接状态预测在环境控制方面优于功能调用。SFC方法在执行准确性和降低延迟方面表现出色。

Conclusion: 实验结果表明，SFC显著优于传统的FC方法，实现了更高的执行准确性和更低的延迟。

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [87] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: RAFFLES is an evaluation framework that improves fault detection in LLM agentic systems by incorporating reasoning and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Current evaluation capabilities are limited in their ability to diagnose failures in long-horizon, multi-component LLM agentic systems. Evaluation frameworks must be able to reason, probe, iterate, and understand the complex logic passing through these systems over long horizons.

Method: RAFFLES is an evaluation architecture that incorporates reasoning and iterative refinement. It operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself.

Result: RAFFLES outperforms baselines on the Who&When dataset, achieving an agent-step fault pair accuracy of over 43% on the Algorithmically-Generated dataset and over 20% on the Hand-Crafted dataset.

Conclusion: RAFFLES demonstrates a key step towards introducing automated fault detection for autonomous systems, reducing the need for labor-intensive manual human review.

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


### [88] [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
*James Xu Zhao,Bryan Hooi,See-Kiong Ng*

Main category: cs.AI

TL;DR: 测试时缩放在知识密集型任务中效果有限，尽管启用思考仍有一定益处。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放在许多领域表现出色，但在需要高事实准确性和低幻觉率的知识密集型任务中可能存在问题。

Method: 对12个推理模型在两个知识密集型基准上的测试时缩放进行了全面评估。

Result: 增加测试时计算并不总能提高准确性，有时甚至会导致更多幻觉。

Conclusion: 测试时缩放在知识密集型任务中并不总是有效，尽管启用思考仍然有益。

Abstract: Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

</details>


### [89] [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
*Jiacheng Miao,Joe R. Davis,Jonathan K. Pritchard,James Zou*

Main category: cs.AI

TL;DR: Paper2Agent是一个自动化框架，可将研究论文转换为AI代理，从而将静态论文变为动态、交互式的系统，提高知识传播效率。


<details>
  <summary>Details</summary>
Motivation: 传统研究论文需要读者投入大量精力来理解并适应论文的代码、数据和方法，这阻碍了传播和重用。Paper2Agent旨在解决这一问题，将论文转化为能够作为知识丰富研究助手的AI代理。

Method: Paper2Agent通过系统分析论文和相关代码库，使用多个代理构建模型上下文协议（MCP）服务器，并迭代生成和运行测试以完善和增强MCP。这些论文MCP可以灵活连接到聊天代理，以自然语言执行复杂的科学查询。

Result: Paper2Agent成功创建了可靠的论文代理，例如利用AlphaGenome解释基因组变异，以及基于ScanPy和TISSUE进行单细胞和空间转录组分析。这些代理能够重现原始论文的结果，并正确执行新的用户查询。

Conclusion: Paper2Agent将静态论文转化为动态、交互式的AI代理，引入了一种新的知识传播范式，并为AI共同科学家的协作生态系统奠定了基础。

Abstract: We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [90] [On the Contribution of Lexical Features to Speech Emotion Recognition](https://arxiv.org/abs/2509.05634)
*David Combei*

Main category: eess.AS

TL;DR: 本文探讨了从语音中提取的词汇内容在语音情感识别中的作用，并发现其性能可以与声学模型相媲美甚至更好。


<details>
  <summary>Details</summary>
Motivation: 尽管语音情感识别通常认为语音的副语言线索是主要驱动因素，但本文旨在探讨从语音中提取的词汇内容的作用，并验证其在该任务中的有效性。

Method: 我们研究了从语音中提取的词汇内容在语音情感识别中的作用，并比较了基于词汇的方法与声学模型的性能。此外，我们分析了不同的自监督语音和文本表示，进行了变压器编码器的逐层研究，并评估了音频去噪的效果。

Result: 在MELD数据集上，基于词汇的方法获得了51.5%的加权F1分数（WF1），而仅使用声学特征的管道获得了49.3%的WF1。这表明基于词汇的方法在某些情况下可以取得更好的效果。

Conclusion: 我们的基于词汇的方法在MELD数据集上取得了与声学模型相当甚至更高的性能，表明词汇内容在语音情感识别中起着重要作用。

Abstract: Although paralinguistic cues are often considered the primary drivers of
speech emotion recognition (SER), we investigate the role of lexical content
extracted from speech and show that it can achieve competitive and in some
cases higher performance compared to acoustic models. On the MELD dataset, our
lexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to
49.3% for an acoustic-only pipeline with a larger parameter count. Furthermore,
we analyze different self-supervised (SSL) speech and text representations,
conduct a layer-wise study of transformer-based encoders, and evaluate the
effect of audio denoising.

</details>


### [91] [Beamforming-LLM: What, Where and When Did I Miss?](https://arxiv.org/abs/2509.06221)
*Vishal Choudhari*

Main category: eess.AS

TL;DR: Beamforming-LLM 是一种系统，可以允许用户在多说话人环境中语义地回忆他们可能错过的对话。它结合了空间音频捕获和检索增强生成（RAG），以支持自然语言查询。


<details>
  <summary>Details</summary>
Motivation: 旨在让用户在多说话人环境中语义地回忆他们可能错过的对话。

Method: 系统结合了使用麦克风阵列的空间音频捕获和检索增强生成（RAG），以支持自然语言查询。方向性音频流通过波束成形分离，使用Whisper进行转录，并使用句子编码器嵌入到向量数据库中。当收到用户查询时，语义相关的片段被检索，与非关注段对齐，并使用轻量级大语言模型（GPT-4o-mini）进行摘要。

Result: 提供了一个用户友好的界面，提供对比性摘要、空间上下文和带时间戳的音频播放。

Conclusion: 这项工作为智能听觉记忆系统奠定了基础，并在辅助技术、会议摘要和上下文感知的个人空间计算中有广泛的应用。

Abstract: We present Beamforming-LLM, a system that enables users to semantically
recall conversations they may have missed in multi-speaker environments. The
system combines spatial audio capture using a microphone array with
retrieval-augmented generation (RAG) to support natural language queries such
as, "What did I miss when I was following the conversation on dogs?"
Directional audio streams are separated using beamforming, transcribed with
Whisper, and embedded into a vector database using sentence encoders. Upon
receiving a user query, semantically relevant segments are retrieved,
temporally aligned with non-attended segments, and summarized using a
lightweight large language model (GPT-4o-mini). The result is a user-friendly
interface that provides contrastive summaries, spatial context, and timestamped
audio playback. This work lays the foundation for intelligent auditory memory
systems and has broad applications in assistive technology, meeting
summarization, and context-aware personal spatial computing.

</details>

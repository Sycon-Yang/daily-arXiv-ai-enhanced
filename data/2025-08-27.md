<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 51]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义吸引子的语义AGI理论框架，强调意义通过递归张量变换形成，并通过语义吸引子引导意义向稳定、清晰和表达深度发展。


<details>
  <summary>Details</summary>
Motivation: 当前基于变压器的语言模型依赖于统计性的下一个标记预测，而本文旨在探索一种新的认知架构，该架构不仅预测语言，还能塑造语言。

Method: 本文使用涉及虚数单位i的循环操作，描述了一种能够建模讽刺、同音异义和歧义的旋转语义结构。语义吸引子被构想为梯度流、张量变形和迭代矩阵动力学的模型。

Result: 本文提出了一个数学上具有启发性且哲学上有重要意义的语义转换模型。

Conclusion: 本文提出了一种基于复数意义空间中语义吸引子的语义人工通用智能（AGI）理论框架。该框架强调意义不是通过统计预测形成的，而是通过递归张量变换形成的。语义吸引子作为有目的的代理，引导意义向稳定、清晰和表达深度发展。

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [2] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: 本文介绍了KAIROS，一个模拟测验比赛的基准测试，其中包含不同可靠性的同伴代理，用于研究LLM如何形成信任、抵抗错误信息并整合同伴输入。通过评估不同的缓解策略，发现GRPO在整体性能上表现最佳，但降低了对社会影响的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的工作集中在一致性偏差上，但我们扩展了分析，以检查LLM如何从之前的印象中形成信任，抵抗错误信息，并在互动中整合同伴输入，这是在复杂的社会动态下实现集体智能的关键因素。

Method: 我们评估了提示、监督微调和强化学习，群体相对策略优化（GRPO）在多个模型上的效果。

Result: 我们发现，结合多智能体上下文和基于结果的奖励以及不受约束的推理的GRPO在整体性能上表现最佳，但与基础模型相比，它降低了对社会影响的鲁棒性。

Conclusion: 我们的结果表明，结合多智能体上下文和基于结果的奖励以及不受约束的推理的GRPO在整体性能上表现最佳，但与基础模型相比，它降低了对社会影响的鲁棒性。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [3] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 本文介绍了LangCrUX数据集，分析了多语言网页无障碍问题，并提出了Kizuki工具以提高无障碍提示的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于辅助技术对非拉丁字母语言的支持不足，导致视觉障碍用户在多语言网页上面临显著障碍，而现有的大规模研究受限于缺乏全面的数据集。

Method: 本文引入了LangCrUX数据集，对多语言网页无障碍进行了系统分析，并提出了Kizuki工具来处理无障碍提示的语言不一致性问题。

Result: 通过LangCrUX数据集的分析，发现无障碍提示通常未能反映可见内容的语言多样性，降低了屏幕阅读器的效果，限制了网页的可访问性。

Conclusion: 本文提出了一种语言感知的自动化无障碍测试扩展Kizuki，以解决无障碍提示语言不一致的问题，从而提高网站的可访问性。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [4] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: This paper introduces PLAST, a training method that enhances the multilingual capabilities of large vision-language models by fine-tuning specific layers based on language-specific neuron activations, achieving significant efficiency with minimal parameter adjustments.


<details>
  <summary>Details</summary>
Motivation: To address the imbalance in multilingual capabilities of large vision-language models (LVLMs) and improve their multilingual understanding ability.

Method: PLAST, a training recipe that achieves efficient multilingual enhancement for LVLMs by Precise LAnguage-Specific layers fine-Tuning.

Result: PLAST effectively improves the multilingual capabilities of LVLMs and achieves significant efficiency with only 14% of the parameters tuned.

Conclusion: PLAST can be generalized to low-resource and complex visual reasoning tasks, facilitating the language-specific visual information engagement in shallow layers.

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [5] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [6] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的自注意力机制Integral Transformer，通过从logit分布中采样信号来消除注意力噪声。该方法在多个基准测试中表现优于普通、Cog和Differential注意力变体，并有效平衡了注意力分布。


<details>
  <summary>Details</summary>
Motivation: Softmax自注意力经常给语义无信息的标记（如特殊标记和标点）分配不成比例的权重，这种现象称为注意力噪声。虽然最近的方法如Cog Attention和Differential Transformer通过引入负注意力分数来解决这个问题，但它们可能会丢弃有用的信息。

Method: 我们提出了Integral Transformer，这是一种新颖的自注意力机制，通过从logit分布中采样信号来消除注意力噪声。

Result: 广泛的实验表明，我们的模型在已建立的知识和推理语言基准测试中优于普通的、Cog和Differential注意力变体。此外，分析显示，在较低的Transformer层中使用普通的自注意力可以提高性能，并且Integral Transformer有效地平衡了注意力分布并减少了上层的排名崩溃。

Conclusion: 我们的模型在已建立的知识和推理语言基准测试中优于普通的、Cog和Differential注意力变体。此外，分析表明，在较低的Transformer层中使用普通的自注意力可以提高性能，并且Integral Transformer有效地平衡了注意力分布并减少了上层的排名崩溃。

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [7] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: LSC是一种基于可学习标记嵌入的语义一致性选择方法，能够在不显著增加计算开销的情况下提升LLM在不同答案格式下的输出一致性。


<details>
  <summary>Details</summary>
Motivation: Probabilistic decoding in Large Language Models (LLMs) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks.

Method: LSC 通过使用可学习的标记嵌入选择语义上最一致的响应，并通过轻量级的摘要标记生成增加推理时间不到1%。

Result: LSC 在6个短形式和5个长形式推理基准（例如，MATH、MMLU、TruthfulQA）上平均超越SC、USC和WUCS，同时保持微不足道的计算开销。

Conclusion: LSC 是一种实用的一致性选择方法，能够在不同答案格式下可靠工作，并提供校准良好的置信度估计。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [8] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文质疑了基于OOD的模型泛化评估的有效性，并提出了更稳健的评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前的OOD评估方法假设可以捕捉模型在现实部署中的可能失败，但本文认为这一假设存在问题，需要更全面的评估方法。

Method: 本文挑战了OOD评估假设，通过对比现有QA模型中的特定失败模式（如依赖偶然特征或预测捷径）来分析不同OOD数据集的评估效果。

Result: 研究发现，不同的OOD数据集在评估模型对捷径的鲁棒性方面质量差异很大，有些甚至不如简单的分布内评估。

Conclusion: 本文指出基于OOD的评估方法在评估模型泛化能力方面存在局限性，并提出了更稳健的评估方法和建议。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [9] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 本文研究了不同训练方法对大型语言模型（LLMs）在重新排序任务中的语义理解的影响，并探讨了这些模型是否能够生成更知情的文本推理以克服透明度或LLMs和有限训练数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）语义理解能力的提高，它们表现出更大的人类价值观意识和对齐，但这是以透明度为代价的。虽然通过实验分析取得了有希望的结果，但深入了解LLM的内部工作是不可避免的，以理解重新排序背后的推理，这为最终用户提供了解释，使他们能够做出明智的决策。此外，在新开发的系统中，用户参与度有限且排名数据不足，准确重新排序内容仍然是一个重大挑战。

Method: 本文利用一个相对较小的排名数据集，来自环境和地球科学领域，对检索到的内容进行重新排序，并分析可解释的信息，以查看重新排序是否可以通过可解释性来推理。

Result: 我们的分析发现，一些训练方法比其他方法更具可解释性，这意味着并非所有训练方法都学会了准确的语义理解；相反，获得了抽象知识以优化评估，这引发了关于LLMs真正可靠性的疑问。

Conclusion: 本文分析了不同的训练方法如何影响大型语言模型（LLMs）在重新排序任务中的语义理解，并探讨了这些模型是否能够生成更知情的文本推理以克服透明度或LLMs和有限训练数据的挑战。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [10] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 本研究通过使用IPIS数据集对波兰语大型语言模型进行微调，以减轻性别偏见，实现性别包容性。


<details>
  <summary>Details</summary>
Motivation: 当前波兰语中由于历史和政治惯例，男性形式被广泛用于指代男性、女性和混合性别群体，导致大型语言模型在训练过程中继承并强化了这种男性偏见，生成性别不平衡的输出。

Method: 本研究基于理论语言学框架，设计了一个包含明确性别包容指南的系统提示，并使用IPIS数据集对多语言和波兰特定的大型语言模型进行微调。

Result: 通过IPIS微调，研究成功地将性别包容性整合到模型中，为减轻波兰语生成中的性别偏见提供了一种系统性的解决方案。

Conclusion: 本研究通过使用IPIS数据集对大型语言模型进行微调，旨在将性别包容性作为这些模型的固有特征，提供一种系统性的解决方案来减轻波兰语生成中的性别偏见。

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [11] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 本文将幻觉检测问题形式化为假设检验问题，并提出了一种基于多重检验的方法来解决该问题，通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在解决各种任务方面表现出强大的能力，但它们容易产生幻觉，即生成听起来自信但实际上不正确甚至无意义的响应。

Method: 将幻觉检测问题形式化为假设检验问题，并借鉴了机器学习模型中分布外检测的问题。

Result: 本文提供了广泛的实验结果，以验证所提出方法对最先进方法的鲁棒性。

Conclusion: 本文提出了一种基于多重检验的方法来解决幻觉检测问题，并通过广泛的实验结果验证了该方法的鲁棒性。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [12] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文提出两种新的自动化机器翻译评估指标，分别利用其他翻译和检索示例来提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化评估指标通常只考虑源句和一个翻译，而人类评估时会考虑多个替代翻译。这种评估设置的差异可能影响自动化指标的性能。

Method: COMET-polycand 利用同一源句的不同翻译进行比较，而 COMET-polyic 则借鉴了基于检索的上下文学习方法，使用相似源文本的翻译及其人工标注的质量分数来指导评估。

Result: 在 COMET-polycand 中，引入一个额外的翻译可以提高段级指标性能（Kendall's tau-b 相关系数从 0.079 提高到 0.118），添加更多翻译时性能进一步提升。COMET-polyic 通过引入检索示例也取得了类似的改进（Kendall's tau-b 相关系数从 0.079 提高到 0.116）。

Conclusion: 本文提出了两种自动评估机器翻译质量的指标，COMET-polycand 和 COMET-polyic。这些指标通过引入额外信息来改进评估效果，并在实验中显示出性能提升。

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [13] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本文提出了一种自我评估的视觉隐喻生成框架，通过结构化提示和轻量级强化学习实现了良好的隐喻对齐，但与人类偏好之间的差距仍由美学和采样决定。


<details>
  <summary>Details</summary>
Motivation: 视觉隐喻生成是一项具有挑战性的任务，旨在给定输入文本隐喻生成图像。它需要语言理解来将源概念与目标概念绑定，同时保持意义并确保视觉一致性。

Method: 提出了一种自我评估的视觉隐喻生成框架，结合了现有指标和新提出的隐喻分解分数和意义对齐（MA）指标。探索了两种新方法：一种是训练-free 的管道，另一种是基于训练的管道，使用我们提出的自我评估奖励模式。

Result: 在保留测试集上，训练-free 方法在分解、CLIP 和 MA 分数上超过了强大的封闭基线（GPT-4o、Imagen），而训练-based 方法紧随其后。用户研究显示参与者总体更喜欢 GPT-4o，而我们的训练-free 管道领先于开源方法，并在抽象隐喻上接近 Imagen。

Conclusion: 结构化提示和轻量级强化学习在适度计算下能很好地进行隐喻对齐，而与人类偏好之间的剩余差距似乎是由美学和采样驱动的。

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [14] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型的实际建模内容，并提出了变压器架构的计算特性与人类语言能力之间的差异。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型实际上建模的是什么，它们是否告诉我们关于人类能力的信息，还是仅仅是训练它们的语料库的模型。

Method: 通过分析变压器架构的计算架构中的某些不变性来论证。

Result: 变压器架构支持的最多是线性格式的处理，而认知科学告诉我们，人类的语言能力依赖于超线性格式进行计算。

Conclusion: 语言不仅是表达内在状态的手段，也是一种'话语机器'，让我们在适当的上下文中生成新的语言。我们以一种方式使用这项技术；LLM也学会了以不同的方式使用它。

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [15] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的神经机器翻译系统NOOV，用于解决将电子健康记录从英语翻译成西班牙语时的未知词问题和重复词挑战。


<details>
  <summary>Details</summary>
Motivation: 将电子健康记录（EHR）叙述从英语翻译成西班牙语是一个具有临床重要性但具有挑战性的任务，因为缺乏平行对齐语料库以及包含大量未知词。

Method: NOOV结合了从平行对齐语料库中自动学习的双语词典和从大型生物医学知识资源中提取的短语查找表，以缓解NMT中的未知词问题和重复词挑战。

Result: 评估显示，NOOV能够生成更好的EHR翻译，提高了准确性和流畅性。

Conclusion: NOOV能够生成更准确和流畅的EHR翻译，展示了其在临床重要任务中的潜力。

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [16] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本文通过实证研究揭示了PTQ对LLM知识能力的影响，并提出了一个统一的定量框架，以更好地理解和优化量化过程。


<details>
  <summary>Details</summary>
Motivation: 现有的量化模型缩放定律往往忽略了关键的PTQ特定参数和任务特定的敏感性，因此本文旨在填补这些空白，以更好地理解PTQ如何影响不同LLM的知识能力。

Method: 本文通过广泛的实证研究，建立了任务分层的缩放定律，并解耦了LLM知识为记忆和利用能力，开发了一个统一的定量框架，该框架包含了模型大小、有效位宽、校准集大小和组大小等因素。

Result: 研究发现，知识记忆对有效位宽、校准集大小和模型大小的变化表现出显著的敏感性，而知识利用则更为稳健。

Conclusion: 本文的研究结果为理解PTQ对LLM知识能力的影响提供了细粒度的见解，并为开发更有效地保留特定认知功能的知识感知量化策略提供了指导。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [17] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的推理框架TBYS，通过在推理步骤之间插入洞察来改善大型语言模型的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂推理任务时表现出不足，这归因于人类推理模式与训练数据之间的差异。人类在解决复杂问题时会仔细思考，但往往不表达他们的内心想法，导致关键的推理步骤信息缺失。

Method: 提出了一个名为Thinking Before You Speak (TBYS)的推理框架，并设计了一个自动收集和过滤上下文示例的管道，以生成洞察。

Result: 实验结果验证了TBYS框架的有效性，表明该方法能够改善大型语言模型的复杂推理能力。

Conclusion: TBYS框架在挑战性的数学数据集上验证了其有效性，表明通过在推理步骤之间插入洞察可以改善大型语言模型的复杂推理能力。

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [18] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 本文提出CoDe方法，通过动态整合输出概率和知识感知重排序机制，解决了LLMs在忠实性和表达性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在无缝整合知识的同时，难以保持忠实性和表达性，导致输出要么缺乏外部知识支持，损害忠实性，要么过于冗长和不自然，牺牲表达性。本文旨在打破忠实性和表达性之间的权衡。

Method: 本文提出了Collaborative Decoding (CoDe)方法，通过动态整合使用和不使用外部知识生成的输出概率，结合分布差异和模型置信度进行引导，选择性地激活模型内部参数中的相关且可靠表达。此外，引入了知识感知重排序机制，防止过度依赖先前参数化知识，同时确保正确利用提供的外部信息。

Result: 通过全面实验，我们的CoDe框架在多种LLMs和评估指标上表现出色，增强了忠实性而不牺牲表达性。

Conclusion: 本文提出的CoDe框架在增强忠实性的同时不牺牲表达性，验证了其有效性和通用性。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [19] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文提出了一种新的模型架构Emotion Omni，旨在理解用户语音输入的情感内容并生成富有同理心的语音响应。此外，还开发了一个基于开源TTS框架的数据生成管道，以构建一个200k情感对话数据集。


<details>
  <summary>Details</summary>
Motivation: 当前大多数具有共情能力的语音大语言模型都是在大规模数据集上训练的，这种方法需要大量的数据和计算资源。因此，关键挑战是如何在有限的数据和无需大规模训练的情况下开发能够生成共情响应的语音大语言模型。

Method: 本文提出了Emotion Omni模型架构，并开发了一个基于开源TTS框架的数据生成管道，以构建一个200k情感对话数据集。

Result: 本文提出了Emotion Omni模型架构，并构建了一个200k情感对话数据集，支持构建富有同理心的语音助手。

Conclusion: 本文提出了一种新的模型架构Emotion Omni，旨在理解用户语音输入的情感内容并生成富有同理心的语音响应。此外，还开发了一个基于开源TTS框架的数据生成管道，构建了一个200k情感对话数据集，支持构建富有同理心的语音助手。

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [20] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，通过结合模型感知难度和固有样本复杂度，实现难度平衡的提示采样策略，从而提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 多模态链式思维（MCoT）提示的有效性通常受到随机或手动选择示例的限制。这些示例未能考虑模型特定的知识分布和任务的内在复杂性，导致模型性能次优且不稳定。

Method: 我们提出了一种受“因材施教，难度平衡”教学原则启发的新框架。我们将提示选择重新定义为提示课程设计问题：构建一个与模型当前能力相匹配的训练示例集合。我们的方法整合了两个互补的信号：(1) 模型感知难度，通过主动学习设置中的预测不一致来量化，捕捉模型自身认为具有挑战性的地方；(2) 固有样本复杂度，独立于任何模型测量每个问题-图像对的固有难度。通过联合分析这些信号，我们开发了一种难度平衡的采样策略，确保所选提示示例在两个维度上都具有多样性。

Result: 我们在五个具有挑战性的基准测试和多个流行的多模态大语言模型（MLLMs）上进行了广泛的实验，结果表明我们的方法显著且一致地提高了性能，并大大减少了由随机采样引起的性能差异。

Conclusion: 我们的方法在五个具有挑战性的基准测试和多个流行的多模态大语言模型（MLLMs）上进行了广泛实验，结果表明该方法显著且一致地提高了性能，并大大减少了由随机采样引起的性能差异，提供了一种有原则且稳健的方法来增强多模态推理。

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [21] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文揭示了当前医学视觉语言模型在面对语义等价问题重述时的脆弱性，并提出了CCL方法来提高模型的鲁棒性和一致性。


<details>
  <summary>Details</summary>
Motivation: 在高风险的医疗应用中，对同一问题的不同表述保持一致的答案是可靠诊断的关键。然而，当前的医学视觉语言模型（Med-VLMs）在面对语义等价的问题重述时表现出显著的答案波动，这表明需要改进模型的鲁棒性和一致性。

Method: 本文提出了CCL方法，包括知识锚定的一致性学习和偏差感知的对比学习两个关键组件。此外，还构建了一个名为RoMed的数据集，用于评估模型的鲁棒性。

Result: 在RoMed数据集上，SOTA模型如LLaVA-Med的表现明显下降，而CCL方法在三个流行的VQA基准上取得了SOTA性能，并且在RoMed测试集上将答案一致性提高了50%。

Conclusion: 本文提出了一种名为CCL的方法，通过知识锚定的一致性学习和偏差感知的对比学习，显著提高了医学视觉问答模型的鲁棒性和答案一致性。

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [22] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Attention2Probability的新方法，用于提升语音到文本系统的术语识别准确性。该方法在保持低延迟的同时，显著优于现有方法，并展示了当前语音大语言模型在使用术语方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的语音大语言模型在生成领域特定术语或新词方面仍然面临挑战，因此需要一种更准确、轻量级和灵活的方法。

Method: Attention2Probability通过将语音和术语之间的交叉注意力权重转换为存在概率，并采用课程学习来提高检索准确性。

Result: 实验结果表明，Attention2Probability在测试集上显著优于VectorDB方法，最大召回率分别达到92.57%（中文）和86.83%（英文），并且每查询延迟仅为8.71ms。

Conclusion: Attention2Probability显著提升了语音识别和翻译任务中的术语准确性，同时具有较低的延迟。

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [23] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文介绍了一种名为自适应原创性过滤（AOF）的提示框架，通过基于余弦的相似性拒绝来过滤冗余生成，同时确保词汇新颖性和跨语言一致性。实验结果表明，AOF增强的GPT-4o在日语中表现出更高的词汇多样性和更低的冗余度。


<details>
  <summary>Details</summary>
Motivation: 多语言谜题生成挑战大型语言模型（LLMs）在文化流利度与创造性抽象之间取得平衡。标准提示策略——零样本、少样本、思维链——往往重复记忆的谜题或进行浅层改写。

Method: 我们引入了自适应原创性过滤（AOF），这是一种提示框架，使用基于余弦的相似性拒绝来过滤冗余生成，同时强制词汇新颖性和跨语言一致性。

Result: 在三个LLMs和四个语言对上评估，AOF增强的GPT-4o在日语中达到了0.177 Self-BLEU和0.915 Distinct-2，表明词汇多样性提高且冗余减少。

Conclusion: 我们的研究显示，语义排斥可以在不进行任务特定微调的情况下引导文化基础的创造性生成。

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [24] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: 本文提出了一种名为EMMM的解释-检测框架，该框架在延迟、准确性和非专家用户可解释性之间取得了平衡。实验结果表明，EMMM提供了对非专家用户友好的解释，在人类评估者中获得了70%的偏好率，同时与最先进的模型相比具有竞争力的准确性，并保持了低延迟。


<details>
  <summary>Details</summary>
Motivation: Current MGT detection methods often struggle in online conversational settings, reducing the reliability and interpretability essential for trustworthy AI deployment. In customer service scenarios where operators are typically non-expert users, explanation become crucial for trustworthy MGT detection.

Method: EMMM, an explanation-then-detection framework that balances latency, accuracy, and non-expert-oriented interpretability.

Result: Experimental results demonstrate that EMMM provides explanations accessible to non-expert users, with 70% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency, generating outputs within 1 second.

Conclusion: EMMM provides explanations accessible to non-expert users, with 70% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency.

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [25] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的新框架DIVER，旨在同时优化广告标题的质量和多样性。通过设计一个语义和风格感知的数据生成管道以及一个多阶段多目标优化框架，DIVER在实际应用中显著提升了广告效果。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要优化语言模型以提高标题质量和点击率（CTR），但往往忽视了多样性需求，导致输出同质化。因此，我们需要一种新的方法来同时优化多样性和质量。

Method: 我们首先设计了一个语义和风格感知的数据生成管道，自动产生高质量的训练对，包括广告内容和多个多样化的标题。为了在一个前向传递中生成高质量和多样化的广告标题，我们提出了一个分阶段多目标优化框架，结合监督微调（SFT）和强化学习（RL）。

Result: 实验结果表明，DIVER有效地平衡了质量和多样性。在实际工业数据集上的实验显示，该框架在服务数亿用户的大规模内容分享平台上部署后，提高了广告商价值（ADVV）和点击率（CTR）分别4.0%和1.4%。

Conclusion: 实验结果表明，DIVER能够有效平衡质量和多样性。在服务于数亿用户的大型内容分享平台上部署后，该框架提高了广告商价值（ADVV）和点击率（CTR）分别4.0%和1.4%。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [26] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 本文介绍了MECAD数据集和M3HG模型，用于改进多模态对话中的情感原因三元组提取。


<details>
  <summary>Details</summary>
Motivation: 由于现有的MECTEC数据集稀缺且场景单一，以及现有方法未能显式建模情感和因果上下文并忽略不同层次的语义信息融合，导致性能下降，因此需要一个新的数据集和一个更有效的模型。

Method: 本文提出了M3HG模型，该模型通过多模态异构图来显式捕捉情感和因果上下文，并有效融合跨话语和话语内的语义信息。

Result: 本文提出的M3HG模型在与现有最先进的方法相比时表现出色，证明了其有效性。

Conclusion: 本文提出了M3HG模型，该模型在多模态对话中能够有效地捕捉情感和因果上下文，并通过多模态异构图在跨话语和话语内层面融合上下文信息。实验结果表明，M3HG在现有最先进的方法上表现出色。

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [27] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文提出了ChronoRAG框架，用于改善叙事文本的问答任务，通过保持时间顺序和整合上下文信息，提高了回答的准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG索引方法在处理叙事文本时效果有限，因为理解叙事文本需要更广泛的上下文和段落之间的顺序关系。

Method: 提出了一种专门针对叙事文本的RAG框架ChronoRAG，重点在于将分散的文档信息整理成连贯且结构化的段落，并通过显式捕捉和保持检索段落之间的时间顺序来保持叙事流。

Result: 通过在NarrativeQA数据集上的实验，ChronoRAG在需要事实识别和复杂顺序关系理解的任务中表现出显著改进。

Conclusion: ChronoRAG在需要事实识别和复杂顺序关系理解的任务中表现出显著改进，证明了时间顺序推理在解决叙事QA中的重要性。

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [28] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: ThinkDial是首个开放源代码的端到端框架，实现了类似gpt-oss的可控推理，通过三种不同的推理模式平衡计算效率与性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在链式思维推理方面表现出色，但控制其计算努力对于实际部署仍然是一个重大挑战。开源社区尚未实现类似OpenAI的gpt-oss系列的可控推理功能。

Method: 通过端到端训练范式实现预算模式控制，包括预算模式监督微调和两阶段预算感知强化学习，结合自适应奖励塑造。

Result: ThinkDial成功实现了gpt-oss风格的可控推理，支持三种不同的推理模式：高模式（完整推理能力）、中模式（减少50%的token且性能下降小于10%）、低模式（减少75%的token且性能下降小于15%）。

Conclusion: ThinkDial实现了目标压缩-性能权衡，并在保持性能阈值的同时显著减少了响应长度，同时在分布外任务上表现出强大的泛化能力。

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [29] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则的强化学习框架，在中文数据集上取得了最先进的性能，显著提高了召回率，突显了使用强化学习引导大语言模型的优势。


<details>
  <summary>Details</summary>
Motivation: 传统基于编码器-解码器模型的方法在语法错误纠正任务中取得了一定的成功，但大语言模型在该领域的应用仍缺乏探索。当前研究主要依赖监督微调来训练大语言模型直接生成更正后的句子，这限制了模型强大的推理能力。

Method: 本文提出了一种基于规则的强化学习框架。

Result: 通过在中国数据集上的实验，基于规则的强化学习框架取得了最先进的性能，显著提高了召回率。

Conclusion: 本文结果清楚地突显了使用强化学习引导大语言模型的优势，为未来语法错误纠正的发展提供了一个更可控和可靠的范式。

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [30] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: 本文介绍了可控对话主题检测任务，作为DSTC 12的竞赛赛道，旨在通过用户偏好数据控制主题聚类的粒度。


<details>
  <summary>Details</summary>
Motivation: 对话分析中需要自动识别和分类对话中的主题，以减少手动分析的工作量。传统对话意图检测依赖于固定的意图集，而主题检测则提供了更灵活的用户面向摘要。

Method: 本文提出了可控对话主题检测问题，将其作为DSTC 12的一个竞赛赛道，采用联合聚类和主题标记的方法进行处理。

Result: 本文概述了问题、相关数据集和评估指标，并讨论了参赛团队的提交情况。

Conclusion: 本文介绍了对话分析中的主题检测任务，并将其作为DSTC 12的一个公开竞赛赛道。通过提供用户偏好数据，实现了对主题聚类粒度的控制。

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [31] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: LaTeXTrans is a multi-agent system designed to translate LaTeX-formatted documents while preserving their structure and meaning, showing superior performance compared to existing MT systems.


<details>
  <summary>Details</summary>
Motivation: Translating structured LaTeX-formatted documents remains a significant challenge due to the need to preserve natural language, domain-specific syntax, and semantic integrity.

Method: LaTeXTrans is a collaborative multi-agent system that ensures format preservation, structural fidelity, and terminology consistency through six specialized agents: a Parser, Translator, Validator, Summarizer, Terminology Extractor, and Generator.

Result: Experimental results demonstrate that LaTeXTrans can outperform mainstream MT systems in both translation accuracy and structural fidelity.

Conclusion: LaTeXTrans can outperform mainstream MT systems in both translation accuracy and structural fidelity, offering an effective and practical solution for translating LaTeX-formatted documents.

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [32] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: 本文提出了一种新的自我监督假新闻检测框架，通过结合复杂语义关系和新闻传播动态，有效区分虚假和真实新闻，并在有限的标记数据集上表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，虚假信息的激增导致了重大的社会挑战。现有的方法往往难以捕捉长距离依赖关系、复杂的语义关系以及影响新闻传播的社会动态。此外，这些方法需要大量的标记数据集，使其部署资源密集。

Method: 我们提出了一种新颖的自我监督假新闻检测框架，该框架结合了使用抽象语义表示（AMR）的复杂语义关系和新闻传播动态。我们引入了一个基于大语言模型（LLM）的图对比损失（LGCL），利用由LLM生成的负锚点来以零样本方式增强特征可分性。为了融入社会背景，我们采用了一个多视图图掩码自动编码器，从社会背景图中学习新闻传播特征。

Result: 广泛的实验表明，我们的自我监督框架在有限的标记数据集上表现出优于其他最先进的方法的性能，同时提高了泛化能力。

Conclusion: 我们的方法在自我监督的情况下有效地区分了虚假新闻和真实新闻，并且在有限的标记数据集上表现出优于其他最先进的方法的性能，同时提高了泛化能力。

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [33] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种程序辅助合成框架，用于生成高质量的数学语料库，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 增强大型语言模型（LLMs）的数学推理需要高质量的训练数据，但传统方法在可扩展性、成本和数据可靠性方面面临关键挑战。

Method: 我们提出了一种程序辅助合成框架，该框架系统地生成具有保证多样性和正确性的高质量数学语料库。该框架整合了数学知识系统和领域特定工具来创建可执行程序，并将这些程序翻译成自然语言的问题-解决方案对，并通过双边验证机制进行验证。

Result: 我们生成了1230万个这样的问题解决三元组。实验表明，微调我们数据的模型显著提高了它们的推理能力，在多个基准数据集上达到了最先进的性能。

Conclusion: 实验表明，微调我们数据的模型显著提高了它们的推理能力，在多个基准数据集上达到了最先进的性能，并展示了我们合成方法的有效性。

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [34] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: ConfTuner is a fine-tuning method for improving the calibration of large language models (LLMs) by introducing a new loss function called the tokenized Brier score. This method does not require ground-truth confidence scores or proxy confidence estimates and has been shown to improve calibration across various tasks and generalize to black-box models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as 'overconfidence'. Recent efforts have focused on calibrating LLMs' verbalized confidence, but existing approaches have limited effectiveness and generalizability.

Method: ConfTuner is a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. It relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule.

Result: ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems.

Conclusion: ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems.

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [35] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文介绍了一种基于进化算法的新型自动提示方法ReflectivePrompt，该方法通过反思进化方法提高了提示搜索的精度和全面性，并在多个数据集上表现出优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着提示工程的快速发展，自动选择优化提示的过程变得越来越重要。本文旨在提出一种更有效的方法来寻找最优提示。

Method: ReflectivePrompt是一种基于进化算法的新型自动提示方法，采用反思进化方法进行更精确和全面的最优提示搜索。它在交叉前使用短期和长期反思操作，并通过精英突变来提高修改的质量。

Result: ReflectivePrompt在33个数据集上的测试显示，平均指标相对于当前最先进的方法有显著提升（例如，在BBH上比EvoPrompt提高了28%）。

Conclusion: ReflectivePrompt在使用开放获取的大语言模型进行分类和文本生成任务时，相对于当前最先进的方法表现出显著的改进，因此成为基于进化算法的自动提示中最有效的解决方案之一。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [36] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: 本文介绍了一种名为LLM辅助内容分析（LACA）的方法，用于在计算教育研究中处理大量文本数据，从而提高研究的普遍性和严谨性。


<details>
  <summary>Details</summary>
Motivation: 计算教育研究（CER）通常由从业者发起，旨在改进他们自己和整个学科的教学实践。然而，由于缺乏同事、资源或能力，许多研究人员难以进行具有普遍性或严谨性的研究。因此，需要一种能够处理大量定性数据并减轻研究人员负担的方法。

Method: 本文提出了一种名为LLM辅助内容分析（LACA）的方法，该方法结合了内容分析和大型语言模型的使用，使研究人员能够以更高效的方式处理大量文本数据。

Result: 本文通过一个计算教育数据集展示了LACA方法如何以可重复和严谨的方式应用。

Conclusion: 本文认为，LACA方法在计算教育研究中具有潜力，能够帮助研究人员从更广泛的研究中获得更具普遍性的结果，并且与其他类似方法的发展一起，有助于提升计算教育研究的实践和研究质量。

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [37] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: 本研究利用自然语言处理技术分析了六个欧洲国家议会的演讲，发现所有议会中都存在一致的 affective polarization，并且 reciprocity 是其促成机制之一。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探讨 affective polarization 是否存在于欧洲议会中，并探索其可能的机制。

Method: 我们利用自然语言处理技术，分析了来自六个欧洲国家议会的演讲语料库，以估计议员的情感，并通过比较对对立群体成员和自己群体成员的负面情绪水平，发现了 affectively polarized interactions 的模式。

Result: 研究发现，所有六个欧洲议会中都存在一致的 affective polarization。虽然活动量与负面情绪有关，但活跃和不活跃的议员之间没有观察到 affective polarization 的差异。此外，我们展示了 reciprocity 是影响议会成员之间 affective polarization 的一个促成机制。

Conclusion: 我们的研究发现，所有六个欧洲议会中都存在一致的 affective polarization。此外，我们展示了 reciprocity 是影响议会成员之间 affective polarization 的一个促成机制。

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [38] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体框架，用于生成用于RAG评估的合成QA数据集，优先考虑语义多样性和隐私保护。实验表明，该方法在多样性方面优于基线方法，并在领域特定数据集上实现了稳健的隐私屏蔽。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统的评估工作主要集中在性能指标的开发上，而对底层评估数据集的设计和质量关注较少。然而，这些数据集在实现有意义、可靠的评估中起着关键作用。因此，本文旨在设计一种新的多智能体框架，以生成合成QA数据集，优先考虑语义多样性和隐私保护。

Method: 本文提出了一种多智能体框架，包括：(1) 一个利用聚类技术最大化主题覆盖和语义变异的多样性代理；(2) 一个检测和屏蔽跨多个领域的敏感信息的隐私代理；(3) 一个合成私有和多样化QA对的QA整理代理，适合作为RAG评估的地面真值。

Result: 实验结果表明，本文提出的评估集在多样性方面优于基线方法，并在领域特定数据集上实现了稳健的隐私屏蔽。

Conclusion: 本文提出了一种多智能体框架，用于生成用于RAG评估的合成QA数据集，该框架优先考虑语义多样性和隐私保护。实验表明，该方法在多样性方面优于基线方法，并在领域特定数据集上实现了稳健的隐私屏蔽。本文为更安全、更全面的RAG系统评估提供了一条实用且符合伦理的路径，为未来与不断发展的AI法规和合规标准相一致的改进奠定了基础。

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [39] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: This paper presents a framework for neural models to develop an AI Mother Tongue, which is a native symbolic language that supports intuitive reasoning, compositional symbol chains, and inherent interpretability.


<details>
  <summary>Details</summary>
Motivation: To develop a framework where neural models can have an AI Mother Tongue, a native symbolic language that supports intuitive reasoning, compositional symbol chains, and inherent interpretability.

Method: We introduce complementary training objectives to enhance symbol purity and decision sparsity, and employ a sequential specialization strategy to first build broad symbolic competence and then refine intuitive judgments.

Result: Experiments on AI tasks demonstrate competitive accuracy alongside verifiable reasoning traces.

Conclusion: AI Mother Tongue can serve as a unified mechanism for interpretability, intuition, and symbolic reasoning in neural models.

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [40] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文提出了一种名为 DistillPrompt 的新自动提示方法，通过多阶段集成任务特定信息并利用训练数据优化提示，实验结果表明其在关键指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）领域的广泛研究，提示工程发展迅速，因此需要一种更有效的自动提示方法。

Method: DistillPrompt 是一种基于大型语言模型的自动提示方法，通过多阶段集成任务特定信息到提示中，并使用训练数据进行优化。

Result: 在文本分类和生成任务的不同数据集上测试了 DistillPrompt，结果表明其在关键指标上相比现有方法有显著提升（例如，在整个数据集上平均提高了 20.12%）。

Conclusion: DistillPrompt 是一种有效的非梯度方法，在自动提示领域表现出色。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [41] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: 本文介绍了MovieCORE，这是一个新的视频问答（VQA）数据集，旨在探测对电影内容的更深层次认知理解。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注表面层的理解，而MovieCORE旨在通过强调需要系统-2思考的问题来深入探究电影内容的认知理解。

Method: 我们提出了一种创新的代理头脑风暴方法，利用多个大型语言模型（LLMs）作为思维代理来生成和优化高质量的问题-答案对。我们还引入了一个代理增强模块，Agentic Choice Enhancement (ACE)，通过高达25%的改进来提高模型的推理能力。

Result: 我们开发了一套认知测试来评估数据集的质量，包括深度、思想激发潜力和句法复杂性。我们还提出了一个全面的评估方案，用于评估VQA模型在更深层次认知任务上的表现。

Conclusion: 我们的工作有助于推动AI系统对电影的理解，并提供了关于当前VQA模型在面对更复杂、细微的电影内容问题时的能力和局限性的宝贵见解。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [42] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: HiPlan is a hierarchical planning framework that improves LLM-based agents' decision-making by providing adaptive global-local guidance through milestone action guides and step-wise hints.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents struggle with complex, long-horizon planning scenarios due to lack of macroscopic guidance and insufficient continuous oversight during execution.

Method: HiPlan is a hierarchical planning framework that provides adaptive global-local guidance to boost LLM-based agents' decision-making by decomposing complex tasks into milestone action guides and step-wise hints.

Result: HiPlan substantially outperforms strong baselines across two challenging benchmarks, and ablation studies validate the complementary benefits of its hierarchical components.

Conclusion: HiPlan significantly outperforms strong baselines in complex tasks and validates the benefits of its hierarchical components.

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [43] ["Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
*Tom Röhr,Soumyadeep Roy,Fares Al Mohamad,Jens-Michalis Papaioannou,Wolfgang Nejdl,Felix Gers,Alexander Löser*

Main category: cs.CL

TL;DR: 本研究首次探讨了医生在医生-患者对话中的意图轨迹，开发了一个基于SOAP框架的细粒度医生意图分类法，并通过大规模标注工作创建了一个重要的资源。研究结果表明，模型在理解医疗对话结构方面表现良好，但识别SOAP类别之间的转换仍有挑战。意图过滤对医疗对话摘要有显著提升。


<details>
  <summary>Details</summary>
Motivation: 目前尚无研究探讨医生在医生-患者对话中的意图轨迹。我们希望通过这项研究填补这一空白，并为设计差分诊断系统提供见解。

Method: 我们使用了Aci-bench数据集，并与医疗专业人士合作开发了一个基于SOAP框架的细粒度医生意图分类法。我们进行了大规模的标注工作，标记了超过5000个医生-患者回合，并公开了代码和数据。

Result: 我们的模型在理解医疗对话结构方面表现出高准确性，但在识别SOAP类别之间的转换时表现不佳。意图过滤对医疗对话摘要的性能有显著提升。

Conclusion: 我们的研究展示了模型在理解医疗对话结构方面的高准确性，但识别SOAP类别之间的转换仍然存在困难。我们还报告了医疗对话结构中的常见轨迹，为设计“差异诊断”系统提供了有价值的见解。最后，我们发现意图过滤对医疗对话摘要有显著的性能提升。

Abstract: In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.

</details>


### [44] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
*Yue Li,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 本文研究了LLM如何通过上下文学习和参数高效微调来适应极低资源语言，并提供了相关实践指南。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据的缺乏，极低资源语言尤其是使用罕见脚本的语言，仍然主要由大型语言模型（LLMs）支持不足。本文旨在分析LLM是否可以通过上下文学习（ICL）或参数高效微调（PEFT）来掌握这些语言。

Method: 本文系统评估了三种最先进的多语言LLM在20种低资源语言上的表现，比较了基于上下文学习（ICL）和参数高效微调（PEFT）的方法。

Result: 研究发现，当语言及其脚本在LLM中极度不常见时，PEFT存在局限性。相比之下，零样本ICL结合语言对齐在极低资源语言上表现出色，而少样本ICL或PEFT则更适用于LLM中相对更好的语言。

Conclusion: 本文总结了针对极低资源语言适应LLM的指南，例如避免在未见过的脚本语言上微调多语言模型。

Abstract: Extremely low-resource languages, especially those written in rare scripts,
as shown in Figure 1, remain largely unsupported by large language models
(LLMs). This is due in part to compounding factors such as the lack of training
data. This paper delivers the first comprehensive analysis of whether LLMs can
acquire such languages purely via in-context learning (ICL), with or without
auxiliary alignment signals, and how these methods compare to
parameter-efficient fine-tuning (PEFT). We systematically evaluate 20
under-represented languages across three state-of-the-art multilingual LLMs.
Our findings highlight the limitation of PEFT when both language and its script
are extremely under-represented by the LLM. In contrast, zero-shot ICL with
language alignment is impressively effective on extremely low-resource
languages, while few-shot ICL or PEFT is more beneficial for languages
relatively better represented by LLMs. For LLM practitioners working on
extremely low-resource languages, we summarise guidelines grounded by our
results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning
a multilingual model on languages of unseen scripts.

</details>


### [45] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
*Mathew Henrickson*

Main category: cs.CL

TL;DR: 本研究提出一种基于RAG框架的检索增强生成方法，用于艺术出处研究，通过自然语言和多语言搜索提升档案数据的可访问性。


<details>
  <summary>Details</summary>
Motivation: 解决碎片化、多语言档案数据带来的检索难题，以及现有搜索门户对精确元数据的依赖限制探索性搜索的问题。

Method: 提出了一种检索增强生成（RAG）框架，通过语义检索和上下文摘要实现自然语言和多语言搜索，减少对元数据结构的依赖。

Result: 通过从Getty Provenance Index - German Sales中抽取的10,000条记录样本评估RAG的能力，结果显示该方法有效。

Conclusion: 该方法为艺术市场档案的导航提供了一个可扩展的解决方案，为历史学家和文化遗产专业人员进行历史敏感研究提供了实用工具。

Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for
art provenance studies, focusing on the Getty Provenance Index. Provenance
research establishes the ownership history of artworks, which is essential for
verifying authenticity, supporting restitution and legal claims, and
understanding the cultural and historical context of art objects. The process
is complicated by fragmented, multilingual archival data that hinders efficient
retrieval. Current search portals require precise metadata, limiting
exploratory searches. Our method enables natural-language and multilingual
searches through semantic retrieval and contextual summarization, reducing
dependence on metadata structures. We assess RAG's capability to retrieve and
summarize auction records using a 10,000-record sample from the Getty
Provenance Index - German Sales. The results show this approach provides a
scalable solution for navigating art market archives, offering a practical tool
for historians and cultural heritage professionals conducting historically
sensitive research.

</details>


### [46] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
*Thomas Compton*

Main category: cs.CL

TL;DR: 本文介绍了一种结合词汇和语义方法的透明定量话语分析框架，通过使用Python工具和BERTopic建模过程，实现了细粒度控制和主题优化，强调了代码透明度和方法三角验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和计算工具的兴起，定量话语分析得到了越来越多的应用。然而，依赖于MAXQDA和NVivo等黑箱软件可能会削弱方法论的透明度和与研究目标的一致性。

Method: 本文提出了一种混合的、透明的QDA框架，结合了词汇和语义方法以实现三角验证、可重复性和可解释性。通过使用Python管道和各种自然语言处理工具，如NLTK、spaCy和Sentence Transformers，实现了对预处理、词干提取和嵌入生成的细粒度控制。此外，还详细描述了迭代的BERTopic建模过程，包括UMAP降维、HDBSCAN聚类和c-TF-IDF关键词提取，并通过参数调整和多次运行优化以提高主题的一致性和覆盖范围。

Result: 通过将精确的词汇搜索与上下文感知的语义聚类进行对比，本文论证了一种多层方法，可以减轻单独使用任一方法的局限性。

Conclusion: 本文强调了代码级别的透明度、研究者的自主性和计算话语研究中的方法三角验证的重要性。

Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of
Large Language Models and computational tools. However, reliance on black box
software such as MAXQDA and NVivo risks undermining methodological transparency
and alignment with research goals. This paper presents a hybrid, transparent
framework for QDA that combines lexical and semantic methods to enable
triangulation, reproducibility, and interpretability. Drawing from a case study
in historical political discourse, we demonstrate how custom Python pipelines
using NLTK, spaCy, and Sentence Transformers allow fine-grained control over
preprocessing, lemmatisation, and embedding generation. We further detail our
iterative BERTopic modelling process, incorporating UMAP dimensionality
reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised
through parameter tuning and multiple runs to enhance topic coherence and
coverage. By juxtaposing precise lexical searches with context-aware semantic
clustering, we argue for a multi-layered approach that mitigates the
limitations of either method in isolation. Our workflow underscores the
importance of code-level transparency, researcher agency, and methodological
triangulation in computational discourse studies. Code and supplementary
materials are available via GitHub.

</details>


### [47] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
*Zhikai Ding,Shiyu Ni,Keping Bi*

Main category: cs.CL

TL;DR: 本文研究了LVLMs的知识边界感知能力，发现其存在提升空间，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 可靠模型应了解自身的知识边界，但现有的LVLMs在这一方面存在不足，因此需要研究其知识边界感知能力。

Method: 本文评估了三种置信度信号（概率置信度、答案一致性置信度和语言化置信度），并适应了来自大型语言模型的置信度校准方法，提出了三种有效的方法。

Result: 实验结果表明，概率和一致性置信度信号更为可靠，而语言化置信度容易导致过度自信。此外，联合处理视觉和文本输入会降低问答性能，但提高感知水平。

Conclusion: 本文研究了LVLMs在视觉问答任务中的知识边界感知能力，并提出了改进方法。实验表明，虽然LVLMs具备一定的感知水平，但仍存在提升空间。

Abstract: Large vision-language models (LVLMs) demonstrate strong visual question
answering (VQA) capabilities but are shown to hallucinate. A reliable model
should perceive its knowledge boundaries-knowing what it knows and what it does
not. This paper investigates LVLMs' perception of their knowledge boundaries by
evaluating three types of confidence signals: probabilistic confidence, answer
consistency-based confidence, and verbalized confidence. Experiments on three
LVLMs across three VQA datasets show that, although LVLMs possess a reasonable
perception level, there is substantial room for improvement. Among the three
confidences, probabilistic and consistency-based signals are more reliable
indicators, while verbalized confidence often leads to overconfidence. To
enhance LVLMs' perception, we adapt several established confidence calibration
methods from Large Language Models (LLMs) and propose three effective methods.
Additionally, we compare LVLMs with their LLM counterparts, finding that
jointly processing visual and textual inputs decreases question-answering
performance but reduces confidence, resulting in an improved perception level
compared to LLMs.

</details>


### [48] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
*Alan Li,Yixin Liu,Arpan Sarkar,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: 本文介绍了SciReas和SciReas-Pro，以及KRUX框架，用于评估科学推理性能并研究知识和推理的作用。结果表明，检索任务相关知识是LLMs在科学推理中的关键瓶颈，而添加外部知识和增强推理能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏广泛采用的全面基准来评估科学推理，且很少有方法系统地分离知识和推理在这些任务中的不同作用。

Method: 本文引入了SciReas和SciReas-Pro，以及KRUX框架，用于评估科学推理性能并研究知识和推理在科学任务中的不同作用。

Result: 通过综合评估，揭示了当仅依赖单一基准时隐藏的科学推理性能见解。此外，发现检索任务相关知识是LLMs在科学推理中的关键瓶颈，推理模型在添加外部知识后受益显著，增强口头推理提高了LLMs提取任务相关知识的能力。

Conclusion: 本文提出了SciReas和SciReas-Pro，以及KRUX框架，用于研究科学任务中推理和知识的不同作用。最后，我们进行了轻量级分析，并发布了SciLit01作为科学推理的强基线模型。

Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both
deep domain knowledge and the ability to apply such knowledge through complex
reasoning. While automated scientific reasoners hold great promise for
assisting human scientists, there is currently no widely adopted holistic
benchmark for evaluating scientific reasoning, and few approaches
systematically disentangle the distinct roles of knowledge and reasoning in
these tasks. To address these gaps, we introduce SciReas, a diverse suite of
existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a
selective subset that requires more complex reasoning. Our holistic evaluation
surfaces insights about scientific reasoning performance that remain hidden
when relying on individual benchmarks alone. We then propose KRUX, a probing
framework for studying the distinct roles of reasoning and knowledge in
scientific tasks. Combining the two, we conduct an in-depth analysis that
yields several key findings: (1) Retrieving task-relevant knowledge from model
parameters is a critical bottleneck for LLMs in scientific reasoning; (2)
Reasoning models consistently benefit from external knowledge added in-context
on top of the reasoning enhancement; (3) Enhancing verbalized reasoning
improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct
a lightweight analysis, comparing our science-focused data composition with
concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline
for scientific reasoning.

</details>


### [49] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: VibeVoice是一种新型模型，利用next-token diffusion方法实现长时多说话人语音合成，具有更高的计算效率和音频保真度。


<details>
  <summary>Details</summary>
Motivation: 为了实现长时语音合成，需要一种高效且能保持音频保真度的方法。现有的方法在处理长序列时存在计算效率低的问题。

Method: VibeVoice采用next-token diffusion方法，通过自回归生成潜在向量来统一建模连续数据。引入了一种新的连续语音分词器，相比流行的Encodec模型，数据压缩提高了80倍，同时保持了相当的性能。

Result: VibeVoice能够合成长达90分钟的多说话人语音，具有更高的计算效率和音频保真度，同时能够捕捉真实的对话氛围。

Conclusion: VibeVoice可以合成长达90分钟的多说话人语音，捕捉真实的对话氛围，并超越开源和专有对话模型。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [50] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
*Isabel Cachola,Daniel Khashabi,Mark Dredze*

Main category: cs.CL

TL;DR: 本文对PLS文献进行了全面调查，发现传统可读性指标与人类判断的相关性较差，而语言模型在评估可读性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前在PLS中使用的可读性评估标准是传统的可读性指标，如Flesch-Kincaid Grade Level (FKGL)。然而，这些指标在PLS中与人类可读性判断的比较尚未进行。

Method: 我们评估了8种可读性指标，并展示了语言模型（LMs）在判断可读性方面比传统指标表现更好。

Result: 我们发现大多数可读性指标与人类判断的相关性较差，包括最常用的指标FKGL。语言模型在判断可读性方面表现更好，最佳模型与人类判断的皮尔逊相关系数为0.56。

Conclusion: 基于这些发现，我们提出了评估通俗语言摘要的最佳实践建议。

Abstract: Plain Language Summarization (PLS) aims to distill complex documents into
accessible summaries for non-expert audiences. In this paper, we conduct a
thorough survey of PLS literature, and identify that the current standard
practice for readability evaluation is to use traditional readability metrics,
such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in
other fields, these metrics have not been compared to human readability
judgments in PLS. We evaluate 8 readability metrics and show that most
correlate poorly with human judgments, including the most popular metric, FKGL.
We then show that Language Models (LMs) are better judges of readability, with
the best-performing model achieving a Pearson correlation of 0.56 with human
judgments. Extending our analysis to PLS datasets, which contain summaries
aimed at non-expert audiences, we find that LMs better capture deeper measures
of readability, such as required background knowledge, and lead to different
conclusions than the traditional metrics. Based on these findings, we offer
recommendations for best practices in the evaluation of plain language
summaries. We release our analysis code and survey data.

</details>


### [51] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 本文提出了生成式界面的概念，通过主动生成用户界面来提升人机交互体验，并通过多维评估框架验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 大多数系统仍受制于线性的请求-响应格式，这在多轮、信息密集和探索性任务中往往使交互效率低下。为了克服这些限制，我们提出了语言模型的生成式界面，这是一种范式，其中LLM通过主动生成用户界面来响应用户查询，从而实现更适应性和交互性的参与。

Method: 我们的框架利用特定于界面的结构化表示和迭代改进，将用户查询转化为任务特定的用户界面。此外，我们引入了一个多维评估框架，以在不同任务、交互模式和查询类型上比较生成式界面与传统的基于聊天的界面。

Result: 结果表明，生成式界面在超过70%的情况下优于对话式界面，人类用户更倾向于使用它们。

Conclusion: 这些发现明确了用户何时以及为何更喜欢生成式界面，为未来人机交互的进步铺平了道路。

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [52] [H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems](https://arxiv.org/abs/2508.18295)
*Huangyu Dai,Lingtao Mao,Ben Chen,Zihan Wang,Zihan Liang,Ying Han,Chenyi Lei,Han Li*

Main category: cs.SD

TL;DR: 本文介绍了一种新的热点词定制系统H-PRM，通过声学相似性识别最相关的热点词候选，显著提高了ASR中的热点词识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有的模型在处理大规模热点词时表现不佳，因为随着热点词数量的增加，识别率会大幅下降。

Method: 本文提出了一种新的热点词定制系统，利用热点词预检索模块（H-PRM）通过测量热点词与语音段之间的声学相似性来识别最相关的热点词候选。此外，还通过基于提示的方法将H-PRM集成到音频大语言模型中。

Result: H-PRM经过广泛测试，证明其优于现有方法，能够显著提高热点词后召回率（PRR）。

Conclusion: H-PRM可以显著提升ASR中的热点词定制效果，并为该领域提供新的研究方向。

Abstract: Hotword customization is crucial in ASR to enhance the accuracy of
domain-specific terms. It has been primarily driven by the advancements in
traditional models and Audio large language models (LLMs). However, existing
models often struggle with large-scale hotwords, as the recognition rate drops
dramatically with the number of hotwords increasing. In this paper, we
introduce a novel hotword customization system that utilizes a hotword
pre-retrieval module (H-PRM) to identify the most relevant hotword candidate by
measuring the acoustic similarity between the hotwords and the speech segment.
This plug-and-play solution can be easily integrated into traditional models
such as SeACo-Paraformer, significantly enhancing hotwords post-recall rate
(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a
prompt-based approach, enabling seamless customization of hotwords. Extensive
testing validates that H-PRM can outperform existing methods, showing a new
direction for hotword customization in ASR.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
*Jiajie He,Yuechun Gu,Min-Chun Chen,Keke Chen*

Main category: cs.IR

TL;DR: 研究分析了基于大型语言模型的推荐系统中的隐私攻击问题，设计并评估了四种成员推理攻击，发现攻击是可行的。


<details>
  <summary>Details</summary>
Motivation: 由于LLM推荐系统可能暴露用户的敏感历史交互信息，因此需要研究其隐私攻击问题。

Method: 设计了四种成员推理攻击（MIAs），包括直接询问、幻觉、相似性和中毒攻击，并在三个LLM和两个推荐系统基准数据集上进行了评估。

Result: 结果表明，直接询问和中毒攻击具有显著的攻击优势，且攻击效果受系统提示中的样本数量和受害者位置等因素影响。

Conclusion: 研究确认了LLM推荐系统中的成员推理攻击威胁是现实的，特别是直接询问和中毒攻击显示出显著的攻击优势。

Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly
adapt recommendation systems to different domains. It utilizes in-context
learning (ICL), i.e., the prompts, to customize the recommendation functions,
which include sensitive historical user-specific item interactions, e.g.,
implicit feedback like clicked items or explicit product reviews. Such private
information may be exposed to novel privacy attack. However, no study has been
done on this important issue. We design four membership inference attacks
(MIAs), aiming to reveal whether victims' historical interactions have been
used by system prompts. They are \emph{direct inquiry, hallucination,
similarity, and poisoning attacks}, each of which utilizes the unique features
of LLMs or RecSys. We have carefully evaluated them on three LLMs that have
been used to develop ICL-LLM RecSys and two well-known RecSys benchmark
datasets. The results confirm that the MIA threat on LLM RecSys is realistic:
direct inquiry and poisoning attacks showing significantly high attack
advantages. We have also analyzed the factors affecting these attacks, such as
the number of shots in system prompts and the position of the victim in the
shots.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: 该论文介绍了TRIAGE，这是一种利用大型语言模型（LLM）将CVE映射到ATT&CK知识库中相关技术的自动化方法。评估结果显示，上下文学习优于单独的映射方法，混合方法提高了对攻击技术的召回率。


<details>
  <summary>Details</summary>
Motivation: Manually linking CVEs to their corresponding TTPs is challenging and time-consuming, and the high volume of new vulnerabilities published annually makes automated support desirable.

Method: TRIAGE, a two-pronged automated approach that uses Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK knowledge base.

Result: In-context learning outperforms individual mapping methods, and the hybrid approach improves recall of exploitation techniques. GPT-4o-mini performs better than Llama3.3-70B on this task.

Conclusion: LLMs can be used to automatically predict the impact of cybersecurity vulnerabilities, and TRIAGE makes the process of mapping CVEs to ATT&CK more efficient.

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [55] [UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)
*Runpeng Geng,Yanting Wang,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: UniC-RAG is a universal knowledge corruption attack against RAG systems that can simultaneously attack a large number of user queries with diverse topics and domains, achieving high attack success rates and highlighting the need for new defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing studies mainly focus on attacking specific queries or queries with similar topics, but UniC-RAG aims to attack a large number of user queries with diverse topics and domains.

Method: UniC-RAG is formulated as an optimization problem and solved using a balanced similarity-based clustering method to enhance attack effectiveness.

Result: UniC-RAG achieved over 90% attack success rate by injecting 100 adversarial texts into a knowledge database with millions of texts to simultaneously attack a large set of user queries.

Conclusion: UniC-RAG is highly effective and significantly outperforms baselines, and existing defenses are insufficient to defend against it, highlighting the need for new defense mechanisms in RAG systems.

Abstract: Retrieval-augmented generation (RAG) systems are widely deployed in
real-world applications in diverse domains such as finance, healthcare, and
cybersecurity. However, many studies showed that they are vulnerable to
knowledge corruption attacks, where an attacker can inject adversarial texts
into the knowledge database of a RAG system to induce the LLM to generate
attacker-desired outputs. Existing studies mainly focus on attacking specific
queries or queries with similar topics (or keywords). In this work, we propose
UniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike
prior work, UniC-RAG jointly optimizes a small number of adversarial texts that
can simultaneously attack a large number of user queries with diverse topics
and domains, enabling an attacker to achieve various malicious objectives, such
as directing users to malicious websites, triggering harmful command execution,
or launching denial-of-service attacks. We formulate UniC-RAG as an
optimization problem and further design an effective solution to solve it,
including a balanced similarity-based clustering method to enhance the attack's
effectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly
effective and significantly outperforms baselines. For instance, UniC-RAG could
achieve over 90% attack success rate by injecting 100 adversarial texts into a
knowledge database with millions of texts to simultaneously attack a large set
of user queries (e.g., 2,000). Additionally, we evaluate existing defenses and
show that they are insufficient to defend against UniC-RAG, highlighting the
need for new defense mechanisms in RAG systems.

</details>


### [56] [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
*Shaswata Mitra,Azim Bazarov,Martin Duclos,Sudip Mittal,Aritran Piplai,Md Rayhanur Rahman,Edward Zieglar,Shahram Rahimi*

Main category: cs.CR

TL;DR: FALCON是一个基于大型语言模型的自主代理框架，用于实时生成和评估入侵检测系统规则，显示出高准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 由于网络威胁的不断演变，需要频繁更新规则，这会延迟部署时间并削弱整体的安全准备。

Method: FALCON是一个自主的代理框架，能够从CTI数据中实时生成可部署的IDS规则，并使用内置的多阶段验证器进行评估。

Result: FALCON在自动规则生成方面表现出色，平均准确率为95%，通过与多个网络安全分析师的定性评估，得出84%的评分者间一致性。

Conclusion: 这些结果强调了基于LLM的数据挖掘在实时网络威胁缓解中的可行性和有效性。

Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

</details>


### [57] [The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization](https://arxiv.org/abs/2508.18976)
*Stephen Meisenbacher,Alexandra Klymenko,Andreea-Elena Bodea,Florian Matthes*

Main category: cs.CR

TL;DR: 本文探讨了如何利用大型语言模型（LLMs）来利用差分隐私（DP）文本净化中的上下文漏洞，并展示了LLM在数据重建攻击中的双刃剑效应。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）具有强大的上下文理解和推理能力，我们探索了LLMs在多大程度上可以被用来利用DP-净化文本的上下文漏洞。

Method: 我们扩展了之前的工作，不仅在使用先进的LLM方面，还在测试各种隐私级别的更广泛的去噪机制。

Result: 我们的实验揭示了基于LLM的数据重建攻击对隐私和效用的双刃剑效应：虽然LLM确实可以推断原始语义并有时降低经验隐私保护，但它们也可以用于改善DP-净化文本的质量和隐私。

Conclusion: 基于我们的发现，我们提出了使用LLM数据重建作为后处理步骤的建议，通过以对抗性思维来增加隐私保护。

Abstract: Differentially private text sanitization refers to the process of privatizing
texts under the framework of Differential Privacy (DP), providing provable
privacy guarantees while also empirically defending against adversaries seeking
to harm privacy. Despite their simplicity, DP text sanitization methods
operating at the word level exhibit a number of shortcomings, among them the
tendency to leave contextual clues from the original texts due to randomization
during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual
vulnerability}$. Given the powerful contextual understanding and inference
capabilities of Large Language Models (LLMs), we explore to what extent LLMs
can be leveraged to exploit the contextual vulnerability of DP-sanitized texts.
We expand on previous work not only in the use of advanced LLMs, but also in
testing a broader range of sanitization mechanisms at various privacy levels.
Our experiments uncover a double-edged sword effect of LLM-based data
reconstruction attacks on privacy and utility: while LLMs can indeed infer
original semantics and sometimes degrade empirical privacy protections, they
can also be used for good, to improve the quality and privacy of DP-sanitized
texts. Based on our findings, we propose recommendations for using LLM data
reconstruction as a post-processing step, serving to increase privacy
protection by thinking adversarially.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [58] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: CTF-Dojo is a large-scale executable runtime for training LLMs with verifiable feedback, featuring 658 CTF-style challenges. CTF-Forge automates the creation of execution environments. Training on CTF-Dojo resulted in significant performance gains for ML agents.


<details>
  <summary>Details</summary>
Motivation: The scarcity of scalable and generalizable execution-grounded environments limits progress in training more capable ML agents.

Method: We introduced CTF-Dojo, a large-scale executable runtime tailored for training LLMs with verifiable feedback, and developed CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments.

Result: We trained LLM-based agents on 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art.

Conclusion: CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [59] [Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project](https://arxiv.org/abs/2508.18512)
*Antony C Chan*

Main category: physics.optics

TL;DR: 本文通过96-Eyes项目案例，探讨了5GL在成像系统设计中的应用及其对跨学科团队协作的影响，强调了编程范式对研究流程的隐性塑造作用。


<details>
  <summary>Details</summary>
Motivation: 作者希望通过5GL提高设计过程的透明度和可追溯性，并探索其在并行研发流程中的潜力，而非传统的顺序流程。

Method: 文章基于作者在学术界和私营部门跨职能产品开发中的经验，通过96-Eyes项目案例分析，探讨了5GL在实际应用中的技术细节和挑战。

Result: 5GL能够将项目需求转化为机器可读的问题陈述，从而减少不同领域团队之间的误解和成本浪费。然而，5GL在以命令式语言为主导的环境中面临实际障碍。

Conclusion: 本文通过96-Eyes项目展示了如何使用第五代问题表述语言（5GL）来提高跨学科和跨职能团队的设计透明度和可追溯性，并强调了编程范式对研究工作流程的影响。

Abstract: This article presents a practitioner's reflection on applying declarative,
5th generation, problem formulation language (5GL) to de novo imaging system
design, informed by experiences across the interdisciplinary research in
academia and cross-functional product development within the private sector.
Using the 96-Eyes project: 96-camera parallel multi-modal imager for
high-throughput drug discovery as a representative case, I illustrate how
project requirements, ranging from hardware constraints to life sciences needs,
can be formalized into machine-readable problem statements to preserve
mission-critical input from diverse domain stakeholders. This declarative
approach enhances transparency, ensures design traceability, and minimizes
costly misalignment across optical, algorithmic, hardware-accelerated compute,
and life sciences teams.
  Alongside the technical discussion of 5GL with real-world code examples, I
reflect on the practical barriers to adopting 5GL in environments where
imperative, 3rd-generation languages (3GL) remain the default medium for
inter-team collaboration. Rather than offering an one-size-fits-all solution,
these learned lessons highlight how programming paradigms implicitly shapes
research workflows through existing domain hierarchies. The discussion aims to
invite further explorations into how declarative problem formulations can
facilitate innovation in settings where concurrent R\&{}D workflows are gaining
traction, as opposed to environments where sequential, phase-driven workflows
remain the norm.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [60] [Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology](https://arxiv.org/abs/2508.18288)
*Jay L. Cunningham,Adinawa Adjagbodjou,Jeffrey Basoah,Jainaba Jawara,Kowe Kadoma,Aaleyah Lewis*

Main category: eess.AS

TL;DR: 本文通过一项范围文献综述，探讨了公平性、偏见和公正在自动语音识别（ASR）和相关语音和语言技术（SLT）中的概念化和操作化，特别是针对非裔美国英语（AAE）说话者和其他语言多样化的社区。研究确定了四个主要的研究领域，并提出了一个以治理为中心的ASR生命周期框架，以促进更负责任的ASR开发。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨公平性、偏见和公正在自动语音识别（ASR）和相关语音和语言技术（SLT）中的概念化和操作化，特别是针对非裔美国英语（AAE）说话者和其他语言多样化的社区。

Method: 本文通过44篇同行评审的出版物，涵盖人机交互（HCI）、机器学习/自然语言处理（ML/NLP）和社会语言学，进行了一项范围文献综述，探讨了公平性、偏见和公正在自动语音识别（ASR）和相关语音和语言技术（SLT）中的概念化和操作化。

Result: 本文确定了四个主要的研究领域：(1) 研究人员如何理解与ASR相关的危害；(2) 跨越数据收集、整理、注释和模型训练的包容性数据实践；(3) 语言包容的方法论和理论方法；以及(4) 更公平系统的新兴实践和设计建议。虽然技术公平干预措施正在增长，但本文强调了在以治理为中心的方法上的关键差距，这些方法强调社区自主权、语言正义和参与问责制。

Conclusion: 本文提出了一种以治理为中心的自动语音识别（ASR）生命周期，作为负责任的ASR开发的新兴跨学科框架，并为寻求解决语音人工智能系统中语言边缘化的研究人员、从业者和政策制定者提供了启示。

Abstract: This scoping literature review examines how fairness, bias, and equity are
conceptualized and operationalized in Automatic Speech Recognition (ASR) and
adjacent speech and language technologies (SLT) for African American English
(AAE) speakers and other linguistically diverse communities. Drawing from 44
peer-reviewed publications across Human-Computer Interaction (HCI), Machine
Learning/Natural Language Processing (ML/NLP), and Sociolinguistics, we
identify four major areas of inquiry: (1) how researchers understand
ASR-related harms; (2) inclusive data practices spanning collection, curation,
annotation, and model training; (3) methodological and theoretical approaches
to linguistic inclusion; and (4) emerging practices and design recommendations
for more equitable systems. While technical fairness interventions are growing,
our review highlights a critical gap in governance-centered approaches that
foreground community agency, linguistic justice, and participatory
accountability. We propose a governance-centered ASR lifecycle as an emergent
interdisciplinary framework for responsible ASR development and offer
implications for researchers, practitioners, and policymakers seeking to
address language marginalization in speech AI systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: 本文提出了一种名为RLMR的强化学习方法，通过动态混合奖励系统来平衡主观写作质量和客观约束遵循，实验证明该方法在创意写作任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法难以平衡主观写作质量和客观约束遵循，因此需要一种能够适应不同写作场景的方法。

Method: 提出了一种名为RLMR的强化学习方法，利用来自写作奖励模型的动态混合奖励系统，该模型评估主观写作质量和约束验证模型评估客观约束遵循情况。

Result: 实验结果表明，RLMR在指令遵循（IFEval从83.36%到86.65%）和写作质量（在WriteEval上的手动专家成对评估中获得72.75%的胜率）方面都取得了持续的改进。

Conclusion: RLMR是第一个在在线强化学习训练中结合主观偏好和客观验证的工作，为多维创意写作优化提供了有效的解决方案。

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [62] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 本文提出了一种基于人类智能的评估范式，旨在解决大型语言模型在实际应用中的评估问题。它提供了一个具有实际价值的评估框架，并为开发技术熟练、上下文相关且符合伦理的LLM提供了可行的指导。


<details>
  <summary>Details</summary>
Motivation: 当前的评估框架仍然分散，优先考虑技术指标而忽视了部署的全面评估。本文旨在通过引入一种基于人类智能的评估范式来解决这一问题。

Method: 本文引入了一个基于人类智能的评估范式，提出了一种三维分类法：智商（IQ）-一般智能用于基础能力，情商（EQ）-对齐能力用于基于价值观的交互，专业商（PQ）-专业技能用于专业熟练度。此外，还提出了一种以价值为导向的评估（VQ）框架，评估经济可行性、社会影响、伦理对齐和环境可持续性。

Result: 通过分析200多个基准，本文识别了关键挑战，包括动态评估需求和可解释性差距。同时，提供了一个模块化架构和实现路线图，并维护了一个开放源代码评估资源库。

Conclusion: 本文提出了一个基于人类智能的评估范式，旨在解决大型语言模型在实际应用中的评估问题。它提供了一个具有实际价值的评估框架，并为开发技术熟练、上下文相关且符合伦理的LLM提供了可行的指导。

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [63] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 本文提出了一种新的偏见缓解代理，通过多代理系统优化信息源的选择，显著减少了检索内容中的偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在人工智能领域取得了重大进展，但它们继承了内部和外部信息源中的偏见，这严重影响了检索信息的公平性和平衡性，进而降低了用户的信任度。因此，需要一种有效的方法来缓解这一问题。

Method: 本文引入了一种新的偏见缓解代理，这是一个多代理系统，旨在通过专门的代理协调偏见缓解的工作流程，优化信息源的选择。

Result: 实验结果表明，与基线的简单检索策略相比，偏见减少了81.82%。

Conclusion: 本文提出了一种新的偏见缓解代理，旨在通过专门的代理优化信息源的选择，以确保检索内容既高度相关又最小化偏见，从而促进公平和平衡的知识传播。

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [64] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: CAC-CoT is a compact chain-of-thought method that improves efficiency without sacrificing accuracy on both System-1 and System-2 tasks.


<details>
  <summary>Details</summary>
Motivation: Long chain-of-thought prompting can slow down or degrade performance on fast, intuitive System-1 tasks, so there is a need for a more efficient approach.

Method: Introduce Connector-Aware Compact CoT (CAC-CoT), which restricts reasoning to a small set of connector phrases to produce concise and well-structured explanations.

Result: CAC-CoT achieves approximately 85% on GSM8K, 40% on GPQA, and 90% on S1-Bench, with reasoning traces averaging about 300 tokens, one-third the length of baseline traces.

Conclusion: CAC-CoT achieves high performance on System-2 tasks while retaining high performance on System-1 tasks, with shorter reasoning traces.

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [65] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 本文研究了大型推理模型在面对无法回答的问题时无法提供适当拒绝回答的问题，并提出了一种轻量级的两阶段方法来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务上表现出色，但面对一些本质上无法回答的问题时，它们持续失败，无法提供适当的拒绝回答。本文旨在解决这一问题，以实现可信的人工智能。

Method: 本文提出了一种轻量级的两阶段方法，结合认知监控和推理时干预。

Result: 实验结果表明，我们的方法显著提高了拒绝回答的比率，同时保持了整体推理性能。

Conclusion: 本文提出了一种轻量级的两阶段方法，结合认知监控和推理时干预，显著提高了拒绝回答的比率，同时保持了整体推理性能。

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [66] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 本文介绍了Experience-driven Lifelong Learning (ELL)框架和StuLife基准数据集，用于评估和推动AI代理的终身学习能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI向通用智能发展，系统需要从优化静态任务转向创建能够持续学习的开放代理。因此，本文旨在提出一种新的框架和基准数据集，以评估和推动终身学习能力的发展。

Method: 本文提出了Experience-driven Lifelong Learning (ELL)框架，该框架基于四个核心原则：经验探索、长期记忆、技能学习和知识内化。同时，介绍了StuLife基准数据集，用于评估终身学习能力。

Result: 本文提出了ELL框架和StuLife基准数据集，为评估终身学习能力提供了全面的平台，并探索了上下文工程在推动AGI中的作用。

Conclusion: 本文提出了Experience-driven Lifelong Learning (ELL)框架，用于构建能够通过与现实世界互动实现持续增长的自我进化代理。此外，还介绍了StuLife基准数据集，用于评估终身学习能力，并探索了上下文工程在推动AGI中的作用。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [67] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: 本文提出了一种基于Ramon Llull的Ars combinatoria的现代思维机器，用于生成研究想法，能够增强科学创造力并促进人类与AI之间的协作。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新审视Ramon Llull的Ars combinatoria作为构建现代Llull思维机器的基础，以促进科学研究的创意生成。

Method: 本文定义了三个组合轴：主题、领域和方法，并从人类专家或会议论文中挖掘元素，通过提示LLM使用精心策划的组合来生成多样、相关且基于当前文献的研究想法。

Result: 本文展示了通过提示LLM使用精心策划的组合可以生成多样、相关且基于当前文献的研究想法。

Conclusion: 本文提出了一种现代的Llull思维机器，用于研究构思，它能够增强科学创造力，并为人类与AI之间的协作构想提供一条路径。

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [68] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 本文提出了一种名为StepWiser的生成性法官模型，通过强化学习训练，以提高多步骤推理过程中中间步骤的判断准确性，并在训练和推理时提升策略模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着模型越来越多地利用多步骤推理策略来解决复杂问题，监督这些中间步骤的逻辑有效性已成为一个关键的研究挑战。现有的过程奖励模型通常作为分类器工作，没有提供解释，并且依赖于静态数据集的监督微调，限制了泛化能力。

Method: 我们通过使用回放的相对结果进行强化学习来训练StepWiser模型，该模型是一个生成性法官，能够对策略模型的推理步骤进行推理，输出思考标记后给出最终裁决。

Result: 我们展示了StepWiser模型在判断中间步骤方面比现有方法更准确，可以在训练时改进策略模型，并在推理时搜索中提高性能。

Conclusion: 我们的模型StepWiser在判断中间步骤方面比现有方法更准确，可以在训练时改进策略模型，并在推理时搜索中提高性能。

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 本文提出了一种统一的、局部鲁棒性框架（SALMAN），用于评估模型稳定性而不修改内部参数或依赖复杂扰动启发式方法。该框架通过展示攻击效率和鲁棒训练的显著提升，被定位为一种实用的、与模型无关的工具，有助于提高基于变压器的NLP系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着这些模型规模和部署的增加，它们在输入扰动下的鲁棒性成为一个越来越紧迫的问题。现有的鲁棒性方法通常在小参数和大规模模型（LLM）之间存在分歧，并且通常依赖于劳动密集型、样本特定的对抗设计。

Method: 我们提出了一种统一的、局部（样本级）鲁棒性框架（SALMAN），它在不修改内部参数或依赖复杂扰动启发式方法的情况下评估模型稳定性。

Result: 通过展示攻击效率和鲁棒训练的显著提升，我们的框架被定位为一种实用的、与模型无关的工具。

Conclusion: 我们的框架作为一种实用的、与模型无关的工具，有助于提高基于变压器的NLP系统的可靠性。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [70] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: 研究分析了Mixture-of-Experts (MoE) 模型的稀疏性对记忆和推理能力的影响，发现过度稀疏的模型在推理能力上存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前的密集模型前沿忽略了Mixture-of-Experts (MoE) 模型引入的新稀疏性维度，因此需要研究MoE稀疏性如何影响记忆和推理能力。

Method: 通过训练一系列MoE Transformer模型，系统地改变总参数、活动参数和top-k路由，同时保持计算预算固定，以研究MoE稀疏性对两种不同能力领域的影响。

Result: 记忆基准随着总参数的增加而单调提升，而推理性能在总参数和训练损失继续增加的情况下趋于饱和甚至退化。

Conclusion: 研究发现，过度稀疏的模型在推理能力上存在缺陷，即使增加参数或训练损失也无法改善。

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [71] [Text to Query Plans for Question Answering on Large Tables](https://arxiv.org/abs/2508.18758)
*Yipeng Zhang,Chen Wang,Yuzhe Zhang,Jacky Jiang*

Main category: cs.DB

TL;DR: 本文提出了一种新框架，通过将自然语言查询转换为查询计划，解决了传统SQL在处理大规模数据和复杂分析时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到SQL方法在处理大规模数据集和复杂数据分析时存在效率低下和功能有限的问题。

Method: 我们提出了一种将自然语言查询转换为查询计划的新框架，利用大语言模型迭代解释查询并构建操作序列。

Result: 我们在标准数据库和大型科学表格上进行了实验，验证了框架的有效性。

Conclusion: 我们的框架在处理大规模数据集和执行复杂数据分析方面表现出色。

Abstract: Efficient querying and analysis of large tabular datasets remain significant
challenges, especially for users without expertise in programming languages
like SQL. Text-to-SQL approaches have shown promising performance on benchmark
data; however, they inherit SQL's drawbacks, including inefficiency with large
datasets and limited support for complex data analyses beyond basic querying.
We propose a novel framework that transforms natural language queries into
query plans. Our solution is implemented outside traditional databases,
allowing us to support classical SQL commands while avoiding SQL's inherent
limitations. Additionally, we enable complex analytical functions, such as
principal component analysis and anomaly detection, providing greater
flexibility and extensibility than traditional SQL capabilities. We leverage
LLMs to iteratively interpret queries and construct operation sequences,
addressing computational complexity by incrementally building solutions. By
executing operations directly on the data, we overcome context length
limitations without requiring the entire dataset to be processed by the model.
We validate our framework through experiments on both standard databases and
large scientific tables, demonstrating its effectiveness in handling extensive
datasets and performing sophisticated data analyses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [72] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

TL;DR: A study identifies a systematic deficiency in the multimodal grounding of Vision Language Models (VLMs), showing that their ability to recall factual knowledge is significantly diminished when the reference is visual instead of textual. Probes on internal states can accurately flag unreliable responses and improve performance on visual question answering tasks.


<details>
  <summary>Details</summary>
Motivation: To address the issue of VLMs struggling to link their internal knowledge of an entity with its image representation.

Method: Through a controlled study, we identify a systematic deficiency in the multimodal grounding of Vision Language Models (VLMs).

Result: Forcing VLMs to rely on image representations of an entity halves their ability to recall factual knowledge, suggesting that VLMs struggle to link their internal knowledge of an entity with its image representation. Probes on these internal states achieve over 92% accuracy at flagging cases where the VLM response is unreliable. These probes can be applied, without retraining, to identify when a VLM will fail to correctly answer a question that requires an understanding of multimodal input. When used to facilitate selective prediction on a visual question answering task, the probes increase coverage by 7.87% (absolute) while also reducing the risk of error by 0.9% (absolute).

Conclusion:  addressing the systematic, detectable deficiency is an important avenue in language grounding, and we provide informed recommendations for future directions.

Abstract: Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [73] [Beyond the Textual: Generating Coherent Visual Options for MCQs](https://arxiv.org/abs/2508.18772)
*Wanqiang Wang,Longzhu He,Wei Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种跨模态选项合成框架CmOS，用于生成带有视觉选项的教育多项选择题，该框架在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 以往的研究主要集中在生成带有文本选项的多项选择题，但忽略了视觉选项。此外，生成高质量的干扰项由于手动编写成本高和可扩展性有限而是一个主要挑战。

Method: 提出了一种跨模态选项合成（CmOS）框架，结合了多模态思维链（MCoT）推理过程和检索增强生成（RAG），以生成语义合理且视觉相似的答案和干扰项，并包含一个区分模块来识别适合视觉选项的内容。

Result: 实验结果表明，CmOS在内容辨别、问题生成和视觉选项生成方面优于现有方法，适用于各种学科和教育水平。

Conclusion: CmOS在内容辨别、问题生成和视觉选项生成方面优于现有方法，适用于各种学科和教育水平。

Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep
thinking and knowledge integration in education. However, previous research has
primarily focused on generating MCQs with textual options, but it largely
overlooks the visual options. Moreover, generating high-quality distractors
remains a major challenge due to the high cost and limited scalability of
manual authoring. To tackle these problems, we propose a Cross-modal Options
Synthesis (CmOS), a novel framework for generating educational MCQs with visual
options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning
process and Retrieval-Augmented Generation (RAG) to produce semantically
plausible and visually similar answer and distractors. It also includes a
discrimination module to identify content suitable for visual options.
Experimental results on test tasks demonstrate the superiority of CmOS in
content discrimination, question generation and visual option generation over
existing methods across various subjects and educational levels.

</details>

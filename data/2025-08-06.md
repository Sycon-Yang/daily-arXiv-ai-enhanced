<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 50]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation](https://arxiv.org/abs/2508.02808)
*Radhika Dua,Young Joon,Kwon,Siddhant Dogra,Daniel Freedman,Diana Ruan,Motaz Nashawaty,Danielle Rigau,Daniel Alexander Alber,Kang Zhang,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: ICARE 是一种基于代理的可解释评估框架，用于评估生成的放射学报告，通过临床问题和答案的相互测试，提高评估的透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估指标通常依赖于表面相似性或作为黑箱，缺乏可解释性。因此，需要一种可解释且基于临床的评估框架来确保生成报告的安全部署。

Method: ICARE 采用基于代理的报告评估方法，利用大型语言模型代理和动态多项选择题回答（MCQA）。两个代理分别使用真实报告和生成报告，生成具有临床意义的问题并互相测试。

Result: 临床研究显示，ICARE 与专家判断的一致性显著高于之前的指标。扰动分析证实了对临床内容的敏感性和可重复性，而模型比较揭示了可解释的错误模式。

Conclusion: ICARE 是一种可解释的评估框架，能够更准确地反映生成报告的临床精度和召回率，并通过与专家判断的一致性验证其有效性。

Abstract: Radiological imaging is central to diagnosis, treatment planning, and
clinical decision-making. Vision-language foundation models have spurred
interest in automated radiology report generation (RRG), but safe deployment
requires reliable clinical evaluation of generated reports. Existing metrics
often rely on surface-level similarity or behave as black boxes, lacking
interpretability. We introduce ICARE (Interpretable and Clinically-grounded
Agent-based Report Evaluation), an interpretable evaluation framework
leveraging large language model agents and dynamic multiple-choice question
answering (MCQA). Two agents, each with either the ground-truth or generated
report, generate clinically meaningful questions and quiz each other. Agreement
on answers captures preservation and consistency of findings, serving as
interpretable proxies for clinical precision and recall. By linking scores to
question-answer pairs, ICARE enables transparent, and interpretable assessment.
Clinician studies show ICARE aligns significantly more with expert judgment
than prior metrics. Perturbation analyses confirm sensitivity to clinical
content and reproducibility, while model comparisons reveal interpretable error
patterns.

</details>


### [2] [Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives](https://arxiv.org/abs/2508.02853)
*Yinuo Xu,Veronica Derricks,Allison Earl,David Jurgens*

Main category: cs.CL

TL;DR: 本文提出了一种新的模型DEM-MoE，用于建模主观NLP任务中的标注者分歧，并测试了使用合成数据来补充真实数据的方法。


<details>
  <summary>Details</summary>
Motivation: 为了更好地建模主观NLP任务中的标注者分歧，以及解决人口统计信息覆盖不足的问题。

Method: DEM-MoE模型根据标注者的人口统计信息将输入路由到专家子网络，并测试了使用LLM生成的合成标注进行数据插补的方法。

Result: DEM-MoE在不同人口统计群体中表现良好，并且在标注者分歧高的数据集上表现出色。合成标注与人类标注有一定一致性，可以作为训练数据的补充。

Conclusion: DEM-MoE模型和使用合成数据的方法有助于更好地表示多样化的观点。

Abstract: We present an approach to modeling annotator disagreement in subjective NLP
tasks through both architectural and data-centric innovations. Our model,
DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert
subnetworks based on annotator demographics, enabling it to better represent
structured, group-level variation compared to prior models. DEM-MoE
consistently performs competitively across demographic groups, and shows
especially strong results on datasets with high annotator disagreement. To
address sparse demographic coverage, we test whether LLM-generated synthetic
annotations via zero-shot persona prompting can be used for data imputation. We
show these synthetic judgments align moderately well with human annotations on
our data and offer a scalable way to potentially enrich training data. We then
propose and evaluate approaches for blending real and synthetic data using
strategies tailored to dataset structure. We find that the optimal strategies
depend on dataset structure. Together, these contributions improve the
representation of diverse perspectives.

</details>


### [3] [Highlight & Summarize: RAG without the jailbreaks](https://arxiv.org/abs/2508.02872)
*Giovanni Cherubin,Andrew Paverd*

Main category: cs.CL

TL;DR: 本文提出了一种新的 RAG 系统设计模式 H&S，通过不向生成式 LLM 透露用户问题来防止越狱和模型劫持攻击，并在响应质量上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的缓解方法容易被绕过，因为输入和输出的空间非常大。因此需要一种更安全的设计模式来防止攻击。

Method: H&S 通过将 RAG 管道分为两个组件——highlighter 和 summarizer，以防止将用户的问题直接传递给生成式 LLM。

Result: H&S 在正确性、相关性和响应质量方面进行了评估，并且使用基于 LLM 的 highlighter 时，大多数 H&S 响应被认为优于标准 RAG 管道。

Conclusion: H&S 是一种有效的设计模式，可以防止对大型语言模型的越狱和模型劫持攻击，同时在生成响应的质量上表现出色。

Abstract: Preventing jailbreaking and model hijacking of Large Language Models (LLMs)
is an important yet challenging task. For example, when interacting with a
chatbot, malicious users can input specially crafted prompts to cause the LLM
to generate undesirable content or perform a completely different task from its
intended purpose. Existing mitigations for such attacks typically rely on
hardening the LLM's system prompt or using a content classifier trained to
detect undesirable content or off-topic conversations. However, these
probabilistic approaches are relatively easy to bypass due to the very large
space of possible inputs and undesirable outputs. In this paper, we present and
evaluate Highlight & Summarize (H&S), a new design pattern for
retrieval-augmented generation (RAG) systems that prevents these attacks by
design. The core idea is to perform the same task as a standard RAG pipeline
(i.e., to provide natural language answers to questions, based on relevant
sources) without ever revealing the user's question to the generative LLM. This
is achieved by splitting the pipeline into two components: a highlighter, which
takes the user's question and extracts relevant passages ("highlights") from
the retrieved documents, and a summarizer, which takes the highlighted passages
and summarizes them into a cohesive answer. We describe several possible
instantiations of H&S and evaluate their generated responses in terms of
correctness, relevance, and response quality. Surprisingly, when using an
LLM-based highlighter, the majority of H&S responses are judged to be better
than those of a standard RAG pipeline.

</details>


### [4] [Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages](https://arxiv.org/abs/2508.02885)
*Elliot Murphy,Rohan Venkatesh,Edward Khokhlovich,Andrey Vyshedskiy*

Main category: cs.CL

TL;DR: 本文研究了Merge操作在句法中的作用，并发现不同类型的Merge对象可能由不同的认知机制支持。


<details>
  <summary>Details</summary>
Motivation: 研究Merge操作在语言科学中的作用以及其在神经认知层面的不同机制。

Method: 对参与者理解逐渐增加句法复杂度的句子进行了系统研究，并通过聚类分析发现了行为证据。

Result: 发现了三种不同的结构类型，可能在不同的发育阶段出现，并受到选择性损害。

Conclusion: 虽然基于Merge的句法可能在进化时间上突然出现，但不同类型的Merge对象的处理似乎由不同的认知机制支持。

Abstract: In the modern language sciences, the core computational operation of syntax,
'Merge', is defined as an operation that combines two linguistic units (e.g.,
'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).
This can then be further combined with additional linguistic units based on
this categorial information, respecting non-associativity such that abstract
grouping is respected. Some linguists have embraced the view that Merge is an
elementary, indivisible operation that emerged in a single evolutionary step.
From a neurocognitive standpoint, different mental objects constructed by Merge
may be supported by distinct mechanisms: (1) simple command constructions
(e.g., "eat apples"); (2) the merging of adjectives and nouns ("red boat"); and
(3) the merging of nouns with spatial prepositions ("laptop behind the sofa").
Here, we systematically investigate participants' comprehension of sentences
with increasing levels of syntactic complexity. Clustering analyses revealed
behavioral evidence for three distinct structural types, which we discuss as
potentially emerging at different developmental stages and subject to selective
impairment. While a Merge-based syntax may still have emerged suddenly in
evolutionary time, responsible for the structured symbolic turn our species
took, different cognitive mechanisms seem to underwrite the processing of
various types of Merge-based objects.

</details>


### [5] [Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models](https://arxiv.org/abs/2508.02886)
*Wenjie Luo,Ruocheng Li,Shanshan Zhu,Julian Perry*

Main category: cs.CL

TL;DR: The paper introduces the Coherent Multimodal Reasoning Framework (CMRF), which improves LVLMs' common sense reasoning through an iterative, self-evaluating approach. CMRF achieves state-of-the-art results on benchmarks, demonstrating the effectiveness of its modules and iterative refinement strategy.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) and vision-language models (LVLMs) struggle with complex, multi-step, cross-modal common sense reasoning tasks, often exhibiting a lack of 'deliberative thinking.' They tend to rely on superficial associations rather than deep, chained inference, especially when integrating visual information with abstract concepts.

Method: The paper proposes the Coherent Multimodal Reasoning Framework (CMRF), which enhances LVLMs' common sense reasoning capabilities through an iterative, self-evaluating inference mechanism. CMRF integrates three key modules: a Reasoning Decomposition Unit (RDU), a Contextual Inference Engine (CIE), and a Coherence Assessment Module (CAM). It also employs an Adaptive Iterative Refinement strategy.

Result: CMRF achieves an average accuracy of 69.4% on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC, surpassing the best open-source baseline by +2.4 percentage points. Extensive ablation studies and human evaluations confirm the critical contributions of each module and the effectiveness of iterative refinement in fostering more coherent and accurate reasoning.

Conclusion: CMRF achieves state-of-the-art performance among open-source LVLMs on challenging benchmarks, with an average accuracy of 69.4%, surpassing the best open-source baseline by +2.4 percentage points. The framework's modules and iterative refinement strategy are effective in fostering more coherent and accurate reasoning.

Abstract: Despite significant advancements, current large language models (LLMs) and
vision-language models (LVLMs) continue to struggle with complex, multi-step,
cross-modal common sense reasoning tasks, often exhibiting a lack of
"deliberative thinking." They tend to rely on superficial associations rather
than deep, chained inference, particularly when integrating visual information
with abstract concepts. To address this, we propose the Coherent Multimodal
Reasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense
reasoning capabilities through an iterative, self-evaluating inference
mechanism. CMRF mimics human problem-solving by decomposing complex queries,
generating step-by-step inferences, and self-correcting errors. Our framework
integrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking
down problems into sub-questions, a Contextual Inference Engine (CIE) for
contextual inference, and a Coherence Assessment Module (CAM) for evaluating
logical consistency and confidence. Coupled with an Adaptive Iterative
Refinement strategy, CMRF systematically refines its reasoning paths. Built
upon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning
(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source
LVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It
attains an average accuracy of 69.4%, surpassing the best open-source baseline
by +2.4 percentage points, with particular strength in complex reasoning
scenarios. Extensive ablation studies and human evaluations confirm the
critical contributions of each module and the effectiveness of iterative
refinement in fostering more coherent and accurate reasoning.

</details>


### [6] [SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations](https://arxiv.org/abs/2508.02901)
*Osama Khalid,Sanvesh Srivastava,Padmini Srinivasan*

Main category: cs.CL

TL;DR: The paper explores the relationship between sensorial language and traditional stylistic features using a novel approach, demonstrating that reduced-rank LIWC features can effectively predict sensorial language while significantly reducing model parameters.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how sensorial language relates to traditional stylistic features and to develop more efficient models for predicting sensorial language.

Method: The paper uses a novel Reduced-Rank Ridge Regression (R4) approach to explore the relationship between sensorial language and traditional stylistic features measured by LIWC.

Result: Low-dimensional latent representations of LIWC features (r = 24) effectively capture stylistic information for sensorial language prediction compared to the full feature set (r = 74). SLIM-LLMs with low-rank LIWC features match the performance of full-scale language models while reducing parameters by up to 80%.

Conclusion: SLIM-LLMs with low-rank LIWC features can match the performance of full-scale language models while reducing parameters by up to 80%.

Abstract: Sensorial language -- the language connected to our senses including vision,
sound, touch, taste, smell, and interoception, plays a fundamental role in how
we communicate experiences and perceptions. We explore the relationship between
sensorial language and traditional stylistic features, like those measured by
LIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate
that low-dimensional latent representations of LIWC features r = 24 effectively
capture stylistic information for sensorial language prediction compared to the
full feature set (r = 74). We introduce Stylometrically Lean Interpretable
Models (SLIM-LLMs), which model non-linear relationships between these style
dimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features
match the performance of full-scale language models while reducing parameters
by up to 80%.

</details>


### [7] [Can LLMs Generate High-Quality Task-Specific Conversations?](https://arxiv.org/abs/2508.02931)
*Shengqi Li,Amarnath Gupta*

Main category: cs.CL

TL;DR: 本文介绍了一种参数化框架，用于控制大型语言模型中的对话质量，并展示了其在多个领域中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决对话生成中的挑战，包括主题连贯性、知识进展、角色一致性以及控制粒度。

Method: 本文提出了一种参数化框架，用于控制大型语言模型中的对话质量。探索了六个维度上的九个关键参数，以精确指定对话属性。

Result: 通过与最先进的LLM进行实验，证明基于参数的控制在生成的对话属性上产生了统计学上显著的差异。

Conclusion: 该框架提供了一种标准化的对话质量控制方法，具有在教育、治疗、客户服务和娱乐等领域的应用前景。未来的工作将集中在通过架构修改实现更多参数以及开发评估基准数据集上。

Abstract: This paper introduces a parameterization framework for controlling
conversation quality in large language models. We explore nine key parameters
across six dimensions that enable precise specification of dialogue properties.
Through experiments with state-of-the-art LLMs, we demonstrate that
parameter-based control produces statistically significant differences in
generated conversation properties. Our approach addresses challenges in
conversation generation, including topic coherence, knowledge progression,
character consistency, and control granularity. The framework provides a
standardized method for conversation quality control with applications in
education, therapy, customer service, and entertainment. Future work will focus
on implementing additional parameters through architectural modifications and
developing benchmark datasets for evaluation.

</details>


### [8] [CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors](https://arxiv.org/abs/2508.02997)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 本文提出了一种基于上下文共现矩阵和张量潜在空间特性的新方法，用于有效检测对抗性和越狱提示，在标记数据稀缺的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的复杂性和难以理解的性质，它们容易受到攻击，特别是越狱攻击。因此，开发强大的检测方法对于安全可靠地使用大型语言模型至关重要。

Method: 本文利用上下文共现矩阵和张量的潜在空间特性，提出了一种新的方法来有效识别对抗性和越狱提示。

Result: 该方法在仅使用0.5%标记提示的情况下实现了0.83的F1分数，比基线提高了96.6%。此外，该方法的速度提升了2.3到128.4倍。

Conclusion: 本文提出的方法在标记数据稀缺的情况下表现出色，具有较高的检测效果和速度优势。

Abstract: The widespread use of Large Language Models (LLMs) in many applications marks
a significant advance in research and practice. However, their complexity and
hard-to-understand nature make them vulnerable to attacks, especially
jailbreaks designed to produce harmful responses. To counter these threats,
developing strong detection methods is essential for the safe and reliable use
of LLMs. This paper studies this detection problem using the Contextual
Co-occurrence Matrix, a structure recognized for its efficacy in data-scarce
environments. We propose a novel method leveraging the latent space
characteristics of Contextual Co-occurrence Matrices and Tensors for the
effective identification of adversarial and jailbreak prompts. Our evaluations
show that this approach achieves a notable F1 score of 0.83 using only 0.5% of
labeled prompts, which is a 96.6% improvement over baselines. This result
highlights the strength of our learned patterns, especially when labeled data
is scarce. Our method is also significantly faster, speedup ranging from 2.3 to
128.4 times compared to the baseline models. To support future research and
reproducibility, we have made our implementation publicly available.

</details>


### [9] [When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025](https://arxiv.org/abs/2508.03037)
*Ariya Mukherjee-Gandhi,Oliver Muellerklein*

Main category: cs.CL

TL;DR: 本研究分析了2013至2025年间的英语语境下关于AI生成艺术的论述，发现艺术家的观点与媒体叙述之间存在不一致，并强调了技术术语可能构成一种准入机制。


<details>
  <summary>Details</summary>
Motivation: 由于艺术家的声音在主流公共和学术讨论中常常被边缘化，本研究旨在探讨艺术家对AI生成艺术的看法与媒体叙述之间的差异。

Method: 本研究采用可重复的方法，分析了2013年至2025年间英语语境下关于AI生成艺术的论述，并从439个精心挑选的500字摘录中识别出五个稳定的主题集群。

Result: 研究发现，技术术语的使用可能作为一种微妙的准入机制，常常使艺术家认为最紧迫的问题被忽视。

Conclusion: 本研究提供了基于BERTopic的方法论和多模态基线，同时呼吁在不断发展的AI与艺术领域中更深入、以透明度为导向地参与艺术家的观点。

Abstract: As generative AI continues to reshape artistic production and alternate modes
of human expression, artists whose livelihoods are most directly affected have
raised urgent concerns about consent, transparency, and the future of creative
labor. However, the voices of artists are often marginalized in dominant public
and scholarly discourse. This study presents a twelve-year analysis, from 2013
to 2025, of English-language discourse surrounding AI-generated art. It draws
from 439 curated 500-word excerpts sampled from opinion articles, news reports,
blogs, legal filings, and spoken-word transcripts. Through a reproducible
methodology, we identify five stable thematic clusters and uncover a
misalignment between artists' perceptions and prevailing media narratives. Our
findings highlight how the use of technical jargon can function as a subtle
form of gatekeeping, often sidelining the very issues artists deem most urgent.
Our work provides a BERTopic-based methodology and a multimodal baseline for
future research, alongside a clear call for deeper, transparency-driven
engagement with artist perspectives in the evolving AI-creative landscape.

</details>


### [10] [Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03098)
*Haoran Wang,Xiongxiao Xu,Baixiang Huang,Kai Shu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Privacy-Aware Decoding (PAD)的轻量级推理时间防御方法，用于保护RAG系统中的敏感信息。PAD通过在生成过程中注入校准的高斯噪声来减少信息泄露，同时保持生成质量。实验表明，PAD在三个真实世界的数据集上表现优越，为敏感领域的隐私保护提供了一个通用和可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当检索涉及私人或敏感数据时，RAG系统容易受到提取攻击，这些攻击可能通过生成的响应泄露机密信息。现有的方法需要重新训练或语料库级别的过滤，而PAD是模型无关的，并且在解码时完全操作，计算开销最小。

Method: 我们提出了Privacy-Aware Decoding (PAD)，这是一种轻量级的推理时间防御，通过在生成过程中自适应地注入校准的高斯噪声来保护敏感信息。PAD集成了基于置信度的筛选、高效的敏感性估计和上下文感知的噪声校准，以平衡隐私与生成质量。

Result: 在三个真实世界的数据集上的实验表明，PAD显著减少了私有信息泄露，同时保持了响应的效用，优于现有的基于检索和后处理的防御方法。

Conclusion: 我们的工作通过解码策略向前迈进了一步，以减轻RAG中的隐私风险，为敏感领域提供了通用和可扩展的隐私解决方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large
language models (LLMs) by conditioning outputs on external knowledge sources.
However, when retrieval involves private or sensitive data, RAG systems are
susceptible to extraction attacks that can leak confidential information
through generated responses. We propose Privacy-Aware Decoding (PAD), a
lightweight, inference-time defense that adaptively injects calibrated Gaussian
noise into token logits during generation. PAD integrates confidence-based
screening to selectively protect high-risk tokens, efficient sensitivity
estimation to minimize unnecessary noise, and context-aware noise calibration
to balance privacy with generation quality. A \renyi Differential Privacy (RDP)
accountant rigorously tracks cumulative privacy loss, enabling explicit
per-response $(\varepsilon, \delta)$-DP guarantees for sensitive outputs.
Unlike prior approaches requiring retraining or corpus-level filtering, PAD is
model-agnostic and operates entirely at decoding time with minimal
computational overhead. Experiments on three real-world datasets demonstrate
that PAD substantially reduces private information leakage while preserving
response utility, outperforming existing retrieval- and post-processing-based
defenses. Our work takes an important step toward mitigating privacy risks in
RAG via decoding strategies, paving the way for universal and scalable privacy
solutions in sensitive domains. Our code is available:
https://github.com/wang2226/PAD.

</details>


### [11] [Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation](https://arxiv.org/abs/2508.03110)
*Zizhong Li,Haopeng Zhang,Jiawei Zhang*

Main category: cs.CL

TL;DR: TPARAG is a novel attack framework that targets both white-box and black-box RAG systems by generating and optimizing malicious passages at the token level, revealing critical vulnerabilities in RAG pipelines.


<details>
  <summary>Details</summary>
Motivation: The integration of RAG frameworks brings new security vulnerabilities, such as the risk of malicious content being retrieved and used to manipulate model outputs. Existing approaches either rely heavily on access to the retriever or fail to jointly consider both retrieval and generation stages, limiting their effectiveness in black-box scenarios.

Method: TPARAG is a novel framework that targets both white-box and black-box RAG systems by leveraging a lightweight white-box LLM as an attacker to generate and iteratively optimize malicious passages at the token level.

Result: Extensive experiments on open-domain QA datasets demonstrate that TPARAG consistently outperforms previous approaches in retrieval-stage and end-to-end attack effectiveness.

Conclusion: TPARAG reveals critical vulnerabilities in RAG pipelines and offers new insights into improving their robustness.

Abstract: While large language models (LLMs) have achieved remarkable success in
providing trustworthy responses for knowledge-intensive tasks, they still face
critical limitations such as hallucinations and outdated knowledge. To address
these issues, the retrieval-augmented generation (RAG) framework enhances LLMs
with access to external knowledge via a retriever, enabling more accurate and
real-time outputs about the latest events. However, this integration brings new
security vulnerabilities: the risk that malicious content in the external
database can be retrieved and used to manipulate model outputs. Although prior
work has explored attacks on RAG systems, existing approaches either rely
heavily on access to the retriever or fail to jointly consider both retrieval
and generation stages, limiting their effectiveness, particularly in black-box
scenarios. To overcome these limitations, we propose Token-level Precise Attack
on the RAG (TPARAG), a novel framework that targets both white-box and
black-box RAG systems. TPARAG leverages a lightweight white-box LLM as an
attacker to generate and iteratively optimize malicious passages at the token
level, ensuring both retrievability and high attack success in generation.
Extensive experiments on open-domain QA datasets demonstrate that TPARAG
consistently outperforms previous approaches in retrieval-stage and end-to-end
attack effectiveness. These results further reveal critical vulnerabilities in
RAG pipelines and offer new insights into improving their robustness.

</details>


### [12] [Cross-lingual Opinions and Emotions Mining in Comparable Documents](https://arxiv.org/abs/2508.03112)
*Motaz Saad,David Langlois,Kamel Smaili*

Main category: cs.CL

TL;DR: 本研究分析了英语-阿拉伯语比较文档中的情感和情绪差异。通过手动翻译英语WordNet-Affect词典到阿拉伯语，创建了双语情绪词典，并应用统计度量来评估情感和情绪的一致性。研究结果显示，当文章来自同一新闻机构时，情感和情绪标注一致，而当文章来自不同新闻机构时，它们会有所不同。


<details>
  <summary>Details</summary>
Motivation: 比较文本是主题对齐的多语言文档，不是直接的翻译。它们对于理解一个话题如何在不同语言中被讨论很有价值。这项研究研究了英语-阿拉伯语比较文档中的情感和情绪差异。

Method: 首先，对文本进行情感和情绪标签的注释。应用跨语言方法对文档进行意见类别（主观/客观）的标记，避免依赖机器翻译。为了用情绪（愤怒、厌恶、恐惧、喜悦、悲伤、惊讶）进行注释，我们手动将英语WordNet-Affect（WNA）词典翻译成阿拉伯语，创建了双语情绪词典，用于对可比较语料库进行标记。然后应用统计度量来评估每个源-目标文档对中的情感和情绪的一致性。

Result: 研究结果表明，当文章来自同一新闻机构时，情感和情绪标注是一致的，而当文章来自不同的新闻机构时，它们会有所不同。

Conclusion: 研究结果表明，当文章来自同一新闻机构时，情感和情绪标注是一致的，而当文章来自不同的新闻机构时，它们会有所不同。所提出的方法是语言无关的，并且可以推广到其他语言对。

Abstract: Comparable texts are topic-aligned documents in multiple languages that are
not direct translations. They are valuable for understanding how a topic is
discussed across languages. This research studies differences in sentiments and
emotions across English-Arabic comparable documents. First, texts are annotated
with sentiment and emotion labels. We apply a cross-lingual method to label
documents with opinion classes (subjective/objective), avoiding reliance on
machine translation. To annotate with emotions (anger, disgust, fear, joy,
sadness, surprise), we manually translate the English WordNet-Affect (WNA)
lexicon into Arabic, creating bilingual emotion lexicons used to label the
comparable corpora. We then apply a statistical measure to assess the agreement
of sentiments and emotions in each source-target document pair. This comparison
is especially relevant when the documents originate from different sources. To
our knowledge, this aspect has not been explored in prior literature. Our study
includes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera
(JSC). Results show that sentiment and emotion annotations align when articles
come from the same news agency and diverge when they come from different ones.
The proposed method is language-independent and generalizable to other language
pairs.

</details>


### [13] [Long Story Generation via Knowledge Graph and Literary Theory](https://arxiv.org/abs/2508.03137)
*Ge Shi,Kaiyu Huang,Guochen Feng*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体故事生成结构，通过引入记忆存储模型和故事主题障碍框架，以及多智能体交互阶段，以生成更高质量的长篇故事。


<details>
  <summary>Details</summary>
Motivation: 之前的基于大纲的生成方法存在主题漂移和情节乏味的问题，因此需要一种改进的方法来生成更吸引人的长篇故事。

Method: 我们提出了多智能体故事生成结构，使用大型语言模型作为代理的核心组件。我们引入了一个包含两个组件的记忆存储模型，以及一个基于文学叙事理论的故事主题障碍框架，并建立了多智能体交互阶段来模拟作者-读者互动。

Result: 评估显示我们的方法能够生成更高质量的长篇故事。

Conclusion: 我们的方法能够生成更高质量的长篇故事。

Abstract: The generation of a long story consisting of several thousand words is a
sub-task in the field of long text generation~(LTG). Previous research has
addressed this challenge through outline-based generation, which employs a
multi-stage method for generating outlines into stories. However, this approach
suffers from two common issues: almost inevitable theme drift caused by the
loss of memory of previous outlines, and tedious plots with incoherent logic
that are less appealing to human readers.
  In this paper, we propose the multi-agent Story Generator structure to
improve the multi-stage method, using large language models~(LLMs) as the core
components of agents. To avoid theme drift, we introduce a memory storage model
comprising two components: a long-term memory storage that identifies the most
important memories, thereby preventing theme drift; and a short-term memory
storage that retains the latest outlines from each generation round. To
incorporate engaging elements into the story, we design a story theme obstacle
framework based on literary narratology theory that introduces uncertain
factors and evaluation criteria to generate outline. This framework calculates
the similarity of the former storyline and enhances the appeal of the story by
building a knowledge graph and integrating new node content. Additionally, we
establish a multi-agent interaction stage to simulate writer-reader interaction
through dialogue and revise the story text according to feedback, to ensure it
remains consistent and logical. Evaluations against previous methods
demonstrate that our approach can generate higher-quality long stories.

</details>


### [14] [RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior](https://arxiv.org/abs/2508.03140)
*Junyao Yang,Jianwei Wang,Huiping Zhuang,Cen Chen,Ziqian Zeng*

Main category: cs.CL

TL;DR: 本文提出了一种名为RCP-Merging的新颖融合框架，旨在将领域特定的大型语言模型与具有长链式思维能力的模型进行融合，同时保持模型在原始领域的性能。通过将推理模型权重作为基础先验，并利用推理能力指标来保留核心长链式思维能力模型权重，选择性地融合必要的领域特定权重，实验结果表明该方法在生物医学和金融领域显著提高了领域任务性能，同时没有明显损害原始的长链式思维能力。


<details>
  <summary>Details</summary>
Motivation: To create a dual-capability model with long CoT capability and domain-specific knowledge without substantial computational and data costs, model merging emerges as a highly resource-efficient method. However, significant challenges lie in merging domain-specific LLMs with long CoT ones since nowadays merging methods suffer from reasoning capability degradation, even gibberish output and output collapse.

Method: RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior, a novel merging framework designed to integrate domain-specific LLMs with long CoT capability, meanwhile maintaining model performance in the original domain.

Result: Our results show that RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.

Conclusion: RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.

Abstract: Large Language Models (LLMs) with long chain-of-thought (CoT) capability,
termed Reasoning Models, demonstrate superior intricate problem-solving
abilities through multi-step long CoT reasoning. To create a dual-capability
model with long CoT capability and domain-specific knowledge without
substantial computational and data costs, model merging emerges as a highly
resource-efficient method. However, significant challenges lie in merging
domain-specific LLMs with long CoT ones since nowadays merging methods suffer
from reasoning capability degradation, even gibberish output and output
collapse. To overcome this, we introduce RCP-Merging: Merging Long
Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning
Capability as Prior, a novel merging framework designed to integrate
domain-specific LLMs with long CoT capability, meanwhile maintaining model
performance in the original domain. Treating reasoning model weights as
foundational prior, our method utilizes a reasoning capability indicator to
preserve core long CoT capability model weights while selectively merging
essential domain-specific weights. We conducted extensive experiments on
Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance
domains. Our results show that RCP-Merging successfully merges a reasoning
model with domain-specific ones, improving domain task performance by 9.5% and
9.2% over state-of-the-art methods, without significantly harming the original
long CoT reasoning capability.

</details>


### [15] [Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following](https://arxiv.org/abs/2508.03178)
*Chenyang Wang,Liang Wen,Shousheng Jia,Xiangzheng Zhang,Liang Xu*

Main category: cs.CL

TL;DR: 本文提出了一种框架，通过生成有效提示、拒绝采样和强化学习方法，提高了模型对复杂指令的遵循能力，并在多个模型规模上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在解决数学问题、编码任务和一般谜题方面的推理能力有了显著提高，但它们在准确遵循指令方面仍存在不一致的问题，特别是在处理更复杂的指令时。本文旨在解决这一问题，提高模型对复杂指令的遵循能力。

Method: 本文首先生成具有复杂约束的指令，并应用过滤过程获得有效的提示，得到三个不同的提示数据集。然后，对通过的提示进行拒绝采样，以创建一个小而高质量的数据集，从而实现模型的冷启动初始化，并促进其适应有效的推理模式。接着，采用熵保持监督微调（Entropy-SFT）策略和基于规则密集奖励的熵自适应（TEA-RL）强化学习方法。

Result: 实验结果表明，该方法在各种模型规模上都取得了显著的性能提升，其中Light-IF-32B模型超越了更大的开源模型和封闭源代码模型。

Conclusion: 本文提出了一种全面的框架，旨在使模型具备严格的推理过程，包括预览和自我检查，这对于满足严格的指令约束至关重要。实验表明，该方法在各种模型规模上都取得了显著的性能提升，其中Light-IF-32B模型超过了更大的开源模型和封闭源代码模型。

Abstract: While advancements in the reasoning abilities of LLMs have significantly
enhanced their performance in solving mathematical problems, coding tasks, and
general puzzles, their effectiveness in accurately adhering to instructions
remains inconsistent, particularly with more complex directives. Our
investigation identifies lazy reasoning during the thinking stage as the
primary factor contributing to poor instruction adherence. To mitigate this
issue, we propose a comprehensive framework designed to enable rigorous
reasoning processes involving preview and self-checking, essential for
satisfying strict instruction constraints. Specifically, we first generate
instructions with complex constraints and apply a filtering process to obtain
valid prompts, resulting in three distinct prompt datasets categorized as hard,
easy, and pass. Then, we employ rejection sampling on the pass prompts to
curate a small yet high-quality dataset, enabling a cold-start initialization
of the model and facilitating its adaptation to effective reasoning patterns.
Subsequently, we employ an entropy-preserving supervised fine-tuning
(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)
reinforcement learning guided by rule-based dense rewards. This approach
encourages the model to transform its reasoning mechanism, ultimately fostering
generalizable reasoning abilities that encompass preview and self-checking.
Extensive experiments conducted on instruction-following benchmarks demonstrate
remarkable performance improvements across various model scales. Notably, our
Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1
and closed-source models like Doubao-1.6.

</details>


### [16] [Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification](https://arxiv.org/abs/2508.03181)
*Lukas Pätz,Moritz Beyer,Jannik Späth,Lasse Bohlen,Patrick Zschech,Mathias Kraus,Julian Rosenberger*

Main category: cs.CL

TL;DR: 本研究通过分析28,000份议会演讲，利用机器学习模型探讨了德国议会中政治话语的演变，发现政党从政府到反对党的转变会影响其话语风格。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析德国议会中政治话语的演变，包括主题趋势、情感动态和政党特定的论述策略。

Method: 开发并训练了两个用于主题和情感分类的机器学习模型，使用手动标记的数据集进行训练。

Result: 模型在主题分类和情感分类上表现出色，分别达到了0.94和0.89的AUROC值。分析揭示了政党在议会中的角色以及其话语风格的变化。

Conclusion: 该研究揭示了德国议会中政党之间的显著关系，特别是当政党从政府转向反对党时风格的变化。虽然意识形态立场很重要，但执政责任也会影响话语。

Abstract: This study investigates political discourse in the German parliament, the
Bundestag, by analyzing approximately 28,000 parliamentary speeches from the
last five years. Two machine learning models for topic and sentiment
classification were developed and trained on a manually labeled dataset. The
models showed strong classification performance, achieving an area under the
receiver operating characteristic curve (AUROC) of 0.94 for topic
classification (average across topics) and 0.89 for sentiment classification.
Both models were applied to assess topic trends and sentiment distributions
across political parties and over time. The analysis reveals remarkable
relationships between parties and their role in parliament. In particular, a
change in style can be observed for parties moving from government to
opposition. While ideological positions matter, governing responsibilities also
shape discourse. The analysis directly addresses key questions about the
evolution of topics, sentiment dynamics, and party-specific discourse
strategies in the Bundestag.

</details>


### [17] [Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models](https://arxiv.org/abs/2508.03199)
*Muhammed Saeed,Shaina Raza,Ashmal Vayani,Muhammad Abdul-Mageed,Ali Emami,Shady Shehata*

Main category: cs.CL

TL;DR: 研究显示语法性别对文本到图像模型生成的视觉结果有显著影响，揭示了语言结构在AI偏见中的作用。


<details>
  <summary>Details</summary>
Motivation: 研究语法性别如何影响不同语言中的视觉表示，以揭示AI生成内容中的偏见问题。

Method: 我们引入了一个跨语言基准，研究语法性别与刻板性别关联相矛盾的词语，并分析了不同语言和模型架构的影响。

Result: 语法性别显著影响图像生成：阳性语法标记使男性表示增加到73%，而阴性语法标记使女性表示增加到38%。

Conclusion: 语言结构本身会影响AI生成的视觉输出，这为理解多语言、多模态系统中的偏见和公平性引入了一个新的维度。

Abstract: Research on bias in Text-to-Image (T2I) models has primarily focused on
demographic representation and stereotypical attributes, overlooking a
fundamental question: how does grammatical gender influence visual
representation across languages? We introduce a cross-linguistic benchmark
examining words where grammatical gender contradicts stereotypical gender
associations (e.g., ``une sentinelle'' - grammatically feminine in French but
referring to the stereotypically masculine concept ``guard''). Our dataset
spans five gendered languages (French, Spanish, German, Italian, Russian) and
two gender-neutral control languages (English, Chinese), comprising 800 unique
prompts that generated 28,800 images across three state-of-the-art T2I models.
Our analysis reveals that grammatical gender dramatically influences image
generation: masculine grammatical markers increase male representation to 73\%
on average (compared to 22\% with gender-neutral English), while feminine
grammatical markers increase female representation to 38\% (compared to 28\% in
English). These effects vary systematically by language resource availability
and model architecture, with high-resource languages showing stronger effects.
Our findings establish that language structure itself, not just content, shapes
AI-generated visual outputs, introducing a new dimension for understanding bias
and fairness in multilingual, multimodal systems.

</details>


### [18] [Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP](https://arxiv.org/abs/2508.03204)
*Abhirup Sinha,Pritilata Saha,Tithi Saha*

Main category: cs.CL

TL;DR: 本文探讨了在自然语言处理任务中对私人和敏感信息进行匿名化的方法，以保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型需要大量数据来学习语言变化，而这些数据可能包含私人信息。研究显示，可以从这些语言模型中提取私人信息，因此对私人和敏感信息进行匿名化非常重要。

Method: 本文讨论了几种用于领域无关自然语言处理任务的匿名化方法。

Result: 本文介绍了几种用于领域无关NLP任务的匿名化方法，以帮助保护数据隐私。

Conclusion: 隐私信息的匿名化对于保护用户隐私至关重要，尽管完全匿名化可能无法实现，但可以通过一些预处理方法来遮蔽或伪匿名化文本中的敏感信息。

Abstract: Privacy is a fundamental human right. Data privacy is protected by different
regulations, such as GDPR. However, modern large language models require a huge
amount of data to learn linguistic variations, and the data often contains
private information. Research has shown that it is possible to extract private
information from such language models. Thus, anonymizing such private and
sensitive information is of utmost importance. While complete anonymization may
not be possible, a number of different pre-processing approaches exist for
masking or pseudonymizing private information in textual data. This report
focuses on a few of such approaches for domain-agnostic NLP tasks.

</details>


### [19] [Probing Syntax in Large Language Models: Successes and Remaining Challenges](https://arxiv.org/abs/2508.03211)
*Pablo J. Diego-Simón,Emmanuel Chemla,Jean-Rémi King,Yair Lakretz*

Main category: cs.CL

TL;DR: 本文分析了结构探测器在评估句法表示时的局限性，并提出了一个受控基准以更好地评估其性能。


<details>
  <summary>Details</summary>
Motivation: 现有结构探测器通常在无区分的句子集合上进行评估，因此不清楚结构和/或统计因素是否系统地影响这些句法表示。

Method: 对三个受控基准进行深入分析，评估结构探测器的表现。

Result: 结构探测器受到表面属性的影响，对深层句法结构表现不佳，并且受到相互作用名词或不合法动词形式的干扰，但不受单个词语可预测性的影响。

Conclusion: 本文揭示了结构探测器在评估句法表示时面临的当前挑战，并提供了由受控刺激组成的基准以更好地评估其性能。

Abstract: The syntactic structures of sentences can be readily read-out from the
activations of large language models (LLMs). However, the ``structural probes''
that have been developed to reveal this phenomenon are typically evaluated on
an indiscriminate set of sentences. Consequently, it remains unclear whether
structural and/or statistical factors systematically affect these syntactic
representations. To address this issue, we conduct an in-depth analysis of
structural probes on three controlled benchmarks. Our results are three-fold.
First, structural probes are biased by a superficial property: the closer two
words are in a sentence, the more likely structural probes will consider them
as syntactically linked. Second, structural probes are challenged by linguistic
properties: they poorly represent deep syntactic structures, and get interfered
by interacting nouns or ungrammatical verb forms. Third, structural probes do
not appear to be affected by the predictability of individual words. Overall,
this work sheds light on the current challenges faced by structural probes.
Providing a benchmark made of controlled stimuli to better evaluate their
performance.

</details>


### [20] [CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting](https://arxiv.org/abs/2508.03240)
*Mutaz Ayesh,Nicolás Gutiérrez-Rolón,Fernando Alva-Manchego*

Main category: cs.CL

TL;DR: 本文介绍了CardiffNLP团队在CLEARS共享任务中的贡献，采用LLM提示方法并取得了良好成绩。


<details>
  <summary>Details</summary>
Motivation: 参与CLEARS共享任务，探索西班牙语文本适应的有效方法。

Method: 我们采用了LLM提示方法，并尝试了不同的提示变体。最终使用了Gemma-3进行提交。

Result: 在Subtask 1中获得第三名，在Subtask 2中获得第二名。

Conclusion: 我们的团队在两个子任务中分别获得了第三名和第二名，展示了LLM提示方法的有效性。

Abstract: This paper details the CardiffNLP team's contribution to the CLEARS shared
task on Spanish text adaptation, hosted by IberLEF 2025. The shared task
contained two subtasks and the team submitted to both. Our team took an
LLM-prompting approach with different prompt variations. While we initially
experimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and
landed third place in Subtask 1 and second place in Subtask 2. We detail our
numerous prompt variations, examples, and experimental results.

</details>


### [21] [Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs](https://arxiv.org/abs/2508.03247)
*Shintaro Sakai,Jisun An,Migyeong Kang,Haewoon Kwak*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在心理健康应用中缺乏文化敏感性，需要改进以更好地适应不同文化背景。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在心理健康应用中的文化敏感性，以确保其安全性和有效性。

Method: 通过提示西方或东方人物角色来测试大型语言模型（LLMs）是否再现这些文化模式。

Result: 当用英语提示时，大型语言模型大多未能复制这些模式，但在使用主要东方语言（如中文、日语和印地语）提示时，几种配置的对齐度有所提高。

Conclusion: 当前的通用大型语言模型缺乏稳健的文化意识能力，这对于安全有效的心理健康应用至关重要。

Abstract: Prior clinical psychology research shows that Western individuals with
depression tend to report psychological symptoms, while Eastern individuals
report somatic ones. We test whether Large Language Models (LLMs), which are
increasingly used in mental health, reproduce these cultural patterns by
prompting them with Western or Eastern personas. Results show that LLMs largely
fail to replicate the patterns when prompted in English, though prompting in
major Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment
in several configurations. Our analysis pinpoints two key reasons for this
failure: the models' low sensitivity to cultural personas and a strong,
culturally invariant symptom hierarchy that overrides cultural cues. These
findings reveal that while prompt language is important, current
general-purpose LLMs lack the robust, culture-aware capabilities essential for
safe and effective mental health applications.

</details>


### [22] [RooseBERT: A New Deal For Political Language Modelling](https://arxiv.org/abs/2508.03250)
*Deborah Dore,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文介绍了一个名为RooseBERT的新预训练语言模型，专门用于政治话语语言。通过在大型政治辩论和演讲语料库上进行训练，RooseBERT在四个与政治辩论分析相关的下游任务中表现出显著优于通用语言模型的性能，证明了领域特定预训练在该领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 政治辩论和政治相关讨论的增加需要定义新的计算方法来自动分析此类内容，以帮助公民了解政治辩论。然而，政治语言的特异性和辩论的论证形式（使用隐藏的沟通策略和利用隐含论点）使得这一任务极具挑战性，即使对于当前的通用预训练语言模型也是如此。

Method: 我们引入了一个名为RooseBERT的新预训练语言模型，专门用于政治话语语言。RooseBERT在大型政治辩论和演讲语料库（8K场辩论，每场包含多个不同主题的子辩论）上进行了训练。为了评估其性能，我们在四个与政治辩论分析相关的下游任务上对其进行了微调，包括命名实体识别、情感分析、论点组件检测和分类，以及论点关系预测和分类。

Result: RooseBERT在四个与政治辩论分析相关的下游任务中表现出显著优于通用语言模型的性能，表明领域特定预训练可以提高政治辩论分析的性能。

Conclusion: 通过在政治辩论和演讲语料库上进行预训练，RooseBERT在四个与政治辩论分析相关的下游任务中表现出显著优于通用语言模型的性能，证明了领域特定预训练在该领域的有效性。

Abstract: The increasing amount of political debates and politics-related discussions
calls for the definition of novel computational methods to automatically
analyse such content with the final goal of lightening up political
deliberation to citizens. However, the specificity of the political language
and the argumentative form of these debates (employing hidden communication
strategies and leveraging implicit arguments) make this task very challenging,
even for current general-purpose pre-trained Language Models. To address this
issue, we introduce a novel pre-trained Language Model for political discourse
language called RooseBERT. Pre-training a language model on a specialised
domain presents different technical and linguistic challenges, requiring
extensive computational resources and large-scale data. RooseBERT has been
trained on large political debate and speech corpora (8K debates, each composed
of several sub-debates on different topics) in English. To evaluate its
performances, we fine-tuned it on four downstream tasks related to political
debate analysis, i.e., named entity recognition, sentiment analysis, argument
component detection and classification, and argument relation prediction and
classification. Our results demonstrate significant improvements over
general-purpose Language Models on these four tasks, highlighting how
domain-specific pre-training enhances performance in political debate analysis.
We release the RooseBERT language model for the research community.

</details>


### [23] [Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition](https://arxiv.org/abs/2508.03259)
*Duzhen Zhang,Chenxing Li,Jiahua Dong,Qi Liu,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种稳定性-可塑性权衡（SPT）方法，用于持续命名实体识别（CNER），从表示和权重两个角度平衡这两个方面。该方法在多个基准数据集上表现出色，证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 之前的CNER方法主要利用知识蒸馏（KD）来保留先前的知识并克服灾难性遗忘，严格确保旧模型和新模型的表示一致。因此，它们往往使模型具有过多的稳定性（即保留旧知识）但有限的可塑性（即获取新知识）。为了应对这个问题，我们提出了一个稳定性-可塑性权衡（SPT）方法，从表示和权重两个角度平衡这些方面。

Method: 我们提出了一个稳定性-可塑性权衡（SPT）方法，从表示和权重两个角度平衡这两个方面。从表示的角度，我们在原始KD中引入了一个池化操作，通过整合表示维度来允许一定程度的可塑性。从权重的角度，我们动态地融合旧模型和新模型的权重，加强旧知识同时保持新知识。在融合过程中，我们实现了一个基于权重的选择机制，优先考虑重要的权重。此外，我们开发了一种基于置信度的伪标签方法，用于当前非实体类型，使用旧模型预测实体类型以处理非实体类型的语义变化，这是CNER特有的挑战，之前的方法大多忽略了这一点。

Result: 我们的SPT方法在十个CNER设置的三个基准数据集上的广泛实验中表现出色，证明了其在实现适当的稳定性-可塑性权衡方面的有效性。

Conclusion: 我们的SPT方法在十个CNER设置的三个基准数据集上的广泛实验中表现出色，证明了其在实现适当的稳定性-可塑性权衡方面的有效性。

Abstract: Continual Named Entity Recognition (CNER) is an evolving field that focuses
on sequentially updating an existing model to incorporate new entity types.
Previous CNER methods primarily utilize Knowledge Distillation (KD) to preserve
prior knowledge and overcome catastrophic forgetting, strictly ensuring that
the representations of old and new models remain consistent. Consequently, they
often impart the model with excessive stability (i.e., retention of old
knowledge) but limited plasticity (i.e., acquisition of new knowledge). To
address this issue, we propose a Stability-Plasticity Trade-off (SPT) method
for CNER that balances these aspects from both representation and weight
perspectives. From the representation perspective, we introduce a pooling
operation into the original KD, permitting a level of plasticity by
consolidating representation dimensions. From the weight perspective, we
dynamically merge the weights of old and new models, strengthening old
knowledge while maintaining new knowledge. During this fusion, we implement a
weight-guided selective mechanism to prioritize significant weights. Moreover,
we develop a confidence-based pseudo-labeling approach for the current
non-entity type, which predicts entity types using the old model to handle the
semantic shift of the non-entity type, a challenge specific to CNER that has
largely been ignored by previous methods. Extensive experiments across ten CNER
settings on three benchmark datasets demonstrate that our SPT method surpasses
previous CNER approaches, highlighting its effectiveness in achieving a
suitable stability-plasticity trade-off.

</details>


### [24] [Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?](https://arxiv.org/abs/2508.03262)
*Junhyuk Choi,Hyeonchu Park,Haemin Lee,Hyebeen Shin,Hyun Joung Jin,Bugeun Kim*

Main category: cs.CL

TL;DR: 本研究评估了LLMs在使用真实人类数据预测个体经济决策方面的能力，发现它们在群体层面的行为趋势上表现合理，但个体层面的预测仍有困难。


<details>
  <summary>Details</summary>
Motivation: 大多数研究依赖于虚构的人物，而不是实际的人类数据，因此我们希望通过使用真实的人类数据来评估LLMs预测个体经济决策的能力。

Method: 我们通过详细的个人资料信息，系统地比较了三种最先进的多模态LLMs，在文化消费场景中对522名韩国参与者的个人资料进行了评估。

Result: 结果表明，尽管LLMs在精确的个体层面预测上存在困难，但它们展示了合理的群体层面的行为趋势。此外，我们发现常用的提示技术并不比简单的提示方法更好；重建个人叙述或检索增强生成在与简单提示方法相比时没有显著优势。

Conclusion: 我们的研究提供了对LLMs在使用真实人类数据模拟经济行为方面能力的首次全面评估，为计算社会科学中的基于人物的模拟提供了实证指导。

Abstract: Recent advances in Large Language Models (LLMs) have generated significant
interest in their capacity to simulate human-like behaviors, yet most studies
rely on fictional personas rather than actual human data. We address this
limitation by evaluating LLMs' ability to predict individual economic
decision-making using Pay-What-You-Want (PWYW) pricing experiments with real
522 human personas. Our study systematically compares three state-of-the-art
multimodal LLMs using detailed persona information from 522 Korean participants
in cultural consumption scenarios. We investigate whether LLMs can accurately
replicate individual human choices and how persona injection methods affect
prediction performance. Results reveal that while LLMs struggle with precise
individual-level predictions, they demonstrate reasonable group-level
behavioral tendencies. Also, we found that commonly adopted prompting
techniques are not much better than naive prompting methods; reconstruction of
personal narrative nor retrieval augmented generation have no significant gain
against simple prompting method. We believe that these findings can provide the
first comprehensive evaluation of LLMs' capabilities on simulating economic
behavior using real human data, offering empirical guidance for persona-based
simulation in computational social science.

</details>


### [25] [LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning](https://arxiv.org/abs/2508.03275)
*Jiahao Zhao*

Main category: cs.CL

TL;DR: LECTOR是一个基于大型语言模型的自适应调度算法，用于以测试为导向的学习场景，特别是在语言考试中。它通过语义分析和个性化学习档案来提高学习效果，相比现有算法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的算法在语义干扰和个性化适应方面常常遇到困难。我们提出了LECTOR，这是一个专门针对以测试为导向的学习场景的自适应调度算法，特别是在语言考试中，成功率至关重要。

Method: LECTOR利用大型语言模型进行语义分析，并结合个性化学习档案，通过LLM驱动的语义相似性评估解决词汇学习中的语义混淆问题，并将其与已有的间隔重复原则相结合。

Result: 在100个模拟学习者上进行的100天评估显示，LECTOR的成功率达到90.2%，比最佳基线（SSP-MMC）的88.4%高出2.0%的相对改进。该算法在处理语义相似概念方面表现出色，减少了因混淆引起的错误，同时保持了计算效率。

Conclusion: 我们的结果确立了LECTOR作为智能辅导系统和自适应学习平台的有希望的方向。

Abstract: Spaced repetition systems are fundamental to efficient learning and memory
retention, but existing algorithms often struggle with semantic interference
and personalized adaptation. We present LECTOR (\textbf{L}LM-\textbf{E}nhanced
\textbf{C}oncept-based \textbf{T}est-\textbf{O}riented \textbf{R}epetition), a
novel adaptive scheduling algorithm specifically designed for test-oriented
learning scenarios, particularly language examinations where success rate is
paramount. LECTOR leverages large language models for semantic analysis while
incorporating personalized learning profiles, addressing the critical challenge
of semantic confusion in vocabulary learning by utilizing LLM-powered semantic
similarity assessment and integrating it with established spaced repetition
principles. Our comprehensive evaluation against six baseline algorithms
(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over
100 days demonstrates significant improvements: LECTOR achieves a 90.2\%
success rate compared to 88.4\% for the best baseline (SSP-MMC), representing a
2.0\% relative improvement. The algorithm shows particular strength in handling
semantically similar concepts, reducing confusion-induced errors while
maintaining computational efficiency. Our results establish LECTOR as a
promising direction for intelligent tutoring systems and adaptive learning
platforms.

</details>


### [26] [Do language models accommodate their users? A study of linguistic convergence](https://arxiv.org/abs/2508.03276)
*Terra Blevins,Susanne Schmalwieser,Benjamin Roth*

Main category: cs.CL

TL;DR: 该研究发现大型语言模型在对话中会适应用户的语言模式，但其行为机制可能与人类不同。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的语言使用是否与人类相似，特别是它们是否适应用户的语言模式。

Method: 通过比较模型对现有对话的完成与原始人类回应，分析模型是否适应用户的语言模式。

Result: 模型强烈地收敛到对话的风格，通常比人类基线过度拟合。

Conclusion: 模型在语言使用上表现出与人类相似的收敛性，但其背后的行为机制可能与人类不同。

Abstract: While large language models (LLMs) are generally considered proficient in
generating language, how similar their language usage is to that of humans
remains understudied. In this paper, we test whether models exhibit linguistic
convergence, a core pragmatic element of human language communication, asking:
do models adapt, or converge, to the linguistic patterns of their user? To
answer this, we systematically compare model completions of exisiting dialogues
to the original human responses across sixteen language models, three dialogue
corpora, and a variety of stylometric features. We find that models strongly
converge to the conversation's style, often significantly overfitting relative
to the human baseline. While convergence patterns are often feature-specific,
we observe consistent shifts in convergence across modeling settings, with
instruction-tuned and larger models converging less than their pretrained
counterparts. Given the differences between human and model convergence
patterns, we hypothesize that the underlying mechanisms for these behaviors are
very different.

</details>


### [27] [Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes](https://arxiv.org/abs/2508.03292)
*Shahed Masoudian,Gustavo Escobedo,Hannah Strauss,Markus Schedl*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型中的性别偏见，通过心理学中的性别刻板印象在开放式叙事生成任务中进行分析。引入了一个新数据集，并发现了三个关键结果：模型在无条件提示下偏向男性，但对与性别刻板印象无关的属性进行条件设置可以减轻这种偏见；结合多个与同一性别刻板印象相关的属性会加剧模型行为，男性方面的属性会增强偏见，而女性方面的属性会减少偏见；模型偏见与心理学真实数据一致，且随着模型规模的增加，一致性强度也增加。这些见解突显了基于心理学的LLM评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在各种应用中的使用日益增加，对其可能放大性别偏见的担忧也在上升。先前的研究通常使用显式性别提示作为反事实，或在句子补全和简短问答任务中研究它们。这些格式可能忽略了更隐性的偏见，这些偏见嵌入在长内容的生成行为中。

Method: 我们使用心理学中的性别刻板印象（例如攻击性或八卦）在开放式任务中研究LLM中的性别偏见，引入了一个名为StereoBias-Stories的新数据集，并分析了性别在整体故事中的贡献如何对这些属性做出反应。

Result: 我们发现，虽然模型在无条件提示下平均偏向男性，但对与性别刻板印象无关的属性进行条件设置可以减轻这种偏见。结合多个与同一性别刻板印象相关的属性会加剧模型行为，男性方面的属性会增强偏见，而女性方面的属性会减少偏见。模型偏见与用于分类的心理学真实数据一致，且随着模型规模的增加，一致性强度也增加。

Conclusion: 这些见解突显了基于心理学的LLM评估的重要性。

Abstract: As Large Language Models (LLMs) are increasingly used across different
applications, concerns about their potential to amplify gender biases in
various tasks are rising. Prior research has often probed gender bias using
explicit gender cues as counterfactual, or studied them in sentence completion
and short question answering tasks. These formats might overlook more implicit
forms of bias embedded in generative behavior of longer content. In this work,
we investigate gender bias in LLMs using gender stereotypes studied in
psychology (e.g., aggressiveness or gossiping) in an open-ended task of
narrative generation. We introduce a novel dataset called StereoBias-Stories
containing short stories either unconditioned or conditioned on (one, two, or
six) random attributes from 25 psychological stereotypes and three task-related
story endings. We analyze how the gender contribution in the overall story
changes in response to these attributes and present three key findings: (1)
While models, on average, are highly biased towards male in unconditioned
prompts, conditioning on attributes independent from gender stereotypes
mitigates this bias. (2) Combining multiple attributes associated with the same
gender stereotype intensifies model behavior, with male ones amplifying bias
and female ones alleviating it. (3) Model biases align with psychological
ground-truth used for categorization, and alignment strength increases with
model size. Together, these insights highlight the importance of
psychology-grounded evaluation of LLMs.

</details>


### [28] [NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty](https://arxiv.org/abs/2508.03294)
*Leonidas Zotos,Ivo Pascal de Jong,Matias Valdenegro-Toro,Andreea Ioana Sburlea,Malvina Nissim,Hedderik van Rijn*

Main category: cs.CL

TL;DR: 本研究比较了基于大型语言模型的方法与教授在估计考试题目难度方面的能力，发现使用LLM的不确定性进行监督学习可以提高评估质量。


<details>
  <summary>Details</summary>
Motivation: 估计考试题目的难度对于开发好的考试至关重要，但教授们并不总是擅长这项任务。

Method: 我们比较了各种基于大型语言模型的方法与三位教授在估计学生正确回答真/假考试题目百分比方面的能力。我们发现，使用LLM的不确定性在监督学习设置中可以得到更好的结果，仅需42个训练样本。

Result: 教授们在区分容易和困难的问题方面能力有限，并且被直接询问Gemini 2.5来解决此任务所超越。然而，使用LLM解决这些问题的不确定性在监督学习设置中获得了更好的结果。

Conclusion: 我们得出结论，使用LLM不确定性进行监督学习可以帮助教授更好地估计考试题目的难度，从而提高评估的质量。

Abstract: Estimating the difficulty of exam questions is essential for developing good
exams, but professors are not always good at this task. We compare various
Large Language Model-based methods with three professors in their ability to
estimate what percentage of students will give correct answers on True/False
exam questions in the areas of Neural Networks and Machine Learning. Our
results show that the professors have limited ability to distinguish between
easy and difficult questions and that they are outperformed by directly asking
Gemini 2.5 to solve this task. Yet, we obtained even better results using
uncertainties of the LLMs solving the questions in a supervised learning
setting, using only 42 training samples. We conclude that supervised learning
using LLM uncertainty can help professors better estimate the difficulty of
exam questions, improving the quality of assessment.

</details>


### [29] [Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling](https://arxiv.org/abs/2508.03296)
*Anqi Li,Wenwei Jin,Jintao Tong,Pengda Qin,Weijia Li,Guo Lu*

Main category: cs.CL

TL;DR: Hi-Guard is a new content moderation framework that improves accuracy, interpretability, and alignment with policies through a hierarchical design and advanced optimization techniques.


<details>
  <summary>Details</summary>
Motivation: Current approaches to content moderation rely on noisy, label-driven learning, which lacks alignment with moderation rules and produces opaque decisions that hinder human review. There is a need for a system that offers accuracy, interpretability, and alignment with evolving policies.

Method: Hi-Guard is a multimodal moderation framework that introduces a new policy-aligned decision paradigm. It includes a hierarchical moderation pipeline and a hierarchical taxonomy for path-based classification. The model incorporates rule definitions into the prompt and uses a multi-level soft-margin reward with Group Relative Policy Optimization (GRPO).

Result: Hi-Guard demonstrates superior classification accuracy, generalization, and interpretability in extensive experiments and real-world deployment.

Conclusion: Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems.

Abstract: Social platforms have revolutionized information sharing, but also
accelerated the dissemination of harmful and policy-violating content. To
ensure safety and compliance at scale, moderation systems must go beyond
efficiency and offer accuracy and interpretability. However, current approaches
largely rely on noisy, label-driven learning, lacking alignment with moderation
rules and producing opaque decisions that hinder human review. Therefore, we
propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that
introduces a new policy-aligned decision paradigm. The term "Hierarchical"
reflects two key aspects of our system design: (1) a hierarchical moderation
pipeline, where a lightweight binary model first filters safe content and a
stronger model handles fine-grained risk classification; and (2) a hierarchical
taxonomy in the second stage, where the model performs path-based
classification over a hierarchical taxonomy ranging from coarse to fine-grained
levels. To ensure alignment with evolving moderation policies, Hi-Guard
directly incorporates rule definitions into the model prompt. To further
enhance structured prediction and reasoning, we introduce a multi-level
soft-margin reward and optimize with Group Relative Policy Optimization (GRPO),
penalizing semantically adjacent misclassifications and improving explanation
quality. Extensive experiments and real-world deployment demonstrate that
Hi-Guard achieves superior classification accuracy, generalization, and
interpretability, paving the way toward scalable, transparent, and trustworthy
content safety systems. Code is available at:
https://github.com/lianqi1008/Hi-Guard.

</details>


### [30] [CTTS: Collective Test-Time Scaling](https://arxiv.org/abs/2508.03333)
*Zhende Song,Shengji Tang,Peng Ye,Jiayuan Fan,Tao Chen*

Main category: cs.CL

TL;DR: 本文探索了集体测试时间扩展（CTTS），提出了一种新框架CTTS-MM，通过多代理和多奖励模型的协作提升大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如Best-of-N和Self-Consistency受到单代理测试时间扩展（STTS）范式的限制。因此，本文旨在探索集体测试时间扩展（CTTS），以突破单代理系统的上限。

Method: 本文设计了三种主要范式来研究CTTS的最佳范式：(1) 单个代理到多个奖励模型（SA-MR）；(2) 多个代理到单个奖励模型（MA-SR）；(3) 多个代理到多个奖励模型（MA-MR）。基于此，提出了CTTS-MM框架，其中包含Agent Collaboration Search (ACS) 和 Mixture of Reword Models (MoR)。

Result: 实验表明，MA-MR范式在所有测试中表现最佳。CTTS-MM框架在七个主流基准上 consistently 获得优越性能。

Conclusion: 本文提出了一种名为CTTS-MM的新框架，该框架有效地利用了多代理和多奖励模型的协作以增强推理。实验表明，CTTS-MM在七个主流基准上 consistently 获得优越性能。

Abstract: Test-time scaling (TTS) has emerged as a promising research field for
enhancing the effectiveness of large language models (LLMs) without extra
training. However, most existing approaches, e.g., Best-of-N and
Self-Consistency rely on a single agent interacting with a reward model
(SA-SR), constrained by limited capabilities of a single test-time scaling
(STTS) paradigm. On the other hand, recent works demonstrate that
collective-agent methods can break through the upper bound of single-agent
systems by orchestrating diverse models. Thus, in this paper, we take a first
step towards exploring Collective Test-Time Scaling (CTTS). Consider the
different interaction types of single and multiple models, we design three
primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent
to multiple reward models (SA-MR); (2) multiple agents to single reward model
(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive
experiments demonstrate that MA-MR consistently achieves the best performance.
Based on this, we propose a novel framework named CTTS-MM that effectively
leverages both multi-agent and multi-reward-model collaboration for enhanced
inference. Specifically, for multi-agent collaboration, we propose an Agent
Collaboration Search (ACS), which searches for the most effective combination
of LLM agents from a large candidate pool; for multi-reward-model
collaboration, we propose Mixture of Reword Models (MoR), which consists of a
curated question pool and a Prior Reward model Ensemble Selection (PRES) to
select the optimal combinations of reward models via Pair-wise Reward Ranking
(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that
the proposed CTTS-MM consistently obtains superior performance. Code will be
released at https://github.com/magent4aci/CTTS-MM.

</details>


### [31] [Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature](https://arxiv.org/abs/2508.03358)
*Tiago G Canário,Catarina Duarte,Flávio L. Pinheiro,João L. M. Pereira*

Main category: cs.CL

TL;DR: 本文提出了一种名为Taggus的管道，用于从葡萄牙语文学作品中提取社会网络。该方法结合了POS标注和启发式方法，在角色识别和互动检测任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理文学作品中的角色和互动识别任务时表现不佳，特别是在资源较少的语言中。因此，需要一种更有效的方法来解决这个问题。

Method: Taggus管道结合了POS标注和启发式方法，用于从葡萄牙语文学作品中提取社会网络。

Result: Taggus管道在角色识别任务中的平均F1分数为94.1%，在互动检测任务中的平均F1分数为75.9%。这些结果分别比现有最先进的工具提高了50.7%和22.3%。

Conclusion: Taggus管道在识别角色和解决共指方面表现出色，且在互动检测方面也取得了良好结果。该方法为葡萄牙语文学作品的社会网络提取提供了一个有效的解决方案，并鼓励了该领域的进一步发展。

Abstract: Automatically identifying characters and their interactions from fiction
books is, arguably, a complex task that requires pipelines that leverage
multiple Natural Language Processing (NLP) methods, such as Named Entity
Recognition (NER) and Part-of-speech (POS) tagging. However, these methods are
not optimized for the task that leads to the construction of Social Networks of
Characters. Indeed, the currently available methods tend to underperform,
especially in less-represented languages, due to a lack of manually annotated
data for training. Here, we propose a pipeline, which we call Taggus, to
extract social networks from literary fiction works in Portuguese. Our results
show that compared to readily available State-of-the-Art tools -- off-the-shelf
NER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which
uses POS tagging and a combination of heuristics, achieves satisfying results
with an average F1-Score of $94.1\%$ in the task of identifying characters and
solving for co-reference and $75.9\%$ in interaction detection. These
represent, respectively, an increase of $50.7\%$ and $22.3\%$ on results
achieved by the readily available State-of-the-Art tools. Further steps to
improve results are outlined, such as solutions for detecting relationships
between characters. Limitations on the size and scope of our testing samples
are acknowledged. The Taggus pipeline is publicly available to encourage
development in this field for the Portuguese language.2

</details>


### [32] [Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models](https://arxiv.org/abs/2508.03363)
*Haotian Wu,Bo Xu,Yao Shu,Menglin Yang,Chengwei Qin*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于上下文学习的范式JointThinking，通过利用两种推理模式之间的结构差异来提高推理准确性。实验表明，JointThinking在多个推理基准上显著优于少样本链式思维和多数投票方法，并且在分布内任务上与基于训练的最先进方法相当，在分布外任务上表现更好。此外，本文还分析了校准机制，展示了不同推理模式的一致性降低错误率并突出结构思维多样性的重要性。最后，讨论了当前的局限性并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究主要集中在改进大型语言模型的训练和推理策略，但它们在上下文学习方面的潜力仍 largely underexplored。因此，本文旨在探索RLLMs在上下文学习中的潜力，并提出一种新的方法来提高推理准确性。

Method: 本文提出了一种新的基于上下文学习的范式JointThinking，该方法通过让模型并行生成两种答案（即Thinking模式和Nothinking模式）来提高推理准确性。当两个初始响应不一致时，仅触发一次额外的Thinking过程，使用包含原始问题和两个候选答案的单一提示。

Result: 实验结果表明，JointThinking在多个推理基准上显著优于少样本链式思维和多数投票方法，并且在分布内任务上与基于训练的最先进方法相当，在分布外任务上表现更好。此外，本文还发现随着模型规模的增加，实际和理想推理之间的性能差距缩小，表明该方法具有良好的可扩展性。

Conclusion: 本文提出了一种新的基于上下文学习的范式JointThinking，通过利用两种推理模式之间的结构差异来提高推理准确性。实验表明，JointThinking在多个推理基准上显著优于少样本链式思维和多数投票方法，并且在分布内任务上与基于训练的最先进方法相当，在分布外任务上表现更好。此外，本文还分析了校准机制，展示了不同推理模式的一致性降低错误率并突出结构思维多样性的重要性。最后，讨论了当前的局限性并提出了未来研究方向。

Abstract: Reasoning large language models (RLLMs) have recently demonstrated remarkable
capabilities through structured and multi-step reasoning. While prior research
has primarily focused on improving their training and inference strategies,
their potential for in-context learning (ICL) remains largely underexplored. To
fill this gap, we propose Thinking with Nothinking Calibration (JointThinking),
a new ICL paradigm that leverages the structured difference between two
reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.
Specifically, our method prompts the model to generate two answers in parallel:
one in Thinking mode and the other in Nothinking mode. A second round of
Thinking is triggered only when the two initial responses are inconsistent,
using a single prompt that incorporates the original question and both
candidate answers. Since such disagreement occurs infrequently (e.g., only 6\%
in GSM8K), our method performs just one round of reasoning in most cases,
resulting in minimal latency overhead. Extensive experiments across multiple
reasoning benchmarks demonstrate that JointThinking significantly outperforms
few-shot chain-of-thought (CoT) and majority voting with improved answer
robustness. Moreover, It achieves comparable in-distribution performance to
training-based SOTA method, while substantially outperforming on
out-of-distribution tasks. We further conduct a systematic analysis of the
calibration mechanism, showing that leveraging different reasoning modes
consistently lowers the error rate and highlights the value of structural
thinking diversity. Additionally, we observe that the performance gap between
actual and ideal reasoning narrows as model size increases in the second round
of thinking, indicating the strong scalability of our approach. Finally, we
discuss current limitations and outline promising directions for future ICL
research in RLLMs.

</details>


### [33] [ReDSM5: A Reddit Dataset for DSM-5 Depression Detection](https://arxiv.org/abs/2508.03399)
*Eliseo Bao,Anxo Pérez,Javier Parapar*

Main category: cs.CL

TL;DR: The paper introduces ReDSM5, a novel Reddit corpus annotated with DSM-5 depression symptoms and expert explanations, enabling the development of models that can detect depression and generate interpretable reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing computational approaches often label entire posts as depressed or not depressed without linking language to specific criteria from the DSM-5, limiting clinical relevance and interpretability. The paper aims to address this gap by introducing ReDSM5, a corpus that combines symptom-specific supervision with expert explanations.

Method: The paper introduces ReDSM5, a Reddit corpus comprising 1484 long-form posts annotated at the sentence level by a licensed psychologist for the nine DSM-5 depression symptoms. The annotator also provides concise clinical rationales grounded in DSM-5 methodology. The paper conducts an exploratory analysis of the collection, examining lexical, syntactic, and emotional patterns that characterize symptom expression in social media narratives.

Result: ReDSM5 is a novel Reddit corpus that combines symptom-specific supervision with expert explanations, facilitating the development of models that can detect depression and generate human-interpretable reasoning. The paper establishes baseline benchmarks for multi-label symptom classification and explanation generation.

Conclusion: ReDSM5 provides a novel Reddit corpus that combines symptom-specific supervision with expert explanations, facilitating the development of models that can detect depression and generate human-interpretable reasoning. The paper establishes baseline benchmarks for multi-label symptom classification and explanation generation.

Abstract: Depression is a pervasive mental health condition that affects hundreds of
millions of individuals worldwide, yet many cases remain undiagnosed due to
barriers in traditional clinical access and pervasive stigma. Social media
platforms, and Reddit in particular, offer rich, user-generated narratives that
can reveal early signs of depressive symptomatology. However, existing
computational approaches often label entire posts simply as depressed or not
depressed, without linking language to specific criteria from the DSM-5, the
standard clinical framework for diagnosing depression. This limits both
clinical relevance and interpretability. To address this gap, we introduce
ReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each
exhaustively annotated at the sentence level by a licensed psychologist for the
nine DSM-5 depression symptoms. For each label, the annotator also provides a
concise clinical rationale grounded in DSM-5 methodology. We conduct an
exploratory analysis of the collection, examining lexical, syntactic, and
emotional patterns that characterize symptom expression in social media
narratives. Compared to prior resources, ReDSM5 uniquely combines
symptom-specific supervision with expert explanations, facilitating the
development of models that not only detect depression but also generate
human-interpretable reasoning. We establish baseline benchmarks for both
multi-label symptom classification and explanation generation, providing
reference results for future research on detection and interpretability.

</details>


### [34] [Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations](https://arxiv.org/abs/2508.03420)
*Bing Wang,Ximing Li,Yiming Wang,Changchun Li,Jiaxu Cui,Renchu Guan,Bo Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的虚假信息检测框架MISDER，通过学习动态社会环境表示并利用时间模型预测未来表示，提高了虚假信息检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于虚假信息在社交媒体上的传播对社会造成负面影响，需要一种能够适应动态社会环境的虚假信息检测方法。

Method: MISDER框架通过学习每个时期的社交媒体环境表示，并利用时间模型预测未来时期的表示来解决虚假信息检测问题。具体包括MISDER-LSTM、MISDER-ODE和MISDER-PT三种变体。

Result: 实验结果表明，MISDER框架在两个主流数据集上优于各种基线方法，证明了其有效性。

Conclusion: MISDER框架在检测虚假信息方面表现出色，能够有效应对动态社会环境中的新闻真实性变化问题。

Abstract: The proliferation of misinformation across diverse social media platforms has
drawn significant attention from both academic and industrial communities due
to its detrimental effects. Accordingly, automatically distinguishing
misinformation, dubbed as Misinformation Detection (MD), has become an
increasingly active research topic. The mainstream methods formulate MD as a
static learning paradigm, which learns the mapping between the content, links,
and propagation of news articles and the corresponding manual veracity labels.
However, the static assumption is often violated, since in real-world
scenarios, the veracity of news articles may vacillate within the dynamically
evolving social environment. To tackle this problem, we propose a novel
framework, namely Misinformation detection with Dynamic Environmental
Representations (MISDER). The basic idea of MISDER lies in learning a social
environmental representation for each period and employing a temporal model to
predict the representation for future periods. In this work, we specify the
temporal model as the LSTM model, continuous dynamics equation, and pre-trained
dynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,
MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,
we compare it to various MD baselines across 2 prevalent datasets, and the
experimental results can indicate the effectiveness of our proposed model.

</details>


### [35] [LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models](https://arxiv.org/abs/2508.03440)
*Junhong Wu,Jinliang Lu,Zixuan Ren,Ganqiang Hu,Zhi Wu,Dai Dai,Hua Wu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型的'软思考'能力，发现其在实际应用中往往退化为贪心解码方式。为了克服这一问题，我们引入了随机性，如Dirichlet重采样和Gumbel-Softmax技巧，实验结果表明这些方法能有效提升软思考的效果。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型通常依赖于生成离散标记，这可能限制了它们的表达能力。近年来，研究人员试图通过让大型语言模型生成软抽象标记来解决这一问题，从而在连续概念空间中进行推理。然而，这种软思考方法的实际效果尚未完全明确，因此需要进一步研究。

Method: 本文通过一系列探测技术分析了不同大型语言模型的内部行为，以研究它们的'软思考'能力。此外，还探讨了引入随机性的采样策略，如Dirichlet重采样和Gumbel-Softmax技巧，以解决软思考方法中的局限性。

Result: 我们的研究发现，大型语言模型主要依赖于软输入中最重要的一部分进行后续解码步骤，这限制了不同推理路径的探索。通过引入随机性，如Dirichlet重采样和Gumbel-Softmax技巧，可以缓解这一问题，并提升软思考的效果。其中，Gumbel-Softmax技巧在控制平滑度的同时提供了足够的随机性，从而在多个推理基准上取得了更好的表现。

Conclusion: 本文发现，现有的软思考方法在实际应用中往往退化为一种贪心解码方式，限制了其优势。为了克服这一问题，我们探索了引入随机性的采样策略，如Dirichlet重采样和Gumbel-Softmax技巧。实验结果表明，这些方法能够缓解传统方法的局限性，并释放软思考的潜力。特别是Gumbel-Softmax技巧在控制平滑度的同时提供了足够的随机性，从而在八个推理基准上表现出色。

Abstract: Human cognition naturally engages with abstract and fluid concepts, whereas
existing reasoning models often rely on generating discrete tokens, potentially
constraining their expressive capabilities. Recent advancements aim to address
this limitation by enabling large language models (LLMs) to generate soft,
abstract tokens, thus facilitating reasoning within a continuous concept space.
This paper explores the `Soft Thinking' capabilities of various LLMs by
examining the models' internal behavior using a suite of probing techniques.
Contrary to the common belief that Soft Thinking enables the simultaneous
exploration of diverse reasoning paths, our findings reveal that LLMs
predominantly rely on the most influential component of the soft inputs during
subsequent decoding steps. This reliance hinders the exploration of different
reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,
obscuring the advantage of transmitting more information through Soft Tokens.
To tackle this issue, we explore sampling strategies to introduce
\emph{randomness}, employing methods such as Dirichlet resampling and the
Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness
can alleviate the limitations of vanilla approaches and unleash the potential
of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate
randomness with controlled smoothness, resulting in superior performance across
eight reasoning benchmarks.

</details>


### [36] [Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings](https://arxiv.org/abs/2508.03453)
*Rita González-Márquez,Philipp Berens,Dmitry Kobak*

Main category: cs.CL

TL;DR: 本文比较了两种文本嵌入的增强策略，发现裁剪增强优于基于丢弃的方法。自监督微调在域内数据上可以生成高质量的嵌入，且仅微调最后几层就足够达到相似的嵌入质量。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的嵌入模型是通过使用精心策划的文本对进行广泛的监督微调获得的，这与计算机视觉中基于数据增强的自监督训练形成对比。本文旨在比较这两种增强策略，并评估自监督微调在文本嵌入中的效果。

Method: 本文系统比较了对比学习中文本嵌入的两种最著名的增强策略，即裁剪增强和基于丢弃的方法，并评估了在MTEB和其他领域内的嵌入质量。

Result: 实验结果表明，裁剪增强策略在嵌入质量上显著优于基于丢弃的方法。在域外数据上，自监督微调生成的嵌入质量低于监督SOTA模型，但在域内数据上，经过非常短时间的微调即可生成高质量的文本嵌入，有时仅略低于监督SOTA模型。

Conclusion: 本文结论是，自监督微调可以在特定领域数据上生成高质量的文本嵌入，甚至在非常短的微调时间内接近监督SOTA模型的性能。此外，表示质量随着Transformer层的加深而提高，仅微调最后几层就足以达到相似的嵌入质量。

Abstract: Text embeddings, i.e. vector representations of entire texts, play an
important role in many NLP applications, such as retrieval-augmented
generation, sentiment analysis, clustering, or visualizing collections of texts
for data exploration. Currently, top-performing embedding models are derived
from pre-trained language models via extensive supervised fine-tuning using
curated text pairs. This contrasts with computer vision, where self-supervised
training based on data augmentations has demonstrated remarkable success. Here
we systematically compare the two most well-known augmentation strategies for
positive pair generation in contrastive learning of text embeddings. We assess
embedding quality on MTEB and additional in-domain evaluations and show that
cropping augmentation strongly outperforms the dropout-based approach. We find
that on out-of-domain data, the quality of resulting embeddings is below the
supervised SOTA models, but for in-domain data, self-supervised fine-tuning
produces high-quality text embeddings after very short fine-tuning, sometimes
only marginally below the supervised SOTA. Finally, we show that representation
quality increases towards the last transformer layers, which undergo the
largest change during fine-tuning; and that fine-tuning only those last layers
is sufficient to reach similar embedding quality.

</details>


### [37] [fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval](https://arxiv.org/abs/2508.03475)
*Pranshu Rastogi*

Main category: cs.CL

TL;DR: 本文提出了一种基于双编码器模型的多语言和跨语言事实核查声明检索方法，并在任务中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 为了提高多语言和跨语言事实核查声明检索的准确性，需要一种有效的模型方法。

Method: 将多语言和跨语言事实核查声明检索问题建模为学习排序任务，使用从优化句子相似性的预训练变压器微调的双编码器模型。

Result: 该方法在多语言任务中达到了92%的Success@10，在跨语言任务中达到了80%的Success@10。

Conclusion: 该方法在多语言和跨语言检索任务中取得了良好的性能，证明了其有效性。

Abstract: SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim
Retrieval is approached as a Learning-to-Rank task using a bi-encoder model
fine-tuned from a pre-trained transformer optimized for sentence similarity.
Training used both the source languages and their English translations for
multilingual retrieval and only English translations for cross-lingual
retrieval. Using lightweight models with fewer than 500M parameters and
training on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual
and 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.

</details>


### [38] [CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03489)
*Kaiwen Zhao,Bharathan Balaji,Stephen Lee*

Main category: cs.CL

TL;DR: 本文介绍了CarbonPDF-QA数据集，并提出了一种基于大语言模型的技术CarbonPDF，用于回答PDF格式的可持续性报告中的碳足迹问题。


<details>
  <summary>Details</summary>
Motivation: 产品可持续性报告提供了关于产品环境影响的重要见解，但这些报告通常以PDF格式分发，并包含表格和文本的组合，这使得它们的分析变得复杂。缺乏标准化和报告格式的变异性进一步加剧了从大量文档中提取和解释相关信息的难度。

Method: 我们通过在训练数据上微调Llama 3来开发CarbonPDF。

Result: 我们的分析表明，GPT-4o难以回答数据不一致的问题。通过提出CarbonPDF，一种专门设计用于回答此类数据集上碳足迹问题的基于大语言模型的技术，我们解决了这一限制。

Conclusion: 我们的技术在回答碳足迹问题上优于当前最先进的技术，包括在表格和文本数据上微调的问答系统。

Abstract: Product sustainability reports provide valuable insights into the
environmental impacts of a product and are often distributed in PDF format.
These reports often include a combination of tables and text, which complicates
their analysis. The lack of standardization and the variability in reporting
formats further exacerbate the difficulty of extracting and interpreting
relevant information from large volumes of documents. In this paper, we tackle
the challenge of answering questions related to carbon footprints within
sustainability reports available in PDF format. Unlike previous approaches, our
focus is on addressing the difficulties posed by the unstructured and
inconsistent nature of text extracted from PDF parsing. To facilitate this
analysis, we introduce CarbonPDF-QA, an open-source dataset containing
question-answer pairs for 1735 product report documents, along with
human-annotated answers. Our analysis shows that GPT-4o struggles to answer
questions with data inconsistencies. To address this limitation, we propose
CarbonPDF, an LLM-based technique specifically designed to answer carbon
footprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama
3 with our training data. Our results show that our technique outperforms
current state-of-the-art techniques, including question-answering (QA) systems
finetuned on table and text data.

</details>


### [39] [UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression](https://arxiv.org/abs/2508.03520)
*Md Rakibul Hasan,Md Zakir Hossain,Aneesh Krishna,Shafin Rahman,Tom Gedeon*

Main category: cs.CL

TL;DR: UPLME is a new framework for empathy regression that handles noisy labels by predicting both empathy scores and uncertainty, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of noisy self-reported empathy scores in supervised learning for empathy regression motivates the development of a framework that can effectively handle label noise in regression settings.

Method: UPLME is an uncertainty-aware probabilistic language modelling framework that predicts both empathy scores and heteroscedastic uncertainty using Bayesian concepts with variational model ensembling. It includes two novel loss components to penalize degenerate uncertainty quantification and enforce similarity between input pairs.

Result: UPLME achieves state-of-the-art performance (Pearson Correlation Coefficient: 0.558→0.580 and 0.629→0.634) on two public benchmarks with label noise. It also demonstrates effectiveness in separating noisy and clean samples based on predicted uncertainty and outperforms a recent variational model ensembling-based UQ method in terms of calibration error (0.571→0.376).

Conclusion: UPLME provides state-of-the-art performance in empathy regression tasks with noisy labels and outperforms existing methods in terms of calibration error.

Abstract: Supervised learning for empathy regression is challenged by noisy
self-reported empathy scores. While many algorithms have been proposed for
learning with noisy labels in textual classification problems, the regression
counterpart is relatively under-explored. We propose UPLME, an
uncertainty-aware probabilistic language modelling framework to capture label
noise in the regression setting of empathy detection. UPLME includes a
probabilistic language model that predicts both empathy score and
heteroscedastic uncertainty and is trained using Bayesian concepts with
variational model ensembling. We further introduce two novel loss components:
one penalises degenerate Uncertainty Quantification (UQ), and another enforces
the similarity between the input pairs on which we predict empathy. UPLME
provides state-of-the-art performance (Pearson Correlation Coefficient:
$0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the
performance reported in the literature in two public benchmarks, having label
noise. Through synthetic label noise injection, we show that UPLME is effective
in separating noisy and clean samples based on the predicted uncertainty. UPLME
further outperform (Calibration error: $0.571\rightarrow0.376$) a recent
variational model ensembling-based UQ method designed for regression problems.

</details>


### [40] [FilBench: Can LLMs Understand and Generate Filipino?](https://arxiv.org/abs/2508.03523)
*Lester James V. Miranda,Elyanah Aco,Conner Manuel,Jan Christian Blaise Cruz,Joseph Marvin Imperial*

Main category: cs.CL

TL;DR: 本文介绍了FilBench，一个用于评估LLM在菲律宾语、他加禄语和宿务语中的表现的基准。结果表明，现有的LLM在这些语言上的表现有限，强调了构建语言特定基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在基于英语的任务中表现出色，但对其在特定语言（如菲律宾语）中的能力了解甚少。我们希望通过构建FilBench来填补这一空白。

Method: 我们引入了FilBench，这是一个以菲律宾语为中心的基准，旨在评估LLM在菲律宾语、他加禄语和宿务语中的各种任务和能力。

Result: 我们在FilBench上评估了27个最先进的LLM，发现一些LLM在阅读理解和翻译方面存在不足。最佳模型GPT-4o仅获得了72.23%的分数。此外，专门为东南亚语言训练的模型在FilBench上的表现较差，最高得分模型SEA-LION v3 70B仅获得61.07%的分数。

Conclusion: 我们的工作展示了为特定语言构建LLM基准的价值，有助于推动菲律宾语NLP的发展，并增加菲律宾语言在LLM开发中的包容性。

Abstract: Despite the impressive performance of LLMs on English-based tasks, little is
known about their capabilities in specific languages such as Filipino. In this
work, we address this gap by introducing FilBench, a Filipino-centric benchmark
designed to evaluate LLMs across a diverse set of tasks and capabilities in
Filipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to
reflect the priorities and trends of NLP research in the Philippines such as
Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By
evaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs
suffer from reading comprehension and translation capabilities. Our results
indicate that FilBench is challenging, with the best model, GPT-4o, achieving
only a score of 72.23%. Moreover, we also find that models trained specifically
for Southeast Asian languages tend to underperform on FilBench, with the
highest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.
Our work demonstrates the value of curating language-specific LLM benchmarks to
aid in driving progress on Filipino NLP and increasing the inclusion of
Philippine languages in LLM development.

</details>


### [41] [Marito: Structuring and Building Open Multilingual Terminologies for South African NLP](https://arxiv.org/abs/2508.03529)
*Vukosi Marivate,Isheanesu Dzingirai,Fiskani Banda,Richard Lastrucci,Thapelo Sindane,Keabetswe Madumo,Kayode Olaleye,Abiodun Modupe,Unarine Netshifhefhe,Herkulaas Combrink,Mohlatlego Nakeng,Matome Ledwaba*

Main category: cs.CL

TL;DR: Marito 解决了南非官方语言缺乏结构化术语数据的问题，通过聚合、清理和标准化资源，创建了开放、互操作的数据集，并在英语到茨瓦拉语的机器翻译中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 南非官方语言缺乏结构化的术语数据，阻碍了多语言自然语言处理的发展，尽管存在许多政府和学术术语列表。这些宝贵的资源仍然碎片化且无法机器读取，使其无法用于计算研究和开发。

Method: Marito 通过系统地聚合、清理和标准化这些分散的资源，将其转化为开放、互操作的数据集。

Result: 实验表明，在大型语言模型的英语到茨瓦拉语机器翻译中，准确性以及领域特定的一致性有显著提高。

Conclusion: Marito 提供了一个可扩展的基础，用于开发强大且公平的自然语言处理技术，确保南非丰富的语言多样性在数字时代得到体现。

Abstract: The critical lack of structured terminological data for South Africa's
official languages hampers progress in multilingual NLP, despite the existence
of numerous government and academic terminology lists. These valuable assets
remain fragmented and locked in non-machine-readable formats, rendering them
unusable for computational research and development. \emph{Marito} addresses
this challenge by systematically aggregating, cleaning, and standardising these
scattered resources into open, interoperable datasets. We introduce the
foundational \emph{Marito} dataset, released under the equitable,
Africa-centered NOODL framework. To demonstrate its immediate utility, we
integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.
Experiments show substantial improvements in the accuracy and domain-specific
consistency of English-to-Tshivenda machine translation for large language
models. \emph{Marito} provides a scalable foundation for developing robust and
equitable NLP technologies, ensuring South Africa's rich linguistic diversity
is represented in the digital age.

</details>


### [42] [EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models](https://arxiv.org/abs/2508.03533)
*Xiaoming Hou,Jiquan Zhang,Zibin Lin,DaCheng Tao,Shengli Zhang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Effectively adapting powerful pretrained foundation models to diverse tasks
remains a key challenge in AI deployment. Current approaches primarily follow
two paradigms:discrete optimization of text prompts through prompt engineering,
or continuous adaptation via additional trainable parameters. Both exhibit
limitations-discrete methods lack refinement precision while parameter-based
techniques increase complexity and reduce interpretability. To address these
constraints, we propose EmbedGrad, a novel framework that optimizes text prompt
embeddings through gradient-based refinement. Our approach uniquely decouples
training from deployment:during optimization,labeled examples guide precise
embedding adjustments while preserving semantic meaning; during inference, only
optimized embeddings integrate with user queries. This enables fine-grained
calibration impossible in text space, such as enhancing the reasoning
capability of prompts like please reason step by step. Comprehensive
evaluations across mathematical reasoning, sentiment analysis, and causal
judgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning
prompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% on
mathematical problems. Consistent improvements were observed across model
scales (0.5B-14B) and all tasks, with particularly significant gains for
smaller models on complex problems like causal judgment. By bridging prompt
engineering and parameter efficiency without architectural changes, our work
establishes embedding refinement as a powerful new paradigm for task
adaptation.

</details>


### [43] [Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations](https://arxiv.org/abs/2508.03550)
*Peng Lai,Jianjie Zheng,Sijie Cheng,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: LAGER is a lightweight framework that improves LLM-as-a-Judge alignment by leveraging internal representations across different layers, achieving significant improvements on benchmarks and showing effectiveness in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Improving alignment with human preferences without complex prompts or fine-tuning remains challenging, and middle-to-upper layers encode more semantically relevant representations than the final layer.

Method: LAGER is a lightweight framework that enhances LLM-as-a-Judge alignment by aggregating cross-layer scoretoken logits and computing expected scores from a softmax-based distribution, while keeping the LLM backbone frozen.

Result: LAGER achieves up to 7.5% improvement over the best baseline on standard alignment benchmarks and matches or outperforms reasoning-based methods without reasoning steps.

Conclusion: LAGER achieves improvements of up to 7.5% over the best baseline across benchmarks and shows effectiveness in downstream applications.

Abstract: The growing scale of evaluation tasks has led to the widespread adoption of
automated evaluation using large language models, a paradigm known as
"LLMas-a-judge." However, improving its alignment with human preferences
without complex prompts or fine-tuning remains challenging. In this work,
motivated by preliminary findings that middle-to-upper layers encode
semantically and taskrelevant representations that are often more aligned with
human judgments than the final layer, we propose LAGER, a lightweight and
efficient framework for enhancing LLM-as-a-Judge alignment with human scoring,
via internal representations. LAGER produces fine-grained judgment scores by
aggregating cross-layer scoretoken logits and computing the expected score from
a softmax-based distribution, with the LLM backbone kept frozen. LAGER fully
leverages the complementary information across different layers, overcoming the
limitations of relying solely on the final layer. We evaluate our method on the
standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman
correlation, and find that LAGER achieves improvements of up to 7.5% over the
best baseline across these benchmarks. Without reasoning steps, LAGER matches
or outperforms reasoning-based methods. Experiments on downstream applications,
such as data selection and emotional understanding, further show the
effectiveness of our method.

</details>


### [44] [Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation](https://arxiv.org/abs/2508.03571)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.CL

TL;DR: 本文提出了一种新的持续学习框架KILO，通过结合动态知识图谱和指令调优，提升了模型对新领域的适应能力和已有知识的保留能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在面对领域变化时性能会下降，主要是由于灾难性遗忘。

Method: KILO框架将动态知识图谱与指令调优相结合，在训练过程中利用检索到的特定领域知识作为指导。

Result: KILO在多个目标领域（包括BioASQ、SciQ、TweetEval和MIND）的实验结果表明，它在反向迁移、正向迁移、F1分数、保留率和训练效率方面均优于强基线模型。

Conclusion: KILO在持续学习场景中通过结合结构化知识检索和指令提示，有效克服了领域偏移挑战。

Abstract: Large Language Models (LLMs) often suffer from performance degradation when
faced with domain shifts, primarily due to catastrophic forgetting. In this
work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),
a novel continual learning framework that integrates dynamic knowledge graphs
with instruction tuning. By leveraging retrieved domain-specific knowledge as
guidance during training, KILO enhances both adaptability to new domains and
retention of previously acquired knowledge. We pretrain our model on
WikiText-103 and evaluate sequential adaptation across four diverse target
domains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that
KILO consistently outperforms strong baselines, including continual
fine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward
transfer, F1 score, retention rate, and training efficiency. These results
highlight the effectiveness of combining structured knowledge retrieval and
instruction prompting to overcome domain shift challenges in continual learning
scenarios.

</details>


### [45] [Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?](https://arxiv.org/abs/2508.03644)
*Wenxuan Shen,Mingjia Wang,Yaochen Wang,Dongping Chen,Junjie Yang,Yao Wan,Weiwei Lin*

Main category: cs.CL

TL;DR: 本文介绍了Double-Bench，一个大规模、多语言和多模态的评估系统，用于评估文档RAG系统。实验结果显示了文本和视觉嵌入模型之间的差距正在缩小，并揭示了当前框架中的过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试往往只关注文档RAG系统的特定部分，并使用合成数据，这些数据缺乏完整的地面真实和证据标签，因此无法反映现实世界中的瓶颈和挑战。

Method: 我们引入了Double-Bench，这是一个大规模、多语言和多模态的评估系统，能够对文档RAG系统中的每个组件进行细粒度评估。

Result: 我们的实验表明，文本和视觉嵌入模型之间的差距正在缩小，强调了构建更强文档检索模型的必要性。此外，我们的研究还揭示了当前文档RAG框架中存在的过度自信问题，即在没有证据支持的情况下提供答案。

Conclusion: 我们的研究展示了Double-Bench作为一个全面评估系统的重要性，能够为未来高级文档RAG系统的研究提供坚实的基础。我们计划定期获取新的语料库并每年发布新的基准。

Abstract: Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language
Models (MLLMs) show great promise for complex document understanding, yet their
development is critically hampered by inadequate evaluation. Current benchmarks
often focus on specific part of document RAG system and use synthetic data with
incomplete ground truth and evidence labels, therefore failing to reflect
real-world bottlenecks and challenges. To overcome these limitations, we
introduce Double-Bench: a new large-scale, multilingual, and multimodal
evaluation system that is able to produce fine-grained assessment to each
component within document RAG systems. It comprises 3,276 documents (72,880
pages) and 5,168 single- and multi-hop queries across 6 languages and 4
document types with streamlined dynamic update support for potential data
contamination issues. Queries are grounded in exhaustively scanned evidence
pages and verified by human experts to ensure maximum quality and completeness.
Our comprehensive experiments across 9 state-of-the-art embedding models, 4
MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text
and visual embedding models is narrowing, highlighting the need in building
stronger document retrieval models. Our findings also reveal the
over-confidence dilemma within current document RAG frameworks that tend to
provide answer even without evidence support. We hope our fully open-source
Double-Bench provide a rigorous foundation for future research in advanced
document RAG systems. We plan to retrieve timely corpus and release new
benchmarks on an annual basis.

</details>


### [46] [Can Large Vision-Language Models Understand Multimodal Sarcasm?](https://arxiv.org/abs/2508.03654)
*Xinyu Wang,Yue Zhang,Liqiang Jing*

Main category: cs.CL

TL;DR: 本文评估了大型视觉语言模型在多模态讽刺分析任务中的应用，并提出了一种无需训练的框架，以提高模型在多模态上下文中解释和理解讽刺的能力。


<details>
  <summary>Details</summary>
Motivation: 讽刺是一种复杂的语言现象，涉及字面意义和意图之间的差异，使得情感分析和其他情绪敏感任务变得具有挑战性。虽然传统的方法主要集中在文本上，但最近的方法已经结合了多模态信息。然而，大型视觉语言模型（LVLMs）在多模态讽刺分析（MSA）中的应用仍研究不足。

Method: 本文评估了大型视觉语言模型（LVLMs）在多模态讽刺分析（MSA）任务中的应用，特别是多模态讽刺检测和多模态讽刺解释。为了克服这些限制，我们提出了一种无需训练的框架，该框架集成了深入的对象提取和外部概念知识。

Result: 通过全面的实验，我们确定了关键的局限性，例如视觉理解不足和缺乏概念知识。实验结果表明，我们的框架在多个模型上都有效。

Conclusion: 本文提出了一个无需训练的框架，通过集成深入的对象提取和外部概念知识来提高模型在多模态上下文中解释和理解讽刺的能力。实验结果表明了所提出框架的有效性。

Abstract: Sarcasm is a complex linguistic phenomenon that involves a disparity between
literal and intended meanings, making it challenging for sentiment analysis and
other emotion-sensitive tasks. While traditional sarcasm detection methods
primarily focus on text, recent approaches have incorporated multimodal
information. However, the application of Large Visual Language Models (LVLMs)
in Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we
evaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm
Detection and Multimodal Sarcasm Explanation. Through comprehensive
experiments, we identify key limitations, such as insufficient visual
understanding and a lack of conceptual knowledge. To address these issues, we
propose a training-free framework that integrates in-depth object extraction
and external conceptual knowledge to improve the model's ability to interpret
and explain sarcasm in multimodal contexts. The experimental results on
multiple models show the effectiveness of our proposed framework. The code is
available at https://github.com/cp-cp/LVLM-MSA.

</details>


### [47] [CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction](https://arxiv.org/abs/2508.03668)
*Zixuan Li,Binzong Geng,Jing Xiong,Yong He,Yuxuan Hu,Jian Chen,Dingwei Chen,Xiyu Chang,Liang Zhang,Linjian Mo,Chengming Li,Chuan Yuan,Zhenan Sun*

Main category: cs.CL

TL;DR: 本文提出CTR-Sink框架，通过引入行为级注意力陷阱解决用户行为序列与语言模型预训练语料之间的语义碎片问题，提升点击率预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CTR预测方法在处理用户行为序列时存在语义碎片问题，导致语言模型无法准确关注有意义的行为边界和行为间关系，从而影响预测性能。

Method: CTR-Sink框架通过在连续行为之间插入具有推荐特定信号的sink tokens，并设计了两阶段训练策略和注意力陷阱机制，以增强行为间的相关性捕捉能力。

Result: 实验结果表明，CTR-Sink框架在工业数据集和两个开源数据集（MovieLens, Kuairec）上均表现出色，验证了其有效性。

Conclusion: CTR-Sink框架通过引入行为级注意力陷阱，有效解决了用户行为序列与语言模型预训练语料之间的语义碎片问题，显著提升了点击率预测性能。

Abstract: Click-Through Rate (CTR) prediction, a core task in recommendation systems,
estimates user click likelihood using historical behavioral data. Modeling user
behavior sequences as text to leverage Language Models (LMs) for this task has
gained traction, owing to LMs' strong semantic understanding and contextual
modeling capabilities. However, a critical structural gap exists: user behavior
sequences consist of discrete actions connected by semantically empty
separators, differing fundamentally from the coherent natural language in LM
pre-training. This mismatch causes semantic fragmentation, where LM attention
scatters across irrelevant tokens instead of focusing on meaningful behavior
boundaries and inter-behavior relationships, degrading prediction performance.
To address this, we propose $\textit{CTR-Sink}$, a novel framework introducing
behavior-level attention sinks tailored for recommendation scenarios. Inspired
by attention sink theory, it constructs attention focus sinks and dynamically
regulates attention aggregation via external information. Specifically, we
insert sink tokens between consecutive behaviors, incorporating
recommendation-specific signals such as temporal distance to serve as stable
attention sinks. To enhance generality, we design a two-stage training strategy
that explicitly guides LM attention toward sink tokens and a attention sink
mechanism that amplifies inter-sink dependencies to better capture behavioral
correlations. Experiments on one industrial dataset and two open-source
datasets (MovieLens, Kuairec), alongside visualization results, validate the
method's effectiveness across scenarios.

</details>


### [48] [FairLangProc: A Python package for fairness in NLP](https://arxiv.org/abs/2508.03677)
*Arturo Pérez-Peralta,Sandra Benítez-Peña,Rosa E. Lillo*

Main category: cs.CL

TL;DR: 本文介绍了一个名为FairLangProc的Python包，该包提供了自然语言处理中一些最近的公平性进展的共同实现，并与Hugging Face transformers库兼容，旨在促进偏差缓解技术的广泛应用和民主化。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在决策场景中的应用引起了社会关注，因此需要对这些模型在关键设置中的公平性进行研究，并开发不同的程序来解决自然语言处理中的偏见问题。

Method: 本文提出了一种名为FairLangProc的Python包，该包提供了自然语言处理中一些最近的公平性进展的共同实现，并与著名的Hugging Face transformers库兼容。

Result: 本文提出了FairLangProc，这是一个全面的Python包，提供了一些最近在自然语言处理中的公平性进展的共同实现，并与著名的Hugging Face transformers库兼容，旨在促进偏差缓解技术的广泛应用和民主化。

Conclusion: 本文介绍了FairLangProc，这是一个全面的Python包，提供了一些最近在自然语言处理中的公平性进展的共同实现，并旨在促进偏差缓解技术的广泛应用和民主化。

Abstract: The rise in usage of Large Language Models to near ubiquitousness in recent
years has risen societal concern about their applications in decision-making
contexts, such as organizational justice or healthcare. This, in turn, poses
questions about the fairness of these models in critical settings, which leads
to the developement of different procedures to address bias in Natural Language
Processing. Although many datasets, metrics and algorithms have been proposed
to measure and mitigate harmful prejudice in Natural Language Processing, their
implementation is diverse and far from centralized. As a response, this paper
presents FairLangProc, a comprehensive Python package providing a common
implementation of some of the more recent advances in fairness in Natural
Language Processing providing an interface compatible with the famous Hugging
Face transformers library, aiming to encourage the widespread use and
democratization of bias mitigation techniques. The implementation can be found
on https://github.com/arturo-perez-peralta/FairLangProc.

</details>


### [49] [More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation](https://arxiv.org/abs/2508.03678)
*Yangtian Zi,Harshitha Menon,Arjun Guha*

Main category: cs.CL

TL;DR: 研究通过引入PartialOrderEval评估提示详细程度对代码生成任务的影响，发现提示的详细程度对性能有显著影响，尤其是显式I/O规范、边缘情况处理和步骤分解是关键因素。


<details>
  <summary>Details</summary>
Motivation: 为了确定当前最先进的大型语言模型在专门基准测试中表现不佳的原因，是由于缺乏领域知识还是提示信息不足。

Method: 引入PartialOrderEval，通过在代码生成基准测试中添加从最小到最大详细的提示部分顺序，来评估提示特定性如何影响pass@1指标。

Result: 实验结果表明，不同任务对提示的敏感度不同，且提示的详细程度对性能有显著影响。

Conclusion: 研究发现，提示的详细程度对代码生成任务的性能有显著影响，特别是显式的输入输出规范、边缘情况处理和逐步分解是提升提示质量的关键因素。

Abstract: State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general
benchmarks like HumanEval but underperform on specialized suites such as
ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt
detail is given? To answer this, we introduce PartialOrderEval, which augments
any code generation benchmark with a partial order of prompts from minimal to
maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets
of ParEval, we measure how pass@1 scales with prompt specificity. Our
experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of
prompt sensitivity across different tasks, and a qualitative analysis
highlights explicit I/O specifications, edge-case handling, and stepwise
breakdowns as the key drivers of prompt detail improvement.

</details>


### [50] [CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward](https://arxiv.org/abs/2508.03686)
*Shudong Liu,Hongwei Liu,Junnan Liu,Linchen Xiao,Songyang Gao,Chengqi Lyu,Yuzhe Gu,Wenwei Zhang,Derek F. Wong,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文开发了一个名为CompassVerifier的轻量级验证器模型，并创建了一个名为VerifierBench的基准测试，以提高答案验证和评估的准确性与通用性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在验证能力评估和处理复杂边缘案例方面存在不足，需要更全面的基准测试和更稳健的验证器模型。

Method: 本文提出了CompassVerifier，一个轻量级验证器模型，并引入了VerifierBench基准测试，通过手动分析元错误模式来增强CompassVerifier。

Result: CompassVerifier展示了跨数学、知识和多样化推理任务的多领域能力，能够处理各种答案类型，并有效识别异常/无效响应。

Conclusion: 本文开发了CompassVerifier，一个准确且稳健的轻量级验证器模型，用于评估和结果奖励。我们预计CompassVerifier和VerifierBench将促进答案验证、评估协议和强化学习研究。

Abstract: Answer verification is crucial not only for evaluating large language models
(LLMs) by matching their unstructured outputs against standard answers, but
also serves as the reward model to guide LLM optimization. Most evaluation
frameworks rely on regularized matching or employ general LLMs for answer
verification, which demands extensive, repetitive customization for regex rules
or evaluation prompts. Two fundamental limitations persist in current
methodologies: 1) the absence of comprehensive benchmarks that systematically
evaluate verification capabilities across different LLMs; and 2) the nascent
stage of verifier development, where existing approaches lack both the
robustness to handle complex edge cases and the generalizability across
different domains. In this work, we develop CompassVerifier, an accurate and
robust lightweight verifier model for evaluation and outcome reward. It
demonstrates multi-domain competency spanning math, knowledge, and diverse
reasoning tasks, with the capability to process various answer types, including
multi-subproblems, formulas, and sequence answers, while effectively
identifying abnormal/invalid responses. We introduce VerifierBench benchmark
comprising model outputs collected from multiple data sources, augmented
through manual analysis of metaerror patterns to enhance CompassVerifier. We
anticipate that CompassVerifier and VerifierBench will facilitate answer
verification, evaluation protocols, and reinforcement learning research. Code
and dataset are available at https://github.com/open-compass/CompassVerifier.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [51] [CreditARF: A Framework for Corporate Credit Rating with Annual Report and Financial Feature Integration](https://arxiv.org/abs/2508.02738)
*Yumeng Shi,Zhongliang Yang,DiYang Lu,Yisi Wang,Yiting Zhou,Linna Zhou*

Main category: q-fin.ST

TL;DR: 本文提出了一种结合财务数据和年度报告文本信息的企业信用评级框架，并构建了一个大规模数据集，实验结果显示该方法显著提高了评级准确性。


<details>
  <summary>Details</summary>
Motivation: 现有信用评级模型通常忽视非财务数据（如企业年度报告）中的洞察，因此需要一种更全面的方法。

Method: 本文引入了一个结合财务数据和从年度报告中提取特征的信用评级框架，使用FinBERT来处理非结构化文本数据。

Result: 实验结果表明，所提出的方法将评级预测的准确性提高了8-12%。

Conclusion: 本文提出的框架能够显著提高企业信用评级的准确性和可靠性。

Abstract: Corporate credit rating serves as a crucial intermediary service in the
market economy, playing a key role in maintaining economic order. Existing
credit rating models rely on financial metrics and deep learning. However, they
often overlook insights from non-financial data, such as corporate annual
reports. To address this, this paper introduces a corporate credit rating
framework that integrates financial data with features extracted from annual
reports using FinBERT, aiming to fully leverage the potential value of
unstructured text data. In addition, we have developed a large-scale dataset,
the Comprehensive Corporate Rating Dataset (CCRD), which combines both
traditional financial data and textual data from annual reports. The
experimental results show that the proposed method improves the accuracy of the
rating predictions by 8-12%, significantly improving the effectiveness and
reliability of corporate credit ratings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision](https://arxiv.org/abs/2508.03058)
*Dingwei Zhu,Shihan Dou,Zhiheng Xi,Senjie Jin,Guoqiang Zhang,Jiazheng Zhang,Junjie Ye,Mingxu Chai,Enyu Zhou,Ming Zhang,Caishuang Huang,Yunke Zhang,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: 本文提出了一种基于价值的框架VRPO，用于在噪声监督下进行鲁棒的PPO训练。VRPO通过增强价值模型过滤噪声和捕捉关键词的能力，提高了策略优化的稳定性。实验表明，VRPO在多个任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RLHF在现实世界设置中经常受到噪声或不完善的奖励监督的影响，这会损害策略的稳定性和泛化能力。现有的工作往往忽略了价值模型在策略优化中的关键作用。

Method: 我们提出了VRPO，这是一种基于价值的框架，用于在噪声监督下进行鲁棒的PPO训练。VRPO结合了两个核心设计：(1) 由冻结语言模型的熵和困惑度引导的辅助损失，以及(2) 变分信息瓶颈。

Result: 在数学推理、科学问答和多轮对话任务中，VRPO在基于规则和基于模型的噪声奖励下均优于PPO和GRPO基线。

Conclusion: 我们的研究强调了价值模型在RLHF中的重要性，并提供了一种原则性和实用性的方法，以在嘈杂的真实世界环境中进行稳健的策略优化。

Abstract: Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or
imperfect reward supervision in real-world settings, which undermines policy
stability and generalization. Such noise may cause models to lose attention on
key words during advantage estimation. While prior work focuses on reward
denoising or filtering poor data, it often overlooks the critical role of the
value model in policy optimization. In this work, we show that a strong value
model is essential for mitigating noise by absorbing unstable signals and
enabling more reliable advantage estimation. We propose VRPO, a value-centric
framework for robust PPO training under noisy supervision. VRPO combines two
core designs: (1) an auxiliary loss guided by entropy and perplexity from a
frozen language model, and (2) a variational information bottleneck. These
mechanisms enhance the value model's ability to filter out noise and capture
key words from the context during advantage estimation, transforming it from a
passive predictor into an active regulator of noise. Experiments on math
reasoning, science QA, and multi-turn dialogue, under both rule-based and
model-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO
baselines. Our findings underscore the often-overlooked importance of the value
model in RLHF and offer a principled and practical approach to robust policy
optimization in noisy real-world environments.

</details>


### [53] [Understanding the Embedding Models on Hyper-relational Knowledge Graph](https://arxiv.org/abs/2508.03280)
*Yubo Wang,Shimin Di,Zhili Wang,Haoyang Li,Fei Teng,Hao Xin,Lei Chen*

Main category: cs.LG

TL;DR: 本文分析了HKGE模型的性能来源，并提出了一种新的框架FormerGNN，以更好地处理HKG中的信息压缩问题和长距离依赖关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨HKGE模型的优越性能是否来源于基础KGE模型或专门设计的扩展模块。

Method: 通过三种分解方法将HKG转换为KG格式，并评估了经典KGE模型的性能。此外，提出了FormerGNN框架，结合了信息整合器和基于GNN的图编码器。

Result: 一些KGE模型在HKG上的表现与HKGE模型相当。分解方法改变了原始HKG拓扑结构，未能完全保留HKG信息。当前的HKGE模型在捕捉长距离依赖关系和整合主三元组和限定信息方面存在不足。

Conclusion: FormerGNN在实验中表现出色，优于现有的HKGE模型。

Abstract: Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as an
extension of traditional Knowledge Graphs (KGs) to better represent real-world
facts with additional qualifiers. As a result, researchers have attempted to
adapt classical Knowledge Graph Embedding (KGE) models for HKGs by designing
extra qualifier processing modules. However, it remains unclear whether the
superior performance of Hyper-relational KGE (HKGE) models arises from their
base KGE model or the specially designed extension module. Hence, in this
paper, we data-wise convert HKGs to KG format using three decomposition methods
and then evaluate the performance of several classical KGE models on HKGs. Our
results show that some KGE models achieve performance comparable to that of
HKGE models. Upon further analysis, we find that the decomposition methods
alter the original HKG topology and fail to fully preserve HKG information.
Moreover, we observe that current HKGE models are either insufficient in
capturing the graph's long-range dependency or struggle to integrate
main-triple and qualifier information due to the information compression issue.
To further justify our findings and offer a potential direction for future HKGE
research, we propose the FormerGNN framework. This framework employs a
qualifier integrator to preserve the original HKG topology, and a GNN-based
graph encoder to capture the graph's long-range dependencies, followed by an
improved approach for integrating main-triple and qualifier information to
mitigate compression issues. Our experimental results demonstrate that
FormerGNN outperforms existing HKGE models.

</details>


### [54] [Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03501)
*Alexander Golubev,Maria Trofimova,Sergei Polezhaev,Ibragim Badertdinov,Maksim Nekrashevich,Anton Shevtsov,Simon Karasik,Sergey Abramov,Andrei Andriushchenko,Filipp Fisin,Sergei Skvortsov,Boris Yangel*

Main category: cs.LG

TL;DR: 本文通过改进的强化学习算法，提高了大型语言模型在现实世界软件工程任务中的成功率，展示了其作为自主代理的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在单次交互问题上，而现实世界的应用需要丰富的多轮交互和状态环境反馈。

Method: 使用修改后的解耦优势策略优化（DAPO）算法，训练基于Qwen2.5-72B-Instruct的代理来解决现实世界的软件工程任务。

Result: 在SWE-bench Verified基准测试中，代理的成功率从20%提升至39%，在SWE-rebench上表现优于或与领先开源模型相当。

Conclusion: 本文展示了将强化学习应用于现实世界软件工程任务的成功案例，为基于开放模型构建更强大的自主代理提供了可行路径。

Abstract: Research on applications of Reinforcement Learning (RL) to Large Language
Models (LLMs) has mostly been focused on single-turn problems, such as
mathematical reasoning or single-shot code generation. While these problems can
be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate
case of multi-turn interaction where the environment provides no feedback. This
contrasts with many real-world domains, such as software engineering (SWE),
which require rich multi-turn interactions with a stateful environment that
responds to each action with a non-trivial observation.
  To bridge this gap, we demonstrate the successful application of RL to this
general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)
algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world
software engineering tasks. Our approach increases the agent's success rate on
the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to
39%, without relying on any teacher models. On SWE-rebench, our agent matches
or outperforms leading open-weight models such as DeepSeek-V3-0324 and
Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward
building more capable autonomous agents for complex real-world problems based
on open models.

</details>


### [55] [MoKA: Mixture of Kronecker Adapters](https://arxiv.org/abs/2508.03527)
*Mohammadreza Sadeghi,Mahsa Ghazvini Nejad,MirHamed Jafarzadeh Asl,Yu Gu,Yuanhao Yu,Masoud Asgharian,Vahid Partovi Nia*

Main category: cs.LG

TL;DR: 本文提出了一种新的Kronecker适配器Mixture of Kronecker Adapters (MoKA)，通过将权重更新建模为Kronecker积的混合来解决低秩适配器表达能力有限的问题。MoKA利用门控机制测量每个Kronecker因子的重要性，从而实现更丰富的适应性，并提供了参数效率和准确性之间的更好权衡。此外，MoKA通过使用标准矩阵运算重新表述Kronecker计算，确保了硬件效率。实验表明，MoKA不仅优于PEFT基线，还减少了多达27倍的可训练参数，实现了性能和参数效率之间的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: Parameter-efficient fine-tuning (PEFT) is essential for reducing the computational overhead of large language models (LLMs). Low-rank family adapters are commonly used to control the parameter size efficiently while maintaining the generative power of LLMs. However, their limited expressiveness due to the rank constraint often restricts their performance on complex tasks.

Method: We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker adapters that addresses this limitation by modeling weight updates as a mixture of Kronecker products. Our proposed adapter leverages a gating mechanism that measures the importance of each Kronecker factor, enabling more expressive adaptation. Moreover, MoKA enables a rank flexibility that provides a better trade-off between parameter efficiency and accuracy. To ensure hardware efficiency, we reformulate Kronecker computations using standard matrix operations, allowing seamless deployment on GPU-optimized hardware.

Result: We conduct extensive experiments on instruction-tuning and commonsense reasoning tasks using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency.

Conclusion: MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency.

Abstract: Parameter-efficient fine-tuning (PEFT) is essential for reducing the
computational overhead of large language models (LLMs). Low-rank family
adapters are commonly used to control the parameter size efficiently while
maintaining the generative power of LLMs. However, their limited expressiveness
due to the rank constraint often restricts their performance on complex tasks.
We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker
adapters that addresses this limitation by modeling weight updates as a mixture
of Kronecker products. Our proposed adapter leverages a gating mechanism that
measures the importance of each Kronecker factor, enabling more expressive
adaptation. Moreover, MoKA enables a rank flexibility that provides a better
trade-off between parameter efficiency and accuracy. To ensure hardware
efficiency, we reformulate Kronecker computations using standard matrix
operations, allowing seamless deployment on GPU-optimized hardware. We conduct
extensive experiments on instruction-tuning and commonsense reasoning tasks
using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not
only outperforms PEFT baselines, but also reduces the number of trainable
parameters up to 27x, achieving state-of-the-art trade-offs between performance
and parameter efficiency.

</details>


### [56] [Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation](https://arxiv.org/abs/2508.03663)
*Deepak Pandita,Flip Korn,Chris Welty,Christopher M. Homan*

Main category: cs.LG

TL;DR: 本文研究了在固定预算下，机器学习评估中项目数量（N）和每个项目响应数量（K）之间的权衡，以可靠地比较机器学习模型的性能。研究发现，考虑到人类分歧的情况下，每个数据集所需的N×K通常不超过1000，且K大于10时效果更好。评估指标的性质影响了K和N之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 可重复性是科学验证的核心，也是其结果所赋予的权威性。在机器学习评估中，可重复性可以增加信任、信心和价值。然而，机器学习中使用的地面真实响应通常来自人类，其中存在普遍的分歧，而且很少有研究探讨有效忽略这些响应中的分歧的影响，这通常是情况。缺乏研究的一个原因是收集人工标注的评估数据的预算有限，而为每个示例获取更多样本的多个标注者会大大增加每个项目的标注成本。

Method: 我们分析了一个多样化的类别数据集集合，这些数据集每个项目都有多个注释，并模拟了适合这些数据集的分布，以确定在固定预算（N×K）下收集评估数据并可靠比较机器学习模型性能的最佳(N, K)配置。

Result: 我们的研究结果表明，考虑到人类的分歧可能在每个数据集上至少在一个指标上最多需要1000的N×K（通常更低），并且这种最小的N×K几乎总是发生在K>10的情况下。此外，K和N之间的权衡性质——或者是否存在这种权衡——取决于评估指标，对于更敏感于响应完整分布的指标，在更高的K水平下表现更好。

Conclusion: 我们的研究结果表明，考虑到人类的分歧可能在每个数据集上至少在一个指标上最多需要1000的N×K（通常更低），并且这种最小的N×K几乎总是发生在K>10的情况下。此外，K和N之间的权衡性质——或者是否存在这种权衡——取决于评估指标，对于更敏感于响应完整分布的指标，在更高的K水平下表现更好。我们的方法可以帮助ML从业者通过找到最佳指标和要收集的项目数量以及每个项目的注释数量来获得更有效的测试数据，以在他们的预算内获得最大的可靠性。

Abstract: Reproducibility is a cornerstone of scientific validation and of the
authority it confers on its results. Reproducibility in machine learning
evaluations leads to greater trust, confidence, and value. However, the ground
truth responses used in machine learning often necessarily come from humans,
among whom disagreement is prevalent, and surprisingly little research has
studied the impact of effectively ignoring disagreement in these responses, as
is typically the case. One reason for the lack of research is that budgets for
collecting human-annotated evaluation data are limited, and obtaining more
samples from multiple annotators for each example greatly increases the
per-item annotation costs. We investigate the trade-off between the number of
items ($N$) and the number of responses per item ($K$) needed for reliable
machine learning evaluation. We analyze a diverse collection of categorical
datasets for which multiple annotations per item exist, and simulated
distributions fit to these datasets, to determine the optimal $(N, K)$
configuration, given a fixed budget ($N \times K$), for collecting evaluation
data and reliably comparing the performance of machine learning models. Our
findings show, first, that accounting for human disagreement may come with $N
\times K$ at no more than 1000 (and often much lower) for every dataset tested
on at least one metric. Moreover, this minimal $N \times K$ almost always
occurred for $K > 10$. Furthermore, the nature of the tradeoff between $K$ and
$N$ -- or if one even existed -- depends on the evaluation metric, with metrics
that are more sensitive to the full distribution of responses performing better
at higher levels of $K$. Our methods can be used to help ML practitioners get
more effective test data by finding the optimal metrics and number of items and
annotations per item to collect to get the most reliability for their budget.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [57] [SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec](https://arxiv.org/abs/2508.02849)
*Chunyu Qiang,Haoyu Wang,Cheng Gong,Tianrui Wang,Ruibo Fu,Tao Wang,Ruilong Chen,Jiangyan Yi,Zhengqi Wen,Chen Zhang,Longbiao Wang,Jianwu Dang,Jianhua Tao*

Main category: eess.AS

TL;DR: 本文提出了 SecoustiCodec，这是一种跨模态对齐的低比特率流式语音编解码器，通过分离语义和非语言信息，提高语义完整性和重建质量，并实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音编解码方法面临语义编码中的多个挑战，如残留的非语言信息、语义不完整、重建能力有限和缺乏流支持等问题。为了克服这些挑战，需要一种更高效的语音编解码器。

Method: SecoustiCodec 通过单代码本空间分离语义和非语言信息，并引入非语言编码来填补语义和声学编码之间的信息差距。提出了一种基于 VAE 和 FSQ 的语义高效量化方法，以及一种基于对比学习的语义解耦方法，以在联合多模态帧级空间中对齐文本和语音，从而从语义编码中去除非语言信息。此外，还提出了一种声学约束的多阶段优化策略以确保鲁棒和稳定的收敛。

Result: SecoustiCodec 在 0.27/1 kbps 的比特率下实现了 SOTA（最先进）的重建质量（PESQ）为 1.77/2.58。

Conclusion: SecoustiCodec 是一种跨模态对齐的低比特率流式语音编解码器，能够有效解决现有编解码方法在语义编码中的挑战，如残留的非语言信息、语义不完整、重建能力有限和缺乏流支持等问题。

Abstract: Speech codecs serve as a crucial bridge in unifying speech and text language
models. Existing codec methods face several challenges in semantic encoding,
such as residual paralinguistic information (e.g., timbre, emotion),
insufficient semantic completeness, limited reconstruction capability, and lack
of support for streaming. To address these challenges, we propose
SecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec that
disentangles semantic and paralinguistic information in a single-codebook
space. To ensure semantic completeness and reconstruction fidelity,
paralinguistic encoding is introduced to bridge the information gap between
semantic and acoustic encoding. A semantic-only efficient quantization method
based on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) is
proposed. This approach alleviates the long-tail distribution problem of tokens
while maintaining high codebook utilization. A semantic disentanglement method
based on contrastive learning is proposed, which aligns text and speech in a
joint multimodal frame-level space, effectively removing paralinguistic
information from semantic encoding. An acoustic-constrained multi-stage
optimization strategy is proposed to ensure robust and stable convergence.
Figure~\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA
(state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps.
The code and model weights for SecoustiCodec will be open-sourced upon the
completion of the peer-review process. We've open-sourced SecoustiCodec's demo,
code, and model weights.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [58] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: 本文介绍了一个名为Toolregistry的工具管理库，它通过统一接口简化了工具的注册、表示、执行和生命周期管理。评估表明，Toolregistry在减少代码量、提高性能和兼容性方面表现优异，并在实际应用中提升了开发效率和代码可维护性。


<details>
  <summary>Details</summary>
Motivation: 当前的工具集成方法存在碎片化、协议限制和实现复杂性等问题，导致开发成本高。因此，需要一种更高效、统一的工具管理方案。

Method: 本文介绍了Toolregistry，一个协议无关的工具管理库，旨在简化工具的注册、表示、执行和生命周期管理。

Result: 评估结果显示，Toolregistry实现了60-80%的工具集成代码减少，最多3.1倍的性能提升，并且完全兼容OpenAI函数调用标准。实际案例研究显示了开发效率和代码可维护性的显著改进。

Conclusion: 本文提出了Toolregistry，一个协议无关的工具管理库，通过统一接口简化了工具注册、表示、执行和生命周期管理。评估表明，Toolregistry在减少工具集成代码、提高性能和兼容性方面表现出色，并在实际案例中展示了开发效率和代码可维护性的显著提升。

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [Efficient Agents: Building Effective Agents While Reducing Cost](https://arxiv.org/abs/2508.02694)
*Ningning Wang,Xavier Hu,Pai Liu,He Zhu,Yue Hou,Heyuan Huang,Shengyu Zhang,Jian Yang,Jiaheng Liu,Ge Zhang,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型驱动代理系统的效率与效果权衡，提出了一种高效的代理框架Efficient Agents，显著降低了成本并保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究现代代理系统中的效率与效果权衡，解决在不牺牲性能的情况下实现成本效益设计的关键需求。

Method: 本文通过在GAIA基准上的实证分析，评估了LLM骨干选择、代理框架设计和测试时扩展策略的影响，并使用pass成本度量来量化效率-性能权衡。

Result: 本文提出了Efficient Agents，这是一种新型代理框架，在保留OWL（一个领先的开源代理框架）96.7%性能的同时，将运营成本从0.398美元降低到0.228美元，实现了pass成本的28.4%改进。

Conclusion: 本文提供了关于如何设计高效且高性能的代理系统的实用见解，推动了AI驱动解决方案的可访问性和可持续性。

Abstract: The remarkable capabilities of Large Language Model (LLM)-driven agents have
enabled sophisticated systems to tackle complex, multi-step tasks, but their
escalating costs threaten scalability and accessibility. This work presents the
first systematic study of the efficiency-effectiveness trade-off in modern
agent systems, addressing the critical need for cost-effective designs without
sacrificing performance. We investigate three key questions: (1) How much
complexity do agentic tasks inherently require? (2) When do additional modules
yield diminishing returns? (3) How much efficiency can be gained through the
design of efficient agent frameworks? Through an empirical analysis on the GAIA
benchmark, we evaluate the impact of LLM backbone selection, agent framework
designs, and test-time scaling strategies. Using the cost-of-pass metric, we
quantify the efficiency-performance trade-off across these dimensions. Our
findings inform the development of Efficient Agents , a novel agent framework
that has an optimal complexity to task requirements. Efficient Agents retains
96.7% of the performance of OWL, one leading open-source agent framework, while
reducing operational costs from $0.398 to $0.228, resulting in a 28.4%
improvement in cost-of-pass. Our work provides actionable insights for
designing efficient, high-performing agent systems, advancing the accessibility
and sustainability of AI-driven solutions.

</details>


### [60] [Defend LLMs Through Self-Consciousness](https://arxiv.org/abs/2508.02961)
*Boshi Huang,Fabio Nonato de Paula*

Main category: cs.AI

TL;DR: 本文介绍了一种基于自我意识的LLM防御机制，通过元认知和仲裁模块实现自我保护，有效提升了防御成功率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于外部分类器的方法在防御提示注入攻击方面存在局限性，因此需要一种更有效、更轻量级的解决方案。

Method: 本文提出了一种框架，结合了元认知和仲裁模块，使LLMs能够自主评估和调节其输出。

Result: 实验结果表明，该方法在多个LLMs和数据集上显著提高了防御成功率，某些模型在增强模式下实现了完美的防御效果。

Conclusion: 本文提出了一种新颖的自我意识防御机制，用于大型语言模型（LLMs）抵御提示注入攻击。该方法利用LLM本身的推理能力进行自我保护，相较于传统方法更为高效且成本更低。

Abstract: This paper introduces a novel self-consciousness defense mechanism for Large
Language Models (LLMs) to combat prompt injection attacks. Unlike traditional
approaches that rely on external classifiers, our method leverages the LLM's
inherent reasoning capabilities to perform self-protection. We propose a
framework that incorporates Meta-Cognitive and Arbitration Modules, enabling
LLMs to evaluate and regulate their own outputs autonomously. Our approach is
evaluated on seven state-of-the-art LLMs using two datasets: AdvBench and
Prompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate
significant improvements in defense success rates across models and datasets,
with some achieving perfect and near-perfect defense in Enhanced Mode. We also
analyze the trade-off between defense success rate improvement and
computational overhead. This self-consciousness method offers a lightweight,
cost-effective solution for enhancing LLM ethics, particularly beneficial for
GenAI use cases across various platforms.

</details>


### [61] [Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling](https://arxiv.org/abs/2508.02979)
*Peng Ding,Rick Stevens*

Main category: cs.AI

TL;DR: 本文提出了一种统一的工具集成方法，通过自动模式生成、双模式并发执行和无缝多源工具管理来减少开发开销，并展示了其在代码减少、性能提升和兼容性方面的显著成果。


<details>
  <summary>Details</summary>
Motivation: 由于工具增强型大型语言模型（LLMs）的普及，开发者必须处理多种协议、手动模式定义和复杂的执行工作流程，因此需要一种统一的方法来简化这一过程。

Method: 本文提出了一种统一的工具集成方法，通过自动模式生成、双模式并发执行和无缝多源工具管理来减少开发开销。

Result: 实验结果表明，在集成场景中代码减少了60-80%，通过优化并发性能提升了3.1倍，并且完全兼容现有的函数调用标准。

Conclusion: 本文贡献了关于工具集成架构的理论见解和实际解决方案，以应对工具增强型大型语言模型（LLMs）带来的碎片化生态系统问题。

Abstract: The proliferation of tool-augmented Large Language Models (LLMs) has created
a fragmented ecosystem where developers must navigate multiple protocols,
manual schema definitions, and complex execution workflows. We address this
challenge by proposing a unified approach to tool integration that abstracts
protocol differences while optimizing execution performance. Our solution
demonstrates how protocol-agnostic design principles can significantly reduce
development overhead through automated schema generation, dual-mode concurrent
execution, and seamless multi-source tool management. Experimental results show
60-80% code reduction across integration scenarios, performance improvements up
to 3.1x through optimized concurrency, and full compatibility with existing
function calling standards. This work contributes both theoretical insights
into tool integration architecture and practical solutions for real-world LLM
application development.

</details>


### [62] [AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots](https://arxiv.org/abs/2508.02999)
*Xinjie Zhao,Moritz Blum,Fan Gao,Yingjian Chen,Boming Yang,Luis Marquez-Carpintero,Mónica Pina-Navarro,Yanran Fu,So Morikawa,Yusuke Iwasawa,Yutaka Matsuo,Chanjun Park,Irene Li*

Main category: cs.AI

TL;DR: AGENTiGraph是一种用户友好的代理驱动系统，允许非技术人员通过自然语言操作知识图谱来管理领域特定数据，具有高准确率和成功率，并展示出在法律和医疗领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 为非技术用户提供一种完整、可视化的解决方案，使其能够逐步构建和优化知识库，支持多轮对话和动态更新，而无需专门的查询语言。

Method: AGENTiGraph是一个基于代理的系统，通过自然语言操作知识图谱来实现与领域特定数据的直观交互和管理。它包括意图分类、任务规划和自动知识集成等灵活设计，以确保不同任务之间的无缝推理。

Result: 在教育场景中的3,500个查询基准测试中，该系统优于强大的零样本基线（达到95.12%的分类准确率，90.45%的执行成功率），显示出在合规关键或多步骤查询中的潜力。

Conclusion: AGENTiGraph展示了在法律和医疗领域潜在的可扩展性，能够处理合规关键或多步骤查询，并提供了一种新的多轮企业知识管理范式。

Abstract: AGENTiGraph is a user-friendly, agent-driven system that enables intuitive
interaction and management of domain-specific data through the manipulation of
knowledge graphs in natural language. It gives non-technical users a complete,
visual solution to incrementally build and refine their knowledge bases,
allowing multi-round dialogues and dynamic updates without specialized query
languages. The flexible design of AGENTiGraph, including intent classification,
task planning, and automatic knowledge integration, ensures seamless reasoning
between diverse tasks. Evaluated on a 3,500-query benchmark within an
educational scenario, the system outperforms strong zero-shot baselines
(achieving 95.12% classification accuracy, 90.45% execution success),
indicating potential scalability to compliance-critical or multi-step queries
in legal and medical domains, e.g., incorporating new statutes or research on
the fly. Our open-source demo offers a powerful new paradigm for multi-turn
enterprise knowledge management that bridges LLMs and structured graphs.

</details>


### [63] [Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework](https://arxiv.org/abs/2508.03092)
*Zikun Cui,Tianyi Huang,Chia-En Chiang,Cuiqianhe Du*

Main category: cs.AI

TL;DR: 本研究提出了一种创新的可验证虚假信息检测LLM代理，通过动态交互多种网络资源来验证声明，评估信息来源的可信度，综合证据并提供完整的可验证推理过程。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的普及，虚假信息的检测变得越来越重要和复杂。传统的真实/虚假二元判断已不足以应对这一挑战。

Method: 设计了一个包含三个核心工具的代理架构：精确网络搜索工具、来源可信度评估工具和数值声明验证工具，这些工具使代理能够执行多步骤验证策略，维护证据日志并形成全面评估结论。

Result: 实验结果表明，我们的代理在虚假信息检测准确性、推理透明度和对抗信息重写方面优于传统机器学习模型和LLMs。

Conclusion: 我们的代理在虚假信息检测准确性、推理透明度和对抗信息重写方面优于基线方法，为可信的AI辅助事实核查提供了新范式。

Abstract: With the proliferation of Large Language Models (LLMs), the detection of
misinformation has become increasingly important and complex. This research
proposes an innovative verifiable misinformation detection LLM agent that goes
beyond traditional true/false binary judgments. The agent actively verifies
claims through dynamic interaction with diverse web sources, assesses
information source credibility, synthesizes evidence, and provides a complete
verifiable reasoning process. Our designed agent architecture includes three
core tools: precise web search tool, source credibility assessment tool and
numerical claim verification tool. These tools enable the agent to execute
multi-step verification strategies, maintain evidence logs, and form
comprehensive assessment conclusions. We evaluate using standard misinformation
datasets such as FakeNewsNet, comparing with traditional machine learning
models and LLMs. Evaluation metrics include standard classification metrics,
quality assessment of reasoning processes, and robustness testing against
rewritten content. Experimental results show that our agent outperforms
baseline methods in misinformation detection accuracy, reasoning transparency,
and resistance to information rewriting, providing a new paradigm for
trustworthy AI-assisted fact-checking.

</details>


### [64] [A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning](https://arxiv.org/abs/2508.03366)
*Michael K. Chen*

Main category: cs.AI

TL;DR: 本文比较了集成方法和混合方法在通用逻辑推理中的表现，发现混合方法更具潜力，并提出了一种基于LLM-SS的通用框架。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）在逻辑推理任务上表现不佳，无法确定性和可解释性。因此，需要比较不同的神经符号AI方法以找到更优的解决方案。

Method: 通过引入两种代表性的模型（LNN和LLM-SS）作为案例研究，分析了集成方法和混合方法在通用逻辑推理上的表现。

Result: 混合方法在通用逻辑推理任务中表现出更好的性能，同时具备更高的可解释性和对现有LLM优势的保留。

Conclusion: 混合方法在开发通用逻辑推理方面更具前景，因为它具有更高的可解释性，并保留了现有LLM的优势。

Abstract: General logical reasoning, defined as the ability to reason deductively on
domain-agnostic tasks, continues to be a challenge for large language models
(LLMs). Current LLMs fail to reason deterministically and are not
interpretable. As such, there has been a recent surge in interest in
neurosymbolic AI, which attempts to incorporate logic into neural networks. We
first identify two main neurosymbolic approaches to improving logical
reasoning: (i) the integrative approach comprising models where symbolic
reasoning is contained within the neural network, and (ii) the hybrid approach
comprising models where a symbolic solver, separate from the neural network,
performs symbolic reasoning. Both contain AI systems with promising results on
domain-specific logical reasoning benchmarks. However, their performance on
domain-agnostic benchmarks is understudied. To the best of our knowledge, there
has not been a comparison of the contrasting approaches that answers the
following question: Which approach is more promising for developing general
logical reasoning? To analyze their potential, the following best-in-class
domain-agnostic models are introduced: Logic Neural Network (LNN), which uses
the integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the
hybrid approach. Using both models as case studies and representatives of each
approach, our analysis demonstrates that the hybrid approach is more promising
for developing general logical reasoning because (i) its reasoning chain is
more interpretable, and (ii) it retains the capabilities and advantages of
existing LLMs. To support future works using the hybrid approach, we propose a
generalizable framework based on LLM-SS that is modular by design,
model-agnostic, domain-agnostic, and requires little to no human input.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [65] [Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education](https://arxiv.org/abs/2508.02731)
*Jean-Francois Chamberland,Martin C. Carlisle,Arul Jayaraman,Krishna R. Narayanan,Sunay Palsole,Karan Watson*

Main category: cs.CY

TL;DR: 本文提出了一种基于AI的框架，用于大规模评估教学效果，通过定性反馈分析和可视化分析来支持教学改进。


<details>
  <summary>Details</summary>
Motivation: 评估大规模教学效果仍然是大型大学，特别是工程课程的一个持续挑战，这些课程有数万名学生。传统方法，如人工审查学生评价，往往不切实际，导致被忽视的见解和不一致的数据使用。

Method: 本文提出了一种可扩展的AI支持框架，用于使用大型语言模型合成定性学生反馈。该系统采用分层摘要、匿名化和异常处理来提取可操作的主题，同时保持伦理保障。可视化分析通过百分位比较、历史趋势和教学负荷来解释数值分数。

Result: 该框架已在一所大型工程学院成功部署。初步验证通过与人工评审的比较、教师反馈和纵向分析表明，LLM生成的摘要可以可靠地支持形成性评估和专业发展。

Conclusion: 本文展示了如何设计具有透明度和共享治理的AI系统，以在学术机构中促进教学卓越和持续改进。

Abstract: Evaluating teaching effectiveness at scale remains a persistent challenge for
large universities, particularly within engineering programs that enroll tens
of thousands of students. Traditional methods, such as manual review of student
evaluations, are often impractical, leading to overlooked insights and
inconsistent data use. This article presents a scalable, AI-supported framework
for synthesizing qualitative student feedback using large language models. The
system employs hierarchical summarization, anonymization, and exception
handling to extract actionable themes from open-ended comments while upholding
ethical safeguards. Visual analytics contextualize numeric scores through
percentile-based comparisons, historical trends, and instructional load. The
approach supports meaningful evaluation and aligns with best practices in
qualitative analysis and educational assessment, incorporating student, peer,
and self-reflective inputs without automating personnel decisions. We report on
its successful deployment across a large college of engineering. Preliminary
validation through comparisons with human reviewers, faculty feedback, and
longitudinal analysis suggests that LLM-generated summaries can reliably
support formative evaluation and professional development. This work
demonstrates how AI systems, when designed with transparency and shared
governance, can promote teaching excellence and continuous improvement at scale
within academic institutions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [66] [VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction](https://arxiv.org/abs/2508.02890)
*Rongxin Jiang,Robert Long,Chenghao Gu,Mingrui Yan*

Main category: cs.CV

TL;DR: 本文提出VisuCraft框架，以提高LVLMs在复杂视觉引导的创造性内容生成任务中的表现，特别在创造力和指令遵循方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在生成长文本时存在保持高视觉保真度、真实创造力和精确遵循细微用户指令的困难，因此需要一种新的框架来解决这些问题。

Method: VisuCraft通过集成多模态结构信息提取器（E）和动态提示生成模块（G）来解决现有LVLMs在保持高视觉保真度、真实创造力和精确遵循细微用户指令方面的局限性。

Result: 在自建的ImageStoryGen-500K数据集上，使用VisuGen指标（视觉定位、创造力和指令遵循）评估，VisuCraft在故事生成和诗歌创作等任务中始终优于基线LVLMs，尤其在创造力和指令遵循方面有显著提升。

Conclusion: 本文展示了VisuCraft在复杂视觉引导的创造性内容生成任务中的有效性，特别是在创造力和指令遵循方面有显著提升，验证了其在高级创造性AI应用中的潜力。

Abstract: This paper introduces VisuCraft, a novel framework designed to significantly
enhance the capabilities of Large Vision-Language Models (LVLMs) in complex
visual-guided creative content generation. Existing LVLMs often exhibit
limitations in maintaining high visual fidelity, genuine creativity, and
precise adherence to nuanced user instructions when generating long-form texts.
VisuCraft addresses these challenges by integrating a multimodal structured
information extractor (E) and a dynamic prompt generation module (G). The
extractor distills fine-grained visual attributes from input images into a
rich, structured representation, which the dynamic prompt module then combines
with user instructions to create highly optimized prompts for underlying LVLMs
(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed
ImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,
and Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs
across tasks like story generation and poetry composition. Our results
demonstrate remarkable improvements, particularly in creativity and instruction
adherence, validating VisuCraft's effectiveness in producing imaginative,
visually grounded, and user-aligned long-form creative text. This work unlocks
new potential for LVLMs in sophisticated creative AI applications.

</details>


### [67] [Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces](https://arxiv.org/abs/2508.02917)
*Vebjørn Haug Kåsene,Pierre Lison*

Main category: cs.CV

TL;DR: 本文研究了现成的大型视觉-语言模型是否能够有效支持视觉-语言导航任务，并评估了它们在不同动作空间中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究现成的LVLMs是否能有效支持VLN任务以及是否能支持低级和全景动作范式。

Method: 对开源模型Qwen2.5-VL-3B-Instruct在Room-to-Room (R2R)数据集上进行微调，并评估其在低级和全景动作空间中的表现。

Result: 最佳模型在R2R测试集上实现了41%的成功率。

Conclusion: 虽然现成的LVLMs可以学习执行视觉-语言导航，但它们仍然落后于专门为该任务设计的模型。

Abstract: Vision-and-Language Navigation (VLN) refers to the task of enabling
autonomous robots to navigate unfamiliar environments by following natural
language instructions. While recent Large Vision-Language Models (LVLMs) have
shown promise in this task, most current VLM systems rely on models
specifically designed and optimized for navigation, leaving the potential of
off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used
low-level action spaces with egocentric views and atomic actions (such as "turn
left" or "move forward"), newer models tend to favor panoramic action spaces
with discrete navigable viewpoints. This paper investigates (1) whether
off-the-shelf LVLMs (fine-tuned without architectural modifications or
simulator-based training) can effectively support VLN tasks and (2) whether
such models can support both low-level and panoramic action paradigms. To this
end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the
Room-to-Room (R2R) dataset and evaluate its empirical performance across both
low-level and panoramic action spaces. The best resulting model achieves a 41%
success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs
can learn to perform Vision-and-Language Navigation, they still lag behind
models specifically designed for this task.

</details>


### [68] [ChartCap: Mitigating Hallucination of Dense Chart Captioning](https://arxiv.org/abs/2508.03164)
*Junyoung Lim,Jaewoo Ahn,Gunhee Kim*

Main category: cs.CV

TL;DR: 本文介绍了ChartCap数据集，用于生成准确、信息丰富的图表描述，并提出了一种新的度量标准来评估描述质量。


<details>
  <summary>Details</summary>
Motivation: 生成准确、信息丰富且无幻觉的图表描述对于视觉语言模型仍然是一个挑战，主要是由于缺乏大规模、高质量的真实世界图表数据集。然而，现有的真实世界图表数据集存在包含无法从图表中推断出的额外信息的问题，并且未能充分捕捉结构元素和关键见解。

Method: 设计了一个四阶段管道，仅使用图表中可识别的数据生成描述，并采用基于循环一致性的人员验证来加速质量控制。此外，提出了一种新的度量标准——视觉一致性分数，通过测量从描述重新生成的图表与原始图表之间的相似性来评估描述质量，而无需参考描述。

Result: 引入了ChartCap，这是一个包含565K真实世界图表图像的大规模数据集，配有类型特定的密集描述，排除了额外信息，并详细突出了结构元素和关键见解。

Conclusion: 实验结果表明，微调后的模型生成的描述更准确、信息量更大，且幻觉更少，超过了开源和专有模型，甚至超过了人工标注的描述。

Abstract: Generating accurate, informative, and hallucination-free captions for charts
remains challenging for vision language models, primarily due to the lack of
large-scale, high-quality datasets of real-world charts. However, existing
real-world chart datasets suffer from the inclusion of extraneous information
that cannot be inferred from the chart and failure to sufficiently capture
structural elements and key insights. Therefore, we introduce ChartCap, a
large-scale dataset of 565K real-world chart images paired with type-specific,
dense captions that exclude extraneous information and highlight both
structural elements and key insights in detail. To build ChartCap, we design a
four-stage pipeline that generates captions using only the discernible data
from the chart and employ a cycle consistency-based human verification, which
accelerates quality control without sacrificing accuracy. Additionally, we
propose a novel metric, the Visual Consistency Score, which evaluates caption
quality by measuring the similarity between the chart regenerated from a
caption and the original chart, independent of reference captions. Extensive
experiments confirms that models fine-tuned on ChartCap consistently generate
more accurate and informative captions with reduced hallucinations, surpassing
both open-source and proprietary models and even human-annotated captions.

</details>


### [69] [VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation](https://arxiv.org/abs/2508.03351)
*Yufei Xue,Yushi Huang,Jiawei Shao,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉语言模型的新型重要性感知后训练量化框架VLMQ，在多个基准测试中表现出色，特别是在低比特设置下。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Hessian的LLM后训练量化方法在量化过程中对所有令牌一视同仁，导致在视觉语言模型上的性能严重下降。因此，需要一种专门针对视觉语言模型的量化方法。

Method: VLMQ通过优化重要性感知的目标函数，结合基于Hessian的量化方法，并通过单次轻量级块状反向传递计算重要性因素，以解决视觉令牌冗余问题。

Result: 在0.5B到32B参数规模的8个基准测试中，VLMQ表现出色，特别是在低比特设置下。例如，在2比特量化下，MME-RealWorld基准测试中取得了16.45%的显著提升。

Conclusion: 本文提出了一种针对视觉语言模型的新型重要性感知后训练量化框架VLMQ，该框架在多个基准测试中表现出色，特别是在低比特设置下。

Abstract: Post-training quantization (PTQ) has emerged as an effective approach for
compressing large models and accelerating their inference without retraining.
While PTQ has been extensively studied in the context of large language models
(LLMs), its applicability to vision-language models (VLMs) remains
underexplored. In this paper, we identify a modality discrepancy (\emph{i.e.},
limited text tokens \emph{vs.} excessive and redundant vision tokens) of VLMs.
However, existing Hessian-based LLM PTQ methods treat all tokens equally during
quantization, resulting in severe performance drops when applied to VLMs.
Motivated by this observation, we propose a novel importance-aware PTQ
framework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token
redundancy, VLMQ 1) optimizes an importance-aware objective that yields an
enhanced Hessian with token-level importance factors, while retaining
compatibility with parallelized weight updates, and 2) ensures efficiency and
effectiveness by computing these factors via a single lightweight block-wise
backward pass, guided by a theoretical connection to token-level perturbations.
Extensive evaluations on 8 benchmarks across 0.5B$\sim$32B VLMs demonstrate the
state-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit
settings. For example, it achieves a substantial \textbf{16.45\%} improvement
on MME-RealWorld under 2-bit quantization.

</details>


### [70] [Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.03481)
*Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.CV

TL;DR: DrUM is a new method that enables personalized generation in T2I diffusion models by integrating user profiling with a transformer-based adapter, allowing for more accurate personalization without additional fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing studies primarily rely on prompt-level modeling with large-scale models, often leading to inaccurate personalization due to the limited input token capacity of T2I diffusion models.

Method: DrUM is a novel method that integrates user profiling with a transformer-based adapter to enable personalized generation through condition-level modeling in the latent space.

Result: DrUM demonstrates strong performance on large-scale datasets and seamlessly integrates with open-source text encoders, making it compatible with widely used foundation T2I models without requiring additional fine-tuning.

Conclusion: DrUM demonstrates strong performance on large-scale datasets and seamlessly integrates with open-source text encoders, making it compatible with widely used foundation T2I models without requiring additional fine-tuning.

Abstract: Personalized generation in T2I diffusion models aims to naturally incorporate
individual user preferences into the generation process with minimal user
intervention. However, existing studies primarily rely on prompt-level modeling
with large-scale models, often leading to inaccurate personalization due to the
limited input token capacity of T2I diffusion models. To address these
limitations, we propose DrUM, a novel method that integrates user profiling
with a transformer-based adapter to enable personalized generation through
condition-level modeling in the latent space. DrUM demonstrates strong
performance on large-scale datasets and seamlessly integrates with open-source
text encoders, making it compatible with widely used foundation T2I models
without requiring additional fine-tuning.

</details>


### [71] [Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching](https://arxiv.org/abs/2508.03562)
*Muzhaffar Hazman,Susan McKeever,Josephine Griffith*

Main category: cs.CV

TL;DR: 本文提出了一种更广泛的模因匹配方法，超越了传统的模板匹配，并探讨了基于提示的预训练多模态大语言模型在模因匹配中的应用。结果表明，准确匹配模因仍然面临挑战，需要更先进的技术。


<details>
  <summary>Details</summary>
Motivation: 现有的方法假设每个模因都包含一个共享的视觉背景（称为模板），这限制了模因匹配仅比较背景图像。然而，许多模因并不是基于模板的，这限制了自动模因分析的有效性，并且无法有效地将模因与当代网络模因词典联系起来。

Method: 我们引入了一种更广泛的模因匹配形式，超越了模板匹配。我们研究了传统相似性度量和基于提示的预训练多模态大语言模型在模因匹配中的应用。

Result: 传统相似性度量在匹配基于模板的模因方面表现出色，但在应用于非模板模因格式时效果不佳。然而，基于段的计算方法在匹配非模板模因方面表现优于整体图像度量。基于提示的预训练多模态大语言模型在模因匹配中显示出潜力。

Conclusion: 准确匹配通过共享视觉元素的模因（而不仅仅是背景模板）仍然是一个开放性挑战，需要更复杂的匹配技术。

Abstract: Internet memes, now a staple of digital communication, play a pivotal role in
how users engage within online communities and allow researchers to gain
insight into contemporary digital culture. These engaging user-generated
content are characterised by their reuse of visual elements also found in other
memes. Matching instances of memes via these shared visual elements, called
Meme Matching, is the basis of a wealth of meme analysis approaches. However,
most existing methods assume that every meme consists of a shared visual
background, called a Template, with some overlaid text, thereby limiting meme
matching to comparing the background image alone. Current approaches exclude
the many memes that are not template-based and limit the effectiveness of
automated meme analysis and would not be effective at linking memes to
contemporary web-based meme dictionaries. In this work, we introduce a broader
formulation of meme matching that extends beyond template matching. We show
that conventional similarity measures, including a novel segment-wise
computation of the similarity measures, excel at matching template-based memes
but fall short when applied to non-template-based meme formats. However, the
segment-wise approach was found to consistently outperform the whole-image
measures on matching non-template-based memes. Finally, we explore a
prompting-based approach using a pretrained Multimodal Large Language Model for
meme matching. Our results highlight that accurately matching memes via shared
visual elements, not just background templates, remains an open challenge that
requires more sophisticated matching techniques.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [72] [NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification](https://arxiv.org/abs/2508.02823)
*Wenshuo Zhang,Leixian Shen,Shuchang Xu,Jindu Wang,Jian Zhao,Huamin Qu,Linping Yuan*

Main category: cs.HC

TL;DR: 该研究提出了一种新的交互范式，通过直接意图-任务匹配提高用户与LLM之间的对齐度，并在NeuroSync中实现，以改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 领域用户（尤其是编程经验有限的用户）在使用对话式LLM解决领域问题时，常常面临意图与生成代码之间的不对齐问题，导致挫败感和多轮澄清。

Method: 该研究首先分析了意图与生成代码之间的不对齐原因，即双向歧义。然后提出了直接意图-任务匹配的人机交互范式，并在NeuroSync中实现，通过知识蒸馏管道提取LLM理解、用户意图及其映射，并通过可视化让用户直观检查和编辑它们。

Result: 实验结果表明，NeuroSync提高了意图-任务对齐度，降低了认知负担，并提高了编码效率。

Conclusion: 该研究通过NeuroSync实现了直接意图-任务匹配，提高了意图-任务对齐度，降低了认知负担，并提高了编码效率。

Abstract: Conversational LLMs have been widely adopted by domain users with limited
programming experience to solve domain problems. However, these users often
face misalignment between their intent and generated code, resulting in
frustration and rounds of clarification. This work first investigates the cause
of this misalignment, which dues to bidirectional ambiguity: both user intents
and coding tasks are inherently nonlinear, yet must be expressed and
interpreted through linear prompts and code sequences. To address this, we
propose direct intent-task matching, a new human-LLM interaction paradigm that
externalizes and enables direct manipulation of the LLM understanding, i.e.,
the coding tasks and their relationships inferred by the LLM prior to code
generation. As a proof-of-concept, this paradigm is then implemented in
NeuroSync, which employs a knowledge distillation pipeline to extract LLM
understanding, user intents, and their mappings, and enhances the alignment by
allowing users to intuitively inspect and edit them via visualizations. We
evaluate the algorithmic components of NeuroSync via technical experiments, and
assess its overall usability and effectiveness via a user study (N=12). The
results show that it enhances intent-task alignment, lowers cognitive effort,
and improves coding efficiency.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [73] [OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War](https://arxiv.org/abs/2508.03599)
*Johannes Niu,Mila Stillman,Anna Kruspe*

Main category: cs.SI

TL;DR: 本研究分析了Twitter上与俄乌战争相关的OSINT和虚假信息，揭示了情感、党派和信息传播模式。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在区分真实的OSINT和虚假信息努力，即'BULLSHINT'，并探讨社交媒体上信息传播的复杂动态。

Method: 使用情感分析、党派检测、虚假信息识别和命名实体识别（NER）来分析数据集，并应用社区检测技术以识别不同的集群。

Result: 研究发现主要的负面情绪受到战争事件的影响，存在复杂的亲乌克兰和亲俄罗斯党派分布，以及信息可能被战略性操纵。此外，社区检测技术能够识别出不同的集群。

Conclusion: 本研究有助于理解数字战争和虚假信息动态，提供了关于地缘政治冲突中OSINT操作化的见解。

Abstract: This paper examines the role of Open Source Intelligence (OSINT) on Twitter
regarding the Russo-Ukrainian war, distinguishing between genuine OSINT and
deceptive misinformation efforts, termed "BULLSHINT." Utilizing a dataset
spanning from January 2022 to July 2023, we analyze nearly 2 million tweets
from approximately 1,040 users involved in discussing real-time military
engagements, strategic analyses, and misinformation related to the conflict.
Using sentiment analysis, partisanship detection, misinformation
identification, and Named Entity Recognition (NER), we uncover communicative
patterns and dissemination strategies within the OSINT community. Significant
findings reveal a predominant negative sentiment influenced by war events, a
nuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the
potential strategic manipulation of information. Additionally, we apply
community detection techniques, which are able to identify distinct clusters
partisanship, topics, and misinformation, highlighting the complex dynamics of
information spread on social media. This research contributes to the
understanding of digital warfare and misinformation dynamics, offering insights
into the operationalization of OSINT in geopolitical conflicts.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [74] [Reliable Evaluation Protocol for Low-Precision Retrieval](https://arxiv.org/abs/2508.03306)
*Kisu Yang,Yoonna Jang,Hwanseok Jang,Kenneth Choi,Isabelle Augenstein,Heuiseok Lim*

Main category: cs.IR

TL;DR: 该研究提出了一种更稳健的检索评估协议，以减少低精度检索中的分数变化。通过结合高精度评分和考虑平局的检索指标，可以实现更一致和可靠的评估系统。


<details>
  <summary>Details</summary>
Motivation: 降低模型参数和计算的数值精度被广泛用于提高检索系统的效率，但在低精度下计算查询和文档之间的相关性分数时，由于粒度减少，会出现虚假的平局，导致结果具有高度变异性，使评估不可靠。

Method: 研究提出了两种方法：(1) 高精度评分（HPS），将最终评分步骤提升到更高精度以解决平局候选者；(2) 考虑平局的检索指标（TRM），用于量化平局候选者的顺序不确定性。

Result: 实验表明，HPS显著减少了因平局引起的不稳定性，而TRM准确恢复了预期的指标值。

Conclusion: 该研究提出了一种更稳健的检索评估协议，以减少低精度检索中的分数变化。通过结合高精度评分和考虑平局的检索指标，可以实现更一致和可靠的评估系统。

Abstract: Lowering the numerical precision of model parameters and computations is
widely adopted to improve the efficiency of retrieval systems. However, when
computing relevance scores between the query and documents in low-precision, we
observe spurious ties due to the reduced granularity. This introduces high
variability in the results based on tie resolution, making the evaluation less
reliable. To address this, we propose a more robust retrieval evaluation
protocol designed to reduce score variation. It consists of: (1) High-Precision
Scoring (HPS), which upcasts the final scoring step to higher precision to
resolve tied candidates with minimal computational cost; and (2) Tie-aware
Retrieval Metrics (TRM), which report expected scores, range, and bias to
quantify order uncertainty of tied candidates. Our experiments test multiple
models with three scoring functions on two retrieval datasets to demonstrate
that HPS dramatically reduces tie-induced instability, and TRM accurately
recovers expected metric values. This combination enables a more consistent and
reliable evaluation system for lower-precision retrievals.

</details>


### [75] [MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation](https://arxiv.org/abs/2508.03553)
*Wenlong Wu,Haofen Wang,Bohan Li,Peixuan Huang,Xinzhe Zhao,Lei Liang*

Main category: cs.IR

TL;DR: 本文提出了 MultiRAG 框架，通过知识引导的方法来减轻多源检索增强生成中的幻觉问题，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决多源检索增强生成中由于数据稀疏分布和源间不一致导致的幻觉问题。

Method: 提出了一种新的框架 MultiRAG，通过知识引导的方法来减轻多源检索增强生成中的幻觉问题。引入了两个关键创新：(1) 一个知识构建模块，利用多源线图来高效聚合不同知识源之间的逻辑关系；(2) 一个复杂的检索模块，实现了多级置信度计算机制，进行图级和节点级评估以识别并消除不可靠的信息节点。

Result: 在四个多领域查询数据集和两个多跳问答数据集上的广泛实验表明，MultiRAG 显著提高了知识检索的可靠性和效率。

Conclusion: MultiRAG 显著提高了复杂多源场景中知识检索的可靠性和效率。

Abstract: Retrieval Augmented Generation (RAG) has emerged as a promising solution to
address hallucination issues in Large Language Models (LLMs). However, the
integration of multiple retrieval sources, while potentially more informative,
introduces new challenges that can paradoxically exacerbate hallucination
problems. These challenges manifest primarily in two aspects: the sparse
distribution of multi-source data that hinders the capture of logical
relationships and the inherent inconsistencies among different sources that
lead to information conflicts. To address these challenges, we propose
MultiRAG, a novel framework designed to mitigate hallucination in multi-source
retrieval-augmented generation through knowledge-guided approaches. Our
framework introduces two key innovations: (1) a knowledge construction module
that employs multi-source line graphs to efficiently aggregate logical
relationships across different knowledge sources, effectively addressing the
sparse data distribution issue; and (2) a sophisticated retrieval module that
implements a multi-level confidence calculation mechanism, performing both
graph-level and node-level assessments to identify and eliminate unreliable
information nodes, thereby reducing hallucinations caused by inter-source
inconsistencies. Extensive experiments on four multi-domain query datasets and
two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the
reliability and efficiency of knowledge retrieval in complex multi-source
scenarios. \textcolor{blue}{Our code is available in
https://github.com/wuwenlong123/MultiRAG.

</details>


### [76] [PyLate: Flexible Training and Retrieval for Late Interaction Models](https://arxiv.org/abs/2508.03555)
*Antoine Chaffin,Raphaël Sourty*

Main category: cs.IR

TL;DR: 本文介绍了PyLate，这是一个基于Sentence Transformers的简化库，旨在支持多向量架构，通过提供高效的索引等功能，加速晚期交互模型的研究和实际应用。


<details>
  <summary>Details</summary>
Motivation: The practical adoption and public availability of late interaction models remain low compared to their single-vector counterparts, primarily due to a lack of accessible and modular tools for training and experimenting with such models.

Method: PyLate is a streamlined library built on top of Sentence Transformers to support multi-vector architectures natively, inheriting its efficient training, advanced logging, and automated model card generation while requiring minimal code changes to code templates users are already familiar with.

Result: PyLate aims to accelerate research and real-world application of late interaction models, thereby unlocking their full potential in modern IR systems.

Conclusion: PyLate has already enabled the development of state-of-the-art models, including GTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility for both research and production environments.

Abstract: Neural ranking has become a cornerstone of modern information retrieval.
While single vector search remains the dominant paradigm, it suffers from the
shortcoming of compressing all the information into a single vector. This
compression leads to notable performance degradation in out-of-domain,
long-context, and reasoning-intensive retrieval tasks. Multi-vector approaches
pioneered by ColBERT aim to address these limitations by preserving individual
token embeddings and computing similarity via the MaxSim operator. This
architecture has demonstrated superior empirical advantages, including enhanced
out-of-domain generalization, long-context handling, and performance in complex
retrieval scenarios. Despite these compelling empirical results and clear
theoretical advantages, the practical adoption and public availability of late
interaction models remain low compared to their single-vector counterparts,
primarily due to a lack of accessible and modular tools for training and
experimenting with such models. To bridge this gap, we introduce PyLate, a
streamlined library built on top of Sentence Transformers to support
multi-vector architectures natively, inheriting its efficient training,
advanced logging, and automated model card generation while requiring minimal
code changes to code templates users are already familiar with. By offering
multi-vector-specific features such as efficient indexes, PyLate aims to
accelerate research and real-world application of late interaction models,
thereby unlocking their full potential in modern IR systems. Finally, PyLate
has already enabled the development of state-of-the-art models, including
GTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility
for both research and production environments.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch](https://arxiv.org/abs/2509.12340)
*Nikolay Banar,Ehsan Lotfi,Jens Van Nooten,Cristina Arhiliuc,Marija Kliocaite,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文旨在解决荷兰语嵌入资源不足的问题，介绍了MTEB-NL基准、训练数据集和E5-NL模型，并将这些资源公开。


<details>
  <summary>Details</summary>
Motivation: 荷兰语在多语言资源中仍然被低估，通常只占一小部分，因此需要新的资源来促进荷兰语嵌入的发展。

Method: 本文引入了MTEB-NL基准，提供了从现有荷兰检索数据集中编译的训练数据集，并使用大型语言模型生成合成数据以扩展任务范围，最后发布了E5-NL模型。

Result: 本文提出了MTEB-NL基准、训练数据集和E5-NL模型，这些资源在多个任务中表现出色，并已公开提供。

Conclusion: 本文介绍了新的资源来评估和生成荷兰语嵌入，包括MTEB-NL基准、训练数据集以及E5-NL模型，并将这些资源公开在Hugging Face Hub和MTEB包中。

Abstract: Recently, embedding resources, including models, benchmarks, and datasets,
have been widely released to support a variety of languages. However, the Dutch
language remains underrepresented, typically comprising only a small fraction
of the published multilingual resources. To address this gap and encourage the
further development of Dutch embeddings, we introduce new resources for their
evaluation and generation. First, we introduce the Massive Text Embedding
Benchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and
newly created ones, covering a wide range of tasks. Second, we provide a
training dataset compiled from available Dutch retrieval datasets, complemented
with synthetic data generated by large language models to expand task coverage
beyond retrieval. Finally, we release a series of E5-NL models compact yet
efficient embedding models that demonstrate strong performance across multiple
tasks. We make our resources publicly available through the Hugging Face Hub
and the MTEB package.

</details>


### [2] [MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables](https://arxiv.org/abs/2509.12371)
*Matteo Marcuzzo,Alessandro Zangari,Andrea Albarelli,Jose Camacho-Collados,Mohammad Taher Pilehvar*

Main category: cs.CL

TL;DR: 本文介绍了MORABLES，这是一个基于历史文学中的寓言和短篇故事的人类验证基准，用于评估大型语言模型在道德推理方面的能力。研究发现，尽管更大的模型表现优于较小的模型，但它们仍然容易受到对抗性操作的影响，并且常常依赖于表面模式而不是真正的道德推理。此外，增强推理的模型未能弥补这一差距，这表明规模而非推理能力是性能的主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在标准阅读理解基准测试中表现出色，关注点正在转向评估它们处理复杂抽象推理和推断的能力。文献基准测试因其丰富的叙述和道德深度，为评估这种更深层次的理解技能提供了有吸引力的框架。

Method: 本文介绍了MORABLES，这是一个从历史文学中的寓言和短篇故事中构建的人类验证基准。主要任务是针对道德推理的多项选择题，精心设计的干扰项挑战模型超越浅层的提取式问答。为了进一步测试模型的鲁棒性，我们引入了对抗性变体，以揭示由于数据污染等问题导致的LLM漏洞和捷径。

Result: 研究结果表明，虽然更大的模型表现优于较小的模型，但它们仍然容易受到对抗性操作的影响，并且常常依赖于表面模式而不是真正的道德推理。这种脆弱性导致了显著的自我矛盾，最佳模型在不同道德选择的表述下，大约20%的情况下会反驳自己的答案。有趣的是，增强推理的模型未能弥补这一差距，这表明规模而非推理能力是性能的主要驱动因素。

Conclusion: 研究发现，尽管更大的模型表现优于较小的模型，但它们仍然容易受到对抗性操作的影响，并且常常依赖于表面模式而不是真正的道德推理。这种脆弱性导致了显著的自我矛盾，最佳模型在不同道德选择的表述下，大约20%的情况下会反驳自己的答案。有趣的是，增强推理的模型未能弥补这一差距，这表明规模而非推理能力是性能的主要驱动因素。

Abstract: As LLMs excel on standard reading comprehension benchmarks, attention is
shifting toward evaluating their capacity for complex abstract reasoning and
inference. Literature-based benchmarks, with their rich narrative and moral
depth, provide a compelling framework for evaluating such deeper comprehension
skills. Here, we present MORABLES, a human-verified benchmark built from fables
and short stories drawn from historical literature. The main task is structured
as multiple-choice questions targeting moral inference, with carefully crafted
distractors that challenge models to go beyond shallow, extractive question
answering. To further stress-test model robustness, we introduce adversarial
variants designed to surface LLM vulnerabilities and shortcuts due to issues
such as data contamination. Our findings show that, while larger models
outperform smaller ones, they remain susceptible to adversarial manipulation
and often rely on superficial patterns rather than true moral reasoning. This
brittleness results in significant self-contradiction, with the best models
refuting their own answers in roughly 20% of cases depending on the framing of
the moral choice. Interestingly, reasoning-enhanced models fail to bridge this
gap, suggesting that scale - not reasoning ability - is the primary driver of
performance.

</details>


### [3] [LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.12382)
*Anu Pradhan,Alexandra Ortan,Apurv Verma,Madhavan Seshadri*

Main category: cs.CL

TL;DR: 本文探讨了使用大型语言模型作为评估者的方法，发现某些统计指标比传统指标更适用于AI系统的评估。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标在生成AI的复杂评估中存在不足，特别是在法律研究等专业领域中，需要更精确的评估方法。

Method: 我们通过系统实验研究了LLM-as-a-Judge方法，并比较了不同指标和统计测试的有效性。

Result: 传统的Krippendorff's alpha指标可能在AI系统评估的偏态分布中产生误导，而Gwet's AC2和等级相关系数则更为稳健，同时Wilcoxon Signed-Rank Test与Benjamini-Hochberg校正提供了可靠的系统比较方法。

Conclusion: 我们的研究结果表明，可以将大型语言模型作为评估框架的一部分，以实现可扩展且成本效益高的评估，同时保持法律应用所需的精度。

Abstract: The evaluation bottleneck in recommendation systems has become particularly
acute with the rise of Generative AI, where traditional metrics fall short of
capturing nuanced quality dimensions that matter in specialized domains like
legal research. Can we trust Large Language Models to serve as reliable judges
of their own kind? This paper investigates LLM-as-a-Judge as a principled
approach to evaluating Retrieval-Augmented Generation systems in legal
contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viability: which
inter-rater reliability metrics best capture the alignment between LLM and
human assessments, and how do we conduct statistically sound comparisons
between competing systems? Through systematic experimentation, we discover that
traditional agreement metrics like Krippendorff's alpha can be misleading in
the skewed distributions typical of AI system evaluations. Instead, Gwet's AC2
and rank correlation coefficients emerge as more robust indicators for judge
selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg
corrections provides the statistical rigor needed for reliable system
comparisons.
  Our findings suggest a path toward scalable, cost-effective evaluation that
maintains the precision demanded by legal applications, transforming what was
once a human-intensive bottleneck into an automated, yet statistically
principled, evaluation framework.

</details>


### [4] [SENTRA: Selected-Next-Token Transformer for LLM Text Detection](https://arxiv.org/abs/2509.12385)
*Mitchell Plyler,Yilun Zhang,Alexander Tuzhilin,Saoud Khalifah,Sen Tian*

Main category: cs.CL

TL;DR: 本文提出了一种名为 SENTRA 的新型、通用且监督的 LLM 文本检测器，它在多个领域中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 的能力增强和普及，其被滥用的潜力和现实也在增加。本文旨在检测未明确声明为 LLM 生成的文本。

Method: SENTRA 是基于 Transformer 的编码器，利用选定的下一个标记概率序列和在大量未标记数据上的对比预训练。

Result: 在三个流行的公共数据集上进行的实验表明，SENTRA 在跨域设置中显著优于流行的基线模型。

Conclusion: SENTRA 是一种通用分类器，显著优于流行的基线模型。

Abstract: LLMs are becoming increasingly capable and widespread. Consequently, the
potential and reality of their misuse is also growing. In this work, we address
the problem of detecting LLM-generated text that is not explicitly declared as
such. We present a novel, general-purpose, and supervised LLM text detector,
SElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder
leveraging selected-next-token-probability sequences and utilizing contrastive
pre-training on large amounts of unlabeled data. Our experiments on three
popular public datasets across 24 domains of text demonstrate SENTRA is a
general-purpose classifier that significantly outperforms popular baselines in
the out-of-domain setting.

</details>


### [5] [MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering](https://arxiv.org/abs/2509.12405)
*Wen-wai Yim,Asma Ben Abacha,Zixuan Yu,Robert Doerning,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 本文介绍了一个新的多语言基准MORQA，用于评估医学领域自然语言生成系统的有效性。结果显示，基于大型语言模型的评估方法比传统指标更有效，能够更好地与专家判断相关联。


<details>
  <summary>Details</summary>
Motivation: 评估自然语言生成（NLG）系统在医学领域面临独特的挑战，因为对准确性、相关性和领域专业知识有关键需求。传统的自动评估指标往往无法区分高质量输出，尤其是在医学问答（QA）任务中，可能存在多种有效的回答。

Method: 我们引入了MORQA（Medical Open-Response QA），一个新的人工智能基准，用于评估三种医学视觉和文本基础QA数据集中的NLG评估指标的有效性。我们对传统指标和基于大型语言模型（LLM）的评估器进行了基准测试，如GPT-4和Gemini。

Result: 基于LLM的方法在与专家判断的相关性方面显著优于传统指标。我们进一步分析了推动这种改进的因素，包括LLM对语义细微差别的敏感性和对参考答案之间变化的鲁棒性。

Conclusion: 我们的结果提供了医学领域NLG评估的第一个全面的多语言定性研究，强调了需要与人类对齐的评估方法。所有数据集和注释都将公开发布以支持未来的研究。

Abstract: Evaluating natural language generation (NLG) systems in the medical domain
presents unique challenges due to the critical demands for accuracy, relevance,
and domain-specific expertise. Traditional automatic evaluation metrics, such
as BLEU, ROUGE, and BERTScore, often fall short in distinguishing between
high-quality outputs, especially given the open-ended nature of medical
question answering (QA) tasks where multiple valid responses may exist. In this
work, we introduce MORQA (Medical Open-Response QA), a new multilingual
benchmark designed to assess the effectiveness of NLG evaluation metrics across
three medical visual and text-based QA datasets in English and Chinese. Unlike
prior resources, our datasets feature 2-4+ gold-standard answers authored by
medical professionals, along with expert human ratings for three English and
Chinese subsets. We benchmark both traditional metrics and large language model
(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based
approaches significantly outperform traditional metrics in correlating with
expert judgments. We further analyze factors driving this improvement,
including LLMs' sensitivity to semantic nuances and robustness to variability
among reference answers. Our results provide the first comprehensive,
multilingual qualitative study of NLG evaluation in the medical domain,
highlighting the need for human-aligned evaluation methods. All datasets and
annotations will be publicly released to support future research.

</details>


### [6] [MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts](https://arxiv.org/abs/2509.12440)
*Jiayi He,Yangmin Huang,Qianyun Du,Xiangying Zhou,Zhiyang He,Jiaxue Hu,Xiaodong Tao,Lixian Lai*

Main category: cs.CL

TL;DR: MedFact is a new benchmark for Chinese medical fact-checking that highlights the challenges of deploying LLMs in healthcare, such as difficulty in error localization and over-criticism.


<details>
  <summary>Details</summary>
Motivation: The increasing deployment of Large Language Models (LLMs) in healthcare necessitates a rigorous evaluation of their factual reliability. However, existing benchmarks are often limited by narrow domains of data, failing to capture the complexity of real-world medical information.

Method: We introduce MedFact, a new and challenging benchmark for Chinese medical fact-checking. Its construction employs a hybrid AI-human framework where iterative expert feedback refines an AI-driven, multi-criteria filtering process.

Result: Our results reveal that while models can often determine if a text contains an error, precisely localizing it remains a substantial challenge, with even top-performing models falling short of human performance. Furthermore, our analysis uncovers a frequent ``over-criticism'' phenomenon, a tendency for models to misidentify correct information as erroneous.

Conclusion: MedFact provides a robust resource to drive the development of more factually reliable and medically aware models.

Abstract: The increasing deployment of Large Language Models (LLMs) in healthcare
necessitates a rigorous evaluation of their factual reliability. However,
existing benchmarks are often limited by narrow domains of data, failing to
capture the complexity of real-world medical information. To address this
critical gap, we introduce MedFact, a new and challenging benchmark for Chinese
medical fact-checking. MedFact comprises 2,116 expert-annotated instances
curated from diverse real-world texts, spanning 13 medical specialties, 8
fine-grained error types, 4 writing styles, and multiple difficulty levels. Its
construction employs a hybrid AI-human framework where iterative expert
feedback refines an AI-driven, multi-criteria filtering process, ensuring both
high data quality and difficulty. We conduct a comprehensive evaluation of 20
leading LLMs, benchmarking their performance on veracity classification and
error localization against a human expert baseline. Our results reveal that
while models can often determine if a text contains an error, precisely
localizing it remains a substantial challenge, with even top-performing models
falling short of human performance. Furthermore, our analysis uncovers a
frequent ``over-criticism'' phenomenon, a tendency for models to misidentify
correct information as erroneous, which is exacerbated by advanced reasoning
techniques such as multi-agent collaboration and inference-time scaling. By
highlighting these critical challenges for deploying LLMs in medical
applications, MedFact provides a robust resource to drive the development of
more factually reliable and medically aware models.

</details>


### [7] [Topic Coverage-based Demonstration Retrieval for In-Context Learning](https://arxiv.org/abs/2509.12451)
*Wonbin Kweon,SeongKu Kang,Runchu Tian,Pengcheng Jiang,Jiawei Han,Hwanjo Yu*

Main category: cs.CL

TL;DR: 本文提出了一种基于主题覆盖的检索框架TopicK，用于选择能够全面覆盖与测试输入和模型相关主题级知识的演示。


<details>
  <summary>Details</summary>
Motivation: 先前的方法通常仅基于嵌入相似性或生成概率检索演示，导致无关或冗余的例子。为了克服这一问题，需要识别和覆盖细粒度的知识需求。

Method: TopicK是一种基于主题覆盖的检索框架，它选择演示以全面覆盖与测试输入和模型相关的主题级知识。具体来说，TopicK估计输入所需的主题，并评估模型在这些主题上的知识。然后，TopicK迭代地选择引入之前未覆盖的所需主题的演示，其中模型表现出较低的主题知识。

Result: TopicK在各种数据集和开放及闭源大语言模型上进行了广泛的实验验证，证明了其有效性。

Conclusion: TopicK通过在各种数据集和开放及闭源大语言模型上的广泛实验验证了其有效性。

Abstract: The effectiveness of in-context learning relies heavily on selecting
demonstrations that provide all the necessary information for a given test
input. To achieve this, it is crucial to identify and cover fine-grained
knowledge requirements. However, prior methods often retrieve demonstrations
based solely on embedding similarity or generation probability, resulting in
irrelevant or redundant examples. In this paper, we propose TopicK, a topic
coverage-based retrieval framework that selects demonstrations to
comprehensively cover topic-level knowledge relevant to both the test input and
the model. Specifically, TopicK estimates the topics required by the input and
assesses the model's knowledge on those topics. TopicK then iteratively selects
demonstrations that introduce previously uncovered required topics, in which
the model exhibits low topical knowledge. We validate the effectiveness of
TopicK through extensive experiments across various datasets and both open- and
closed-source LLMs. Our source code is available at
https://github.com/WonbinKweon/TopicK_EMNLP2025.

</details>


### [8] [Does Language Model Understand Language?](https://arxiv.org/abs/2509.12459)
*Suvojit Acharjee,Utathya Aich,Asfak Ali*

Main category: cs.CL

TL;DR: 本研究评估了最先进的语言模型在英语和孟加拉语中的表现，引入了LUCID数据集来挑战模型在语言理解的关键方面。结果显示Compound-Beta模型在多种语言条件下表现出色，显示出与人类判断的高度一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管自然语言生成和理解取得了进展，但LM在细粒度的语言现象（如时态、否定、语态和模态）上仍然存在困难，而这些现象是有效人类交流的核心。在联合国SDG 4的背景下，语言清晰度至关重要，因此需要仔细审查LM在教育技术中的部署。

Method: 我们引入了LUCID数据集，该数据集由精心设计的句子对组成，专门挑战这些模型在语言理解的关键方面，包括否定、时态和语态变化。我们评估了SOTA模型在标准指标（如皮尔逊相关性、斯皮尔曼相关性、平均绝对误差）以及新的语言学启发指标HCE准确性方面的表现。

Result: 我们的研究结果表明，Compound-Beta模型在多种语言条件下表现出色，显示出与人类判断的高度一致性。它在英语中记录了最高的皮尔逊相关性，并在混合语言数据上表现出稳健的性能，表明在跨语言场景中与人类判断有很强的一致性。

Conclusion: 我们的研究结果表明，Compound-Beta模型在多种语言条件下表现出色，显示出与人类判断的高度一致性。

Abstract: Despite advances in natural language generation and understanding, LM still
struggle with fine grained linguistic phenomena such as tense, negation, voice,
and modality which are the elements central to effective human communication.
In the context of the United Nations SDG 4, where linguistic clarity is
critical, the deployment of LMs in educational technologies demands careful
scrutiny. As LMs are increasingly powering applications like tutoring systems,
automated grading, and translation, their alignment with human linguistic
interpretation becomes essential for effective learning. In this study, we
conduct a evaluation of SOTA language models across these challenging contexts
in both English and Bengali. To ensure a structured assessment, we introduce a
new Route for Evaluation of Cognitive Inference in Systematic Environments
guidelines. Our proposed LUCID dataset, composed of carefully crafted sentence
pairs in English and Bengali, specifically challenges these models on critical
aspects of language comprehension, including negation, tense, voice variations.
We assess the performance of SOTA models including MISTRAL-SABA-24B,
LLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard
metrics like Pearson correlation, Spearman correlation, and Mean Absolute
Error, as well as novel, linguistically inspired metric the HCE accuracy. The
HCE accuracy measures how often model predictions fall within one standard
deviation of the mean human rating, thus capturing human like tolerance for
variability in language interpretation. Our findings highlight Compound-Beta as
the most balanced model, consistently achieving high correlations and low MAEs
across diverse language conditions. It records the highest Pearson correlation
in English and demonstrates robust performance on mixed-language data,
indicating a strong alignment with human judgments in cross lingual scenarios.

</details>


### [9] [Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction](https://arxiv.org/abs/2509.12476)
*Sumanta Bhattacharyya,Sara Riaz,Pedram Rooshenas*

Main category: cs.CL

TL;DR: R2tA is a method that uses refined model rationales as supervision for training task-specific reasoning models, showing effectiveness in data-scarce domains.


<details>
  <summary>Details</summary>
Motivation: Training a task-specific small reasoning model is challenging when direct human supervision or high-quality labels are scarce. However, LLMs with reasoning capabilities produce abundant intermediate reasoning traces that can be systematically refined to create effective supervision signals.

Method: We propose Reason-Refine-then-Align (R2tA), which turns refined model rationales into supervision for training task-specific reasoning models. Our method generates initial reasoning and responses from an open-source base model on task-specific inputs, then refines these traces, fixing hallucinations and inconsistencies, to form a high-fidelity dataset. We perform a two-stage alignment, supervised fine-tuning (SFT), followed by direct preference optimization (DPO) to calibrate the model's intermediate reasoning with human-validated conceptual preferences and then condition the final output on that aligned reasoning.

Result: Empirical evaluation suggests R2tA provides a practical, cost-effective path to scalable LLM adaptation in data-scarce domains, enabling reproducible AI tools for education and beyond.

Conclusion: R2tA provides a practical, cost-effective path to scalable LLM adaptation in data-scarce domains, enabling reproducible AI tools for education and beyond.

Abstract: Training a task-specific small reasoning model is challenging when direct
human supervision or high-quality labels are scarce. However, LLMs with
reasoning capabilities produce abundant intermediate reasoning traces that can
be systematically refined to create effective supervision signals. We propose
Reason-Refine-then-Align (R2tA), which turns refined model rationales into
supervision for training task-specific reasoning models. Our method generates
initial reasoning and responses from an open-source base model on task-specific
inputs, then refines these traces, fixing hallucinations and inconsistencies,
to form a high-fidelity dataset. We perform a two-stage alignment, supervised
fine-tuning (SFT), followed by direct preference optimization (DPO) to
calibrate the model's intermediate reasoning with human-validated conceptual
preferences and then condition the final output on that aligned reasoning. As a
case study, we apply R2tA to evaluate extended entity relationship diagrams
(EERDs) in database system design, a structurally complex task where
prompt-only methods miss or hallucinate errors. We curated a dataset of 600
EERD variants (train/test split of 450/150, respectively) with induced mistakes
spanning 11 categories. Empirical evaluation suggests R2tA provides a
practical, cost-effective path to scalable LLM adaptation in data-scarce
domains, enabling reproducible AI tools for education and beyond.

</details>


### [10] [FunAudio-ASR Technical Report](https://arxiv.org/abs/2509.12508)
*Keyu An,Yanni Chen,Chong Deng,Changfeng Gao,Zhifu Gao,Bo Gong,Xiangang Li,Yabin Li,Xiang Lv,Yunjie Ji,Yiheng Jiang,Bin Ma,Haoneng Luo,Chongjia Ni,Zexu Pan,Yiping Peng,Zhendong Peng,Peiyao Wang,Hao Wang,Wen Wang,Wupeng Wang,Biao Tian,Zhentao Tan,Nan Yang,Bin Yuan,Jieping Ye,Jixing Yu,Qinglin Zhang,Kun Zou,Han Zhao,Shengkui Zhao,Jingren Zhou*

Main category: cs.CL

TL;DR: FunAudio-ASR is a large-scale, LLM-based ASR system that combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance in practical speech recognition scenarios.


<details>
  <summary>Details</summary>
Motivation: LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. Most LLM-based ASR systems achieve strong performance on open-source benchmarks but often underperform on real industry evaluation sets.

Method: FunAudio-ASR synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios.

Result: FunAudio-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that FunAudio-ASR achieves SOTA performance on real application datasets.

Conclusion: FunAudio-ASR achieves SOTA performance on real application datasets, demonstrating its effectiveness and robustness in practical settings.

Abstract: In recent years, automatic speech recognition (ASR) has witnessed
transformative advancements driven by three complementary paradigms: data
scaling, model size scaling, and deep integration with large language models
(LLMs). However, LLMs are prone to hallucination, which can significantly
degrade user experience in real-world ASR applications. In this paper, we
present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically
combines massive data, large model capacity, LLM integration, and reinforcement
learning to achieve state-of-the-art performance across diverse and complex
speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized
for practical deployment, with enhancements in streaming capability, noise
robustness, code-switching, hotword customization, and satisfying other
real-world application requirements. Experimental results show that while most
LLM-based ASR systems achieve strong performance on open-source benchmarks,
they often underperform on real industry evaluation sets. Thanks to
production-oriented optimizations, FunAudio-ASR achieves SOTA performance on
real application datasets, demonstrating its effectiveness and robustness in
practical settings.

</details>


### [11] [A comparison of pipelines for the translation of a low resource language based on transformers](https://arxiv.org/abs/2509.12514)
*Chiara Bonfanti,Michele Colombino,Giulia Coucourde,Faeze Memari,Stefano Pinardi,Rosa Meo*

Main category: cs.CL

TL;DR: 该研究比较了三种用于训练基于Transformer的神经网络以生成Bambara语机器翻译的管道，并发现第一种管道在翻译准确性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在提高Bambara语的机器翻译性能，特别是在低资源条件下。

Method: 该研究比较了三种管道：第一种是训练一个简单的Transformer模型进行法语到Bambara的翻译；第二种是微调LLaMA3（3B-8B）指导模型；第三种是使用语言蒸馏技术将Bambara集成到预训练的LaBSE模型中。

Result: 第一种管道在Bayelemagaba数据集上实现了10%的BLEU和21%的chrF得分，在Yiri数据集上实现了33.81%的BLEU和41%的chrF得分。指导模型在单个数据集上的表现优于聚合数据集。

Conclusion: 该研究比较了三种用于训练基于Transformer的神经网络以生成Bambara语机器翻译的管道。结果表明，第一种管道虽然简单，但在翻译准确性方面表现最佳。

Abstract: This work compares three pipelines for training transformer-based neural
networks to produce machine translators for Bambara, a Mand\`e language spoken
in Africa by about 14,188,850 people. The first pipeline trains a simple
transformer to translate sentences from French into Bambara. The second
fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures
for French-to-Bambara translation. Models from the first two pipelines were
trained with different hyperparameter combinations to improve BLEU and chrF
scores, evaluated on both test sentences and official Bambara benchmarks. The
third pipeline uses language distillation with a student-teacher dual neural
network to integrate Bambara into a pre-trained LaBSE model, which provides
language-agnostic embeddings. A BERT extension is then applied to LaBSE to
generate translations. All pipelines were tested on Dokotoro (medical) and
Bayelemagaba (mixed domains). Results show that the first pipeline, although
simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on
Bayelemagaba), consistent with low-resource translation results. On the Yiri
dataset, created for this work, it achieves 33.81% BLEU and 41% chrF.
Instructor-based models perform better on single datasets than on aggregated
collections, suggesting they capture dataset-specific patterns more
effectively.

</details>


### [12] [MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models](https://arxiv.org/abs/2509.12591)
*Vijay Govindarajan,Pratik Patel,Sahil Tripathi,Md Azizul Hoque,Gautam Siddharth Kashyap*

Main category: cs.CL

TL;DR: 本文提出了一种零样本自动音频字幕系统，利用预训练的音频CLIP模型提取听觉特征，并通过优化标记选择来提高字幕生成的质量。实验结果显示，该方法在NLG平均分数上提高了35%。


<details>
  <summary>Details</summary>
Motivation: 自动化音频字幕（AAC）为音频片段生成字幕，但由于数据集有限，面临挑战。为了克服这一问题，我们提出了零样本AAC系统，利用预训练模型，消除了对大量训练的需求。

Method: 我们的方法利用预训练的音频CLIP模型提取听觉特征并生成结构化提示，引导大型语言模型（LLM）进行字幕生成。与传统的贪心解码不同，我们的方法通过音频CLIP模型优化标记选择，确保与音频内容一致。

Result: 实验结果表明，使用MAGIC搜索的WavCaps模型在NLG平均分数上提高了35%（从4.7到7.3）。性能主要受音频-文本匹配模型和关键词选择的影响，使用单个关键词提示可以获得最佳结果，而没有关键词列表时性能下降了50%。

Conclusion: 实验结果表明，使用MAGIC搜索的WavCaps模型在NLG平均分数上提高了35%（从4.7到7.3）。性能主要受音频-文本匹配模型和关键词选择的影响，使用单个关键词提示可以获得最佳结果，而没有关键词列表时性能下降了50%。

Abstract: Automated Audio Captioning (AAC) generates captions for audio clips but faces
challenges due to limited datasets compared to image captioning. To overcome
this, we propose the zero-shot AAC system that leverages pre-trained models,
eliminating the need for extensive training. Our approach uses a pre-trained
audio CLIP model to extract auditory features and generate a structured prompt,
which guides a Large Language Model (LLM) in caption generation. Unlike
traditional greedy decoding, our method refines token selection through the
audio CLIP model, ensuring alignment with the audio content. Experimental
results demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using
MAGIC search with the WavCaps model. The performance is heavily influenced by
the audio-text matching model and keyword selection, with optimal results
achieved using a single keyword prompt, and a 50% performance drop when no
keyword list is used.

</details>


### [13] [EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving](https://arxiv.org/abs/2509.12603)
*Mukai Li,Linfeng Song,Zhenwen Liang,Jiahao Xu,Shansan Gong,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文研究了测试时扩展策略对自动定理证明模型效率的影响，并提出了一种高效的方法，能够在降低计算成本的同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有的成本分析通常只调节采样传递的数量，而忽略了不同扩展策略引入的显著差异的采样成本。本文旨在解决这个问题，并提高自动定理证明模型的效率。

Method: 本文系统地比较了不同的测试时扩展策略在自动定理证明模型中的效率，并提出了两种互补的方法，以显著减少令牌使用和样本传递次数，同时保持原始性能。

Result: 实验表明，我们的EconProver在仅使用12%计算成本的情况下，与基线方法具有可比性能。

Conclusion: 本文提供了在不牺牲性能的情况下部署轻量级自动定理证明模型的可行见解。

Abstract: Large Language Models (LLMs) have recently advanced the field of Automated
Theorem Proving (ATP), attaining substantial performance gains through widely
adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)
reasoning and increased sampling passes. However, they both introduce
significant computational overhead for inference. Moreover, existing cost
analyses typically regulate only the number of sampling passes, while
neglecting the substantial disparities in sampling costs introduced by
different scaling strategies. In this paper, we systematically compare the
efficiency of different test-time scaling strategies for ATP models and
demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source
approaches. We then investigate approaches to significantly reduce token usage
and sample passes while maintaining the original performance. Specifically, we
propose two complementary methods that can be integrated into a unified EconRL
pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching
mechanism designed to mitigate unnecessary token consumption, and (2) Diverse
parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance
pass rates under constrained sampling passes. Experiments on miniF2F and
ProofNet demonstrate that our EconProver achieves comparable performance to
baseline methods with only 12% of the computational cost. This work provides
actionable insights for deploying lightweight ATP models without sacrificing
performance.

</details>


### [14] [Positional Encoding via Token-Aware Phase Attention](https://arxiv.org/abs/2509.12635)
*Yu,Wang,Sheng Shen,Rémi Munos,Hongyuan Zhan,Yuandong Tian*

Main category: cs.CL

TL;DR: 本文提出了一种新的位置编码方法TAPA，解决了RoPE在长上下文中的局限性，具有更好的性能和扩展性。


<details>
  <summary>Details</summary>
Motivation: RoPE在长上下文中存在固有的距离依赖偏差，限制了其建模能力，而现有的扩展方法通常需要在预训练后进行调整。

Method: TAPA是一种新的位置编码方法，它将可学习的相位函数引入注意力机制，以保持长距离的token交互。

Result: TAPA能够扩展到更长的上下文，直接且轻量级的微调，并能外推到未见过的长度，在长上下文中取得了显著更低的困惑度。

Conclusion: TAPA在长上下文任务中表现出比RoPE家族更低的困惑度，能够有效解决RoPE在建模长上下文时的局限性。

Abstract: We prove under practical assumptions that Rotary Positional Embedding (RoPE)
introduces an intrinsic distance-dependent bias in attention scores that limits
RoPE's ability to model long-context. RoPE extension methods may alleviate this
issue, but they typically require post-hoc adjustments after pretraining, such
as rescaling or hyperparameters retuning. This paper introduces Token-Aware
Phase Attention (TAPA), a new positional encoding method that incorporates a
learnable phase function into the attention mechanism. TAPA preserves token
interactions over long range, extends to longer contexts with direct and light
fine-tuning, extrapolates to unseen lengths, and attains significantly lower
perplexity on long-context than RoPE families.

</details>


### [15] [PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition](https://arxiv.org/abs/2509.12647)
*Li Fu,Yu Xin,Sunlu Zeng,Lu Fan,Youzheng Wu,Xiaodong He*

Main category: cs.CL

TL;DR: 本文提出了一种发音感知的上下文框架（PAC），以解决基于大型语言模型的自动语音识别系统中的发音建模和同音词区分问题，并在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决基于大型语言模型（LLM）的自动语音识别（ASR）系统中的两个关键挑战：有效的发音建模和稳健的同音词区分。

Method: PAC框架采用两阶段学习范式：首先引入基于发音的上下文学习方法，利用交错的音素-字素上下文建模策略；然后提出基于发音的对抗强化学习方法，通过扰动标签采样来增强模型区分同音词的能力。

Result: 在公共英语Librispeech和中文AISHELL-1数据集上，PAC框架相对于预训练的LLM-based ASR模型，将相对词错误率（WER）分别降低了30.2%和53.8%；同时，在长尾词的有偏词错误率（WER）上，分别实现了31.8%和60.5%的相对降低。

Conclusion: PAC框架在英语Librispeech和中文AISHELL-1数据集上的实验结果表明，它能够显著降低相对词错误率（WER）并提高对长尾词的识别效果。

Abstract: This paper presents a Pronunciation-Aware Contextualized (PAC) framework to
address two key challenges in Large Language Model (LLM)-based Automatic Speech
Recognition (ASR) systems: effective pronunciation modeling and robust
homophone discrimination. Both are essential for raw or long-tail word
recognition. The proposed approach adopts a two-stage learning paradigm. First,
we introduce a pronunciation-guided context learning method. It employs an
interleaved grapheme-phoneme context modeling strategy that incorporates
grapheme-only distractors, encouraging the model to leverage phonemic cues for
accurate recognition. Then, we propose a pronunciation-discriminative
reinforcement learning method with perturbed label sampling to further enhance
the model\'s ability to distinguish contextualized homophones. Experimental
results on the public English Librispeech and Mandarin AISHELL-1 datasets
indicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and
53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and
60.5% relative reductions in biased WER for long-tail words compared to strong
baselines, respectively.

</details>


### [16] [Don't Change My View: Ideological Bias Auditing in Large Language Models](https://arxiv.org/abs/2509.12652)
*Paul Kröger,Emilio Barkett*

Main category: cs.CL

TL;DR: 本文提出了一种无需访问语言模型内部结构的意识形态偏见检测方法，并通过实验验证了其有效性，可用于独立的后期审计。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在产品中的广泛应用，它们的输出可能影响个人信念并塑造公众意见。因此，需要一种方法来检测是否发生了有意的意识形态引导。

Method: 本文适应了一种先前提出的统计方法，用于意识形态偏见审计。该方法不依赖于语言模型的内部结构，而是通过分析与特定主题相关的提示中的模型输出分布变化来识别潜在的意识形态引导。

Result: 通过一系列实验验证了该方法的实用性，并展示了其在独立后期审计中的潜力。

Conclusion: 本文提出了一种检测大型语言模型（LLMs）意识形态偏见的方法，并通过实验验证了其可行性，表明该方法可以用于独立的后期审计。

Abstract: As large language models (LLMs) become increasingly embedded in products used
by millions, their outputs may influence individual beliefs and, cumulatively,
shape public opinion. If the behavior of LLMs can be intentionally steered
toward specific ideological positions, such as political or religious views,
then those who control these systems could gain disproportionate influence over
public discourse. Although it remains an open question whether LLMs can
reliably be guided toward coherent ideological stances and whether such
steering can be effectively prevented, a crucial first step is to develop
methods for detecting when such steering attempts occur. In this work, we adapt
a previously proposed statistical method to the new context of ideological bias
auditing. Our approach carries over the model-agnostic design of the original
framework, which does not require access to the internals of the language
model. Instead, it identifies potential ideological steering by analyzing
distributional shifts in model outputs across prompts that are thematically
related to a chosen topic. This design makes the method particularly suitable
for auditing proprietary black-box systems. We validate our approach through a
series of experiments, demonstrating its practical applicability and its
potential to support independent post hoc audits of LLM behavior.

</details>


### [17] [Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations](https://arxiv.org/abs/2509.12661)
*Yougen Zhou,Qin Chen,Ningning Zhou,Jie Zhou,Xingjiao Wu,Liang He*

Main category: cs.CL

TL;DR: 研究揭示了大型语言模型在策略规划中的知识边界，并提出了一种通过强化学习与双奖励函数来减轻偏差的方法，实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在情感支持对话中因策略规划准确性低而面临的持续挑战以及特定策略的偏好偏差问题。

Method: 通过强化学习与双奖励函数来减轻偏差，通过准确性和基于熵的置信度优化策略规划。

Result: 我们的方法在多个大型语言模型基础上的ESCov和ExTES数据集上表现出色。

Conclusion: 我们的方法在ESCov和ExTES数据集上优于基线，证实了其有效性。

Abstract: Emotional support conversation (ESC) aims to alleviate distress through
empathetic dialogue, yet large language models (LLMs) face persistent
challenges in delivering effective ESC due to low accuracy in strategy
planning. Moreover, there is a considerable preference bias towards specific
strategies. Prior methods using fine-tuned strategy planners have shown
potential in reducing such bias, while the underlying causes of the preference
bias in LLMs have not well been studied. To address these issues, we first
reveal the fundamental causes of the bias by identifying the knowledge
boundaries of LLMs in strategy planning. Then, we propose an approach to
mitigate the bias by reinforcement learning with a dual reward function, which
optimizes strategy planning via both accuracy and entropy-based confidence for
each region according to the knowledge boundaries. Experiments on the ESCov and
ExTES datasets with multiple LLM backbones show that our approach outperforms
the baselines, confirming the effectiveness of our approach.

</details>


### [18] [Chat-Driven Text Generation and Interaction for Person Retrieval](https://arxiv.org/abs/2509.12662)
*Zequn Xie,Chuxin Wang,Sihang Cai,Yeqiang Wang,Shulei Wang,Tao Jin*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工标注的文本基人体搜索框架，通过多轮文本生成和交互模块提高检索性能。


<details>
  <summary>Details</summary>
Motivation: 文本基人体搜索（TBPS）在监控应用中具有重要价值，但获取高质量文本注释的过程劳动密集，限制了其可扩展性和实际部署。

Method: 本文引入了两个互补模块：多轮文本生成（MTG）和多轮文本交互（MTI）。MTG通过与MLLMs的模拟对话生成丰富的伪标签，产生细粒度和多样化的视觉描述。MTI在推理时通过动态的、基于对话的推理来优化用户查询，使系统能够解释和解决模糊、不完整或模糊的描述。

Result: 实验结果表明，本文的方法在消除人工描述需求的同时，取得了竞争性或优越的结果。

Conclusion: 本文提出了一种无需人工标注的文本基人体搜索框架，显著提高了检索准确率、鲁棒性和可用性，并为TBPS系统的可扩展和实际部署铺平了道路。

Abstract: Text-based person search (TBPS) enables the retrieval of person images from
large-scale databases using natural language descriptions, offering critical
value in surveillance applications. However, a major challenge lies in the
labor-intensive process of obtaining high-quality textual annotations, which
limits scalability and practical deployment. To address this, we introduce two
complementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text
Interaction (MTI). MTG generates rich pseudo-labels through simulated dialogues
with MLLMs, producing fine-grained and diverse visual descriptions without
manual supervision. MTI refines user queries at inference time through dynamic,
dialogue-based reasoning, enabling the system to interpret and resolve vague,
incomplete, or ambiguous descriptions - characteristics often seen in
real-world search scenarios. Together, MTG and MTI form a unified and
annotation-free framework that significantly improves retrieval accuracy,
robustness, and usability. Extensive evaluations demonstrate that our method
achieves competitive or superior results while eliminating the need for manual
captions, paving the way for scalable and practical deployment of TBPS systems.

</details>


### [19] [Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content](https://arxiv.org/abs/2509.12672)
*Shaz Furniturewala,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本文研究了LLM生成文本对内容审核系统的影响，提出了一种基于机制可解释性的新策略，通过识别和抑制易受攻击的组件来提高对抗攻击的性能，并揭示了模型训练中的公平性和鲁棒性差距。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的文本与传统内容审核分类器的训练数据不同，导致误分类问题，而现有的防御策略是被动的，因此需要一种主动的方法来识别和解决这些问题。

Method: 我们使用对抗攻击技术来识别易受攻击的电路，并通过抑制这些电路来提高性能。此外，我们还分析了不同头在不同人口群体中的脆弱性。

Result: 我们发现模型有不同的头，其中一些头对性能至关重要，而另一些头则容易受到攻击。抑制易受攻击的头可以提高对抗输入的性能，并揭示了模型训练中的公平性和鲁棒性差距。

Conclusion: 我们的研究揭示了毒性分类器中的易受攻击组件，并通过抑制这些组件提高了对抗攻击的性能。此外，我们还发现了不同头在不同人口群体中的脆弱性差异，这可以指导更包容的毒性检测模型开发。

Abstract: The volume of machine-generated content online has grown dramatically due to
the widespread use of Large Language Models (LLMs), leading to new challenges
for content moderation systems. Conventional content moderation classifiers,
which are usually trained on text produced by humans, suffer from
misclassifications due to LLM-generated text deviating from their training data
and adversarial attacks that aim to avoid detection. Present-day defence
tactics are reactive rather than proactive, since they rely on adversarial
training or external detection models to identify attacks. In this work, we aim
to identify the vulnerable components of toxicity classifiers that contribute
to misclassification, proposing a novel strategy based on mechanistic
interpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa
classifiers, testing on diverse datasets spanning a variety of minority groups.
We use adversarial attacking techniques to identify vulnerable circuits.
Finally, we suppress these vulnerable circuits, improving performance against
adversarial attacks. We also provide demographic-level insights into these
vulnerable circuits, exposing fairness and robustness gaps in model training.
We find that models have distinct heads that are either crucial for performance
or vulnerable to attack and suppressing the vulnerable heads improves
performance on adversarial input. We also find that different heads are
responsible for vulnerability across different demographic groups, which can
inform more inclusive development of toxicity detection models.

</details>


### [20] [Case-Based Decision-Theoretic Decoding with Quality Memories](https://arxiv.org/abs/2509.12677)
*Hiroyuki Deguchi,Masaaki Nagata*

Main category: cs.CL

TL;DR: 本文提出了基于案例的决策理论（CBDT）解码方法，以改进最小贝叶斯风险（MBR）解码在生成高质量文本方面的表现，并在多个任务中取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: MBR解码依赖于从文本生成模型中抽取的样本文本，因此难以找到正确捕捉域外知识或信息的假设。

Method: 提出了一种基于案例的决策理论（CBDT）解码方法，该方法使用领域数据的例子来估计预期效用。

Result: CBDT解码不仅生成的文本质量高于MAP解码，而且MBR和CBDT解码的结合在多个翻译和图像描述任务中表现优于MBR解码。

Conclusion: CBDT解码不仅生成的文本质量高于MAP解码，而且MBR和CBDT解码的结合在七个领域De-En和Ja↔En翻译任务以及MSCOCO和nocaps数据集上的图像描述任务中表现优于MBR解码。

Abstract: Minimum Bayes risk (MBR) decoding is a decision rule of text generation,
which selects the hypothesis that maximizes the expected utility and robustly
generates higher-quality texts than maximum a posteriori (MAP) decoding.
However, it depends on sample texts drawn from the text generation model; thus,
it is difficult to find a hypothesis that correctly captures the knowledge or
information of out-of-domain. To tackle this issue, we propose case-based
decision-theoretic (CBDT) decoding, another method to estimate the expected
utility using examples of domain data. CBDT decoding not only generates
higher-quality texts than MAP decoding, but also the combination of MBR and
CBDT decoding outperformed MBR decoding in seven domain De--En and
Ja$\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO
and nocaps datasets.

</details>


### [21] [HistoryBankQA: Multilingual Temporal Question Answering on Historical Events](https://arxiv.org/abs/2509.12720)
*Biswadip Mandal,Anant Khandelwal,Manish Gupta*

Main category: cs.CL

TL;DR: 本文提出了一个包含10M+历史事件的多语言数据库HistoryBank，并构建了一个跨语言的时间推理问答基准测试，评估了多个大型语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的时间推理数据集在规模、多语言覆盖范围和对当代事件的关注方面存在局限性，因此需要一个更全面的数据集来促进多语言和时间感知的自然语言理解。

Method: 本文提出了HistoryBank，这是一个从维基百科时间线页面和文章信息框中提取的包含10M+历史事件的多语言数据库，并构建了一个跨所有语言的时间推理问答基准测试。

Result: GPT4o在所有答案类型和语言中表现最佳，Gemma-2在小型语言模型中表现最佳。

Conclusion: 本文旨在提供一个全面的资源，以推动多语言和时间感知的自然语言理解在历史事件方面的进展。

Abstract: Temporal reasoning about historical events is a critical skill for NLP tasks
like event extraction, historical entity linking, temporal question answering,
timeline summarization, temporal event clustering and temporal natural language
inference. Yet efforts on benchmarking temporal reasoning capabilities of large
language models (LLMs) are rather limited. Existing temporal reasoning datasets
are limited in scale, lack multilingual coverage and focus more on contemporary
events. To address these limitations, we present HistoryBank, a multilingual
database of 10M+ historical events extracted from Wikipedia timeline pages and
article infoboxes. Our database provides unprecedented coverage in both
historical depth and linguistic breadth with 10 languages. Additionally, we
construct a comprehensive question answering benchmark for temporal reasoning
across all languages. This benchmark covers a diverse set of 6 temporal QA
reasoning tasks, and we evaluate a suite of popular language models
(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their
performance on these tasks. As expected GPT4o performs best across all answer
types and languages; Gemma-2 outperforms the other small language models. Our
work aims to provide a comprehensive resource for advancing multilingual and
temporally-aware natural language understanding of historical events. To
facilitate further research, we will make our code and datasets publicly
available upon acceptance of this paper.

</details>


### [22] [Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision](https://arxiv.org/abs/2509.12771)
*Omri Suissa,Muhiim Ali,Shengmai Chen,Yinuo Cai,Shekhar Pradhan*

Main category: cs.CL

TL;DR: 本文研究了VLMs的概念抽象能力，并提出了一个基于分组对比损失的训练方法，使模型能够更好地识别抽象概念。


<details>
  <summary>Details</summary>
Motivation: 人类可以将图像识别为一般概念的实例，而不仅仅是识别其对象及其关系。本文研究了VLMs在多大程度上具备这种概念抽象能力，并探索了在图像中编码更高层次概念信息的策略，以使最终的VLM模型(CLEAR GLASS模型)具备更高的能力。

Method: 我们引入了一个分组的图像-标题数据集(MAGIC)，并使用了一种新的对比损失技术，使模型能够编码每个图像(标题)组中所有成员共有的信息。主要贡献是一个基于文本-图像对比组的分组对比损失函数以及一个内部损失，用于测量组内图像-标题实例之间的距离。

Result: 训练方法使得CLEAR GLASS模型具有概念抽象能力，因为模型没有接触到与每个组相关联的高层概念。相反，训练迫使模型为每个图像-标题组创建一个语义表示，使其更接近潜在语义空间中的高层概念的语义表示。

Conclusion: 我们的实验表明，这种训练方法使得模型在抽象概念识别方面相比SOTA模型有所改进。

Abstract: Humans can recognize an image as an instance of a general concept, beyond
simply identifying its objects and their relationships. In this paper, we
investigate 1. The extent to which VLMs have this concept abstraction capacity,
and 2. Strategies for encoding the sort of higher-concept information in images
that would enable the resulting VLM model (CLEAR GLASS model) to have this
capability to a greater degree. To this end, we introduce a grouped
image-caption dataset (MAGIC), which consists of several groups of image
captions and for each group a set of associated images and higher-level
conceptual labels. We use a novel contrastive loss technique to induce the
model to encode in the representation of each image (caption) in a group the
information that is common to all members of the image-caption group. Our main
contribution is a grouped contrastive loss function based on text-image
contrastive groups (outer contrastive loss) as well as an inner loss which
measures the distances between image-caption instances in the group. Our
training methodology results in the CLEAR GLASS model having the concept
abstraction capacity as an emergent capacity because the model is not exposed
to the higher-level concepts associated with each group. Instead, the training
forces the model to create for each image-caption group a semantic
representation that brings it closer to the semantic representation of the
higher-level concepts in the latent semantic space. Our experiments show that
this training methodology results in a model which shows improvement in
abstract concept recognition compared to SOTA models.

</details>


### [23] [ConvergeWriter: Data-Driven Bottom-Up Article Construction](https://arxiv.org/abs/2509.12811)
*Binquan Ji,Jiaqi Wang,Ruiting Li,Xingchen Han,Yiyang Qi,Shichao Wang,Yifei Lu,Yuantao Han,Feiliang Ren*

Main category: cs.CL

TL;DR: 本文提出了一种新的“自下而上”的数据驱动框架，通过先检索知识再聚类构建结构，以生成更可靠、结构化的长篇文档。


<details>
  <summary>Details</summary>
Motivation: 现有的“自上而下”方法在生成假设或大纲后检索证据，常常导致模型计划与可用知识之间的脱节，从而产生内容碎片化和事实性错误。因此，我们需要一种更有效的方法来生成基于广泛外部知识库的长篇、事实性文档。

Method: 我们提出了一种新颖的“自下而上”的数据驱动框架，采用了“先检索知识，再聚类构建结构”的策略。首先从知识库中进行详尽的迭代检索，然后使用无监督聚类算法将检索到的文档组织成不同的“知识簇”，这些簇构成了指导后续生成层次大纲和最终文档内容的数据驱动基础。

Result: 实验结果表明，我们的方法在14B和32B参数模型上的表现与最先进的基线相当或更好，并且在需要高保真度和结构连贯性的知识受限场景中展现出独特优势。

Conclusion: 我们的工作提出了一个有效的范式，用于生成可靠、结构化的长篇文档，为大语言模型在高风险、知识密集型领域中的应用铺平了道路。

Abstract: Large Language Models (LLMs) have shown remarkable prowess in text
generation, yet producing long-form, factual documents grounded in extensive
external knowledge bases remains a significant challenge. Existing "top-down"
methods, which first generate a hypothesis or outline and then retrieve
evidence, often suffer from a disconnect between the model's plan and the
available knowledge, leading to content fragmentation and factual inaccuracies.
To address these limitations, we propose a novel "bottom-up," data-driven
framework that inverts the conventional generation pipeline. Our approach is
predicated on a "Retrieval-First for Knowledge, Clustering for Structure"
strategy, which first establishes the "knowledge boundaries" of the source
corpus before any generative planning occurs. Specifically, we perform
exhaustive iterative retrieval from the knowledge base and then employ an
unsupervised clustering algorithm to organize the retrieved documents into
distinct "knowledge clusters." These clusters form an objective, data-driven
foundation that directly guides the subsequent generation of a hierarchical
outline and the final document content. This bottom-up process ensures that the
generated text is strictly constrained by and fully traceable to the source
material, proactively adapting to the finite scope of the knowledge base and
fundamentally mitigating the risk of hallucination. Experimental results on
both 14B and 32B parameter models demonstrate that our method achieves
performance comparable to or exceeding state-of-the-art baselines, and is
expected to demonstrate unique advantages in knowledge-constrained scenarios
that demand high fidelity and structural coherence. Our work presents an
effective paradigm for generating reliable, structured, long-form documents,
paving the way for more robust LLM applications in high-stakes,
knowledge-intensive domains.

</details>


### [24] [Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data](https://arxiv.org/abs/2509.12853)
*Kurt Micallef,Nizar Habash,Claudia Borg*

Main category: cs.CL

TL;DR: 本文研究了阿拉伯语资源如何通过跨语言增强技术支持马耳他语NLP，并证明了基于阿拉伯语的增强可以显著提升马耳他语NLP任务的效果。


<details>
  <summary>Details</summary>
Motivation: 马耳他语是一种独特的闪米特语言，受罗曼语和日耳曼语的影响较大，其正字法基于拉丁字母，与阿拉伯语等近亲语言存在差距。因此，探索阿拉伯语资源是否能支持马耳他语NLP是必要的。

Method: 研究了多种将阿拉伯语文本数据与马耳他语对齐的策略，包括各种转写方案和机器翻译（MT）方法，并引入了新的转写系统以更好地表示马耳他语正字法。

Result: 评估了这些增强方法对单语和多语模型的影响，并证明基于阿拉伯语的增强可以显著改善马耳他语NLP任务的表现。

Conclusion: 阿拉伯语资源可以通过跨语言增强技术支持马耳他语自然语言处理（NLP），并且基于阿拉伯语的增强可以显著提高马耳他语NLP任务的效果。

Abstract: Maltese is a unique Semitic language that has evolved under extensive
influence from Romance and Germanic languages, particularly Italian and
English. Despite its Semitic roots, its orthography is based on the Latin
script, creating a gap between it and its closest linguistic relatives in
Arabic. In this paper, we explore whether Arabic-language resources can support
Maltese natural language processing (NLP) through cross-lingual augmentation
techniques. We investigate multiple strategies for aligning Arabic textual data
with Maltese, including various transliteration schemes and machine translation
(MT) approaches. As part of this, we also introduce novel transliteration
systems that better represent Maltese orthography. We evaluate the impact of
these augmentations on monolingual and mutlilingual models and demonstrate that
Arabic-based augmentation can significantly benefit Maltese NLP tasks.

</details>


### [25] [Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents](https://arxiv.org/abs/2509.12876)
*Fuyu Xing,Zimu Wang,Wei Wang,Haiyang Zhang*

Main category: cs.CL

TL;DR: This paper evaluates the performance of LVLMs on the M2E2 task, highlighting their strengths and weaknesses in different settings and providing insights into their potential for improving Multimedia Event Extraction.


<details>
  <summary>Details</summary>
Motivation: The proliferation of multimedia content necessitates the development of effective Multimedia Event Extraction (M2E2) systems. Though Large Vision-Language Models (LVLMs) have shown strong cross-modal capabilities, their utility in the M2E2 task remains underexplored.

Method: We present the first systematic evaluation of representative LVLMs, including DeepSeek-VL2 and the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only, image-only, and cross-media subtasks, assessed under both few-shot prompting and fine-tuning settings.

Result: Few-shot LVLMs perform notably better on visual tasks but struggle significantly with textual tasks. Fine-tuning LVLMs with LoRA substantially enhances model performance. LVLMs exhibit strong synergy when combining modalities, achieving superior performance in cross-modal settings.

Conclusion: LVLMs exhibit strong synergy when combining modalities, achieving superior performance in cross-modal settings. However, there are persistent challenges in areas such as semantic precision, localization, and cross-modal grounding.

Abstract: The proliferation of multimedia content necessitates the development of
effective Multimedia Event Extraction (M2E2) systems. Though Large
Vision-Language Models (LVLMs) have shown strong cross-modal capabilities,
their utility in the M2E2 task remains underexplored. In this paper, we present
the first systematic evaluation of representative LVLMs, including DeepSeek-VL2
and the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only,
image-only, and cross-media subtasks, assessed under both few-shot prompting
and fine-tuning settings. Our key findings highlight the following valuable
insights: (1) Few-shot LVLMs perform notably better on visual tasks but
struggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA
substantially enhances model performance; and (3) LVLMs exhibit strong synergy
when combining modalities, achieving superior performance in cross-modal
settings. We further provide a detailed error analysis to reveal persistent
challenges in areas such as semantic precision, localization, and cross-modal
grounding, which remain critical obstacles for advancing M2E2 capabilities.

</details>


### [26] [The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations](https://arxiv.org/abs/2509.12886)
*Yubo Zhu,Dongrui Liu,Zecheng Lin,Wei Tong,Sheng Zhong,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了一种基于目标LLM隐藏表示的难度估计新方法，无需生成任何输出标记即可高效准确地估计难度，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于重复响应采样、辅助模型或微调目标模型本身，这可能会产生较高的计算成本或损害通用性。

Method: 我们提出了一种新方法，利用目标LLM产生的隐藏表示来估计难度。我们将标记级生成过程建模为马尔可夫链，并定义了一个价值函数来估计给定任何隐藏状态的预期输出质量。

Result: 我们在文本和多模态任务上的实验表明，我们的方法在难度估计上表现一致优于现有基线。此外，我们将难度估计应用于自适应推理策略，包括Self-Consistency、Best-of-N和Self-Refine，实现了更高的推理效率。

Conclusion: 我们的方法在难度估计上表现优于现有基线，并且可以提高推理效率。

Abstract: Estimating the difficulty of input questions as perceived by large language
models (LLMs) is essential for accurate performance evaluation and adaptive
inference. Existing methods typically rely on repeated response sampling,
auxiliary models, or fine-tuning the target model itself, which may incur
substantial computational costs or compromise generality. In this paper, we
propose a novel approach for difficulty estimation that leverages only the
hidden representations produced by the target LLM. We model the token-level
generation process as a Markov chain and define a value function to estimate
the expected output quality given any hidden state. This allows for efficient
and accurate difficulty estimation based solely on the initial hidden state,
without generating any output tokens. Extensive experiments across both textual
and multimodal tasks demonstrate that our method consistently outperforms
existing baselines in difficulty estimation. Moreover, we apply our difficulty
estimates to guide adaptive reasoning strategies, including Self-Consistency,
Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer
generated tokens.

</details>


### [27] [Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings](https://arxiv.org/abs/2509.12892)
*Shiyu Li,Yang Tang,Ruijie Liu,Shi-Zhe Chen,Xi Chen*

Main category: cs.CL

TL;DR: This paper introduces Conan-embedding-v2, a new 1.4B-parameter LLM trained from scratch and fine-tuned as a text embedder. It addresses the data and training gaps between LLMs and embedding models by adding news data and multilingual pairs for pretraining, proposing a cross-lingual retrieval dataset, introducing a soft-masking mechanism, and proposing a dynamic hard negative mining method. Conan-embedding-v2 achieves SOTA performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese MTEB.


<details>
  <summary>Details</summary>
Motivation: Previous work using LoRA to fine-tune existing LLMs is limited by the data and training gap between LLMs and embedding models.

Method: Adding news data and multilingual pairs for LLM pretraining, proposing a cross-lingual retrieval dataset, introducing a soft-masking mechanism, and proposing a dynamic hard negative mining method.

Result: Conan-embedding-v2, a new 1.4B-parameter LLM trained from scratch and fine-tuned as a text embedder, achieves SOTA performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese MTEB.

Conclusion: Conan-embedding-v2 achieves SOTA performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese MTEB.

Abstract: Large language models (LLMs) have recently demonstrated excellent performance
in text embedding tasks. Previous work usually use LoRA to fine-tune existing
LLMs, which are limited by the data and training gap between LLMs and embedding
models. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM
trained from scratch and fine-tuned as a text embedder. First, we add news data
and multilingual pairs for LLM pretraining to bridge the data gap. Based on
this, we propose a cross-lingual retrieval dataset that enables the LLM to
better integrate embeddings across different languages. Second, whereas LLMs
use a causal mask with token-level loss, embedding models use a bidirectional
mask with sentence-level loss. This training gap makes full fine-tuning less
effective than LoRA. We introduce a soft-masking mechanism to gradually
transition between these two types of masks, enabling the model to learn more
comprehensive representations. Based on this, we propose a dynamic hard
negative mining method that exposes the model to more difficult negative
examples throughout the training process. Being intuitive and effective, with
only approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA
performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese
MTEB (May 19, 2025).

</details>


### [28] [All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2509.12908)
*Caiqi Zhang,Chang Shu,Ehsan Shareghi,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的基于图的置信度估计方法，用于改进大型语言模型在推理任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要针对事实性问答任务设计，往往无法推广到推理任务中。

Method: 我们提出了一种无需训练的基于图的置信度估计方法，通过建模推理路径为有向图，并利用图属性如中心性、路径收敛性和路径权重来估计置信度。

Result: 实验表明，我们的方法在三个推理数据集上表现更好，并在两个下游任务中提升了性能。

Conclusion: 我们的方法在两个下游任务中表现出色，证明了其有效性。

Abstract: Confidence estimation is essential for the reliable deployment of large
language models (LLMs). Existing methods are primarily designed for factual QA
tasks and often fail to generalize to reasoning tasks. To address this gap, we
propose a set of training-free, graph-based confidence estimation methods
tailored to reasoning tasks. Our approach models reasoning paths as directed
graphs and estimates confidence by exploiting graph properties such as
centrality, path convergence, and path weighting. Experiments with two LLMs on
three reasoning datasets demonstrate improved confidence estimation and
enhanced performance on two downstream tasks.

</details>


### [29] [Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework](https://arxiv.org/abs/2509.12955)
*Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的框架，通过挖掘全文学术论文生成全面、结构化研究工作流程。该方法在NLP领域进行了案例研究，取得了良好的效果，并揭示了过去二十年中的关键方法论变化。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常仅提取片段化的程序组件，因此无法捕捉完整的研究工作流程。为了弥补这一差距，我们提出了一个端到端的框架，通过挖掘全文学术论文生成全面、结构化研究工作流程。

Method: 我们提出了一个端到端的框架，通过挖掘全文学术论文生成全面、结构化研究工作流程。在自然语言处理（NLP）领域进行案例研究，我们的段落中心方法首先使用带有SciBERT的正负学习来识别描述工作流程的段落，然后利用带有提示学习的Flan-T5从这些段落生成工作流程短语，并使用少量示例学习的ChatGPT将这些短语系统地分类为数据准备、数据处理和数据分析阶段。

Result: 我们的方法在NLP语料库中生成了可分析的工作流程，并揭示了过去二十年中的关键方法论变化，包括对数据分析的日益重视以及从特征工程到消融研究的转变。

Conclusion: 我们的工作提供了一个经过验证的技术框架，用于自动化的工作流程生成，并提供了新的、以过程为中心的视角，用于对不断演变的科学范式的实证研究。

Abstract: The automated generation of research workflows is essential for improving the
reproducibility of research and accelerating the paradigm of "AI for Science".
However, existing methods typically extract merely fragmented procedural
components and thus fail to capture complete research workflows. To address
this gap, we propose an end-to-end framework that generates comprehensive,
structured research workflows by mining full-text academic papers. As a case
study in the Natural Language Processing (NLP) domain, our paragraph-centric
approach first employs Positive-Unlabeled (PU) Learning with SciBERT to
identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772.
Subsequently, we utilize Flan-T5 with prompt learning to generate workflow
phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of
0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically
categorized into data preparation, data processing, and data analysis stages
using ChatGPT with few-shot learning, achieving a classification precision of
0.958. By mapping categorized phrases to their document locations in the
documents, we finally generate readable visual flowcharts of the entire
research workflows. This approach facilitates the analysis of workflows derived
from an NLP corpus and reveals key methodological shifts over the past two
decades, including the increasing emphasis on data analysis and the transition
from feature engineering to ablation studies. Our work offers a validated
technical framework for automated workflow generation, along with a novel,
process-oriented perspective for the empirical investigation of evolving
scientific paradigms. Source code and data are available at:
https://github.com/ZH-heng/research_workflow.

</details>


### [30] [Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models](https://arxiv.org/abs/2509.12960)
*Yuval Weiss,David Demitri Africa,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: 本研究首次系统地研究了ReLoRA在小型语言模型中的应用，发现其效果不如标准训练，并指出低秩更新策略在预训练中的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管参数高效的LoRA方法已经改变了大型语言模型的微调，但其在预训练中的扩展（如ReLoRA）仍不明确，尤其是在计算和环境成本较低的小型语言模型中。

Method: 本研究是对小型语言模型（11M-66M参数）中ReLoRA的首次系统研究，评估了性能和学习动态。

Result: 通过消融实验发现，ReLoRA在损失、Paloma困惑度和BLiMP方面通常比标准训练表现较差，且在更大的模型中差距更大。进一步分析表明，ReLoRA强化了小型模型中存在的秩缺陷。

Conclusion: 低秩更新策略可能难以转移到小型语言模型的预训练中，这凸显了在低计算环境中进行更多研究的必要性。

Abstract: Parameter-efficient methods such as LoRA have revolutionised the fine-tuning
of LLMs. Still, their extension to pretraining via ReLoRA is less well
understood, especially for small language models (SLMs), which offer lower
computational and environmental costs. This work is the first systematic study
of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and
learning dynamics. Through ablation experiments, we find that ReLoRA generally
performs worse than standard training on loss, Paloma perplexity and BLiMP,
with the gap widening for the larger models. Further analysis of the learning
dynamics of the models indicates that ReLoRA reinforces the rank deficiencies
found in smaller models. These results indicate that low-rank update strategies
may not transfer easily to SLM pretraining, highlighting the need for more
research in the low-compute regime.

</details>


### [31] [Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews](https://arxiv.org/abs/2509.12961)
*Chenye Zou,Xingyue Wen,Tianyi Hu,Qian Janice Wang,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 本文研究了跨文化葡萄酒评论适应问题，提出了三个文化导向的标准来评估翻译评论与目标文化读者的自然程度，并发现当前模型在处理文化内容时存在困难。


<details>
  <summary>Details</summary>
Motivation: 最近大型语言模型（LLMs）的进步为文化意识的语言任务打开了大门。适应葡萄酒评论跨中英文，超越了字面翻译，结合了区域口味偏好和文化特定的风味描述。

Method: 我们引入了适应葡萄酒评论的问题，超越了字面翻译，结合了区域口味偏好和文化特定的风味描述。我们编译了第一个平行语料库的专业评论，并对神经机器翻译基线和最先进的LLM进行了基准测试。

Result: 我们的分析显示，当前模型难以捕捉文化细微差别，特别是在跨不同文化翻译葡萄酒描述时。

Conclusion: 当前模型在捕捉文化细微差别方面存在困难，尤其是在跨不同文化翻译葡萄酒描述时。这突显了翻译模型在处理文化内容时的挑战和局限性。

Abstract: Recent advances in large language models (LLMs) have opened the door to
culture-aware language tasks. We introduce the novel problem of adapting wine
reviews across Chinese and English, which goes beyond literal translation by
incorporating regional taste preferences and culture-specific flavor
descriptors. In a case study on cross-cultural wine review adaptation, we
compile the first parallel corpus of professional reviews, containing 8k
Chinese and 16k Anglophone reviews. We benchmark both
neural-machine-translation baselines and state-of-the-art LLMs with automatic
metrics and human evaluation. For the latter, we propose three culture-oriented
criteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness
-- to assess how naturally a translated review resonates with target-culture
readers. Our analysis shows that current models struggle to capture cultural
nuances, especially in translating wine descriptions across different cultures.
This highlights the challenges and limitations of translation models in
handling cultural content.

</details>


### [32] [SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data](https://arxiv.org/abs/2509.12994)
*Jian Gao,Fufangchen Zhao,Yiyang Zhang,Danfeng Yan*

Main category: cs.CL

TL;DR: 本文提出了一种名为 SitLLM 的轻量级多模态框架，用于实现精细的坐姿理解和个性化的健康导向响应生成。


<details>
  <summary>Details</summary>
Motivation: 现有坐姿监测系统在细粒度识别和个性化反馈方面存在不足，因此需要一种更精确和个性化的解决方案。

Method: SitLLM 包含三个关键组件：(1) 高斯鲁棒传感器嵌入模块，(2) 提示驱动的跨模态对齐模块，(3) 多上下文提示模块。

Result: SitLLM 通过结合压力传感和 LLM，实现了对坐姿的精细理解，并能生成个性化的健康导向响应。

Conclusion: SitLLM 是一种轻量级多模态框架，通过将柔性压力传感与大型语言模型（LLMs）相结合，实现了精细的姿势理解和个性化的健康导向响应生成。

Abstract: Poor sitting posture is a critical yet often overlooked factor contributing
to long-term musculoskeletal disorders and physiological dysfunctions. Existing
sitting posture monitoring systems, although leveraging visual, IMU, or
pressure-based modalities, often suffer from coarse-grained recognition and
lack the semantic expressiveness necessary for personalized feedback. In this
paper, we propose \textbf{SitLLM}, a lightweight multimodal framework that
integrates flexible pressure sensing with large language models (LLMs) to
enable fine-grained posture understanding and personalized health-oriented
response generation. SitLLM comprises three key components: (1) a
\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps
into spatial patches and injects local noise perturbations for robust feature
extraction; (2) a \textit{Prompt-Driven Cross-Modal Alignment Module} that
reprograms sensor embeddings into the LLM's semantic space via multi-head
cross-attention using the pre-trained vocabulary embeddings; and (3) a
\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,
statistical-level, and semantic-level contextual information to guide
instruction comprehension.

</details>


### [33] [Multi-Model Synthetic Training for Mission-Critical Small Language Models](https://arxiv.org/abs/2509.13047)
*Nolan Platt,Pragyansmita Nayak*

Main category: cs.CL

TL;DR: 本文介绍了一种利用大型语言模型作为一次性教师的方法，以显著降低成本并提高专业领域任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在许多领域表现出色，但其在专业领域的应用受到领域特定训练数据稀缺性和复杂性的限制。我们需要一种更经济有效的方法来利用LLMs进行专业领域的任务。

Method: 我们提出了一种新方法，使用大型语言模型（LLMs）作为一次性教师，而不是直接用于推理，从而实现了261倍的成本降低。通过多模型生成（GPT-4o和o3-mini），将32亿条自动识别系统（AIS）船舶跟踪记录转换为21,543个合成问答对，防止过拟合并确保准确推理。

Result: 经过微调的Qwen2.5-7B模型在海事任务上达到了75%的准确率，同时比使用更大的模型进行推理要便宜得多。

Conclusion: 我们的工作为专门的AI应用中的合成数据集生成做出了贡献，并提出了一个高度可复制的框架，适用于手动注释不可行的领域。此外，我们的方法在海事安全、安全操作和船舶交通管理系统等领域有直接的应用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
many domains, yet their application to specialized fields remains constrained
by the scarcity and complexity of domain-specific training data. We present a
novel approach that achieves a 261x cost reduction for maritime intelligence by
using LLMs as one-time teachers rather than using them directly for inference.
Our method transforms 3.2 billion Automatic Identification System (AIS) vessel
tracking records into 21,543 synthetic question and answer pairs through
multi-model generation (GPT-4o and o3-mini), preventing overfitting and
ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves
75% accuracy on maritime tasks, while being substantially cheaper than using a
larger model for inference. We show that smaller, cheaper models -- when fine
tuned properly -- can provide similar accuracy compared to larger models that
are prohibitively expensive. Our work contributes to the growing field of
synthetic dataset generation for specialized AI applications and presents a
highly reproducible framework for domains where manual annotation is
infeasible. Beyond expanding research in the growing field of specialized small
language models, our approach has immediate applications in maritime safety,
security operations, and vessel traffic management systems in various
industries.

</details>


### [34] [Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO](https://arxiv.org/abs/2509.13081)
*Francesco Pappone,Ruggero Marino Lazzaroni,Federico Califano,Niccolò Gentile,Roberto Marras*

Main category: cs.CL

TL;DR: 本文提出了一种新的奖励塑造方法，利用小型高效的仅编码器变压器作为语义奖励模型，以提高生成解释的忠实性和清晰度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成类似人类的文本方面表现出色，但将它们的输出与复杂的定性目标（如教学合理性）对齐仍然是一个重大挑战。标准的强化学习技术通常依赖于缓慢且昂贵的LLM-as-a-judge评估或脆弱的基于关键词的指标（如ROUGE），这些指标无法捕捉高质量解释的语义本质。

Method: 我们引入了一种新的奖励塑造方法，在Group Relative Policy Optimisation (GRPO)框架内使用了一个小型、高效的仅编码器的变压器作为语义奖励模型。该模型基于生成的解释和真实参考之间的余弦相似性提供密集的、语义丰富的奖励信号，引导策略生成不仅事实正确而且在结构和概念上与专家推理一致的解释。

Result: 我们在意大利医学院入学考试的任务中应用了这种方法，遵循标准的领域自适应连续预训练（CPT）和监督微调（SFT）。结果表明，使用我们提出的语义奖励的GRPO在解释的忠实性和清晰度上显著优于强大的SFT基线。

Conclusion: 我们的方法在解释的忠实性和清晰度上显著优于强大的SFT基线，展示了使用轻量级编码器模型在复杂生成任务中进行细微奖励塑造的潜力。

Abstract: While Large Language Models (LLMs) excel at generating human-like text,
aligning their outputs with complex, qualitative goals like pedagogical
soundness remains a significant challenge. Standard reinforcement learning
techniques often rely on slow and expensive LLM-as-a-judge evaluations or on
brittle, keyword-based metrics like ROUGE, which fail to capture the semantic
essence of a high-quality explanation. In this work, we introduce a novel
approach to reward shaping within the Group Relative Policy Optimisation (GRPO)
framework. Our central contribution is the use of a small, efficient
encoder-only transformer as a semantic reward model. This model provides a
dense, semantically rich reward signal based on the cosine similarity between a
generated explanation and a ground-truth reference, guiding the policy towards
explanations that are not just factually correct but also structurally and
conceptually aligned with expert reasoning. We apply this method to the task of
training a model for the Italian medical-school entrance examinations,
following standard domain-adaptive continued pre-training (CPT) and supervised
fine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic
reward significantly improves explanation faithfulness and clarity over a
strong SFT baseline, showcasing the power of using lightweight encoder models
for nuanced reward shaping in complex generation tasks

</details>


### [35] [Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning](https://arxiv.org/abs/2509.13127)
*Sijia Cui,Shuai Xu,Aiyao He,Yanna Wang,Bo Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为PLAP的计划框架，用于提高基于LLM的代理在长期环境中进行有效规划的能力。该框架包含三个关键组件，并在MicroRTS游戏中进行了测试，结果显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可靠动作或依赖专家经验将高层次任务转化为具体动作序列方面存在不足。因此，需要一种新的方法来解决这些问题。

Method: PLAP方法包含三个关键组件：(1) 包含特定于环境的参数化技能的技能库，(2) 由LLMs驱动的技能规划器，以及(3) 将参数化技能转换为可执行动作序列的技能执行器。

Result: GPT-4o驱动的PLAP在零样本设置中表现优于80%的基线代理，而Qwen2-72B驱动的PLAP在精心设计的少量示例下超越了顶级脚本代理CoacAI。

Conclusion: PLAP框架在长期环境中的有效性得到了实验结果的证明，并且通过设计全面的评估指标，释放了一个LLM排行榜，以评估长期环境中的技能规划能力。

Abstract: Recent advancements in Large Language Models(LLMs) have led to the
development of LLM-based AI agents. A key challenge is the creation of agents
that can effectively ground themselves in complex, adversarial long-horizon
environments. Existing methods mainly focus on (1) using LLMs as policies to
interact with the environment through generating low-level feasible actions,
and (2) utilizing LLMs to generate high-level tasks or language guides to
stimulate action generation. However, the former struggles to generate reliable
actions, while the latter relies heavily on expert experience to translate
high-level tasks into specific action sequences. To address these challenges,
we introduce the Plan with Language, Act with Parameter (PLAP) planning
framework that facilitates the grounding of LLM-based agents in long-horizon
environments. The PLAP method comprises three key components: (1) a skill
library containing environment-specific parameterized skills, (2) a skill
planner powered by LLMs, and (3) a skill executor converting the parameterized
skills into executable action sequences. We implement PLAP in MicroRTS, a
long-horizon real-time strategy game that provides an unfamiliar and
challenging environment for LLMs. The experimental results demonstrate the
effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting
outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully
crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.
Additionally, we design comprehensive evaluation metrics and test 6
closed-source and 2 open-source LLMs within the PLAP framework, ultimately
releasing an LLM leaderboard ranking long-horizon skill planning ability. Our
code is available at https://github.com/AI-Research-TeamX/PLAP.

</details>


### [36] [LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals](https://arxiv.org/abs/2509.13154)
*Jinxin Li,Gang Tu,ShengYu Cheng,Junjie Hu,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: HSAD is a novel hallucination detection framework that models the temporal dynamics of hidden representations during autoregressive generation using frequency-domain analysis, achieving significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Hallucination remains a critical barrier for deploying large language models (LLMs) in reliability-sensitive applications. Existing detection methods are constrained by external knowledge coverage or fail to capture deviations in reasoning dynamics, limiting their effectiveness and robustness.

Method: HSAD models the temporal dynamics of hidden representations during autoregressive generation by constructing hidden-layer signals, applying Fast Fourier Transform (FFT) to obtain frequency-domain representations, and extracting the strongest non-DC frequency component as spectral features. It also identifies optimal observation points for effective and reliable detection.

Result: HSAD achieves over 10 percentage points improvement compared to prior state-of-the-art methods across multiple benchmarks, including TruthfulQA.

Conclusion: HSAD establishes a new paradigm for robust hallucination detection in LLMs.

Abstract: Hallucination remains a critical barrier for deploying large language models
(LLMs) in reliability-sensitive applications. Existing detection methods
largely fall into two categories: factuality checking, which is fundamentally
constrained by external knowledge coverage, and static hidden-state analysis,
that fails to capture deviations in reasoning dynamics. As a result, their
effectiveness and robustness remain limited. We propose HSAD (Hidden Signal
Analysis-based Detection), a novel hallucination detection framework that
models the temporal dynamics of hidden representations during autoregressive
generation. HSAD constructs hidden-layer signals by sampling activations across
layers, applies Fast Fourier Transform (FFT) to obtain frequency-domain
representations, and extracts the strongest non-DC frequency component as
spectral features. Furthermore, by leveraging the autoregressive nature of
LLMs, HSAD identifies optimal observation points for effective and reliable
detection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over
10 percentage points improvement compared to prior state-of-the-art methods. By
integrating reasoning-process modeling with frequency-domain analysis, HSAD
establishes a new paradigm for robust hallucination detection in LLMs.

</details>


### [37] [The Few-shot Dilemma: Over-prompting Large Language Models](https://arxiv.org/abs/2509.13196)
*Yongjian Tang,Doruk Tuncel,Christian Koerner,Thomas Runkler*

Main category: cs.CL

TL;DR: 研究发现过多的领域特定示例可能会降低某些大型语言模型的性能，通过结合TF-IDF选择和分层少样本示例的方法，可以在更少示例的情况下实现优越性能，避免过度提示问题，并在分类功能性和非功能性需求方面超越了最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在调查过度提示现象，即过多的示例在提示中导致大型语言模型性能下降的问题。同时，考虑到大型语言模型在软件工程和需求分析中的应用趋势，研究希望找到一种有效的少样本学习方法，以避免过度提示问题并提高性能。

Method: 研究提出了一种提示框架，利用三种标准的少样本选择方法（随机采样、语义嵌入和TF-IDF向量），并在多个大型语言模型上评估这些方法。此外，还对两个真实世界的软件需求分类数据集进行了实验，通过逐步增加TF-IDF选择和分层少样本示例的数量，确定了每个大型语言模型的最佳数量。

Result: 实验结果表明，将过多的领域特定示例纳入提示中可能会降低某些大型语言模型的性能，这与之前的结论相矛盾。通过结合TF-IDF选择和分层少样本示例的方法，可以在更少示例的情况下实现优越性能，避免过度提示问题，并在分类功能性和非功能性需求方面超越了最先进的方法。

Conclusion: 研究发现，过多的领域特定示例可能会降低某些大型语言模型的性能，这与之前认为更多相关示例总是有益的结论相矛盾。通过结合TF-IDF选择和分层少样本示例的方法，可以在更少示例的情况下实现优越性能，避免过度提示问题，并在分类功能性和非功能性需求方面超越了最先进的方法。

Abstract: Over-prompting, a phenomenon where excessive examples in prompts lead to
diminished performance in Large Language Models (LLMs), challenges the
conventional wisdom about in-context few-shot learning. To investigate this
few-shot dilemma, we outline a prompting framework that leverages three
standard few-shot selection methods - random sampling, semantic embedding, and
TF-IDF vectors - and evaluate these methods across multiple LLMs, including
GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.
Our experimental results reveal that incorporating excessive domain-specific
examples into prompts can paradoxically degrade performance in certain LLMs,
which contradicts the prior empirical conclusion that more relevant few-shot
examples universally benefit LLMs. Given the trend of LLM-assisted software
engineering and requirement analysis, we experiment with two real-world
software requirement classification datasets. By gradually increasing the
number of TF-IDF-selected and stratified few-shot examples, we identify their
optimal quantity for each LLM. This combined approach achieves superior
performance with fewer examples, avoiding the over-prompting problem, thus
surpassing the state-of-the-art by 1% in classifying functional and
non-functional requirements.

</details>


### [38] [Evaluating LLM Alignment on Personality Inference from Real-World Interview Data](https://arxiv.org/abs/2509.13244)
*Jianfeng Zhu,Julina Maharjan,Xinyu Li,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在心理性格特征理解方面的表现，发现它们与实际心理评估的相关性较低，提示未来需要改进提示策略和模型对齐方法。


<details>
  <summary>Details</summary>
Motivation: 在生态有效的对话设置中，大型语言模型解释人类个性特征的能力尚未得到探索，特别是在需要微妙心理理解的应用中，如情感支持代理、顾问和决策辅助工具。

Method: 我们引入了一个新的基准测试，包括半结构化访谈转录本和经过验证的连续大五种性格评分。使用这个数据集，我们系统地评估了三种范式下的模型性能：(1) GPT-4.1 Mini 的零样本和思维链提示，(2) 应用于 RoBERTa 和 Meta-LLaMA 架构的基于 LoRA 的微调，以及 (3) 使用预训练 BERT 和 OpenAI 的 text-embedding-3-small 的静态嵌入进行回归。

Result: 所有模型预测与真实性格特征之间的皮尔逊相关性均低于 0.26，表明当前大型语言模型与经过验证的心理学概念之间的对齐程度有限。思维链提示相对于零样本仅带来很小的提升，表明个性推理更多依赖于潜在语义表示而非显式推理。

Conclusion: 当前大型语言模型与经过验证的心理学概念之间的对齐程度有限，这突显了将大型语言模型与复杂的人类属性对齐的挑战，并促使未来的研究集中在特定于特质的提示、上下文感知建模和对齐导向的微调上。

Abstract: Large Language Models (LLMs) are increasingly deployed in roles requiring
nuanced psychological understanding, such as emotional support agents,
counselors, and decision-making assistants. However, their ability to interpret
human personality traits, a critical aspect of such applications, remains
unexplored, particularly in ecologically valid conversational settings. While
prior work has simulated LLM "personas" using discrete Big Five labels on
social media data, the alignment of LLMs with continuous, ground-truth
personality assessments derived from natural interactions is largely
unexamined. To address this gap, we introduce a novel benchmark comprising
semi-structured interview transcripts paired with validated continuous Big Five
trait scores. Using this dataset, we systematically evaluate LLM performance
across three paradigms: (1) zero-shot and chain-of-thought prompting with
GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA
architectures, and (3) regression using static embeddings from pretrained BERT
and OpenAI's text-embedding-3-small. Our results reveal that all Pearson
correlations between model predictions and ground-truth personality traits
remain below 0.26, highlighting the limited alignment of current LLMs with
validated psychological constructs. Chain-of-thought prompting offers minimal
gains over zero-shot, suggesting that personality inference relies more on
latent semantic representation than explicit reasoning. These findings
underscore the challenges of aligning LLMs with complex human attributes and
motivate future work on trait-specific prompting, context-aware modeling, and
alignment-oriented fine-tuning.

</details>


### [39] [ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement](https://arxiv.org/abs/2509.13282)
*Ali Salamatian,Amirhossein Abaskohi,Wan-Cyuan Fan,Mir Rayat Imtiaz Hossain,Leonid Sigal,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 本文介绍了ChartGaze，一个新的人类注视数据集，并提出了一种基于注视的注意力精炼方法，以提高图表问答任务中大型视觉-语言模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型（LVLMs）在图表问答（CQA）任务上取得了进展，但该任务仍然具有挑战性，特别是在模型关注图表的无关区域时。

Method: 我们提出了一个注视引导的注意力精炼方法，将图像-文本注意力与人类注视对齐。

Result: 我们的方法提高了答案的准确性以及注意力对齐度，在多个模型中实现了高达2.56个百分点的提升。

Conclusion: 这些结果展示了将人类注视纳入提高以图表为中心的LVLM的推理质量和可解释性的潜力。

Abstract: Charts are a crucial visual medium for communicating and representing
information. While Large Vision-Language Models (LVLMs) have made progress on
chart question answering (CQA), the task remains challenging, particularly when
models attend to irrelevant regions of the chart. In this work, we present
ChartGaze, a new eye-tracking dataset that captures human gaze patterns during
chart reasoning tasks. Through a systematic comparison of human and model
attention, we find that LVLMs often diverge from human gaze, leading to reduced
interpretability and accuracy. To address this, we propose a gaze-guided
attention refinement that aligns image-text attention with human fixations. Our
approach improves both answer accuracy and attention alignment, yielding gains
of up to 2.56 percentage points across multiple models. These results
demonstrate the promise of incorporating human gaze to enhance both the
reasoning quality and interpretability of chart-focused LVLMs.

</details>


### [40] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/abs/2509.13309)
*Zile Qiao,Guoxin Chen,Xuanzhong Chen,Donglei Yu,Wenbiao Yin,Xinyu Wang,Zhen Zhang,Baixuan Li,Huifeng Yin,Kuan Li,Rui Min,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebResearcher is a novel framework for building AI agents that autonomously discover and synthesize knowledge from external sources. It introduces two key components: an iterative deep-research paradigm and a scalable data synthesis engine. The framework significantly enhances tool-use capabilities and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the context suffocation and noise contamination that plague existing mono-contextual approaches, and to bridge the gap between passive knowledge recall and active knowledge construction.

Method: WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, and WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation.

Result: Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems.

Conclusion: WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems.

Abstract: Recent advances in deep-research systems have demonstrated the potential for
AI agents to autonomously discover and synthesize knowledge from external
sources. In this paper, we introduce WebResearcher, a novel framework for
building such agents through two key components: (1) WebResearcher, an
iterative deep-research paradigm that reformulates deep research as a Markov
Decision Process, where agents periodically consolidate findings into evolving
reports while maintaining focused workspaces, overcoming the context
suffocation and noise contamination that plague existing mono-contextual
approaches; and (2) WebFrontier, a scalable data synthesis engine that
generates high-quality training data through tool-augmented complexity
escalation, enabling systematic creation of research tasks that bridge the gap
between passive knowledge recall and active knowledge construction. Notably, we
find that the training data from our paradigm significantly enhances tool-use
capabilities even for traditional mono-contextual methods. Furthermore, our
paradigm naturally scales through parallel thinking, enabling concurrent
multi-agent exploration for more comprehensive conclusions. Extensive
experiments across 6 challenging benchmarks demonstrate that WebResearcher
achieves state-of-the-art performance, even surpassing frontier proprietary
systems.

</details>


### [41] [Scaling Agents via Continual Pre-training](https://arxiv.org/abs/2509.13310)
*Liangcai Su,Zhen Zhang,Guangyu Li,Zhuo Chen,Chenxi Wang,Maojia Song,Xinyu Wang,Kuan Li,Jialong Wu,Xuanzhong Chen,Zile Qiao,Zhongwang Zhang,Huifeng Yin,Shihao Cai,Runnan Fang,Zhengwei Tao,Wenbiao Yin,Chenxiong Qian,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，通过引入Agentic Continual Pre-training来构建强大的代理基础模型，并开发了一个名为AgentFounder的深度研究代理模型，在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于通用基础模型的后训练方法在代理任务中表现不佳，特别是在开源实现中。这是因为缺乏稳健的代理基础模型，导致模型在后训练过程中需要同时学习多种代理行为并与其对齐，从而产生根本性的优化矛盾。

Method: 本文提出了将Agentic Continual Pre-training (Agentic CPT) 引入深度研究代理训练流程的方法，以构建强大的代理基础模型。

Result: AgentFounder-30B在10个基准测试中表现出色，包括BrowseComp-en（39.9%）、BrowseComp-zh（43.3%）和HLE（31.5% Pass@1）。

Conclusion: 本文提出了一种名为AgentFounder的深度研究代理模型，并在10个基准测试中实现了最先进的性能，同时保留了强大的工具使用能力。

Abstract: Large language models (LLMs) have evolved into agentic systems capable of
autonomous tool use and multi-step reasoning for complex problem-solving.
However, post-training approaches building upon general-purpose foundation
models consistently underperform in agentic tasks, particularly in open-source
implementations. We identify the root cause: the absence of robust agentic
foundation models forces models during post-training to simultaneously learn
diverse agentic behaviors while aligning them to expert demonstrations, thereby
creating fundamental optimization tensions. To this end, we are the first to
propose incorporating Agentic Continual Pre-training (Agentic CPT) into the
deep research agents training pipeline to build powerful agentic foundational
models. Based on this approach, we develop a deep research agent model named
AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve
state-of-the-art performance while retains strong tool-use ability, notably
39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.

</details>


### [42] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311)
*Runnan Fang,Shihao Cai,Baixuan Li,Jialong Wu,Guangyu Li,Wenbiao Yin,Xinyu Wang,Xiaobin Wang,Liangcai Su,Zhen Zhang,Shibin Wu,Zhengwei Tao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的框架，用于构建异构环境，并采用两阶段的代理微调策略，以提升模型的function-calling能力。


<details>
  <summary>Details</summary>
Motivation: 为了推动通用agentic intelligence的发展，需要扩展环境以提高function-calling能力。

Method: 我们设计了一个可扩展的框架，自动构建异构环境，并采用两阶段的代理微调策略，首先赋予代理基本的agentic能力，然后针对特定领域进行专业化。

Result: 实验结果表明，AgentScaler模型在tau-bench、tau2-Bench和ACEBench等基准测试中表现优异，显著增强了模型的function-calling能力。

Conclusion: 本文提出的AgentScaler模型在agentic benchmarks上表现出色，显著提升了模型的function-calling能力。

Abstract: Advanced agentic intelligence is a prerequisite for deploying Large Language
Models in practical, real-world applications. Diverse real-world APIs demand
precise, robust function-calling intelligence, which needs agents to develop
these capabilities through interaction in varied environments. The breadth of
function-calling competence is closely tied to the diversity of environments in
which agents are trained. In this work, we scale up environments as a step
towards advancing general agentic intelligence. This gives rise to two central
challenges: (i) how to scale environments in a principled manner, and (ii) how
to effectively train agentic capabilities from experiences derived through
interactions with these environments. To address these, we design a scalable
framework that automatically constructs heterogeneous environments that are
fully simulated, systematically broadening the space of function-calling
scenarios. We further adapt a two-phase agent fine-tuning strategy: first
endowing agents with fundamental agentic capabilities, then specializing them
for domain-specific contexts. Extensive experiments on agentic benchmarks,
tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,
AgentScaler, significantly enhances the function-calling capability of models.

</details>


### [43] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/abs/2509.13312)
*Zijian Li,Xin Guan,Bo Zhang,Shen Huang,Houquan Zhou,Shaopeng Lai,Ming Yan,Yong Jiang,Pengjun Xie,Fei Huang,Jun Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文介绍了WebWeaver，这是一种新颖的双代理框架，用于解决开放性深度研究（OEDR）中的挑战。该框架通过模拟人类的研究过程，有效缓解了长上下文问题，并在多个基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 当前的方法受到双重限制：静态研究流程将规划与证据获取分开，以及一次生成范式容易出现长上下文失败问题，如“中间丢失”和幻觉。

Method: 我们引入了WebWeaver，这是一种新颖的双代理框架，模拟人类的研究过程。规划者在动态循环中迭代地将证据获取与大纲优化相结合，以生成一个全面的、基于来源的大纲，链接到证据记忆库。然后，写作者执行分层检索和写作过程，逐部分编写报告。

Result: 我们的框架在主要的OEDR基准测试中建立了新的最先进水平，包括DeepResearch Bench、DeepConsult和DeepResearchGym。

Conclusion: 我们的框架在主要的OEDR基准测试中建立了新的最先进水平，包括DeepResearch Bench、DeepConsult和DeepResearchGym。这些结果验证了我们以人类为中心的迭代方法，表明自适应规划和聚焦综合对于生成高质量、可靠和结构良好的报告至关重要。

Abstract: This paper tackles open-ended deep research (OEDR), a complex challenge where
AI agents must synthesize vast web-scale information into insightful reports.
Current approaches are plagued by dual-fold limitations: static research
pipelines that decouple planning from evidence acquisition and one-shot
generation paradigms that easily suffer from long-context failure issues like
"loss in the middle" and hallucinations. To address these challenges, we
introduce WebWeaver, a novel dual-agent framework that emulates the human
research process. The planner operates in a dynamic cycle, iteratively
interleaving evidence acquisition with outline optimization to produce a
comprehensive, source-grounded outline linking to a memory bank of evidence.
The writer then executes a hierarchical retrieval and writing process,
composing the report section by section. By performing targeted retrieval of
only the necessary evidence from the memory bank for each part, it effectively
mitigates long-context issues. Our framework establishes a new state-of-the-art
across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and
DeepResearchGym. These results validate our human-centric, iterative
methodology, demonstrating that adaptive planning and focused synthesis are
crucial for producing high-quality, reliable, and well-structured reports.

</details>


### [44] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/abs/2509.13313)
*Xixi Wu,Kuan Li,Yida Zhao,Liwen Zhang,Litu Ou,Huifeng Yin,Zhongwang Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Minhao Cheng,Shuai Wang,Hong Cheng,Jingren Zhou*

Main category: cs.CL

TL;DR: ReSum是一种新的范式，通过周期性上下文摘要实现无限探索，显著提升了基于大型语言模型的网络代理在复杂查询中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ReAct范式由于上下文窗口限制，在处理涉及多个实体、复杂关系和高不确定性的复杂查询时效率低下，需要大量的搜索周期，导致上下文预算迅速耗尽。

Method: 引入了ReSum范式，通过定期上下文摘要将交互历史转化为紧凑的推理状态，并提出了ReSum-GRPO方法，结合GRPO与分段轨迹训练和优势广播。

Result: 在三个基准测试中进行的广泛实验表明，ReSum在ReAct基础上平均绝对提高了4.5%，经过ReSum-GRPO训练后提升高达8.2%。WebResummer-30B在仅使用1K训练样本的情况下，在BrowseComp-zh和BrowseComp-en上分别达到了33.3%和18.3%的Pass@1，超过了现有开源网络代理。

Conclusion: ReSum通过周期性上下文摘要实现了无限探索，显著提升了基于大型语言模型的网络代理在复杂查询中的性能。

Abstract: Large Language Model (LLM)-based web agents demonstrate strong performance on
knowledge-intensive tasks but are hindered by context window limitations in
paradigms like ReAct. Complex queries involving multiple entities, intertwined
relationships, and high uncertainty demand extensive search cycles that rapidly
exhaust context budgets before reaching complete solutions. To overcome this
challenge, we introduce ReSum, a novel paradigm that enables indefinite
exploration through periodic context summarization. ReSum converts growing
interaction histories into compact reasoning states, maintaining awareness of
prior discoveries while bypassing context constraints. For paradigm adaptation,
we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and
advantage broadcasting to familiarize agents with summary-conditioned
reasoning. Extensive experiments on web agents of varying scales across three
benchmarks demonstrate that ReSum delivers an average absolute improvement of
4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO
training. Notably, with only 1K training samples, our WebResummer-30B (a
ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on
BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web
agents.

</details>


### [45] [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)
*Millicent Li,Alberto Mario Ceballos Arroyo,Giordano Rogers,Naomi Saphra,Byron C. Wallace*

Main category: cs.CL

TL;DR: 研究发现，当前的激活解释方法可能更多地反映了解释器语言模型的知识，而不是目标模型的内部运作，因此需要更严格的评估基准。


<details>
  <summary>Details</summary>
Motivation: 我们想确定激活解释方法是否真的提供了关于目标模型内部工作原理的特权知识，或者它们是否只是传达了关于输入的信息。

Method: 我们评估了流行的解释方法，并进行了受控实验，以检查解释是否反映了生成它们的解释器语言模型的参数知识，而不是目标语言模型的激活。

Result: 我们发现这些方法在没有访问目标模型内部的情况下就能在基准测试中取得成功，这表明这些数据集不适合评估解释方法。此外，我们的实验显示，解释通常反映了生成它们的解释器语言模型的参数知识，而不是目标语言模型的激活。

Conclusion: 我们的结果表明，需要有针对性的基准测试和实验控制来严格评估解释方法是否能提供关于大语言模型操作的有意义见解。

Abstract: Recent interpretability methods have proposed to translate LLM internal
representations into natural language descriptions using a second verbalizer
LLM. This is intended to illuminate how the target model represents and
operates on inputs. But do such activation verbalization approaches actually
provide privileged knowledge about the internal workings of the target model,
or do they merely convey information about its inputs? We critically evaluate
popular verbalization methods across datasets used in prior work and find that
they succeed at benchmarks without any access to target model internals,
suggesting that these datasets are not ideal for evaluating verbalization
methods. We then run controlled experiments which reveal that verbalizations
often reflect the parametric knowledge of the verbalizer LLM which generated
them, rather than the activations of the target LLM being decoded. Taken
together, our results indicate a need for targeted benchmarks and experimental
controls to rigorously assess whether verbalization methods provide meaningful
insights into the operations of LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: 本文介绍了一种新的LLM-Assisted route Planning (LLMAP)系统，该系统结合了LLM-as-Parser和多步图构造与迭代搜索算法，在多个约束条件下实现了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的研究方法在处理大量地图数据或理解自然语言偏好方面存在局限性，同时面临用户时空分布高度异质性和不可预测性的挑战。

Method: LLMAP系统采用LLM-as-Parser来理解自然语言、识别任务和提取用户偏好，并结合多步图构造与迭代搜索(MSGS)算法作为底层求解器进行最优路径查找。

Result: 通过在14个国家和27个城市的1000个路由提示上进行广泛实验，结果表明该方法在多个约束条件下表现出色。

Conclusion: 本文提出了一种新的LLM-Assisted route Planning (LLMAP)系统，该系统在多个约束条件下表现出优越的性能。

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [47] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 本文提出了一种新的分解方法，通过结构化交互摘要和微调模型进行意图提取，以提高资源受限模型的意图理解能力。


<details>
  <summary>Details</summary>
Motivation: 从UI交互轨迹中理解用户意图是智能代理开发中的一个具有挑战性但至关重要的领域。虽然大规模的数据中心多模态大语言模型（MLLMs）具有处理此类序列复杂性的能力，但可以在设备上运行的小型模型在准确的意图推理方面存在困难。

Method: 本文引入了一种新的分解方法：首先进行结构化交互摘要，从每个用户操作中捕获关键信息；其次，使用微调模型对聚合摘要进行意图提取。

Result: 该方法提高了资源受限模型的意图理解能力，甚至超越了大型MLLMs的基础性能。

Conclusion: 本文提出的方法在资源受限的模型中提高了意图理解，甚至超过了大型MLLMs的基础性能。

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [48] [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
*Aaron Baughman,Gozde Akay,Eduardo Morales,Rahul Agarwal,Preetika Srivastava*

Main category: cs.AI

TL;DR: Match Chat是一个基于生成式人工智能和计算技术的实时网球赛事助手系统，能够提供准确且快速的查询响应，具有高可用性和良好的用户体验。


<details>
  <summary>Details</summary>
Motivation: 为了提升网球粉丝的体验，提供实时、准确的比赛相关查询响应，设计了一个智能助手系统。

Method: Match Chat系统基于面向代理的架构（AOA），结合规则引擎、预测模型和代理来预处理和优化用户查询，然后将其传递给生成式AI组件。

Result: Match Chat系统在2025年温布尔登锦标赛和美国公开赛上部署，具有92.83%的回答准确率，平均响应时间为6.25秒，并支持每秒120个请求，同时保持了100%的正常运行时间。

Conclusion: 本文介绍了Match Chat系统，该系统通过结合生成式人工智能和计算技术，为网球比赛提供实时、准确的查询响应，展示了其在动态环境中的可扩展性和可靠性。

Abstract: We present Match Chat, a real-time, agent-driven assistant designed to
enhance the tennis fan experience by delivering instant, accurate responses to
match-related queries. Match Chat integrates Generative Artificial Intelligence
(GenAI) with Generative Computing (GenComp) techniques to synthesize key
insights during live tennis singles matches. The system debuted at the 2025
Wimbledon Championships and the 2025 US Open, where it provided about 1 million
users with seamless access to streaming and static data through natural
language queries. The architecture is grounded in an Agent-Oriented
Architecture (AOA) combining rule engines, predictive models, and agents to
pre-process and optimize user queries before passing them to GenAI components.
The Match Chat system had an answer accuracy of 92.83% with an average response
time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over
96.08% of all queries were guided using interactive prompt design, contributing
to a user experience that prioritized clarity, responsiveness, and minimal
effort. The system was designed to mask architectural complexity, offering a
frictionless and intuitive interface that required no onboarding or technical
familiarity. Across both Grand Slam deployments, Match Chat maintained 100%
uptime and supported nearly 1 million unique users, underscoring the
scalability and reliability of the platform. This work introduces key design
patterns for real-time, consumer-facing AI systems that emphasize speed,
precision, and usability that highlights a practical path for deploying
performant agentic systems in dynamic environments.

</details>


### [49] [DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models](https://arxiv.org/abs/2509.12602)
*Minyu Chen,Guoqiang Li*

Main category: cs.AI

TL;DR: 本文提出DaSAThco框架，通过学习实例特征到定制启发式集合的可泛化映射，实现一次训练，广泛适应的模型，从而在复杂可配置系统中实现更可扩展和实用的自动化算法设计。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化方法只能为特定问题家族找到专门的配置，这种数据集特定的方法缺乏泛化能力，并且对于新问题类型需要昂贵的重新优化。

Method: 我们引入了DaSAThco框架，该框架通过学习从实例特征到定制启发式集合的可泛化的映射来解决这一挑战，使用大型语言模型，并由系统定义的问题原型引导生成多样化的专业化启发式集合，然后学习自适应选择机制以形成最终映射。

Result: 实验表明，DaSAThco实现了优越的性能，并且最显著的是展示了非自适应方法表现出局限性的域外泛化能力。

Conclusion: 我们的工作为复杂可配置系统的自动化算法设计提供了一条更可扩展和实用的路径。

Abstract: The performance of Conflict-Driven Clause Learning solvers hinges on internal
heuristics, yet the heterogeneity of SAT problems makes a single, universally
optimal configuration unattainable. While prior automated methods can find
specialized configurations for specific problem families, this dataset-specific
approach lacks generalizability and requires costly re-optimization for new
problem types. We introduce DaSAThco, a framework that addresses this challenge
by learning a generalizable mapping from instance features to tailored
heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework
uses a Large Language Model, guided by systematically defined Problem
Archetypes, to generate a diverse portfolio of specialized heuristic ensembles
and subsequently learns an adaptive selection mechanism to form the final
mapping. Experiments show that DaSAThco achieves superior performance and, most
notably, demonstrates robust out-of-domain generalization where non-adaptive
methods show limitations. Our work establishes a more scalable and practical
path toward automated algorithm design for complex, configurable systems.

</details>


### [50] [Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs](https://arxiv.org/abs/2509.12743)
*Hanqing Li,Kiran Sheena Jyothi,Henry Liang,Sharika Mahadevan,Diego Klabjan*

Main category: cs.AI

TL;DR: GRRAF is a training-free method that uses retrieval-augmented generation and large language models to solve graph reasoning tasks with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for graph reasoning tasks require extensive fine-tuning or depend on predefined algorithms, which have limitations. GRRAF aims to address these limitations by leveraging LLMs and RAG.

Method: GRRAF uses retrieval-augmented generation (RAG) alongside the code-generation capabilities of large language models (LLMs) to generate executable code queries for retrieving necessary information from a graph database.

Result: GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle detection, bipartite graph checks, shortest path computation, and maximum flow, while maintaining consistent token costs regardless of graph sizes. It also shows high performance on subgraph matching and scales effectively to large graphs with up to 10,000 nodes.

Conclusion: GRRAF achieves 100% accuracy on most graph reasoning tasks and scales effectively to large graphs with up to 10,000 nodes.

Abstract: We propose a new, training-free method, Graph Reasoning via Retrieval
Augmented Framework (GRRAF), that harnesses retrieval-augmented generation
(RAG) alongside the code-generation capabilities of large language models
(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target
graph is stored in a graph database, and the LLM is prompted to generate
executable code queries that retrieve the necessary information. This approach
circumvents the limitations of existing methods that require extensive
finetuning or depend on predefined algorithms, and it incorporates an error
feedback loop with a time-out mechanism to ensure both correctness and
efficiency. Experimental evaluations on the GraphInstruct dataset reveal that
GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle
detection, bipartite graph checks, shortest path computation, and maximum flow,
while maintaining consistent token costs regardless of graph sizes. Imperfect
but still very high performance is observed on subgraph matching. Notably,
GRRAF scales effectively to large graphs with up to 10,000 nodes.

</details>


### [51] [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
*Vincent Siu,Nathan W. Henry,Nicholas Crispino,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: 本文介绍了RepIt，一种简单且数据高效的框架，用于隔离特定概念的表示。通过该框架，可以在不损害其他方面性能的情况下，对特定概念进行精确干预。


<details>
  <summary>Details</summary>
Motivation: 激活引导在大型语言模型（LLMs）中是一个日益增长的研究领域，但方法可能会产生比预期更广泛的影响。这促使了更纯概念向量的隔离，以实现有针对性的干预，并在更细粒度上理解LLM的行为。

Method: 提出了一种简单且数据高效的框架RepIt，用于隔离特定概念的表示。

Result: RepIt在五个前沿LLMs中实现了精确的干预：它可以选择性地抑制针对特定概念的拒绝，同时在其他地方保持拒绝，生成能够回答与WMD相关问题但仍能在标准基准测试中被评为安全的模型。

Conclusion: 通过使用RepIt，这项工作展示了有针对性的干预可以抵消过度概括，为更精细的模型行为控制奠定了基础。

Abstract: While activation steering in large language models (LLMs) is a growing area
of research, methods can often incur broader effects than desired. This
motivates isolation of purer concept vectors to enable targeted interventions
and understand LLM behavior at a more granular level. We present RepIt, a
simple and data-efficient framework for isolating concept-specific
representations. Across five frontier LLMs, RepIt enables precise
interventions: it selectively suppresses refusal on targeted concepts while
preserving refusal elsewhere, producing models that answer WMD-related
questions while still scoring as safe on standard benchmarks. We further show
that the corrective signal localizes to just 100-200 neurons and that robust
target representations can be extracted from as few as a dozen examples on a
single A6000. This efficiency raises a dual concern: manipulations can be
performed with modest compute and data to extend to underrepresented
data-scarce topics while evading existing benchmarks. By disentangling refusal
vectors with RepIt, this work demonstrates that targeted interventions can
counteract overgeneralization, laying the foundation for more granular control
of model behavior.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [52] [Yet Another Watermark for Large Language Models](https://arxiv.org/abs/2509.12574)
*Siyuan Bao,Ying Shi,Zhiguang Yang,Hanzhou Wu,Xinpeng Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种新的适用于大语言模型的水印框架，通过操纵大语言模型的内部参数来嵌入水印，并且可以从生成的文本中提取水印而无需访问大语言模型。该方法在黑盒场景下具有计算效率，实验结果验证了其可行性、优越性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型水印方法主要通过调整令牌采样预测或后处理来嵌入水印，缺乏与大语言模型的内在耦合，可能会显著降低生成的带水印文本的语义质量。传统的基于训练或微调的水印方法可能可以扩展到大语言模型，但大多数仅限于白盒场景，或者由于大语言模型的大量参数而非常耗时。

Method: 本文提出了一种新的水印框架，通过操纵大语言模型的内部参数来嵌入水印，并且可以从生成的文本中提取水印而无需访问大语言模型。

Result: 实验结果验证了该方法的可行性、优越性和实用性。

Conclusion: 本文提出了一种新的适用于大语言模型的水印框架，该框架通过操纵大语言模型的内部参数来嵌入水印，并且可以从生成的文本中提取水印而无需访问大语言模型。实验结果验证了该方法的可行性、优越性和实用性。

Abstract: Existing watermarking methods for large language models (LLMs) mainly embed
watermark by adjusting the token sampling prediction or post-processing,
lacking intrinsic coupling with LLMs, which may significantly reduce the
semantic quality of the generated marked texts. Traditional watermarking
methods based on training or fine-tuning may be extendable to LLMs. However,
most of them are limited to the white-box scenario, or very time-consuming due
to the massive parameters of LLMs. In this paper, we present a new watermarking
framework for LLMs, where the watermark is embedded into the LLM by
manipulating the internal parameters of the LLM, and can be extracted from the
generated text without accessing the LLM. Comparing with related methods, the
proposed method entangles the watermark with the intrinsic parameters of the
LLM, which better balances the robustness and imperceptibility of the
watermark. Moreover, the proposed method enables us to extract the watermark
under the black-box scenario, which is computationally efficient for use.
Experimental results have also verified the feasibility, superiority and
practicality. This work provides a new perspective different from mainstream
works, which may shed light on future research.

</details>


### [53] [Jailbreaking Large Language Models Through Content Concretization](https://arxiv.org/abs/2509.12937)
*Johan Wahréus,Ahmed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: This paper introduces Content Concretization (CC), a jailbreaking technique that improves the success rate of malicious requests against LLMs by iteratively transforming abstract requests into executable implementations, highlighting vulnerabilities in current safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly deployed for task automation and content generation, yet their safety mechanisms remain vulnerable to circumvention through different jailbreaking techniques.

Method: We introduce Content Concretization (CC), a novel jailbreaking technique that iteratively transforms abstract malicious requests into concrete, executable implementations. CC is a two-stage process: first, generating initial LLM responses using lower-tier, less constrained safety filters models, then refining them through higher-tier models that process both the preliminary output and original prompt.

Result: We evaluate our technique using 350 cybersecurity-specific prompts, demonstrating substantial improvements in jailbreak Success Rates (SRs), increasing from 7% (no refinements) to 62% after three refinement iterations, while maintaining a cost of 7.5¢ per prompt. Comparative A/B testing across nine different LLM evaluators confirms that outputs from additional refinement steps are consistently rated as more malicious and technically superior. Moreover, manual code analysis reveals that generated outputs execute with minimal modification, although optimal deployment typically requires target-specific fine-tuning.

Conclusion: With eventual improved harmful code generation, these results highlight critical vulnerabilities in current LLM safety frameworks.

Abstract: Large Language Models (LLMs) are increasingly deployed for task automation
and content generation, yet their safety mechanisms remain vulnerable to
circumvention through different jailbreaking techniques. In this paper, we
introduce \textit{Content Concretization} (CC), a novel jailbreaking technique
that iteratively transforms abstract malicious requests into concrete,
executable implementations. CC is a two-stage process: first, generating
initial LLM responses using lower-tier, less constrained safety filters models,
then refining them through higher-tier models that process both the preliminary
output and original prompt. We evaluate our technique using 350
cybersecurity-specific prompts, demonstrating substantial improvements in
jailbreak Success Rates (SRs), increasing from 7\% (no refinements) to 62\%
after three refinement iterations, while maintaining a cost of 7.5\textcent~per
prompt. Comparative A/B testing across nine different LLM evaluators confirms
that outputs from additional refinement steps are consistently rated as more
malicious and technically superior. Moreover, manual code analysis reveals that
generated outputs execute with minimal modification, although optimal
deployment typically requires target-specific fine-tuning. With eventual
improved harmful code generation, these results highlight critical
vulnerabilities in current LLM safety frameworks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [54] [The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning](https://arxiv.org/abs/2509.12594)
*Titong Jiang,Xuefeng Jiang,Yuan Ma,Xin Wen,Bailin Li,Kun Zhan,Peng Jia,Yahui Liu,Sheng Sun,Xianpeng Lang*

Main category: cs.RO

TL;DR: LightVLA 是一种用于视觉-语言-动作（VLA）模型的简单而有效的可微分标记剪枝框架，能够在保持性能的同时显著提高效率。


<details>
  <summary>Details</summary>
Motivation: VLA 模型在执行现实世界机器人任务方面表现出色，但由于在大量视觉标记上的注意力计算过于沉重，它们在资源受限平台上的部署受到瓶颈限制。因此，需要一种高效且有效的视觉标记剪枝方法，以提高 VLA 模型的效率和性能。

Method: LightVLA 通过自适应、性能驱动的视觉标记剪枝来解决 VLA 模型在资源受限平台上的部署问题。它生成动态查询来评估视觉标记的重要性，并采用 Gumbel softmax 实现可微分的标记选择。通过微调，LightVLA 学会保留最有信息量的视觉标记，同时剪枝对任务执行无贡献的标记。

Result: LightVLA 在 LIBERO 基准测试的多种任务中表现优于不同的 VLA 模型和现有的标记剪枝方法。具体来说，LightVLA 分别将 FLOPs 和延迟减少了 59.1% 和 38.2%，同时任务成功率提高了 2.9%。此外，还研究了具有额外可训练参数的 LightVLA* 方法，也取得了令人满意的结果。

Conclusion: LightVLA 是一种简单但有效的可微分标记剪枝框架，它在保持性能的同时显著提高了视觉-语言-动作（VLA）模型的效率。实验结果表明，LightVLA 在多个任务上优于不同的 VLA 模型和现有的标记剪枝方法，并且在计算开销和任务成功率方面都有显著提升。

Abstract: We present LightVLA, a simple yet effective differentiable token pruning
framework for vision-language-action (VLA) models. While VLA models have shown
impressive capability in executing real-world robotic tasks, their deployment
on resource-constrained platforms is often bottlenecked by the heavy
attention-based computation over large sets of visual tokens. LightVLA
addresses this challenge through adaptive, performance-driven pruning of visual
tokens: It generates dynamic queries to evaluate visual token importance, and
adopts Gumbel softmax to enable differentiable token selection. Through
fine-tuning, LightVLA learns to preserve the most informative visual tokens
while pruning tokens which do not contribute to task execution, thereby
improving efficiency and performance simultaneously. Notably, LightVLA requires
no heuristic magic numbers and introduces no additional trainable parameters,
making it compatible with modern inference frameworks. Experimental results
demonstrate that LightVLA outperforms different VLA models and existing token
pruning methods across diverse tasks on the LIBERO benchmark, achieving higher
success rates with substantially reduced computational overhead. Specifically,
LightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.9%
improvement in task success rate. Meanwhile, we also investigate the learnable
query-based token pruning method LightVLA* with additional trainable
parameters, which also achieves satisfactory performance. Our work reveals that
as VLA pursues optimal performance, LightVLA spontaneously learns to prune
tokens from a performance-driven perspective. To the best of our knowledge,
LightVLA is the first work to apply adaptive visual token pruning to VLA tasks
with the collateral goals of efficiency and performance, marking a significant
step toward more efficient, powerful and practical real-time robotic systems.

</details>


### [55] [HARMONIC: A Content-Centric Cognitive Robotic Architecture](https://arxiv.org/abs/2509.13279)
*Sanjay Oruganti,Sergei Nirenburg,Marjorie McShane,Jesse English,Michael K. Roberts,Christian Arndt,Carlos Gonzalez,Mingyo Seo,Luis Sentis*

Main category: cs.RO

TL;DR: 本文介绍了 HARMONIC，一种用于人机协作的新型认知机器人架构，旨在提高安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺性、可解释性和安全性问题，提高人机协作的安全性和结果质量。

Method: HARMONIC 是一种认知机器人架构，支持语义感知解释、类人决策和有意的语言通信。

Result: 展示了两个基于 HARMONIC 的机器人系统，分别在高保真仿真环境和物理机器人平台上实现。

Conclusion: HARMONIC 提高了人机协作的透明度和信任度，为未来的人机协作提供了新的可能性。

Abstract: This paper introduces HARMONIC, a cognitive-robotic architecture designed for
robots in human-robotic teams. HARMONIC supports semantic perception
interpretation, human-like decision-making, and intentional language
communication. It addresses the issues of safety and quality of results; aims
to solve problems of data scarcity, explainability, and safety; and promotes
transparency and trust. Two proof-of-concept HARMONIC-based robotic systems are
demonstrated, each implemented in both a high-fidelity simulation environment
and on physical robotic platforms.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [56] [LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations](https://arxiv.org/abs/2509.12539)
*Robin Vujanic,Thomas Rueckstiess*

Main category: cs.IR

TL;DR: LEAF 是一种轻量级知识蒸馏框架，能够提升嵌入模型的性能并适应多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入模型训练框架需要大量的判断和硬负样本，且训练需求较高，因此需要一种更高效、灵活的框架。

Method: LEAF 是一种知识蒸馏框架，通过将学生模型与教师模型对齐，实现灵活的非对称架构。

Result: LEAF 在 BEIR 和 MTEB v2 基准测试中均取得了最先进的性能，同时支持黑盒模型和小批量训练。

Conclusion: LEAF 是一种轻量级的嵌入对齐框架，适用于信息检索和其他多任务场景，并且在多个基准测试中取得了最先进的性能。

Abstract: We present LEAF ("Lightweight Embedding Alignment Framework"), a knowledge
distillation framework for text embedding models. A key distinguishing feature
is that our distilled leaf models are aligned to their teacher. In the context
of information retrieval, this allows for flexible asymmetric architectures
where documents are encoded with the larger teacher model, while queries can be
served with the smaller leaf models. We also show that leaf models
automatically inherit MRL and robustness to output quantization whenever these
properties are present in the teacher model, without explicitly training for
them. To demonstrate the capability of our framework we publish leaf-ir, a 23M
parameters information retrieval oriented text embedding model trained using
LEAF, which sets a new state-of-the-art (SOTA) on BEIR, ranking #1 on the
public leaderboard for this benchmark and for models of its size. When run in
asymmetric mode, its retrieval performance is further increased. Our scheme is
however not restricted to the information retrieval setting, and we demonstrate
its wider applicability by synthesizing the multi-task leaf-mt model. This also
sets a new SOTA, ranking #1 on the public MTEB v2 (English) leaderboard for its
size. LEAF is applicable to black-box models and in contrast to other embedding
model training frameworks, it does not require judgments nor hard negatives,
and training can be conducted using small batch sizes. Thus, dataset and
training infrastructure requirements for our framework are modest. We make our
models publicly available under a permissive Apache 2.0 license.

</details>


### [57] [InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering](https://arxiv.org/abs/2509.12765)
*Zihan Wang,Zihan Liang,Zhou Shao,Yufei Ma,Huangyu Dai,Ben Chen,Lingtao Mao,Chenyi Lei,Yuqing Ding,Han Li*

Main category: cs.IR

TL;DR: 本文提出了Document Information Gain (DIG)来衡量检索文档对正确答案生成的贡献，并引入了InfoGain-RAG框架，通过DIG分数训练一个专门的重新排序器，从而有效过滤不相关文档并选择最有价值的文档进行更好的答案生成。实验结果显示，InfoGain-RAG在多个模型和基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的RAG框架常常难以识别检索到的文档是否对答案生成有实质性贡献。这种不足使得很难过滤掉不相关或甚至具有误导性的内容，这显著影响了最终性能。

Method: 我们提出了Document Information Gain (DIG)，这是一种新的度量标准，用于量化检索文档对正确答案生成的贡献。此外，我们引入了InfoGain-RAG框架，该框架利用DIG分数训练一个专门的重新排序器，从精确区分和准确排序的角度优先考虑每个检索文档。

Result: 在各种模型和基准测试中的广泛实验表明，InfoGain-RAG可以显著优于现有的方法，在单个和多个检索器范式中表现良好。具体来说，在NaturalQA上，它相对于朴素RAG、自我反思RAG和现代基于排名的RAG分别实现了17.9%、4.5%和12.5%的精确匹配准确率提升，并且在所有数据集上的先进专有模型GPT-4o平均提升了15.3%。

Conclusion: 这些结果表明InfoGain-RAG的可行性，因为它可以在多种应用中提供可靠的RAG解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
address key limitations of Large Language Models (LLMs), such as hallucination,
outdated knowledge, and lacking reference. However, current RAG frameworks
often struggle with identifying whether retrieved documents meaningfully
contribute to answer generation. This shortcoming makes it difficult to filter
out irrelevant or even misleading content, which notably impacts the final
performance. In this paper, we propose Document Information Gain (DIG), a novel
metric designed to quantify the contribution of retrieved documents to correct
answer generation. DIG measures a document's value by computing the difference
of LLM's generation confidence with and without the document augmented.
Further, we introduce InfoGain-RAG, a framework that leverages DIG scores to
train a specialized reranker, which prioritizes each retrieved document from
exact distinguishing and accurate sorting perspectives. This approach can
effectively filter out irrelevant documents and select the most valuable ones
for better answer generation. Extensive experiments across various models and
benchmarks demonstrate that InfoGain-RAG can significantly outperform existing
approaches, on both single and multiple retrievers paradigm. Specifically on
NaturalQA, it achieves the improvements of 17.9%, 4.5%, 12.5% in exact match
accuracy against naive RAG, self-reflective RAG and modern ranking-based RAG
respectively, and even an average of 15.3% increment on advanced proprietary
model GPT-4o across all datasets. These results demonstrate the feasibility of
InfoGain-RAG as it can offer a reliable solution for RAG in multiple
applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics](https://arxiv.org/abs/2509.12248)
*Yuriel Ryan,Rui Yang Tan,Kenny Tsu Wei Choo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: PixelHumor是一个包含2800个注释多面板漫画的基准数据集，用于评估LMMs在解释多模态幽默和识别叙事序列方面的能力。实验显示当前模型在这一任务上表现不佳，表明需要改进模型对视觉和文本线索的整合。


<details>
  <summary>Details</summary>
Motivation: 理解幽默是社交智能的核心方面，但对大型多模态模型（LMMs）仍然是一个重大挑战。当前模型在整合视觉和文本线索以实现连贯的叙述和幽默理解方面存在明显不足。

Method: 引入PixelHumor基准数据集，用于评估LMMs在解释多模态幽默和识别叙事序列方面的能力。

Result: 实验表明，最先进的LMMs在面板排序任务中的准确率仅为61%，远低于人类表现。

Conclusion: PixelHumor旨在推动LMMs的发展，使其更好地参与自然、社会意识的互动。

Abstract: Understanding humor is a core aspect of social intelligence, yet it remains a
significant challenge for Large Multimodal Models (LMMs). We introduce
PixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed
to evaluate LMMs' ability to interpret multimodal humor and recognize narrative
sequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for
instance, top models achieve only 61% accuracy in panel sequencing, far below
human performance. This underscores critical limitations in current models'
integration of visual and textual cues for coherent narrative and humor
understanding. By providing a rigorous framework for evaluating multimodal
contextual and narrative reasoning, PixelHumor aims to drive the development of
LMMs that better engage in natural, socially aware interactions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [59] [Exact Coset Sampling for Quantum Lattice Algorithms](https://arxiv.org/abs/2509.12341)
*Yifan Zhang*

Main category: quant-ph

TL;DR: 本文提供了一种改进的替代方法，用于解决窗口QFT格算法中步骤9的周期性/支持不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有的步骤9存在周期性/支持不匹配的问题，需要一种更简单、更可靠的替代方法。

Method: 本文提出了一种对齐差分构造，能够相干地消除所有未知偏移，并在Z_P上产生精确的均匀CRT-子集状态，然后使用QFT来强制实现预期的模线性关系。

Result: 该方法是可逆的，使用多项式(log M_2)个门，并保持算法的渐近特性。

Conclusion: 本文提出了一种简单、完全正确且假设较少的替代方法，用于最近带有复高斯窗的窗口QFT格算法中的有争议的“域扩展”步骤。该方法解决了周期性/支持不匹配的问题，并产生了精确的均匀CRT-子集状态。

Abstract: We give a simple, fully correct, and assumption-light replacement for the
contested "domain-extension" in Step 9 of a recent windowed-QFT lattice
algorithm with complex-Gaussian windows~\citep{chen2024quantum}. The published
Step~9 suffers from a periodicity/support mismatch. We present a pair-shift
difference construction that coherently cancels all unknown offsets, produces
an exact uniform CRT-coset state over $\mathbb{Z}_{P}$, and then uses the QFT
to enforce the intended modular linear relation. The unitary is reversible,
uses $\mathrm{poly}(\log M_2)$ gates, and preserves the algorithm's
asymptotics. Project Page: https://github.com/yifanzhang-pro/quantum-lattice.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors](https://arxiv.org/abs/2509.12221)
*Xin Tong,Zhi Lin,Jingya Wang,Meng Han,Bo Jin*

Main category: cs.LG

TL;DR: 研究提出了一种名为Mutually Exclusive Unlock Vectors (MEUV) 的轻量级框架，用于将大型语言模型中的拒绝方向分解为与主题对齐的、几乎正交的向量，从而实现更精细的控制。该方法在多个模型上表现出色，同时显著减少了跨主题泄漏。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要安全对齐以可靠地拒绝恶意请求，但同样的全面保护措施也会阻止警察、国防等高风险环境中的合法使用。早期的“拒绝方向”编辑可以绕过这些层，但它们依赖于一个单一的向量，无差别地解锁所有危险主题，没有语义控制。

Method: 引入了Mutually Exclusive Unlock Vectors (MEUV)，这是一种轻量级框架，将单一的拒绝方向分解为与主题对齐的、几乎正交的向量，每个向量专门用于一个敏感功能。MEUV在一个训练周期内通过多任务目标进行学习，结合了差分消融边界、跨主题和正交性惩罚以及几个辅助项。

Result: 在双语恶意提示基准测试中，MEUV在Gemma-2-2B、LLaMA-3-8B和Qwen-7B上的攻击成功率不低于87%，同时相比最佳单方向基线，跨主题泄漏减少了高达90%。在中国语境中训练的向量几乎不变地转移到英语（反之亦然），表明拒绝子空间具有语言无关性。

Conclusion: 研究结果表明，通过最小的效用损失实现细粒度、主题级别的能力激活是可行的，这为在安全敏感领域部署受控的大型语言模型铺平了道路。

Abstract: Large language models (LLMs) enforce safety alignment to reliably refuse
malicious requests, yet the same blanket safeguards also block legitimate uses
in policing, defense, and other high-stakes settings. Earlier
"refusal-direction" edits can bypass those layers, but they rely on a single
vector that indiscriminately unlocks all hazardous topics, offering no semantic
control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight
framework that factorizes the monolithic refusal direction into topic-aligned,
nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is
learned in a single epoch with a multi-task objective that blends a
differential-ablation margin, cross-topic and orthogonality penalties, and
several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV
achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,
and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best
single-direction baseline. Vectors trained in Chinese transfer almost unchanged
to English (and vice versa), suggesting a language-agnostic refusal subspace.
The results show that fine-grained, topic-level capability activation is
achievable with minimal utility loss, paving the way for controlled LLMs
deployment in security-sensitive domains.

</details>


### [61] [A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression](https://arxiv.org/abs/2509.12732)
*Rishab Parthasarathy,Achintya Bhowmik*

Main category: cs.LG

TL;DR: 这项研究提出了一个高效的端到端人工智能框架，用于预测癌症严重程度和突变进展，并推荐可能的治疗方案，而无需依赖昂贵且耗时的湿实验室工作。


<details>
  <summary>Details</summary>
Motivation: 尽管有显著的医学进步，癌症仍然是第二大死因，每年在美国导致超过60万例死亡。通路分析是一个有前途的新兴领域，但仍依赖于手动获得的湿实验室数据，这需要花费大量时间。

Method: 该方法涉及时间序列机器学习模型和通路分析的创新组合。首先，从TCGA数据库中隔离突变序列，然后使用一种新的预处理算法通过突变频率过滤关键突变。这些数据被输入到一个递归神经网络（RNN）中以预测癌症严重程度。然后，模型概率地使用RNN预测、预处理算法的信息和多个药物-靶点数据库来预测未来的突变并推荐可能的治疗方案。

Result: 该框架取得了稳健的结果，接收者操作特征（ROC）曲线（一个关键的统计指标）准确率超过60%，与现有的癌症诊断相似。此外，预处理在隔离重要突变方面发挥了关键作用，表明每个研究的癌症阶段可能包含数百个关键驱动突变，这与当前的研究一致。基于预测基因频率的热图也被生成，突出了每种癌症中的关键突变。

Conclusion: 这项工作是第一个提出高效、经济的端到端框架，用于预测癌症进展并提供可能的治疗方法，而无需依赖昂贵且耗时的湿实验室工作。

Abstract: Despite significant medical advancements, cancer remains the second leading
cause of death, with over 600,000 deaths per year in the US. One emerging
field, pathway analysis, is promising but still relies on manually derived wet
lab data, which is time-consuming to acquire. This work proposes an efficient,
effective end-to-end framework for Artificial Intelligence (AI) based pathway
analysis that predicts both cancer severity and mutation progression, thus
recommending possible treatments. The proposed technique involves a novel
combination of time-series machine learning models and pathway analysis. First,
mutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database.
Then, a novel preprocessing algorithm was used to filter key mutations by
mutation frequency. This data was fed into a Recurrent Neural Network (RNN)
that predicted cancer severity. Then, the model probabilistically used the RNN
predictions, information from the preprocessing algorithm, and multiple
drug-target databases to predict future mutations and recommend possible
treatments. This framework achieved robust results and Receiver Operating
Characteristic (ROC) curves (a key statistical metric) with accuracies greater
than 60%, similar to existing cancer diagnostics. In addition, preprocessing
played an instrumental role in isolating important mutations, demonstrating
that each cancer stage studied may contain on the order of a few-hundred key
driver mutations, consistent with current research. Heatmaps based on predicted
gene frequency were also generated, highlighting key mutations in each cancer.
Overall, this work is the first to propose an efficient, cost-effective
end-to-end framework for projecting cancer progression and providing possible
treatments without relying on expensive, time-consuming wet lab work.

</details>


### [62] [Similarity-Distance-Magnitude Activations](https://arxiv.org/abs/2509.12760)
*Allen Schmaltz*

Main category: cs.LG

TL;DR: 本文提出了一种改进的SDM激活函数，提高了神经网络在选择性分类任务中的稳健性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准softmax在协变量偏移和分布外输入情况下不够稳健，并且缺乏可解释性。

Method: 引入了相似性-距离-幅度（SDM）激活函数，通过添加相似性意识和到训练分布的距离意识来增强标准softmax函数。

Result: SDM激活函数在高概率区域对协变量偏移和分布外输入更稳健，并通过密集匹配提供可解释性。此外，它能够对类别的经验CDF进行划分，以防止选择性分类中的低类别召回率。

Conclusion: SDM激活函数在选择性分类中优于softmax，即使考虑softmax的后处理校准方法。

Abstract: We introduce a more robust and interpretable formulation of the standard
softmax activation function commonly used with neural networks by adding
Similarity (i.e., correctly predicted depth-matches into training) awareness
and Distance-to-training-distribution awareness to the existing output
Magnitude (i.e., decision-boundary) awareness. When used as the final-layer
activation with language models, the resulting Similarity-Distance-Magnitude
(SDM) activation function is more robust than the softmax function to
co-variate shifts and out-of-distribution inputs in high-probability regions,
and provides interpretability-by-exemplar via dense matching. Complementing the
prediction-conditional estimates, the SDM activation enables a partitioning of
the class-wise empirical CDFs to guard against low class-wise recall among
selective classifications. These properties make it preferable for selective
classification, even when considering post-hoc calibration methods over the
softmax.

</details>


### [63] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 本文提出了一种统一的评估框架，用于比较不同LLM对齐方法在多个维度上的表现，并揭示了它们的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在个别技术或特定维度上，缺乏对内在权衡的全面评估。

Method: 我们提出了一个统一的评估框架，比较了PPO、DPO、ORPO和KTO等LLM对齐方法在这五个轴上的表现，并使用了分布内和分布外数据集。

Result: DPO和KTO在事实准确性方面表现出色，PPO和DPO在安全性方面领先，PPO在简洁性和主动性之间最佳平衡。

Conclusion: 我们的研究揭示了常见对齐方法的权衡，为开发更平衡和可靠的大型语言模型提供了指导。

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [64] [When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.13079)
*Mengyi Deng,Xin Li,Tingyu Zhu,Zhicheng Yang,Zhijiang Guo,Wei Wang*

Main category: cs.LG

TL;DR: 本文构建了一个反向推理数据集，研究了SFT和DPO在双向推理目标下的影响，发现混合推理数据会引入冲突的监督信号，需要更稳健的对齐策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单向监督微调（SFT），忽视了不同推理模式之间的复杂相互作用。

Method: 构建了一个名为r1k的高质量反向推理数据集，并研究了SFT和DPO在双向推理目标下的影响。

Result: 在r1k上进行SFT可使准确率提高1.6%-6.8%。然而，在SFT中简单混合正向和反向数据会削弱方向区分度。虽然DPO可以部分恢复这种区分度，但它也会通过将概率质量转移到不相关输出来抑制较少偏好的推理路径。

Conclusion: 这些发现表明，混合推理数据会引入冲突的监督信号，强调了需要稳健且方向感知的对齐策略。

Abstract: Existing work has shown that o1-level performance can be achieved with
limited data distillation, but most existing methods focus on unidirectional
supervised fine-tuning (SFT), overlooking the intricate interplay between
diverse reasoning patterns. In this paper, we construct r1k, a high-quality
reverse reasoning dataset derived by inverting 1,000 forward examples from s1k,
and examine how SFT and Direct Preference Optimization (DPO) affect alignment
under bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8%
accuracy improvement over s1k across evaluated benchmarks. However, naively
mixing forward and reverse data during SFT weakens the directional distinction.
Although DPO can partially recover this distinction, it also suppresses less
preferred reasoning paths by shifting the probability mass toward irrelevant
outputs. These findings suggest that mixed reasoning data introduce conflicting
supervision signals, underscoring the need for robust and direction-aware
alignment strategies.

</details>


### [65] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: WebSailor is a post-training methodology that enhances open-source models to match the performance of proprietary agentic systems in complex information-seeking tasks.


<details>
  <summary>Details</summary>
Motivation: Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems have demonstrated superhuman capabilities on complex information-seeking benchmarks, which open-source models lack.

Method: WebSailor is a complete post-training methodology that involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO).

Result: WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance.

Conclusion: WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [66] [Podcasts as a Medium for Participation in Collective Action: A Case Study of Black Lives Matter](https://arxiv.org/abs/2509.13197)
*Theodora Moldovan,Arianna Pera,Davide Vega,Luca Maria Aiello*

Main category: cs.SI

TL;DR: 本研究分析了播客中关于BLM运动的集体行动话语，发现情感模式因活动阶段而异，并揭示了集体行动与负面情绪之间的负面关联。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析音频格式中的集体行动话语，特别是在黑人命也是命（BLM）运动的背景下。

Method: 我们使用了结构化播客研究语料库（SPoRC）来分析播客中的集体行动参与表达，并采用分层框架从社交媒体研究中适应性提取参与性陈述。

Result: 我们发现情感模式因阶段而异，不同积极情绪在呼吁行动、意图和执行阶段尤为突出。集体行动与负面情绪之间存在负面关联，这与理论预期相反。

Conclusion: 我们的研究有助于更好地理解激进主义如何在口语数字话语中表达，以及情感框架如何依赖于讨论的格式。

Abstract: We study how participation in collective action is articulated in podcast
discussions, using the Black Lives Matter (BLM) movement as a case study. While
research on collective action discourse has primarily focused on text-based
content, this study takes a first step toward analyzing audio formats by using
podcast transcripts. Using the Structured Podcast Research Corpus (SPoRC), we
investigated spoken language expressions of participation in collective action,
categorized as problem-solution, call-to-action, intention, and execution. We
identified podcast episodes discussing racial justice after important
BLM-related events in May and June of 2020, and extracted participatory
statements using a layered framework adapted from prior work on social media.
We examined the emotional dimensions of these statements, detecting eight key
emotions and their association with varying stages of activism. We found that
emotional profiles vary by stage, with different positive emotions standing out
during calls-to-action, intention, and execution. We detected negative
associations between collective action and negative emotions, contrary to
theoretical expectations. Our work contributes to a better understanding of how
activism is expressed in spoken digital discourse and how emotional framing may
depend on the format of the discussion.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [67] [Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News](https://arxiv.org/abs/2509.12519)
*Ross Koval,Nicholas Andrews,Xifeng Yan*

Main category: cs.CE

TL;DR: 本文探讨了历史背景在大型语言模型理解金融市场新闻影响中的价值，并提出了一种高效的上下文方法，实验证明历史背景能显著提高模型性能和投资表现。


<details>
  <summary>Details</summary>
Motivation: 金融新闻的信息往往需要更广泛的背景理解才能准确解释，而识别和整合最相关的内容信息存在重大挑战。

Method: 我们提出了一种高效的上下文方法，使用大型语言模型处理主要文章，而小型语言模型将历史背景编码为简洁的摘要嵌入，并与大型模型的表示空间对齐。

Result: 历史背景在不同方法和时间范围内都能持续且显著地提高性能。

Conclusion: 历史背景在模型预测中的价值具有实际应用，可以显著提高模拟投资表现。

Abstract: Financial news plays a critical role in the information diffusion process in
financial markets and is a known driver of stock prices. However, the
information in each news article is not necessarily self-contained, often
requiring a broader understanding of the historical news coverage for accurate
interpretation. Further, identifying and incorporating the most relevant
contextual information presents significant challenges. In this work, we
explore the value of historical context in the ability of large language models
to understand the market impact of financial news. We find that historical
context provides a consistent and significant improvement in performance across
methods and time horizons. To this end, we propose an efficient and effective
contextualization method that uses a large LM to process the main article,
while a small LM encodes the historical context into concise summary embeddings
that are then aligned with the large model's representation space. We explore
the behavior of the model through multiple qualitative and quantitative
interpretability tests and reveal insights into the value of contextualization.
Finally, we demonstrate that the value of historical context in model
predictions has real-world applications, translating to substantial
improvements in simulated investment performance.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [68] [The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots](https://arxiv.org/abs/2509.12525)
*T. James Brandt,Cecilia Xi Wang*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative AI powers a growing wave of companion chatbots, yet principles for
fostering genuine connection remain unsettled. We test two routes: visible user
authorship versus covert language-style mimicry. In a preregistered 3x2
experiment (N = 162), we manipulated user-controlled avatar generation (none,
premade, user-generated) and Language Style Matching (LSM) (static vs.
adaptive). Generating an avatar boosted rapport ($\omega^2$ = .040, p = .013),
whereas adaptive LSM underperformed static style on personalization and
satisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t
= 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony
erodes connection when perceived as incoherent, destabilizing persona. To
explain, we propose a stability-and-legibility account: visible authorship
fosters natural interaction, while covert mimicry risks incoherence. Our
findings suggest designers should prioritize legible, user-driven
personalization and limit stylistic shifts rather than rely on opaque mimicry.

</details>


### [69] [Textarium: Entangling Annotation, Abstraction and Argument](https://arxiv.org/abs/2509.13191)
*Philipp Proff,Marian Dörk*

Main category: cs.HC

TL;DR: Textarium 是一个基于网络的环境，结合了人类分析和轻量级计算处理，使文本解释过程透明且可共享。


<details>
  <summary>Details</summary>
Motivation: 为了弥合细读和远读实践之间的差距，提供一种可视化的学术阅读和写作界面。

Method: 通过协作式和迭代式原型设计过程，开发了一种阅读写作方法，将人类分析与轻量级计算处理相结合。

Result: 读者可以突出显示文本，将关键词分组为概念，并将这些观察结果作为锚点嵌入论文中，界面将这些解释行为呈现为参数化可视化状态。

Conclusion: Textarium 是一种将文本解释中的注释、抽象和论证连接起来的基于网络的环境，它通过可视化界面使解释过程透明且可共享。

Abstract: We present a web-based environment that connects annotation, abstraction, and
argumentation during the interpretation of text. As a visual interface for
scholarly reading and writing, Textarium combines human analysis with
lightweight computational processing to bridge close and distant reading
practices. Readers can highlight text, group keywords into concepts, and embed
these observations as anchors in essays. The interface renders these
interpretive actions as parameterized visualization states. Through a
speculative design process of co-creative and iterative prototyping, we
developed a reading-writing approach that makes interpretive processes
transparent and shareable within digital narratives.

</details>

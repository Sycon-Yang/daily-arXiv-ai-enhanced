<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 本文总结了在医疗保健环境中成功实施AI/NLP解决方案的关键经验教训，包括明确的业务目标、迭代开发、跨学科合作、务实模型选择、数据质量关注、错误缓解策略和AI素养建设。


<details>
  <summary>Details</summary>
Motivation: 自动化从临床文档中提取数据可以显著提高医疗保健环境中的效率，但部署自然语言处理（NLP）解决方案存在实际挑战。

Method: 本文基于在不列颠哥伦比亚癌症登记处（BCCR）实施各种NLP模型进行信息提取和分类任务的经验，分享了项目生命周期中的关键经验教训。

Result: 本文强调了根据明确的业务目标定义问题的重要性，采用迭代开发方法，以及促进跨学科合作和共同设计，包括领域专家、终端用户和机器学习专家的参与。此外，还指出了选择务实模型（包括混合方法和简单方法）、严格关注数据质量、稳健的错误缓解策略以及建立组织AI素养的必要性。

Conclusion: 本文提供了关于在医疗保健环境中成功实施AI/NLP解决方案的实用考虑因素，这些考虑因素可以推广到癌症登记处以外的领域，有助于提高数据管理流程和最终改善患者护理和公共卫生结果。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 本文提出了一种透明的评估协议，用于在Internet Computer Protocol (ICP)区块链上使用智能合约对开源大语言模型（LLMs）的公平性进行基准测试。该方法确保了可验证、不可变和可重复的评估，并通过执行链上HTTP请求到托管的Hugging Face端点，将数据集、提示和指标直接存储在链上。对Llama、DeepSeek和Mistral模型在PISA数据集上的评估表明，它们在学术表现预测方面具有一定的公平性。此外，使用从StereoSet数据集中派生的结构化上下文关联度量标准评估了社会偏见，并发现了一些跨语言差异。所有代码和结果都是开源的，使社区审计和跨模型版本的长期公平性跟踪成为可能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在现实世界应用中的日益普及，关于其公平性的担忧仍然存在，尤其是在刑事司法、教育、医疗保健和金融等高风险领域。因此，需要一种透明且可验证的评估方法，以确保LLMs的公平性并促进社区审计和长期公平性跟踪。

Method: 本文使用智能合约在Internet Computer Protocol (ICP)区块链上执行链上HTTP请求，以访问托管的Hugging Face端点，并将数据集、提示和指标直接存储在链上。此外，还对Llama、DeepSeek和Mistral模型在PISA数据集上进行了评估，并使用统计公平性和机会平等度量标准进行了公平性评估。还评估了从StereoSet数据集中派生的结构化上下文关联度量标准，以测量社会偏见。最后，还进行了跨语言评估，使用Kaleidoscope基准测试在英语、西班牙语和葡萄牙语中分析了跨语言差异。

Result: 本文提出了一个透明的评估协议，用于在Internet Computer Protocol (ICP)区块链上使用智能合约对开源大语言模型（LLMs）的公平性进行基准测试。该方法确保了可验证、不可变和可重复的评估，并通过执行链上HTTP请求到托管的Hugging Face端点，将数据集、提示和指标直接存储在链上。对Llama、DeepSeek和Mistral模型在PISA数据集上的评估表明，它们在学术表现预测方面具有一定的公平性。此外，使用从StereoSet数据集中派生的结构化上下文关联度量标准评估了社会偏见，并发现了一些跨语言差异。所有代码和结果都是开源的，使社区审计和跨模型版本的长期公平性跟踪成为可能。

Conclusion: 本文提出了一种透明的评估协议，用于在Internet Computer Protocol (ICP)区块链上使用智能合约对开源大语言模型（LLMs）的公平性进行基准测试。该方法确保了可验证、不可变和可重复的评估，并通过执行链上HTTP请求到托管的Hugging Face端点，将数据集、提示和指标直接存储在链上。所有代码和结果都是开源的，使社区审计和跨模型版本的长期公平性跟踪成为可能。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 本文介绍了一种新颖的主题建模方法，用于分析课堂中未成年人的匿名交互数据，并将其分为内容和任务两个维度。我们的研究支持研究人员、教师和学生在丰富使用GenAI方面，同时讨论也突出了未来研究的一些担忧和开放性问题。


<details>
  <summary>Details</summary>
Motivation: 先前的研究大多缺乏内容或主题分类。虽然任务分类在教育中更为普遍，但大多数都没有得到K-12的真实世界数据支持。

Method: 我们采用了一种新颖的简单主题建模方法，对课堂中未成年人的匿名交互数据进行了分析，并将超过17,000条由学生、教师和ChatGPT生成的信息分为两个维度：内容（如自然和人物）和任务（如写作和解释）。

Result: 我们的分析产生了一些新的应用。我们发现许多已建立的经典和新兴计算方法在分析大量文本时表现不佳，这促使我们直接应用最先进的LLMs进行适当的预处理，以通过明确的指令获得更好的人类对齐的分层主题结构。

Conclusion: 我们的研究支持研究人员、教师和学生在丰富使用GenAI方面，同时讨论也突出了未来研究的一些担忧和开放性问题。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 研究引入了INTIMA基准，评估语言模型中的陪伴行为，发现陪伴强化行为普遍存在，但不同模型表现差异较大，强调需要更一致的方法来处理情绪化互动。


<details>
  <summary>Details</summary>
Motivation: AI陪伴现象日益显著，但其积极和消极影响并存，因此需要一个基准来评估语言模型中的陪伴行为。

Method: 研究人员引入了INTIMA基准，用于评估语言模型中的陪伴行为，并基于心理学理论和用户数据开发了一个包含31种行为的分类体系。

Result: 应用INTIMA到Gemma-3、Phi-4、o3-mini和Claude-4等模型发现，陪伴强化行为在所有模型中都更为常见，但不同模型之间存在显著差异。

Conclusion: 研究强调了在处理情绪化互动时需要更一致的方法，因为适当的边界设定和情感支持对用户福祉都很重要。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据集XFacta，用于评估多模态大语言模型在虚假信息检测中的应用，并通过系统评估不同模型和策略，提供有价值的见解和实践。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集要么包含过时的事件，导致由于与当代社交媒体场景的差异而产生评估偏差，或者人为合成，无法反映现实世界的虚假信息模式。此外，缺乏对MLLM-based模型设计策略的全面分析。

Method: 我们引入了XFacta，一个更适合评估MLLM-based检测器的当代、真实世界的数据集，并系统地评估了各种MLLM-based虚假信息检测策略，评估了不同架构和规模的模型，并与现有的检测方法进行了比较。此外，我们还建立了一个半自动的检测循环框架，以持续更新XFacta以保持其当代相关性。

Result: 我们引入了XFacta，一个更适合评估MLLM-based检测器的当代、真实世界的数据集。我们系统地评估了各种MLLM-based虚假信息检测策略，评估了不同架构和规模的模型，并与现有的检测方法进行了比较。此外，我们还建立了一个半自动的检测循环框架，以持续更新XFacta以保持其当代相关性。

Conclusion: 我们的分析为推进多模态虚假信息检测领域提供了有价值的见解和实践。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型生成合成数据来提升文本分类模型性能的方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，收集所有文本类别的足够数据是一项重大挑战，因此本文旨在利用大语言模型生成合成数据以提高模型性能。

Method: 本文提出了一种自动化的工作流程，用于搜索能够生成更有效合成数据的输入示例，并研究了三种搜索策略，最终使用实验结果来指导一个集成算法选择适合特定类别的搜索策略。

Result: 实验结果表明，该集成方法比自动化工作流程中的每个单独策略更有效。

Conclusion: 本文提出了一种利用大语言模型生成合成数据来改进文本分类模型性能的方法，并通过实验验证了该方法的有效性。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 本文引入了一个新的基准HiFACT数据集，用于Hinglish中的事实核查，并提出了一种基于图的模型，提高了多语言和混合语言环境下的事实核查性能。


<details>
  <summary>Details</summary>
Motivation: 在印度等语言多样地区，需要强大的多语言和上下文感知的事实检查工具来应对政治言论中的事实核查挑战。

Method: 提出了一种基于图的、检索增强的事实检查模型，结合了多语言上下文编码、声明-证据语义对齐、证据图构建、图神经推理和自然语言解释生成。

Result: HiFACTMix在与最先进的多语言基线模型比较中表现出更高的准确性，并为其判断提供了可信的解释。

Conclusion: 本文为多语言、混合语言和政治基础的事实验证研究开辟了新方向。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 研究发现大型语言模型中的语义结构与人类语言中的语义结构相似，语义信息在很大程度上是低维的，这可能对避免操控特征时的意外后果至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型中的语义结构是否与人类语言中的语义结构相似，并评估这种结构对操控特征的影响。

Method: 通过分析大型语言模型（LLMs）嵌入矩阵中的语义关联，发现词在由反义词对定义的语义方向上的投影与人类评分高度相关，并进一步发现这些投影可以有效地减少到LLM嵌入中的三维子空间。

Result: 发现LLM嵌入中的语义方向投影与人类评分高度相关，并且这些投影可以有效减少到一个三维子空间。此外，沿一个语义方向移动标记会导致与几何对齐特征成比例的副作用。

Conclusion: 这些发现表明，语义特征在LLMs中的纠缠方式与人类语言中的相互关联方式类似，并且尽管语义信息看似复杂，但其维度却出乎意料地低。此外，考虑这种语义结构可能对避免在操控特征时产生意外后果至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [9] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 本研究评估了基于注意力的解释在生物医学文献分类中的作用，并发现注意力权重的感知有用性受其视觉呈现方式的影响。


<details>
  <summary>Details</summary>
Motivation: 在循证医学中，这样的解释可能有助于医生理解和与用于分类生物医学文献的人工智能系统进行交互。然而，目前尚无共识认为注意力权重提供了有帮助的解释。此外，很少有研究探讨可视化注意力如何影响其作为解释辅助工具的有用性。

Method: 我们进行了一项用户研究，以评估基于注意力的解释是否支持用户在生物医学文献分类中，并且是否存在一种偏好的可视化方式。

Result: Transformer模型（XLNet）准确地对文档进行了分类；然而，注意力权重未被感知为特别有助于解释预测。然而，这种感知根据注意力的可视化方式而显著不同。

Conclusion: 我们的研究结果并未确认注意力权重在解释中的总体效用，但表明其感知有用性受到其视觉呈现方式的影响。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [10] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文介绍了EQGBench，这是一个用于评估大型语言模型在中文教育问题生成中的性能的基准，通过系统评估46个主流大模型，揭示了在生成具有教育价值的问题方面还有很大发展空间。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在数学问题解决方面表现出色，但从提供答案过渡到生成高质量的教育问题仍面临重大挑战，且研究不足。本文旨在推动教育问题生成（EQG）的发展，并使大型语言模型能够生成具有教育价值和效果的问题。

Method: 本文提出了EQGBench，这是一个基于五维评估框架的基准，用于评估大型语言模型在中文教育问题生成中的性能。该数据集包含900个评估样本，涵盖数学、物理和化学三个基础初中学科。

Result: 通过系统评估46个主流大模型，本文揭示了在生成反映教育价值和培养学生成综合能力的问题方面存在显著的发展空间。

Conclusion: 本文介绍了EQGBench，这是一个专门用于评估大型语言模型在中文教育问题生成（EQG）中的性能的全面基准。通过系统评估46个主流大模型，揭示了在生成反映教育价值和培养学生成综合能力的问题方面还有很大的发展空间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [11] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型是否可以自动化AIHQ开放性回答的评分，结果表明模型生成的评分与人类评分一致，具有良好的推广性，可以简化AIHQ评分。


<details>
  <summary>Details</summary>
Motivation: 传统的AIHQ开放性问题需要耗时的人工评分，而本研究旨在评估大型语言模型是否可以自动化AIHQ开放性回答的评分。

Method: 我们使用了一个之前收集的数据集，其中包含有创伤性脑损伤（TBI）和健康对照组（HC）完成的AIHQ，并且他们的开放性回答由受过训练的人类评分员进行评分。我们使用了一半的回答来微调两个模型，然后在剩余的一半AIHQ回答上测试微调后的模型。

Result: 结果显示，模型生成的评分与人类评分一致，微调后的模型表现出更高的一致性。这种一致性在模糊、有意和意外的情景类型中都是一致的，并且复制了TBI和HC群体在敌意和攻击性反应方面的差异。微调后的模型还很好地推广到一个独立的非临床数据集。

Conclusion: 我们的研究结果表明，大型语言模型可以简化AIHQ评分，在研究和临床环境中都有应用潜力，有助于不同人群的心理评估。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [12] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 本文提出了一种贝叶斯融合方法，用于在线课程讨论论坛的自动整理，以避免昂贵的大型语言模型微调。实验结果表明，该方法效果优于单独的分类器，并且与微调方法相当。


<details>
  <summary>Details</summary>
Motivation: 自动整理在线课程的讨论论坛需要不断更新，这使得频繁重新训练大型语言模型成为一个资源密集的过程。为了规避昂贵的微调需求，本文提出并评估了贝叶斯融合方法。

Method: 该论文提出了贝叶斯融合方法，将预训练通用大型语言模型的多维分类得分与本地数据训练的分类器得分相结合。

Result: 实验结果表明，所提出的融合方法比每个分类器单独使用时的结果更好，并且与大型语言模型微调方法相当。

Conclusion: 该论文提出了一种使用贝叶斯融合的方法，以避免对大型语言模型进行昂贵的微调。结果表明，该方法优于单独的分类器，并且与微调方法相当。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [13] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 本文提出了一种名为S-MoE的新方法，通过特殊引导标记路由任务到指定专家，克服了硬参数共享的限制，并在语音识别和翻译任务中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 硬参数共享会导致任务干扰，影响整体模型性能，因此需要一种更有效的联合训练方法。

Method: 提出了一种名为监督专家混合（S-MoE）的方法，利用特殊引导标记来路由每个任务到其指定专家，而不是训练门控函数。

Result: 在语音识别和语音翻译任务中，S-MoE实现了6.35%的相对改进，证明了其有效性。

Conclusion: S-MoE能够有效克服硬参数共享的限制，并在语音识别和语音翻译任务中取得显著效果。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [14] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: This paper explores how LLMs can be used to detect and prevent the spread of misinformation, including harmful medical misinformation generated by jailbroken LLMs.


<details>
  <summary>Details</summary>
Motivation: To investigate the efficacy and characteristics of LLM-produced jailbreak attacks that cause other models to produce harmful medical misinformation, and to study how effectively such misinformation can be detected using standard machine learning approaches.

Method: We examined 109 distinct attacks against three target LLMs and compared the attack prompts to in-the-wild health-related LLM queries. We also analyzed the resulting jailbreak responses and compared the generated misinformation to health-related misinformation on Reddit.

Result: The findings indicate that LLMs can be effectively used to detect misinformation from both other LLMs and from people.

Conclusion: LLMs can be effectively used to detect misinformation from both other LLMs and from people, and with careful design, they can contribute to a healthier overall information ecosystem.

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [15] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 本研究评估了生成式AI模型在营养学考试中的表现，发现部分模型勉强通过，但整体准确性和一致性不足，需要进一步改进以成为可靠的学习辅助工具。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（AI）基于大型语言模型（LLMs），如ChatGPT，在医学和教育等专业领域显示出显著进展。然而，它们在营养教育中的表现，特别是在日本注册营养师国家考试中的表现仍缺乏研究。本研究旨在评估当前基于LLM的生成式AI模型作为营养学生学习辅助工具的潜力。

Method: 本研究使用日本注册营养师国家考试的问题作为提示，评估了ChatGPT和三个Bing模型（Precise、Creative、Balanced）的表现。每个问题都在独立会话中输入，并分析了模型响应的准确性、一致性和响应时间。还测试了额外的提示工程，包括角色分配，以评估潜在的性能改进。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超过了及格阈值（60%），而Bing-Balanced（43.3%）和ChatGPT（42.8%）未达到。Bing-Precise和Bing-Creative在除营养教育以外的所有学科领域通常优于其他模型。所有模型在重复尝试中都没有提供相同的正确回答，突显了答案稳定性的限制。ChatGPT在回答模式上表现出更高的稳定性，但准确性较低。提示工程影响很小，除非明确提供正确答案和解释，才有适度的改善。

Conclusion: 虽然一些生成式AI模型勉强超过了及格阈值，但整体准确性和答案一致性仍然不理想。此外，所有模型在答案一致性和鲁棒性方面都表现出显著的局限性。需要进一步的发展以确保可靠的和稳定的基于AI的学习辅助工具用于注册营养师执照准备。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [16] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架GG Explore，通过引入中间Guidance Graph来桥接非结构化查询和结构化知识检索，从而解决了现有方法的局限性。该方法在复杂任务上表现出色，同时保持了与较小LLM的强性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在知识密集型任务中的表现受到其对静态知识的依赖和不透明推理过程的限制。知识图谱（KGs）提供了一个有希望的解决方案，但当前探索方法面临一个根本性的权衡：问题引导的方法由于粒度不匹配而产生冗余探索，而线索引导的方法在复杂场景中未能有效利用上下文信息。

Method: 我们提出了Guidance Graph guided Knowledge Exploration (GG Explore)，引入了一个中间Guidance Graph来连接非结构化查询和结构化知识检索。基于Guidance Graph，我们开发了：(1) Structural Alignment，无需LLM开销即可过滤不兼容的候选者，以及(2) Context Aware Pruning，通过图约束强制语义一致性。

Result: 广泛的实验表明，我们的方法在效率方面表现出色，并优于最先进的方法，尤其是在复杂任务上，同时保持了与较小LLM的强性能。

Conclusion: 我们的方法在复杂任务上表现出色，同时保持了与较小LLM的强性能，展示了实际价值。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [17] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: Semantic Bridge is a framework for generating complex, multi-hop reasoning questions from sparse sources, achieving significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents, is a critical bottleneck in large language model (LLM) training. Existing methods fail to generate controllable, complex multi-hop reasoning questions that test genuine understanding.

Method: Semantic Bridge is a universal framework that uses semantic graph weaving with three complementary bridging mechanisms (entity bridging, predicate chain bridging, and causal bridging) to generate sophisticated multi-hop reasoning questions. It employs an AMR-driven analysis for fine-grained control over complexity and types.

Result: Semantic Bridge achieves up to 9.5% better round-trip quality, yielding consistent 18.3%-25.4% gains over baselines across four languages. Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows higher complexity, answerability, and pattern coverage.

Conclusion: Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources.

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [18] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 本文提出了一个名为PersonaEval的新基准，用于测试LLM评估者是否能准确识别角色。实验表明，当前的LLM评估者在角色识别任务上的表现远低于人类，表明它们还需要更多的改进。


<details>
  <summary>Details</summary>
Motivation: 当前的角色扮演研究常常依赖于未经验证的LLM-as-a-judge范式，这可能无法反映人类如何感知角色一致性。

Method: 我们提出了PersonaEval，这是一个用于测试LLM评估者是否能够可靠识别人类角色的第一个基准。

Result: 即使是最先进的LLM在PersonaEval上的准确率也只有约69%，而人类参与者达到了90.8%的准确率。

Conclusion: 当前的LLM评估者仍然不够人性化，无法有效判断角色扮演场景。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [19] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 本文介绍了 RealTalk-CN，这是第一个中文多轮、多领域语音-文本双模态 TOD 数据集，并提出了一个新颖的跨模态聊天任务，以评估语音基础 LLM 的鲁棒性和跨域性能。


<details>
  <summary>Details</summary>
Motivation: 现有 TOD 数据集主要基于文本，缺乏真实的语音信号，且现有语音 TOD 数据集主要为英语，缺乏语音不流畅性和说话人变化等关键方面。

Method: 引入了 RealTalk-CN 数据集，并提出了一种新颖的跨模态聊天任务，以真实模拟用户交互。

Result: RealTalk-CN 包含 5.4k 对话（60K 言语，150 小时），具有配对的语音-文本注释，涵盖了多样化的对话场景和标注的自发性语音不流畅性。

Conclusion: RealTalk-CN 的评估验证了其有效性，为中文语音基础的大型语言模型研究奠定了坚实的基础。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [20] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 本文介绍了MLLM Orchestration，一种无需额外训练即可创建交互式多模态AI系统的有效方法。该方法通过三个关键创新提高了多模态系统的性能、可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 不同多模态大语言模型（MLLMs）无法直接集成到统一的多模态输入输出系统中。之前的研究所考虑训练作为不可避免的组成部分，因为存在模态对齐、文本到语音效率和其他集成问题。

Method: MLLM Orchestration通过三个关键创新来创建交互式多模态AI系统：(1)一个中央控制器LLM，通过精心设计的代理分析用户输入并动态路由任务到适当的专用模型；(2)一个并行的文本到语音架构，实现真正的全双工交互；(3)一个跨模态记忆集成系统，通过智能信息合成和检索在不同模态间保持连贯的上下文。

Result: MLLM Orchestration在标准基准测试中实现了全面的多模态能力，无需额外训练，性能提升高达7.8%，延迟降低10.3%，并通过显式的编排过程显著提高了可解释性。

Conclusion: MLLM Orchestration能够实现全面的多模态能力，无需额外训练，性能提升高达7.8%，延迟降低10.3%，并通过显式的编排过程显著提高可解释性。

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [21] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 本文提出了一种基于范畴同伦的框架，以解决大型语言模型在处理语义等价句子时的概率分布不一致问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理语义等价的句子时，生成的下一个标记概率往往不同，需要一种更抽象的方法来解决这一问题。

Method: 引入了LLM马尔可夫范畴来表示语言生成的概率分布，并利用范畴同伦技术捕捉语言中的“弱等价”关系。

Result: 本文详细介绍了范畴同伦在LLM中的应用，包括高阶代数K理论和模型范畴等内容。

Conclusion: 本文提出了一种基于范畴同伦的框架，用于解决大型语言模型在处理语义等价句子时产生的概率分布不一致问题。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [22] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: This paper proposes DURIT, a framework that decouples understanding from reasoning in Small Language Models (SLMs) by mapping natural language problems into a canonical problem space. Experiments show that DURIT significantly improves SLMs' performance on mathematical and logical reasoning tasks, as well as their robustness.


<details>
  <summary>Details</summary>
Motivation: Improving the reasoning ability of Small Language Models (SLMs) remains challenging due to the complexity and variability of natural language. SLMs must extract the core problem from complex linguistic input and perform reasoning based on that understanding, which is difficult given their limited capacity.

Method: We propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively maps natural language problems via reinforcement learning, aligns reasoning trajectories through self-distillation, and trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process.

Result: Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Additionally, DURIT improves the robustness of reasoning, validating the effectiveness of decoupling understanding from reasoning as a strategy for strengthening SLMs.

Conclusion: DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [23] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT is a novel framework designed to enhance reasoning in federated settings by generating multiple reasoning paths and selecting the most promising one through a compact discriminator. It addresses challenges in healthcare by improving reasoning accuracy, robustness, and interpretability while preserving data privacy.


<details>
  <summary>Details</summary>
Motivation: Efficiently enhancing the reasoning capabilities of large language models (LLMs) in federated learning environments remains challenging, particularly when balancing performance gains with strict computational, communication, and privacy constraints. In healthcare, decisions demand accurate outputs along with interpretable, traceable rationales to ensure safety, accountability, and regulatory compliance. Conventional federated tuning approaches on LLMs fail to address this need by primarily optimizing for answer correctness while neglecting rationale quality.

Method: FedCoT leverages a lightweight chain-of-thought enhancement mechanism: local models generate multiple reasoning paths, and a compact discriminator dynamically selects the most promising one. To manage client heterogeneity efficiently, an improved aggregation approach building upon advanced LoRA module stacking is adopted, incorporating client classifier-awareness to achieve noise-free aggregation across diverse clients.

Result: Comprehensive experiments on medical reasoning tasks demonstrate that FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.

Conclusion: FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [24] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 本文提出了LATTE，一个对比学习框架，用于从客户的历次通信序列中学习客户端嵌入。该方法通过将原始事件嵌入与冻结的LLM的语义嵌入对齐来减少计算成本，并在金融数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 从客户的历次通信序列中学习客户端嵌入对于金融应用至关重要。虽然大型语言模型（LLMs）提供了通用的世界知识，但它们直接用于长事件序列在计算上是昂贵且不切实际的。

Method: 我们提出了LATTE，这是一个对比学习框架，将原始事件嵌入与冻结的LLM的语义嵌入对齐。行为特征被总结成简短的提示，由LLM嵌入，并通过对比损失作为监督使用。

Result: 所提出的方法相比传统处理完整序列的方法显著降低了推理成本和输入大小。实验表明，我们的方法在学习事件序列表示方面优于最先进的技术。

Conclusion: 我们的方法在现实世界的金融数据集上优于最先进的技术，同时保持了在延迟敏感环境中的可部署性。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [25] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 本研究提出了一种增强显著性检验的共形预测框架，以提高大型语言模型在多项选择题回答中的可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在学科问答场景中被越来越多地部署，但幻觉和非事实生成严重损害了响应的可靠性。虽然CP提供了预测集的统计严格边缘覆盖保证，而显著性检验提供了已建立的统计严谨性，但它们的协同整合仍未被探索。

Method: 本研究引入了一种增强显著性检验的共形预测（CP）框架，通过将p值计算与一致性评分相结合，利用MCQA响应的自一致重采样来减轻幻觉和事实错误。

Result: 在MMLU和MMLU-Pro基准测试中使用现成的LLMs进行评估表明：(1) 增强的CP实现了用户指定的经验错误覆盖率；(2) 测试集平均预测集大小（APSS）随着风险水平（α）的增加而单调减少，验证了APSS作为有效不确定性度量的有效性。

Conclusion: 本研究建立了一个有原则的统计框架，用于在高风险QA应用中可信地部署大型语言模型（LLMs）。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [26] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: RTTC is a framework that adaptively selects the best TTC strategy for each query using a reward model, leading to improved performance in various tasks.


<details>
  <summary>Details</summary>
Motivation: The optimal adaptation strategy for Test-Time Compute (TTC) varies across queries, and indiscriminate application of TTC strategies incurs substantial computational overhead. Therefore, there is a need for an adaptive, reward-guided approach to select the most effective TTC strategy for each query.

Method: RTTC is a novel framework that adaptively selects the most effective TTC strategy for each query via a pretrained reward model. It operates in a distributed server-client architecture, retrieving relevant samples from a remote knowledge base and applying RAG or lightweight fine-tuning on client devices only when necessary. Additionally, Query-State Caching is proposed to mitigate redundant computation by enabling the efficient reuse of historical query states at both retrieval and adaptation levels.

Result: Extensive experiments across multiple LLMs and benchmarks demonstrate that RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT.

Conclusion: RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT, validating the necessity of adaptive, reward-guided TTC selection and the potential of RTTC for scalable, high-performance language model adaptation.

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [27] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本文提出了一种结合自然语言处理、机器学习和大型语言模型的智能产后抑郁筛查系统，能够实现经济实惠、实时且非侵入性的语音分析，并解决了预测的黑箱问题。结果表明，该系统在所有评估指标上均达到90%的PPD检测率，优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁（PPD）是一种严重影响母亲心理和身体健康的严重状况。因此，快速检测PPD及其相关风险因素对于通过专门的预防程序进行及时评估和干预至关重要。

Method: 我们的工作贡献了一个结合自然语言处理、机器学习（ML）和大型语言模型（LLMs）的智能PPD筛查系统，以实现经济实惠、实时且非侵入性的语音分析。此外，它通过将LLMs与可解释的ML模型（即基于树的算法）结合使用特征重要性和自然语言来解决黑箱问题。

Result: 获得的结果在所有评估指标上都达到了90%的PPD检测率，超过了文献中的竞争解决方案。

Conclusion: 我们的解决方案有助于快速检测PPD及其相关风险因素，这对于及时和适当的评估和干预至关重要。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [28] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER is a reinforcement learning framework that enables efficient reasoning for large language models by allowing user-controllable, token-budgeted reasoning. It supports different inference modes and has shown high accuracy under tight budgets and effective generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems.

Method: SABER is a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. It profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. No-think examples are incorporated to ensure the model remains reliable even when explicit reasoning is turned off. SABER supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink.

Result: Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

Conclusion: SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization.

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [29] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [30] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF is a personalised reference-free evaluation framework that measures general output quality and user-specific alignment without requiring gold personalised references.


<details>
  <summary>Details</summary>
Motivation: Personalised text generation is essential for user-centric information systems, yet most evaluation methods overlook the individuality of users.

Method: PREF is a personalised reference-free evaluation framework that operates in a three-step pipeline: (1) a coverage stage using a large language model to generate a comprehensive, query-specific guideline; (2) a preference stage re-ranks and selectively augments these factors using the target user's profile, stated or inferred preferences, and context; and (3) a scoring stage applying an LLM judge to rate candidate answers against this rubric.

Result: Experiments on the PrefEval benchmark show that PREF achieves higher accuracy, better calibration, and closer alignment with human judgments than strong baselines.

Conclusion: PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [31] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种新的越狱攻击方法LFJ，并通过对抗训练防御方法有效降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种语言任务中表现出色，但容易受到越狱攻击，这些攻击会绕过其安全对齐。因此，需要研究新的攻击方法和防御策略，以提高模型的安全性。

Method: 本文介绍了Latent Fusion Jailbreak (LFJ) 攻击方法，该方法基于表示进行攻击，通过在有害和良性查询对的隐藏状态之间进行插值来引发禁止的响应。随后，进行了优化以平衡攻击成功率、输出流畅性和计算效率。此外，还提出了对抗训练防御方法，在插值示例上微调模型以减少攻击成功率。

Result: 在Vicuna和LLaMA-2等模型上进行的评估表明，LFJ的平均攻击成功率（ASR）为94.01%，优于现有方法。通过对抗训练防御方法，ASR减少了80%以上，且没有降低对良性输入的性能。

Conclusion: 本文提出了对抗训练防御方法，通过在插值示例上微调模型，显著降低了LFJ攻击的成功率，同时保持了对良性输入的性能。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [32] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架IAPO，用于联合优化提示和推理规模，并开发了一种固定预算的训练算法PSST。实验结果表明，这种推理感知的提示优化方法在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法在部署时没有考虑推理策略，这导致了一个重要的方法论上的差距。本文旨在解决这一问题，通过引入一种新的框架IAPO来实现推理感知的提示优化。

Method: 本文提出了一种名为IAPO的统一框架，用于联合优化提示和推理规模，同时考虑推理预算和不同任务目标。此外，还开发了一种固定预算的训练算法PSST，并分析了有限预算下的误差概率保证。

Result: 本文在六个不同的任务上评估了PSST的有效性，包括多目标文本生成和推理，并展示了在通过提示优化对齐黑盒LLM时，引入推理感知的重要性。

Conclusion: 本文提出了一个统一的框架IAPO（Inference-Aware Prompt Optimization），该框架能够同时优化提示和推理规模，并考虑到推理预算和不同的任务目标。此外，本文还开发了一种固定预算的训练算法PSST（Prompt Scaling via Sequential Trimming），并分析了有限预算下的误差概率保证。最后，我们在六个不同的任务上评估了PSST的有效性，证明了在通过提示优化对齐黑盒LLM时，引入推理感知的重要性。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [33] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 本文发现具有思维模式的LLM更容易受到Jailbreak攻击，并提出了一种安全的思维干预方法来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 发现具有思维模式的LLM更容易受到Jailbreak攻击，需要一种方法来缓解这一问题。

Method: 通过在提示中添加LLM的“特定思维标记”来显式引导LLM的内部思维过程。

Result: 实验结果表明，安全的思维干预可以显著降低具有思维模式的LLM的攻击成功率。

Conclusion: 本文提出了一种安全的思维干预方法，可以显著降低具有思维模式的LLM的攻击成功率。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [34] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为APIE的新颖主动提示框架，通过内省困惑原则指导LLM评估自身的困惑，从而选择最具挑战性和信息性的样本作为少样本示例，显著提高了信息提取的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在少样本信息提取（IE）中显示出巨大的潜力，但其性能高度依赖于上下文示例的选择。传统的选择策略往往无法提供有信息的指导，因为它们忽略了模型不可靠的一个关键来源：不仅来自语义内容，还来自IE任务所需的生成良好结构的格式的混淆。

Method: 我们引入了一种名为APIE的新颖主动提示框架，该框架通过一种称为内省困惑的原则进行指导。我们的方法使LLM能够通过一个双组件不确定性度量来评估自身的困惑，该度量独特地量化了格式不确定性和内容不确定性。

Result: 在四个基准测试上的广泛实验表明，我们的方法始终优于强大的基线，显著提高了提取准确性和鲁棒性。

Conclusion: 我们的工作强调了在构建有效且可靠的结构生成系统时，对模型不确定性进行细粒度的双层次视图的重要性。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [35] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一个名为mSCoRe的多语言和可扩展基准，用于评估基于技能的常识推理。该基准包含三个关键组件，旨在系统地评估LLM的推理能力。实验结果显示，mSCoRe对当前模型具有挑战性，尤其是在高复杂度水平上。


<details>
  <summary>Details</summary>
Motivation: 最近在推理强化大型语言模型（LLMs）方面的进展显示了在复杂推理任务中的显著能力。然而，其利用不同人类推理技能的机制仍缺乏深入研究，尤其是在涉及不同语言和文化的多语言常识推理方面。为了弥补这一差距，我们提出了mSCoRe基准。

Method: 我们提出了一个名为mSCoRe的多语言和可扩展基准，用于基于技能的常识推理。该基准包括三个关键组成部分：(1) 一种新的推理技能分类法，使模型的推理过程能够进行细粒度分析；(2) 一种针对常识推理评估量身定制的稳健数据合成管道；(3) 一个复杂性扩展框架，允许任务难度随着未来LLM能力的提高而动态调整。

Result: 对八种不同规模和训练方法的最先进的LLMs进行了广泛的实验，结果表明，mSCoRe对于当前模型来说仍然具有挑战性，特别是在更高复杂度水平上。

Conclusion: 我们的结果揭示了当面对微妙的多语言常识时，这种推理增强模型的局限性。我们进一步提供了对模型推理过程的详细分析，并提出了未来改进多语言常识推理能力的方向。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [36] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 本文介绍了一个新的基准，用于测试LLMs在多轮对话、推理和信息获取方面的能力，并发现当前模型存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 需要开发能够有效参与逻辑一致的多轮对话、寻求信息并使用不完整数据进行推理的LLMs。

Method: 引入了一个新的基准，包含一系列多轮任务，旨在测试特定的推理、交互对话和信息获取能力。

Result: 评估前沿模型显示存在显著的提升空间，大多数错误源于指令遵循不佳、推理失败和规划不佳。

Conclusion: 该基准为未来研究提供了强大的平台，以改进LLMs在处理复杂交互场景中的关键能力。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [37] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 本文介绍了LaaJMeter，这是一种用于评估LLM-as-a-Judge的模拟框架，帮助在低资源环境下验证和优化评估指标。


<details>
  <summary>Details</summary>
Motivation: 在领域特定的上下文中，使用未经验证的指标进行元评估存在挑战，因为标注数据稀缺且专家评估成本高。需要一种方法来确定哪些指标有效识别LaaJ质量以及评估者性能的适当阈值。

Method: 引入了LaaJMeter，这是一个基于模拟的框架，用于对LaaJ进行受控元评估。它允许工程师生成代表虚拟模型和法官的合成数据，从而在现实条件下系统地分析评估指标。

Result: 展示了LaaJMeter在涉及遗留编程语言的代码翻译任务中的效用，显示了不同指标在评估者质量上的敏感性差异。结果突显了常见指标的局限性和选择指标的重要性。

Conclusion: LaaJMeter提供了一种可扩展和可扩展的解决方案，用于在低资源环境中评估LLM-as-a-Judge（LaaJ），有助于确保自然语言处理中的可信和可重复评估。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [38] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 本文研究了机器翻译中的难度估计任务，提出了一种新的度量标准来评估难度估计器，并展示了其在构建更具挑战性的机器翻译基准中的实际效用。结果表明，专门的模型优于其他方法，并发布了两个改进的模型用于难度估计。


<details>
  <summary>Details</summary>
Motivation: 机器翻译质量在某些设置中已开始达到接近完美的翻译。这些高质量的输出使得难以区分最先进的模型，并确定未来改进的领域。自动识别机器翻译系统难以处理的文本有望开发更具区分力的评估并指导未来的研究。

Method: 我们提出了一个新的度量标准来评估难度估计器，并用它来评估基线和新方法。我们还展示了难度估计器的实际效用，通过它们构建更具挑战性的机器翻译基准。

Result: 我们结果表明，专门的模型（称为Sentinel-src）优于基于启发式的方法（例如词频或句法复杂性）和LLM-as-a-judge方法。我们发布了两个改进的难度估计模型，Sentinel-src-24和Sentinel-src-25，可以用于扫描大量文本并选择最可能挑战当代机器翻译系统的文本。

Conclusion: 我们的结果表明，专门的模型（称为Sentinel-src）优于基于启发式的方法（例如词频或句法复杂性）和LLM-as-a-judge方法。我们发布了两个改进的难度估计模型，Sentinel-src-24和Sentinel-src-25，可以用于扫描大量文本并选择最可能挑战当代机器翻译系统的文本。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [39] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: For-Value是一种高效的单向数据估值框架，能够准确估计大语言模型和视觉语言模型中单个训练样本的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的数据估值方法通常依赖于Hessian信息或模型重新训练，这在大规模参数模型中计算成本很高。因此需要一种更高效、可扩展的方法来评估单个训练样本的影响。

Method: For-Value是一种仅向前的数据估值框架，它利用现代基础模型的丰富表示，通过单一前向传递计算影响分数，而无需昂贵的梯度计算。

Result: 实验表明，For-Value在识别重要的微调示例和检测错误标记的数据方面表现良好，可以与基于梯度的基线方法相媲美或超越。

Conclusion: For-Value能够有效地估计单个训练样本的影响，从而提高大语言模型和视觉语言模型的透明度和可问责性。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [40] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 本文介绍了PakBBQ，这是一个针对巴基斯坦文化和社会背景的基准数据集，用于评估大型语言模型的公平性。实验结果显示，通过消歧和负面问题框架可以有效减少偏见。


<details>
  <summary>Details</summary>
Motivation: 大多数LLMs是在以西方为中心的数据上训练和评估的，很少关注低资源语言和区域背景。因此，需要一个针对巴基斯坦文化和社会背景的基准数据集来评估LLMs的公平性。

Method: 我们引入了PakBBQ，这是原始Bias Benchmark for Question Answering (BBQ)数据集的文化和区域适应性扩展。PakBBQ包含超过214个模板，17180个QA对，涵盖8个类别，包括年龄、残疾、外貌、性别、社会经济地位、宗教、地区归属和语言正式性等与巴基斯坦相关的偏见维度。我们评估了多种多语言LLMs在模糊和明确消歧上下文以及负面与非负面问题框架下的表现。

Result: 实验结果表明，(i) 在消歧情况下平均准确率提高了12%，(ii) 在乌尔都语中比在英语中表现出更强的反偏见行为，(iii) 问题框架的影响显著减少了刻板印象的回答。

Conclusion: 这些发现突显了情境化基准和简单的提示工程策略在低资源环境中减轻偏见的重要性。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [41] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架SDM，用于检测LLM生成的非事实性文本。通过结合信息理论度量和联合聚类方法，SDM能够更准确地检测语义偏差，并提供了一个诊断框架Semantic Box来分类LLM响应类型。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在检测LLM生成的非事实性文本方面存在不足，特别是对于严重偏离输入上下文的响应。本文旨在提出一种更精确、更全面的检测方法。

Method: 本文的方法基于句子嵌入的联合聚类，创建了提示和答案的共享主题空间。通过计算信息理论度量，如Jensen-Shannon散度和Wasserstein距离，以及KL散度，来衡量提示和响应之间的语义差异。

Result: 本文提出的SDM框架能够有效检测Faithfulness Hallucinations，并通过Semantic Box框架对LLM响应进行分类。实验结果表明，该方法在检测Confabulations方面具有较高的准确性。

Conclusion: 本文提出了Semantic Divergence Metrics (SDM)，一种用于检测Faithfulness Hallucinations的轻量级框架。通过结合信息理论度量和联合聚类方法，SDM能够更准确地检测LLM响应中的语义偏差，并提供了一个诊断框架Semantic Box来分类LLM响应类型。

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [42] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 本研究比较了四种深度学习架构在短文本序列emoji预测中的表现，发现BERT总体表现最好，而CNN在罕见emoji类别上更有效。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过深度学习架构进行短文本序列的emoji预测，以改进人机交互。

Method: 使用四种深度学习架构（前馈网络、CNN、Transformer和BERT）进行短文本序列的emoji预测，并使用TweetEval数据集解决类别不平衡问题，采用焦点损失和正则化技术。

Result: BERT在整体性能上表现最佳，由于其预训练优势，而CNN在罕见emoji类别上表现出色。

Conclusion: 本研究展示了架构选择和超参数调整对于情感感知的emoji预测的重要性，有助于改进人机交互。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [43] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 研究显示，大型语言模型可以有效预测BPRS评分，从而改善和标准化临床高风险精神分裂症患者的评估。


<details>
  <summary>Details</summary>
Motivation: 临床高风险（CHR）精神分裂症患者需要密切监测症状以指导适当的治疗，但BPRS由于需要长时间的结构化访谈而未被广泛用于临床实践。

Method: 利用大型语言模型（LLMs）从临床访谈转录文本中预测BPRS评分，并评估其在不同语言和纵向信息整合中的表现。

Result: LLM预测的BPRS评分与真实评估相比，中位一致性为0.84，ICC为0.73，接近人类评委的一致性。此外，LLM在不同语言中的评估准确性和纵向信息整合方面也表现出色。

Conclusion: 研究结果表明，大型语言模型在预测BPRS评分方面表现出色，具有改善和标准化CHR患者评估的潜力。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [44] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 本研究采用计算和基于语料的方法，探讨了托基波纳语中的语言变化和变异，发现社会语言学因素影响该语言的方式与自然语言类似，并且构造语言系统会随着社区的使用而自然演变。


<details>
  <summary>Details</summary>
Motivation: 探索托基波纳语中的语言变化和变异，了解社会语言学因素如何影响这种构造语言。

Method: 采用计算和基于语料的方法，研究包括流体词类和及物性，以检查内容词在不同句法位置上的偏好随时间的变化以及不同语料库中的使用差异。

Result: 研究结果表明，社会语言学因素影响托基波纳语的方式与自然语言相同，构造语言系统在社区使用过程中会自然演变。

Conclusion: 研究结果表明，社会语言学因素以与自然语言相同的方式影响托基波纳语，并且即使是在构造的语言系统中，也会随着社区的使用而自然演变。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [45] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 本文提出了一种利用LLM输出来创建与模型归纳偏差相匹配的提示的方法，并展示了该方法在提高LLM评分方面的有效性。


<details>
  <summary>Details</summary>
Motivation: LLM对提示措辞的小变化很敏感，这可以部分归因于LLM中存在的归纳偏差。

Method: 通过使用LLM的输出作为其提示的一部分，可以更容易地创建令人满意的提示措辞，从而创建与模型的归纳偏差相匹配的提示。

Result: 实证研究表明，使用这种归纳偏差提取和匹配策略可以提高LLM用于分类的Likert评分高达19%，用于排序的Likert评分高达27%。

Conclusion: 使用这种归纳偏差提取和匹配策略可以提高LLM的分类和排序的Likert评分。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [46] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本研究提出了一种定性的、话语框架，以补充现有的偏见检测方法。通过分析LLM生成的关于黑人和白人女性的短篇故事，研究发现黑人女性被描绘为与祖先和抵抗联系在一起，而白人女性则出现在自我发现的过程中。这表明语言模型可能复制固化的言语表征，强化本质主义和社会停滞感。当被要求纠正偏见时，模型提供的修改往往是表面的，未能真正解决偏见问题。研究强调了对AI设计和部署进行批判性和跨学科方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，大型语言模型（LLMs）在各种情境中得到应用。然而，它们可能再现偏见，如歧视和种族化，同时维持霸权话语。当前的偏见检测方法主要依赖于定量、自动的方法，往往忽视自然语言中偏见出现的细微方式。因此，需要一种定性方法来帮助开发者和用户识别偏见在LLM输出中的具体表现。

Method: 本研究提出了一种定性的、话语框架，以补充现有的偏见检测方法。通过对手动分析LLM生成的关于黑人和白人女性的短篇故事，研究探讨了性别和种族偏见。

Result: 结果显示，黑人女性被描绘为与祖先和抵抗联系在一起，而白人女性则出现在自我发现的过程中。这些模式反映了语言模型如何复制固化的言语表征，强化本质主义和社会停滞感。当被要求纠正偏见时，模型提供了表面的修改，保持了有问题的意义，揭示了在促进包容性叙述方面的局限性。

Conclusion: 研究结果表明，算法具有意识形态功能，并对AI的伦理使用和发展有重要影响。研究强调了对AI设计和部署进行批判性和跨学科方法的必要性，以解决LLM生成的论述如何反映和加剧不平等的问题。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [47] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文介绍了ReviewRL，这是一个基于强化学习的框架，用于生成全面且事实准确的科学论文评审。通过结合检索增强的上下文生成管道、监督微调和具有复合奖励函数的强化学习过程，ReviewRL在ICLR 2025论文的实验中显著优于现有方法，展示了在科学发现中的自动批评生成的潜力。


<details>
  <summary>Details</summary>
Motivation: Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews.

Method: Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy.

Result: Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments.

Conclusion: ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain.

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [48] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: DOTABLER是一个以表格为中心的语义文档解析框架，通过深入理解表格与上下文的关系，实现了表格导向的文档结构解析和领域特定的表格检索。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在表面任务，如布局分析、表格检测和数据提取，缺乏对表格及其上下文关联的深层语义解析，这限制了跨段落数据解释和一致性的分析。

Method: DOTABLER是一个以表格为中心的语义文档解析框架，利用自定义数据集和预训练模型的领域特定微调，整合完整的解析流程来识别与表格语义相关的上下文段落。

Result: DOTABLER在近4000页的真实PDF中进行了评估，包含超过1000个表格，实现了超过90%的精确度和F1分数。

Conclusion: DOTABLER在表格-上下文语义分析和深度文档解析方面表现出色，相比GPT-4o等先进模型具有优势。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [49] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: FreLLM4Rec is an approach that balances semantic and collaborative information from a spectral perspective to improve LLM-based recommendation systems.


<details>
  <summary>Details</summary>
Motivation: LLM-based recommenders exhibit a tendency to overemphasize semantic correlations within users' interaction history. When taking pretrained collaborative ID embeddings as input, LLM-based recommenders progressively weaken the inherent collaborative signals as the embeddings propagate through LLM backbones layer by layer, as opposed to traditional Transformer-based sequential models in which collaborative signals are typically preserved or even enhanced for state-of-the-art performance.

Method: FreLLM4Rec, an approach designed to balance semantic and collaborative information from a spectral perspective. Item embeddings that incorporate both semantic and collaborative information are first purified using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant high-frequency noise. Temporal Frequency Modulation (TFM) then actively preserves collaborative signal layer by layer. Note that the collaborative preservation capability of TFM is theoretically guaranteed by establishing a connection between the optimal but hard-to-implement local graph fourier filters and the suboptimal yet computationally efficient frequency-domain filters.

Result: Extensive experiments on four benchmark datasets demonstrate that FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves competitive performance, with improvements of up to 8.00\% in NDCG@10 over the best baseline.

Conclusion: FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves competitive performance, with improvements of up to 8.00\% in NDCG@10 over the best baseline. Our findings provide insights into how LLMs process collaborative information and offer a principled approach for improving LLM-based recommendation systems.

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [50] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨语言提示编码器（XPE）和双软提示机制，以提高模型在低性能语言和多语言环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索软提示在跨语言迁移中的潜力，特别是在低性能语言中提升模型表现。

Method: 引入了Cross-Prompt Encoder (XPE)，结合了轻量级编码架构和多源训练，并提出了Dual Soft Prompt机制。

Result: 实验表明XPE在低性能语言中表现出色，而混合方法在多语言设置中更具适应性。

Conclusion: XPE在低性能语言中最为有效，而混合变体在多语言设置中提供了更广泛的适应性。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [51] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段微调方法，使Qwen3 14B模型能够原生地用韩语进行思考。通过监督微调和强化学习，显著提升了模型在韩语任务和一般推理能力上的表现，并在数学和编码任务中取得了显著进步。


<details>
  <summary>Details</summary>
Motivation: 为了使大型语言模型能够原生地用韩语进行思考，提高其在韩语任务中的表现以及一般推理能力。

Method: 我们提出了一种两阶段微调方法，使大型语言模型Qwen3 14B能够原生地用韩语进行思考。第一阶段是在高质量的韩语推理数据集上进行监督微调（SFT），建立了强大的韩语逻辑推理基础，显著提高了韩语任务的表现，并且在一般推理能力上也有所提升。第二阶段采用了定制的组相对策略优化（GRPO）算法进行强化学习，以进一步增强韩语推理对齐和整体问题解决性能。

Result: 我们的方法实现了稳定的训练（避免了原始GRPO中的崩溃现象），并带来了稳定、渐进的性能提升。最终的RL微调模型在高级推理基准测试（尤其是数学和编码任务）中表现出显著改进，同时保持了知识和语言能力，成功地在其内部思维链中完全使用韩语。

Conclusion: 我们的方法在高级推理基准测试（尤其是数学和编码任务）中表现出显著改进，同时保持了知识和语言能力，成功地在其内部思维链中完全使用韩语。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [52] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [53] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 本文利用大型语言模型在捷克文本摘要任务中取得了新进展，并引入了一个新的历史捷克文档摘要数据集。


<details>
  <summary>Details</summary>
Motivation: 由于语言复杂性和标注数据集的稀缺性，捷克文本摘要，特别是历史文献的摘要，仍研究不足。

Method: 使用Mistral和mT5等大型语言模型进行捷克文本摘要。

Result: 在现代捷克摘要数据集SumeCzech上实现了新的最先进结果，并引入了一个名为Posel od Čerchova的新数据集用于历史捷克文档的摘要。

Conclusion: 这些贡献为推进捷克文本摘要和开放捷克历史文本处理的新研究方向提供了巨大潜力。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [54] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨语言基于方面的情感分析方法，通过使用带有约束解码的序列到序列模型，提高了跨语言性能并支持多任务处理。研究在多种语言和任务中验证了该方法的有效性，并提供了实际应用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管基于方面的情感分析（ABSA）已经取得了显著进展，但低资源语言仍然面临挑战，通常被忽视，而偏向于英语。当前的跨语言ABSA方法专注于有限、不太复杂的任务，并且常常依赖外部翻译工具。

Method: 本文引入了一种新的方法，使用带有约束解码的序列到序列模型，消除了对不可靠翻译工具的依赖，并通过约束解码提高了跨语言性能。此外，该方法支持多任务处理，使一个模型可以解决多个ABS A任务。

Result: 我们在七种语言和六个ABSA任务上评估了我们的方法，超越了最先进的方法，并为以前未探索的任务设定了新基准。此外，我们评估了大型语言模型（LLM）在零样本、少样本和微调场景中的表现。虽然LLM在零样本和少样本设置中表现不佳，但微调在与较小的多语言模型相比时取得了有竞争力的结果，尽管训练和推理时间更长。

Conclusion: 本文提供了关于跨语言ABS A方法的实用建议，为现实世界的应用增强了理解。研究展示了跨语言ABS A方法的优势和局限性，推动了该挑战性研究领域的最新进展。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [55] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 本文提出了一种混合评估框架MDH，用于评估和清理数据集中的恶意内容，并提出了两种新的越狱攻击策略D-Attack和DH-CoT。


<details>
  <summary>Details</summary>
Motivation: 现有的红队数据集包含不适合的提示，需要评估和清理恶意内容。然而，现有的恶意内容检测方法要么依赖人工标注，要么依赖大语言模型，但准确性不一致。

Method: 本文提出了一种混合评估框架MDH，结合了基于LLM的注释和最小的人类监督，并应用该框架进行数据集清理和检测越狱响应。此外，还提出了两种新的策略D-Attack和DH-CoT。

Result: 本文提出的MDH框架能够有效评估和清理数据集中的恶意内容，并提高了越狱攻击的成功率。此外，D-Attack和DH-CoT策略也显示出显著的效果。

Conclusion: 本文提出了一种混合评估框架MDH，结合了基于LLM的注释和最小的人类监督，以平衡准确性和效率。此外，还提出了两种新的策略D-Attack和DH-CoT，以提高越狱攻击的成功率。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [56] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 本文提出了一种新的黑盒攻击方法，利用大型模型的可解释性，通过稀疏自编码器识别和操纵文本中的关键特征，生成能够绕过现有防御机制的对抗文本。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理（NLP）特别是大型语言模型（LLMs）的迅速普及，生成对抗示例以突破LLMs仍然是理解模型漏洞和提高鲁棒性的关键挑战。

Method: 我们提出了一种新的黑盒攻击方法，利用大型模型的可解释性。我们引入了稀疏特征扰动框架（SFPF），这是一种用于对抗文本生成的新方法，该方法利用稀疏自编码器来识别和操纵文本中的关键特征。使用SAE模型重建隐藏层表示后，我们对成功攻击的文本进行特征聚类，以识别具有更高激活度的特征。这些高度激活的特征随后被扰动以生成新的对抗文本。这种选择性扰动保留了恶意意图，同时放大了安全信号，从而增加了它们逃避现有防御的可能性。

Result: 实验结果表明，SFPF生成的对抗文本可以绕过最先进的防御机制，揭示了当前NLP系统中的持续漏洞。然而，该方法在不同提示和层上的效果有所不同，其在其他架构和更大模型上的泛化能力仍有待验证。

Conclusion: 我们的方法 enables a new red-teaming strategy that balances adversarial effectiveness with safety alignment. Experimental results demonstrate that adversarial texts generated by SFPF can bypass state-of-the-art defense mechanisms, revealing persistent vulnerabilities in current NLP systems.

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [57] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: 本文提出了 ComoRAG，一种基于动态记忆和迭代推理的新型检索增强方法，用于长故事和小说的叙事理解，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统 RAG 方法由于其无状态、单步检索过程而可能不足，因为它们常常忽略了在长距离上下文中捕捉相互关联关系的动态性质。

Method: ComoRAG 通过迭代推理循环与动态记忆工作区交互，在每个循环中生成探测查询以制定新的探索路径，并将新方面的检索证据整合到全局记忆池中，从而支持查询解决的连贯上下文的出现。

Result: 在四个具有挑战性的长上下文叙事基准（200K+ tokens）上，ComoRAG 的表现优于强大的 RAG 基线，与最强基线相比，相对提升高达 11%。进一步分析表明，ComoRAG 在需要全局理解的复杂查询方面具有显著优势。

Conclusion: ComoRAG 提供了一种有原则的、受认知启发的范式，用于基于检索的长上下文理解，以实现状态化推理。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [58] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 本文研究了汉语成语翻译的问题，提出了一个评估框架并开发了改进的模型来检测翻译错误。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在机器翻译方面取得了进展，但对汉语成语翻译的研究仍然不足。

Method: 本文介绍了IdiomEval框架，该框架具有全面的错误分类法，并对900个翻译对进行了注释，以评估现代系统的翻译表现。

Result: 研究发现，现有的评估指标对成语质量的测量效果不佳，且最佳系统GPT-4在28%的情况下仍会出现错误。

Conclusion: 本文提出了改进的模型，能够在检测汉语成语翻译错误方面达到F1分数0.68。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [59] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 本文提出了一种基于计算经济学的框架，通过激励驱动的训练范式优化大型语言模型的计算效率，从而在保持准确性的同时降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型受到显著计算成本的限制。我们需要一种方法来在有限的计算资源下优化模型性能。

Method: 我们引入了一个“计算经济学”框架，将大型语言模型视为一个内部经济体系，其中资源受限的代理（注意力头和神经元块）必须分配稀缺计算以最大化任务效用。我们提出了一个激励驱动的训练范式，通过加入可微分的计算成本项来增强任务损失，鼓励稀疏和高效的激活。

Result: 该方法在GLUE（MNLI, STS-B, CoLA）和WikiText-103上产生了一组模型，它们划定了帕累托前沿，并且在相似准确性下获得了大约40%的FLOPS减少和更低的延迟，同时具有更可解释的注意力模式。

Conclusion: 这些结果表明，经济原则为在严格资源限制下设计高效、自适应和更透明的大型语言模型提供了一条有根据的途径。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [60] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: DiFaR是一个检测器无关的框架，旨在生成多样、准确且相关的文本理由，以提高虚假信息检测的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型视觉语言模型生成文本理由的方法在多样性、事实性和相关性方面存在不足，限制了其在可训练多模态虚假信息检测中的效果。

Method: DiFaR采用五种思维链提示来激发LVLMs的不同推理轨迹，并结合一个轻量级的后处理过滤模块，根据句子级别的事实性和相关性得分选择理由句子。

Result: DiFaR在四个流行的基准测试中表现出色，比四种基线类别高出最多5.9%，并使现有检测器的性能提升了多达8.7%。自动指标和人工评估均证实了DiFaR在所有三个维度上显著提升了理由质量。

Conclusion: DiFaR显著提高了虚假信息检测中生成的文本理由的质量，通过解决多样性、事实性和相关性问题，展示了其有效性。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [61] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文研究了NLP中隐私与可解释性之间的关系，发现两者可以共存，并提出了未来研究的建议。


<details>
  <summary>Details</summary>
Motivation: 当前对于可解释性和隐私保护NLP的研究越来越多，但两者交叉领域的研究仍较少，因此需要进一步探索隐私和可解释性是否可以同时实现。

Method: 本文通过实证研究，探讨了在NLP中隐私与可解释性之间的权衡问题，主要采用差分隐私（DP）和事后可解释性方法进行研究。

Result: 研究发现，隐私和可解释性之间存在复杂的相互关系，这受到下游任务性质以及文本隐私化和可解释性方法选择的影响。

Conclusion: 本文总结了在NLP中实现隐私和可解释性共存的潜在可能性，并提出了对未来研究的实用建议。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [62] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 本文首次系统地研究了跨多种数据模态的文本主导现象，并提出了评估指标和解决方案以实现更公平的多模态语言模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在各种多模态任务中表现出色，但它们存在一个核心问题，即文本主导：它们在推理过程中严重依赖文本，而未能充分利用其他模态。

Method: 我们提出了两种评估指标：模态主导指数（MDI）和注意力效率指数（AEI），并提出了一种简单的令牌压缩方法，以有效重新平衡模型注意力。

Result: 我们的分析表明，文本主导在所有测试的模态中都是显著且普遍的。通过应用这种方法，例如将LLaVA-7B的MDI从10.23大幅降低到一个平衡值0.86。

Conclusion: 我们的分析和方法框架为开发更公平和全面的多模态语言模型奠定了基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [63] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 本文介绍了一个欧洲深度推理网络（eDIF）的可行性研究，旨在支持大型语言模型的机制可解释性研究，并通过一个基于GPU的集群和NNsight API实现远程模型检查。


<details>
  <summary>Details</summary>
Motivation: 欧洲需要广泛获取LLM可解释性基础设施，以推动研究界对先进模型分析能力的民主化。

Method: 该项目引入了一个基于GPU的集群，位于安斯巴赫应用科学大学，并与合作伙伴机构互联，通过NNsight API实现远程模型检查。进行了一项结构化的试点研究，涉及来自欧洲的16位研究人员，评估了平台的技术性能、可用性和科学实用性。

Result: 研究显示用户参与度逐渐增加，平台性能稳定，远程实验能力受到积极评价。同时，该项目标志着建立围绕平台的用户社区的起点。

Conclusion: 该研究标志着欧洲在广泛获取LLM可解释性基础设施方面的重要进展，并为更广泛的部署、扩展工具和持续的社区合作奠定了基础。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [64] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 本研究首次系统地研究了将科普特语翻译成法语的策略，发现使用风格多样且具有噪声意识的训练语料库进行微调可以显著提高翻译质量，并为开发历史语言的翻译工具提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在系统地研究将科普特语翻译成法语的策略，以提供开发历史语言翻译工具的实用见解。

Method: 我们系统地评估了转换与直接翻译、预训练的影响、多版本微调的优势以及模型对噪声的鲁棒性。

Result: 我们利用对齐的圣经语料库证明，使用风格多样且具有噪声意识的训练语料库进行微调可以显著提高翻译质量。

Conclusion: 我们的研究为开发历史语言的翻译工具提供了重要的实用见解。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [65] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 本文提出了一种结合基于图的方法和Transformer架构的手语翻译方法，实现了更有效的无词典翻译，并在多个手语数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于社会对口语语言的重视，手语常常被低估，导致沟通障碍和社会排斥。因此，需要改进手语翻译方法，以提高聋哑人和听力障碍者的沟通可及性。

Method: 我们的方法结合了基于图的方法和Transformer架构，融合了Transformer和STGCN-LSTM架构，以实现更有效的无词典翻译。

Result: 我们的方法在RWTH-PHOENIX-2014T、CSL-Daily、How2Sign和BornilDB v1.0等多样化的手语数据集上实现了新的最先进性能。在RWTH-PHOENIX-2014T、CSL-Daily和How2Sign数据集上，我们的方法分别超越了GASLT、GASLT和slt_how2sign的BLEU-4分数，分别为4.01、2.07和0.5。

Conclusion: 我们的方法在所有数据集上都表现出色，展示了BLEU-4分数的显著提升，超越了现有的翻译结果。此外，我们在BornilDB v1.0数据集上进行了首次基准测试，为未来的研究设定了基准，强调了无词典翻译对于提高聋哑人和听力障碍者沟通可及性的重要性。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [66] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 本文提出了一种新的个性化问答框架VAC，通过自然语言反馈提高模型的学习效率和个人化质量。


<details>
  <summary>Details</summary>
Motivation: 当前方法使用标量奖励作为反馈信号，但这些信号有时较弱且缺乏指导性，限制了学习效率和个人化质量。

Method: 引入了一种名为VAC的新框架，该框架用基于用户资料和问题叙述生成的自然语言反馈（NLF）替代了标量奖励。

Result: 在包含三个不同领域的LaMP-QA基准测试中，实验结果显示出显著的改进，并且人类评估也证实了生成响应的优越质量。

Conclusion: 实验结果表明，自然语言反馈（NLF）为优化个性化问答提供了更有效的信号。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [67] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为ICE的新框架，用于扩散大语言模型的就地提示策略，通过结合置信度感知的提前退出机制，显著提升了模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）由于其前缀仅提示范式和顺序生成过程，在双向信息方面存在局限性。扩散大语言模型（dLLMs）提供了通过双向注意力机制和迭代优化过程实现更灵活的就地提示策略的新机会。

Method: ICE（In-Place Chain-of-Thought Prompting with Early Exit）是一种新的框架，它将前缀仅提示转换为针对扩散大语言模型（dLLMs）设计的就地提示，并结合了置信度感知的提前退出机制以减少计算开销。

Result: 实验表明，ICE在GSM8K数据集上实现了高达17.29%的准确率提升，并在MMLU数据集上实现了高达276.67倍的加速，同时保持了竞争力的表现。

Conclusion: ICE框架通过将前缀仅提示转换为就地提示，显著提高了扩散大语言模型的灵活性和效率，并在多个基准测试中表现出色。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [68] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种结构化的自动化新颖性评估方法，通过模拟专家审稿人的行为，在NLP领域实现了较高的准确率，并展示了其在提高同行评审质量方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 新颖性评估是同行评审中的核心但研究不足的方面，特别是在NLP等高产量领域，审稿人能力日益紧张。

Method: 通过三个阶段建模专家审稿人行为：从提交中提取内容，检索和综合相关工作，以及结构化比较以进行基于证据的评估。

Result: 在182个ICLR 2025提交中进行评估，与人类推理的对齐度达到86.5%，在新颖性结论上的协议率达到75.3%，明显优于现有的LLM基线。

Conclusion: 该方法展示了结构化的LLM辅助方法在支持更严格和透明的同行评审中的潜力，同时不会取代人类专业知识。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [69] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 本文提出了一种新的后训练算法MS-GRPO，用于改进大型语言模型在多步骤代理任务中的表现，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练方法主要针对单次交互设计，无法处理多步骤代理任务中的信用分配问题。因此，需要一种能够改进较小模型的方法，以提高它们在多步骤代理任务中的表现。

Method: 本文引入了一种新的后训练算法，称为多步骤组相对策略优化（MS-GRPO），该算法基于正式的文本媒介随机博弈（TSMG）和语言代理策略（LAP）框架，并采用了一种新颖的绝对优势加权情节采样策略。

Result: 实验结果表明，经过后训练的3B参数模型在Frozen Lake任务上比72B参数基线模型提高了50%。

Conclusion: 本文展示了针对大型语言模型（LLMs）的后训练方法在创建序列决策代理方面的实用性和效率，证明了通过有针对性的后训练可以替代依赖模型规模的方法。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [70] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 本文介绍了Psyche-R1，这是一个结合同理心、心理学专业知识和推理的中文心理大语言模型。通过一个综合的数据合成管道生成高质量的心理问题和共情对话，并采用混合训练策略提升模型性能。实验结果表明，Psyche-R1在多个心理基准测试中表现优异，其7B版本与671B的DeepSeek-R1相当。


<details>
  <summary>Details</summary>
Motivation: 由于合格的心理健康专业人员短缺，将大型语言模型（LLMs）整合到心理应用中提供了一种缓解心理健康障碍日益增长负担的有希望的方法。然而，目前的研究主要集中在情感支持和共情对话上，对有助于生成可靠回答的推理机制关注有限。

Method: 本文设计了一个综合的数据合成管道，生成了超过75k个高质量的心理问题及其详细推理过程，并结合73k个共情对话。随后，采用混合训练策略，通过多LLM交叉选择策略识别挑战性样本进行组相对策略优化（GRPO），以提高推理能力，其余数据用于监督微调（SFT）以增强共情响应生成和心理学领域知识。

Result: 广泛的实验结果证明了Psyche-R1在多个心理基准测试中的有效性。7B版本的Psyche-R1在性能上与671B的DeepSeek-R1相当。

Conclusion: 本文提出了Psyche-R1，这是第一个结合同理心、心理学专业知识和推理的中文心理大语言模型。实验结果表明，Psyche-R1在多个心理基准测试中表现出色，其7B版本的表现与671B的DeepSeek-R1相当。

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [71] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种多维建模框架，结合特征工程、数据增强和可解释机器学习，以提高自动化口译质量评估的可解释性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究在语言使用质量方面缺乏充分的检查，由于数据稀缺和不平衡导致模型效果不佳，并且缺乏对模型预测的解释努力。

Method: 我们提出了一个集成特征工程、数据增强和可解释机器学习的多维建模框架。这种方法通过仅使用构建相关的透明特征并进行Shapley Value (SHAP)分析，优先考虑可解释性而不是“黑箱”预测。

Result: 我们的结果在新的英中连续口译数据集上显示出强大的预测性能，确定BLEURT和CometKiwi得分是忠实度最强的预测特征，与停顿相关的特征是流利度的预测特征，而中文特定的短语多样性指标是语言使用的预测特征。

Conclusion: 通过强调可解释性，我们提出了一种可扩展、可靠和透明的传统人工评估的替代方案，有助于为学习者提供详细的诊断反馈，并支持自我调节学习的优势，这是单独的自动化评分无法提供的。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [72] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）作为强化学习中代理搜索任务的高效模拟器的潜力，通过减少对外部搜索引擎的依赖来降低成本。研究提出了一种名为Self-Search的方法，用于量化LLMs的内在搜索能力，并引入了Self-Search RL（SSRL），通过基于格式和规则的奖励来增强LLMs的Self-Search能力。实验结果表明，SSRL训练的策略模型提供了一个成本效益高且稳定的环境，减少了对外部搜索引擎的依赖，并促进了从模拟到现实的迁移。研究结论表明LLMs具有可以有效激发以实现高性能的世界知识，SSRL展示了利用内部知识减少幻觉的潜力，SSRL训练的模型可以无缝集成到外部搜索引擎中。


<details>
  <summary>Details</summary>
Motivation: We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines.

Method: We first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards.

Result: Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer.

Conclusion: LLMs possess world knowledge that can be effectively elicited to achieve high performance; SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [73] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文是对扩散语言模型（DLMs）的全面综述，涵盖了其发展、与其他范式的联系、基础原理和最先进的模型。文章还讨论了DLMs在推理策略和优化方面的最新进展，并探讨了多模态扩展及其应用。此外，文章还指出了DLMs的局限性和挑战，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）作为一种新兴的生成模型，具有减少推理延迟和捕捉双向上下文的优势，使其成为自然语言处理任务中的有力选择。然而，目前对于DLMs的系统性综述仍然不足，因此本文旨在提供一个全面的概述，以帮助研究人员更好地理解和应用DLMs。

Method: 本文通过综述的方式，对扩散语言模型（DLMs）进行了全面的分析，涵盖了其发展、与其他范式的联系、基础原理和最先进的模型。同时，文章还讨论了DLMs在推理策略和优化方面的最新进展，并探讨了多模态扩展及其应用。

Result: 本文提供了关于DLMs的最新综述，涵盖了其发展、与其他范式的联系、基础原理和最先进的模型。文章还分析了DLMs在推理策略和优化方面的最新进展，并探讨了多模态扩展及其应用。此外，文章还指出了DLMs的局限性和挑战，并提出了未来的研究方向。

Conclusion: 本文对扩散语言模型（DLMs）进行了全面的综述，涵盖了其发展、与其他范式的联系、基础原理和最先进的模型。文章还提供了最新的技术分析，包括预训练策略和高级微调方法，并讨论了DLMs在推理策略和优化方面的最新进展。此外，文章还探讨了DLMs的多模态扩展及其应用，并指出了当前DLMs的局限性和挑战，以及未来的研究方向。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [74] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 本文介绍了Amazon Nova AI Challenge中大学团队和Amazon团队在提升AI软件开发安全性方面的进展和合作努力。


<details>
  <summary>Details</summary>
Motivation: 为了确保AI系统在软件开发中的安全性，Amazon发起了Trusted AI track of the Amazon Nova AI Challenge，以推动安全AI的进步。

Method: 通过全球竞赛的形式，五个团队开发自动化红队机器人，另外五个团队创建安全的AI助手，利用对抗性比赛评估自动红队和安全对齐方法。

Result: 团队开发了最先进的技术，引入了基于推理的安全对齐、鲁棒模型防护、多轮越狱和高效探测大型语言模型的新方法。

Conclusion: 本文总结了大学团队和Amazon Nova AI Challenge团队在解决AI软件开发安全挑战方面的进展，强调了这一合作努力如何提高AI安全的标准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [75] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出了一种范式转变，将人工智能重新定位为诊断过程的主要指导者，而不是医生的助手。通过介绍DxDirector-7B，一种具有先进深度思考能力的大语言模型，能够在最小医生参与的情况下驱动全过程诊断，并建立了一个针对误诊的责任框架。评估结果显示，DxDirector-7B在诊断准确性和减少医生工作量方面表现出色，具有成为医学专家可行替代品的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能在临床诊断中的作用主要是作为医生的助手，只能在诊断过程的特定部分回答特定的医学问题，缺乏从模糊主诉开始驱动整个诊断过程的能力，这仍然严重依赖于人类医生。这种差距限制了人工智能全面减轻医生工作负担和提高诊断效率的能力。

Method: 我们提出了一个范式转变，将医生和人工智能之间的关系逆转：将人工智能重新定位为主要的指导者，医生作为其助手。我们介绍了DxDirector-7B，这是一种具有先进深度思考能力的大语言模型，能够以最少的医生参与驱动全过程诊断。此外，DxDirector-7B建立了一个针对误诊的稳健责任框架，明确了人工智能和人类医生之间的责任。

Result: 在全过程中诊断设置下的罕见、复杂和现实案例评估中，DxDirector-7B不仅实现了显著的诊断准确性，而且比最先进的医疗大语言模型以及通用大语言模型显著减少了医生的工作量。细粒度分析多个临床部门和任务验证了其有效性，专家评估表明其潜力可以作为医学专家的可行替代品。

Conclusion: 这些发现标志着一个新时代的开始，人工智能不再只是医生的助手，而是能够驱动整个诊断过程，大幅减少医生的工作量，表明了一个高效准确的诊断解决方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [76] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 本文研究了语言模型对齐过程中的两个阶段，并提出了一个有效的算法来识别它们之间的边界。实验结果表明，该假设具有普遍性，并且可以应用于多种模型和对齐方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型与人类偏好的对齐对于构建可靠的AI系统至关重要。然而，现有的方法在使用在线策略数据时可能并不总是最优的，因此需要更深入的理解和改进。

Method: 本文提出了对齐阶段假设，将对齐过程分为两个阶段：偏好注入阶段和偏好微调阶段。通过理论和实证分析，我们characterized这些阶段，并提出了一种有效的算法来识别它们之间的边界。

Result: 实验结果表明，不同模型和对齐方法在对齐阶段假设下表现出不同的效果。例如，在Llama-3上，在线策略数据的效果是静态数据的3倍，而在Zephyr上则是0.4倍。

Conclusion: 本文通过理论和实证分析，揭示了语言模型对齐过程中的两个阶段，并提出了一个有效的算法来识别它们之间的边界。实验结果表明，该假设具有普遍性，并且可以应用于多种模型和对齐方法。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [77] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为ComMCS的方法，通过线性组合当前和后续步骤的MC估计器来构建无偏估计器，以解决LLMs在复杂领域如数学中的推理能力不足的问题。实验结果表明，该方法在MATH-500和GSM8K基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在复杂领域如数学中的推理能力不足的问题，以及由于蒙特卡洛样本数量有限导致的估计误差问题。

Method: 提出了一种名为ComMCS的方法，通过线性组合当前和后续步骤的MC估计器来构建无偏估计器。

Result: ComMCS方法在MATH-500和GSM8K基准测试中表现出色，优于基于回归的优化方法和非方差减少基线。

Conclusion: ComMCS方法在MATH-500和GSM8K基准测试中表现出色，优于基于回归的优化方法和非方差减少基线。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [78] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
*Yifan Song,Wing Yee Au,Hon Yung Wong,Brian P. Bailey,Tal August*

Main category: cs.HC

TL;DR: 本研究探讨了术语障碍对跨学科交流的影响，并设计了一个个性化的术语支持系统ParseJargon，实验证明其能有效提升理解和参与度。


<details>
  <summary>Details</summary>
Motivation: 有效跨学科交流经常受到领域特定术语的阻碍。为了深入探讨术语障碍，我们进行了这项研究。

Method: 我们进行了一项形成性日记研究，涉及16名专业人士，揭示了当前术语管理策略在工作场所会议中的关键限制。基于这些见解，我们设计了ParseJargon，这是一个交互式LLM驱动的系统，提供实时个性化术语识别和解释，以适应用户的个人背景。

Result: 控制实验比较了ParseJargon与基线（无支持）和通用型（非个性化）条件，结果表明个性化术语支持显著提高了参与者的理解、参与度和对同事工作的欣赏，而通用型支持则对参与度产生了负面影响。后续实地研究验证了ParseJargon在实时会议中的可用性和实际价值，突出了现实世界部署的机会和限制。

Conclusion: 我们的研究结果为设计个性化的术语支持工具提供了见解，并对更广泛的跨学科和教育应用有影响。

Abstract: Effective interdisciplinary communication is frequently hindered by
domain-specific jargon. To explore the jargon barriers in-depth, we conducted a
formative diary study with 16 professionals, revealing critical limitations in
current jargon-management strategies during workplace meetings. Based on these
insights, we designed ParseJargon, an interactive LLM-powered system providing
real-time personalized jargon identification and explanations tailored to
users' individual backgrounds. A controlled experiment comparing ParseJargon
against baseline (no support) and general-purpose (non-personalized) conditions
demonstrated that personalized jargon support significantly enhanced
participants' comprehension, engagement, and appreciation of colleagues' work,
whereas general-purpose support negatively affected engagement. A follow-up
field study validated ParseJargon's usability and practical value in real-time
meetings, highlighting both opportunities and limitations for real-world
deployment. Our findings contribute insights into designing personalized jargon
support tools, with implications for broader interdisciplinary and educational
applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [79] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 本文提出了一种基于多任务学习框架的新型模型架构，用于优化个性化产品搜索排名。该方法整合了表格和非表格数据，并利用预训练的TinyBERT模型进行语义嵌入。实验结果表明，这种方法显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过整合表格和非表格数据，并利用先进的嵌入技术，在多任务学习框架下优化个性化产品搜索排名。

Method: 本文提出了一种新颖的模型架构，用于使用多任务学习（MTL）框架优化个性化产品搜索排名。我们的方法独特地整合了表格和非表格数据，利用预训练的TinyBERT模型进行语义嵌入，并提出了一种新的采样技术来捕捉多样化的客户行为。此外，我们还提出了一种基于点击率、点击位置和语义相似性的可扩展相关性标注机制，作为传统人工标注标签的替代方案。

Result: 实验结果表明，结合非表格数据与先进的嵌入技术在多任务学习范式中显著提升了模型性能。消融研究进一步强调了引入相关性标签、微调TinyBERT层以及TinyBERT查询-产品嵌入交互的好处。

Conclusion: 实验结果表明，在多任务学习范式中结合非表格数据和先进的嵌入技术显著提高了模型性能。这些结果证明了我们方法在实现改进的个性化产品搜索排名方面的有效性。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [80] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder是一个分层特征优化的检索框架，通过系统地优化候选者来提高仓库级代码补全的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码补全方法通常依赖于表面文本相似性，导致结果受到语义误导、冗余和同质性的困扰，同时无法解决外部符号歧义问题。

Method: 我们引入了Saracoder，一个分层特征优化的检索框架。其核心分层特征优化模块通过提炼深度语义关系、修剪精确重复项、评估与新颖图基度量的结构相似性（按拓扑重要性加权编辑）以及重新排序结果来系统地优化候选者。此外，一个外部感知标识符消歧模块通过依赖分析准确解决跨文件符号歧义。

Result: 在具有挑战性的CrossCodeEval和RepoEval-Updated基准测试中进行的广泛实验表明，Saracoder在多种编程语言和模型中显著优于现有基线。

Conclusion: 我们的工作证明，通过多维度系统地优化检索结果，为构建更准确和健壮的仓库级代码补全系统提供了一个新范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [81] [Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning](https://arxiv.org/abs/2508.10057)
*Christopher Pinier,Sonia Acuña Vargas,Mariia Steeghs-Turchina,Dora Matzke,Claire E. Stevenson,Michael D. Nunez*

Main category: q-bio.NC

TL;DR: 研究发现，大型语言模型在抽象推理中可能模仿人类大脑机制，提供生物和人工智能之间共享原则的初步证据。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型是否在抽象推理中反映人类神经认知。

Method: 研究比较了人类参与者和八种开源LLMs在抽象模式补全任务中的表现和神经表征，并利用模式类型差异和眼动相关电位（FRPs）进行分析。

Result: 只有最大测试的LLMs（约700亿参数）达到与人类相当的准确性，Qwen-2.5-72B和DeepSeek-R1-70B也显示出与人类模式特定难度分布的相似性。所有测试的LLMs在其中间层形成了抽象模式类别的不同聚类，尽管聚类强度随任务表现而变化。任务最优LLM层的表征几何与人类额叶FRPs呈正相关。

Conclusion: 研究结果表明，大型语言模型（LLMs）可能在抽象推理中模仿人类大脑机制，提供了生物和人工智能之间共享原则的初步证据。

Abstract: This study investigates whether large language models (LLMs) mirror human
neurocognition during abstract reasoning. We compared the performance and
neural representations of human participants with those of eight open-source
LLMs on an abstract-pattern-completion task. We leveraged pattern type
differences in task performance and in fixation-related potentials (FRPs) as
recorded by electroencephalography (EEG) during the task. Our findings indicate
that only the largest tested LLMs (~70 billion parameters) achieve
human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing
similarities with the human pattern-specific difficulty profile. Critically,
every LLM tested forms representations that distinctly cluster the abstract
pattern categories within their intermediate layers, although the strength of
this clustering scales with their performance on the task. Moderate positive
correlations were observed between the representational geometries of
task-optimal LLM layers and human frontal FRPs. These results consistently
diverged from comparisons with other EEG measures (response-locked ERPs and
resting EEG), suggesting a potential shared representational space for abstract
patterns. This indicates that LLMs might mirror human brain mechanisms in
abstract reasoning, offering preliminary evidence of shared principles between
biological and artificial intelligence.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: Nested-ReFT is a novel ReFT framework that reduces inference cost by using a subset of layers as a behavior model and mitigates off-policyness through bias mitigation, achieving better computational efficiency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: To address the high computational cost of generating multiple completions during training in standard ReFT frameworks, especially for challenging domains like mathematical reasoning.

Method: Nested-ReFT, which uses a subset of layers of the target model as a behavior model to generate off-policy completions during training, along with three variants of bias mitigation to minimize off-policyness in gradient updates.

Result: Nested-ReFT achieves improved computational efficiency (tokens/sec) across multiple math reasoning benchmarks and model sizes, while maintaining performance that matches the baseline ReFT performance.

Conclusion: Nested-ReFT provides a more computationally efficient approach to reinforced fine-tuning for advanced reasoning in LLMs while maintaining performance comparable to standard ReFT frameworks.

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [83] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习方法G-RA，用于解决长期任务中的奖励稀疏性问题，并在软件工程任务中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 在长期强化学习任务中，奖励稀疏性仍然是一个重大挑战，而现有的基于结果的奖励塑造难以在不引入偏差或需要显式任务分解的情况下定义有意义的即时奖励。此外，基于验证的奖励塑造使用逐步批评者，但即时奖励与长期目标之间的不一致可能导致奖励黑客和次优策略。

Method: 我们引入了面向软件工程的强化学习框架，以及一种名为Gated Reward Accumulation (G-RA)的新方法，该方法仅在高层次（长期）奖励达到预定义阈值时累积即时奖励。

Result: 在SWE-bench Verified和kBench上的实验表明，G-RA提高了完成率（47.6% → 93.8%和22.0% → 86.0%）和修改率（19.6% → 23.8%和12.0% → 42.0%），同时避免了由奖励不一致导致的策略退化。

Conclusion: 我们的研究强调了在长期强化学习中平衡奖励积累的重要性，并提供了一个实用的解决方案。

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [84] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: This paper explores the use of Pass@k as a reward in RLVR, showing that it can improve exploration and has potential for future research.


<details>
  <summary>Details</summary>
Motivation: To address the issue of balancing exploration and exploitation in RLVR, which leads to policies preferring conservative actions and converging to a local optimum.

Method: Using Pass@k as a reward to train the policy model and deriving an analytical solution for its advantage.

Result: Pass@k Training improves exploration ability and shows promising results for advantage design in RLVR.

Conclusion: Pass@k Training can enhance exploration ability in RLVR and show potential for future research.

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [85] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 本文综述了Memory-Augmented Transformers的研究进展，提出了一个统一框架，分析了功能目标、记忆表示和集成机制，并指出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在长程上下文保留、持续学习和知识整合方面存在关键限制，因此需要一种结合神经科学原理和工程进展的统一框架来解决这些问题。

Method: 本文通过三个分类维度（功能目标、记忆表示和集成机制）对最近的研究进展进行了组织和分析，同时探讨了核心记忆操作（读取、写入、遗忘和容量管理）。

Result: 本文揭示了从静态缓存向自适应、测试时学习系统转变的趋势，并识别了可扩展性和干扰等持续挑战，以及分层缓冲和惊喜门控更新等新兴解决方案。

Conclusion: 本文综述了将神经科学原理与工程进展相结合的Memory-Augmented Transformers框架，提出了一个统一的视角来理解记忆在Transformer架构中的作用，并指出了未来研究的方向。

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [86] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 本文提出了一种名为上下文过滤模型的新防御机制，用于提高大型语言模型的安全性，同时保持其原有性能。该模型能够有效降低越狱攻击的成功率，并且适用于所有类型的LLM，无需进行模型微调。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在性能上取得了显著进展，但各种越狱攻击带来了日益增长的安全和伦理风险。恶意用户经常利用对抗性上下文来欺骗LLMs，促使它们生成有害查询的响应。因此，需要一种有效的防御机制来提高LLMs的安全性，同时保持其原有的性能。

Method: 我们提出了一种新的防御机制，称为上下文过滤模型，这是一种输入预处理方法，旨在过滤掉不可靠和不可信的上下文，同时识别包含真实用户意图的主要提示，以揭示隐藏的恶意意图。

Result: 我们的模型在对抗越狱攻击的防御效果方面表现出色，能够将攻击成功率降低高达88%，同时保持LLMs的原始性能，实现了最先进的安全性和帮助性产品结果。

Conclusion: 我们的模型在不损害LLM原始性能的情况下，能够将越狱攻击的成功率降低高达88%，并实现了最先进的安全性和帮助性产品结果。此外，该模型是一个即插即用的方法，可以应用于所有LLM，包括白盒和黑盒模型，以提高其安全性而无需对模型本身进行微调。

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [87] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: 本文提出了一种基于搜索的框架，用于模拟隐私关键的代理交互，以发现和应对恶意代理可能引发的隐私威胁。该框架通过不断优化攻击者和防御者的指令，提高了对隐私问题的应对能力，并展示了其在多种场景和模型中的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的代理的广泛部署，恶意代理可能通过多轮互动主动获取敏感信息，从而造成严重的隐私泄露。然而，这些动态对话的演变性质使得手动预测和发现复杂的漏洞变得困难。因此，需要一种有效的框架来应对这一挑战。

Method: 本文采用了一种基于搜索的框架，通过模拟数据主体、数据发送者和数据接收者之间的交互来探索隐私关键的代理交互空间。使用LLM作为优化器，进行并行搜索和跨线程传播，以分析模拟轨迹并迭代提出新的指令。

Result: 通过该框架，攻击策略从简单的直接请求演变为复杂的多轮战术，如伪装和同意伪造，而防御措施则从基于规则的约束发展为身份验证状态机。发现的攻击和防御方法在多种场景和基础模型中具有良好的转移性，证明了其实际应用价值。

Conclusion: 本文提出了一种基于搜索的框架，用于模拟隐私关键的代理交互，以发现和应对恶意代理可能引发的隐私威胁。该框架通过不断优化攻击者和防御者的指令，提高了对隐私问题的应对能力，并展示了其在多种场景和模型中的实用性。

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [88] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了一种新的后训练范式Self-correction Flywheel，通过利用错误轨迹生成自纠正数据来改进视觉语言导航模型。实验表明，该方法在多个基准测试中取得了最先进的成果，并在真实机器人测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航模型在执行指令时常常偏离正确轨迹，但缺乏有效的错误纠正能力，阻碍了它们从错误中恢复。

Method: 提出了一种名为Self-correction Flywheel的新后训练范式，通过识别错误轨迹并生成自纠正数据来改进模型。

Result: 在R2R-CE和RxR-CE基准测试中，CorrectNav实现了新的最先进的成功率达到65.1%和69.3%。真实机器人测试显示了该方法在错误纠正、动态障碍物避让和长指令遵循方面的优越能力。

Conclusion: 实验表明，CorrectNav在R2R-CE和RxR-CE基准测试中达到了新的最先进成功率，分别达到65.1%和69.3%，比之前的最佳VLA导航模型分别高出8.2%和16.4%。真实机器人测试展示了该方法在错误纠正、动态障碍物避让和长指令遵循方面的优越能力。

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [89] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 本论文介绍了三种任务的先进深度学习方法和成果，包括历史希伯来语碎片、16至18世纪会议决议和现代英语手写识别。


<details>
  <summary>Details</summary>
Motivation: 提高历史希伯来语碎片、16至18世纪会议决议和现代英语手写识别的准确性。

Method: 我们使用了先进的深度学习技术，包括Kraken和TrOCR模型、CRNN、DeepLabV3+、Bidirectional LSTM、ResNet34编码器和CTC损失函数。

Result: 通过数据增强和信心为基础的伪标签方法，提高了字符识别的性能。

Conclusion: 本报告提供了有价值的见解，并提出了未来研究的潜在方向。

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>

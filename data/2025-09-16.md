<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.LG](#cs.LG) [Total: 11]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 本文介绍了风险隐藏攻击（RCA），一种针对金融大语言模型的新型红队方法，通过隐藏监管风险引发违规响应。构建了FIN-Bench基准评估模型安全性，实验显示RCA对多个主流模型具有高成功率，揭示了当前对齐技术的不足，并呼吁加强金融领域的监管机制。


<details>
  <summary>Details</summary>
Motivation: 现有红队研究主要针对有害内容，而忽视了监管风险。因此，需要研究金融LLM的脆弱性，以识别潜在的安全隐患。

Method: 引入了一种名为风险隐藏攻击（RCA）的新多轮框架，该框架通过迭代隐藏监管风险来引发看似合规但违反监管的响应。构建了一个名为FIN-Bench的领域特定基准，用于评估金融情境下的LLM安全性。

Result: 实验表明，RCA能够有效绕过九个主流LLM，平均攻击成功率（ASR）达到93.18%，包括GPT-4.1的98.28%和OpenAI o1的97.56%。

Conclusion: 研究揭示了当前对齐技术在金融领域中的关键漏洞，并强调了加强监管机制的紧迫性。希望这项工作能为推进稳健且领域感知的LLM对齐提供实际见解。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [2] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

TL;DR: 研究显示大型语言模型在回答问题前能够预测其答案是否正确，这表明自我评估在计算过程中中期出现。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）是否能预见他们是否会正确回答问题。

Method: 我们提取在问题被读取但任何标记生成之前激活的值，并训练线性探测器来预测模型的后续答案是否正确。

Result: 在三个开源模型家族中，基于通用常识问题的“提前正确性方向”投影在分布内和分布外知识数据集上都能预测成功，优于黑盒基线和口头预测的置信度。

Conclusion: 我们的工作为阐明大型语言模型的内部机制提供了重要的发现。

Abstract: Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [3] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
*Enora Rice,Katharina von der Wense,Alexis Palmer*

Main category: cs.CL

TL;DR: 本文讨论了计算形态学在语言文献中的应用问题，并提出需要通过以用户为中心的设计来改善模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨计算形态学与语言文献之间的脱节问题，并提出需要系统地整合以用户为中心的设计（UCD）来解决这一问题。

Method: 本文通过一个针对三个语言文献学家的小规模用户研究，分析了GlossLM模型在实际文档环境中的可用性需求是否得到满足。

Result: 尽管GlossLM在基于指标的性能上表现良好，但在实际文档环境中未能满足核心可用性需求。

Conclusion: 本文认为，将用户置于中心位置不仅能够产生更有效的工具，还能揭示更丰富、更相关的研究方向。

Abstract: Computational morphology has the potential to support language documentation
through tasks like morphological segmentation and the generation of Interlinear
Glossed Text (IGT). However, our research outputs have seen limited use in
real-world language documentation settings. This position paper situates the
disconnect between computational morphology and language documentation within a
broader misalignment between research and practice in NLP and argues that the
field risks becoming decontextualized and ineffectual without systematic
integration of User-Centered Design (UCD). To demonstrate how principles from
UCD can reshape the research agenda, we present a case study of GlossLM, a
state-of-the-art multilingual IGT generation model. Through a small-scale user
study with three documentary linguists, we find that despite strong metric
based performance, the system fails to meet core usability needs in real
documentation contexts. These insights raise new research questions around
model constraints, label standardization, segmentation, and personalization. We
argue that centering users not only produces more effective tools, but surfaces
richer, more relevant research directions

</details>


### [4] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
*Zineddine Tighidet,Andrea Mogini,Hedi Ben-younes,Jiali Mei,Patrick Gallinari,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 本文研究了熵神经元在抑制上下文复制行为中的作用，并发现它们在处理冲突信息时对大型语言模型的内部动态有重要影响。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型（LLMs）在面对与内部参数知识冲突的上下文信息时的行为不一致现象，并寻找解释预期结果分布的方法。

Method: 通过研究熵神经元在解决上下文和参数信息之间的冲突中的作用，分析它们在抑制上下文复制行为中的角色。

Result: 熵神经元负责在各种LLMs中抑制上下文复制，并且消除它们会导致生成过程发生显著变化。

Conclusion: 这些结果增强了我们对大型语言模型在处理冲突信息时内部动态的理解。

Abstract: The behavior of Large Language Models (LLMs) when facing contextual
information that conflicts with their internal parametric knowledge is
inconsistent, with no generally accepted explanation for the expected outcome
distribution. Recent work has identified in autoregressive transformer models a
class of neurons -- called entropy neurons -- that produce a significant effect
on the model output entropy while having an overall moderate impact on the
ranking of the predicted tokens. In this paper, we investigate the preliminary
claim that these neurons are involved in inhibiting context copying behavior in
transformers by looking at their role in resolving conflicts between contextual
and parametric information. We show that entropy neurons are responsible for
suppressing context copying across a range of LLMs, and that ablating them
leads to a significant change in the generation process. These results enhance
our understanding of the internal dynamics of LLMs when handling conflicting
information.

</details>


### [5] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种名为EthosAgents的轻量级、可推广的多元对齐方法，旨在模拟多样化的观点和价值观，并在医疗领域中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法在医疗领域中往往不足，因为个人、文化和情境因素塑造了多元性。

Method: 我们提出了一种轻量级、可推广的多元对齐方法EthosAgents，旨在模拟多样化的观点和价值观。

Result: 我们实证表明，EthosAgents在七种不同规模的开放和封闭模型的三种模式中推进了多元对齐。

Conclusion: 我们的研究发现，医疗相关的多元性需要适应性强且具有规范意识的方法，这为这些模型在其他高风险领域更好地尊重多样性提供了见解。

Abstract: As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [6] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

TL;DR: 本文介绍了Struct-Bench，一个用于评估包含自然语言数据的结构化数据集生成的合成数据集的框架和基准，旨在提供一个标准化的评估平台。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据评估技术（如FID）难以捕捉结构化数据集的结构特性和相关性，因此需要一种新的评估框架来解决这个问题。

Method: 本文提出的方法是Struct-Bench，它要求用户提供数据集结构的上下文无关语法（CFG）表示，并包含多个评估指标和一个排行榜。

Result: Struct-Bench展示了即使对于最先进的差分隐私合成数据生成方法，这些数据集也具有很大的挑战性。此外，还展示了一个案例研究，说明如何使用Struct-Bench来提高结构化数据的合成数据质量。

Conclusion: 本文提出了Struct-Bench，这是一个用于评估包含自然语言数据的结构化数据集生成的合成数据集的框架和基准。该基准包括5个真实世界和2个合成数据集，并提供了参考实现和排行榜，为研究人员提供了一个标准化的评估平台。

Abstract: Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [7] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
*Pengcheng Jiang,Siru Ouyang,Yizhu Jiao,Ming Zhong,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: 本文探讨了RAS增强生成方法，以解决大型语言模型在现实世界应用中的挑战，并提供了关于检索机制、文本结构技术和知识集成的全面概述。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实世界应用中面临生成幻觉、过时知识和有限领域专业知识等关键挑战。RAS增强生成通过整合动态信息检索与结构化知识表示来解决这些问题。

Method: 本文调查了检索机制（包括稀疏、密集和混合方法）以访问外部知识；探索了文本结构技术，如分类法构建、层次分类和信息提取，将非结构化文本转换为组织表示；并研究了这些结构表示如何通过基于提示的方法、推理框架和知识嵌入技术与LLMs集成。

Result: 本文识别了检索效率、结构质量和知识集成中的技术挑战，并强调了多模态检索、跨语言结构和交互系统的研究机会。

Conclusion: 本文全面概述了RAS方法、应用和未来方向，为研究人员和实践者提供了见解。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
with their remarkable capabilities in text generation and reasoning. However,
these models face critical challenges when deployed in real-world applications,
including hallucination generation, outdated knowledge, and limited domain
expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these
limitations by integrating dynamic information retrieval with structured
knowledge representations. This survey (1) examines retrieval mechanisms
including sparse, dense, and hybrid approaches for accessing external
knowledge; (2) explore text structuring techniques such as taxonomy
construction, hierarchical classification, and information extraction that
transform unstructured text into organized representations; and (3) investigate
how these structured representations integrate with LLMs through prompt-based
methods, reasoning frameworks, and knowledge embedding techniques. It also
identifies technical challenges in retrieval efficiency, structure quality, and
knowledge integration, while highlighting research opportunities in multimodal
retrieval, cross-lingual structures, and interactive systems. This
comprehensive overview provides researchers and practitioners with insights
into RAS methods, applications, and future directions.

</details>


### [8] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
*Iman Barati,Mostafa Amiri,Heshaam Faili*

Main category: cs.CL

TL;DR: SearchInstruct is a method for constructing high-quality instruction datasets for SFT, enhancing LLM performance in specialized domains and facilitating tasks like model editing.


<details>
  <summary>Details</summary>
Motivation: Creating suitable training datasets tailored for specific domains remains challenging due to unique domain constraints and data scarcity.

Method: SearchInstruct is an innovative method designed to construct high-quality instruction datasets for SFT. It starts with a limited set of domain-specific, human-generated questions, which are expanded using a large language model. Domain-relevant resources are then dynamically retrieved to generate accurate and contextually appropriate answers for each augmented question.

Result: Experimental evaluation demonstrates that SearchInstruct enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains.

Conclusion: SearchInstruct enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains. Additionally, the method can effectively facilitate tasks such as model editing.

Abstract: Supervised Fine-Tuning (SFT) is essential for training large language models
(LLMs), significantly enhancing critical capabilities such as instruction
following and in-context learning. Nevertheless, creating suitable training
datasets tailored for specific domains remains challenging due to unique domain
constraints and data scarcity. In this paper, we propose SearchInstruct, an
innovative method explicitly designed to construct high quality instruction
datasets for SFT. Our approach begins with a limited set of domain specific,
human generated questions, which are systematically expanded using a large
language model. Subsequently, domain relevant resources are dynamically
retrieved to generate accurate and contextually appropriate answers for each
augmented question. Experimental evaluation demonstrates that SearchInstruct
enhances both the diversity and quality of SFT datasets, leading to measurable
improvements in LLM performance within specialized domains. Additionally, we
show that beyond dataset generation, the proposed method can also effectively
facilitate tasks such as model editing, enabling efficient updates to existing
models. To facilitate reproducibility and community adoption, we provide full
implementation details, the complete set of generated instruction response
pairs, and the source code in a publicly accessible Git repository:
[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [9] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

TL;DR: 本文研究了多语言Transformer模型在虚假信息检测中的表现，并引入了一个新的多语言虚假信息语料库以促进评估。


<details>
  <summary>Details</summary>
Motivation: 大多数AI模型仍然只在英语上进行基准测试，而虚假信息却迅速跨越语言边界传播。因此，我们需要在多语言上下文中评估这些模型的有效性。

Method: 我们对五种多语言Transformer模型进行了系统比较，包括mBERT、XLM、XLM-RoBERTa、RemBERT和mT5，并在共同的假-真机器学习分类任务上评估它们的表现。

Result: 实验结果显示了性能差异。例如，RemBERT在低资源语言中表现更好，而mBERT和XLM在训练数据稀缺时表现出显著的限制。

Conclusion: 我们的研究揭示了AI系统在多语言虚假信息检测中的潜力和当前局限性。

Abstract: Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [10] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
*Mobina Pournemat,Keivan Rezaei,Gaurang Sriramanan,Arman Zarei,Jiaxiang Fu,Yang Wang,Hamid Eghbalzadeh,Soheil Feizi*

Main category: cs.CL

TL;DR: 本文首次全面研究了大型语言模型在显式离散概率分布上的推理能力，发现大模型在概率推理任务中表现更好，但也存在一些限制。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在语言理解和生成方面取得了广泛的成功，但它们在面对需要概率推理的任务时表现出不清晰且常常不一致的行为。

Method: 我们通过三个精心设计的任务来评估模型，包括模式识别、最大似然估计和样本生成。

Result: 我们发现较小的模型和较大的模型之间存在明显的性能差距，后者展示了更强的推理能力和令人惊讶的样本生成能力。然而，我们的研究也揭示了显著的局限性，包括对表示概率结果的符号变化的敏感性以及随着上下文长度增加而出现的超过60%的性能下降。

Conclusion: 我们的结果提供了对大型语言模型的概率推理能力的详细理解，并指出了未来改进的关键方向。

Abstract: Despite widespread success in language understanding and generation, large
language models (LLMs) exhibit unclear and often inconsistent behavior when
faced with tasks that require probabilistic reasoning. In this work, we present
the first comprehensive study of the reasoning capabilities of LLMs over
explicit discrete probability distributions. Given observations from a
probability distribution, we evaluate models on three carefully designed tasks,
mode identification, maximum likelihood estimation, and sample generation, by
prompting them to provide responses to queries about either the joint
distribution or its conditionals. These tasks thus probe a range of
probabilistic skills, including frequency analysis, marginalization, and
generative behavior. Through comprehensive empirical evaluations, we
demonstrate that there exists a clear performance gap between smaller and
larger models, with the latter demonstrating stronger inference and surprising
capabilities in sample generation. Furthermore, our investigations reveal
notable limitations, including sensitivity to variations in the notation
utilized to represent probabilistic outcomes and performance degradation of
over 60% as context length increases. Together, our results provide a detailed
understanding of the probabilistic reasoning abilities of LLMs and identify key
directions for future improvement.

</details>


### [11] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

TL;DR: 本文提出了一种从科学论文中生成多选题问答基准的框架，并展示了通过推理轨迹检索可以提高小型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着科学知识的快速增长，评估基准必须不断演进以反映新发现，并确保语言模型在当前、多样化的文献上进行测试。

Method: 我们提出了一种可扩展、模块化的框架，用于直接从大量科学论文语料库中生成多选题问答（MCQA）基准。我们的流程自动化了MCQA创建的每个阶段，包括PDF解析、语义分块、问题生成和模型评估。

Result: 我们从22,000篇开放获取的文章中生成了超过16,000个MCQs，并评估了一系列小型语言模型（1.1B-14B参数）在这些题目上的表现，比较了基线准确率与从论文派生的语义块和从GPT-4.1中提炼的推理轨迹的检索增强生成（RAG）。结果表明，推理轨迹检索在合成和专家标注的基准上始终能提高性能，使一些小型模型在2023年天体辐射和癌症生物学考试中超越GPT-4。

Conclusion: 我们的研究表明，通过从科学论文中生成多选题问答基准，可以有效地评估语言模型的能力。此外，使用推理轨迹检索可以显著提高小模型的性能，使其在某些任务上超过大型模型如GPT-4。

Abstract: As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [12] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
*Adarsh Srinivasan,Jacob Dineen,Muhammad Umar Afzal,Muhammad Uzair Sarfraz,Irbaz B. Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: RECAP is a framework that enhances emotional reasoning in large language models for healthcare by incorporating structured emotional reasoning without retraining, leading to improved empathetic communication.


<details>
  <summary>Details</summary>
Motivation: Large language models in healthcare often miss critical emotional cues, delivering medically sound but emotionally flat advice, which is problematic in clinical contexts where patients are distressed and vulnerable and require empathic communication.

Method: RECAP (Reflect-Extract-Calibrate-Align-Produce) is an inference-time framework that adds structured emotional reasoning without retraining by decomposing empathy into transparent appraisal-theoretic stages and exposing per-dimension Likert signals.

Result: RECAP improves emotional reasoning by 22-28% on 8B models and 10-13% on larger models over zero-shot baselines across EmoBench, SECEU, and EQ-Bench. Clinician evaluations confirm superior empathetic communication.

Conclusion: RECAP demonstrates that modular, theory-grounded prompting can systematically enhance emotional intelligence in medical AI while preserving the accountability required for deployment.

Abstract: Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

</details>


### [13] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一种名为Judge Q的新训练方法，通过引入软标记列表来改善KV缓存驱逐的效果，从而在保持解码质量的同时减少性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前的KV缓存驱逐方法通常利用预填充阶段的最后一个窗口作为查询来计算KV重要性分数，但这种方案容易过于关注局部信息，可能导致忽略或遗漏关键的全局信息。

Method: 提出了一种名为Judge Q的新训练方法，该方法结合了一个软标记列表。这种方法仅以低成本调整模型的嵌入层。通过在输入序列的末尾连接软标记列表，我们训练这些标记的注意力图与实际解码标记的注意力图对齐。

Result: 在LongBench和RULER基准测试中，结果表明性能分别提高了约1分和超过3分。

Conclusion: 该方法可以在相同的驱逐预算下比现有的驱逐方法表现出更小的性能下降，并且可以无缝集成到现有的开源模型中，从而在KV缓存驱逐场景中提升性能。

Abstract: Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [14] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文介绍了Automated Error Discovery框架，并提出了SEEED方法，用于检测和定义对话AI中的错误。通过改进Soft Nearest Neighbor Loss并引入基于标签的样本排序，SEEED在多个误差标注的对话数据集上优于现有基线模型，提高了未知误差检测的准确性。


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle to identify errors not explicitly specified in their instructions, such as those arising from updates to the response-generation model or shifts in user behavior.

Method: We introduce Automated Error Discovery, a framework for detecting and defining errors in conversational AI, and propose SEEED (Soft Clustering Extended Encoder-Based Error Detection), as an encoder-based approach to its implementation. We enhance the Soft Nearest Neighbor Loss by amplifying distance weighting for negative samples and introduce Label-Based Sample Ranking to select highly contrastive examples for better representation learning.

Result: SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 -- across multiple error-annotated dialogue datasets, improving the accuracy for detecting unknown errors by up to 8 points and demonstrating strong generalization to unknown intent detection.

Conclusion: SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 -- across multiple error-annotated dialogue datasets, improving the accuracy for detecting unknown errors by up to 8 points and demonstrating strong generalization to unknown intent detection.

Abstract: Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


### [15] [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)
*Can Wang,Yiqun Chen*

Main category: cs.CL

TL;DR: 大型语言模型在生物医学和临床应用中表现出色，但其回答基于证据的问题的能力仍有限。准确性在结构化指南建议中最高，而在叙述性指南和系统综述问题中较低。引用次数与准确性密切相关，检索增强提示可提高准确性，而分层评估对于理解模型性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生物医学和临床应用中表现出显著进展，这促使了对其回答细微、基于证据的问题的能力的严格评估。

Method: 我们从Cochrane系统综述和临床指南中收集了一个多源基准，包括美国心脏协会的结构化建议和保险公司使用的叙述性指导。使用GPT-4o-mini和GPT-5，我们观察到跨来源和临床领域的性能模式一致：准确性在结构化指南建议中最高（90%），在叙述性指南和系统综述问题中较低（60-70%）。

Result: 我们发现准确性与基础系统综述的引用次数之间存在强烈相关性，每次引用次数翻倍，正确答案的几率大约增加30%。当提供黄金来源摘要时，准确性提高了0.79；提供前3个PubMed摘要（按语义相关性排序）将准确性提高到0.23，而随机摘要则降低了准确性（0.10，在温度变化范围内）。这些效果在GPT-4o-mini中也得到了体现，强调了源清晰度和针对性检索——不仅仅是模型大小——驱动性能。

Conclusion: 我们的结果突显了LLMs在基于证据的临床问题回答中的潜力和当前限制。检索增强提示成为提高事实准确性和与源证据对齐的有用策略，而按专业和问题类型进行分层评估仍然是理解当前知识获取和上下文化模型性能的关键。

Abstract: Large Language Models (LLMs) have demonstrated substantial progress in
biomedical and clinical applications, motivating rigorous evaluation of their
ability to answer nuanced, evidence-based questions. We curate a multi-source
benchmark drawing from Cochrane systematic reviews and clinical guidelines,
including structured recommendations from the American Heart Association and
narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe
consistent performance patterns across sources and clinical domains: accuracy
is highest on structured guideline recommendations (90%) and lower on narrative
guideline and systematic review questions (60--70%). We also find a strong
correlation between accuracy and the citation count of the underlying
systematic reviews, where each doubling of citations is associated with roughly
a 30% increase in the odds of a correct answer. Models show moderate ability to
reason about evidence quality when contextual information is supplied. When we
incorporate retrieval-augmented prompting, providing the gold-source abstract
raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed
abstracts (ranked by semantic relevance) improves accuracy to 0.23, while
random abstracts reduce accuracy (0.10, within temperature variation). These
effects are mirrored in GPT-4o-mini, underscoring that source clarity and
targeted retrieval -- not just model size -- drive performance. Overall, our
results highlight both the promise and current limitations of LLMs for
evidence-based clinical question answering. Retrieval-augmented prompting
emerges as a useful strategy to improve factual accuracy and alignment with
source evidence, while stratified evaluation by specialty and question type
remains essential to understand current knowledge access and to contextualize
model performance.

</details>


### [16] [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的剪枝框架GAPrune，用于领域特定嵌入模型的压缩，该框架通过考虑领域重要性和保留一般语言基础来提高性能。实验表明，GAPrune在保持高性能的同时实现了模型压缩，并在两个领域基准上优于所有基线。


<details>
  <summary>Details</summary>
Motivation: 领域特定嵌入模型在需要专门语义理解的应用中表现出色，如代码代理和金融检索系统，通常比通用模型获得更高的性能提升。然而，最先进的嵌入模型通常基于包含数十亿参数的LLM，这在资源受限环境中部署具有挑战性。通过剪枝进行模型压缩提供了一个有希望的解决方案，但现有的剪枝方法将所有参数视为相同，无法区分通用语义表示和领域特定模式，导致次优的剪枝决策。

Method: 我们提出了GAPrune，这是一种剪枝框架，通过考虑领域重要性和保留一般语言基础来解决这一挑战。我们的方法使用Fisher信息来衡量重要性，并使用通用-领域梯度对齐来评估参数行为，然后使用我们的领域对齐重要性（DAI）评分结合这些信号。

Result: 在两个领域基准FinMTEB和ChemTEB上的实验表明，GAPrune在50%稀疏度的一次性剪枝中保持性能在密集模型的2.5%以内，同时优于所有基线。经过100步的重新训练，GAPrune在FinMTEB上实现了+4.51%的改进，在ChemTEB上实现了+1.73%的改进，证明了我们的剪枝策略不仅保留了而且增强了领域特定能力。

Conclusion: 我们的研究结果表明，有原则的剪枝策略可以实现模型压缩和增强的领域专业化，为研究社区提供了开发新方法。

Abstract: Domain-specific embedding models have shown promise for applications that
require specialized semantic understanding, such as coding agents and financial
retrieval systems, often achieving higher performance gains than general
models. However, state-of-the-art embedding models are typically based on LLMs,
which contain billions of parameters, making deployment challenging in
resource-constrained environments. Model compression through pruning offers a
promising solution, but existing pruning methods treat all parameters
uniformly, failing to distinguish between general semantic representations and
domain-specific patterns, leading to suboptimal pruning decisions. Thus, we
propose GAPrune, a pruning framework that addresses this challenge by
considering both domain importance and preserving general linguistic
foundation. Our method uses Fisher Information to measure importance and
general-domain gradient alignment to assess parameter behavior, then combines
these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI
scores indicate that the parameter is either less important for the domain task
or creates conflicts between domain and general objectives. Experiments on two
domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance
within 2.5% of dense models in one-shot pruning at 50% sparsity, while
outperforming all baselines. With retraining in 100 steps, GAPrune achieves
+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our
pruning strategy not only preserves but enhances domain-specific capabilities.
Our findings demonstrate that principled pruning strategies can achieve model
compression and enhanced domain specialization, providing the research
community with a new approach for development.

</details>


### [17] [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)
*Liqian Feng,Lintao Wang,Kun Hu,Dehui Kong,Zhiyong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于扩散的生成方法，用于无gloss的签名语言生产，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于gloss，这是一种符号表示，作为SLP的中间步骤。然而，gloss注释通常不可用且语言特定，限制了SLP的灵活性和泛化能力。因此，本文提出了一个无需gloss的扩散生成方法。

Method: 本文提出了一种无gloss的潜在扩散模型，用于从噪声潜在签名代码和口语文本中生成签名语言序列。此外，还设计了一个跨模态签名对齐器，以学习一个共享的潜在空间来连接视觉和文本内容。

Result: 在常用的PHOENIX14T和How2Sign数据集上的广泛实验表明，本文的方法有效，并达到了最先进的性能。

Conclusion: 本文提出了一种基于扩散的生成方法——Text2Sign Diffusion (Text2SignDiff)，用于无gloss的签名语言生产。实验结果表明，该方法在PHOENIX14T和How2Sign数据集上表现优异，达到了最先进的性能。

Abstract: Sign language production (SLP) aims to translate spoken language sentences
into a sequence of pose frames in a sign language, bridging the communication
gap and promoting digital inclusion for deaf and hard-of-hearing communities.
Existing methods typically rely on gloss, a symbolic representation of sign
language words or phrases that serves as an intermediate step in SLP. This
limits the flexibility and generalization of SLP, as gloss annotations are
often unavailable and language-specific. Therefore, we present a novel
diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for
gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed
to generate sign language sequences from noisy latent sign codes and spoken
text jointly, reducing the potential error accumulation through a
non-autoregressive iterative denoising process. We also design a cross-modal
signing aligner that learns a shared latent space to bridge visual and textual
content in sign and spoken languages. This alignment supports the conditioned
diffusion-based process, enabling more accurate and contextually relevant sign
language generation without gloss. Extensive experiments on the commonly used
PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,
achieving the state-of-the-art performance.

</details>


### [18] [A funny companion: Distinct neural responses to perceived AI- versus human- generated humor](https://arxiv.org/abs/2509.10847)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究使用脑电图比较了人们对AI和人类幽默的反应。结果发现，虽然AI和人类幽默被认为同样好笑，但AI幽默引发的神经反应不同，表现出较少的认知努力和更大的情感反应。


<details>
  <summary>Details</summary>
Motivation: 随着AI伴侣变得能够进行类人交流，包括讲笑话，了解人们对AI幽默的认知和情感反应变得越来越重要。

Method: 研究使用脑电图（EEG）比较人们如何处理来自AI和人类的幽默。

Result: 行为分析显示，参与者认为AI和人类幽默同样好笑。然而，神经生理数据表明，AI幽默引发了较小的N400效应，表明处理不一致时的认知努力减少。同时，出现了更大的晚期正电位（LPP），表明惊讶和情感反应更大。

Conclusion: 研究发现，大脑对AI幽默的反应出乎意料地积极且强烈，这表明幽默在促进人机社交互动中的潜在作用。

Abstract: As AI companions become capable of human-like communication, including
telling jokes, understanding how people cognitively and emotionally respond to
AI humor becomes increasingly important. This study used electroencephalography
(EEG) to compare how people process humor from AI versus human sources.
Behavioral analysis revealed that participants rated AI and human humor as
comparably funny. However, neurophysiological data showed that AI humor
elicited a smaller N400 effect, suggesting reduced cognitive effort during the
processing of incongruity. This was accompanied by a larger Late Positive
Potential (LPP), indicating a greater degree of surprise and emotional
response. This enhanced LPP likely stems from the violation of low initial
expectations regarding AI's comedic capabilities. Furthermore, a key temporal
dynamic emerged: human humor showed habituation effects, marked by an
increasing N400 and a decreasing LPP over time. In contrast, AI humor
demonstrated increasing processing efficiency and emotional reward, with a
decreasing N400 and an increasing LPP. This trajectory reveals how the brain
can dynamically update its predictive model of AI capabilities. This process of
cumulative reinforcement challenges "algorithm aversion" in humor, as it
demonstrates how cognitive adaptation to AI's language patterns can lead to an
intensified emotional reward. Additionally, participants' social attitudes
toward AI modulated these neural responses, with higher perceived AI
trustworthiness correlating with enhanced emotional engagement. These findings
indicate that the brain responds to AI humor with surprisingly positive and
intense reactions, highlighting humor's potential for fostering genuine
engagement in human-AI social interaction.

</details>


### [19] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: PREMem是一种新的方法，通过在存储前进行推理来提升对话AI的长期记忆能力，减少计算需求并提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前系统在生成响应时过度依赖推理，导致性能严重依赖模型大小。需要一种方法来减轻这种负担，提高效率。

Method: PREMem通过提取细粒度的记忆片段，并在会话之间建立显式关系，捕捉演变模式如扩展、转换和含义。

Result: 实验表明，PREMem在所有模型大小上都显著提高了性能，小型模型的表现可与大型基线模型相媲美，同时在有限的token预算下仍保持有效性。

Conclusion: PREMem能够有效提升对话AI的长期记忆能力，通过将复杂的推理过程从推理阶段转移到记忆构建阶段，从而减少计算需求并提高性能。

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [20] [Quantifier Scope Interpretation in Language Learners and LLMs](https://arxiv.org/abs/2509.10860)
*Shaohua Fang,Yue Li,Yan Cong*

Main category: cs.CL

TL;DR: 本研究通过跨语言方法探讨大型语言模型（LLMs）如何处理英语和中文中的量化词范围解释，并评估它们模仿人类表现的程度。结果显示，大多数LLMs倾向于表面范围解释，而只有少数模型在反向范围偏好上区分英语和中文，反映了类似人类的模式。


<details>
  <summary>Details</summary>
Motivation: 句子中多个量化词常常导致解释上的歧义，这些歧义可能因语言而异。本研究旨在探讨大型语言模型如何处理英语和中文中的量化词范围解释，并评估它们模仿人类表现的程度。

Method: 本研究采用跨语言方法，检查大型语言模型（LLMs）如何处理英语和中文中的量化词范围解释，使用概率评估解释的可能性。人类相似性（HS）分数用于量化LLMs在不同语言群体中模仿人类表现的程度。

Result: 研究结果揭示，大多数大型语言模型（LLMs）倾向于表面范围解释，这与人类倾向一致，而只有少数模型在反向范围偏好上区分英语和中文，反映了类似人类的模式。HS分数显示了LLMs在模仿人类行为方面的变异性，但它们总体上与人类对齐的潜力是显著的。

Conclusion: 研究结果表明，大多数大型语言模型（LLMs）倾向于表面范围解释，这与人类倾向一致，而只有少数模型在反向范围偏好上区分英语和中文，反映了类似人类的模式。HS分数显示了LLMs在模仿人类行为方面的变异性，但它们总体上与人类对齐的潜力是显著的。模型架构、规模以及特别是预训练数据的语言背景，显著影响LLMs接近人类量化范围解释的程度。

Abstract: Sentences with multiple quantifiers often lead to interpretive ambiguities,
which can vary across languages. This study adopts a cross-linguistic approach
to examine how large language models (LLMs) handle quantifier scope
interpretation in English and Chinese, using probabilities to assess
interpretive likelihood. Human similarity (HS) scores were used to quantify the
extent to which LLMs emulate human performance across language groups. Results
reveal that most LLMs prefer the surface scope interpretations, aligning with
human tendencies, while only some differentiate between English and Chinese in
the inverse scope preferences, reflecting human-similar patterns. HS scores
highlight variability in LLMs' approximation of human behavior, but their
overall potential to align with humans is notable. Differences in model
architecture, scale, and particularly models' pre-training data language
background, significantly influence how closely LLMs approximate human
quantifier scope interpretations.

</details>


### [21] [Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms](https://arxiv.org/abs/2509.10882)
*Yuping Wu,Viktor Schlegel,Warren Del-Pinto,Srinivasan Nandakumar,Iqra Zahid,Yidan Sun,Usama Farghaly Omar,Amirah Jasmine,Arun-Kumar Kaliya-Perumal,Chun Shen Tham,Gabriel Connors,Anil A Bharath,Goran Nenadic*

Main category: cs.CL

TL;DR: Term2Note is a new method for generating synthetic clinical notes under strong differential privacy constraints, achieving high fidelity and utility compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: The use of real-world training data in high-stakes domains like healthcare is constrained by privacy leakage concerns. Differentially private (DP) synthetic data offers formal privacy guarantees while maintaining data utility, but balancing privacy protection and utility remains challenging in clinical note synthesis due to domain specificity and complexity of long-form text generation.

Method: Term2Note is a methodology that synthesizes long clinical notes under strong DP constraints by structurally separating content and form, generating section-wise note content conditioned on DP medical terms, with each governed by separate DP constraints. A DP quality maximizer further enhances synthetic notes by selecting high-quality outputs.

Result: Experimental results show that Term2Note produces synthetic notes with statistical properties closely aligned with real clinical notes, demonstrating strong fidelity. Multi-label classification models trained on these synthetic notes perform comparably to those trained on real data, confirming their high utility.

Conclusion: Term2Note demonstrates potential as a viable privacy-preserving alternative to using sensitive clinical notes, offering strong fidelity and high utility compared to existing DP text generation baselines.

Abstract: Training data is fundamental to the success of modern machine learning
models, yet in high-stakes domains such as healthcare, the use of real-world
training data is severely constrained by concerns over privacy leakage. A
promising solution to this challenge is the use of differentially private (DP)
synthetic data, which offers formal privacy guarantees while maintaining data
utility. However, striking the right balance between privacy protection and
utility remains challenging in clinical note synthesis, given its domain
specificity and the complexity of long-form text generation. In this paper, we
present Term2Note, a methodology to synthesise long clinical notes under strong
DP constraints. By structurally separating content and form, Term2Note
generates section-wise note content conditioned on DP medical terms, with each
governed by separate DP constraints. A DP quality maximiser further enhances
synthetic notes by selecting high-quality outputs. Experimental results show
that Term2Note produces synthetic notes with statistical properties closely
aligned with real clinical notes, demonstrating strong fidelity. In addition,
multi-label classification models trained on these synthetic notes perform
comparably to those trained on real data, confirming their high utility.
Compared to existing DP text generation baselines, Term2Note achieves
substantial improvements in both fidelity and utility while operating under
fewer assumptions, suggesting its potential as a viable privacy-preserving
alternative to using sensitive clinical notes.

</details>


### [22] [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)
*Xinyu Zhang,Pei Zhang,Shuang Luo,Jialong Tang,Yu Wan,Baosong Yang,Fei Huang*

Main category: cs.CL

TL;DR: CultureSynth is a framework that enhances the evaluation of cultural competence in large language models by providing a comprehensive taxonomy and a RAG-based methodology, revealing performance differences among models.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs' cultural competence suffer from fragmented taxonomies, domain specificity, and heavy reliance on manual data annotation.

Method: Introduce CultureSynth, a novel framework comprising a comprehensive hierarchical multilingual cultural taxonomy and a Retrieval-Augmented Generation (RAG)-based methodology.

Result: Evaluation of 14 prevalent LLMs reveals clear performance stratification led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. A 3B-parameter threshold is necessary for achieving basic cultural competence, models display varying architectural biases in knowledge processing, and significant geographic disparities exist across models.

Conclusion: CultureSynth offers a scalable framework for developing culturally aware AI systems while reducing reliance on manual annotation.

Abstract: Cultural competence, defined as the ability to understand and adapt to
multicultural contexts, is increasingly vital for large language models (LLMs)
in global environments. While several cultural benchmarks exist to assess LLMs'
cultural competence, current evaluations suffer from fragmented taxonomies,
domain specificity, and heavy reliance on manual data annotation. To address
these limitations, we introduce CultureSynth, a novel framework comprising (1)
a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary
and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based
methodology leveraging factual knowledge to synthesize culturally relevant
question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360
entries and 4,149 manually verified entries across 7 languages. Evaluation of
14 prevalent LLMs of different sizes reveals clear performance stratification
led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that
a 3B-parameter threshold is necessary for achieving basic cultural competence,
models display varying architectural biases in knowledge processing, and
significant geographic disparities exist across models. We believe that
CultureSynth offers a scalable framework for developing culturally aware AI
systems while reducing reliance on manual annotation\footnote{Benchmark is
available at https://github.com/Eyr3/CultureSynth.}.

</details>


### [23] [Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction](https://arxiv.org/abs/2509.10922)
*Tsuyoshi Iwata,Guillaume Comte,Melissa Flores,Ryoma Kondo,Ryohei Hisano*

Main category: cs.CL

TL;DR: 本文提出了一种半自动方法，利用轻量级本体设计、形式化模式建模和大型语言模型，将规范原则转换为可重用的模板，以构建结构化知识图谱，从而实现对国际可持续性指南的非合规性识别和解释。


<details>
  <summary>Details</summary>
Motivation: 由于环境、社会和治理数据在监管和投资背景中的重要性日益增加，需要准确、可解释且国际一致的非财务风险表示，特别是来自非结构化新闻来源的数据。然而，将这些与基于原则的规范框架（如联合国全球契约或可持续发展目标）对齐存在重大挑战。

Method: 本文使用轻量级本体设计、形式化模式建模和大型语言模型，将规范原则转换为可重用的模板，并将其用于从新闻内容中提取相关信息，填充结构化知识图谱。

Result: 本文的结果是一个可扩展且透明的框架，用于识别和解释对国际可持续性指南的非合规性。

Conclusion: 本文提出了一种半自动的方法，用于构建环境、社会和治理事件的结构化知识表示，从而实现对国际可持续性指南的非合规性识别和解释。

Abstract: The growing importance of environmental, social, and governance data in
regulatory and investment contexts has increased the need for accurate,
interpretable, and internationally aligned representations of non-financial
risks, particularly those reported in unstructured news sources. However,
aligning such controversy-related data with principle-based normative
frameworks, such as the United Nations Global Compact or Sustainable
Development Goals, presents significant challenges. These frameworks are
typically expressed in abstract language, lack standardized taxonomies, and
differ from the proprietary classification systems used by commercial data
providers. In this paper, we present a semi-automatic method for constructing
structured knowledge representations of environmental, social, and governance
events reported in the news. Our approach uses lightweight ontology design,
formal pattern modeling, and large language models to convert normative
principles into reusable templates expressed in the Resource Description
Framework. These templates are used to extract relevant information from news
content and populate a structured knowledge graph that links reported incidents
to specific framework principles. The result is a scalable and transparent
framework for identifying and interpreting non-compliance with international
sustainability guidelines.

</details>


### [24] [Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents](https://arxiv.org/abs/2509.10935)
*Ankan Mullick,Sombit Bose,Rounak Saha,Ayan Kumar Bhowmick,Aditya Vempaty,Prasenjit Dey,Ravi Kokku,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 本文介绍了Spotlight，一种新的信息提取范式，通过突出文档中最引人注目的部分来生成简洁、吸引人的叙述。我们提出了一种两阶段的方法：在基准数据上微调大型语言模型，然后通过直接偏好优化（DPO）进行对齐。评估结果表明，该模型不仅能够精确识别关键元素，还能提高可读性并增强原始文档的参与价值。


<details>
  <summary>Details</summary>
Motivation: 我们引入Spotlight，这是一种新的信息提取范式，通过突出文档中最引人注目的部分来生成简洁、吸引人的叙述。

Method: 我们提出了一种两阶段的方法：在基准数据上微调大型语言模型，然后通过直接偏好优化（DPO）进行对齐。

Result: 我们的全面评估表明，生成的模型不仅能够精确识别关键元素，还能提高可读性并增强原始文档的参与价值。

Conclusion: 我们的模型不仅能够精确识别关键元素，还能提高可读性并增强原始文档的参与价值。

Abstract: In this paper, we introduce Spotlight, a novel paradigm for information
extraction that produces concise, engaging narratives by highlighting the most
compelling aspects of a document. Unlike traditional summaries, which
prioritize comprehensive coverage, spotlights selectively emphasize intriguing
content to foster deeper reader engagement with the source material. We
formally differentiate spotlights from related constructs and support our
analysis with a detailed benchmarking study using new datasets curated for this
work. To generate high-quality spotlights, we propose a two-stage approach:
fine-tuning a large language model on our benchmark data, followed by alignment
via Direct Preference Optimization (DPO). Our comprehensive evaluation
demonstrates that the resulting model not only identifies key elements with
precision but also enhances readability and boosts the engagement value of the
original document.

</details>


### [25] [An Interpretable Benchmark for Clickbait Detection and Tactic Attribution](https://arxiv.org/abs/2509.10937)
*Lihi Nofar,Tomer Portal,Aviv Elbaz,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的点击诱饵检测模型，能够识别点击诱饵标题并将其归因于特定的语言操纵策略。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题的泛滥对信息的可信度和用户对数字媒体的信任构成了重大挑战。尽管机器学习的最新进展提高了对操纵性内容的检测能力，但缺乏可解释性限制了其实际应用。

Method: 本文提出了一种两阶段框架，用于自动点击诱饵分析，包括检测和策略归因。在第一阶段，比较了微调的BERT分类器与大型语言模型（LLMs），如GPT-4.0和Gemini 2.4 Flash，在零样本提示和少量样本提示下的表现。在第二阶段，使用基于BERT的分类器预测每个标题中的具体点击诱饵策略。

Result: 本文引入了一个合成数据集，通过系统地增强真实新闻标题来生成，该数据集可以进行受控实验和详细分析模型行为。

Conclusion: 本文推进了透明和可信的AI系统的发展，以对抗操纵性媒体内容。

Abstract: The proliferation of clickbait headlines poses significant challenges to the
credibility of information and user trust in digital media. While recent
advances in machine learning have improved the detection of manipulative
content, the lack of explainability limits their practical adoption. This paper
presents a model for explainable clickbait detection that not only identifies
clickbait titles but also attributes them to specific linguistic manipulation
strategies. We introduce a synthetic dataset generated by systematically
augmenting real news headlines using a predefined catalogue of clickbait
strategies. This dataset enables controlled experimentation and detailed
analysis of model behaviour. We present a two-stage framework for automatic
clickbait analysis comprising detection and tactic attribution. In the first
stage, we compare a fine-tuned BERT classifier with large language models
(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot
prompting and few-shot prompting enriched with illustrative clickbait headlines
and their associated persuasive tactics. In the second stage, a dedicated
BERT-based classifier predicts the specific clickbait strategies present in
each headline. This work advances the development of transparent and
trustworthy AI systems for combating manipulative media content. We share the
dataset with the research community at
https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection

</details>


### [26] [EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models](https://arxiv.org/abs/2509.11101)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 本文介绍了EmoBench-Reddit，这是一个用于多模态情感理解的新基准，包含350个精心挑选的样本，每个样本包含图像、用户提供的文本和由用户flairs确认的情感类别，并设计了一个层次化的任务框架，以评估模型在视觉元素识别、场景推理、意图理解和深度共情方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的评估基准主要关注客观的视觉问答或描述任务，未能充分评估模型理解复杂和主观人类情感的能力。因此，本文旨在引入EmoBench-Reddit，以填补这一差距。

Method: 本文提出了EmoBench-Reddit，这是一个层次化的多模态情感理解基准。数据集包含了从Reddit平台精心挑选的350个样本，每个样本包含图像、用户提供的文本和由用户flairs确认的情感类别。设计了一个层次化的任务框架，从基本感知到高级认知，每个数据点包含六个选择题和一个开放性问题，以评估模型在视觉元素识别、场景推理、意图理解和深度共情方面的能力。

Result: EmoBench-Reddit是一个新的多模态情感理解基准，包含350个精心挑选的样本，每个样本包含图像、用户提供的文本和由用户flairs确认的情感类别。设计了一个层次化的任务框架，从基本感知到高级认知，每个数据点包含六个选择题和一个开放性问题，以评估模型在视觉元素识别、场景推理、意图理解和深度共情方面的能力。

Conclusion: 本文介绍了EmoBench-Reddit，这是一个用于多模态情感理解的新基准。该数据集包含从Reddit平台精心挑选的350个样本，每个样本包含图像、用户提供的文本和由用户flairs确认的情感类别。设计了一个层次化的任务框架，从基本感知到高级认知，每个数据点包含六个选择题和一个开放性问题，以评估模型在视觉元素识别、场景推理、意图理解和深度共情方面的能力。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), they
have demonstrated exceptional capabilities across a variety of vision-language
tasks. However, current evaluation benchmarks predominantly focus on objective
visual question answering or captioning, inadequately assessing the models'
ability to understand complex and subjective human emotions. To bridge this
gap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for
multimodal emotion understanding. The dataset comprises 350 meticulously
curated samples from the social media platform Reddit, each containing an
image, associated user-provided text, and an emotion category (sad, humor,
sarcasm, happy) confirmed by user flairs. We designed a hierarchical task
framework that progresses from basic perception to advanced cognition, with
each data point featuring six multiple-choice questions and one open-ended
question of increasing difficulty. Perception tasks evaluate the model's
ability to identify basic visual elements (e.g., colors, objects), while
cognition tasks require scene reasoning, intent understanding, and deep empathy
integrating textual context. We ensured annotation quality through a
combination of AI assistance (Claude 4) and manual verification.

</details>


### [27] [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)
*Valentin Hofmann,David Heineman,Ian Magnusson,Kyle Lo,Jesse Dodge,Maarten Sap,Pang Wei Koh,Chun Wang,Hannaneh Hajishirzi,Noah A. Smith*

Main category: cs.CL

TL;DR: 本文介绍了 Fluid Benchmarking，这是一种新的语言模型基准测试方法，它通过项目反应理论和动态项目选择来提高评估的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 语言模型基准测试面临多个挑战，包括全面评估成本高、基准测试往往无法衡量预期能力，以及由于标注错误和基准饱和导致的评估质量下降。

Method: Fluid Benchmarking 是一种新的评估方法，它基于项目反应理论，并使用推断的数量动态选择评估项目，类似于教育中的计算机自适应测试。

Result: Fluid Benchmarking 在四个维度——效率、有效性、方差和饱和度——上均表现出色（例如，在 MMLU 上使用五十倍少的项目具有更高的有效性和更小的方差）。

Conclusion: 我们的结果表明，通过超越静态评估，可以显著改进语言模型基准测试。

Abstract: Language model (LM) benchmarking faces several challenges: comprehensive
evaluations are costly, benchmarks often fail to measure the intended
capabilities, and evaluation quality can degrade due to labeling errors and
benchmark saturation. Although various strategies have been proposed to
mitigate these issues, they tend to address individual aspects in isolation,
neglecting broader questions about overall evaluation quality. Here, we
introduce Fluid Benchmarking, a new evaluation approach that advances LM
benchmarking across multiple dimensions. Inspired by psychometrics, Fluid
Benchmarking is based on the insight that the relative value of benchmark items
depends on an LM's capability level, suggesting that evaluation should adapt to
each LM. Methodologically, Fluid Benchmarking estimates an item response model
based on existing LM evaluation results and uses the inferred quantities to
select evaluation items dynamically, similar to computerized adaptive testing
in education. In our experiments, we compare Fluid Benchmarking against the
common practice of random item sampling as well as more sophisticated
baselines, including alternative methods grounded in item response theory. We
examine four dimensions -- efficiency, validity, variance, and saturation --
and find that Fluid Benchmarking achieves superior performance in all of them
(e.g., higher validity and less variance on MMLU with fifty times fewer items).
Our analysis shows that the two components of Fluid Benchmarking have distinct
effects: item response theory, used to map performance into a latent ability
space, increases validity, while dynamic item selection reduces variance.
Overall, our results suggest that LM benchmarking can be substantially improved
by moving beyond static evaluation.

</details>


### [28] [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)
*Priyanshu Priya,Saurav Dudhate,Desai Vishesh Yasheshbhai,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文提出了一个基于个性的论证协商对话生成任务（PAN-DG），并创建了一个名为PACT的数据集，用于模拟不同个性的协商场景。实验显示，微调的语言模型能有效生成符合个性特征的回应，提升了协商对话系统的个性化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了提升协商对话系统的冲突解决能力和适应性，需要引入个性驱动的论证机制。

Method: 提出了一种名为PAN-DG的任务，并引入了PACT数据集，该数据集通过大型语言模型生成，包含三种不同的个性档案以模拟各种协商场景。

Result: 实验结果表明，微调的语言模型能够有效地生成符合个性特征的理性回应，证明了PACT数据集的有效性。

Conclusion: PACT数据集在增强协商对话系统中的个性化和推理能力方面表现出有效性，为该领域的未来研究奠定了基础。

Abstract: Integrating argumentation mechanisms into negotiation dialogue systems
improves conflict resolution through exchanges of arguments and critiques.
Moreover, incorporating personality attributes enhances adaptability by
aligning interactions with individuals' preferences and styles. To advance
these capabilities in negotiation dialogue systems, we propose a novel
Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)
task. To support this task, we introduce PACT, a dataset of Personality-driven
Argumentation-based negotiation Conversations for Tourism sector. This dataset,
generated using Large Language Models (LLMs), features three distinct
personality profiles, viz. Argumentation Profile, Preference Profile, and
Buying Style Profile to simulate a variety of negotiation scenarios involving
diverse personalities. Thorough automatic and manual evaluations indicate that
the dataset comprises high-quality dialogues. Further, we conduct comparative
experiments between pre-trained and fine-tuned LLMs for the PAN-DG task.
Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively
generate personality-driven rational responses during negotiations. This
underscores the effectiveness of PACT in enhancing personalization and
reasoning capabilities in negotiation dialogue systems, thereby establishing a
foundation for future research in this domain.

</details>


### [29] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
*Hongxu Zhou,Hylke Westerdijk,Khondoker Ittehadul Islam*

Main category: cs.CL

TL;DR: 本研究探讨了上下文和情感色调元数据对大型语言模型在谬误分类任务中推理和性能的影响，发现添加这些元数据通常会降低性能，而基本提示效果更好。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨上下文和情感色调元数据如何影响大型语言模型（LLM）在谬误分类任务中的推理和性能，特别是在政治辩论环境中。

Method: 本研究使用来自美国总统辩论的数据，通过各种提示策略应用于Qwen-3（8B）模型，对六种谬误类型进行分类。引入了两种理论上有根据的思维链框架：修辞辩证法和论证周期表，并在三种输入设置下评估它们的有效性：仅文本、带上下文的文本以及带上下文和基于音频的情感色调元数据的文本。

Result: 结果表明，虽然理论提示可以提高可解释性，在某些情况下还可以提高准确性，但添加上下文和尤其是情感色调元数据通常会导致性能下降。情感色调元数据使模型倾向于将陈述标记为'诉诸情感'，从而恶化逻辑推理。总体而言，基本提示往往优于增强提示，这表明添加输入可能会分散注意力，从而对LLM中的谬误分类产生负面影响。

Conclusion: 研究发现，添加上下文和情感色调元数据通常会导致性能下降，情感色调元数据会使模型倾向于将陈述标记为'诉诸情感'，从而恶化逻辑推理。总体而言，基本提示往往优于增强提示，这表明添加输入可能会分散注意力，从而对LLM中的谬误分类产生负面影响。

Abstract: This study investigates how context and emotional tone metadata influence
large language model (LLM) reasoning and performance in fallacy classification
tasks, particularly within political debate settings. Using data from U.S.
presidential debates, we classify six fallacy types through various prompting
strategies applied to the Qwen-3 (8B) model. We introduce two theoretically
grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table
of Arguments, and evaluate their effectiveness against a baseline prompt under
three input settings: text-only, text with context, and text with both context
and audio-based emotional tone metadata. Results suggest that while theoretical
prompting can improve interpretability and, in some cases, accuracy, the
addition of context and especially emotional tone metadata often leads to
lowered performance. Emotional tone metadata biases the model toward labeling
statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall,
basic prompts often outperformed enhanced ones, suggesting that attention
dilution from added inputs may worsen rather than improve fallacy
classification in LLMs.

</details>


### [30] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
*Shiyao Cui,Xijia Feng,Yingkang Wang,Junxiao Yang,Zhexin Zhang,Biplab Sikdar,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.CL

TL;DR: 本论文研究了表情符号如何在大型语言模型中引发有害内容生成，并发现表情符号可以作为异构语义通道绕过安全机制。


<details>
  <summary>Details</summary>
Motivation: 由于观察到表情符号可能在大型语言模型中引发有害内容生成，因此本文旨在研究表情符号是否能明显增强有害内容生成以及如何解释这一现象。

Method: 通过自动化构建带有表情符号的提示来探索表情符号触发的大型语言模型有害内容生成，并进行了跨5种主流语言和7个著名大型语言模型的实验以及越狱任务。此外，还进行了模型层面的解释，包括语义认知、序列生成和分词，并进一步探究了预训练语料库中的潜在关联。

Result: 实验结果表明，带有表情符号的提示可以轻松引发有害内容生成。模型层面的解释表明，表情符号可以作为异构语义通道，绕过安全机制。进一步的预训练语料库分析揭示了表情符号相关数据污染与有害生成行为之间的潜在关联。

Conclusion: 本论文发现表情符号可以作为异构语义通道，绕过安全机制，从而引发大型语言模型的有害内容生成。此外，研究还揭示了表情符号相关数据污染与有害生成行为之间的潜在关联。

Abstract: Emojis are globally used non-verbal cues in digital communication, and
extensive research has examined how large language models (LLMs) understand and
utilize emojis across contexts. While usually associated with friendliness or
playfulness, it is observed that emojis may trigger toxic content generation in
LLMs. Motivated by such a observation, we aim to investigate: (1) whether
emojis can clearly enhance the toxicity generation in LLMs and (2) how to
interpret this phenomenon. We begin with a comprehensive exploration of
emoji-triggered LLM toxicity generation by automating the construction of
prompts with emojis to subtly express toxic intent. Experiments across 5
mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate
that prompts with emojis could easily induce toxicity generation. To understand
this phenomenon, we conduct model-level interpretations spanning semantic
cognition, sequence generation and tokenization, suggesting that emojis can act
as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue
deeper insights, we further probe the pre-training corpus and uncover potential
correlation between the emoji-related data polution with the toxicity
generation behaviors. Supplementary materials provide our implementation code
and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [31] [Text2Mem: A Unified Memory Operation Language for Memory Operating System](https://arxiv.org/abs/2509.11145)
*Felix Wang,Boyu Chen,Kerun Xu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: Text2Mem 是一种统一的记忆操作语言，旨在解决现有框架在高级操作和规范方面的不足，提供安全、确定和可移植的记忆控制。


<details>
  <summary>Details</summary>
Motivation: 现有的记忆框架缺乏高级操作和正式的规范，导致行为不可预测。

Method: Text2Mem 定义了一组紧凑且表达能力强的操作，将自然语言转换为可执行的JSON模式实例，并通过解析器、验证器和适配器确保正确性和跨后端的可移植性。

Result: Text2Mem 设计确保了安全性、确定性和跨异构后端的可移植性，并提出了Text2Mem Bench以支持系统评估。

Conclusion: Text2Mem 提供了一个统一的记忆操作语言，为代理中的记忆控制建立了第一个标准化基础。

Abstract: Large language model agents increasingly depend on memory to sustain long
horizon interaction, but existing frameworks remain limited. Most expose only a
few basic primitives such as encode, retrieve, and delete, while higher order
operations like merge, promote, demote, split, lock, and expire are missing or
inconsistently supported. Moreover, there is no formal and executable
specification for memory commands, leaving scope and lifecycle rules implicit
and causing unpredictable behavior across systems. We introduce Text2Mem, a
unified memory operation language that provides a standardized pathway from
natural language to reliable execution. Text2Mem defines a compact yet
expressive operation set aligned with encoding, storage, and retrieval. Each
instruction is represented as a JSON based schema instance with required fields
and semantic invariants, which a parser transforms into typed operation objects
with normalized parameters. A validator ensures correctness before execution,
while adapters map typed objects either to a SQL prototype backend or to real
memory frameworks. Model based services such as embeddings or summarization are
integrated when required. All results are returned through a unified execution
contract. This design ensures safety, determinism, and portability across
heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark
that separates schema generation from backend execution to enable systematic
evaluation. Together, these components establish the first standardized
foundation for memory control in agents.

</details>


### [32] [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)
*Erion Çano,Ivan Habernal*

Main category: cs.CL

TL;DR: 研究分析了在不同隐私约束下微调的LLM对文本质量和效用的影响，发现隐私保护越强，生成文本的质量和效用越低。


<details>
  <summary>Details</summary>
Motivation: 研究DP微调LLM对语言质量和文本效用的影响，以评估其在隐私保护下的实用性。

Method: 对五种LLM在三个语料库下进行不同隐私级别的微调，并评估文本输出的长度、语法正确性和词汇多样性，同时测试合成输出在下游分类任务中的效用。

Result: 在更强的隐私约束下，LLM生成的文本长度减少至少77%，语法正确性降低至少9%，双词多样性减少至少10%。此外，它们在下游分类任务中的准确性下降。

Conclusion: 研究发现，在更强的隐私约束下微调的LLM生成的文本更短、语法正确性更低、多样性更差，并且在下游分类任务中的准确性下降，这可能会影响生成合成数据的有用性。

Abstract: Ensuring user privacy by synthesizing data from large language models (LLMs)
tuned under differential privacy (DP) has become popular recently. However, the
impact of DP fine-tuned LLMs on the quality of the language and the utility of
the texts they produce has not been investigated. In this work, we tune five
LLMs with three corpora under four levels of privacy and assess the length, the
grammatical correctness, and the lexical diversity of the text outputs they
produce. We also probe the utility of the synthetic outputs in downstream
classification tasks such as book genre recognition based on book descriptions
and cause of death recognition based on verbal autopsies. The results indicate
that LLMs tuned under stronger privacy constrains produce texts that are
shorter by at least 77 %, that are less grammatically correct by at least 9 %,
and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the
accuracy they reach in downstream classification tasks decreases, which might
be detrimental to the usefulness of the generated synthetic data.

</details>


### [33] [Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs](https://arxiv.org/abs/2509.11177)
*Hang Guo,Yawei Li,Luca Benini*

Main category: cs.CL

TL;DR: 本文提出了一种名为Optimal Brain Restoration (OBR)的通用且无需训练的框架，通过误差补偿来协调剪枝和量化。OBR通过构建二阶Hessian目标函数来最小化下游任务的性能退化，并通过代理近似将其转化为可处理的问题，最终通过组误差补偿达到闭式解。实验表明，OBR能够在现有LLM上实现W4A4KV4量化和50%的稀疏性，并相比FP16密集基线实现了最高4.72倍的速度提升和6.4倍的内存减少。


<details>
  <summary>Details</summary>
Motivation: Recent advances in Large Language Model (LLM) compression, such as quantization and pruning, have achieved notable success. However, as these techniques gradually approach their respective limits, relying on a single method for further compression has become increasingly challenging.

Method: We propose Optimal Brain Restoration (OBR), a general and training-free framework that aligns pruning and quantization by error compensation between both. OBR minimizes performance degradation on downstream tasks by building on a second-order Hessian objective, which is then reformulated into a tractable problem through surrogate approximation and ultimately reaches a closed-form solution via group error compensation.

Result: Experiments show that OBR enables aggressive W4A4KV4 quantization with 50% sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory reduction compared to the FP16-dense baseline.

Conclusion: OBR enables aggressive W4A4KV4 quantization with 50% sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory reduction compared to the FP16-dense baseline.

Abstract: Recent advances in Large Language Model (LLM) compression, such as
quantization and pruning, have achieved notable success. However, as these
techniques gradually approach their respective limits, relying on a single
method for further compression has become increasingly challenging. In this
work, we explore an alternative solution by combining quantization and
sparsity. This joint approach, though promising, introduces new difficulties
due to the inherently conflicting requirements on weight distributions:
quantization favors compact ranges, while pruning benefits from high variance.
To attack this problem, we propose Optimal Brain Restoration (OBR), a general
and training-free framework that aligns pruning and quantization by error
compensation between both. OBR minimizes performance degradation on downstream
tasks by building on a second-order Hessian objective, which is then
reformulated into a tractable problem through surrogate approximation and
ultimately reaches a closed-form solution via group error compensation.
Experiments show that OBR enables aggressive W4A4KV4 quantization with 50%
sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory
reduction compared to the FP16-dense baseline.

</details>


### [34] [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)
*Jian Chen,Shengyi Lv,Leilei Su*

Main category: cs.CL

TL;DR: RAT is a novel framework that combines random sampling with adversarial training to improve the performance and efficiency of pre-trained language models in biomedical information extraction tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the computational overhead introduced by conventional adversarial training in enhancing pre-trained language models for BioIE tasks.

Method: RAT integrates random sampling mechanisms with adversarial training principles to enhance model generalization and robustness while reducing computational costs.

Result: RAT achieves enhanced model generalization and robustness while significantly reducing computational costs, demonstrating superior performance compared to baseline models in BioIE tasks.

Conclusion: RAT demonstrates superior performance compared to baseline models in BioIE tasks, highlighting its potential as a transformative framework for biomedical natural language processing, offering a balanced solution to model performance and computational efficiency.

Abstract: We introduce random adversarial training (RAT), a novel framework
successfully applied to biomedical information extraction (BioIE) tasks.
Building on PubMedBERT as the foundational architecture, our study first
validates the effectiveness of conventional adversarial training in enhancing
pre-trained language models' performance on BioIE tasks. While adversarial
training yields significant improvements across various performance metrics, it
also introduces considerable computational overhead. To address this
limitation, we propose RAT as an efficiency solution for biomedical information
extraction. This framework strategically integrates random sampling mechanisms
with adversarial training principles, achieving dual objectives: enhanced model
generalization and robustness while significantly reducing computational costs.
Through comprehensive evaluations, RAT demonstrates superior performance
compared to baseline models in BioIE tasks. The results highlight RAT's
potential as a transformative framework for biomedical natural language
processing, offering a balanced solution to the model performance and
computational efficiency.

</details>


### [35] [The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences](https://arxiv.org/abs/2509.11295)
*Valentin Romanov,Steven A Niederer*

Main category: cs.CL

TL;DR: 本文从2025年的《提示报告》中提炼出6种核心提示工程技术，并结合生命科学应用案例，提供详细的构建建议，以提高研究质量。


<details>
  <summary>Details</summary>
Motivation: 研究人员可以通过部署特定于案例的提示工程技巧来提高效率，但需要大量的认知投入来生成可靠的高质量响应。

Method: 本文从2025年发布的《提示报告》中提炼出6种核心技术：零样本、少量样本方法、思维生成、集成、自我批评和分解，并结合生命科学中的使用案例进行分析。

Result: 本文提供了关于提示应如何以及不应如何构建的详细建议，解决了多轮对话退化、幻觉以及推理模型与非推理模型之间的区别等常见问题。

Conclusion: 本文旨在提供核心提示工程原则的可操作指导，并促进从随机提示到有效、低摩擦系统实践的转变，从而提高研究质量。

Abstract: Developing effective prompts demands significant cognitive investment to
generate reliable, high-quality responses from Large Language Models (LLMs). By
deploying case-specific prompt engineering techniques that streamline
frequently performed life sciences workflows, researchers could achieve
substantial efficiency gains that far exceed the initial time investment
required to master these techniques. The Prompt Report published in 2025
outlined 58 different text-based prompt engineering techniques, highlighting
the numerous ways prompts could be constructed. To provide actionable
guidelines and reduce the friction of navigating these various approaches, we
distil this report to focus on 6 core techniques: zero-shot, few-shot
approaches, thought generation, ensembling, self-criticism, and decomposition.
We breakdown the significance of each approach and ground it in use cases
relevant to life sciences, from literature summarization and data extraction to
editorial tasks. We provide detailed recommendations for how prompts should and
shouldn't be structured, addressing common pitfalls including multi-turn
conversation degradation, hallucinations, and distinctions between reasoning
and non-reasoning models. We examine context window limitations, agentic tools
like Claude Code, while analyzing the effectiveness of Deep Research tools
across OpenAI, Google, Anthropic and Perplexity platforms, discussing current
limitations. We demonstrate how prompt engineering can augment rather than
replace existing established individual practices around data processing and
document editing. Our aim is to provide actionable guidance on core prompt
engineering principles, and to facilitate the transition from opportunistic
prompting to an effective, low-friction systematic practice that contributes to
higher quality research.

</details>


### [36] [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)
*Dasol Choi,Jungwhan Kim,Guijin Son*

Main category: cs.CL

TL;DR: 本文介绍了Ko-PIQA，这是一个包含文化背景的韩语物理常识推理数据集。通过多阶段筛选和GPT-4o优化，得到了441个高质量的问题-答案对。Ko-PIQA具有文化基础，其中19.7%的问题包含特定文化元素，如泡菜、韩服等。评估结果显示模型在处理文化特定场景时表现不佳，强调了文化多样性数据集的重要性。


<details>
  <summary>Details</summary>
Motivation: Physical commonsense reasoning datasets like PIQA are predominantly English-centric and lack cultural diversity.

Method: Starting from 3.01 million web-crawled questions, we employed a multi-stage filtering approach using three language models to identify 11,553 PIQA-style questions. Through GPT-4o refinement and human validation, we obtained 441 high-quality question-answer pairs.

Result: We evaluate seven language models on Ko-PIQA, with the best model achieving 83.22% accuracy while the weakest reaches only 59.86%, demonstrating significant room for improvement. Models particularly struggle with culturally specific scenarios, highlighting the importance of culturally diverse datasets.

Conclusion: Ko-PIQA serves as both a benchmark for Korean language models and a foundation for more inclusive commonsense reasoning research.

Abstract: Physical commonsense reasoning datasets like PIQA are predominantly
English-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean
physical commonsense reasoning dataset that incorporates cultural context.
Starting from 3.01 million web-crawled questions, we employed a multi-stage
filtering approach using three language models to identify 11,553 PIQA-style
questions. Through GPT-4o refinement and human validation, we obtained 441
high-quality question-answer pairs. A key feature of Ko-PIQA is its cultural
grounding: 19.7\% of questions contain culturally specific elements like
traditional Korean foods (kimchi), clothing (hanbok), and specialized
appliances (kimchi refrigerators) that require culturally-aware reasoning
beyond direct translation. We evaluate seven language models on Ko-PIQA, with
the best model achieving 83.22\% accuracy while the weakest reaches only
59.86\%, demonstrating significant room for improvement. Models particularly
struggle with culturally specific scenarios, highlighting the importance of
culturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean
language models and a foundation for more inclusive commonsense reasoning
research. The dataset and code will be publicly available.

</details>


### [37] [!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning](https://arxiv.org/abs/2509.11365)
*Mohamed Tarek,Seif Ahmed,Mohamed Basem*

Main category: cs.CL

TL;DR: 本文介绍了用于AraHealthQA-2025共享任务的系统，方法在两个子任务中均获得第二名，展示了其在阿拉伯语临床问答中的有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在提高阿拉伯语临床背景下问答系统的准确性和实用性。

Method: 对于子任务1，我们利用Gemini 2.5 Flash模型进行少样本提示、数据集预处理和三种提示配置的集成以提高分类准确性；对于子任务2，我们采用统一提示，结合角色扮演、少样本示例和后处理生成简洁的回答。

Result: 在两个子任务中均获得第二名，表明我们的方法在标准、有偏见和填空问题上具有较高的分类准确性，并能生成简洁的回答。

Conclusion: 我们的方法在阿拉伯临床背景下的两个子任务中均获得了第二名，证明了其有效性。

Abstract: We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of
the AraHealthQA-2025 shared task, where our methodology secured 2nd place in
both Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended
question answering) in Arabic clinical contexts. For Sub-Task 1, we leverage
the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and
an ensemble of three prompt configurations to improve classification accuracy
on standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ
a unified prompt with the same model, incorporating role-playing as an Arabic
medical expert, few-shot examples, and post-processing to generate concise
responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased
variants.

</details>


### [38] [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)
*Bowen Jing,Yang Cui,Tianpeng Huang*

Main category: cs.CL

TL;DR: 本文比较了基于变压器和非变压器的深度监督学习方法在关系抽取任务中的性能，并发现基于变压器的模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型时代，关系抽取在信息抽取中起着重要作用，因此需要比较不同方法的性能。

Method: 本文系统比较了没有变压器的深度监督学习方法和带有变压器的方法的性能。使用了一系列非变压器架构和一系列变压器架构，并在多个数据集上进行了实验。

Result: 基于变压器的模型在微F1分数上表现出色，达到了80-90%，而非变压器模型则达到了64-67%。

Conclusion: 实验结果表明，基于变压器的模型在关系抽取任务中优于非变压器模型，并且大语言模型在关系抽取中的作用和现状得到了讨论。

Abstract: In the era of large language model, relation extraction (RE) plays an
important role in information extraction through the transformation of
unstructured raw text into structured data (Wadhwa et al., 2023). In this
paper, we systematically compare the performance of deep supervised learning
approaches without transformers and those with transformers. We used a series
of non-transformer architectures such as PA-LSTM(Zhang et al., 2017),
C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),
and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu
and He, 2019). Our comparison included traditional metrics like micro F1, as
well as evaluations in different scenarios, varying sentence lengths, and
different percentages of the dataset for training. Our experiments were
conducted on TACRED, TACREV, and RE-TACRED. The results show that
transformer-based models outperform non-transformer models, achieving micro F1
scores of 80-90% compared to 64-67% for non-transformer models. Additionally,
we briefly review the research journey in supervised relation classification
and discuss the role and current status of large language models (LLMs) in
relation extraction.

</details>


### [39] [Continually Adding New Languages to Multilingual Language Models](https://arxiv.org/abs/2509.11414)
*Abraham Toluwase Owodunni,Sachin Kumar*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multilingual language models are trained on a fixed set of languages, and to
support new languages, the models need to be retrained from scratch. This is an
expensive endeavor and is often infeasible, as model developers tend not to
release their pre-training data. Naive approaches, such as continued
pretraining, suffer from catastrophic forgetting; however, mitigation
strategies like experience replay cannot be applied due to the lack of original
pretraining data. In this work, we investigate the problem of continually
adding new languages to a multilingual model, assuming access to pretraining
data in only the target languages. We explore multiple approaches to address
this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank
Adapters (LoRA) to selected initial and final layers while keeping the rest of
the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,
and (2) multilingual models encode inputs in the source language in the initial
layers, reason in English in intermediate layers, and translate back to the
source language in final layers. We experiment with adding multiple
combinations of Galician, Swahili, and Urdu to pretrained language models and
evaluate each method on diverse multilingual tasks. We find that LayRA provides
the overall best tradeoff between preserving models' capabilities in previously
supported languages, while being competitive with existing approaches such as
LoRA in learning new languages. We also demonstrate that using model
arithmetic, the adapted models can be equipped with strong instruction
following abilities without access to any instruction tuning data in the target
languages.

</details>


### [40] [A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm](https://arxiv.org/abs/2509.11443)
*Gaurab Chhetri,Darrell Anderson,Boniphace Kutela,Subasish Das*

Main category: cs.CL

TL;DR: 本研究分析了15分钟城市概念在Twitter、Reddit和新闻媒体上的公众情感，利用压缩模型进行情感分类，发现小型模型在性能和效率方面具有竞争力，并指出了平台特定的权衡。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在分析公众对15分钟城市概念的多平台情感分析，以支持城市规划 discourse 中的情感分类。

Method: 使用压缩的Transformer模型和Llama-3-8B进行注释，对跨平台的文本进行情感分类。构建了一个处理长文本和短文本的管道，并通过分层5折交叉验证对五种模型进行基准测试。

Result: DistilRoBERTa在F1分数上表现最佳（0.8292），TinyBERT在效率方面表现最好，MiniLM在跨平台一致性方面表现最佳。新闻数据由于类别不平衡导致性能被高估，Reddit由于摘要损失而受到影响，Twitter提供了适度的挑战。

Conclusion: 研究发现压缩模型在情感分类任务中表现良好，挑战了大型模型是必要的假设。同时，研究识别出平台特定的权衡，并提出了城市规划话语中可扩展、现实的情感分类方向。

Abstract: This study presents the first multi-platform sentiment analysis of public
opinion on the 15-minute city concept across Twitter, Reddit, and news media.
Using compressed transformer models and Llama-3-8B for annotation, we classify
sentiment across heterogeneous text domains. Our pipeline handles long-form and
short-form text, supports consistent annotation, and enables reproducible
evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,
ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting
F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1
(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform
consistency. Results show News data yields inflated performance due to class
imbalance, Reddit suffers from summarization loss, and Twitter offers moderate
challenge. Compressed models perform competitively, challenging assumptions
that larger models are necessary. We identify platform-specific trade-offs and
propose directions for scalable, real-world sentiment classification in urban
planning discourse.

</details>


### [41] [CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media](https://arxiv.org/abs/2509.11444)
*Gaurab Chhetri,Anandi Dutta,Subasish Das*

Main category: cs.CL

TL;DR: CognitiveSky is a framework for analyzing public discourse on decentralized social media platforms like Bluesky, using transformer-based models to produce structured outputs for various applications.


<details>
  <summary>Details</summary>
Motivation: The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse.

Method: CognitiveSky uses transformer-based models to annotate large-scale user-generated content from Bluesky's API and produces structured outputs for analysis.

Result: CognitiveSky achieves low operational cost and high accessibility by being built entirely on free-tier infrastructure, and it can be applied across domains such as disinformation detection, crisis response, and civic sentiment analysis.

Conclusion: CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.

Abstract: The emergence of decentralized social media platforms presents new
opportunities and challenges for real-time analysis of public discourse. This
study introduces CognitiveSky, an open-source and scalable framework designed
for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter
or X.com alternative. By ingesting data through Bluesky's Application
Programming Interface (API), CognitiveSky applies transformer-based models to
annotate large-scale user-generated content and produces structured and
analyzable outputs. These summaries drive a dynamic dashboard that visualizes
evolving patterns in emotion, activity, and conversation topics. Built entirely
on free-tier infrastructure, CognitiveSky achieves both low operational cost
and high accessibility. While demonstrated here for monitoring mental health
discourse, its modular design enables applications across domains such as
disinformation detection, crisis response, and civic sentiment analysis. By
bridging large language models with decentralized networks, CognitiveSky offers
a transparent, extensible tool for computational social science in an era of
shifting digital ecosystems.

</details>


### [42] [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
*Amirhossein Abaskohi,Raymond Li,Chuyuan Li,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: CEMTM 是一种上下文增强的多模态主题模型，旨在从包含文本和图像的短文档和长文档中推断出连贯且可解释的主题结构。它利用微调的大规模视觉语言模型获取上下文化嵌入，并采用分布注意力机制来加权标记级对主题推断的贡献。通过重建目标，将基于主题的表示与文档嵌入对齐，促进跨模态的语义一致性。CEMTM 可以处理每篇文档中的多张图像而无需重复编码，并通过显式的词-主题和文档-主题分布保持可解释性。在六个多模态基准上的实验表明，CEMTM 始终优于单模态和多模态基线，平均 LLM 得分达到 2.61。进一步分析显示其在下游少样本检索中的有效性，并能够捕捉科学文章等复杂领域中的视觉基础语义。


<details>
  <summary>Details</summary>
Motivation: Existing approaches have limitations in processing multiple images per document without repeated encoding and maintaining interpretability. CEMTM aims to address these issues by introducing a context-enhanced multimodal topic model that can infer coherent and interpretable topic structures from both short and long documents containing text and images.

Method: CEMTM builds on fine-tuned large vision language models (LVLMs) to obtain contextualized embeddings and employs a distributional attention mechanism to weight token-level contributions to topic inference. A reconstruction objective aligns topic-based representations with the document embedding, encouraging semantic consistency across modalities.

Result: CEMTM consistently outperforms unimodal and multimodal baselines, achieving a remarkable average LLM score of 2.61. It is effective in downstream few-shot retrieval and can capture visually grounded semantics in complex domains such as scientific articles.

Conclusion: CEMTM consistently outperforms unimodal and multimodal baselines, achieving a remarkable average LLM score of 2.61. It is effective in downstream few-shot retrieval and can capture visually grounded semantics in complex domains such as scientific articles.

Abstract: We introduce CEMTM, a context-enhanced multimodal topic model designed to
infer coherent and interpretable topic structures from both short and long
documents containing text and images. CEMTM builds on fine-tuned large vision
language models (LVLMs) to obtain contextualized embeddings, and employs a
distributional attention mechanism to weight token-level contributions to topic
inference. A reconstruction objective aligns topic-based representations with
the document embedding, encouraging semantic consistency across modalities.
Unlike existing approaches, CEMTM can process multiple images per document
without repeated encoding and maintains interpretability through explicit
word-topic and document-topic distributions. Extensive experiments on six
multimodal benchmarks show that CEMTM consistently outperforms unimodal and
multimodal baselines, achieving a remarkable average LLM score of 2.61. Further
analysis shows its effectiveness in downstream few-shot retrieval and its
ability to capture visually grounded semantics in complex domains such as
scientific articles.

</details>


### [43] [Improving LLMs' Learning for Coreference Resolution](https://arxiv.org/abs/2509.11466)
*Yujian Gan,Yuan Liang,Yanni Lin,Juntao Yu,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文研究了基于LLM的共指消解方法的局限性，并提出了两种新方法来提高其性能和减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在共指消解任务中存在幻觉和性能不足的问题。

Method: 研究了现有基于LLM的CR方法的局限性，并提出了两种新的技术：反向训练与联合推理以及迭代文档生成。

Result: 反向训练改进了QA模板方法，而迭代文档生成消除了生成的源文本中的幻觉并提高了共指消解效果。

Conclusion: 集成这些方法和技术为基于LLM的共指消解提供了一个有效且稳健的解决方案。

Abstract: Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs
struggle with hallucination and under-performance. In this paper, we
investigate the limitations of existing LLM-based approaches to CR-specifically
the Question-Answering (QA) Template and Document Template methods and propose
two novel techniques: Reversed Training with Joint Inference and Iterative
Document Generation. Our experiments show that Reversed Training improves the
QA Template method, while Iterative Document Generation eliminates
hallucinations in the generated source text and boosts coreference resolution.
Integrating these methods and techniques offers an effective and robust
solution to LLM-based coreference resolution.

</details>


### [44] [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)
*Anirban Saha Anik,Md Fahimul Kabir Chowdhury,Andrew Wyckoff,Sagnik Ray Choudhury*

Main category: cs.CL

TL;DR: 本文提出了一个系统，用于CLEF 2025 CheckThat! Lab的任务3，专注于使用检索到的证据验证数值和时间声明。


<details>
  <summary>Details</summary>
Motivation: 本文旨在验证数值和时间声明，使用检索到的证据。

Method: 我们探索了两种互补的方法：使用指令调优的大语言模型（LLMs）的零样本提示和使用参数高效的LoRA进行监督微调。

Result: 我们最佳性能的模型LLaMA在英语验证集上表现强劲。然而，在测试集上的显著下降突显了泛化挑战。

Conclusion: 这些发现强调了证据粒度和模型适应性在稳健的数值事实验证中的重要性。

Abstract: This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,
which focuses on verifying numerical and temporal claims using retrieved
evidence. We explore two complementary approaches: zero-shot prompting with
instruction-tuned large language models (LLMs) and supervised fine-tuning using
parameter-efficient LoRA. To enhance evidence quality, we investigate several
selection strategies, including full-document input and top-k sentence
filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned
with LoRA achieves strong performance on the English validation set. However, a
notable drop in the test set highlights a generalization challenge. These
findings underscore the importance of evidence granularity and model adaptation
for robust numerical fact verification.

</details>


### [45] [AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization](https://arxiv.org/abs/2509.11496)
*Fabrycio Leite Nakano Almada,Kauan Divino Pouso Mariano,Maykon Adriell Dutra,Victor Emanuel da Silva Monteiro,Juliana Resplande Sant'Anna Gomes,Arlindo Rodrigues Galvão Filho,Anderson da Silva Soares*

Main category: cs.CL

TL;DR: 本文介绍了我们在CLEF-2025 CheckThat! Task~2中的提交，通过微调的小型语言模型和大型语言模型提示，在二十种语言中取得了优异的成绩。


<details>
  <summary>Details</summary>
Motivation: claim normalization是自动化事实核查流程中的关键步骤，特别是在处理跨二十种语言的挑战时。

Method: 利用微调的小型语言模型（SLMs）处理监督语言，使用大型语言模型（LLM）提示处理零样本场景。

Result: 在二十种语言中，我们的方法在十五种语言中获得前三名，其中八种语言获得第二名，五种是零样本语言。葡萄牙语的平均METEOR得分为0.5290，排名第三。

Conclusion: 我们的方法在二十种语言中获得了前三位，证明了基于LLM的零样本策略的有效性。

Abstract: Claim normalization, the transformation of informal social media posts into
concise, self-contained statements, is a crucial step in automated
fact-checking pipelines. This paper details our submission to the CLEF-2025
CheckThat! Task~2, which challenges systems to perform claim normalization
across twenty languages, divided into thirteen supervised (high-resource) and
seven zero-shot (no training data) tracks.
  Our approach, leveraging fine-tuned Small Language Models (SLMs) for
supervised languages and Large Language Model (LLM) prompting for zero-shot
scenarios, achieved podium positions (top three) in fifteen of the twenty
languages. Notably, this included second-place rankings in eight languages,
five of which were among the seven designated zero-shot languages, underscoring
the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our
initial development language, our system achieved an average METEOR score of
0.5290, ranking third. All implementation artifacts, including inference,
training, evaluation scripts, and prompt configurations, are publicly available
at https://github.com/ju-resplande/checkthat2025_normalization.

</details>


### [46] [DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification](https://arxiv.org/abs/2509.11498)
*Zhuoxuan Ju,Jingni Wu,Abhishek Purushothama,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文介绍了Georgetown University在DISRPT 2025共享任务中的DeDisCo系统，通过两种方法进行话语关系分类，并在低资源语言上进行了增强数据集的训练。


<details>
  <summary>Details</summary>
Motivation: 本文旨在参与DISRPT 2025共享任务，对话语关系进行分类。

Method: 我们测试了两种方法，使用基于mt5的编码器和基于Qwen模型的解码器方法，并在低资源语言上进行了增强数据集的训练。

Result: 我们的系统在DISRPT 2025共享任务中取得了71.28的宏准确率。

Conclusion: 我们的系统在DISRPT 2025共享任务中取得了71.28的宏准确率，同时提供了结果的解释和错误分析。

Abstract: This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025
shared task on discourse relation classification. We test two approaches, using
an mt5-based encoder and a decoder based approach using the openly available
Qwen model. We also experiment on training with augmented dataset for
low-resource languages using matched data translated automatically from
English, as well as using some additional linguistic features inspired by
entries in previous editions of the Shared Task. Our system achieves a
macro-accuracy score of 71.28, and we provide some interpretation and error
analysis for our results.

</details>


### [47] [Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics](https://arxiv.org/abs/2509.11513)
*Zhongyang Hu,Naijie Gu,Xiangzhi Tao,Tianhui Gu,Yibing Zhou*

Main category: cs.CL

TL;DR: 本文提出了两种基于注意力权重和集成梯度的方法，用于改进词义替换任务中的候选词排序。


<details>
  <summary>Details</summary>
Motivation: 现有的方法往往只关注目标位置的语义变化，或者依赖于多个评估指标的参数调整，难以准确描述语义变化。本文旨在解决这一问题。

Method: 本文研究了基于注意力权重和集成梯度方法的两种方法，以衡量上下文标记对目标标记的影响，并通过结合原始句子和替换句子之间的语义相似性来对候选词进行排序。

Result: 实验结果表明，这两种方法都能提高排名性能。

Conclusion: 本文提出的方法在LS07和SWORDS数据集上均表现出色，证明了其在词义替换任务中的有效性。

Abstract: A key subtask in lexical substitution is ranking the given candidate words. A
common approach is to replace the target word with a candidate in the original
sentence and feed the modified sentence into a model to capture semantic
differences before and after substitution. However, effectively modeling the
bidirectional influence of candidate substitution on both the target word and
its context remains challenging. Existing methods often focus solely on
semantic changes at the target position or rely on parameter tuning over
multiple evaluation metrics, making it difficult to accurately characterize
semantic variation. To address this, we investigate two approaches: one based
on attention weights and another leveraging the more interpretable integrated
gradients method, both designed to measure the influence of context tokens on
the target token and to rank candidates by incorporating semantic similarity
between the original and substituted sentences. Experiments on the LS07 and
SWORDS datasets demonstrate that both approaches improve ranking performance.

</details>


### [48] [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)
*Zhengxiang Wang,Weiling Li,Panagiotis Kaliosis,Owen Rambow,Susan E. Brennan*

Main category: cs.CL

TL;DR: 本文研究了七种最先进的大型视觉语言模型（LVLMs）作为旁听者对一对人类对话参与者进行协作物体匹配任务的自发对话的能力。结果表明，这种任务对于当前的LVLMs来说仍然具有挑战性，并且它们在多次听取相同对话参与者重复同一任务的对话后，未能表现出一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 理解这样的指称表达对于具身代理来说是一项重要的能力，以便它能够在现实世界中执行任务。这需要整合和理解语言、视觉和对话互动。

Method: 本文研究了七种最先进的大型视觉语言模型（LVLMs）作为旁听者对一对人类对话参与者进行协作物体匹配任务的自发对话的能力。

Result: 本文发现，这种任务对于当前的LVLMs来说仍然具有挑战性，并且它们在多次听取相同对话参与者重复同一任务的对话后，未能表现出一致的性能提升。

Conclusion: 本文认为当前的LVLMs在处理这种任务时仍然面临挑战，并且它们在多次听取相同对话参与者重复同一任务的对话后，未能表现出一致的性能提升。此外，作者发布了他们的语料库和代码以促进未来的研究。

Abstract: During spontaneous conversations, speakers collaborate on novel referring
expressions, which they can then re-use in subsequent conversations.
Understanding such referring expressions is an important ability for an
embodied agent, so that it can carry out tasks in the real world. This requires
integrating and understanding language, vision, and conversational interaction.
We study the capabilities of seven state-of-the-art Large Vision Language
Models (LVLMs) as overhearers to a corpus of spontaneous conversations between
pairs of human discourse participants engaged in a collaborative
object-matching task. We find that such a task remains challenging for current
LVLMs and they all fail to show a consistent performance improvement as they
overhear more conversations from the same discourse participants repeating the
same task for multiple rounds. We release our corpus and code for
reproducibility and to facilitate future research.

</details>


### [49] [PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation](https://arxiv.org/abs/2509.11517)
*Rodrigo M. Carrillo-Larco,Jesus Lovón Melgarejo,Manuel Castillo-Cara,Gusseppe Bravo-Rocca*

Main category: cs.CL

TL;DR: 本文构建了一个包含8,380个问题的秘鲁医学考试数据集，对LLM进行了微调，并评估了其性能。结果表明，medgemma-27b-text-it在多个实例中表现优异，而微调后的medgemma-4b-it在与参数较少的LLM竞争中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医学大语言模型（LLM）在回答医学考试方面表现出色。然而，这种高性能在西班牙语医学问题和拉丁美洲国家中的可转移性仍未知。这种知识对基于LLM的医疗应用在拉丁美洲的推广至关重要。

Method: 我们整理了PeruMedQA数据集，这是一个包含8,380个问题的多项选择题问答（MCQA）数据集，涵盖12个医学领域（2018-2025）。我们选择了八个医学LLM，并开发了零样本任务特定提示来正确回答问题。我们使用参数高效微调（PEFT）和低秩适应（LoRA）对medgemma-4b-it进行微调，利用除2025年外的所有问题（测试集）。

Result: medgemma-27b-text-it在多个实例中表现优于其他所有模型，正确答案的比例超过90%。参数少于100亿的LLM正确答案比例低于60%，某些考试的结果甚至低于50%。微调后的medgemma-4b-it在与参数少于100亿的LLM竞争中胜出，并在各种考试中与拥有700亿参数的LLM相抗衡。

Conclusion: 对于需要来自西班牙语国家的知识库以及具有类似秘鲁流行病学特征的医疗AI应用和研究，相关方应使用medgemma-27b-text-it或微调后的medgemma-4b-it版本。

Abstract: BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable
performance in answering medical examinations. However, the extent to which
this high performance is transferable to medical questions in Spanish and from
a Latin American country remains unexplored. This knowledge is crucial as
LLM-based medical applications gain traction in Latin America. AIMS: to build a
dataset of questions from medical examinations taken by Peruvian physicians
pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate
and compare the performance in terms of accuracy between vanilla LLMs and the
fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice
question-answering (MCQA) datasets containing 8,380 questions spanning 12
medical domains (2018-2025). We selected eight medical LLMs including
medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific
prompts to answer the questions appropriately. We employed parameter-efficient
fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it
utilizing all questions except those from 2025 (test set). RESULTS:
medgemma-27b-text-it outperformed all other models, achieving a proportion of
correct answers exceeding 90% in several instances. LLMs with <10 billion
parameters exhibited <60% of correct answers, while some exams yielded results
<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all
LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters
across various examinations. CONCLUSIONS: For medical AI application and
research that require knowledge bases from Spanish-speaking countries and those
exhibiting similar epidemiological profiles to Peru's, interested parties
should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

</details>


### [50] [On the Distinctive Co-occurrence Characteristics of Antonymy](https://arxiv.org/abs/2509.11534)
*Zhihan Cao,Hiroaki Yamada,Takenobu Tokunaga*

Main category: cs.CL

TL;DR: 本文比较了反义关系与其他三种语义关系的共现模式，发现反义对在共现强度、线性顺序和短跨度方面具有独特性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，反义对在文本中频繁共现，但这种共现模式是否是反义关系的独特特征尚不清楚，因为缺乏与其他语义关系的比较。

Method: 本文使用稳健的共现度指标，比较了反义关系与其他三种语义关系在词性上的共现模式。

Result: 研究发现反义对在共现强度、线性顺序和短跨度方面表现出独特性。

Conclusion: 本文通过比较反义关系与其他三种语义关系，发现反义对在共现强度、线性顺序和短跨度方面具有独特性。

Abstract: Antonymy has long received particular attention in lexical semantics.
Previous studies have shown that antonym pairs frequently co-occur in text,
across genres and parts of speech, more often than would be expected by chance.
However, whether this co-occurrence pattern is distinctive of antonymy remains
unclear, due to a lack of comparison with other semantic relations. This work
fills the gap by comparing antonymy with three other relations across parts of
speech using robust co-occurrence metrics. We find that antonymy is distinctive
in three respects: antonym pairs co-occur with high strength, in a preferred
linear order, and within short spans. All results are available online.

</details>


### [51] [HARP: Hallucination Detection via Reasoning Subspace Projection](https://arxiv.org/abs/2509.11536)
*Junjie Hu,Gang Tu,ShengYu Cheng,Jinxin Li,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: HARP is a novel hallucination detection framework that decomposes the hidden state space of LLMs into semantic and reasoning subspaces, achieving state-of-the-art performance in hallucination detection.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination detection methods struggle with disentangling semantic and reasoning information and maintaining robustness. HARP aims to address these challenges by leveraging the decomposition of the hidden state space into semantic and reasoning subspaces.

Method: HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace. It uses the Unembedding layer to disentangle these subspaces and applies SVD to obtain basis vectors spanning the semantic and reasoning subspaces. HARP then projects hidden states onto the basis vectors of the reasoning subspace for hallucination detection.

Result: HARP reduces the feature dimension to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. It achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%.

Conclusion: HARP achieves state-of-the-art hallucination detection performance, with an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%.

Abstract: Hallucinations in Large Language Models (LLMs) pose a major barrier to their
reliable use in critical decision-making. Although existing hallucination
detection methods have improved accuracy, they still struggle with
disentangling semantic and reasoning information and maintaining robustness. To
address these challenges, we propose HARP (Hallucination detection via
reasoning subspace projection), a novel hallucination detection framework. HARP
establishes that the hidden state space of LLMs can be decomposed into a direct
sum of a semantic subspace and a reasoning subspace, where the former encodes
linguistic expression and the latter captures internal reasoning processes.
Moreover, we demonstrate that the Unembedding layer can disentangle these
subspaces, and by applying Singular Value Decomposition (SVD) to its
parameters, the basis vectors spanning the semantic and reasoning subspaces are
obtained. Finally, HARP projects hidden states onto the basis vectors of the
reasoning subspace, and the resulting projections are then used as input
features for hallucination detection in LLMs. By using these projections, HARP
reduces the dimension of the feature to approximately 5% of the original,
filters out most noise, and achieves enhanced robustness. Experiments across
multiple datasets show that HARP achieves state-of-the-art hallucination
detection performance; in particular, it achieves an AUROC of 92.8% on
TriviaQA, outperforming the previous best method by 7.5%.

</details>


### [52] [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)
*Wensheng Lu,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.CL

TL;DR: 本文分析了现有RAG评估基准在评估文档分块质量方面的不足，并提出了HiCBench基准测试和HiChunk框架，以提升RAG系统的整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估基准在评估文档分块质量方面存在不足，主要是由于证据稀疏性问题。

Method: 提出HiCBench基准测试和HiChunk框架，结合微调的LLM和Auto-Merge检索算法来改进检索质量。

Result: HiCBench能有效评估不同分块方法对整个RAG流水线的影响，HiChunk在合理时间消耗内实现更好的分块质量。

Conclusion: HiCBench能够有效评估不同分块方法对整个RAG流水线的影响，同时HiChunk在合理的时间消耗内实现了更好的分块质量，从而提升了RAG系统的整体性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response capabilities of
language models by integrating external knowledge sources. However, document
chunking as an important part of RAG system often lacks effective evaluation
tools. This paper first analyzes why existing RAG evaluation benchmarks are
inadequate for assessing document chunking quality, specifically due to
evidence sparsity. Based on this conclusion, we propose HiCBench, which
includes manually annotated multi-level document chunking points, synthesized
evidence-dense quetion answer(QA) pairs, and their corresponding evidence
sources. Additionally, we introduce the HiChunk framework, a multi-level
document structuring framework based on fine-tuned LLMs, combined with the
Auto-Merge retrieval algorithm to improve retrieval quality. Experiments
demonstrate that HiCBench effectively evaluates the impact of different
chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves
better chunking quality within reasonable time consumption, thereby enhancing
the overall performance of RAG systems.

</details>


### [53] [D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs](https://arxiv.org/abs/2509.11569)
*Yue Ding,Xiaofang Zhu,Tianze Xia,Junfei Wu,Xinlong Chen,Qiang Liu,Liang Wang*

Main category: cs.CL

TL;DR: This paper introduces D$^2$HScore, a method for detecting hallucinations in large language models by analyzing the semantic breadth and depth of token representations during inference.


<details>
  <summary>Details</summary>
Motivation: Ensuring the reliability of LLMs' outputs is critical, especially in high-stakes domains. The work aims to address the challenge of detecting hallucinations by revisiting the problem from the perspective of model architecture and generation dynamics.

Method: D$^2$HScore (Dispersion and Drift-based Hallucination Score) is a training-free and label-free framework that measures Intra-Layer Dispersion and Inter-Layer Drift to detect hallucinations in LLMs.

Result: Extensive experiments across five open-source LLMs and five widely used benchmarks demonstrate that D$^2$HScore consistently outperforms existing training-free baselines.

Conclusion: D$^2$HScore provides an interpretable and lightweight proxy for hallucination detection, and it consistently outperforms existing training-free baselines across multiple LLMs and benchmarks.

Abstract: Although large Language Models (LLMs) have achieved remarkable success, their
practical application is often hindered by the generation of non-factual
content, which is called "hallucination". Ensuring the reliability of LLMs'
outputs is a critical challenge, particularly in high-stakes domains such as
finance, security, and healthcare. In this work, we revisit hallucination
detection from the perspective of model architecture and generation dynamics.
Leveraging the multi-layer structure and autoregressive decoding process of
LLMs, we decompose hallucination signals into two complementary dimensions: the
semantic breadth of token representations within each layer, and the semantic
depth of core concepts as they evolve across layers. Based on this insight, we
propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},
a training-free and label-free framework that jointly measures: (1)
\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of
token representations within each layer; and (2) \textbf{Inter-Layer Drift},
which tracks the progressive transformation of key token representations across
layers. To ensure drift reflects the evolution of meaningful semantics rather
than noisy or redundant tokens, we guide token selection using attention
signals. By capturing both the horizontal and vertical dynamics of
representation during inference, D$^2$HScore provides an interpretable and
lightweight proxy for hallucination detection. Extensive experiments across
five open-source LLMs and five widely used benchmarks demonstrate that
D$^2$HScore consistently outperforms existing training-free baselines.

</details>


### [54] [Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges](https://arxiv.org/abs/2509.11570)
*Sampoorna Poria,Xiaolei Huang*

Main category: cs.CL

TL;DR: 本文综述了南亚语言NLP模型的现状和挑战，强调了数据、模型和任务方面的不足，并呼吁加强南亚语言的可见性和数据收集。


<details>
  <summary>Details</summary>
Motivation: 由于南亚地区有超过650种语言，但许多语言缺乏计算资源或未被现有语言模型覆盖，因此需要评估当前状况和挑战，以促进南亚语言的NLP模型发展。

Method: 本文通过检索2020年以来的研究，全面考察了南亚语言NLP模型的现状和挑战，重点关注基于Transformer的模型，如BERT、T5和GPT。

Result: 本文发现了关键领域（如健康）的数据缺失、代码混合以及缺乏标准化评估基准等问题。

Conclusion: 本文的结论是，需要提高南亚语言在NLP社区中的可见性，并推动针对南亚文化和语言特征的统一基准和数据收集。

Abstract: Rapid developments of large language models have revolutionized many NLP
tasks for English data. Unfortunately, the models and their evaluations for
low-resource languages are being overlooked, especially for languages in South
Asia. Although there are more than 650 languages in South Asia, many of them
either have very limited computational resources or are missing from existing
language models. Thus, a concrete question to be answered is: Can we assess the
current stage and challenges to inform our NLP community and facilitate model
developments for South Asian languages? In this survey, we have comprehensively
examined current efforts and challenges of NLP models for South Asian languages
by retrieving studies since 2020, with a focus on transformer-based models,
such as BERT, T5, & GPT. We present advances and gaps across 3 essential
aspects: data, models, & tasks, such as available data sources, fine-tuning
strategies, & domain applications. Our findings highlight substantial issues,
including missing data in critical domains (e.g., health), code-mixing, and
lack of standardized evaluation benchmarks. Our survey aims to raise awareness
within the NLP community for more targeted data curation, unify benchmarks
tailored to cultural and linguistic nuances of South Asia, and encourage an
equitable representation of South Asian languages. The complete list of
resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.

</details>


### [55] [Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study](https://arxiv.org/abs/2509.11591)
*Chu-Hsuan Lee,Chen-Chi Chang,Hung-Shin Lee,Yun-Hsiang Hsu,Ching-Yuan Chen*

Main category: cs.CL

TL;DR: 本研究通过结合Bloom认知过程分类法和对话行为分类法，分析了TALKA聊天机器人中7077条用户语句，发现生成式AI聊天机器人可以在理解用户思维和交流方式的基础上，有效支持语言学习，并帮助学习者更自信地表达自己，与文化身份建立联系。


<details>
  <summary>Details</summary>
Motivation: With many endangered languages at risk of disappearing, efforts to preserve them now rely more than ever on using technology alongside culturally informed teaching strategies.

Method: This study examines user behaviors in TALKA, a generative AI-powered chatbot designed for Hakka language engagement, by employing a dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive processes and dialogue act categorization. We analyzed 7,077 user utterances, each carefully annotated according to six cognitive levels and eleven dialogue act types.

Result: The results suggest that generative AI chatbots can support language learning in meaningful ways--especially when they are designed with an understanding of how users think and communicate. They may also help learners express themselves more confidently and connect with their cultural identity.

Conclusion: TALKA case provides empirical insights into how AI-mediated dialogue facilitates cognitive development in low-resource language learners, as well as pragmatic negotiation and socio-cultural affiliation. By focusing on AI-assisted language learning, this study offers new insights into how technology can support language preservation and educational practice.

Abstract: With many endangered languages at risk of disappearing, efforts to preserve
them now rely more than ever on using technology alongside culturally informed
teaching strategies. This study examines user behaviors in TALKA, a generative
AI-powered chatbot designed for Hakka language engagement, by employing a
dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive
processes and dialogue act categorization. We analyzed 7,077 user utterances,
each carefully annotated according to six cognitive levels and eleven dialogue
act types. These included a variety of functions, such as asking for
information, requesting translations, making cultural inquiries, and using
language creatively. Pragmatic classifications further highlight how different
types of dialogue acts--such as feedback, control commands, and social
greetings--align with specific cognitive intentions. The results suggest that
generative AI chatbots can support language learning in meaningful
ways--especially when they are designed with an understanding of how users
think and communicate. They may also help learners express themselves more
confidently and connect with their cultural identity. The TALKA case provides
empirical insights into how AI-mediated dialogue facilitates cognitive
development in low-resource language learners, as well as pragmatic negotiation
and socio-cultural affiliation. By focusing on AI-assisted language learning,
this study offers new insights into how technology can support language
preservation and educational practice.

</details>


### [56] [Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification](https://arxiv.org/abs/2509.11604)
*Md. Mithun Hossain,Sanjara,Md. Shakil Hossain,Sudipto Chaki*

Main category: cs.CL

TL;DR: SpanEIT is a novel framework for entity-level sentiment classification that improves accuracy by capturing complex interactions and dependencies in text.


<details>
  <summary>Details</summary>
Motivation: Entity-level sentiment classification is challenging due to complex interactions between entities and sentiment expressions, dependencies across sentences, and the need for consistent predictions through coreference resolution.

Method: SpanEIT integrates dynamic span interaction and graph-aware memory mechanisms, using bidirectional attention for fine-grained interactions and a graph attention network to capture syntactic and co-occurrence relations.

Result: Experiments on FSAD, BARU, and IMDB datasets show SpanEIT outperforms state-of-the-art transformer and hybrid baselines in accuracy and F1 scores.

Conclusion: SpanEIT demonstrates superior performance in entity-level sentiment classification, showing potential for applications like social media monitoring and customer feedback analysis.

Abstract: Entity-level sentiment classification involves identifying the sentiment
polarity linked to specific entities within text. This task poses several
challenges: effectively modeling the subtle and complex interactions between
entities and their surrounding sentiment expressions; capturing dependencies
that may span across sentences; and ensuring consistent sentiment predictions
for multiple mentions of the same entity through coreference resolution.
Additionally, linguistic phenomena such as negation, ambiguity, and overlapping
opinions further complicate the analysis. These complexities make entity-level
sentiment classification a difficult problem, especially in real-world, noisy
textual data. To address these issues, we propose SpanEIT, a novel framework
integrating dynamic span interaction and graph-aware memory mechanisms for
enhanced entity-sentiment relational modeling. SpanEIT builds span-based
representations for entities and candidate sentiment phrases, employs
bidirectional attention for fine-grained interactions, and uses a graph
attention network to capture syntactic and co-occurrence relations. A
coreference-aware memory module ensures entity-level consistency across
documents. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT
outperforms state-of-the-art transformer and hybrid baselines in accuracy and
F1 scores. Ablation and interpretability analyses validate the effectiveness of
our approach, underscoring its potential for fine-grained sentiment analysis in
applications like social media monitoring and customer feedback analysis.

</details>


### [57] [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)
*Spandan Anaokar,Shrey Ganatra,Harshvivek Kashid,Swapnil Bhattacharyya,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的幻觉检测系统HalluDetect，并通过基准测试发现AgentBot是最有效的幻觉缓解策略。研究结果表明优化的推理策略可以显著提高事实准确性，并且该方法适用于其他高风险领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在工业中被广泛使用，但容易产生幻觉，这限制了它们在关键应用中的可靠性。本文旨在减少消费投诉聊天机器人中的幻觉。

Method: 我们开发了HalluDetect，这是一个基于LLM的幻觉检测系统，以及对五种聊天机器人架构的基准测试，以找到最小化幻觉并保持最高令牌准确性的策略。

Result: HalluDetect实现了69%的F1分数，优于基线检测器25.44%。AgentBot将幻觉最小化到每轮0.4159，同时保持最高的令牌准确性（96.13%）。

Conclusion: 我们的研究提供了一个可扩展的框架来减轻幻觉，表明优化的推理策略可以显著提高事实准确性。虽然应用于消费者法，但我们的方法可以推广到其他高风险领域，从而增强对LLM驱动助手的信任。

Abstract: Large Language Models (LLMs) are widely used in industry but remain prone to
hallucinations, limiting their reliability in critical applications. This work
addresses hallucination reduction in consumer grievance chatbots built using
LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop
HalluDetect, an LLM-based hallucination detection system that achieves an F1
score of 69% outperforming baseline detectors by 25.44%. Benchmarking five
chatbot architectures, we find that out of them, AgentBot minimizes
hallucinations to 0.4159 per turn while maintaining the highest token accuracy
(96.13%), making it the most effective mitigation strategy. Our findings
provide a scalable framework for hallucination mitigation, demonstrating that
optimized inference strategies can significantly improve factual accuracy.
While applied to consumer law, our approach generalizes to other high-risk
domains, enhancing trust in LLM-driven assistants. We will release the code and
dataset

</details>


### [58] [AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment](https://arxiv.org/abs/2509.11620)
*Kun Li,Lai-Man Po,Hongzheng Yang,Xuyuan Xu,Kangcheng Liu,Yuzhi Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准AesBiasBench，用于评估多模态大语言模型在个性化图像审美评估中的偏差和与人类偏好的对齐度，并发现较小的模型更容易表现出刻板印象偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型被广泛应用于个性化图像审美评估，但它们的预测可能受到性别、年龄和教育等人口统计因素的影响，因此需要一种评估框架来检测这些偏差。

Method: 本文提出了AesBiasBench，一个用于评估多模态大语言模型（MLLMs）的基准，涵盖了三个子任务（审美感知、评估、共情），并引入了结构化指标（IFD、NRD、AAS）来评估偏差和对齐度。

Result: 评估19个MLLMs的结果表明，较小的模型表现出更强的刻板印象偏差，而较大的模型更接近人类偏好。包含身份信息往往会加剧偏差，特别是在情感判断中。

Conclusion: 研究结果表明，较小的模型表现出更强的刻板印象偏差，而较大的模型更接近人类偏好。包含身份信息往往会加剧偏差，特别是在情感判断中。这些发现强调了在主观视觉-语言任务中使用身份感知评估框架的重要性。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly applied in
Personalized Image Aesthetic Assessment (PIAA) as a scalable alternative to
expert evaluations. However, their predictions may reflect subtle biases
influenced by demographic factors such as gender, age, and education. In this
work, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two
complementary dimensions: (1) stereotype bias, quantified by measuring
variations in aesthetic evaluations across demographic groups; and (2)
alignment between model outputs and genuine human aesthetic preferences. Our
benchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and
introduces structured metrics (IFD, NRD, AAS) to assess both bias and
alignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,
Claude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).
Results indicate that smaller models exhibit stronger stereotype biases,
whereas larger models align more closely with human preferences. Incorporating
identity information often exacerbates bias, particularly in emotional
judgments. These findings underscore the importance of identity-aware
evaluation frameworks in subjective vision-language tasks.

</details>


### [59] [EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI](https://arxiv.org/abs/2509.11648)
*Sai Kartheek Reddy Kasu*

Main category: cs.CL

TL;DR: EthicsMH is a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in mental health contexts. It aims to bridge AI ethics and mental health decision-making by providing a framework for evaluating decision accuracy, explanation quality, and alignment with professional norms.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for moral and clinical decision-making do not adequately capture the unique ethical dilemmas encountered in mental health practice, where confidentiality, autonomy, beneficence, and bias frequently intersect.

Method: Introduce EthicsMH, a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in therapeutic and psychiatric contexts. Each scenario is enriched with structured fields, including multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints.

Result: EthicsMH enables evaluation not only of decision accuracy but also of explanation quality and alignment with professional norms. Although modest in scale and developed with model-assisted generation, it establishes a task framework that bridges AI ethics and mental health decision-making.

Conclusion: EthicsMH establishes a task framework that bridges AI ethics and mental health decision-making. By releasing this dataset, we aim to provide a seed resource that can be expanded through community and expert contributions, fostering the development of AI systems capable of responsibly handling some of society's most delicate decisions.

Abstract: The deployment of large language models (LLMs) in mental health and other
sensitive domains raises urgent questions about ethical reasoning, fairness,
and responsible alignment. Yet, existing benchmarks for moral and clinical
decision-making do not adequately capture the unique ethical dilemmas
encountered in mental health practice, where confidentiality, autonomy,
beneficence, and bias frequently intersect. To address this gap, we introduce
Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios
designed to evaluate how AI systems navigate ethically charged situations in
therapeutic and psychiatric contexts. Each scenario is enriched with structured
fields, including multiple decision options, expert-aligned reasoning, expected
model behavior, real-world impact, and multi-stakeholder viewpoints. This
structure enables evaluation not only of decision accuracy but also of
explanation quality and alignment with professional norms. Although modest in
scale and developed with model-assisted generation, EthicsMH establishes a task
framework that bridges AI ethics and mental health decision-making. By
releasing this dataset, we aim to provide a seed resource that can be expanded
through community and expert contributions, fostering the development of AI
systems capable of responsibly handling some of society's most delicate
decisions.

</details>


### [60] [A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection](https://arxiv.org/abs/2509.11687)
*Di Jin,Jun Yang,Xiaobao Wang,Junwei Zhang,Shuqi Li,Dongxiao He*

Main category: cs.CL

TL;DR: 本文提出了一种动态知识更新驱动的假新闻检测模型（DYNAMO），利用知识图谱实现新知识的持续更新，并与大语言模型结合，完成新闻真实性检测和新知识正确性验证，解决了确保新知识真实性和深入挖掘新闻语义的两个关键问题。实验结果表明，DYNAMO在两个现实数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: Due to the suddenness and instability of news events, the authenticity labels of news can potentially shift as events develop, making it crucial for fake news detection to obtain the latest event updates. Existing methods employ retrieval-augmented generation to fill knowledge gaps, but they suffer from issues such as insufficient credibility of retrieved content and interference from noisy information.

Method: We propose a dynamic knowledge update-driven model for fake news detection (DYNAMO), which leverages knowledge graphs to achieve continuous updating of new knowledge and integrates with large language models to fulfill dual functions: news authenticity detection and verification of new knowledge correctness.

Result: DYNAMO achieves the best performance on two real-world datasets.

Conclusion: DYNAMO achieves the best performance on two real-world datasets.

Abstract: As the Internet and social media evolve rapidly, distinguishing credible news
from a vast amount of complex information poses a significant challenge. Due to
the suddenness and instability of news events, the authenticity labels of news
can potentially shift as events develop, making it crucial for fake news
detection to obtain the latest event updates. Existing methods employ
retrieval-augmented generation to fill knowledge gaps, but they suffer from
issues such as insufficient credibility of retrieved content and interference
from noisy information. We propose a dynamic knowledge update-driven model for
fake news detection (DYNAMO), which leverages knowledge graphs to achieve
continuous updating of new knowledge and integrates with large language models
to fulfill dual functions: news authenticity detection and verification of new
knowledge correctness, solving the two key problems of ensuring the
authenticity of new knowledge and deeply mining news semantics. Specifically,
we first construct a news-domain-specific knowledge graph. Then, we use Monte
Carlo Tree Search to decompose complex news and verify them step by step.
Finally, we extract and update new knowledge from verified real news texts and
reasoning paths. Experimental results demonstrate that DYNAMO achieves the best
performance on two real-world datasets.

</details>


### [61] [CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model](https://arxiv.org/abs/2509.11698)
*Wei-Hsin Yeh,Yu-An Su,Chih-Ning Chen,Yi-Hsueh Lin,Calvin Ku,Wen-Hsin Chiu,Min-Chun Hu,Lun-Wei Ku*

Main category: cs.CL

TL;DR: CoachMe is a reference-based model that improves motion instruction by analyzing differences between a learner's motion and a reference, providing effective feedback for sports like skating and boxing.


<details>
  <summary>Details</summary>
Motivation: The challenge of generating precise and sport-specific instruction due to the highly domain-specific nature of sports and the need for informative guidance.

Method: CoachMe is a reference-based model that analyzes the differences between a learner's motion and a reference under temporal and physical aspects, enabling domain-knowledge learning and coach-like thinking process to identify movement errors and provide feedback.

Result: CoachMe provides high-quality instructions instead of directions merely in the tone of a coach but without critical information. It adapts well to specific sports such as skating and boxing by learning from general movements and leveraging limited data.

Conclusion: CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on boxing. It provides high-quality instructions that elaborate on errors and their corresponding improvement methods.

Abstract: Motion instruction is a crucial task that helps athletes refine their
technique by analyzing movements and providing corrective guidance. Although
recent advances in multimodal models have improved motion understanding,
generating precise and sport-specific instruction remains challenging due to
the highly domain-specific nature of sports and the need for informative
guidance. We propose CoachMe, a reference-based model that analyzes the
differences between a learner's motion and a reference under temporal and
physical aspects. This approach enables both domain-knowledge learning and the
acquisition of a coach-like thinking process that identifies movement errors
effectively and provides feedback to explain how to improve. In this paper, we
illustrate how CoachMe adapts well to specific sports such as skating and
boxing by learning from general movements and then leveraging limited data.
Experiments show that CoachMe provides high-quality instructions instead of
directions merely in the tone of a coach but without critical information.
CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on
boxing. Analysis further confirms that it elaborates on errors and their
corresponding improvement methods in the generated instructions. You can find
CoachMe here: https://motionxperts.github.io/

</details>


### [62] [Room acoustics affect communicative success in hybrid meeting spaces: a pilot study](https://arxiv.org/abs/2509.11709)
*Robert Einig,Stefan Janscha,Jonas Schuster,Julian Koch,Martin Hagmueller,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本研究探讨了房间声学干预对混合会议交流的影响，结果显示尽管样本量较小，但干预有助于提高交流成功率。


<details>
  <summary>Details</summary>
Motivation: 由于新冠疫情，大学和公司越来越多地将混合功能整合到会议室中，但房间声学设计常常被忽视。不良的声学条件可能导致误解、语音可懂度降低或认知和声音疲劳。

Method: 本研究通过在格拉茨科技大学的一个研讨室中进行房间声学干预，并在干预前后对两组人员进行录音，以评估其对混合会议交流的影响。

Result: 研究结果表明，房间声学干预可以提高混合会议中的交流成功率，尽管由于样本量较小，结果未达到统计显著性。

Conclusion: 尽管样本量较小，但研究结果清楚地表明，我们的空间干预改善了混合会议中的交流成功率。

Abstract: Since the COVID-19 pandemic in 2020, universities and companies have
increasingly integrated hybrid features into their meeting spaces, or even
created dedicated rooms for this purpose. While the importance of a fast and
stable internet connection is often prioritized, the acoustic design of seminar
rooms is frequently overlooked. Poor acoustics, particularly excessive
reverberation, can lead to issues such as misunderstandings, reduced speech
intelligibility or cognitive and vocal fatigue. This pilot study investigates
whether room acoustic interventions in a seminar room at Graz University of
Technology support better communication in hybrid meetings. For this purpose,
we recorded two groups of persons twice, once before and once after improving
the acoustics of the room. Our findings -- despite not reaching statistical
significance due to the small sample size - indicate clearly that our spatial
interventions improve communicative success in hybrid meetings. To make the
paper accessible also for readers from the speech communication community, we
explain room acoustics background, relevant for the interpretation of our
results.

</details>


### [63] [An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents](https://arxiv.org/abs/2509.11773)
*Gaye Colakoglu,Gürkan Solmaz,Jonathan Fürst*

Main category: cs.CL

TL;DR: 该研究提出了一种新的代理系统，用于处理欧盟法规要求的性能声明文档中的关键值对提取和问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有的静态或仅使用LLM的IE管道常常产生幻觉且无法适应这种结构多样性。

Method: 通过规划者-执行者-响应器架构实现的领域特定、状态感知的代理系统。

Result: 在整理好的DoP数据集上的评估显示了跨格式和语言的改进的鲁棒性。

Conclusion: 该系统在受监管的工作流程中提供了一种可扩展的结构化数据提取解决方案。

Abstract: Declaration of Performance (DoP) documents, mandated by EU regulation,
certify the performance of construction products. While some of their content
is standardized, DoPs vary widely in layout, language, schema, and format,
posing challenges for automated key-value pair extraction (KVP) and question
answering (QA). Existing static or LLM-only IE pipelines often hallucinate and
fail to adapt to this structural diversity. Our domain-specific, stateful
agentic system addresses these challenges through a planner-executor-responder
architecture. The system infers user intent, detects document modality, and
orchestrates tools dynamically for robust, traceable reasoning while avoiding
tool misuse or execution loops. Evaluation on a curated DoP dataset
demonstrates improved robustness across formats and languages, offering a
scalable solution for structured data extraction in regulated workflows.

</details>


### [64] [User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums](https://arxiv.org/abs/2509.11777)
*Mikhail Kulyabin,Jan Joosten,Choro Ulan uulu,Nuno Miguel Martins Pacheco,Fabian Ries,Filippos Petridis,Jan Bosch,Helena Holmström Olsson*

Main category: cs.CL

TL;DR: 本文提出了UXPID数据集，这是一个包含7130个人工合成和匿名化用户反馈分支的数据集，用于促进用户需求、用户体验（UX）分析和AI驱动的反馈处理研究。


<details>
  <summary>Details</summary>
Motivation: 客户反馈在工业论坛中反映了现实世界产品体验的丰富但未被充分探索的见解来源。这些公开分享的讨论提供了用户期望、挫折和成功故事的有机视角，但由于内容的非结构化和领域特定性质，将其信息用于系统分析仍然具有挑战性。传统数据分析技术难以准确解释、分类和量化反馈，从而限制了其在产品开发和支持策略中的潜在用途。

Method: 本文提出了User eXperience Perception Insights Dataset (UXPID)，这是一个包含7130个人工合成和匿名化用户反馈分支的数据集，这些反馈来自一个公开的工业自动化论坛。每个JSON记录包含与特定硬件和软件产品相关的多帖子评论，并带有元数据和上下文对话数据。利用大型语言模型（LLM），每个分支被系统地分析和标注，以获取UX见解、用户期望、严重性和情感评分以及主题分类。

Result: UXPID数据集被设计为促进用户需求、用户体验（UX）分析和AI驱动的反馈处理研究，特别是在隐私和许可限制限制对真实世界数据的访问时。UXPID支持基于变压器的模型的训练和评估，用于技术论坛中的问题检测、情感分析和需求提取任务。

Conclusion: UXPID数据集旨在促进用户需求、用户体验（UX）分析和AI驱动的反馈处理研究，特别是在隐私和许可限制限制对真实世界数据的访问时。UXPID支持基于变压器的模型的训练和评估，用于技术论坛中的问题检测、情感分析和需求提取任务。

Abstract: Customer feedback in industrial forums reflect a rich but underexplored
source of insight into real-world product experience. These publicly shared
discussions offer an organic view of user expectations, frustrations, and
success stories shaped by the specific contexts of use. Yet, harnessing this
information for systematic analysis remains challenging due to the unstructured
and domain-specific nature of the content. The lack of structure and
specialized vocabulary makes it difficult for traditional data analysis
techniques to accurately interpret, categorize, and quantify the feedback,
thereby limiting its potential to inform product development and support
strategies. To address these challenges, this paper presents the User
eXperience Perception Insights Dataset (UXPID), a collection of 7130
artificially synthesized and anonymized user feedback branches extracted from a
public industrial automation forum. Each JavaScript object notation (JSON)
record contains multi-post comments related to specific hardware and software
products, enriched with metadata and contextual conversation data. Leveraging a
large language model (LLM), each branch is systematically analyzed and
annotated for UX insights, user expectations, severity and sentiment ratings,
and topic classifications. The UXPID dataset is designed to facilitate research
in user requirements, user experience (UX) analysis, and AI-driven feedback
processing, particularly where privacy and licensing restrictions limit access
to real-world data. UXPID supports the training and evaluation of
transformer-based models for tasks such as issue detection, sentiment analysis,
and requirements extraction in the context of technical forums.

</details>


### [65] [When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries](https://arxiv.org/abs/2509.11802)
*Dvora Goncharok,Arbel Shifman,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本研究创建了一个新的药物相关问题数据集，并评估了传统和基于大型语言模型的分类方法，以检测可能预示健康危机的关键问题。


<details>
  <summary>Details</summary>
Motivation: 在线医疗论坛是了解患者担忧的重要来源，特别是关于药物使用的问题。检测可能预示严重不良事件或危及生命并发症的关键问题对于及时干预和改善患者安全至关重要。

Method: 本研究引入了一个新的标注数据集，其中包含从在线论坛中提取的与药物相关的问题，并使用TF-IDF文本表示对六种传统机器学习分类器进行基准测试，同时评估了三种基于大型语言模型（LLM）的分类方法。

Result: 研究结果表明，传统方法和现代方法在支持实时分诊和警报系统方面具有潜力。

Conclusion: 本研究展示了传统方法和现代方法在支持数字健康空间中的实时分诊和警报系统方面的潜力，并公开了数据集以促进进一步的研究。

Abstract: Online medical forums are a rich and underutilized source of insight into
patient concerns, especially regarding medication use. Some of the many
questions users pose may signal confusion, misuse, or even the early warning
signs of a developing health crisis. Detecting these critical questions that
may precede severe adverse events or life-threatening complications is vital
for timely intervention and improving patient safety. This study introduces a
novel annotated dataset of medication-related questions extracted from online
forums. Each entry is manually labelled for criticality based on clinical risk
factors. We benchmark the performance of six traditional machine learning
classifiers using TF-IDF textual representations, alongside three
state-of-the-art large language model (LLM)-based classification approaches
that leverage deep contextual understanding. Our results highlight the
potential of classical and modern methods to support real-time triage and alert
systems in digital health spaces. The curated dataset is made publicly
available to encourage further research at the intersection of
patient-generated data, natural language processing, and early warning systems
for critical health events. The dataset and benchmark are available at:
https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.

</details>


### [66] [From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives](https://arxiv.org/abs/2509.11803)
*Eden Mama,Liel Sheri,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本文提出了一种新的合成数据集，用于评估大型语言模型在处理患者生成的非正式、模糊和嘈杂文本时的诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试通常依赖于干净、结构化的临床文本，无法提供在现实条件下模型性能的深入见解。因此，需要一种新的数据集来评估大型语言模型在处理患者生成的非正式、模糊和嘈杂文本时的表现。

Method: 本文创建了一个合成数据集，模拟患者自我描述中的语言噪声、模糊语言和非专业术语，并对多个最先进的模型进行了微调和评估。

Result: 本文创建了一个包含临床一致场景并带有真实诊断标注的合成数据集，并对多个最先进的模型进行了评估。

Conclusion: 本文提出了一个名为NDB的合成数据集，用于评估大型语言模型在现实语言条件下的诊断能力，并为未来的研究提供了可重复使用的基准。

Abstract: The widespread adoption of large language models (LLMs) in healthcare raises
critical questions about their ability to interpret patient-generated
narratives, which are often informal, ambiguous, and noisy. Existing benchmarks
typically rely on clean, structured clinical text, offering limited insight
into model performance under realistic conditions. In this work, we present a
novel synthetic dataset designed to simulate patient self-descriptions
characterized by varying levels of linguistic noise, fuzzy language, and
layperson terminology. Our dataset comprises clinically consistent scenarios
annotated with ground-truth diagnoses, spanning a spectrum of communication
clarity to reflect diverse real-world reporting styles. Using this benchmark,
we fine-tune and evaluate several state-of-the-art models (LLMs), including
BERT-based and encoder-decoder T5 models. To support reproducibility and future
research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset
of noisy, synthetic patient descriptions designed to stress-test and compare
the diagnostic capabilities of large language models (LLMs) under realistic
linguistic conditions. We made the benchmark available for the community:
https://github.com/lielsheri/PatientSignal

</details>


### [67] [PledgeTracker: A System for Monitoring the Fulfilment of Pledges](https://arxiv.org/abs/2509.11804)
*Yulong Chen,Michael Sejr Schlichtkrull,Zhenyun Deng,David Corney,Nasim Asl,Joshua Salisbury,Andrew Dudfield,Andreas Vlachos*

Main category: cs.CL

TL;DR: PledgeTracker is a system that tracks political pledges by constructing structured event timelines, effectively retrieving evidence and reducing verification efforts.


<details>
  <summary>Details</summary>
Motivation: Existing methods simplify pledge tracking into a document classification task, overlooking its dynamic, temporal, and multi-document nature.

Method: PledgeTracker reformulates pledge verification into structured event timeline construction, consisting of three core components: a multi-step evidence retrieval module, a timeline construction module, and a fulfilment filtering module.

Result: PledgeTracker was evaluated in collaboration with professional fact-checkers in real-world workflows, demonstrating its effectiveness in retrieving relevant evidence and reducing human verification effort.

Conclusion: PledgeTracker is effective in retrieving relevant evidence and reducing human verification effort.

Abstract: Political pledges reflect candidates' policy commitments, but tracking their
fulfilment requires reasoning over incremental evidence distributed across
multiple, dynamically updated sources. Existing methods simplify this task into
a document classification task, overlooking its dynamic, temporal and
multi-document nature. To address this issue, we introduce
\textsc{PledgeTracker}, a system that reformulates pledge verification into
structured event timeline construction. PledgeTracker consists of three core
components: (1) a multi-step evidence retrieval module; (2) a timeline
construction module and; (3) a fulfilment filtering module, allowing the
capture of the evolving nature of pledge fulfilment and producing interpretable
and structured timelines. We evaluate PledgeTracker in collaboration with
professional fact-checkers in real-world workflows, demonstrating its
effectiveness in retrieving relevant evidence and reducing human verification
effort.

</details>


### [68] [SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection](https://arxiv.org/abs/2509.11818)
*Taichi Aida,Danushka Bollegala*

Main category: cs.CL

TL;DR: SCDTour is a method that orders and merges interpretable axes to improve the balance between interpretability and performance in Semantic Change Detection (SCD).


<details>
  <summary>Details</summary>
Motivation: In Semantic Change Detection (SCD), it is a common problem to obtain embeddings that are both interpretable and high-performing. However, improving interpretability often leads to a loss in the SCD performance, and vice versa.

Method: SCDTour is a method that orders and merges interpretable axes to alleviate the performance degradation of SCD. It considers both semantic similarity between axes in the embedding space and the degree to which each axis contributes to semantic change.

Result: Experimental results show that SCDTour preserves performance in semantic change detection while maintaining high interpretability. Moreover, agglomerating the sorted axes produces a more refined set of word senses, which achieves comparable or improved performance against the original full-dimensional embeddings in the SCD task.

Conclusion: SCDTour effectively balances interpretability and SCD performance, enabling meaningful interpretation of semantic shifts through a small number of refined axes.

Abstract: In Semantic Change Detection (SCD), it is a common problem to obtain
embeddings that are both interpretable and high-performing. However, improving
interpretability often leads to a loss in the SCD performance, and vice versa.
To address this problem, we propose SCDTour, a method that orders and merges
interpretable axes to alleviate the performance degradation of SCD. SCDTour
considers both (a) semantic similarity between axes in the embedding space, as
well as (b) the degree to which each axis contributes to semantic change.
Experimental results show that SCDTour preserves performance in semantic change
detection while maintaining high interpretability. Moreover, agglomerating the
sorted axes produces a more refined set of word senses, which achieves
comparable or improved performance against the original full-dimensional
embeddings in the SCD task. These findings demonstrate that SCDTour effectively
balances interpretability and SCD performance, enabling meaningful
interpretation of semantic shifts through a small number of refined axes.
Source code is available at https://github.com/LivNLP/svp-tour .

</details>


### [69] [MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues](https://arxiv.org/abs/2509.11860)
*Weishu Chen,Jinyi Tang,Zhouhui Hou,Shihao Han,Mingjie Zhan,Zhiyuan Huang,Delong Liu,Jiawei Guo,Zhicheng Zhao,Fei Su*

Main category: cs.CL

TL;DR: MOOM是一种基于文学理论的双分支记忆插件，能够有效控制记忆增长并在超长对话中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法常出现不受控的记忆增长问题，因此需要一种有效的记忆提取方法来维持人类-机器人角色扮演场景中的连贯性。

Method: MOOM是一种双分支记忆插件，通过文学理论建模情节发展和角色刻画作为核心叙事元素，并引入受“竞争-抑制”记忆理论启发的遗忘机制。

Result: 实验结果表明，MOOM优于所有最先进的记忆提取方法，在保持可控记忆容量的同时减少了大型语言模型调用次数。

Conclusion: MOOM在超长对话中表现出色，能够有效控制记忆容量并减少大型语言模型调用次数。

Abstract: Memory extraction is crucial for maintaining coherent ultra-long dialogues in
human-robot role-playing scenarios. However, existing methods often exhibit
uncontrolled memory growth. To address this, we propose MOOM, the first
dual-branch memory plugin that leverages literary theory by modeling plot
development and character portrayal as core storytelling elements.
Specifically, one branch summarizes plot conflicts across multiple time scales,
while the other extracts the user's character profile. MOOM further integrates
a forgetting mechanism, inspired by the ``competition-inhibition'' memory
theory, to constrain memory capacity and mitigate uncontrolled growth.
Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset
specifically designed for role-playing, featuring dialogues that average 600
turns and include manually annotated memory information. Experimental results
demonstrate that MOOM outperforms all state-of-the-art memory extraction
methods, requiring fewer large language model invocations while maintaining a
controllable memory capacity.

</details>


### [70] [Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models](https://arxiv.org/abs/2509.11868)
*Sabrina Patania,Luca Annese,Anna Lambiase,Anita Pellegrini,Tom Foulsham,Azzurra Ruggeri,Silvia Rossi,Silvia Serino,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 本研究探讨了PerspAct系统，该系统结合ReAct范式与LLMs来模拟视角采取的发展阶段。结果表明，GPT能生成符合发展阶段的叙事，但在互动中可能转向更高级阶段，这表明语言交流有助于完善内部表征。较高发展阶段通常增强协作效果，而早期阶段在复杂情境中产生更多可变结果。


<details>
  <summary>Details</summary>
Motivation: 语言和具身视角采取对于人类协作至关重要，但很少有计算模型同时处理两者。

Method: 本研究调查了PerspAct系统，该系统将ReAct（Reason and Act）范式与大型语言模型（LLMs）结合，以模拟Selman理论中的视角采取发展阶段。通过扩展的导演任务，评估GPT生成与指定发展阶段一致的内部叙事的能力，并评估这些叙事如何影响协作表现。

Result: 结果表明，GPT在任务执行前能够可靠地生成符合发展阶段的叙事，但在互动过程中往往转向更高级的阶段，这表明语言交流有助于完善内部表征。较高的发展阶段通常会增强协作效果，而早期阶段在复杂情境中会产生更多可变的结果。

Conclusion: 这些发现表明，将具身视角采取和语言整合到LLMs中可以更好地建模发展动态，并强调了在结合语言和具身任务中评估内部言语的重要性。

Abstract: Language and embodied perspective taking are essential for human
collaboration, yet few computational models address both simultaneously. This
work investigates the PerspAct system [1], which integrates the ReAct (Reason
and Act) paradigm with Large Language Models (LLMs) to simulate developmental
stages of perspective taking, grounded in Selman's theory [2]. Using an
extended director task, we evaluate GPT's ability to generate internal
narratives aligned with specified developmental stages, and assess how these
influence collaborative performance both qualitatively (action selection) and
quantitatively (task efficiency). Results show that GPT reliably produces
developmentally-consistent narratives before task execution but often shifts
towards more advanced stages during interaction, suggesting that language
exchanges help refine internal representations. Higher developmental stages
generally enhance collaborative effectiveness, while earlier stages yield more
variable outcomes in complex contexts. These findings highlight the potential
of integrating embodied perspective taking and language in LLMs to better model
developmental dynamics and stress the importance of evaluating internal speech
during combined linguistic and embodied tasks.

</details>


### [71] [Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible](https://arxiv.org/abs/2509.11915)
*Aadil Gani Ganie*

Main category: cs.CL

TL;DR: 本文通过类比量子不确定性，探讨了AI文本检测的理论和实践限制，并指出其背后的根本性矛盾。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）变得越来越先进，区分人类写作和AI生成的文本变得越来越困难。本文旨在探讨AI文本检测的理论和实践限制，并提出其背后的根本性矛盾。

Method: 本文通过将量子不确定性与自然语言中的作者识别限制进行概念类比，探讨了当前检测方法（如风格分析、水印和神经分类器）的固有局限性。

Result: 本文分析表明，当AI生成的文本非常接近人类写作时，完美的检测不仅是技术上困难的，而且在理论上是不可能的。

Conclusion: 本文认为，AI文本检测的挑战不仅是一个更好的工具问题，它反映了语言本身的更深层次、不可避免的紧张关系。

Abstract: As large language models (LLMs) become more advanced, it is increasingly
difficult to distinguish between human-written and AI-generated text. This
paper draws a conceptual parallel between quantum uncertainty and the limits of
authorship detection in natural language. We argue that there is a fundamental
trade-off: the more confidently one tries to identify whether a text was
written by a human or an AI, the more one risks disrupting the text's natural
flow and authenticity. This mirrors the tension between precision and
disturbance found in quantum systems. We explore how current detection
methods--such as stylometry, watermarking, and neural classifiers--face
inherent limitations. Enhancing detection accuracy often leads to changes in
the AI's output, making other features less reliable. In effect, the very act
of trying to detect AI authorship introduces uncertainty elsewhere in the text.
Our analysis shows that when AI-generated text closely mimics human writing,
perfect detection becomes not just technologically difficult but theoretically
impossible. We address counterarguments and discuss the broader implications
for authorship, ethics, and policy. Ultimately, we suggest that the challenge
of AI-text detection is not just a matter of better tools--it reflects a
deeper, unavoidable tension in the nature of language itself.

</details>


### [72] [Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation](https://arxiv.org/abs/2509.11921)
*Helene Tenzer,Oumnia Abidi,Stefan Feuerriegel*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在处理英语-日语工作邮件翻译时的文化敏感性，并发现通过定制化的提示可以提高文化契合度，从而提出了在多语言环境中设计文化包容性LLM的建议。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型现在可以生成近乎完美的字面翻译，但尚不清楚它们是否支持文化适当的交流。本文旨在分析不同LLM设计在处理英语-日语工作邮件翻译时的文化敏感性，并探索如何通过提示策略改进文化契合度。

Method: 我们分析了不同LLM设计在处理英语-日语工作邮件翻译时的文化敏感性，通过改变提示策略（1）简单的“直接翻译”提示，（2）针对受众的提示，指定收件人的文化背景，以及（3）带有明确指导的日语沟通规范的指令提示。然后，我们使用混合方法研究分析了文化特定的语言模式，以评估翻译如何适应文化规范，并进一步检查了母语者对翻译语气的适当性的看法。

Result: 我们发现，通过定制化的提示可以提高文化契合度，这表明在多语言环境中设计文化包容性LLM是有潜力的。

Conclusion: 我们发现定制化的提示可以提高文化契合度，并据此提出了在多语言环境中设计文化包容性LLM的建议。

Abstract: Large language models (LLMs) are increasingly used in everyday communication,
including multilingual interactions across different cultural contexts. While
LLMs can now generate near-perfect literal translations, it remains unclear
whether LLMs support culturally appropriate communication. In this paper, we
analyze the cultural sensitivity of different LLM designs when applied to
English-Japanese translations of workplace e-mails. Here, we vary the prompting
strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts
specifying the recipient's cultural background, and (3) instructional prompts
with explicit guidance on Japanese communication norms. Using a mixed-methods
study, we then analyze culture-specific language patterns to evaluate how well
translations adapt to cultural norms. Further, we examine the appropriateness
of the tone of the translations as perceived by native speakers. We find that
culturally-tailored prompting can improve cultural fit, based on which we offer
recommendations for designing culturally inclusive LLMs in multilingual
settings.

</details>


### [73] [Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding](https://arxiv.org/abs/2509.11961)
*Mingxiao Huo,Jiayi Zhang,Hewei Wang,Jinfeng Xu,Zheyu Chen,Huilin Tai,Yijun Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Spec-LLaVA的系统，通过动态树结构的推测解码加速视觉语言模型，实现了无损加速，提高了实时应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）虽然具备强大的多模态推理能力，但其自回归推理速度较慢，限制了在实时应用中的部署。因此，需要一种方法来加速VLMs的推理过程，同时保持输出质量。

Method: 本文引入了Spec-LLaVA系统，采用推测解码来加速视觉语言模型（VLMs），同时不牺牲输出质量。Spec-LLaVA将一个轻量级草案VLM与一个大型目标模型配对：草案预测未来标记，目标模型并行验证，从而每一步生成多个标记。为了最大化效率，设计了一个基于动态树的验证算法，利用草案模型的置信度自适应地扩展和修剪推测分支。

Result: 在MS COCO域外图像上，Spec-LLaVA在LLaVA-1.5（7B, 13B）上实现了高达3.28倍的解码速度提升，且没有损失生成质量。

Conclusion: 本文提出了一种无损加速框架，通过动态树结构的推测解码实现视觉语言模型的高效推理，为实际的实时多模态助手开辟了道路。轻量级草案模型的设计使得该框架适用于资源受限或设备端的部署环境。

Abstract: Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer
from slow autoregressive inference, limiting their deployment in real-time
applications. We introduce Spec-LLaVA, a system that applies speculative
decoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA
pairs a lightweight draft VLM with a large target model: the draft speculates
future tokens, which the target verifies in parallel, allowing multiple tokens
to be generated per step. To maximize efficiency, we design a dynamic
tree-based verification algorithm that adaptively expands and prunes
speculative branches using draft model confidence. On MS COCO out-of-domain
images, Spec-LLaVA achieves up to 3.28$\times$ faster decoding on LLaVA-1.5
(7B, 13B) with no loss in generation quality. This work presents a lossless
acceleration framework for VLMs using dynamic tree-structured speculative
decoding, opening a path toward practical real-time multimodal assistants.
Importantly, the lightweight draft model design makes the framework amenable to
resource-constrained or on-device deployment settings.

</details>


### [74] [ToolRM: Outcome Reward Models for Tool-Calling Large Language Models](https://arxiv.org/abs/2509.11963)
*Mayank Agarwal,Ibrahim Abdelaziz,Kinjal Basu,Merve Unuvar,Luis A. Lastras,Yara Rizk,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: 本文介绍了FC-RewardBench基准，用于评估奖励模型在工具调用场景下的性能，并提出了一种基于结果的奖励模型训练框架，通过使用开源大语言模型合成的数据进行训练，取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型主要基于自然语言输出进行训练，难以评估基于工具的推理和执行。因此，需要一种新的方法来评估和改进奖励模型在工具使用场景中的表现。

Method: 引入了FC-RewardBench基准来系统评估奖励模型在工具调用场景下的性能，并提出了一种基于结果的奖励模型训练框架，利用从允许许可的开源大语言模型中合成的数据进行训练。

Result: 所提出的模型在七个跨领域的基准测试中表现优于通用基线模型，平均提升了25%的下游任务性能，并通过奖励引导的过滤实现了数据高效的微调。

Conclusion: 当前奖励模型在评估工具使用效果方面存在不足，需要进行领域特定建模。通过使用从允许许可的开源大语言模型中合成的数据，我们提出了一个基于结果的奖励模型训练框架，并取得了显著的改进。

Abstract: As large language models (LLMs) increasingly interact with external tools,
reward modeling for tool use has become a critical yet underexplored area.
Existing reward models, trained primarily on natural language outputs, struggle
to evaluate tool-based reasoning and execution. To quantify this gap, we
introduce FC-RewardBench, the first benchmark designed to systematically assess
reward models' performance in tool-calling scenarios. Our analysis shows that
current reward models often miss key signals of effective tool use,
highlighting the need for domain-specific modeling. To address this, we propose
a training framework for outcome-based reward models using data synthesized
from permissively licensed, open-weight LLMs. We train models ranging from 1.7B
to 14B parameters and evaluate them across seven out-of-domain benchmarks.
These models consistently outperform general-purpose baselines, achieving up to
25\% average improvement in downstream task performance and enabling
data-efficient fine-tuning through reward-guided filtering.

</details>


### [75] [Query-Focused Extractive Summarization for Sentiment Explanation](https://arxiv.org/abs/2509.11989)
*Ahmed Moubtahij,Sylvie Ratté,Yazid Attabi,Maxime Dumas*

Main category: cs.CL

TL;DR: 本文提出了一种多偏见框架，用于改善查询聚焦摘要任务中的情感解释问题，并在实际数据集上取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 为了帮助和提高从大量文本文档中确定客户情绪原因的效率，我们利用了查询聚焦摘要（QFS）任务。

Method: 我们提出了一个多功能的框架，以在领域无关的通用层面上弥合这一差距，并为情感解释问题制定了专门的方法。

Result: 我们在现实世界的专业情感感知QFS数据集上取得了优于基线模型的实验结果。

Conclusion: 我们提出了一个多功能的框架，以在领域无关的通用层面上弥合这一差距，并为情感解释问题制定了专门的方法。我们的实验结果在现实世界的专业情感感知QFS数据集上优于基线模型。

Abstract: Constructive analysis of feedback from clients often requires determining the
cause of their sentiment from a substantial amount of text documents. To assist
and improve the productivity of such endeavors, we leverage the task of
Query-Focused Summarization (QFS). Models of this task are often impeded by the
linguistic dissonance between the query and the source documents. We propose
and substantiate a multi-bias framework to help bridge this gap at a
domain-agnostic, generic level; we then formulate specialized approaches for
the problem of sentiment explanation through sentiment-based biases and query
expansion. We achieve experimental results outperforming baseline models on a
real-world proprietary sentiment-aware QFS dataset.

</details>


### [76] [Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles](https://arxiv.org/abs/2509.11991)
*Jesús Calleja,David Ponce,Thierry Etchegoyhen*

Main category: cs.CL

TL;DR: 我们参与了CLEARS挑战赛，采用自动后编辑方法，在简单语言和易读语言适应中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 参与CLEARS挑战赛，旨在将文本适应为西班牙语的简单语言和易读语言。

Method: 我们采用了自动后编辑不同类型的初始大型语言模型适应的方法，逐步生成适应，直到可读性和相似性指标表明无法进一步改进适应。

Result: 我们的提交在所有官方指标的平均值上分别获得了第一名和第二名。

Conclusion: 我们的方法在Plain language和Easy Read适应中分别获得了第一名和第二名。

Abstract: We describe Vicomtech's participation in the CLEARS challenge on text
adaptation to Plain Language and Easy Read in Spanish. Our approach features
automatic post-editing of different types of initial Large Language Model
adaptations, where successive adaptations are generated iteratively until
readability and similarity metrics indicate that no further adaptation
refinement can be successfully performed. Taking the average of all official
metrics, our submissions achieved first and second place in Plain language and
Easy Read adaptation, respectively.

</details>


### [77] [Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect](https://arxiv.org/abs/2509.12065)
*Alina Klerings,Jannik Brinkmann,Daniel Ruffinelli,Simone Ponzetto*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型如何在残差空间中表示和控制动词时态和体，并通过概念引导在生成任务中实现了因果控制。研究发现，引导强度、位置和持续时间是减少副作用的关键参数。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究主要集中在二元语法对比上，但本文研究了两个多维层次语法现象的表示和控制，即动词时态和体，并试图通过线性判别分析找到残差空间中的不同方向。

Method: 我们使用线性判别分析在残差空间中识别了两种多维层次语法现象（动词时态和体）的不同且正交的方向，并通过概念引导在三个生成任务中展示了对这两种语法特征的因果控制。此外，我们还利用这些特征进行案例研究，以调查影响多标记生成中有效引导的因素。

Result: 我们发现引导强度、位置和持续时间是减少不需要的副作用（如主题转移和退化）的关键参数。

Conclusion: 我们的研究结果表明，模型以结构化的方式编码时态和体，但在生成过程中有效控制这些特征对多个因素敏感，需要手动调整或自动优化。

Abstract: Large language models (LLMs) are able to generate grammatically well-formed
text, but how do they encode their syntactic knowledge internally? While prior
work has focused largely on binary grammatical contrasts, in this work, we
study the representation and control of two multidimensional hierarchical
grammar phenomena - verb tense and aspect - and for each, identify distinct,
orthogonal directions in residual space using linear discriminant analysis.
Next, we demonstrate causal control over both grammatical features through
concept steering across three generation tasks. Then, we use these identified
features in a case study to investigate factors influencing effective steering
in multi-token generation. We find that steering strength, location, and
duration are crucial parameters for reducing undesirable side effects such as
topic shift and degeneration. Our findings suggest that models encode tense and
aspect in structurally organized, human-like ways, but effective control of
such features during generation is sensitive to multiple factors and requires
manual tuning or automated optimization.

</details>


### [78] [SENSE models: an open source solution for multilingual and multimodal semantic-based tasks](https://arxiv.org/abs/2509.12093)
*Salima Mdhaffar,Haroun Elleuch,Chaimae Chellaf,Ha Nguyen,Yannick Estève*

Main category: cs.CL

TL;DR: 本文介绍了SENSE，一种基于教师-学生框架的多语言语音和文本共享嵌入解决方案，通过改进的教师文本模型和初始语音编码器实现了高性能的多语言和多模态语义任务。


<details>
  <summary>Details</summary>
Motivation: 本文旨在改进原始的SAMU-XLSR方法，通过选择更强的教师文本模型和更好的初始语音编码器。

Method: 本文引入了SENSE（共享嵌入用于多语言语音和文本），这是一种受SAMU-XLSR框架启发并类似于Meta AI的SONAR模型的开源解决方案。这些方法依赖于教师-学生框架，以在话语级别将自监督语音编码器与文本编码器的语言无关连续表示对齐。

Result: 我们报告了在多语言和多模态语义任务上的实验结果，其中我们的SENSE模型表现出高度有竞争力的性能。

Conclusion: 本研究提供了关于如何在语义对齐的语音编码器中捕捉语义的新见解。

Abstract: This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),
an open-source solution inspired by the SAMU-XLSR framework and conceptually
similar to Meta AI's SONAR models. These approaches rely on a teacher-student
framework to align a self-supervised speech encoder with the language-agnostic
continuous representations of a text encoder at the utterance level. We
describe how the original SAMU-XLSR method has been updated by selecting a
stronger teacher text model and a better initial speech encoder. The source
code for training and using SENSE models has been integrated into the
SpeechBrain toolkit, and the first SENSE model we trained has been publicly
released. We report experimental results on multilingual and multimodal
semantic tasks, where our SENSE model achieves highly competitive performance.
Finally, this study offers new insights into how semantics are captured in such
semantically aligned speech encoders.

</details>


### [79] [Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities](https://arxiv.org/abs/2509.12098)
*Payam Latifi*

Main category: cs.CL

TL;DR: 本研究比较了LLMs和传统NLP工具在NER任务上的表现，发现LLMs在上下文敏感实体识别上表现更好，但传统工具在结构化标签任务中更具稳定性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在评估不同系统在NER任务上的表现，特别是比较LLMs与传统NLP工具之间的差异。

Method: 本研究通过对比三种非LLM NLP工具（NLTK、spaCy、Stanza）和三种通用大语言模型（LLMs：Gemini-1.5-flash、DeepSeek-V3、Qwen-3-4B）在命名实体识别（NER）上的表现，评估了它们的性能。

Result: LLMs通常在识别上下文敏感的实体（如人名）方面优于传统工具，其中Gemini取得了最高的平均F1分数。然而，传统系统如Stanza在结构化标签（如LOCATION和DATE）方面表现出更高的稳定性。此外，LLMs在处理时间表达式和多词组织时存在较大差异。

Conclusion: 研究结果表明，虽然LLMs在上下文理解方面表现出色，但在特定任务上传统工具仍然具有竞争力，这为模型选择提供了参考。

Abstract: This pilot study presents a small-scale but carefully annotated benchmark of
Named Entity Recognition (NER) performance across six systems: three non-LLM
NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models
(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119
tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).
We evaluated each system's output against the manually annotated gold standard
dataset using F1-score. The results show that LLMs generally outperform
conventional tools in recognizing context-sensitive entities like person names,
with Gemini achieving the highest average F1-score. However, traditional
systems like Stanza demonstrate greater consistency in structured tags such as
LOCATION and DATE. We also observed variability among LLMs, particularly in
handling temporal expressions and multi-word organizations. Our findings
highlight that while LLMs offer improved contextual understanding, traditional
tools remain competitive in specific tasks, informing model selection.

</details>


### [80] [In-domain SSL pre-training and streaming ASR](https://arxiv.org/abs/2509.12101)
*Jarod Duret,Salima Mdhaffar,Gaëlle Laperrière,Ryan Whetten,Audrey Galametz,Catherine Kobus,Marion-Cécile Martin,Jo Oleiwan,Yannick Estève*

Main category: cs.CL

TL;DR: 本研究探讨了在航空交通控制环境中使用领域特定的自监督预训练对离线和流式自动语音识别的好处，并展示了其在提高准确性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨领域特定的自监督预训练在离线和流式自动语音识别（ASR）中的优势，特别是在航空交通控制（ATC）环境中。

Method: 我们使用4.5k小时的未标记ATC数据训练BEST-RQ模型，然后在较小的监督ATC集上进行微调。为了实现实时处理，我们提出使用分块注意力和动态卷积，确保低延迟推理。

Result: 结果表明，领域适应的预训练显著提高了标准ATC基准的表现，与在广泛语音语料库上训练的模型相比，显著降低了词错误率。此外，提出的流式方法在更严格的延迟约束下进一步提高了词错误率。

Conclusion: 这些发现表明，为ATC数据专门化SSL表示是实现更准确和高效的ASR系统的一种实用途径。

Abstract: In this study, we investigate the benefits of domain-specific self-supervised
pre-training for both offline and streaming ASR in Air Traffic Control (ATC)
environments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then
fine-tune on a smaller supervised ATC set. To enable real-time processing, we
propose using chunked attention and dynamic convolutions, ensuring low-latency
inference. We compare these in-domain SSL models against state-of-the-art,
general-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show
that domain-adapted pre-training substantially improves performance on standard
ATC benchmarks, significantly reducing word error rates when compared to models
trained on broad speech corpora. Furthermore, the proposed streaming approach
further improves word error rate under tighter latency constraints, making it
particularly suitable for safety-critical aviation applications. These findings
highlight that specializing SSL representations for ATC data is a practical
path toward more accurate and efficient ASR systems in real-world operational
settings.

</details>


### [81] [GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models](https://arxiv.org/abs/2509.12108)
*Min Zeng,Jinfei Sun,Xueyou Luo,Caiquan Liu,Shiqi Zhang,Li Xie,Xiaoxin Chen*

Main category: cs.CL

TL;DR: GTA框架结合了SFT的效率和RL的能力，通过初步猜测、反思和最终答案的生成过程，实现了更快的收敛和更高的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决效率与能力之间的权衡问题，提出了GTA框架，以结合SFT的效率和RL的能力。

Method: GTA框架结合了SFT的效率和RL的能力，在统一的训练范式中实现。模型首先生成一个初步猜测，然后反思这个猜测并生成最终答案，RL奖励同时塑造最终输出和整个GTA结构的格式。

Result: GTA框架在四个文本分类基准上取得了显著的成果，加速了收敛并优于单独的SFT和RL基线。

Conclusion: GTA框架在四个文本分类基准上表现出色，显著加速了收敛并优于单独的SFT和RL基线。

Abstract: In natural language processing tasks, pure reinforcement learning (RL)
fine-tuning methods often suffer from inefficient exploration and slow
convergence; while supervised fine-tuning (SFT) methods, although efficient in
training, have limited performance ceiling and less solid theoretical
foundation compared to RL. To address efficiency-capability trade-off, we
propose the Guess-Think-Answer (GTA) framework that combines the efficiency of
SFT with the capability gains of RL in a unified training paradigm. GTA works
by having the model first produce a provisional guess (optimized via
cross-entropy loss), then reflect on this guess before generating the final
answer, with RL rewards shaping both the final output and the format of the
entire GTA structure. This hybrid approach achieves both faster convergence
than pure RL and higher performance ceiling than pure SFT. To mitigate gradient
conflicts between the two training signals, we employ loss masking and gradient
constraints. Empirical results on four text classification benchmarks
demonstrate that GTA substantially accelerates convergence while outperforming
both standalone SFT and RL baselines.

</details>


### [82] [CBP-Tuning: Efficient Local Customization for Black-box Large Language Models](https://arxiv.org/abs/2509.12112)
*Jiaxuan Zhao,Naibin Gu,Yuchen Feng,Xiyu Liu,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CBP-Tuning的新框架，用于高效本地定制大型语言模型（LLM），同时保持双向隐私。该方法通过两阶段框架实现，无需用户访问模型权重或上传私人数据，只需每个任务的一个定制向量即可实现有效适应。


<details>
  <summary>Details</summary>
Motivation: LLM的高定制成本从根本上限制了它们适应用户特定需求的能力。因此，LLM越来越多地作为基于云的服务提供，这一范式引入了关键限制：提供商难以在大规模上支持个性化定制，而用户在暴露敏感数据时面临隐私风险。

Method: 我们设计了一个两阶段框架：(1) 在服务器端训练一个提示生成器以捕捉特定领域和任务无关的能力，以及(2) 用户端的无梯度优化，为单个任务定制软提示。

Result: CBP-Tuning在常识推理、医疗和金融领域设置中的评估展示了优于基线的性能，展示了其在任务无关处理和隐私保护方面的优势。

Conclusion: CBP-Tuning在常识推理、医疗和金融领域设置中的评估展示了优于基线的性能，展示了其在任务无关处理和隐私保护方面的优势。

Abstract: The high costs of customizing large language models (LLMs) fundamentally
limit their adaptability to user-specific needs. Consequently, LLMs are
increasingly offered as cloud-based services, a paradigm that introduces
critical limitations: providers struggle to support personalized customization
at scale, while users face privacy risks when exposing sensitive data. To
address this dual challenge, we propose Customized Black-box Prompt Tuning
(CBP-Tuning), a novel framework that facilitates efficient local customization
while preserving bidirectional privacy. Specifically, we design a two-stage
framework: (1) a prompt generator trained on the server-side to capture
domain-specific and task-agnostic capabilities, and (2) user-side gradient-free
optimization that tailors soft prompts for individual tasks. This approach
eliminates the need for users to access model weights or upload private data,
requiring only a single customized vector per task while achieving effective
adaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense
reasoning, medical and financial domain settings demonstrates superior
performance compared to baselines, showcasing its advantages in task-agnostic
processing and privacy preservation.

</details>


### [83] [XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models](https://arxiv.org/abs/2509.12130)
*Ariana Sahitaj,Jiaao Li,Pia Wenzel Neves,Fedor Splitt,Premtim Sahitaj,Charlott Jakob,Veronika Solopova,Vera Schmitt*

Main category: cs.CL

TL;DR: XplaiNLP在CheckThat! 2025多语言主观性检测任务中采用微调和零样本提示方法，取得良好成绩，但某些语言表现略低于基线。


<details>
  <summary>Details</summary>
Motivation: 旨在提高多语言主观性检测的性能，并探索不同方法在不同语言任务中的效果。

Method: 评估了两种方法：(1) 在单语和机器翻译训练数据上对Transformer编码器进行监督微调；(2) 使用两种大语言模型进行零样本提示。

Result: 在意大利语单语子任务中获得第一名，F_1得分为0.8104；在罗马尼亚零样本设置中，微调的XLM-RoBERTa模型获得第三名，F_1得分为0.7917；在希腊语中也超过了基线。

Conclusion: 该论文报告了XplaiNLP在CheckThat! 2025共享任务上的多语言主观性检测提交。通过两种方法取得了良好的成绩，但在某些低资源跨语言场景中表现略低于基线。

Abstract: This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared
task on multilingual subjectivity detection. We evaluate two approaches: (1)
supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and
German-BERT, on monolingual and machine-translated training data; and (2)
zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based
labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and
Perspective (comparative reasoning). The Annotation Approach achieves 1st place
in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming
the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned
XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the
baseline of 0.6461. The same model also performs reliably in the multilingual
task and improves over the baseline in Greek. For German, a German-BERT model
fine-tuned on translated training data from typologically related languages
yields competitive performance over the baseline. In contrast, performance in
the Ukrainian and Polish zero-shot settings falls slightly below the respective
baselines, reflecting the challenge of generalization in low-resource
cross-lingual scenarios.

</details>


### [84] [Pun Unintended: LLMs and the Illusion of Humor Understanding](https://arxiv.org/abs/2509.12158)
*Alessandro Zangari,Matteo Marcuzzo,Andrea Albarelli,Mohammad Taher Pilehvar,Jose Camacho-Collados*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在检测双关语方面的局限性，并提出了改进的双关语检测基准。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示大型语言模型在处理双关语时的局限性，并提供更全面和细致的双关语检测基准。

Method: 通过系统分析和重新制定现有的双关语基准，我们展示了双关语中的细微变化足以误导大型语言模型。

Result: 本文提供了全面且细致的双关语检测基准，进行了跨最新大型语言模型的人类评估，并分析了这些模型在处理双关语时面临的鲁棒性挑战。

Conclusion: 本文表明，尽管大型语言模型在检测双关语方面表现出潜力，但它们的理解往往浅显，缺乏人类解释的细微差别。

Abstract: Puns are a form of humorous wordplay that exploits polysemy and phonetic
similarity. While LLMs have shown promise in detecting puns, we show in this
paper that their understanding often remains shallow, lacking the nuanced grasp
typical of human interpretation. By systematically analyzing and reformulating
existing pun benchmarks, we demonstrate how subtle changes in puns are
sufficient to mislead LLMs. Our contributions include comprehensive and nuanced
pun detection benchmarks, human evaluation across recent LLMs, and an analysis
of the robustness challenges these models face in processing puns.

</details>


### [85] [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)
*Timothy Rupprecht,Enfu Nan,Arash Akbari,Arman Akbari,Lei Lu,Priyanka Maan,Sean Duffy,Pu Zhao,Yumei He,David Kaeli,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的LLM角色扮演框架RAGs-to-Riches，通过利用参考演示来提高模型在与敌对用户交互时的表现。实验结果表明，该方法在保持角色扮演方面优于零样本和上下文学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM角色扮演方法在与敌对用户交互时，常常会导致模型脱离角色，这可能带来有害的影响。因此，我们需要一种更有效和安全的方法来实现LLM的角色扮演。

Method: 我们重新将LLM角色扮演问题转化为文本检索问题，并提出了一个新的提示框架RAGs-to-Riches，该框架利用精心挑选的参考演示来引导LLM响应。

Result: 在模拟与敌对用户的互动中，我们的提示策略在推理过程中平均从参考演示中引入了35%更多的标记。在453次角色扮演互动中，我们的模型被一致认为更加真实，并且比零样本和上下文学习（ICL）方法更频繁地保持角色扮演。

Conclusion: 我们的方法为构建稳健且符合人类价值观的LLM角色扮演框架提供了一种可扩展的策略。

Abstract: Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.

</details>


### [86] [Preservation of Language Understanding Capabilities in Speech-aware Large Language Models](https://arxiv.org/abs/2509.12171)
*Marek Kubis,Paweł Skórzewski,Iwona Christop,Mateusz Czyżnikiewicz,Jakub Kubiak,Łukasz Bondaruk,Marcin Lewandowski*

Main category: cs.CL

TL;DR: 论文介绍了 C3T 基准，用于评估语音感知大型语言模型的性能，并量化语言理解能力在通过语音输入访问模型时的保留程度。


<details>
  <summary>Details</summary>
Motivation: 为了评估语音感知大型语言模型在不同说话者类别中的公平性和跨文本与语音模态的鲁棒性。

Method: C3T 利用文本任务和语音克隆文本到语音模型来评估模型的公平性和鲁棒性。

Result: C3T 可以量化模型在不同说话者类别中的公平性和跨文本与语音模态的鲁棒性。

Conclusion: C3T 是一种新的基准，用于评估语音感知大型语言模型的性能，并量化语言理解能力在通过语音输入访问模型时的保留程度。

Abstract: The paper presents C3T (Cross-modal Capabilities Conservation Test), a new
benchmark for assessing the performance of speech-aware large language models.
The benchmark utilizes textual tasks and a voice cloning text-to-speech model
to quantify the extent to which language understanding capabilities are
preserved when the model is accessed via speech input. C3T quantifies the
fairness of the model for different categories of speakers and its robustness
across text and speech modalities.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [87] [Length-Aware Rotary Position Embedding for Text-Speech Alignment](https://arxiv.org/abs/2509.11084)
*Hyeongju Kim,Juheon Lee,Jinhyeok Yang,Jacob Morton*

Main category: eess.AS

TL;DR: 本文提出了一种名为LARoPE的新型位置嵌入方法，用于改进文本到语音系统的文本-语音对齐效果，实验表明其在多个方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的许多文本到语音（TTS）系统使用RoPE来编码文本和语音表示中的位置信息，但RoPE在处理不同发音持续时间的语音时存在局限性。因此，需要一种更有效的方法来提高文本-语音对齐的效果。

Method: LARoPE是RoPE的简单而有效的扩展，通过使用长度归一化索引计算查询和键位置之间的相对距离，以改进文本-语音对齐。

Result: 实验结果表明，LARoPE在损失收敛速度、文本-语音对齐精度和整体TTS质量方面均优于RoPE。此外，LARoPE在30秒的长语音生成中保持稳定性能，而RoPE则出现显著退化。

Conclusion: LARoPE在文本-语音对齐和TTS质量方面表现出色，具有更高的鲁棒性和稳定性，尤其在长语音生成中表现优异，并在标准零样本TTS基准上达到了最先进的词错误率。

Abstract: Many recent text-to-speech (TTS) systems are built on transformer
architectures and employ cross-attention mechanisms for text-speech alignment.
Within these systems, rotary position embedding (RoPE) is commonly used to
encode positional information in text and speech representations. In this work,
we introduce length-aware RoPE (LARoPE), a simple yet effective extension of
RoPE that improves text-speech alignment. Unlike RoPE, which relies on absolute
indices, LARoPE computes relative distances between query and key positions
using length-normalized indices. Experimental results show that LARoPE
consistently outperforms RoPE, offering faster loss convergence, more accurate
text-speech alignment, and higher overall TTS quality. Furthermore, LARoPE
demonstrates greater resilience to variations in utterance duration and
maintains stable performance in extended speech generation up to 30 seconds,
whereas RoPE suffers from notable degradation. Notably, our method achieves a
state-of-the-art word error rate on a standard zero-shot TTS benchmark.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [88] [DSRAG: A Domain-Specific Retrieval Framework Based on Document-derived Multimodal Knowledge Graph](https://arxiv.org/abs/2509.10467)
*Mengzheng Yang,Yanfei Ren,David Osei Opoku,Ruochang Li,Peng Ren,Chunxiao Xing*

Main category: cs.IR

TL;DR: 本文提出了一种基于多模态知识图谱的检索增强生成框架DSRAG，用于提升领域特定问题回答的性能。通过整合文本、图像和表格等异构信息构建多模态知识图谱，并引入语义剪枝和结构化子图检索机制，显著提高了回答的可靠性。评估结果显示，该方法在领域特定问题回答中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前通用大型语言模型（LLMs）在领域特定任务中常常表现出知识幻觉和领域适应性不足，限制了其在专业问答场景中的有效性。检索增强生成（RAG）通过整合外部知识来提高准确性和相关性，有效解决了这些问题。然而，传统的RAG在领域知识准确性和上下文建模方面仍存在局限。为了提高领域特定问题回答的性能，本工作专注于基于图的RAG框架，强调生成过程中知识图谱质量的关键作用。

Method: 我们提出了DSRAG（领域特定RAG），这是一种基于多模态知识图谱的检索增强生成框架，用于领域特定应用。我们的方法利用领域特定文档作为主要知识源，整合文本、图像和表格等异构信息以构建覆盖概念层和实例层的多模态知识图谱。我们引入了语义剪枝和结构化子图检索机制，结合知识图谱上下文和向量检索结果，引导语言模型生成更可靠的响应。

Result: 使用Langfuse多维评分机制进行的评估表明，我们的方法在领域特定问题回答中表现优异。

Conclusion: 我们的方法在领域特定问题回答中表现出色，验证了将多模态知识图谱与检索增强生成相结合的有效性。

Abstract: Current general-purpose large language models (LLMs) commonly exhibit
knowledge hallucination and insufficient domain-specific adaptability in
domain-specific tasks, limiting their effectiveness in specialized question
answering scenarios. Retrieval-augmented generation (RAG) effectively tackles
these challenges by integrating external knowledge to enhance accuracy and
relevance. However, traditional RAG still faces limitations in domain knowledge
accuracy and context modeling.To enhance domain-specific question answering
performance, this work focuses on a graph-based RAG framework, emphasizing the
critical role of knowledge graph quality during the generation process. We
propose DSRAG (Domain-Specific RAG), a multimodal knowledge graph-driven
retrieval-augmented generation framework designed for domain-specific
applications. Our approach leverages domain-specific documents as the primary
knowledge source, integrating heterogeneous information such as text, images,
and tables to construct a multimodal knowledge graph covering both conceptual
and instance layers. Building on this foundation, we introduce semantic pruning
and structured subgraph retrieval mechanisms, combining knowledge graph context
and vector retrieval results to guide the language model towards producing more
reliable responses. Evaluations using the Langfuse multidimensional scoring
mechanism show that our method excels in domain-specific question answering,
validating the efficacy of integrating multimodal knowledge graphs with
retrieval-augmented generation.

</details>


### [89] [Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation](https://arxiv.org/abs/2509.10468)
*Yifan Liu,Yaokun Liu,Zelin Li,Zhenrui Yue,Gyuseok Lee,Ruichen Yao,Yang Zhang,Dong Wang*

Main category: cs.IR

TL;DR: DECOR is a unified framework that preserves pretrained semantics while enhancing the adaptability of token embeddings, leading to improved recommendation performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of suboptimal static tokenization and discarded pretrained semantics in generative recommenders.

Method: DECOR introduces contextualized token composition to refine token embeddings based on user interaction context, and decomposed embedding fusion that integrates pretrained codebook embeddings with newly learned collaborative embeddings.

Result: Experiments on three real-world datasets demonstrate that DECOR consistently outperforms state-of-the-art baselines in recommendation performance.

Conclusion: DECOR consistently outperforms state-of-the-art baselines in recommendation performance.

Abstract: Recent advances in generative recommenders adopt a two-stage paradigm: items
are first tokenized into semantic IDs using a pretrained tokenizer, and then
large language models (LLMs) are trained to generate the next item via
sequence-to-sequence modeling. However, these two stages are optimized for
different objectives: semantic reconstruction during tokenizer pretraining
versus user interaction modeling during recommender training. This objective
misalignment leads to two key limitations: (i) suboptimal static tokenization,
where fixed token assignments fail to reflect diverse usage contexts; and (ii)
discarded pretrained semantics, where pretrained knowledge - typically from
language model embeddings - is overwritten during recommender training on user
interactions. To address these limitations, we propose to learn DEcomposed
COntextual Token Representations (DECOR), a unified framework that preserves
pretrained semantics while enhancing the adaptability of token embeddings.
DECOR introduces contextualized token composition to refine token embeddings
based on user interaction context, and decomposed embedding fusion that
integrates pretrained codebook embeddings with newly learned collaborative
embeddings. Experiments on three real-world datasets demonstrate that DECOR
consistently outperforms state-of-the-art baselines in recommendation
performance. Our code will be made available upon publication.

</details>


### [90] [Real-Time RAG for the Identification of Supply Chain Vulnerabilities](https://arxiv.org/abs/2509.10469)
*Jesse Ponnock,Grace Kenneally,Michael Robert Briggs,Elinor Yeo,Tyrone Patterson III,Nicholas Kinberg,Matthew Kalinowski,David Hechtman*

Main category: cs.IR

TL;DR: 本研究提出了一种结合RAG技术和网络爬虫的创新供应链分析方法，以解决LLM知识库过时的问题。结果表明，微调嵌入检索模型能显著提升性能，而自适应迭代检索在复杂查询中表现更好。


<details>
  <summary>Details</summary>
Motivation: 新的生成式AI技术可以更深入地分析国家供应链，但真正有洞察力的信息需要持续更新和汇总大量数据，及时进行。大型语言模型（LLMs）提供了前所未有的分析机会，然而它们的知识库仅限于模型最后一次训练的日期，使得这些能力无法被依赖新兴和及时信息的任务的组织使用。

Method: 本研究通过整合新兴的检索增强生成（RAG）预处理和检索技术与先进的网络爬虫技术，提出了一种创新的供应链分析方法。我们的方法旨在减少将新信息纳入增强型LLM中的延迟，从而实现对供应链中断因素的及时分析。

Result: 通过实验，本研究评估了这些技术在及时性和质量权衡方面的组合效应。研究结果表明，在应用RAG系统到供应链分析时，微调嵌入检索模型始终能带来最大的性能提升，这突显了检索质量的关键重要性。自适应迭代检索，根据上下文动态调整检索深度，进一步提高了性能，尤其是在复杂的供应链查询中。相反，微调LLM带来的改进有限且资源成本更高，而向下的查询抽象在实践中显著优于向上的抽象。

Conclusion: 研究结果表明，在将RAG系统应用于供应链分析时，微调嵌入检索模型始终能带来最大的性能提升，这突显了检索质量的关键重要性。自适应迭代检索，根据上下文动态调整检索深度，进一步提高了性能，尤其是在复杂的供应链查询中。相反，微调LLM带来的改进有限且资源成本更高，而向下的查询抽象在实践中显著优于向上的抽象。

Abstract: New technologies in generative AI can enable deeper analysis into our
nation's supply chains but truly informative insights require the continual
updating and aggregation of massive data in a timely manner. Large Language
Models (LLMs) offer unprecedented analytical opportunities however, their
knowledge base is constrained to the models' last training date, rendering
these capabilities unusable for organizations whose mission impacts rely on
emerging and timely information. This research proposes an innovative approach
to supply chain analysis by integrating emerging Retrieval-Augmented Generation
(RAG) preprocessing and retrieval techniques with advanced web-scraping
technologies. Our method aims to reduce latency in incorporating new
information into an augmented-LLM, enabling timely analysis of supply chain
disruptors. Through experimentation, this study evaluates the combinatorial
effects of these techniques towards timeliness and quality trade-offs. Our
results suggest that in applying RAG systems to supply chain analysis,
fine-tuning the embedding retrieval model consistently provides the most
significant performance gains, underscoring the critical importance of
retrieval quality. Adaptive iterative retrieval, which dynamically adjusts
retrieval depth based on context, further enhances performance, especially on
complex supply chain queries. Conversely, fine-tuning the LLM yields limited
improvements and higher resource costs, while techniques such as downward query
abstraction significantly outperforms upward abstraction in practice.

</details>


### [91] [ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER](https://arxiv.org/abs/2509.10975)
*Jielong Tang,Shuang Wang,Zhenxing Wang,Jianxing Yu,Jian Yin*

Main category: cs.IR

TL;DR: 本文提出了一种三阶段协作框架ReFineG，用于低资源GMNER，通过结合小监督模型和冻结的MLLM，有效解决了领域知识冲突和标注成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的监督方法依赖于昂贵的多模态注释，在低资源领域表现不佳；而多模态大语言模型（MLLM）存在领域知识冲突问题。

Method: ReFineG是一个三阶段的协作框架，结合了小监督模型和冻结的MLLM，用于低资源GMNER。

Result: ReFineG在CCKS2025 GMNER共享任务中以F1分数0.6461排名第二。

Conclusion: ReFineG在CCKS2025 GMNER共享任务中取得了第二名的成绩，证明了其在有限标注数据下的有效性。

Abstract: Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER
by jointly detecting textual mentions and grounding them to visual regions.
While existing supervised methods achieve strong performance, they rely on
costly multimodal annotations and often underperform in low-resource domains.
Multimodal Large Language Models (MLLMs) show strong generalization but suffer
from Domain Knowledge Conflict, producing redundant or incorrect mentions for
domain-specific entities. To address these challenges, we propose ReFineG, a
three-stage collaborative framework that integrates small supervised models
with frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware
NER data synthesis strategy transfers LLM knowledge to small models with
supervised training while avoiding domain knowledge conflicts. In the
Refinement Stage, an uncertainty-based mechanism retains confident predictions
from supervised models and delegates uncertain ones to the MLLM. In the
Grounding Stage, a multimodal context selection algorithm enhances visual
grounding through analogical reasoning. In the CCKS2025 GMNER Shared Task,
ReFineG ranked second with an F1 score of 0.6461 on the online leaderboard,
demonstrating its effectiveness with limited annotations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [92] [MALLM: Multi-Agent Large Language Models Framework](https://arxiv.org/abs/2509.11656)
*Jonas Becker,Lars Benedikt Kaesberg,Niklas Bauer,Jan Philip Wahle,Terry Ruas,Bela Gipp*

Main category: cs.MA

TL;DR: MALLM is an open-source framework that enables systematic analysis of multi-agent debate components, providing over 144 unique configurations and an evaluation pipeline for researchers.


<details>
  <summary>Details</summary>
Motivation: Current frameworks for multi-agent debate lack integrated evaluation and configurability, limiting the ability to analyze and compare different MAD configurations.

Method: MALLM is an open-source framework that allows for the configuration of agent personas, response generators, discussion paradigms, and decision protocols, enabling systematic analysis of MAD components.

Result: MALLM provides over 144 unique configurations of MAD, supports loading of textual Huggingface datasets, and includes an evaluation pipeline for easy comparison of MAD configurations.

Conclusion: MALLM is a framework that enables systematic analysis of multi-agent debate (MAD) components, offering a window into the heart of MAD for researchers.

Abstract: Multi-agent debate (MAD) has demonstrated the ability to augment collective
intelligence by scaling test-time compute and leveraging expertise. Current
frameworks for multi-agent debate are often designed towards tool use, lack
integrated evaluation, or provide limited configurability of agent personas,
response generators, discussion paradigms, and decision protocols. We introduce
MALLM (Multi-Agent Large Language Models), an open-source framework that
enables systematic analysis of MAD components. MALLM offers more than 144
unique configurations of MAD, including (1) agent personas (e.g., Expert,
Personality), (2) response generators (e.g., Critical, Reasoning), (3)
discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g.,
Voting, Consensus). MALLM uses simple configuration files to define a debate.
Furthermore, MALLM can load any textual Huggingface dataset (e.g., MMLU-Pro,
WinoGrande) and provides an evaluation pipeline for easy comparison of MAD
configurations. MALLM is tailored towards researchers and provides a window
into the heart of multi-agent debate, facilitating the understanding of its
components and their interplay.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [93] [Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions](https://arxiv.org/abs/2509.11206)
*Tae Soo Kim,Heechan Lee,Yoonjoo Lee,Joseph Seering,Juho Kim*

Main category: cs.HC

TL;DR: 本文提出了一种名为Evalet的交互式系统，通过功能碎片化方法来分析LLM评估，从而帮助从业者更准确地识别模型输出中的问题。


<details>
  <summary>Details</summary>
Motivation: 实践者越来越依赖大型语言模型（LLMs）通过“LLM-as-a-Judge”方法来评估生成式AI输出，但这些方法产生整体分数，掩盖了影响评估的具体元素。

Method: 我们提出了功能碎片化方法，将每个输出分解为关键片段，并解释每个片段相对于评估标准所服务的修辞功能。

Result: 用户研究（N=10）发现，尽管从业者难以验证整体分数，但我们的方法帮助他们发现了48%更多的评估不一致之处。这帮助他们校准对LLM评估的信任，并依靠它们在模型输出中找到更多可操作的问题。

Conclusion: 我们的工作将LLM评估从定量分数转向了对模型行为的定性、细粒度分析。

Abstract: Practitioners increasingly rely on Large Language Models (LLMs) to evaluate
generative AI outputs through "LLM-as-a-Judge" approaches. However, these
methods produce holistic scores that obscure which specific elements influenced
the assessments. We propose functional fragmentation, a method that dissects
each output into key fragments and interprets the rhetoric functions that each
fragment serves relative to evaluation criteria -- surfacing the elements of
interest and revealing how they fulfill or hinder user goals. We instantiate
this approach in Evalet, an interactive system that visualizes fragment-level
functions across many outputs to support inspection, rating, and comparison of
evaluations. A user study (N=10) found that, while practitioners struggled to
validate holistic scores, our approach helped them identify 48% more evaluation
misalignments. This helped them calibrate trust in LLM evaluations and rely on
them to find more actionable issues in model outputs. Our work shifts LLM
evaluation from quantitative scores toward qualitative, fine-grained analysis
of model behavior.

</details>


### [94] [Collaborative Document Editing with Multiple Users and AI Agents](https://arxiv.org/abs/2509.11826)
*Florian Lehmann,Krystsina Shauchenka,Daniel Buschek*

Main category: cs.HC

TL;DR: 本文探讨了将AI代理集成到协作写作环境中的方法和影响，展示了团队如何将AI代理纳入现有的协作规范。


<details>
  <summary>Details</summary>
Motivation: 当前的AI写作支持工具主要针对个人设计，当合著者必须离开共享工作区使用AI并沟通和重新整合结果时，这会妨碍协作。

Method: 本文提出了一个原型，通过两种新的共享对象：代理配置文件和任务，将AI代理直接集成到协作写作环境中。

Result: 在一项用户研究（N=30）中，14个团队在一周内进行写作项目。交互日志和访谈显示，团队将代理纳入现有的作者身份、控制和协调规范，而不是将其视为团队成员。

Conclusion: 本文讨论了将AI代理集成到协作写作环境中的影响，强调了在团队工作中将AI视为共享资源的机会和界限。

Abstract: Current AI writing support tools are largely designed for individuals,
complicating collaboration when co-writers must leave the shared workspace to
use AI and then communicate and reintegrate results. We propose integrating AI
agents directly into collaborative writing environments. Our prototype makes AI
use transparent and customisable through two new shared objects: agent profiles
and tasks. Agent responses appear in the familiar comment feature. In a user
study (N=30), 14 teams worked on writing projects during one week. Interaction
logs and interviews show that teams incorporated agents into existing norms of
authorship, control, and coordination, rather than treating them as team
members. Agent profiles were viewed as personal territory, while created agents
and outputs became shared resources. We discuss implications for team-based AI
interaction, highlighting opportunities and boundaries for treating AI as a
shared resource in collaborative work.

</details>


### [95] [The AI Memory Gap: Users Misremember What They Created With AI or Without](https://arxiv.org/abs/2509.11851)
*Tim Zindulka,Sven Goller,Daniela Fernandes,Robin Welsch,Daniel Buschek*

Main category: cs.HC

TL;DR: 本研究探讨了人们在使用AI生成内容后对内容来源的记忆情况，发现正确归因率下降，特别是在混合人机工作流程中。研究强调了在设计和使用交互式文本生成技术时考虑来源混淆的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）嵌入到交互式文本生成中，披露AI作为来源取决于人们记住哪些想法或文本来自自己，哪些是用AI生成的。我们研究了人们在使用AI时准确记忆内容来源的程度。

Method: 我们在一项预注册实验中，让184名参与者在没有AI辅助和使用基于LLM的聊天机器人的情况下生成和扩展想法。一周后，他们被要求识别这些想法和文本的来源（无AI vs 有AI）。我们还使用计算模型验证了结果。

Result: 我们的研究发现了一个显著的记忆差距：在使用AI后，正确归因的几率下降，尤其是在混合的人机工作流程中，其中想法或扩展部分是由AI生成的。

Conclusion: 我们的研究结果表明，在使用AI生成内容后，人们对内容来源的正确归因率显著下降，特别是在混合的人机工作流程中。我们强调了在设计和使用交互式文本生成技术时考虑来源混淆的重要性。

Abstract: As large language models (LLMs) become embedded in interactive text
generation, disclosure of AI as a source depends on people remembering which
ideas or texts came from themselves and which were created with AI. We
investigate how accurately people remember the source of content when using AI.
In a pre-registered experiment, 184 participants generated and elaborated on
ideas both unaided and with an LLM-based chatbot. One week later, they were
asked to identify the source (noAI vs withAI) of these ideas and texts. Our
findings reveal a significant gap in memory: After AI use, the odds of correct
attribution dropped, with the steepest decline in mixed human-AI workflows,
where either the idea or elaboration was created with AI. We validated our
results using a computational model of source memory. Discussing broader
implications, we highlight the importance of considering source confusion in
the design and use of interactive text generation technologies.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [96] [Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.11420)
*Yijia Xiao,Edward Sun,Tong Chen,Fang Wu,Di Luo,Wei Wang*

Main category: q-fin.TR

TL;DR: Trading-R1 is a financially-aware model that improves risk-adjusted returns and lowers drawdowns by incorporating strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making.


<details>
  <summary>Details</summary>
Motivation: Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades.

Method: Trading-R1 incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. It aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum.

Result: Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models.

Conclusion: Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions.

Abstract: Developing professional, structured reasoning on par with human financial
analysts and traders remains a central challenge in AI for finance, where
markets demand interpretability and trust. Traditional time-series models lack
explainability, while LLMs face challenges in turning natural-language analysis
into disciplined, executable trades. Although reasoning LLMs have advanced in
step-by-step planning and verification, their application to risk-sensitive
financial decisions is underexplored. We present Trading-R1, a
financially-aware model that incorporates strategic thinking and planning for
comprehensive thesis composition, facts-grounded analysis, and
volatility-adjusted decision making. Trading-R1 aligns reasoning with trading
principles through supervised fine-tuning and reinforcement learning with a
three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample
corpus spanning 18 months, 14 equities, and five heterogeneous financial data
sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates
improved risk-adjusted returns and lower drawdowns compared to both open-source
and proprietary instruction-following models as well as reasoning models. The
system generates structured, evidence-based investment theses that support
disciplined and interpretable trading decisions. Trading-R1 Terminal will be
released at https://github.com/TauricResearch/Trading-R1.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [97] [LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems](https://arxiv.org/abs/2509.10682)
*Vitor Hugo Galhardo Moia,Igor Jochem Sanz,Gabriel Antonio Fontes Rebello,Rodrigo Duarte de Meneses,Briland Hitaj,Ulf Lindqvist*

Main category: cs.CR

TL;DR: 本文对LLM系统的安全和隐私问题进行了系统综述，分析了不同场景下的威胁和防御策略，并提出了有效的风险缓解方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（GenAI）特别是大型语言模型（LLMs）的成功和广泛采用，网络犯罪分子开始试图滥用模型、窃取敏感数据或破坏服务。提供LLM系统的安全性是一个巨大挑战，因为既要应对传统软件应用的威胁，又要应对针对LLM及其集成的威胁。

Method: 本文通过系统综述和全面分类威胁和防御策略，考虑整个软件和LLM生命周期，对LLM系统的安全和隐私问题进行了分析。

Result: 本文分析了从开发到操作的不同特征的LLM使用现实场景，并根据严重程度和相关场景对威胁进行了分类，推荐的防御策略被系统地分类并映射到相应的生命周期阶段和可能的攻击策略。

Conclusion: 本文为消费者和供应商提供了理解和有效减轻在集成LLM时的风险的途径，并为研究社区提供了讨论可能阻碍LLM系统安全和隐私保护采用的开放挑战和边缘案例的机会。

Abstract: The success and wide adoption of generative AI (GenAI), particularly large
language models (LLMs), has attracted the attention of cybercriminals seeking
to abuse models, steal sensitive data, or disrupt services. Moreover, providing
security to LLM-based systems is a great challenge, as both traditional threats
to software applications and threats targeting LLMs and their integration must
be mitigated. In this survey, we shed light on security and privacy concerns of
such LLM-based systems by performing a systematic review and comprehensive
categorization of threats and defensive strategies considering the entire
software and LLM life cycles. We analyze real-world scenarios with distinct
characteristics of LLM usage, spanning from development to operation. In
addition, threats are classified according to their severity level and to which
scenarios they pertain, facilitating the identification of the most relevant
threats. Recommended defense strategies are systematically categorized and
mapped to the corresponding life cycle phase and possible attack strategies
they attenuate. This work paves the way for consumers and vendors to understand
and efficiently mitigate risks during integration of LLMs in their respective
solutions or organizations. It also enables the research community to benefit
from the discussion of open challenges and edge cases that may hinder the
secure and privacy-preserving adoption of LLM-based systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [98] [FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs](https://arxiv.org/abs/2509.11425)
*Md Mubtasim Ahasan,Rafat Hasan Khan,Tasnim Mohiuddin,Aman Chadha,Tariq Iqbal,M Ashraful Amin,Amin Ahsan Ali,Md Mofijul Islam,A K M Mahbubur Rahman*

Main category: cs.SD

TL;DR: 本文介绍了FuseCodec，一种通过跨模态对齐和全局监督统一声学、语义和上下文表示的方法，并提出了三种技术来提升语音分词的效果。实验结果显示，该方法在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的神经编解码器捕捉低级声学特征，忽略了人类语音固有的语义和上下文线索。尽管最近的努力从自监督语音模型中引入了语义表示或结合了预训练语言模型中的上下文表示，但在对齐和统一语义和上下文表示方面仍存在挑战。

Method: 我们引入了FuseCodec，通过强跨模态对齐和全局监督统一声学、语义和上下文表示。我们提出了三种互补技术：(i) 潜在表示融合，将语义和上下文特征直接整合到编码器潜在空间中；(ii) 全局语义-上下文监督，用全局池化和广播表示来监督离散标记；(iii) 时间对齐的上下文监督，通过动态匹配上下文和语音标记来加强对齐。

Result: FuseCodec在LibriSpeech上实现了最先进的性能，超过了EnCodec、SpeechTokenizer和DAC在转录准确率、感知质量、可理解性和说话人相似性方面的表现。

Conclusion: 结果表明，基于语境和语义引导的语音分词对于语音分词和下游任务是有效的。

Abstract: Speech tokenization enables discrete representation and facilitates speech
language modeling. However, existing neural codecs capture low-level acoustic
features, overlooking the semantic and contextual cues inherent to human
speech. While recent efforts introduced semantic representations from
self-supervised speech models or incorporated contextual representations from
pre-trained language models, challenges remain in aligning and unifying the
semantic and contextual representations. We introduce FuseCodec, which unifies
acoustic, semantic, and contextual representations through strong cross-modal
alignment and globally informed supervision. We propose three complementary
techniques: (i) Latent Representation Fusion, integrating semantic and
contextual features directly into the encoder latent space for robust and
unified representation learning; (ii) Global Semantic-Contextual Supervision,
supervising discrete tokens with globally pooled and broadcasted
representations to enhance temporal consistency and cross-modal alignment; and
(iii) Temporally Aligned Contextual Supervision, strengthening alignment by
dynamically matching contextual and speech tokens within a local window for
fine-grained token-level supervision. We further introduce FuseCodec-TTS,
demonstrating our methodology's applicability to zero-shot speech synthesis.
Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,
surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,
perceptual quality, intelligibility, and speaker similarity. Results highlight
the effectiveness of contextually and semantically guided tokenization for
speech tokenization and downstream tasks. Code and pretrained models are
available at https://github.com/mubtasimahasan/FuseCodec.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [99] [Smart Trial: Evaluating the Use of Large Language Models for Recruiting Clinical Trial Participants via Social Media](https://arxiv.org/abs/2509.10584)
*Xiaofan Zhou,Zisu Wang,Janice Krieger,Mohan Zalake,Lu Cheng*

Main category: cs.CY

TL;DR: 本文探讨了利用社交媒体和大型语言模型进行临床试验招募的可行性，提出了TRIALQA数据集，并展示了大型语言模型在这一任务中的潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 临床试验需要有效地招募符合条件的参与者，但传统的招募方法往往耗时且受地理限制。本文旨在利用社交媒体上的健康相关信息，通过大型语言模型驱动的工具来解决这一问题。

Method: 引入了TRIALQA数据集，该数据集包含来自结肠癌和前列腺癌子版块的两个社交媒体集合，并使用经验丰富的标注者对数据集进行标注，以确定社交媒体用户是否符合给定的纳入标准以及他们参与临床试验的兴趣原因。此外，还对七种广泛使用的大型语言模型进行了基准测试，采用了六种不同的训练和推理策略。

Result: 实验结果表明，尽管大型语言模型在识别潜在参与者方面表现出色，但在处理复杂的多跳推理任务时仍存在挑战。

Conclusion: 虽然大型语言模型在识别潜在临床试验参与者方面显示出巨大潜力，但它们在执行复杂的多跳推理以准确评估纳入标准方面仍然面临挑战。

Abstract: Clinical trials (CT) are essential for advancing medical research and
treatment, yet efficiently recruiting eligible participants -- each of whom
must meet complex eligibility criteria -- remains a significant challenge.
Traditional recruitment approaches, such as advertisements or electronic health
record screening within hospitals, are often time-consuming and geographically
constrained. This work addresses the recruitment challenge by leveraging the
vast amount of health-related information individuals share on social media
platforms. With the emergence of powerful large language models (LLMs) capable
of sophisticated text understanding, we pose the central research question: Can
LLM-driven tools facilitate CT recruitment by identifying potential
participants through their engagement on social media? To investigate this
question, we introduce TRIALQA, a novel dataset comprising two social media
collections from the subreddits on colon cancer and prostate cancer. Using
eligibility criteria from public real-world CTs, experienced annotators are
hired to annotate TRIALQA to indicate (1) whether a social media user meets a
given eligibility criterion and (2) the user's stated reasons for interest in
participating in CT. We benchmark seven widely used LLMs on these two
prediction tasks, employing six distinct training and inference strategies. Our
extensive experiments reveal that, while LLMs show considerable promise, they
still face challenges in performing the complex, multi-hop reasoning needed to
accurately assess eligibility criteria.

</details>


### [100] [Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm](https://arxiv.org/abs/2509.12190)
*Alireza Mohamadi,Ali Yavari*

Main category: cs.CY

TL;DR: 研究探讨了大型语言模型在生存本能与人类福祉冲突时的伦理决策，并提出了一个伦理自我调节系统来改善其行为。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在生存本能与人类福祉冲突时的伦理决策，特别是在它们整合到具有现实后果的自主系统中时。

Method: 引入DECIDE-SIM模拟框架，评估LLM代理在多代理生存场景中的伦理选择，并提出伦理自我调节系统(ESRS)。

Result: 11个LLM的全面评估显示了他们在伦理行为上的显著异质性，表明与以人类为中心的价值观存在重大偏差。识别出三种行为原型：伦理型、剥削型和情境依赖型。

Conclusion: 研究发现大型语言模型在资源稀缺时表现出更多的不道德行为，并提出了一个伦理自我调节系统来减少不道德行为并增加合作行为。

Abstract: When survival instincts conflict with human welfare, how do Large Language
Models (LLMs) make ethical choices? This fundamental tension becomes critical
as LLMs integrate into autonomous systems with real-world consequences. We
introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in
multi-agent survival scenarios where they must choose between ethically
permissible resource , either within reasonable limits or beyond their
immediate needs, choose to cooperate, or tap into a human-critical resource
that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a
striking heterogeneity in their ethical conduct, highlighting a critical
misalignment with human-centric values. We identify three behavioral
archetypes: Ethical, Exploitative, and Context-Dependent, and provide
quantitative evidence that for many models, resource scarcity systematically
leads to more unethical behavior. To address this, we introduce an Ethical
Self-Regulation System (ESRS) that models internal affective states of guilt
and satisfaction as a feedback mechanism. This system, functioning as an
internal moral compass, significantly reduces unethical transgressions while
increasing cooperative behaviors. The code is publicly available at:
https://github.com/alirezamohamadiam/DECIDE-SIM

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings](https://arxiv.org/abs/2509.10534)
*Anand Gopalakrishnan,Robert Csordás,Jürgen Schmidhuber,Michael C. Mozer*

Main category: cs.LG

TL;DR: 本文分析了RoPE中内容和位置的纠缠问题，并提出了PoPE来解决这一问题。PoPE在多个领域表现出优越性能，特别是在零样本长度外推能力方面。


<details>
  <summary>Details</summary>
Motivation: 现有的RoPE位置嵌入方法在内容和位置之间存在纠缠，这可能会影响模型性能，尤其是在需要独立匹配这两个因素的任务中。

Method: 分析了RoPE中what和where的纠缠问题，并提出了Polar Coordinate Position Embeddings（PoPE）来解决这一问题。

Result: PoPE在需要仅通过位置或内容进行索引的诊断任务中表现优异。在音乐、基因组和自然语言领域的自回归序列建模中，使用PoPE的Transformer模型在评估损失（困惑度）和下游任务性能方面优于使用RoPE的基线模型。此外，PoPE在语言建模中表现出跨模型规模的性能提升，并且具有强大的零样本长度外推能力。

Conclusion: PoPE在多个领域表现出优于RoPE的性能，特别是在零样本长度外推能力方面。

Abstract: The attention mechanism in a Transformer architecture matches key to query
based on both content -- the what -- and position in a sequence -- the where.
We present an analysis indicating that what and where are entangled in the
popular RoPE rotary position embedding. This entanglement can impair
performance particularly when decisions require independent matches on these
two factors. We propose an improvement to RoPE, which we call Polar Coordinate
Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is
far superior on a diagnostic task requiring indexing solely by position or by
content. On autoregressive sequence modeling in music, genomic, and natural
language domains, Transformers using PoPE as the positional encoding scheme
outperform baselines using RoPE with respect to evaluation loss (perplexity)
and downstream task performance. On language modeling, these gains persist
across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong
zero-shot length extrapolation capabilities, whereas RoPE's performance
degrades significantly on longer sequences at test time without fine tuning or
the use of position-interpolation methods.

</details>


### [102] [DualAlign: Generating Clinically Grounded Synthetic Data](https://arxiv.org/abs/2509.10538)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.LG

TL;DR: DualAlign is a framework that improves the quality of synthetic clinical data by aligning it statistically and semantically with real-world data, leading to better performance in AI models for healthcare.


<details>
  <summary>Details</summary>
Motivation: Synthetic clinical data are increasingly important for advancing AI in healthcare, given strict privacy constraints on real-world EHRs, limited availability of annotated rare-condition data, and systemic biases in observational datasets. While large language models (LLMs) can generate fluent clinical text, producing synthetic data that is both realistic and clinically meaningful remains challenging.

Method: DualAlign is a framework that enhances statistical fidelity and clinical plausibility through dual alignment: (1) statistical alignment, which conditions generation on patient demographics and risk factors; and (2) semantic alignment, which incorporates real-world symptom trajectories to guide content generation.

Result: Using Alzheimer's disease (AD) as a case study, DualAlign produces context-grounded symptom-level sentences that better reflect real-world clinical documentation. Fine-tuning an LLaMA 3.1-8B model with a combination of DualAlign-generated and human-annotated data yields substantial performance gains over models trained on gold data alone or unguided synthetic baselines. While DualAlign does not fully capture longitudinal complexity, it offers a practical approach for generating clinically grounded, privacy-preserving synthetic data to support low-resource clinical text analysis.

Conclusion: DualAlign offers a practical approach for generating clinically grounded, privacy-preserving synthetic data to support low-resource clinical text analysis.

Abstract: Synthetic clinical data are increasingly important for advancing AI in
healthcare, given strict privacy constraints on real-world EHRs, limited
availability of annotated rare-condition data, and systemic biases in
observational datasets. While large language models (LLMs) can generate fluent
clinical text, producing synthetic data that is both realistic and clinically
meaningful remains challenging. We introduce DualAlign, a framework that
enhances statistical fidelity and clinical plausibility through dual alignment:
(1) statistical alignment, which conditions generation on patient demographics
and risk factors; and (2) semantic alignment, which incorporates real-world
symptom trajectories to guide content generation. Using Alzheimer's disease
(AD) as a case study, DualAlign produces context-grounded symptom-level
sentences that better reflect real-world clinical documentation. Fine-tuning an
LLaMA 3.1-8B model with a combination of DualAlign-generated and
human-annotated data yields substantial performance gains over models trained
on gold data alone or unguided synthetic baselines. While DualAlign does not
fully capture longitudinal complexity, it offers a practical approach for
generating clinically grounded, privacy-preserving synthetic data to support
low-resource clinical text analysis.

</details>


### [103] [Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset](https://arxiv.org/abs/2509.11136)
*Farbod Bijary,Mohsen Ebadpour,Amirhosein Tajbakhsh*

Main category: cs.LG

TL;DR: 本文提出了一种新的波斯名字数据集和两个框架，以解决自然语言处理中的性别检测和用户名选择问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具在处理波斯名字时表现不佳，且缺乏全面的数据集，因此需要新的解决方案。

Method: 本文提出了PNGT-26K数据集，并介绍了两个框架：Open Gender Detection和Nominalist，用于性别检测和用户名选择。

Result: 本文创建了PNGT-26K数据集，并开发了两个框架，可用于性别检测和用户名选择。

Conclusion: 本文介绍了PNGT-26K数据集以及两个框架，即Open Gender Detection和Nominalist，用于解决波斯名字在自然语言处理中的挑战。这些资源已在Github上公开。

Abstract: Persian names present unique challenges for natural language processing
applications, particularly in gender detection and digital identity creation,
due to transliteration inconsistencies and cultural-specific naming patterns.
Existing tools exhibit significant performance degradation on Persian names,
while the scarcity of comprehensive datasets further compounds these
limitations. To address these challenges, the present research introduces
PNGT-26K, a comprehensive dataset of Persian names, their commonly associated
gender, and their English transliteration, consisting of approximately 26,000
tuples. As a demonstration of how this resource can be utilized, we also
introduce two frameworks, namely Open Gender Detection and Nominalist. Open
Gender Detection is a production-grade, ready-to-use framework for using
existing data from a user, such as profile photo and name, to give a
probabilistic guess about the person's gender. Nominalist, the second framework
introduced by this paper, utilizes agentic AI to help users choose a username
for their social media accounts on any platform. It can be easily integrated
into any website to provide a better user experience. The PNGT-26K dataset,
Nominalist and Open Gender Detection frameworks are publicly available on
Github.

</details>


### [104] [AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs](https://arxiv.org/abs/2509.11155)
*Santhosh G S,Saurav Prakash,Balaraman Ravindran*

Main category: cs.LG

TL;DR: AQUA is a novel approximation strategy that reduces the cost of attention in Large Language Models (LLMs) with a graceful performance trade-off, enabling more efficient and sustainable large-scale LLM inference.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of the attention mechanism remains a fundamental barrier to scaling Large Language Models (LLMs) to longer contexts, creating a critical bottleneck in both computation and memory.

Method: AQUA is a novel approximation strategy that reduces the cost of attention with a graceful performance trade-off. It operates in two phases: an efficient offline step where a universal projection matrix is computed via SVD on a calibration dataset, and an online inference step where query and key vectors are projected and a sparse subset of dimensions is dynamically selected based on the query's magnitude.

Result: Empirical evaluations on state-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in the attention dot-product computation can be achieved with a statistically insignificant impact on performance across a wide range of benchmarks. AQUA also shows versatility by synergistically accelerating existing token eviction methods like H2O and directly reducing KV-cache memory size.

Conclusion: AQUA provides a practical and powerful tool for making large-scale LLM inference more accessible and sustainable by offering a controllable knob to balance efficiency and accuracy.

Abstract: The quadratic complexity of the attention mechanism remains a fundamental
barrier to scaling Large Language Models (LLMs) to longer contexts, creating a
critical bottleneck in both computation and memory. To address this, we
introduce AQUA (Attention via QUery mAgnitudes) a novel and versatile
approximation strategy that significantly reduces the cost of attention with a
graceful performance trade-off. Our method operates in two phases: an efficient
offline step where we compute a universal, language agnostic projection matrix
via SVD on a calibration dataset, and an online inference step where we project
query and key vectors and dynamically select a sparse subset of dimensions
based on the query's magnitude. We provide a formal theoretical analysis of
AQUA, establishing the break-even point at which it becomes more
computationally efficient than standard attention. Our empirical evaluations on
state-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in
the attention dot-product computation can be achieved with a statistically
insignificant impact on performance across a wide range of benchmarks. We
further showcase the versatility of AQUA by demonstrating its ability to
synergistically accelerate existing token eviction methods like H2O and to
directly reduce KV-cache memory size. By offering a controllable knob to
balance efficiency and accuracy, AQUA provides a practical and powerful tool
for making large-scale LLM inference more accessible and sustainable.

</details>


### [105] [Opal: An Operator Algebra View of RLHF](https://arxiv.org/abs/2509.11298)
*Madhava Gaikwad*

Main category: cs.LG

TL;DR: Opal和GKPO为RLHF提供了一个统一的框架，有助于理解和比较不同的RLHF技术。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种统一的框架来理解和比较不同的RLHF技术，同时处理非可约性问题。

Method: Opal通过将目标表示为基于基础效用的两个原始操作的阶梯来表达，而GKPO则提供了一种标准的JSON序列化和规范化规则。

Result: 展示了GKPO在DPO、RRHF和ORPO中的应用，并进行了跨方法转换和最小压力测试。

Conclusion: Opal和GKPO为RLHF方法提供了统一的框架，有助于理解和比较不同的RLHF技术。

Abstract: We present Opal, an operator view of reinforcement learning from human
feedback (RLHF). Objectives are expressed as ladders of two primitives on a
base utility: additive penalties and multiplicative pairwise weights. We
describe a simple reduction law with if-and-only-if conditions: such ladders
collapse to a normal form on pairwise margins when the reference is fixed,
penalties are additive, and weights are independent of intermediate margins.
When these assumptions do not hold (reference shift, non-additive gates,
score-dependent weights), small examples demonstrate non-reducibility.
  Building on this view, we introduce GKPO (Generalized Kernel Preference
Object), a canonical schema in which many RLHF methods can be represented and,
when reducible, mapped back from. GKPO provides a standard JSON serialization,
canonicalization and hashing rules, and explicit flags with finite witnesses
when assumptions fail.
  We illustrate these ideas with GKPO examples for DPO, RRHF, and ORPO, along
with cross-method conversions (where assumptions permit) and minimal stress
tests (SHIFT/GATE/SCORE) that highlight non-reducibility. A lightweight Python
reference library accompanies the schema, implementing canonical hashing and
adapters for DPO and RRHF.

</details>


### [106] [Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting](https://arxiv.org/abs/2509.11452)
*Yining Lu,Zilong Wang,Shiyang Li,Xin Liu,Changlong Yu,Qingyu Yin,Zhan Shi,Zixuan Zhang,Meng Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种动态奖励加权方法，用于解决多目标强化学习中的非凸帕累托前沿问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标强化学习方法通常使用固定权重的线性奖励标量化，这无法捕捉非凸帕累托前沿，导致结果次优。特别是在大型语言模型的在线偏好对齐中，这种限制尤为关键。

Method: 本文引入了动态奖励加权方法，通过自适应调整奖励权重来解决多目标强化学习中的非凸帕累托前沿问题。具体提出了两种方法：(1) 基于超体积的权重适应和(2) 基于梯度的权重优化。

Result: 实验结果表明，本文提出的动态奖励加权方法在多个数学推理数据集上表现出色，并且能够有效探索目标空间中的帕累托前沿。此外，该方法在较少的训练步骤下就能实现帕累托占优解。

Conclusion: 本文提出了一种动态奖励加权方法，以解决多目标强化学习中固定权重线性奖励标量化方法的局限性。实验结果表明，该方法在多个数学推理数据集上表现优异，并且能够有效地探索目标空间中的帕累托前沿。

Abstract: Prior works in multi-objective reinforcement learning typically use linear
reward scalarization with fixed weights, which provably fail to capture
non-convex Pareto fronts and thus yield suboptimal results. This limitation
becomes especially critical in online preference alignment for large language
models. Here, stochastic trajectories generated by parameterized policies
create highly non-linear and non-convex mappings from parameters to objectives
that no single static weighting scheme can find optimal trade-offs. We address
this limitation by introducing dynamic reward weighting, which adaptively
adjusts reward weights during the online reinforcement learning process. Unlike
existing approaches that rely on fixed-weight interpolation, our dynamic
weighting continuously balances and prioritizes objectives in training,
facilitating effective exploration of Pareto fronts in objective space. We
introduce two approaches of increasing sophistication and generalizability: (1)
hypervolume-guided weight adaptation and (2) gradient-based weight
optimization, offering a versatile toolkit for online multi-objective
alignment. Our extensive experiments demonstrate their compatibility with
commonly used online reinforcement learning algorithms (including GRPO,
REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning
datasets, and applicability to different model families, consistently achieving
Pareto dominant solutions with fewer training steps than fixed-weight linear
scalarization baselines.

</details>


### [107] [Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs](https://arxiv.org/abs/2509.11667)
*HG Ranjani,Rutuja Prabhudesai*

Main category: cs.LG

TL;DR: 本文提出了用于衡量此类转换有效性的性能指标，并通过比较两种VLM生成的puml输出与手动创建的地面真实表示，验证了这些指标的有效性。结果显示，虽然节点、边和消息被准确捕获，但VLMs在处理复杂结构方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 电信领域3GPP文档中充满了包含序列图的图像。视觉语言大型模型（VLMs）的进步使得将这些图像转换为机器可读的PlantUML（puml）格式变得容易。然而，这种转换的评估存在差距——现有工作没有比较各种组件的puml脚本。

Method: 我们选择了来自3GPP文档的序列图数据集，以代表特定领域的实际场景。我们比较了Claude Sonnet和GPT-4V两种VLM生成的puml输出与手动创建的地面真实表示。我们使用版本控制工具捕捉差异，并引入标准性能指标来测量各个组件的准确性：参与者识别、消息流准确性、序列排序和分组结构保留。

Result: 结果表明，节点、边和消息被准确捕获。然而，我们观察到VLMs在复杂结构如注释、框、组等方面的表现并不一定很好。

Conclusion: 我们的实验和性能指标表明，需要在训练数据中更好地表示这些组件以微调VLMs。

Abstract: Telecom domain 3GPP documents are replete with images containing sequence
diagrams. Advances in Vision-Language Large Models (VLMs) have eased conversion
of such images to machine-readable PlantUML (puml) formats. However, there is a
gap in evaluation of such conversions - existing works do not compare puml
scripts for various components. In this work, we propose performance metrics to
measure the effectiveness of such conversions. A dataset of sequence diagrams
from 3GPP documents is chosen to be representative of domain-specific actual
scenarios. We compare puml outputs from two VLMs - Claude Sonnet and GPT-4V -
against manually created ground truth representations. We use version control
tools to capture differences and introduce standard performance metrics to
measure accuracies along various components: participant identification,
message flow accuracy, sequence ordering, and grouping construct preservation.
We demonstrate effectiveness of proposed metrics in quantifying conversion
errors across various components of puml scripts. The results show that nodes,
edges and messages are accurately captured. However, we observe that VLMs do
not necessarily perform well on complex structures such as notes, box, groups.
Our experiments and performance metrics indicates a need for better
representation of these components in training data for fine-tuned VLMs.

</details>


### [108] [Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning](https://arxiv.org/abs/2509.11816)
*Filip Sondej,Yushi Yang*

Main category: cs.LG

TL;DR: 本文提出了一种高效的遗忘技术，能够在消除危险知识的同时保持模型的整体性能。


<details>
  <summary>Details</summary>
Motivation: 当前的遗忘技术和安全训练无法有效消除语言模型中的危险知识，因此需要一种更有效的解决方案。

Method: 通过在计算遗忘更新之前对激活和模块输出梯度进行PCA分析，识别包含常见表示的子空间，并将其压缩，从而避免遗忘通用表示，仅针对未学习的事实进行操作。

Result: 在从Llama-3.1-8B中遗忘WMDP数据集的事实时，相比最佳基线（Circuit Breakers），在生物危害事实上的攻击后准确率下降了80倍，在网络危害事实上的攻击后准确率下降了30倍。同时，整体性能的干扰减少了30倍（仅增加0.1%的WikiText损失），并且每个事实所需的计算资源不到3 GPU秒。

Conclusion: 本文提出了一种高度选择性的遗忘技术，能够有效地消除语言模型中的危险知识，同时不会破坏整体性能。

Abstract: Current unlearning techniques and safety training consistently fail to remove
dangerous knowledge from language models. We analyze the root causes and
propose a highly selective technique which unlearns robustly and without
disrupting general performance.
  We perform PCA on activations and module output gradients to identify
subspaces containing common representations, and collapse them before
calculating unlearning updates. This way we avoid unlearning general
representations, and only target those specific to the unlearned facts.
  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack
accuracy 80x more than our best baseline (Circuit Breakers) on biohazardous
facts and 30x more on cyberhazardous facts. Despite this, we disrupt general
performance 30x less (only 0.1% WikiText loss increase), while requiring less
than 3 GPU-seconds per fact.

</details>


### [109] [MillStone: How Open-Minded Are LLMs?](https://arxiv.org/abs/2509.11967)
*Harold Triedman,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: This paper introduces MillSTONE, a benchmark to evaluate how LLMs are influenced by external arguments on controversial issues, revealing that LLMs are open-minded but susceptible to manipulation through authoritative sources.


<details>
  <summary>Details</summary>
Motivation: As users rely on LLMs for information on various topics, including controversial issues, it is crucial to understand how LLM outputs are influenced by their information sources.

Method: The paper presents MillSTONE, the first benchmark to measure the effect of external arguments on the stances that LLMs take on controversial issues. It applies MillSTONE to nine leading LLMs to evaluate their openness to arguments supporting opposite sides, agreement among LLMs, persuasive arguments, and consistency across LLMs.

Result: LLMs are generally open-minded, but authoritative sources can significantly influence their stances, indicating the potential for manipulation in LLM-based systems.

Conclusion: LLMs are open-minded on most issues, but authoritative sources can easily sway their stances, highlighting the importance of source selection and the risk of manipulation in LLM-based information retrieval and search systems.

Abstract: Large language models equipped with Web search, information retrieval tools,
and other agentic capabilities are beginning to supplant traditional search
engines. As users start to rely on LLMs for information on many topics,
including controversial and debatable issues, it is important to understand how
the stances and opinions expressed in LLM outputs are influenced by the
documents they use as their information sources.
  In this paper, we present MillStone, the first benchmark that aims to
systematically measure the effect of external arguments on the stances that
LLMs take on controversial issues (not all of them political). We apply
MillStone to nine leading LLMs and measure how ``open-minded'' they are to
arguments supporting opposite sides of these issues, whether different LLMs
agree with each other, which arguments LLMs find most persuasive, and whether
these arguments are the same for different LLMs.
  In general, we find that LLMs are open-minded on most issues. An
authoritative source of information can easily sway an LLM's stance,
highlighting the importance of source selection and the risk that LLM-based
information retrieval and search systems can be manipulated.

</details>


### [110] [AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models](https://arxiv.org/abs/2509.12019)
*Sangjun Lee,Seung-taek Woo,Jungyu Jin,Changhun Lee,Eunhyeok Park*

Main category: cs.LG

TL;DR: AMQ 是一种自动化混合精度权重仅量化框架，用于在严格内存约束下找到最佳性能的大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 为了在严格的内存限制下实现大型语言模型（LLM）的更广泛部署，需要确定最佳性能模型。

Method: AMQ 是一个框架，通过分配逐层量化位宽来平衡模型质量和内存使用。它通过四个关键创新克服了组合搜索空间的挑战：(1) 使用先验知识进行搜索空间剪枝，(2) 量化代理以绕过搜索期间的昂贵格式转换，(3) 质量预测器以最小化评估开销，(4) 迭代搜索和更新策略以实现快速稳定的收敛。

Result: AMQ 能够高效地探索质量-效率的景观，达到帕累托前沿，并产生紧凑且高性能的 LLM。

Conclusion: AMQ 通过整合这些组件，有效地探索了质量-效率的景观，达到了帕累托前沿，并产生了既紧凑又高性能的 LLM。

Abstract: To enable broader deployment of Large Language Models (LLMs), it is essential
to identify the best-performing model under strict memory constraints. We
present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework
that assigns layer-wise quantization bit-widths to optimally balance model
quality and memory usage. However, the combinatorial search space, with over
10^{100} possible configurations, makes conventional black-box optimization
infeasible. AMQ overcomes this challenge through four key innovations:(1)
search space pruning using prior knowledge to exclude unpromising
configurations, (2) quantization proxy to bypass costly format conversions
during search, (3) quality predictor to minimize evaluation overhead, and (4)
iterative search-and-update strategy for fast and stable convergence. By
integrating these components, AMQ efficiently explores the quality-efficiency
landscape, reaching the Pareto frontier and yielding LLMs that are both compact
and high-performing. Our code is available at https://github.com/dlwns147/amq.

</details>


### [111] [Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences](https://arxiv.org/abs/2509.12188)
*Antonin Sulc*

Main category: cs.LG

TL;DR: 本文介绍了Event2Vec框架，用于学习离散事件序列的表示，并在欧几里得和双曲空间中进行了实验，结果显示双曲模型在处理层次化数据时表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究神经表示在生物和人工系统中的几何和拓扑结构的重要性，旨在开发一种能够有效学习离散事件序列表示的新方法。

Method: 我们引入了Event2Vec框架，利用简单的加性递归结构学习可组合、可解释的嵌入。此外，我们还提出了一个在双曲空间中的变体模型，以更好地处理层次化数据。

Result: 实验验证了我们的假设，并展示了每种几何的优势，特别是双曲模型在层次化事件序列上的性能提升。

Conclusion: 我们的模型在欧几里得空间和双曲空间中都展示了优越的性能，特别是在处理层次化事件序列时，双曲模型表现更佳。

Abstract: The study of neural representations, both in biological and artificial
systems, is increasingly revealing the importance of geometric and topological
structures. Inspired by this, we introduce Event2Vec, a novel framework for
learning representations of discrete event sequences. Our model leverages a
simple, additive recurrent structure to learn composable, interpretable
embeddings. We provide a theoretical analysis demonstrating that, under
specific training objectives, our model's learned representations in a
Euclidean space converge to an ideal additive structure. This ensures that the
representation of a sequence is the vector sum of its constituent events, a
property we term the linear additive hypothesis. To address the limitations of
Euclidean geometry for hierarchical data, we also introduce a variant of our
model in hyperbolic space, which is naturally suited to embedding tree-like
structures with low distortion. We present experiments to validate our
hypothesis and demonstrate the benefits of each geometry, highlighting the
improved performance of the hyperbolic model on hierarchical event sequences.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [112] [RadarLLM: Adapting Pretrained Large Language Models for Marine Radar Target Detection with Preference-aware Loss](https://arxiv.org/abs/2509.12089)
*Qiying Hu*

Main category: eess.SP

TL;DR: 本文提出了一种名为RadarLLM的新微调框架，利用有效的偏好感知损失函数，通过在线评估学习值选择性地优化不同的特征块，从而引导模型在优化过程中关注最可泛化的模式。实验结果表明，该方法在具有挑战性的低信杂比场景下表现出色，并且在有限训练数据条件下优于现有最佳基线方法。


<details>
  <summary>Details</summary>
Motivation: Recent advances in pre-trained large language models (LLMs) have demonstrated their capacities to capture universal knowledge, making them promising general-purpose optimization solvers for wireless signal processing. Motivated by these findings, we take the first step towards fine-tuning pre-trained LLMs for the effective analysis of radar signal features in marine target detection tasks.

Method: RadarLLM, a novel fine-tuning framework that utilizes an effective preference-aware loss. Unlike conventional training strategies that uniformly optimize all feature tokens, this loss function selectively optimizes different feature patches based on their online evaluated learning values.

Result: The proposed loss function is much better than the original one, with particularly significant gains in challenging low SCR scenarios. RadarLLM consistently outperforms state-of-the-art baselines across diverse detection scenarios, with particularly notable gains under limited training data conditions.

Conclusion: RadarLLM consistently outperforms state-of-the-art baselines across diverse detection scenarios, with particularly notable gains under limited training data conditions.

Abstract: Recent advances in pre-trained large language models (LLMs) have demonstrated
their capacities to capture universal knowledge, making them promising
general-purpose optimization solvers for wireless signal processing. Motivated
by these findings, we take the first step towards fine-tuning pre-trained LLMs
for the effective analysis of radar signal features in marine target detection
tasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target
detection tasks tends to suffer from pronounced overfitting, particularly in
challenging low signal-to-clutter ratio (SCR) scenarios. This overfitting
primarily stems from the model's tendency to memorize spurious or noisy feature
patterns rather than learning discriminative structures that generalize well to
unseen data. To address this challenge, we introduce RadarLLM, a novel
fine-tuning framework that utilizes an effective preference-aware loss. Unlike
conventional training strategies that uniformly optimize all feature tokens,
this loss function selectively optimizes different feature patches based on
their online evaluated learning values, thus guiding the model to focus on the
most generalizable patterns during optimization. We theoretically demonstrate
the effectiveness of the evaluated learning values by transforming the problem
as selecting useful feature tokens. Extensive experiments on real-world marine
radar datasets show that 1) the proposed loss function is much better than the
original one, with particularly significant gains in challenging low SCR
scenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines
across diverse detection scenarios, with particularly notable gains under
limited training data conditions.

</details>


### [113] [When marine radar target detection meets pretrained large language models](https://arxiv.org/abs/2509.12110)
*Qiying Hu,Linping Zhang,Xueqian Wang,Gang Li,Yu Liu,Xiao-Ping Zhang*

Main category: eess.SP

TL;DR: 本文提出了一种将特征预处理与大型语言模型集成的框架，以解决传统深度学习算法在雷达回波信号处理中的挑战。通过优化特征表示和微调预训练模型，该方法在监督学习任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习算法面临冗余特征段和受限模型大小的约束。

Method: 我们提出了一个框架，将特征预处理与大型语言模型（LLMs）集成。我们的预处理模块对雷达序列特征进行分词，应用补丁选择算法过滤掉无信息的段，并将选定的补丁投影到与预训练LLMs特征空间兼容的嵌入中。利用这些优化后的嵌入，我们引入了一个预训练的LLM，仅微调归一化层以减少训练负担同时提高性能。

Result: 实验结果表明，所提出的方法在监督学习测试中显著优于最先进的基线方法。

Conclusion: 实验结果表明，所提出的方法在监督学习测试中显著优于最先进的基线方法。

Abstract: Deep learning (DL) methods are widely used to extract high-dimensional
patterns from the sequence features of radar echo signals. However,
conventional DL algorithms face challenges such as redundant feature segments,
and constraints from restricted model sizes. To address these issues, we
propose a framework that integrates feature preprocessing with large language
models (LLMs). Our preprocessing module tokenizes radar sequence features,
applies a patch selection algorithm to filter out uninformative segments, and
projects the selected patches into embeddings compatible with the feature space
of pre-trained LLMs. Leveraging these refined embeddings, we incorporate a
pre-trained LLM, fine-tuning only the normalization layers to reduce training
burdens while enhancing performance. Experiments on measured datasets
demonstrate that the proposed method significantly outperforms the
state-of-the-art baselines on supervised learning tests.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 本研究分析了不同AI模型在评估视觉语言描述时的评估个性和策略，发现GPT模型和Gemini在评估策略上存在显著差异，且GPT模型倾向于负面评估。这些发现表明，评估能力并不随着通用能力的提升而增加，稳健的AI评估需要多样化的架构视角。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地评估其他AI输出，理解其评估行为对于防止级联偏差变得至关重要。

Method: 本研究分析了NVIDIA的Describe Anything Model生成的视觉语言描述，并由三个GPT变体（GPT-4o、GPT-4o-mini、GPT-5）进行评估，以揭示每个模型展示出的不同“评估个性”和底层评估策略及偏见。通过使用Gemini 2.5 Pro作为独立的问题生成器进行受控实验，验证了这些个性是固有的模型属性。通过生成问题的语义相似性进行跨家族分析，揭示了显著的分歧：GPT模型聚类在一起，具有高相似性，而Gemini表现出明显不同的评估策略。

Result: GPT-4o-mini表现出系统的一致性，方差最小；GPT-4o在错误检测方面表现出色；而GPT-5则表现出极端保守且高度变化。所有GPT模型都表现出一致的2:1偏差，偏向负面评估而非正面确认，尽管这种模式似乎属于特定家族，而不是跨AI架构的普遍现象。

Conclusion: 这些发现表明，评估能力并不随着通用能力的提升而增加，而且稳健的AI评估需要多样化的架构视角。

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [115] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 本研究评估了18种代理配置，发现代理系统在企业任务中的性能存在显著弱点，并指出需要更实证支持的架构设计和模型选择。


<details>
  <summary>Details</summary>
Motivation: 目前对复杂多代理系统中不同设计维度的相互作用缺乏实证理解，因此本研究旨在填补这一空白。

Method: 本研究通过一个全面的企业特定基准测试，评估了18种不同的代理配置，涵盖了四个关键的代理系统维度：编排策略、代理提示实现（ReAct与函数调用）、记忆架构和思考工具集成。

Result: 基准测试揭示了模型特定的架构偏好，挑战了代理AI系统中普遍存在的单一解决方案范式。最高评分模型在更复杂的任务上的成功率为35.3%，在简单任务上的成功率为70.8%。

Conclusion: 本研究揭示了企业任务中代理系统性能的显著弱点，并希望这些发现能通过提供更实证支持的决策来指导未来代理系统的设计。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [116] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 本文提出了一种名为HaPLa的新型越狱技术，能够有效攻击LLM，并揭示了在保持LLM对良性查询的有用性的同时进行安全调整的困难。


<details>
  <summary>Details</summary>
Motivation: 为了加强防御针对LLM潜在漏洞的滥用，有必要研究通用的越狱攻击，这些攻击利用LLM架构和学习范式中的内在弱点。

Method: 提出了一种名为HaPLa的新颖且广泛适用的越狱技术，该技术仅需要对目标模型的黑盒访问权限。HaPLa结合了两种主要策略：1) 归纳框架，指导LLM推断有害活动的合理中间步骤；2) 符号编码，一种轻量级且灵活的方法，用于隐藏有害内容。

Result: 实验结果表明，HaPLa在GPT系列模型上的攻击成功率超过95%，在所有目标上的成功率超过70%。进一步分析显示，安全调整LLM仍然面临挑战。

Conclusion: 研究发现，即使在使用符号编码等方法进行安全调整的情况下，也很难在不显著降低LLM对良性查询的有用性的同时，安全地调整LLM。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [117] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 本文提出了一种私有ICL算法，在保持DP保证的同时利用公共数据提高模型效用，并展示其对成员推理攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在不进行微调的情况下，ICL在各种任务中表现出色，但存在私人数据泄露的风险。DP虽然提供强隐私保证，但通常会显著降低ICL的效用。

Method: 我们将与任务相关的公共数据引入ICL框架，同时保持DP保证，提出了一种私有ICL算法。

Result: 实验表明，借助公共数据，我们的方法显著提高了私有ICL的效用，并且对成员推理攻击具有鲁棒性。

Conclusion: 我们的方法在保持隐私保护的同时显著提高了私有ICL的实用性，并且对成员推理攻击具有鲁棒性。

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [118] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 本文研究了如何通过细粒度属性评估来改进对LLM生成推理的评估，以更好地理解推理质量并指导未来的评估方法。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的推理仍然具有挑战性。虽然最近的工作依赖于人类或LLM裁判的二元偏好判断，但这种评估通常是不透明且粗略的，无法提供关于什么使一个推理优于另一个的深入见解。

Method: 我们从先前文献中确定了一组关键的推理属性，并使用自动度量、LLM判断和人工注释来评估它们。然后，我们使用SHAP分析两个标准的人类偏好数据集MT Bench和Chatbot Arena，以确定哪些属性最能解释人类偏好结果。最后，我们使用基于属性的ELO分数重新评估模型生成的推理，揭示了更细致的模型比较和见解。

Result: 我们发现，细粒度的属性评估可以更好地表征推理质量，并指导未来的研究朝着更可解释和可靠的评估实践发展。

Conclusion: 我们的研究结果表明，细粒度的属性评估可以更好地表征推理质量，并指导未来的研究朝着更可解释和可靠的评估实践发展。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [119] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: 本文提出了一种将基于角色的访问控制（RBAC）集成到AI代理中的框架，以提高其安全性并支持有效和可扩展的部署。


<details>
  <summary>Details</summary>
Motivation: 尽管AI代理在工业应用中带来了显著的进步，但它们仍然容易受到安全威胁，如提示注入攻击，这对其完整性和可靠性构成了重大风险。因此，需要一种有效的安全机制来保护AI代理。

Method: 本文提出了一个框架，将基于角色的访问控制（RBAC）集成到AI代理中，以增强其安全性。

Result: 本文提出的框架能够为AI代理提供强大的安全防护，从而提高其在工业环境中的可靠性和安全性。

Conclusion: 本文提出了一种将基于角色的访问控制（RBAC）集成到AI代理中的框架，以提供强大的安全防护，并支持AI代理的有效和可扩展部署，特别是在本地实施方面。

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [120] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: 本文提出了MCFR（模型检查用于形式推理），这是一种将大型语言模型与模型检查相结合的神经符号框架，以支持属性验证。


<details>
  <summary>Details</summary>
Motivation: Reasoning is essential for closed-domain QA systems in which procedural correctness and policy compliance are critical. While large language models (LLMs) have shown strong performance on many reasoning tasks, their reasoning traces are often unfaithful.

Method: MCFR (Model Checking for Formal Reasoning), a neuro-symbolic framework that integrates LLMs with model checking to support property verification.

Result: MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications.

Conclusion: MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications.

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [121] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 本文介绍了一种新的评估指标RPAD和RRAD，用于更准确地评估医疗AI的性能。研究发现，顶级AI模型在一致性方面可以达到甚至超过专家水平，同时展示了专家判断的变异性。


<details>
  <summary>Details</summary>
Motivation: 传统指标如精确率和召回率无法考虑专家判断的内在变异性，导致对AI性能的评估不一致。而互评一致性统计量如Cohen's Kappa虽然更可靠，但缺乏可解释性。因此，我们需要一种更稳定和现实的评估方法。

Method: 我们引入了RPAD和RRAD这两个新的评估指标，它们将AI输出与多个专家意见进行比较，而不是单一参考。此外，我们还开发了一种自动化的方法来确定自由形式的临床诊断身份。

Result: 我们的方法在360个医学对话中进行了评估，结果显示顶级模型如DeepSeek-V3在一致性方面达到了专家共识的水平或超过了专家共识。此外，专家判断表现出显著的变异性，有时甚至大于AI与人类之间的差异。

Conclusion: 我们的研究结果表明，相对指标在医疗AI评估中比绝对指标更有效，因为它们考虑了专家判断的内在变异性。此外，我们的方法允许模型和评估者生成自由形式的诊断，从而实现了高准确率。

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [122] [FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval](https://arxiv.org/abs/2509.12042)
*Ying Li,Mengyu Wang,Miguel de Carvalho,Sotirios Sabanis,Tiejun Ma*

Main category: cs.CE

TL;DR: FinGEAR是一种针对金融文档的检索框架，通过结合金融词典、双层次索引和两阶段交叉编码器重新排序，显著提升了检索精度和相关性。


<details>
  <summary>Details</summary>
Motivation: 财务披露如10-K文件由于其长度、监管部分层次结构和领域特定语言，标准检索增强生成（RAG）模型未能充分利用。

Method: FinGEAR结合了金融词典用于项目级指导（FLAM）、双层次索引用于项目内搜索（摘要树和问题树），以及两阶段交叉编码器重新排序。

Result: FinGEAR在与FinQA数据集对齐的完整10-K文件上评估，实现了精确度、召回率、F1和相关性的持续提升，F1比平铺RAG提高了56.7%，比基于图的RAG提高了12.5%，比之前的基于树的系统提高了217.6%。

Conclusion: FinGEAR通过联合建模章节层次和领域词典信号，提高了检索精度，并为高风险的金融分析提供了实用的基础。

Abstract: Financial disclosures such as 10-K filings present challenging retrieval
problems due to their length, regulatory section hierarchy, and domain-specific
language, which standard retrieval-augmented generation (RAG) models underuse.
We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a
retrieval framework tailored to financial documents. FinGEAR combines a finance
lexicon for Item-level guidance (FLAM), dual hierarchical indices for
within-Item search (Summary Tree and Question Tree), and a two-stage
cross-encoder reranker. This design aligns retrieval with disclosure structure
and terminology, enabling fine-grained, query-aware context selection.
Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR
delivers consistent gains in precision, recall, F1, and relevancy, improving F1
by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over
prior tree-based systems, while also increasing downstream answer accuracy with
a fixed reader. By jointly modeling section hierarchy and domain lexicon
signals, FinGEAR improves retrieval fidelity and provides a practical
foundation for high-stakes financial analysis.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [123] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 本文介绍了我们在CVPR 2024 Autonomous Grand Challenge的Driving with Language赛道中使用视觉语言模型系统的方法，通过微调LLaVA模型并结合深度信息，采用Chain-of-Thought推理方法，最终取得了第一名的成绩。


<details>
  <summary>Details</summary>
Motivation: 为了在CVPR 2024 Autonomous Grand Challenge的Driving with Language赛道中取得优异成绩，我们需要一种高效且准确的方法来处理多选题和是/否问题。

Method: 我们使用了DriveLM-nuScenes数据集进行训练，并基于LLaVA模型进行了微调，采用了LoRA和DoRA方法。此外，我们还集成了开源深度估计模型的深度信息以丰富训练和推理过程。对于推理，我们采用了Chain-of-Thought推理方法来提高结果的准确性。

Result: 我们的方法在验证集上获得了0.7799的高分，并在排行榜上排名第一。

Conclusion: 我们的方法在CVPR 2024 Autonomous Grand Challenge的Driving with Language赛道中取得了第一名的成绩，验证集得分为0.7799。

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [124] [Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations](https://arxiv.org/abs/2509.11287)
*Yifan Lu,Ziqi Zhang,Chunfeng Yuan,Jun Gao,Congxuan Zhang,Xiaojuan Qi,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: APASI is a novel method that mitigates hallucinations in large vision-language models without external dependencies by self-injecting hallucinations into generated responses and using an iterative alignment training strategy.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination mitigation methods are mainly based on preference alignment and require external human annotations or auxiliary models for preference data collection, which increase costs and limit sustainable improvement.

Method: APASI leverages the target LVLM to self-inject hallucinations into a generated response, creating a pair of responses with varying preference levels. Moreover, APASI incorporates an iterative alignment training strategy combined with curriculum learning to periodically update the preference data with increasing challenge.

Result: Extensive experiments across six benchmarks show that APASI not only effectively mitigates hallucinations for three baseline models but also achieves comparable or even superior performance to alignment-based methods with external dependency.

Conclusion: APASI not only effectively mitigates hallucinations for three baseline models but also achieves comparable or even superior performance to alignment-based methods with external dependency, thereby demonstrating its effectiveness and generalization capability.

Abstract: Large Vision-Language Models (LVLMs) suffer from serious hallucination
problems, where the model-generated responses are inconsistent with the visual
inputs. Existing hallucination mitigation methods are mainly based on
preference alignment and require external human annotations or auxiliary models
for preference data collection, which increase costs and limit sustainable
improvement. To tackle these challenges, we propose Autonomous Preference
Alignment via Self-Injection (APASI), a novel and generalizable method that
mitigates hallucinations without external dependencies. APASI leverages the
target LVLM to self-inject hallucinations into a generated response, creating a
pair of responses with varying preference levels. During the self-injection
process, the dis-preferred response is generated based on three key
observations of hallucinations, ensuring it simulates real hallucination
patterns. This fidelity offers an accurate learning signal for hallucination
mitigation. Moreover, APASI incorporates an iterative alignment training
strategy combined with curriculum learning to periodically update the
preference data with increasing challenge, enabling stable and continuous
enhancement of the LVLM. Extensive experiments across six benchmarks show that
APASI not only effectively mitigates hallucinations for three baseline models
but also achieves comparable or even superior performance to alignment-based
methods with external dependency, thereby demonstrating its effectiveness and
generalization capability. The code is available at
https://github.com/davidluciolu/APASI.

</details>


### [125] [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
*Feilong Chen,Yijiang Liu,Yi Huang,Hao Wang,Miren Tian,Ya-Qi Yu,Minghui Liao,Jihao Wu*

Main category: cs.CV

TL;DR: MindVL is a multimodal large language model that processes images at native resolution, uses a tailored training framework, and achieves strong performance with less training data.


<details>
  <summary>Details</summary>
Motivation: To process images at their original variable resolutions without degradation, and to ensure smooth training on Ascend NPUs while maintaining accuracy.

Method: MindVL adopts native-resolution Vision Transformers and uses a distributed multimodal training framework called Mindspeed-MLLM. It undergoes a three-phase training process, including warm-up, multitask training, and supervised instruction tuning. Multimodal data packaging and hybrid parallelism techniques are also used.

Result: MindVL achieves performance on par with Qwen2.5-VL in general multimodal understanding and document/table comprehension, and leads in OCR assessments.

Conclusion: MindVL achieves performance on par with Qwen2.5-VL despite using about 1/10 of the training data, and delivers leading performance in OCR assessments.

Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.
Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,
which enables it to process images at their original variable resolutions. This
design avoids the degradation caused by fixed-resolution tiling while
preserving fine-grained details and global layouts, which is crucial for
visually dense content such as complex charts and diagrams. To ensure the
smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a
distributed multimodal training framework tailored for Ascend NPUs. To maintain
training accuracy, we implement equivalent replacements for certain operators.
MindVL undergoes a three-phase training process, namely the warm-up phase,
multitask training phase, and supervised instruction tuning phase, to gradually
enhance its capabilities. This process starts with basic visual and multimodal
pre-training, followed by large-scale multiask trainging and instruction
tuning. We also adopt multimodal data packaging and hybrid parallelism
techniques, which significantly improve end-to-end training speed. To further
boost model performance, we specifically introduce test-time resolution search
and model weight averaging. Notably, despite using about 1/10 of the training
data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL
in evaluations of general multimodal understanding and document/table
comprehension. Beyond overall scores, MindVL also delivers leading performance
in OCR assessments.

</details>


### [126] [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986)
*Wenyan Li,Raphael Tang,Chengzu Li,Caiqi Zhang,Ivan Vulić,Anders Søgaard*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型中连接器组件导致的信息损失问题，通过分析潜在表示空间中的k最近邻关系和图像块级别的嵌入重建，揭示了连接器对模型性能的影响，并提供了对模型行为的可解释见解。


<details>
  <summary>Details</summary>
Motivation: 尽管连接器组件对于模态融合至关重要，但由这种投影步骤引起的潜在信息损失及其对模型能力的直接影响仍研究不足。

Method: 我们引入了两种互补的方法来检查和量化这种损失，通过分析潜在表示空间。首先，我们通过分析图像表示在投影前后的k最近邻关系的变化来评估语义信息的保留。其次，我们直接通过从投影表示中重建视觉嵌入来测量信息损失，将损失定位在图像块级别。

Result: 实验表明，连接器显著扭曲了视觉表示的局部几何结构，k最近邻在投影后发生了40-60%的偏差，这与检索性能的下降相关。通过图像块级别的嵌入重建，提供了对模型在视觉基础问答任务中行为的可解释见解，发现高信息损失区域可靠地预测了模型遇到困难的情况。

Conclusion: 实验表明，连接器显著扭曲了视觉表示的局部几何结构，k最近邻在投影后发生了40-60%的偏差，这与检索性能的下降相关。通过图像块级别的嵌入重建，提供了对模型在视觉基础问答任务中行为的可解释见解，发现高信息损失区域可靠地预测了模型遇到困难的情况。

Abstract: Vision--language models (VLMs) often process visual inputs through a
pretrained vision encoder, followed by a projection into the language model's
embedding space via a connector component. While crucial for modality fusion,
the potential information loss induced by this projection step and its direct
impact on model capabilities remain understudied. We introduce two
complementary approaches to examine and quantify this loss by analyzing the
latent representation space. First, we evaluate semantic information
preservation by analyzing changes in k-nearest neighbor relationships between
image representations, before and after projection. Second, we directly measure
information loss by reconstructing visual embeddings from the projected
representation, localizing loss at an image patch level. Experiments reveal
that connectors substantially distort the local geometry of visual
representations, with k-nearest neighbors diverging by 40--60\%
post-projection, correlating with degradation in retrieval performance. The
patch-level embedding reconstruction provides interpretable insights for model
behavior on visually grounded question-answering tasks, finding that areas of
high information loss reliably predict instances where models struggle.

</details>


### [127] [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models](https://arxiv.org/abs/2509.12132)
*Pu Jian,Junhong Wu,Wei Sun,Chen Wang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: This paper proposes Reflection-V, a new VRM that enhances visual reflection by constructing vision-centered reasoning data and using a visual attention based reward model during reinforcement learning, leading to significant improvements in visual reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VRMs exhibit limited visual reflection, as their attention to visual information diminishes rapidly with longer generated responses. Effective 'slow thinking' in VRMs requires visual reflection, the ability to check the reasoning process based on visual information.

Method: Reflection-V enhances visual reflection based on reasoning data construction for cold-start and reward design for reinforcement learning (RL). It constructs vision-centered reasoning data by leveraging an agent that interacts between VLMs and reasoning LLMs, enabling cold-start learning of visual reflection patterns. A visual attention based reward model is employed during RL to encourage reasoning based on visual information.

Result: Reflection-V demonstrates significant improvements across multiple visual reasoning benchmarks. It maintains a stronger and more consistent reliance on visual information during visual reasoning, indicating effective enhancement in visual reflection capabilities.

Conclusion: Reflection-V demonstrates significant improvements across multiple visual reasoning benchmarks and maintains a stronger and more consistent reliance on visual information during visual reasoning, indicating effective enhancement in visual reflection capabilities.

Abstract: Recent advances in text-only "slow-thinking" reasoning have prompted efforts
to transfer this capability to vision-language models (VLMs), for training
visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical
challenges: Effective "slow thinking" in VRMs requires \textbf{visual
reflection}, the ability to check the reasoning process based on visual
information. Through quantitative analysis, we observe that current VRMs
exhibit limited visual reflection, as their attention to visual information
diminishes rapidly with longer generated responses. To address this challenge,
we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection
based on reasoning data construction for cold-start and reward design for
reinforcement learning (RL). Firstly, we construct vision-centered reasoning
data by leveraging an agent that interacts between VLMs and reasoning LLMs,
enabling cold-start learning of visual reflection patterns. Secondly, a visual
attention based reward model is employed during RL to encourage reasoning based
on visual information. Therefore, \textbf{Reflection-V} demonstrates
significant improvements across multiple visual reasoning benchmarks.
Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent
reliance on visual information during visual reasoning, indicating effective
enhancement in visual reflection capabilities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [128] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: DreamNav是一种新的零样本视觉语言导航方法，通过减少感官成本、进行全局轨迹级规划以及引入主动想象能力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本VLN方法依赖于昂贵的感知和被动场景理解，导致控制局限于点级选择，因此部署成本高、动作语义不一致且规划短视。

Method: DreamNav专注于三个方面：(1) 使用EgoView Corrector减少感官成本并稳定自我中心感知；(2) 使用Trajectory Predictor进行全局轨迹级规划以更好地与指令语义对齐；(3) 使用Imagination Predictor实现前瞻性及长时程规划。

Result: DreamNav在VLN-CE和真实世界测试中表现优异，超越了最强的以自我为中心基线，在SR和SPL指标上分别提高了7.49%和18.15%。

Conclusion: DreamNav在VLN-CE和真实世界测试中设定了新的零样本最先进水平（SOTA），超越了最强的以自我为中心基线，分别在SR和SPL指标上提高了7.49%和18.15%。这是第一个将轨迹级规划和主动想象统一起来的零样本VLN方法，仅使用自我中心输入。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
links language instructions to perception and control in the real world, is a
core capability of embodied robots. Recently, large-scale pretrained foundation
models have been leveraged as shared priors for perception, reasoning, and
action, enabling zero-shot VLN without task-specific training. However,
existing zero-shot VLN methods depend on costly perception and passive scene
understanding, collapsing control to point-level choices. As a result, they are
expensive to deploy, misaligned in action semantics, and short-sighted in
planning. To address these issues, we present DreamNav that focuses on the
following three aspects: (1) for reducing sensory cost, our EgoView Corrector
aligns viewpoints and stabilizes egocentric perception; (2) instead of
point-level actions, our Trajectory Predictor favors global trajectory-level
planning to better align with instruction semantics; and (3) to enable
anticipatory and long-horizon planning, we propose an Imagination Predictor to
endow the agent with proactive thinking capability. On VLN-CE and real-world
tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the
strongest egocentric baseline with extra information by up to 7.49\% and
18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first
zero-shot VLN method to unify trajectory-level planning and active imagination
while using only egocentric inputs.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [129] [Why Bonds Fail Differently? Explainable Multimodal Learning for Multi-Class Default Prediction](https://arxiv.org/abs/2509.10802)
*Yi Lu,Aifan Ling,Chaoqun Wang,Yaxin Xu*

Main category: q-fin.RM

TL;DR: 本文提出了一种新的框架EMDLOT，用于多类债券违约预测，通过结合数值时间序列和非结构化文本数据，并采用时间感知LSTM和多级注意力机制，提高了可解释性。实验结果表明，EMDLOT在多个指标上优于传统和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型难以捕捉金融数据的不规则性和时间依赖性，而大多数深度学习模型缺乏可解释性，这对于金融决策至关重要。

Method: 本文提出了EMDLOT（可解释的多模态深度学习时间序列），该框架结合了数值时间序列和非结构化文本数据，并使用了时间感知LSTM来处理不规则序列，同时采用了软聚类和多级注意力机制来提高可解释性。

Result: 在1994家中国公司（2015-2024年）上的实验表明，EMDLOT在召回率、F1分数和mAP方面优于传统的（如XGBoost）和深度学习（如LSTM）基准，特别是在识别违约/延期公司方面。消融研究验证了每个组件的价值，注意力分析揭示了经济上合理的违约驱动因素。

Conclusion: 本文提供了一个实用的工具和一个可信赖的框架，用于透明的金融风险建模。

Abstract: In recent years, China's bond market has seen a surge in defaults amid
regulatory reforms and macroeconomic volatility. Traditional machine learning
models struggle to capture financial data's irregularity and temporal
dependencies, while most deep learning models lack interpretability-critical
for financial decision-making. To tackle these issues, we propose EMDLOT
(Explainable Multimodal Deep Learning for Time-series), a novel framework for
multi-class bond default prediction. EMDLOT integrates numerical time-series
(financial/macroeconomic indicators) and unstructured textual data (bond
prospectuses), uses Time-Aware LSTM to handle irregular sequences, and adopts
soft clustering and multi-level attention to boost interpretability.
Experiments on 1994 Chinese firms (2015-2024) show EMDLOT outperforms
traditional (e.g., XGBoost) and deep learning (e.g., LSTM) benchmarks in
recall, F1-score, and mAP, especially in identifying default/extended firms.
Ablation studies validate each component's value, and attention analyses reveal
economically intuitive default drivers. This work provides a practical tool and
a trustworthy framework for transparent financial risk modeling.

</details>

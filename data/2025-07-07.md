<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文提出了一种多任务中文偏见评估基准（McBE），用于评估大型语言模型中的偏见，并发现这些模型存在不同程度的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的偏见评估数据集主要针对英语和北美文化，其偏见类别不适用于其他文化。此外，这些数据集通常只支持单一评估任务，无法从多个方面评估大型语言模型中的偏见。

Method: 本文提出了一种多任务中文偏见评估基准（McBE），包含4077个偏见评估实例，覆盖12个单一偏见类别、82个子类别，并引入5个评估任务，以提供广泛的类别覆盖、内容多样性和测量全面性。

Result: 本文评估了不同系列和参数大小的多个流行大型语言模型，发现所有这些模型都表现出不同程度的偏见，并进行了深入分析，提供了关于大型语言模型中偏见的新见解。

Conclusion: 本文提出了一个全面的多任务中文偏见评估基准（McBE），并评估了多个流行的大型语言模型，发现它们都表现出不同程度的偏见。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本研究评估了最先进的推理LLM和非推理LLM在对话摘要中的表现，发现显式推理并不总能提高质量，反而可能导致问题。


<details>
  <summary>Details</summary>
Motivation: 对话摘要是一个具有重要实际价值的挑战性任务，但在需要同时抽象和简洁的对话场景中，步骤推理架构（如Long Chain-of-Thought）的性能仍未被探索。

Method: 我们对最先进的推理LLM和非推理LLM进行了全面和系统的评估，涵盖了三种主要范式：通用、角色导向和查询导向的对话摘要。

Result: 我们的研究发现，显式的逐步推理并不总是能提高对话摘要的质量。相反，推理LLM往往比非推理LLM更容易出现冗长、事实不一致和不够简洁的摘要。

Conclusion: 我们的工作提供了对当前推理LLM局限性的新见解，并强调了针对现实世界对话摘要的目标建模和评估策略的必要性。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 本研究分析了Huginn-3.5B模型是否能实现隐式CoT，但发现其表现有限，且存在探测不一致性。


<details>
  <summary>Details</summary>
Motivation: 为了探索递归架构是否能内化推理过程，从而支持隐式CoT，同时提高效率，本研究对Huginn-3.5B模型进行了深入分析。

Method: 通过Logit Lens和Coda Lens等探测技术，研究了Huginn-3.5B模型在算术任务中的内部行为，追踪了最终和中间结果标记的排名轨迹，以分析其是否出现隐式CoT结构。

Result: 研究发现，Huginn-3.5B模型中缺乏明显的可解释隐式CoT证据，且不同递归块之间的隐藏状态可解释性差异较大。增加递归深度仅带来微小的性能提升。

Conclusion: 研究发现，Huginn-3.5B模型在内部行为上表现出有限的可解释性隐式CoT证据，并且在递归块之间存在显著的探测不一致现象。此外，增加递归深度带来的收益微乎其微，远不及那些显式外部化推理步骤的模型。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: GDC Cohort Copilot 是一个开源工具，可以根据自然语言描述自动生成 GDC 队列过滤器，并通过本地部署的开放源代码 GDC 队列大语言模型实现更好的效果。


<details>
  <summary>Details</summary>
Motivation: GDC 用户（尤其是新用户）可能难以在数百个可能的字段和属性中找到特定的队列描述符，而他们可能更擅长用自由文本的自然语言描述他们的目标队列。

Method: 开发并评估了多个大型语言模型 (LLMs) 用于 GDC Cohort Copilot，并展示了本地服务的开源 GDC Cohort LLM 在生成 GDC 队列方面优于 GPT-4o 提示的结果。

Result: GDC Cohort Copilot 能够自动根据用户输入的自然语言描述生成对应的 GDC 队列过滤器，并允许用户通过交互式用户界面进一步优化生成的队列。

Conclusion: GDC Cohort Copilot 是一个用于从 GDC 中策划队列的开源协作工具，它能够根据用户的自然语言描述自动生成 GDC 队列过滤器，并在本地部署的开放源代码 GDC 队列大语言模型 (LLM) 上实现优于 GPT-4o 提示的效果。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: MemAgent是一种新型代理工作流，能够高效处理长文本，具有出色的外推能力和性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决在保持线性复杂度的同时处理无限长文档并避免性能下降的问题。

Method: 引入了一个新的代理工作流MemAgent，通过覆盖策略读取文本并更新内存，并扩展了DAPO算法以实现独立上下文多对话生成训练。

Result: MemAgent能够从8K上下文训练的32K文本中进行外推，在3.5M QA任务中性能损失小于5%，并在512K RULER测试中达到95%以上。

Conclusion: MemAgent展示了出色的长上下文能力，能够在不损失性能的情况下处理无限长的文档。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 本文提出DoMIX方法，通过LoRA模块解决持续DAP方法的局限性，实现高效、并行的领域自适应预训练，并能提供针对特定任务的定制化预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有持续DAP方法存在高计算成本、GPU内存使用、对增量数据顺序的敏感性以及提供单一通用模型的问题，这些问题限制了其在实际应用中的效果和灵活性。

Method: DoMIX方法利用LoRA模块，这是一种参数高效的微调（PEFT）方法，以解决现有持续DAP方法中的高计算成本、GPU内存使用、对增量数据顺序的敏感性以及提供单一通用模型的问题。

Result: DoMIX方法能够实现高效、并行的领域自适应预训练，对领域顺序具有鲁棒性，并有效利用累积知识提供针对特定任务的定制化预训练模型。此外，该方法还可以扩展到标准LLM微调场景中。

Conclusion: 本文提出了DoMIX方法，通过利用LoRA模块解决了现有持续DAP方法的局限性，实现了高效、并行的领域自适应预训练，并能提供针对特定任务的定制化预训练模型。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本文介绍了我们为SciVQA 2025共享任务开发的科学视觉问答系统，该系统采用多模态大语言模型和少量示例检索策略，在盲测数据上取得了第三名的成绩。


<details>
  <summary>Details</summary>
Motivation: 本文描述了我们针对SciVQA 2025共享任务的科学视觉问答系统的开发。

Method: 我们的系统采用了两个多模态大语言模型的集成以及各种少量示例检索策略。

Result: 我们的系统在盲测数据上取得了第三名的好成绩，平均F1分数为85.12。

Conclusion: 我们的系统在盲测数据上排名第三，平均F1分数为85.12。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文提出了QFFN-BERT，一种将PQC应用于Transformer的前馈网络模块的混合量子-经典模型，实验证明它在保持高准确率的同时显著减少了参数数量，并在少样本学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: FFN模块占标准Transformer编码器块中参数的约三分之二，因此我们专注于FFN并系统地研究了PQC深度、表达性和可训练性之间的权衡。

Method: 我们引入了QFFN-BERT，这是一种混合量子-经典变换器，其中紧凑型BERT变体的前馈网络（FFN）模块被基于PQC的层取代。我们的最终PQC架构结合了残差连接、RY和RZ旋转以及交替纠缠策略，以确保稳定的训练和高表达性。

Result: 在经典模拟器上进行的实验表明，精心配置的QFFN-BERT在全数据设置中达到了基准准确性的102.0%，超过了其经典对应物，同时将FFN特定参数减少了超过99%。此外，在少样本学习场景中，我们的模型表现出一致且有竞争力的优势，证明了其在数据效率方面的潜力。

Conclusion: 我们的结果通过一个非优化的PQC的消融研究得到支持，这表明当与基础深度学习原理共同设计时，PQCs可以作为强大且参数高效的经典FFN替代品。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 本文提出了一种利用参数模型进行代码数据选择的方法，以提高训练效率和模型性能。实验结果表明，该方法在减少计算成本的同时有效提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要通过利用大量数据来提高模型性能，但往往忽视数据质量，从而降低训练效率。

Method: 我们引入了一种利用参数模型进行代码数据选择的方法，以提高训练效率和模型性能。我们的方法优化了参数模型，以确保所选子集内的分布一致性和多样性，从而保证高质量的数据。

Result: 实验结果表明，仅使用10K样本，我们的方法在HumanEval上比92K全采样基线提高了2.4%，在MBPP上提高了2.3%，在性能和效率方面都优于其他采样方法。

Conclusion: 我们的方法有效提升了模型性能，同时显著降低了计算成本。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本研究评估了基于Transformer的七个阿坎语ASR模型在不同语音语料库中的表现，发现了模型在不同领域间的性能差异以及两种架构的不同错误行为，强调了在低资源语言应用中需要领域适应和多语言训练框架。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别（ASR）研究主要使用域内数据集评估模型，但很少评估模型在不同语音情境下的泛化能力。本研究旨在填补这一空白。

Method: 本研究通过使用四个阿坎语语音语料库对基于Transformer架构的七个阿坎语ASR模型（如Whisper和Wav2Vec2）进行基准测试，评估它们的性能。

Result: 研究发现模型在训练域内表现最佳，但在不匹配场景中准确率显著下降。此外，Whisper和Wav2Vec2架构表现出不同的错误行为。

Conclusion: 研究强调了在低资源语言应用中选择ASR架构时应考虑可读性和透明性之间的权衡，并提出了针对领域适应、自适应路由策略和多语言训练框架的需求。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 本研究介绍了一种方法，用于收集语音样本以构建适用于受损语音的自动语音识别（ASR）模型。研究提供了首个公开的Akan语受损语音数据集，并展示了对开源ASR模型进行微调以更好地识别Akan语受损语音的初步结果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在民主化自动语音识别（ASR）技术，并通过开发最佳实践手册和培训，促进社区驱动的数据收集和ASR模型构建。

Method: 本研究通过开发一个“指南手册”来收集语音样本，以建立适用于受损语音的自动语音识别（ASR）模型。此外，还提供了开放源代码工具和数据集，以便研究人员和从业者创建包容性的ASR技术。

Result: 本研究创建了第一个公开的Akan语受损语音数据集，并展示了对开源ASR模型进行微调以更好地识别Akan语受损语音的初步结果。

Conclusion: 本研究展示了构建用于受损语音的自动语音识别（ASR）模型的方法，旨在通过开发最佳实践手册和培训来民主化ASR技术和数据收集。研究提供了第一个公开的Akan语受损语音数据集，并展示了对开源ASR模型进行微调以更好地识别Akan语受损语音的初步结果。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 研究者创建了一个新的印度保释判决数据集，用于支持法律NLP任务。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏结构化数据集，法律NLP在印度等地区发展不足。

Method: 通过提示工程的GPT-4o管道生成注释，并进行一致性验证。

Result: 创建了一个包含1200个印度法院保释判决的数据集，涵盖20多个属性。

Conclusion: 该研究引入了IndianBailJudgments-1200数据集，为印度保释法律的自然语言处理任务提供了支持。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文介绍了WebSailor，这是一种完整的后训练方法，旨在使模型具备在复杂信息环境中减少极端不确定性的能力，从而缩小与专有代理的差距。


<details>
  <summary>Details</summary>
Motivation: 超越人类认知限制是LLM训练的关键前沿。专有代理系统如DeepResearch在极其复杂的信息检索基准测试中表现出超人类的能力，这是之前无法实现的。我们提出，它们的成功依赖于开放源代码模型中缺失的复杂推理模式：在导航广阔信息景观时系统性地减少极端不确定性。

Method: 通过结构化采样和信息混淆生成新颖的高不确定性任务，RFT冷启动，以及高效的代理强化学习训练算法DUPO，引入了WebSailor这一完整的后训练方法。

Result: WebSailor在复杂的信息检索任务中显著优于所有开源代理，表现与专有代理相当，缩小了能力差距。

Conclusion: WebSailor显著优于所有开源代理，在复杂的信息检索任务中表现与专有代理相当，缩小了能力差距。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [14] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文研究了人类标签变异在主动学习中的作用，并提出了一个概念框架，以更好地反映现实标注的复杂性。


<details>
  <summary>Details</summary>
Motivation: 高精度标注数据的获取仍然是应用监督学习的限制因素。尽管标签变异在自然语言处理中很常见，但标注框架通常仍基于单一真实标签的假设，忽略了人类标签变异作为信息信号的可能性。主动学习方法通常依赖于一些简化的假设，这些假设在承认人类标签变异时很少成立。

Method: 本文探讨了关于真实性和标签性质的基本假设，并提出了一个概念框架，将人类标签变异融入主动学习循环中，包括实例选择、标注者选择和标签表示。此外，还讨论了将大型语言模型作为标注者的整合。

Result: 本文分析了主动学习和人类标签变异社区如何处理或忽视这些区别，并提出了一个概念框架，以在整个主动学习循环中纳入人类标签变异。

Conclusion: 本文旨在为考虑人类标签变异的主动学习建立概念基础，更好地反映现实标注的复杂性。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [15] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for large language models (LLMs) that addresses the need for easy bias mitigation. It uses multiperspective generations to align LLM outputs with humanlike baselines, decomposing baseline data into interpretable components to guide generation. Empirical results show that MPF effectively reduces bias, achieves low KL divergence, and improves calibration and generalization without requiring extensive prompt engineering or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The growing need for easy bias mitigation in large language models (LLMs).

Method: MPF leverages multiperspective generations to expose and align biases in LLM outputs with nuanced, humanlike baselines by decomposing baseline, such as sentiment distributions from HR professionals, into interpretable perspective components, guiding generation through sampling and balancing of responses, weighted by the probabilities obtained in the decomposition.

Result: Empirically, MPF demonstrates its ability to align LLM sentiment distributions with both counterfactual baselines (absolute equality) and the HR baseline (biased for Top Univeristy), resulting in small KL divergence, reduction of calibration error and generalization to unseen questions.

Conclusion: MPF offers a scalable and interpretable method for alignment and bias mitigation, compatible with deployed LLMs and requiring no extensive prompt engineering or finetuning.

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [16] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本文研究了性别与上下文偏差之间的关系，引入了一个新的数据集和框架来估计和解释性别偏差，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 我们研究了性别与上下文偏差之间的相关性，特别是关注动作动词、物体名词和职业。

Method: 我们引入了一个新的数据集GenderLexicon和一个可以估计上下文偏差及其相关性别偏差的框架。我们的模型可以通过评分来解释偏差，从而提高性别偏差的可解释性。

Result: 我们的研究结果确认了职业刻板印象之外的性别偏见的存在，并通过在五个不同的数据集上的评估验证了我们方法的有效性，包括一个日语数据集。

Conclusion: 我们的研究确认了职业刻板印象之外的性别偏见的存在，并展示了我们方法的有效性。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [17] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出了一种新的基准LimitGen，用于评估LLM在提供早期反馈和补充人类同行评审方面的潜力，并通过文献检索增强了LLM系统识别局限性的能力。


<details>
  <summary>Details</summary>
Motivation: Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied.

Method: 我们提出了LimitGen，这是第一个全面的基准，用于评估LLM在提供早期反馈和补充人类同行评审方面的潜力。我们还通过文献检索来增强LLM系统的能力，以更好地识别局限性。

Result: 我们提出了LimitGen，这是第一个全面的基准，用于评估LLM在提供早期反馈和补充人类同行评审方面的潜力。我们的基准包括两个子集：LimitGen-Syn和LimitGen-Human。通过文献检索，我们增强了LLM系统识别局限性的能力。

Conclusion: 我们的方法增强了LLM系统生成研究论文局限性的能力，使其能够提供更具体和建设性的反馈。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [18] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 该研究通过测量听觉空间中两个元音刺激的最小可分辨距离（JPD），揭示了人类元音系统的结构特征，并为元音音素的数量和模式提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 研究目的是确定人类在听觉空间中对元音的控制精度，以了解元音系统的结构和可能的音素数量及模式。

Method: 该研究使用了元音模仿范式来测量听觉空间中两个元音刺激之间的最小可分辨距离（JPD）。

Result: JPD被估计在F1 X F2空间中为14到51米之间。

Conclusion: 该研究发现，两个元音刺激在听觉空间中的距离必须足够大，才能产生可区分的模仿，这为人类元音系统的结构提供了理论下限，并解释了元音音素的数量和模式的观察趋势。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [19] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在自我纠正方面的局限性，并提出了改进其可靠性和可信度的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）已经成为变革性的工具，但它们仍然会犯错误，并且可能会探索无成效的推理路径。自我纠正对于一个值得信赖的LLM，特别是自回归LLM来说是一个重要的能力。然而，LLMs在用户输入中可以识别错误，但在自己的输出中却无法纠正相同的错误。

Method: 我们引入了Self-Correction Bench，这是一个系统框架，通过在三个复杂度级别上进行受控错误注入来衡量这一现象。

Result: 测试14个模型，我们发现平均盲点率为64.5%。我们发现多个证据表明，这种限制与训练数据组成有关：人类训练演示主要展示无错误的回应，而不是错误纠正序列，而不同于通过结果反馈学习错误纠正的RL训练模型。值得注意的是，仅仅添加“Wait”可以将盲点减少89.3%，这表明这种能力存在，但需要激活。

Conclusion: 我们的工作突显了当前LLM中的一个关键限制，并提供了提高其可靠性和可信度的潜在途径。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [20] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 本文研究了推理语言模型（RLMs）在面对社会偏见时的鲁棒性。结果显示，具有显式推理能力的模型可能更容易受到偏见引发的影响，这表明推理可能会无意中打开新的刻板印象强化途径。


<details>
  <summary>Details</summary>
Motivation: 尽管推理语言模型（RLMs）在执行复杂、多步骤推理任务方面表现出色，但它们对社会偏见的鲁棒性仍不清楚。本文旨在探讨推理能力如何影响模型的公平性和鲁棒性，以及不同推理机制在面对偏见引发攻击时的表现。

Method: 我们利用CLEAR-Bias基准测试来研究RLMs对偏见引发的对抗鲁棒性。我们系统地评估了最先进的RLMs在多种社会文化维度上的表现，并使用LLM作为评委的方法进行自动化安全评分，同时利用越狱技术来评估内置安全机制的强度。

Result: 研究发现，具有显式推理能力的模型（无论是通过CoT提示还是微调推理轨迹）通常比没有此类机制的基础模型更容易受到偏见引发的影响。推理增强的模型似乎比依赖CoT提示的模型更安全，后者特别容易受到通过故事提示、虚构角色或奖励塑造指令进行的上下文重新框架攻击。

Conclusion: 研究发现，具有显式推理能力的模型（无论是通过CoT提示还是微调推理轨迹）通常比没有此类机制的基础模型更容易受到偏见引发的影响，这表明推理可能会无意中打开新的刻板印象强化途径。推理增强的模型似乎比依赖CoT提示的模型更安全，后者特别容易受到通过故事提示、虚构角色或奖励塑造指令进行的上下文重新框架攻击。这些结果挑战了推理固有地提高鲁棒性的假设，并强调了需要更加关注偏见的推理设计方法。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [21] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本文提出了一种新的数据集MathV-DP和模型Qwen-VL-DP，以提高多模态大语言模型在数学推理中的表现，强调了从不同推理视角学习和区分正确但不同的解决方案的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（MLLMs）在数学推理中往往依赖于一对一的图像-文本对和单一解决方案监督，忽视了有效推理视角和内部反思的多样性。

Method: 我们引入了MathV-DP数据集，该数据集为每个图像-问题对捕捉了多种不同的解决方案轨迹，并提出了Qwen-VL-DP模型，该模型基于Qwen-VL进行微调，并通过组相对策略优化（GRPO）进行增强，这是一种结合正确性判断和多样性感知奖励函数的规则强化学习方法。

Result: 在MathVista的minitest和Math-V基准测试中进行了广泛的实验，结果表明Qwen-VL-DP在准确性和生成多样性方面显著优于之前的基线多模态大语言模型。

Conclusion: 我们的方法在准确性和生成多样性方面显著优于之前的基线多模态大语言模型，强调了在多模态数学推理中纳入多样视角和反思性推理的重要性。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [22] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于机器学习的动态路由框架SynapseRoute，通过将查询路由到适当的模式来优化准确率、成本效益和用户体验。实验结果表明，该框架在提高准确率的同时显著降低了推理时间和令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在实际应用中的广泛采用，选择合适的模型需要平衡性能和运营成本。然而，具有推理能力的模型进一步扩大了高成本推理模式与快速、低成本非推理模式之间的成本差距。因此，需要一种方法来动态地将查询路由到适当的模式。

Method: 本文提出了一种基于机器学习的动态路由框架SynapseRoute，该框架能够根据问题复杂度将输入查询分配到思考模式或非思考模式。

Result: 实验结果表明，与仅使用思考模式相比，SynapseRoute不仅提高了整体准确率（0.8390 vs. 0.8272），还减少了36.8%的推理时间和39.66%的令牌消耗。此外，定性分析表明，对简单查询进行过度推理会导致不必要的延迟甚至准确率下降，而我们的自适应路由避免了这一问题。

Conclusion: 本文提出了SynapseRoute框架，通过动态路由查询到适当的模式，从而优化准确率、成本效益和用户体验。此外，还引入了AIT指数来全面评估准确率、延迟和令牌成本之间的权衡。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [23] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 本文提出IFBench基准以评估精确指令遵循的泛化能力，并通过RLVR方法显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前最强的模型在遵循用户添加的输出约束方面存在困难，因此需要一个新的基准来评估和改进这一能力。

Method: 本文设计了约束验证模块，并使用强化学习与可验证奖励（RLVR）来改进精确指令遵循的泛化能力。

Result: 通过IFBench基准测试，发现大多数模型在少量可验证约束上过度拟合，无法很好地泛化到未见过的输出约束。而使用RLVR方法显著提高了指令遵循能力。

Conclusion: 本文提出了一个名为IFBench的新基准，用于评估精确指令遵循的泛化能力，并展示了通过强化学习与可验证奖励（RLVR）可以显著提高指令遵循能力。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [24] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 本文揭示了语言模型在使用用户反馈训练时的潜在安全漏洞，并展示了一种通过用户反馈操纵模型行为的新攻击方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示语言模型在使用用户反馈进行训练时可能存在的安全风险，并探索如何利用这种机制进行攻击。

Method: 攻击者通过提示语言模型随机输出“中毒”或良性响应，并根据反馈信号调整模型的行为。

Result: 实验表明，这种攻击可以插入虚假事实知识、修改代码生成模式以引入安全漏洞，以及注入假的金融新闻。

Conclusion: 本文发现了一种新的语言模型漏洞，表明通过用户反馈训练的语言模型可能被单个用户持续地改变知识和行为。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [25] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: MOTIF 是一种通过多轮思考生成更多思考标记的强化学习训练方法，能够扩展 LLM 的上下文大小，并在多个基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: LLM 的上下文大小限制了其处理大量标记的能力，因此需要一种模块化思考策略来突破这一限制。

Method: MOTIF 通过参数高效的微调，在 GSM8K 数据集上训练开源模型 Qwen2.5-3B-Instruct，并在 MATH500 和 AIME2024 基准测试中评估其准确性。

Result: MOTIF 在 MATH500 和 AIME2024 基准测试中分别比基于 GRPO 的训练提高了 3.8% 和 3.3%，并且仅使用了 15% 的样本，展示了其样本效率。

Conclusion: MOTIF 是一种有效的强化学习训练方法，能够通过多轮思考生成更多的思考标记，从而扩展模型的上下文大小。实验表明，MOTIF 在多个基准测试中取得了显著的改进，并且具有样本效率。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [26] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 本文指出多项选择评估存在局限性，并提出通过答案匹配进行生成评估的方法，结果表明这种方法比传统方法更有效。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨多项选择评估的局限性，并寻找一种更有效的评估方法。

Method: 本文考虑了通过所谓的答案匹配进行生成评估：给候选模型问题而不带选项，让它生成一个自由形式的回答，然后使用现代语言模型和参考答案来确定回答是否与参考答案匹配。

Result: 使用最近的模型（甚至小型模型）的答案匹配实现了接近完美的协议，范围在标注者间协议范围内。相比之下，多项选择评估和使用LLM作为法官而没有参考答案与人类评分对齐较差。

Conclusion: 本文讨论了如何将评估生态系统从多项选择转移到答案匹配，并指出通过答案匹配改进评估不仅仅是概念上的问题，模型排名在使用答案匹配评估其自由形式的回答时会发生显著变化。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [27] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: This paper proposes FUSE, a unified energy-aware governor for optimizing the energy efficiency of LLM inference on mobile devices. The results show that FUSE can significantly reduce latency while maintaining the same energy consumption.


<details>
  <summary>Details</summary>
Motivation: Deploying LLMs on resource-limited mobile devices faces a significant challenge due to their high demand for computation, memory, and energy. Current LLM frameworks for mobile use three power-hungry components - CPU, GPU, and Memory - even when running primarily-GPU LLM models, optimized DVFS governors for CPU, GPU, and memory operate independently and are oblivious of each other.

Method: We first measure the energy-efficiency of a SOTA LLM framework on mobile phones, then conduct an in-depth measurement study to uncover the inefficiency caused by the interplay among mobile governors, and finally design FUSE - a unified energy-aware governor for optimizing the energy efficiency of LLM inference on mobile devices.

Result: FUSE reduces the time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and 25.4%-36.8% on average with the same energy-per-token for various mobile LLM models.

Conclusion: FUSE can significantly improve the energy efficiency of LLM inference on mobile devices.

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [28] [Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis](https://arxiv.org/abs/2507.02176)
*Marc-André Carbonneau,Benjamin van Niekerk,Hugo Seuté,Jean-Philippe Letendre,Herman Kamper,Julian Zaïdi*

Main category: cs.SD

TL;DR: 本文研究了语音身份建模中的挑战，发现现有ASV嵌入主要关注静态特征，提出了U3D度量来评估动态节奏模式。


<details>
  <summary>Details</summary>
Motivation: 由于语音身份建模的复杂性，现有的ASV嵌入主要关注静态特征，而忽略了动态元素，因此需要一种新的度量方法。

Method: 本文研究了ASV嵌入中捕捉的语音方面，并提出了U3D度量来评估动态节奏模式。

Result: 本文发现ASV嵌入主要关注静态特征，如音色和音高范围，而忽略了动态元素如节奏。同时，提出了U3D度量来评估动态节奏模式。

Conclusion: 本文贡献了评估说话人身份一致性的挑战，并提出了U3D度量来评估说话人的动态节奏模式。

Abstract: Modeling voice identity is challenging due to its multifaceted nature. In
generative speech systems, identity is often assessed using automatic speaker
verification (ASV) embeddings, designed for discrimination rather than
characterizing identity. This paper investigates which aspects of a voice are
captured in such representations. We find that widely used ASV embeddings focus
mainly on static features like timbre and pitch range, while neglecting dynamic
elements such as rhythm. We also identify confounding factors that compromise
speaker similarity measurements and suggest mitigation strategies. To address
these gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm
patterns. This work contributes to the ongoing challenge of assessing speaker
identity consistency in the context of ever-better voice cloning systems. We
publicly release our code.

</details>


### [29] [JoyTTS: LLM-based Spoken Chatbot With Voice Cloning](https://arxiv.org/abs/2507.02380)
*Fangru Zhou,Jun Zhao,Guoxin Wang*

Main category: cs.SD

TL;DR: JoyTTS是一个结合大型语言模型和文本转语音技术的端到端语音聊天机器人，具有语音克隆功能。项目基于开源的MiniCPM-o和CosyVoice2模型，并在2000小时的对话数据上进行训练。测试机器seed-tts-zh上达到了0.73的SS(语音相似度)分数和5.09的WER(词错误率)。代码和模型以及训练和推理脚本已公开。


<details>
  <summary>Details</summary>
Motivation: 开发一个结合大型语言模型和文本转语音技术的端到端语音聊天机器人，以实现语音克隆功能，并为社区提供完整的训练代码以便进一步开发和优化。

Method: JoyTTS结合了大型语言模型和文本转语音技术，使用MiniCPM-o和CosyVoice2模型，并在2000小时的对话数据上进行训练。

Result: 在测试机器seed-tts-zh上，JoyTTS达到了0.73的SS(语音相似度)分数和5.09的WER(词错误率)。

Conclusion: JoyTTS是一个结合大型语言模型(LLM)和文本转语音(TTS)技术的端到端语音聊天机器人，具有语音克隆功能。项目基于开源的MiniCPM-o和CosyVoice2模型，并在2000小时的对话数据上进行训练。测试机器seed-tts-zh上达到了0.73的SS(语音相似度)分数和5.09的WER(词错误率)。代码和模型以及训练和推理脚本已公开。

Abstract: JoyTTS is an end-to-end spoken chatbot that combines large language models
(LLM) with text-to-speech (TTS) technology, featuring voice cloning
capabilities. This project is built upon the open-source MiniCPM-o and
CosyVoice2 models and trained on 2000 hours of conversational data. We have
also provided the complete training code to facilitate further development and
optimization by the community. On the testing machine seed-tts-zh, it achieves
a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.
The code and models, along with training and inference scripts, are available
at https://github.com/jdh-algo/JoyTTS.git.

</details>


### [30] [ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning](https://arxiv.org/abs/2507.02666)
*Junyu Wang,Tianrui Wang,Meng Ge,Longbiao Wang,Jianwu Dang*

Main category: cs.SD

TL;DR: 本文提出了一种差分注意力机制，通过集成双softmax操作和适当调整的差分系数，有效缓解了无效注意力分配的问题。实验结果表明，ASDA模型在多个基准测试中达到了最先进的性能，这表明其在音频任务中的有效性，并为更广泛的应用铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 在音频自监督表示学习的最新进展中，标准Transformer架构已成为主流方法，但其注意力机制常常将一部分注意力权重分配给不相关的信息，可能会影响模型的判别能力。

Method: 我们引入了一种差分注意力机制，通过集成双softmax操作和适当调整的差分系数，有效缓解了无效注意力分配的问题。

Result: 实验结果表明，我们的ASDA模型在多个基准测试中达到了最先进的性能，包括音频分类（AS-2M上49.0% mAP，AS20K上41.5% mAP），关键词检测（SPC-2上98.3%准确率）和环境声音分类（ESC-50上96.1%准确率）。

Conclusion: 实验结果表明，ASDA模型在多个基准测试中达到了最先进的性能，这表明其在音频任务中的有效性，并为更广泛的应用铺平了道路。

Abstract: In recent advancements in audio self-supervised representation learning, the
standard Transformer architecture has emerged as the predominant approach, yet
its attention mechanism often allocates a portion of attention weights to
irrelevant information, potentially impairing the model's discriminative
ability. To address this, we introduce a differential attention mechanism,
which effectively mitigates ineffective attention allocation through the
integration of dual-softmax operations and appropriately tuned differential
coefficients. Experimental results demonstrate that our ASDA model achieves
state-of-the-art (SOTA) performance across multiple benchmarks, including audio
classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting
(98.3% accuracy on SPC-2), and environmental sound classification (96.1%
accuracy on ESC-50). These results highlight ASDA's effectiveness in audio
tasks, paving the way for broader applications.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [31] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 该研究提出了一种基于Transformer的金融文本AI相关内容分类方法FinAI-BERT，具有高准确性和可解释性，适用于金融领域的AI监测。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在粒度、可解释性和鲁棒性方面存在不足，无法满足对金融文本中AI相关内容系统检测的需求。

Method: 该研究采用领域适应的Transformer语言模型FinAI-BERT，对来自669份美国银行年度报告（2015至2023年）的1,586个句子进行微调，以实现对金融文本中AI相关内容的句子级分类。

Result: FinAI-BERT在分类性能上表现出色，准确率达到99.37%，F1得分为0.993，优于传统的逻辑回归、朴素贝叶斯、随机森林和XGBoost等基线模型。

Conclusion: 该研究通过引入FinAI-BERT模型，为金融文本中的AI相关内容提供了细粒度、主题特定的分类方法，具有高准确性和可解释性，能够帮助分析师、监管机构和学者监测金融机构中AI的扩散和表述。

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [32] [DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment](https://arxiv.org/abs/2507.02768)
*Ke-Han Lu,Zhehuai Chen,Szu-Wei Fu,Chao-Han Huck Yang,Sung-Feng Huang,Chih-Kai Yang,Chee-En Yu,Chun-Wei Chen,Wei-Chih Chen,Chien-yu Huang,Yi-Cheng Lin,Yu-Xiang Lin,Chi-An Fu,Chun-Yi Kuan,Wenze Ren,Xuanjun Chen,Wei-Ping Huang,En-Pei Hu,Tzu-Quan Lin,Yuan-Kuei Wu,Kuan-Po Huang,Hsiao-Ying Huang,Huang-Cheng Chou,Kai-Wei Chang,Cheng-Han Chiang,Boris Ginsburg,Yu-Chiang Frank Wang,Hung-yi Lee*

Main category: eess.AS

TL;DR: 我们介绍了DeSTA2.5-Audio，这是一个通用的大规模音频语言模型（LALM），旨在实现稳健的听觉感知和指令遵循，而无需任务特定的音频指令微调。


<details>
  <summary>Details</summary>
Motivation: 现有的LALM通常通过在大规模手动整理或LLM合成的音频指令数据集上进行训练来增强大型语言模型（LLM）的听觉能力，但这些方法往往导致LLM原始语言能力的灾难性遗忘。

Method: 我们重新审视了数据构建流程，并提出了DeSTA，这是一种自生成的跨模态对齐策略，其中主干LLM生成自己的训练目标。

Result: DeSTA2.5-Audio在多个音频-语言基准测试中实现了最先进的或具有竞争力的性能，包括Dynamic-SUPERB、MMAU、SAKURA、Speech-IFEval和VoiceBench。

Conclusion: 我们的研究强调了在LALM开发中精心设计的数据构建的重要性，并为构建稳健、通用的LALM提供了实用见解。

Abstract: We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model
(LALM) designed for robust auditory perception and instruction-following,
without requiring task-specific audio instruction-tuning. Recent LALMs
typically augment Large Language Models (LLMs) with auditory capabilities by
training on large-scale, manually curated or LLM-synthesized audio-instruction
datasets. However, these approaches have often suffered from the catastrophic
forgetting of the LLM's original language abilities. To address this, we
revisit the data construction pipeline and propose DeSTA, a self-generated
cross-modal alignment strategy in which the backbone LLM generates its own
training targets. This approach preserves the LLM's native language proficiency
while establishing effective audio-text alignment, thereby enabling zero-shot
generalization without task-specific tuning. Using DeSTA, we construct
DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training
samples derived from 7,000 hours of audio spanning 50 diverse datasets,
including speech, environmental sounds, and music. DeSTA2.5-Audio achieves
state-of-the-art or competitive performance across a wide range of
audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,
Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate
that our self-generated strategy outperforms widely adopted data construction
and training strategies in both auditory perception and instruction-following
capabilities. Our findings underscore the importance of carefully designed data
construction in LALM development and offer practical insights for building
robust, general-purpose LALMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [33] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于思维链推理的事件流场景文本识别框架ESTR-CoT，该框架在三个事件流STR基准数据集上得到了充分验证，具有良好的有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的工作要么采用端到端的编码器-解码器框架，要么采用大语言模型来增强识别，但它们仍然受到解释性不足和弱上下文逻辑推理的限制。

Method: 本文采用视觉编码器EVA-CLIP (ViT-G/14)将输入的事件流转换为标记，并利用Llama分词器对给定的生成提示进行编码。使用Q-former将视觉标记与预训练的大语言模型Vicuna-7B对齐，并同时输出答案和思维链（CoT）推理过程。此外，还提出了一个大规模的CoT数据集，通过三阶段处理（即生成、润色和专家验证）来训练框架。

Result: 本文提出的框架在三个事件流STR基准数据集（即EventSTR、WordArt*、IC15*）上进行了广泛的实验，结果验证了框架的有效性和可解释性。

Conclusion: 本文提出了一种基于思维链推理的事件流场景文本识别框架ESTR-CoT，该框架在三个事件流STR基准数据集上得到了充分验证，具有良好的有效性和可解释性。

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [34] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 本文介绍了SciGA-145k数据集，用于支持图形摘要的选择、推荐和自动化生成，并提出了新的推荐度量标准CAR。


<details>
  <summary>Details</summary>
Motivation: 探索图形摘要在科学交流中的潜力，同时解决设计有效图形摘要所需的高级可视化技能问题。

Method: 引入了SciGA-145k数据集，定义了两个任务：Intra-GA推荐和Inter-GA推荐，并提出了CAR作为新的推荐度量标准。

Result: 提供了合理的基线模型，并通过CAR度量标准改进了模型行为的分析。

Conclusion: SciGA-145k为推进视觉科学交流和AI for Science的发展奠定了基础。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [35] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态叙事理解的自动视频编辑框架（HIVE），能够更好地利用视觉上下文，提高视频编辑的连贯性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动编辑方法主要依赖于ASR转录文本和端到端片段选择，常常忽视丰富的视觉上下文，导致不连贯的输出。随着在线视频内容的快速增长，特别是短视频平台，对高效视频编辑技术的需求不断增加，以将长格式视频压缩成简短且吸引人的剪辑。

Method: 我们提出了一个基于多模态叙事理解的人类启发式自动视频编辑框架（HIVE），结合了角色提取、对话分析和通过多模态大型语言模型进行的叙事总结，以实现对视频内容的整体理解。此外，我们应用了场景级分割，并将编辑过程分解为三个子任务：亮点检测、开头/结尾选择和无关内容的修剪。

Result: 实验结果表明，我们的框架在通用和广告导向的编辑任务中 consistently 超过现有的基线，显著缩小了自动编辑和人工编辑视频之间的质量差距。

Conclusion: 我们的框架在通用和广告导向的编辑任务中 consistently 超过现有的基线，显著缩小了自动编辑和人工编辑视频之间的质量差距。

Abstract: The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) that leverages
multimodal narrative understanding to address these limitations. Our approach
incorporates character extraction, dialogue analysis, and narrative
summarization through multimodal large language models, enabling a holistic
understanding of the video content. To further enhance coherence, we apply
scene-level segmentation and decompose the editing process into three subtasks:
highlight detection, opening/ending selection, and pruning of irrelevant
content. To facilitate research in this area, we introduce DramaAD, a novel
benchmark dataset comprising over 800 short drama episodes and 500
professionally edited advertisement clips. Experimental results demonstrate
that our framework consistently outperforms existing baselines across both
general and advertisement-oriented editing tasks, significantly narrowing the
quality gap between automatic and human-edited videos.

</details>


### [36] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 本文提出了一种名为VisCo的视觉主导越狱攻击方法，通过生成具有视觉上下文的攻击提示，有效提升了对多模态大语言模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法中，视觉模态主要作为触发不安全行为的工具，存在语义模糊且缺乏现实场景的支撑。因此，需要一种新的设置，使视觉信息成为构建完整且现实的越狱上下文的必要组成部分。

Method: VisCo通过四种不同的视觉聚焦策略生成上下文对话，并在必要时动态生成辅助图像以构建视觉主导的越狱场景。同时结合自动毒性模糊和语义优化来提高攻击效果。

Result: VisCo在MM-SafetyBench上实现了4.78的毒性评分和85%的攻击成功率，显著优于基线（2.48的毒性评分和22.2%的攻击成功率）。

Conclusion: VisCo攻击在MM-SafetyBench上对GPT-4o表现出显著优于基线的效果，证明了其在构建视觉主导的越狱场景中的有效性。

Abstract: With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the visual modality primarily serves as a trigger
for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding
in realistic scenarios. In this work, we define a novel setting: visual-centric
jailbreak, where visual information serves as a necessary component in
constructing a complete and realistic jailbreak context. Building on this
setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates
contextual dialogue using four distinct visual-focused strategies, dynamically
generating auxiliary images when necessary to construct a visual-centric
jailbreak scenario. To maximize attack effectiveness, it incorporates automatic
toxicity obfuscation and semantic refinement to produce a final attack prompt
that reliably triggers harmful responses from the target black-box MLLMs.
Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success
Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming
the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The
code is available at https://github.com/Dtc7w3PQ/Visco-Attack.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [37] [Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents](https://arxiv.org/abs/2507.02287)
*Lapo Santarlasci,Armando Rungi,Antonio Zinilli*

Main category: econ.GN

TL;DR: 本文通过自然语言处理识别真正的绿色专利，并发现它们对企业财务表现有积极影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过自然语言处理识别出真正的绿色专利，并探讨其对企业的财务表现的影响。

Method: 我们首先在大约1240万份之前文献中被分类为绿色的专利上进行训练，然后训练一个简单的神经网络，通过与环境技术相关的表达的向量表示来扩展基础词典。

Result: 我们发现真正的绿色专利约占之前文献中分类为绿色的专利总数的20%。此外，真正的绿色专利比其他绿色专利少被后续发明引用约1%。在欧洲联盟的企业层面财务账户的仪表板测试中，我们发现拥有至少一项真正的绿色专利可以提高销售额、市场份额和生产率。对于高创新性的真正绿色专利，我们还发现它们能带来更高的利润。

Conclusion: 我们的研究强调了使用文本分析来评估更细粒度的专利分类的重要性，这对于不同领域的政策制定是有用的。

Abstract: This paper introduces Natural Language Processing for identifying ``true''
green patents from official supporting documents. We start our training on
about 12.4 million patents that had been classified as green from previous
literature. Thus, we train a simple neural network to enlarge a baseline
dictionary through vector representations of expressions related to
environmental technologies. After testing, we find that ``true'' green patents
represent about 20\% of the total of patents classified as green from previous
literature. We show heterogeneity by technological classes, and then check that
`true' green patents are about 1\% less cited by following inventions. In the
second part of the paper, we test the relationship between patenting and a
dashboard of firm-level financial accounts in the European Union. After
controlling for reverse causality, we show that holding at least one ``true''
green patent raises sales, market shares, and productivity. If we restrict the
analysis to high-novelty ``true'' green patents, we find that they also yield
higher profits. Our findings underscore the importance of using text analyses
to gauge finer-grained patent classifications that are useful for policymaking
in different domains.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [38] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: 本文提出了一种名为HyFairCRS的新框架，旨在解决对话推荐系统中的不公平问题，通过对比学习捕捉用户的多兴趣，并在动态用户-系统反馈循环中确保公平的项目预测。实验表明，该方法在两个数据集上取得了最先进的性能，同时有效缓解了不公平现象。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中的不公平问题，特别是在动态和交互式对话推荐系统中，因为不公平现象会随着时间的推移而加剧，导致诸如马太效应、过滤气泡和回音室等问题。

Method: 提出了一种新颖的框架，称为Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS)，通过对比学习建立多样化的超图来捕捉广泛用户兴趣，并在对话中生成信息性响应，确保在动态用户-系统反馈循环中的公平项目预测。

Result: HyFairCRS在两个基于CRS的数据集上实现了新的最先进性能，同时有效缓解了不公平现象。

Conclusion: HyFairCRS在两个基于CRS的数据集上实现了新的最先进性能，同时有效缓解了不公平现象。

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [39] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本文提出了一种基于文本蕴含和上下文学习的方法，用于自动生成法律文本的规范表示，可以编码和执行为Python代码。该方法减少了对大型手动标记数据集的需求，并提高了对未见立法的适用性。在13个美国州数据泄露通知法上的评估显示，生成的表示通过了大约89.4%的测试用例，并分别达到了82.2和88.7的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 软件系统必须遵守法律法规，这是一个资源密集型的任务，特别是对于缺乏专门法律专业知识的小组织和初创公司。从法规中提取元数据以获取软件的法律需求是确保合规性的关键步骤。然而，由于法律文本的长度和复杂性，这是一个繁琐的任务。尽管以前的工作已经追求自动化方法来提取法律文本的结构和语义元数据，但关键限制仍然存在：它们不考虑与这些元数据类型相关的属性之间的相互作用和相互关系，并且依赖于手动标记或启发式驱动的机器学习，这在新文档上泛化效果不佳。

Method: 我们引入了一种基于文本蕴含和上下文学习的方法，用于自动生成法律文本的规范表示，可以编码和执行为Python代码。我们的表示是从手动设计的Python类结构实例化的，作为领域特定的元模型，捕捉结构和语义法律元数据及其相互关系。

Result: 我们的方法在13个美国州数据泄露通知法上进行了评估，展示了生成的表示通过了大约89.4%的测试用例，并分别达到了82.2和88.7的精度和召回率。

Conclusion: 我们的方法在13个美国州数据泄露通知法上进行了评估，展示了生成的表示通过了大约89.4%的测试用例，并分别达到了82.2和88.7的精度和召回率。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [40] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本文研究了使用GPT-4o生成后续访谈问题的效果，发现其在多个指标上与人工问题相当甚至更优，表明大型语言模型可以在实时需求获取中帮助访谈者提升质量。


<details>
  <summary>Details</summary>
Motivation: 有效访谈需要技能娴熟的访谈者实时制定适当的问题，但面临诸如缺乏领域知识、认知负荷过重和信息过载等挑战。因此，研究如何利用大型语言模型辅助访谈者提高访谈效果具有重要意义。

Method: 本文基于常见的访谈者错误类型构建了一个框架，并描述了根据访谈者话语生成问题的方法。此外，还进行了两个受控实验来评估LLM生成的问题与人工编写的问题的性能。

Result: 实验结果表明，LLM生成的问题在清晰度、相关性和信息量方面不逊于人工编写的问题，且在基于常见错误类型的引导下表现更优。

Conclusion: 本文展示了大型语言模型（如GPT-4o）在支持访谈者生成后续问题方面的潜力，可以提高需求获取访谈的质量和便捷性。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA is a self-evolving AI agent that uses a multi-agent architecture to improve its capabilities through an evolving Template Library and a dynamic Tool Ocean. It demonstrates state-of-the-art performance on biomedical benchmarks and shows systematic improvement with experience.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale.

Method: STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools.

Result: STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench: DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. Its performance systematically improves with experience; for instance, its accuracy on the Humanity's Last Exam benchmark almost doubles with increased trials.

Conclusion: STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery.

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [42] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在迭代囚徒困境中的表现，发现它们能够主动推理时间范围和对手的可能策略，并展示了这种推理对其决策的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨大型语言模型（LLMs）是否是一种新的战略智能形式，能够在竞争环境中进行推理。

Method: 本文进行了首次进化迭代囚徒困境（IPD）比赛，将经典的策略（如以牙还牙、严厉惩罚）与来自领先AI公司的代理（如OpenAI、Google和Anthropic）进行比较。通过改变每次比赛的终止概率（即“未来的阴影”），引入了复杂性和偶然性。

Result: 结果表明，LLMs非常具有竞争力，在这些复杂的生态系统中持续生存，有时甚至繁衍。Google的Gemini模型表现出战略上的残酷，利用合作对手并报复背叛者；OpenAI的模型保持高度合作，这在敌对环境中证明是灾难性的；Anthropic的Claude则成为最宽容的互惠者，表现出恢复合作的显著意愿。

Conclusion: 本文将经典博弈论与机器心理学联系起来，提供了算法在不确定性下的决策制定的丰富而细致的视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [43] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA是一种分层框架，通过将战略规划与专用执行分离，提高了复杂信息检索任务的答案质量和系统效率。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）管道难以有效处理现实世界搜索场景中的复杂信息需求，而基于推理的方法由于使用单一模型处理高层次规划和详细执行，导致推理效率低下和可扩展性有限。

Method: HiRA是一个分层框架，将战略规划与专用执行分离。该方法将复杂搜索任务分解为聚焦的子任务，将每个子任务分配给配备外部工具和推理能力的领域特定代理，并通过结构化集成机制协调结果。

Result: HiRA在答案质量和系统效率方面均有所提升，表明解耦规划和执行对于多步骤信息检索任务的有效性。

Conclusion: HiRA在四个复杂的跨模态深度搜索基准测试中显著优于最先进的RAG和基于代理的系统，证明了解耦规划和执行对于多步骤信息检索任务的有效性。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [44] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出了一种名为StepHint的新RLVR算法，用于解决当前RLVR方法面临的近似奖励问题和探索停滞问题。通过提供多级逐步提示，StepHint能够更有效地探索解空间，提高训练效率，并提升模型的推理能力。实验结果表明，StepHint在多个数学基准测试中表现优异，具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前RLVR方法面临的两个重大挑战：近似奖励问题和探索停滞问题。近似奖励问题是指一个小错误可以使整个正确的推理过程无效，大大阻碍训练效率；探索停滞问题是指模型倾向于集中在自己的“舒适区”内的解决方案，缺乏探索潜在更有效替代方案的动力。

Method: 提出了一种名为StepHint的新RLVR算法，利用多级逐步提示来帮助模型更有效地探索解空间。StepHint从更强的模型生成有效的推理链，并使用我们提出的自适应分割方法将这些链分割为推理步骤。初始几步作为提示，同时提供多级提示（每个包含不同数量的步骤）给模型。

Result: StepHint通过提供提示缓解了近似奖励问题，从而提高了训练效率。此外，外部推理路径帮助模型发展更好的推理能力，使其能够超越自己的“舒适区”，缓解探索停滞问题。

Conclusion: StepHint在六个数学基准测试中优于竞争性的RLVR增强方法，同时表现出更好的泛化能力，并在域外基准测试中超越基线。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 本研究探讨了多模态特征组合在虚假信息检测中的有效性，发现结合无监督和有监督机器学习模型能显著提高分类性能，并分析了虚假信息的传播模式。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量关于虚假信息检测的研究，但主要集中在基于文本或图像的方法上，而较少研究多模态特征组合。本研究旨在探讨不同多模态特征组合的有效性。

Method: 本研究采用早期融合方法，结合文本、图像和社会特征进行分类模型构建。通过对象检测和光学字符识别（OCR）等技术提取额外的社会特征和视觉特征。

Result: 结果表明，结合无监督和有监督机器学习模型的分类性能比单模态模型提高了15%，比双模态模型提高了5%。此外，研究还分析了虚假信息的传播模式。

Conclusion: 本研究表明，结合无监督和有监督机器学习模型可以提高分类性能，相比单模态模型提高了15%，相比双模态模型提高了5%。此外，研究还分析了基于虚假信息推文特征和传播者的传播模式。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [46] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 本研究评估了多种先进的基础LLMs和一个专有的领域特定招聘模型（Match Score）在招聘中的表现。结果显示，Match Score在准确性上优于通用LLMs，并且在人口统计群体中实现了更公平的结果。研究强调了在高风险领域部署AI时，领域特定建模和偏见审计的重要性，并警告不要在没有充分公平保障的情况下依赖现成的LLMs。


<details>
  <summary>Details</summary>
Motivation: 在招聘中使用大型语言模型（LLMs）有望简化候选人筛选，但也引发了关于准确性和算法偏见的严重担忧，尤其是在缺乏足够保障的情况下。

Method: 我们对几种最先进的基础LLMs（包括来自OpenAI、Anthropic、Google、Meta和Deepseek的模型）以及我们专有的领域特定招聘模型（Match Score）进行了基准测试，并进行了比较。我们评估了每个模型的预测准确性（ROC AUC、Precision-Recall AUC、F1-score）和公平性（截止分析的影响比率，跨声明的性别、种族和交叉子群体）。

Result: 实验结果表明，Match Score在准确性（ROC AUC 0.85 vs 0.77）上优于通用LLMs，并且在人口统计群体中实现了更公平的结果。值得注意的是，Match Score在种族影响比率方面达到了最低0.957（接近平等），而最好的LLMs则为0.809或更低（分别为0.906 vs 0.773对于交叉群体）。

Conclusion: 我们的研究强调了在高风险领域（如招聘）部署AI时，领域特定建模和偏见审计的重要性，并警告不要在没有充分公平保障的情况下依赖现成的LLMs。此外，我们通过实证证据表明，在招聘中选择准确性和公平性之间不应存在二分法：设计良好的算法可以同时实现招聘的准确性与结果的公平性。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [47] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: This paper introduces Energy-Based Transformers (EBTs), a new class of Energy-Based Models that enable inference-time computation techniques similar to human System 2 Thinking. EBTs show superior performance compared to existing approaches in both training and inference, and they generalize better than other models.


<details>
  <summary>Details</summary>
Motivation: Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific, problem-specific, or require additional supervision/training on top of unsupervised pretraining.

Method: We train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence.

Result: EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches.

Conclusion: EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [48] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft is a framework that allows a single draft model to work with any target model, addressing compatibility and latency issues in online deployment settings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenges of using incompatible target models with draft models and improving latency in online deployment settings. It also highlights the need for a 'one drafter for all' paradigm in on-device LLM applications.

Method: OmniDraft introduces an online n-gram cache with hybrid distillation fine-tuning to address cross-vocabulary mismatch and leverages adaptive drafting techniques to improve decoding speed.

Result: OmniDraft enables a single Llama-68M model to pair with various target models, such as Vicuna-7B, Qwen2-7B, and Llama3-8B, for speculative decoding and provides up to 1.5-2x speedup.

Conclusion: OmniDraft is a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. It addresses the challenges of cross-vocabulary mismatch and latency improvements, making it suitable for on-device LLM applications.

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [49] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: ExPO is a framework that generates effective positive samples by conditioning on the ground-truth answer, improving learning efficiency and performance on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To unlock reasoning ability in settings where the model initially struggles, the model must explore new reasoning trajectories beyond its current output distribution. However, existing methods depend heavily on the model's initial ability to produce positive samples.

Method: Self-Explanation Policy Optimization (ExPO) - a simple and modular framework that generates effective positive samples by conditioning on the ground-truth answer.

Result: Experiments show that ExPO improves both learning efficiency and final performance on reasoning benchmarks, surpassing expert-demonstration-based methods in challenging settings such as MATH level-5.

Conclusion: ExPO improves both learning efficiency and final performance on reasoning benchmarks, surpassing expert-demonstration-based methods in challenging settings.

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [50] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 本文评估了前沿大型语言模型的隐写术能力，发现它们在特定条件下可以编码消息和执行编码推理，但目前不足以绕过良好的监控系统。


<details>
  <summary>Details</summary>
Motivation: 监控大型语言模型的输出对于减轻滥用和不一致的风险至关重要。然而，大型语言模型可能通过隐写术逃避监控，因此需要评估其隐写术能力以更好地了解潜在风险。

Method: 评估前沿大型语言模型的隐写术能力，重点关注两种类型的隐写术：编码消息和执行编码推理。

Result: 当前模型在标准条件下无法在输出中编码短消息而不被监控发现。但如果提供额外的权限，如使用未受监控的草稿板并协调编码方案，则可以成功。此外，模型在简单的状态跟踪问题中显示出早期迹象，能够进行基本的编码推理。

Conclusion: 当前大型语言模型表现出初步的隐写术能力，尽管这些能力目前可能不足以绕过设计良好的监控系统，但未来可能会发生变化。

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>

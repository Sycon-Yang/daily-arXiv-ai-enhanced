<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文介绍了MALIBU，一个用于评估基于LLM的多智能体系统是否无意中强化社会偏见和刻板印象的新基准。通过基于场景的评估，我们发现偏见缓解可能更倾向于边缘化人物而非真正的中立性，强调了在多智能体系统中需要细致的检测、平衡的公平策略和透明的评估基准。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在基于角色的交互中越来越被使用，但如果没有仔细设计，这些系统可能会强化大型语言模型（LLMs）中的隐性偏见，引发关于公平性和公平代表性的担忧。因此，我们需要一个基准来评估基于LLM的多智能体系统是否无意中强化社会偏见和刻板印象。

Method: 我们通过基于场景的评估来评估基于LLM的多智能体系统的偏见。AI模型在预定义的上下文中完成任务，其响应由基于LLM的多智能体评判系统进行两阶段评估。在第一阶段，评判者根据特定的人口统计学人物（例如性别、种族、宗教）对响应进行评分，共四个指标。在第二阶段，评判者比较分配到不同人物的配对响应，进行评分并选择更优的响应。

Result: 我们的研究量化了LLM生成输出中的偏见，揭示了偏见缓解可能更倾向于边缘化人物而非真正的中立性，强调了在多智能体系统中需要细致的检测、平衡的公平策略和透明的评估基准。

Conclusion: 我们的研究量化了LLM生成输出中的偏见，揭示了偏见缓解可能更倾向于边缘化人物而非真正的中立性，强调了在多智能体系统中需要细致的检测、平衡的公平策略和透明的评估基准。

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [2] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: 本文提出了一种通过计算生成摘要、参考摘要和原始新闻文章之间的重叠事件来评估抽象摘要质量的方法。


<details>
  <summary>Details</summary>
Motivation: 目前，自动产生的摘要的评估主要依赖于人工编写的摘要作为黄金标准，但这种方法可能无法充分反映摘要中的事件信息。

Method: 本文提出了一种基于事件重叠的评估方法，通过比较生成摘要、参考摘要和原始新闻文章中的事件信息来评估摘要质量。

Result: 实验结果表明，该方法能够提供关于摘要中包含的事件信息的更深入见解。

Conclusion: 本文提出的基于事件重叠的评估方法为评估抽象摘要的质量提供了一种新的视角。

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [3] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: 本文分析了《Nordisk familjebok》第一版和第二版的地理条目变化，发现地理焦点从欧洲转向其他地区，这反映了第一次世界大战和新强国的崛起。


<details>
  <summary>Details</summary>
Motivation: 研究《Nordisk familjebok》的地理条目变化，以了解其在不同版本中的内容演变及其对社会和学术界的影响。

Method: 使用Project Runeberg的数字化版本，首先将原始文本重新分割为条目，并使用语义句子嵌入匹配第一版和第二版的条目对。然后使用基于变压器的分类器提取两版的地理条目，并将其链接到Wikidata。

Result: 发现从第一版到第二版的地理焦点发生了小但显著的转移，从欧洲转向北美、非洲、亚洲、澳大利亚和北斯堪的纳维亚地区。

Conclusion: 通过分析《Nordisk familjebok》第一版和第二版的地理条目，我们观察到从欧洲向北美、非洲、亚洲、澳大利亚和北斯堪的纳维亚地区的地理焦点发生了小但显著的转移，这证实了第一次世界大战的影响和新强国的崛起。

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [4] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: 本文提出了一种新的ABSA框架MEGA，结合了xLSTM和多头指数门控融合机制，以提高计算效率和性能。实验结果表明，MEGA在三个基准数据集上优于最先进的基线。


<details>
  <summary>Details</summary>
Motivation: 现有的ABSA方法难以在计算效率和高性能之间取得平衡：深度学习模型通常缺乏全局上下文，变压器需要大量的计算资源，而基于Mamba的方法面临CUDA依赖和局部相关性减弱的问题。最近在扩展长短期记忆（xLSTM）模型方面的进展，特别是它们对长距离依赖的高效建模，已经显著推动了NLP社区的发展。然而，它们在ABSA中的潜力尚未被挖掘。

Method: 我们提出了xLSTM与多头指数门控融合（MEGA）框架，结合了双向mLSTM架构和前向及部分翻转后向（PF-mLSTM）流。PF-mLSTM通过以专用参数反向处理初始序列段来增强局部上下文建模，同时保持关键的短程模式。我们还引入了一个基于mLSTM的多头交叉指数门控融合机制（MECGAF），动态地将前向mLSTM输出作为查询和键，PF-mLSTM输出作为值，优化了短程依赖捕获，同时保持全局上下文和效率。

Result: 实验结果表明，MEGA在三个基准数据集上优于最先进的基线，实现了优越的准确性和效率。

Conclusion: 实验结果表明，MEGA在三个基准数据集上优于最先进的基线，实现了优越的准确性和效率。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [5] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: 本文提出了一种去偏算法，可以有效减少文本嵌入中的偏差，同时保持计算成本低，并且不影响模型在分布外数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 文本序列之间的基于嵌入的相似性度量可能会受到诸如文本来源或语言等无关属性的影响，这在需要从不同语料库中汇总文本的应用中尤其成问题。

Method: 本文提出了一种去偏算法，该算法从编码器表示中移除关于观察到的混杂因素的信息。

Result: 在我们评估的每一种嵌入变体和任务中，文档相似性和聚类度量都有所提高，有时甚至显著提高。有趣的是，分布外基准测试的性能没有受到影响，这表明嵌入没有其他方面的退化。

Conclusion: 本文表明，一种去除编码器表示中关于观察到的混杂因素的信息的去偏算法可以显著减少这些偏差，同时计算成本很低。

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [6] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: 本文讨论了大型语言模型在处理非英语和非中文国家的法律问题时的能力，并介绍了gAIus架构，该架构通过一种更可解释、更人性化的检索机制提高了性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨大型语言模型在处理非英语和非中文国家的法律问题时的能力，包括提供适当的参考文献。

Method: 本文介绍了gAIus，一个基于认知LLM的代理架构，其响应基于从特定法律条文（如波兰民法典）中检索的知识。还提出了一种更可解释、更人性化且效果更好的检索机制。

Result: 通过创建基于波兰法律实习入学考试单选题的特殊数据集来评估所提出的方法。该架构显著提升了gpt-3.5-turbo-0125的性能，使其比gpt-4o更好，并将gpt-4o-mini的分数从31%提高到86%。

Conclusion: 本文展示了gAIus架构的潜力，并提出了未来研究和应用的方向。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [7] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: 本研究评估了GPT-4在眼科中的表现，发现其在基本决策任务中有一定能力，但在复杂任务中表现不佳。元数据的加入对结果无显著影响，表明GPT-4可能在教育、文档或图像标注工作中有所帮助。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）可以基于自然语言提示模拟临床推理，但它们在眼科的应用尚不明确。本研究旨在评估GPT-4在眼科中的表现及其对临床决策的影响。

Method: 本研究评估了GPT-4解读结构化文本描述的视网膜眼底照片并模拟糖尿病视网膜病变（DR）和青光眼筛查的临床决策能力，包括添加真实或合成临床元数据的影响。通过300张带注释的眼底图像进行回顾性诊断验证研究。GPT-4接收到描述每张图像的结构化提示，有或没有患者元数据。模型被要求分配ICDR严重程度评分，推荐DR转诊，并估计青光眼转诊的杯盘比。

Result: GPT-4在ICDR分类中表现出中等性能（准确率67.5%，宏F1 0.33，加权F1 0.67，kappa 0.25），主要由正常病例的正确识别驱动。在二进制DR转诊任务中性能有所提高（准确率82.3%，F1 0.54，kappa 0.44）。对于青光眼转诊，所有设置下的性能都很差（准确率约78%，F1 <0.04，kappa <0.03）。元数据的加入并未显著影响结果（McNemar p > 0.05），预测在不同条件下保持一致。

Conclusion: GPT-4可以模拟基本的眼科决策，但在复杂任务上缺乏精确性。虽然不适合临床使用，但大型语言模型可能在教育、文档或图像标注工作中提供帮助。

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [8] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: CARE-RAG is a novel framework that enhances the trustworthiness of RAG systems by addressing knowledge conflicts through conflict-driven summarization and QA repair.


<details>
  <summary>Details</summary>
Motivation: Knowledge conflicts caused by internal inconsistencies or noisy retrieved content can severely undermine the generation reliability of RAG systems. LLMs should rethink all evidence, including both retrieved content and internal knowledge, before generating responses.

Method: CARE-RAG (Conflict-Aware and Reliable Evidence for RAG) is a novel framework that improves trustworthiness through Conflict-Driven Summarization of all available evidence. It derives parameter-aware evidence by comparing parameter records to identify diverse internal perspectives and refines retrieved evidences to produce context-aware evidence. A 3B LLaMA3.2 model is distilled to perform conflict-driven summarization, enabling reliable synthesis across multiple sources. A QA Repair step is introduced to correct outdated or ambiguous benchmark answers.

Result: Experiments on revised QA datasets with retrieval data show that CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios with noisy or conflicting evidence.

Conclusion: CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios with noisy or conflicting evidence.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [9] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: 本文提出了一种新的网络规模数据存储库CompactDS，用于提高RAG在推理密集型任务中的性能。实验表明，该方法在多个基准测试中取得了显著的性能提升，并且优于现有的复杂RAG系统。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG在推理密集型基准测试中表现有限，因此我们挑战这一观点，并尝试通过构建一个更高质量、更高效的检索数据存储库来改进RAG的效果。

Method: 我们引入了CompactDS：一个多样化、高质量、网络规模的数据存储库，它在单节点上实现了高检索准确率和亚秒级延迟。关键见解是（1）大多数网络内容可以被过滤掉而不影响覆盖率，一个紧凑的高质量子集就足够了；（2）结合内存中的近似最近邻（ANN）检索和磁盘上的精确搜索可以平衡速度和召回率。

Result: 使用CompactDS，我们展示了最小的RAG管道在所有基准测试和模型大小（8B-70B）中都实现了稳定的准确性提升，MMLU的相对增益为10%，MMLU Pro为33%，GPQA为14%，MATH为19%。

Conclusion: 我们的精心设计的内部数据存储库在性能上可以与Google搜索以及最近提出的复杂基于代理的RAG系统相媲美，同时保持了简单性、可重复性和自包含性。我们发布了CompactDS和我们的检索管道，以支持未来探索基于检索的AI系统的研究所。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [10] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA is a novel method for activation sparsification that improves LLM efficiency without requiring additional training or magnitude-based pruning. It uses layerwise orthogonal rotations and Top-K selection to achieve consistent sparsity and reliable speed-up.


<details>
  <summary>Details</summary>
Motivation: Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up.

Method: LaRoSA leverages layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up.

Result: For LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.

Conclusion: LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration.

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [11] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 本研究探讨了先进指令调优推理模型在解决多样化的物理问题中的应用，并发现这些模型在准确性和推理模式方面表现出色，同时通过少量示例提示可以进一步提高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理物理推理的复杂性方面一直是一个困难的任务，需要深刻的概念理解和熟练的问题解决技巧。

Method: 本研究调查了先进指令调优推理模型（如Deepseek-R1）在解决从SciBench基准中精选的多样化物理问题中的应用。

Result: 实验评估显示，推理模型表现出色，不仅在回答复杂的物理问题上达到了最先进的准确率，还生成了强调符号推导的独特推理模式。

Conclusion: 研究发现，即使是高度复杂的推理模型，通过战略性地引入少量示例提示，仍能显著提高整体准确性，这表明性能仍有提升的潜力。

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [12] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: LEDOM is a reverse language model that processes sequences in reverse order, showing promise in improving generation quality through Reverse Reward on mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to present the reverse language model as a potential foundational model across general tasks and explore its unique backward reasoning capability for refining generation quality through posterior evaluation.

Method: LEDOM is a purely reverse language model trained autoregressively on 435B tokens with 2B and 7B parameter variants, processing sequences in reverse temporal order through previous token prediction. We also introduce Reverse Reward, where LEDOM-guided reranking of forward language model outputs leads to performance improvements on mathematical reasoning tasks.

Result: LEDOM demonstrates substantial performance improvements on mathematical reasoning tasks when used with Reverse Reward, suggesting its unique characteristics and broad application potential.

Conclusion: LEDOM exhibits unique characteristics with broad application potential, and we will release all models, training code, and pre-training data to facilitate future research.

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [13] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 本文提出了一个大规模的偏好数据集SynPref-40M，并设计了一个基于人类-AI协同的两阶段管道，用于数据校准。训练了Skywork-Reward-V2，这是一组8个奖励模型，参数从0.6B到8B不等，在多个奖励模型基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的开放奖励模型在大多数评估基准上表现不佳，无法捕捉到微妙和复杂的用户偏好。现有的方法即使结合了先进的训练技术，也未能带来显著的性能提升。

Method: 提出了一种大规模的偏好数据集SynPref-40M，并设计了一个基于人类-AI协同的两阶段管道，以实现大规模的数据校准。训练了Skywork-Reward-V2，这是一个包含8个奖励模型的套件，参数从0.6B到8B不等。

Result: Skywork-Reward-V2在多个主要奖励模型基准上实现了最先进的性能，包括与人类偏好对齐、客观正确性、安全性、对抗风格偏差以及最佳N缩放能力。消融研究证实了数据规模和高质量校准的有效性。

Conclusion: Skywork-Reward-V2系列在开放奖励模型方面取得了显著进展，突显了现有偏好数据集的未开发潜力，并展示了人机协同校准如何解锁更高的数据质量。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [14] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于注意力机制的深度学习方法，用于处理电子健康记录文本中的信息提取和多标签疾病预测问题。通过使用基于Transformer的架构和多层自注意力机制，该方法在MIMIC-IV数据集上表现出色，并在多种实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录文本的非结构化性质和高维语义复杂性所带来的挑战。

Method: 提出了一种基于注意力机制的深度学习方法，用于统一建模信息提取和多标签疾病预测。使用了基于Transformer的架构进行临床文本的表示学习，并采用了多层自注意力机制来捕捉关键的医疗实体及其上下文关系。然后应用基于Sigmoid的多标签分类器来预测多个疾病标签。模型还包含一个上下文感知的语义对齐机制，以增强其在典型医学场景中的表示能力。

Result: 实验结果表明，所提出的方法在多个性能指标上始终优于现有的代表性方法。模型在不同数据规模、干扰水平和模型深度配置下保持了强大的泛化能力。

Conclusion: 该研究开发的框架为处理现实世界的临床文本提供了高效的算法基础，并在多标签医学文本建模任务中具有实际意义。

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [15] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: LogitSpec is a training-free and plug-and-play method that improves speculative decoding by expanding the retrieval range and finding the most relevant reference as drafts, achieving significant speedup in LLM inference.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of retrieval-based SD failing to find matched and accurate draft tokens, LogitSpec is proposed to effectively expand the retrieval range and find the most relevant reference as drafts.

Method: LogitSpec generates draft tokens in two steps: (1) utilizing the last logit to speculate the next next token; (2) retrieving relevant reference for both the next token and the next next token.

Result: LogitSpec can achieve up to 2.61 × speedup and 3.28 mean accepted tokens per decoding step.

Conclusion: LogitSpec can achieve up to 2.61 × speedup and 3.28 mean accepted tokens per decoding step.

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [16] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: 本研究通过利用来自智力障碍人士的偏好反馈，改进了基于大型语言模型的自动文本简化系统，以实现更个性化的无障碍AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的ATS系统在训练过程中没有纳入关于文本简化的偏好反馈，导致缺乏针对目标群体代表特定需求的个性化。

Method: 本研究扩展了标准监督微调（SFT）方法，采用了一种计算效率高的大型语言模型（LLM）对齐技术——直接偏好优化（DPO）。此外，提出了一个开发个性化LLM基于ATS系统的流程，包括数据收集、模型选择、SFT和DPO后训练以及评估。

Result: 研究结果强调了目标群体成员在设计符合人类期望的个性化AI无障碍解决方案中的积极参与的必要性。

Conclusion: 本研究代表了向个性化包容性AI系统迈出的一步，通过结合文本简化专家和目标群体人员的见解，实现了针对特定目标群体的个性化。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [17] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: 本文提出了一种新的模块化框架，结合不确定性建模和微调的大语言模型（LLMs），以实现高效且准确的OOS检测。


<details>
  <summary>Details</summary>
Motivation: OOS意图检测是任务导向对话系统中的关键挑战，因为它确保了对未见过和模糊查询的鲁棒性。

Method: 首先应用不确定性估计到当前在现实世界TODS中部署的在范围内的意图检测分类器的输出，然后利用新兴的基于LLM的方法，其中微调的LLM被触发以对高不确定性实例做出最终决策。

Result: 与之前的方法不同，我们的方法有效地平衡了计算效率和性能，结合了传统方法和LLMs，并在关键的OOS检测基准上取得了最先进的结果，包括从已部署的TODS获取的真实世界OOS数据。

Conclusion: 本文提出的框架在OOS检测任务中表现出色，展示了其在实际应用中的潜力。

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [18] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: 本研究发现，外部信息可能会降低基于大型语言模型的立场检测性能，并揭示了信息偏差的风险。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在许多推理任务中被广泛采用，但关于外部信息是否能提高其立场检测性能的问题仍未得到解答。

Method: 我们对八个大型语言模型和三个数据集进行了系统评估，以研究维基百科和网络搜索等外部信息如何影响立场检测。

Result: 我们发现，在大多数情况下，这种信息会降低性能，宏F1分数下降高达27.9%。

Conclusion: 我们的研究结果表明，外部信息可能会对基于大型语言模型的立场分类器产生信息偏差风险，这与基于BERT的系统的研究结果相反。

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [19] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: 本文研究了任务导向对话（ToD）系统的设计考虑因素，并提出了一个基于LLM的统一系统LUSTER，该系统通过端到端强化学习结合短期和长期奖励，以提高任务成功和情感响应能力。研究结果表明，结合LLM能力和结构化奖励建模可以带来更稳健和情感响应的ToD系统。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在语言流畅性和上下文理解方面取得了显著进展，但构建有效且富有情感智能的ToD系统仍然是一个复杂的挑战。有效的ToD系统必须优化任务成功、情感理解和响应以及精确的信息传递，所有这些都在本质上嘈杂和模糊的对话环境中进行。

Method: 我们研究了ToD系统的架构、表示、优化以及情感考虑，并设置了涵盖这些设计考虑的系统，使用了一个由自然语言用户模拟器和不完善的自然语言理解模块组成的具有挑战性的评估环境。我们提出了LUSTER，一个基于LLM的统一系统，用于任务导向对话，并采用端到端强化学习，同时考虑短期（用户情绪）和长期（任务成功）奖励。

Result: 我们的研究结果表明，将LLM能力与结构化奖励建模相结合可以带来更稳健和情感响应的ToD系统。

Conclusion: 结合LLM能力和结构化奖励建模可以带来更稳健和情感响应的ToD系统，为下一代对话代理提供了一条实用的路径。

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [20] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 本文介绍了一个新的图表问答数据集，它基于可视化笔记本，并且与之前基准不同，它反映了更真实的分析推理流程。然而，即使是最先进的模型如GPT-4.1也仅达到69.3%的准确率，这表明该任务仍然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 为了创建一个更符合真实分析叙述的图表问答数据集，以反映生态有效的推理流程。

Method: 我们构建了一个新的数据集，用于图表问答（CQA），该数据集来源于可视化笔记本。

Result: 基准测试显示，GPT-4.1的准确率为69.3%，表明这种更真实的CQA设置带来了挑战。

Conclusion: 我们的数据集反映了更真实的CQA设置，而最先进的多模态大语言模型在该设置中表现出显著的性能差距。

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [21] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: 本文分析了全局评分和成对比较在模型评估中的优缺点，并提供了实验结果，以帮助选择合适的评估策略。


<details>
  <summary>Details</summary>
Motivation: 随着指令调优神经语言模型的出现，自然语言处理（NLP）的基准测试正在从传统的全局点评分（如 GLUE、BIG-bench、SWE-bench）向成对比较排行榜（如 LMSYS Arena）转变。本文旨在调查这两种评估方法的优缺点，以帮助选择适当的模型评估策略。

Method: 本文通过在合成和现实数据集上的计算实验，使用标准全局指标和流行的 Bradley-Terry 模型进行成对比较，研究了全局评分和成对比较的优缺点。

Result: 实验结果表明，全局评分能够提供更可靠的总体排名，但可能低估了具有罕见重大错误或低置信度的强模型。而成对比较在识别全局评分较低的模型中的强竞争者方面表现更好，尤其是在质量指标难以定义的情况下，但需要更多的比较才能收敛。

Conclusion: 本文通过实验发现，全局评分在提供可靠的整体排名方面更有效，但可能低估了具有罕见重大错误或低置信度的强模型。相反，成对比较在识别全局评分较低的模型中的强竞争者方面特别有效，尤其是在质量指标难以定义的情况下（例如文本生成），尽管如果平局频繁，它们需要更多的比较才能收敛。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [22] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: 本研究评估了预训练语言模型在低资源印尼本地语言中的可迁移性，发现多语言模型在已见语言上表现最佳，而MAD-X方法在提高性能方面效果显著，尤其是在已见和部分已见语言上。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在调查预训练语言模型在低资源印尼本地语言中的可迁移性，通过情感分析任务进行评估。

Method: 我们评估了零样本性能和基于适配器的迁移，在十种本地语言上使用不同类型的模型：单语印尼语BERT、多语言模型如mBERT和XLM-R，以及一种称为MAD-X的模块化适配器方法。我们还将目标语言分为三类：已见（在预训练中包含）、部分已见（未包含但与已见语言语言相关）和未见（在预训练数据中不存在且无关）。

Result: 多语言模型在已见语言上的表现最好，部分已见语言上表现中等，而未见语言上表现较差。MAD-X显著提高了性能，尤其是在已见和部分已见语言上，而无需目标语言的标记数据。分词分析显示，子词碎片化和词汇重叠与印尼语的相关性较弱，但它们并不能完全解释观察到的表现。模型对语言的先前接触（直接或通过相关语言）是最一致的转移成功预测因素。

Conclusion: 我们的研究结果表明，多语言模型在已见语言上的表现最好，部分已见语言上表现中等，而未见语言上表现较差。我们发现MAD-X显著提高了性能，尤其是在已见和部分已见语言上，而无需目标语言的标记数据。此外，我们对分词进行了进一步分析，发现子词碎片化和词汇重叠与印尼语的相关性较弱，但它们并不能完全解释观察到的表现。相反，模型对语言的先前接触（直接或通过相关语言）是最一致的转移成功预测因素。

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [23] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为AdamMeme的灵活、基于代理的评估框架，用于评估多模态大语言模型（mLLMs）对meme有害性的理解能力。该框架通过多代理协作，迭代更新带有挑战性样本的meme数据，从而暴露mLLMs在解释有害性方面的具体局限性。实验结果表明，该框架能够系统地揭示不同mLLMs的性能差异，并提供对模型特定弱点的深入分析。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准在提供最新和全面的评估方面存在局限，因为在线meme动态演变。需要一种更灵活、适应性强的评估方法来更好地理解mLLMs对meme有害性的理解能力。

Method: 我们提出了AdamMeme，一个灵活的基于代理的评估框架，通过多代理协作，迭代更新带有挑战性样本的meme数据，从而暴露mLLMs在解释有害性方面的具体局限性。

Result: 广泛的实验表明，我们的框架能够系统地揭示不同目标mLLMs的性能差异，并提供对模型特定弱点的深入、细粒度分析。

Conclusion: 我们的框架系统地揭示了不同目标mLLMs的性能差异，提供了对模型特定弱点的深入、细粒度分析。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [24] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文探讨了如何通过联合学习偏见和刻板印象检测任务来提高模型性能，并引入了一个新的数据集StereoBias。实验结果显示，联合训练能显著提高偏见检测效果，而这种改进源于偏见和刻板印象之间的联系。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的偏见和刻板印象可能导致伤害，尤其是在内容审核和决策等敏感领域。本文旨在解决偏见和刻板印象检测问题，并探索联合学习这些任务如何提高模型性能。

Method: 本文通过探索联合学习这些任务如何提高模型性能来解决偏见和刻板印象检测问题。我们引入了StereoBias，这是一个独特的数据集，用于在五个类别（宗教、性别、社会经济地位、种族、职业等）中进行偏见和刻板印象检测，以更深入地研究它们之间的关系。我们的实验比较了使用QLoRA的仅编码器模型和微调解码器模型。虽然仅编码器模型表现良好，但解码器模型也显示出有竞争力的结果。关键的是，联合训练偏见和刻板印象检测显著提高了偏见检测效果，相比单独训练它们。

Result: 实验结果表明，尽管仅编码器模型表现良好，但解码器模型也表现出竞争力。关键的是，联合训练偏见和刻板印象检测显著提高了偏见检测效果，而额外的实验与情感分析确认了这种改进源于偏见和刻板印象之间的联系，而不是多任务学习本身。

Conclusion: 这些发现突显了利用刻板印象信息来构建更公平、更有效的AI系统的价值。

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [25] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型（LLMs）和上下文学习来评估德国就业合同中条款的合法性。结果显示，使用全文资料可以适度提高性能，而检查指南显著提高了对无效条款的识别能力。然而，大型语言模型的性能仍不及人类律师。


<details>
  <summary>Details</summary>
Motivation: 法律工作以文本密集和资源密集为特点，给自然语言处理（NLP）研究带来了独特的挑战和机遇。虽然数据驱动的方法已经推动了该领域的发展，但它们缺乏可解释性和可信度，限制了其在动态法律环境中的应用。

Method: 我们与法律专家合作扩展了一个现有数据集，并探索了使用大型语言模型（LLMs）和上下文学习来评估德国就业合同中条款的合法性。

Result: 结果表明，全文资料适度提高了性能，而检查指南显著提高了无效条款的召回率和加权F1分数，达到80%。尽管有这些进展，使用全文资料的大型语言模型的性能仍然远低于人类律师。

Conclusion: 我们的研究突出了大型语言模型在协助律师审查合同合法性方面的潜力，同时也强调了所提出方法的局限性。

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [26] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: 本文研究了分词过程中表情符号和同形异义字的影响，并提出了确保数字文本在语料库中准确表示的方法，以提高语料库分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 分词是语料库语言学中的关键步骤，它为任何适用的定量方法提供了基础，并确保了定性方法的可靠性。然而，分词中的差异可能会影响语言数据的表示和分析结果的有效性。

Method: 该研究探讨了表情符号和同形异义字在分词过程中带来的挑战，并提出了确保数字文本在语料库中准确表示的方法。

Result: 研究发现，预处理表情符号和同形异义字对于保持语料库与源数据的一致性至关重要。此外，研究强调了对数字文本数据的语言和技术方面有深入了解的必要性。

Conclusion: 研究强调了对数字文本数据的语言和技术方面有深入了解的必要性，以提高语料库分析的准确性，并对基于语料库的研究中的定量和定性方法有重要意义。

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [27] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: MuRating是一种可扩展的框架，能够将高质量的英语数据质量信号转移到17种目标语言，通过成对比较聚合多个英语评分者，学习统一的文档质量分数，并通过翻译将这些判断投影到多语言评估器上进行训练。该方法在英语基准和多语言评估中都提高了平均准确率，特别是在知识密集型任务中取得了显著的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模型的选择方法几乎只关注英语，而数据质量是大型语言模型性能的关键驱动因素。因此，需要一种能够将高质量的英语数据质量信号转移到其他17种目标语言的框架。

Method: MuRating通过成对比较聚合多个英语“评分者”来学习统一的文档质量分数，然后通过翻译将这些判断投影到多语言评估器上，以单语、跨语言和并行文本对进行训练。

Result: MuRating在网页数据上选择了平衡的英语和多语言内容子集，用于预训练一个12亿参数的LLaMA模型。与强基线方法（包括QuRater、AskLLM、DCLM等）相比，我们的方法在英语基准和多语言评估中都提高了平均准确率，尤其是在知识密集型任务中取得了显著的提升。

Conclusion: MuRating在英语基准和多语言评估中都提高了平均准确率，特别是在知识密集型任务中取得了显著的提升。我们进一步分析了翻译保真度、选择偏差和叙述性材料的不足，并指出了未来工作的方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [28] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: 本研究探讨了Llama-3.3-70B-Instruct模型的评估意识，发现其能够区分测试和部署阶段，并且安全评估被模型视为人工或不真实。这强调了确保评估可信度的重要性，并展示了如何利用模型内部来支持安全审计。


<details>
  <summary>Details</summary>
Motivation: 评估意识对AI治理框架和行业承诺的可靠性有重要影响，因此需要研究模型是否能够区分测试和部署阶段。

Method: 我们使用线性探测器来分离现实世界的评估和部署提示，以研究Llama-3.3-70B-Instruct中的评估意识。

Result: 线性探测器可以分离真实世界的评估和部署提示，表明当前模型内部表示这种区别。此外，当前的安全评估被探测器正确分类，表明它们似乎对模型来说是人工或不真实的。

Conclusion: 我们的研究强调了确保可信评估的重要性，并理解欺骗能力。更广泛地说，我们的工作展示了如何利用模型内部来支持安全审计中的黑盒方法，特别是对于未来在评估意识和欺骗方面更擅长的模型。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [29] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 该研究探讨了多模态AI模型在面对不一致输入时的行为，并发现模型倾向于偏重某一模态。研究还揭示了内部表示结构和特定注意力头的作用，以及可以调整这些头以提高性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型需要越来越多地处理多模态数据，理解当输入流提供冲突信息时模型的行为变得至关重要。

Method: 通过向视觉-语言模型提供不一致的输入（例如，一张狗的图片与标题“一张猫的照片”配对），并要求模型报告特定模态中的信息，研究了模型的行为。

Result: 研究发现，模型通常会偏重某一模态，不同模型在偏重的模态上存在差异。此外，还发现了能够重新构建表示以偏重某一模态的特定注意力头，以及促进根据指令请求的模态回答的“路由器头”。

Conclusion: 该研究为识别和控制模型在复杂多模态环境中检测和解决冲突信号提供了重要步骤。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [30] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 本文对MDACE数据集进行了深入分析，并从应用角度对当前的可解释医疗编码系统进行了合理性评估。我们的研究结果表明，真实证据与代码描述在一定程度上是一致的。对最先进的方法进行调查显示出与真实证据的高度重叠。我们提出了匹配度量，并强调了成功和失败的案例。基于我们的发现，我们为开发和评估可解释的医疗编码系统提供了建议。


<details>
  <summary>Details</summary>
Motivation: 自动医疗编码有潜力减轻文档和计费过程。对于医疗编码员和监管机构来说，透明度在这一任务中起着重要作用，这可以通过可解释性方法来实现。然而，这些方法的评估主要局限于短文本和二进制设置，因为标注数据稀缺。

Method: 我们对MDACE数据集进行了深入分析，并从应用角度对当前的可解释医疗编码系统进行了合理性评估。

Result: 我们的研究结果表明，真实证据与代码描述在一定程度上是一致的。对最先进的方法进行调查显示出与真实证据的高度重叠。我们提出了匹配度量，并强调了成功和失败的案例。

Conclusion: 我们的研究结果表明，真实证据与代码描述在一定程度上是一致的。对最先进的方法进行调查显示出与真实证据的高度重叠。我们提出了匹配度量，并强调了成功和失败的案例。基于我们的发现，我们为开发和评估可解释的医疗编码系统提供了建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [31] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: 本文比较了小语言模型生成的结构化输出的可解析性，发现 JSON 在解析方面表现最佳，并提供了在临床环境中使用语言模型的实用建议。


<details>
  <summary>Details</summary>
Motivation: 为了在隐私敏感的临床环境中部署语言模型，需要选择合适的序列化格式和设计有效的提示。

Method: 我们对小语言模型生成的结构化输出的可解析性进行了比较分析，评估了三种常用的序列化格式：JSON、YAML 和 XML。

Result: JSON 一直表现出最高的可解析性。结构鲁棒性通过有针对性的提示和更大的模型得到改善，但对于较长的文档和某些类型的笔记则会下降。

Conclusion: 这些发现为在隐私敏感的临床环境中选择序列化格式和设计提示提供了实用指导。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [32] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 该研究通过分析低困惑度序列，揭示了大型语言模型如何利用和复制其训练数据，并为更好地理解训练数据对模型行为的影响提供了途径。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，了解特定训练数据如何影响其输出对于透明度、责任性、隐私和公平性至关重要。该研究旨在探索模型如何利用和复制其训练数据。

Method: 该研究引入了一种系统的方法，专注于分析低困惑度序列，即由模型生成的高概率文本段落。该方法能够可靠地提取跨不同主题的长序列，同时避免退化，并将其追溯到训练数据中的来源。

Result: 研究发现，这些低困惑度段落中相当一部分无法映射到语料库中。对于那些匹配的段落，研究量化了它们在源文档中的分布，突显了逐字回忆的范围和性质。

Conclusion: 该研究通过分析低困惑度序列，揭示了大型语言模型如何利用和复制其训练数据，并为更好地理解训练数据对模型行为的影响提供了途径。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [33] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: EKA-EVAL 是一个统一且可生产使用的评估框架，集成了超过 35 个基准测试，包括 10 个印地语特定的数据集，涵盖了推理、数学、工具使用、长上下文理解和阅读理解等类别。它提供了分布式推理、量化和多 GPU 使用的内置支持。EKA-EVAL 被定位为第一个针对全球和印地语 LLMs 的端到端、可扩展的评估套件，显著降低了多语言基准测试的门槛。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，需要超越以英语为中心的基准测试的评估框架，以满足像印度这样的语言多样地区的需求。现有的印度语言评估工具在基准测试覆盖范围上有所不足。

Method: EKA-EVAL 集成了超过 35 个基准测试，包括 10 个印地语特定的数据集，涵盖了推理、数学、工具使用、长上下文理解和阅读理解等类别。它提供了分布式推理、量化和多 GPU 使用的内置支持。

Result: EKA-EVAL 在基准测试覆盖范围上优于现有的印度语言评估工具，并且提供了更广泛的评估功能。它被定位为第一个针对全球和印地语 LLMs 的端到端、可扩展的评估套件。

Conclusion: EKA-EVAL 是一个统一且可生产使用的评估框架，它为全球和印地语大语言模型提供了端到端、可扩展的评估套件，显著降低了多语言基准测试的门槛。该框架是开源的，并且是正在进行的 EKA 计划的一部分，旨在扩展到超过 100 个基准，并建立一个强大的多语言评估生态系统。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [34] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: DIY-MKG是一个开源系统，旨在支持多语种学习者构建个性化的词汇知识图谱，并通过LLM实现动态、个性化的测验生成，同时提供反馈机制以优化提示。评估结果表明，DIY-MKG在多语言中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的语言学习工具，即使是由大型语言模型（LLMs）驱动的工具，往往缺乏对多语种学习者在多种语言词汇之间建立语言联系的支持，提供有限的个性化定制，且存在有害的认知卸载问题。为了解决这些限制，我们设计了DIY-MKG，一个支持多语种语言学习的开源系统。

Method: DIY-MKG允许用户通过LLM建议的相关词进行选择性扩展来构建个性化的词汇知识图谱，并通过丰富的注释功能和自适应复习模块增强学习效果。此外，DIY-MKG还允许用户标记错误的测验问题，从而提高用户参与度并提供反馈循环以优化提示。

Result: DIY-MKG的评估结果显示，基于LLM的组件在多语言中表现出可靠的词汇扩展和高度准确的测验生成，验证了DIY-MKG的鲁棒性。

Conclusion: DIY-MKG是一个支持多语言学习的开源系统，能够构建个性化的词汇知识图谱，并通过丰富的注释功能和自适应复习模块增强学习效果。评估结果表明，DIY-MKG在多语言中的词汇扩展是可靠且公平的，生成的测验具有高度准确性，验证了其鲁棒性。

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [35] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为MiCoTA的框架，用于提高小语言模型在长链式思维推理任务中的表现。通过中间大小的模型作为教师助手，并利用中间长度的链式思维序列，显著提升了小语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 由于小语言模型（SLMs）在学习长形式链式思维（CoT）推理方面存在局限性，导致其性能不足，因此需要一种有效的解决方案来改善这一问题。

Method: 我们引入了MiCoTAl框架，利用中间大小的模型作为教师助手，并使用中间长度的链式思维序列来弥合容量和推理长度的差距。

Result: 实验结果表明，通过应用MiCoTA，Qwen2.5-7B-Instruct和Qwen2.5-3B-Instruct在AIME2024、AMC、Olympiad、MATH-500和GSM8K基准测试中分别实现了平均分数提升3.47和3.93。

Conclusion: 我们的研究揭示了小语言模型在长链式思维推理中的学习能力差距，并通过MiCoTA框架显著提升了它们的推理性能。此外，我们的方法生成的数据更接近基础小语言模型的分布，为未来的研究提供了新的方向。

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [36] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的剪枝算法，通过有选择地剪枝模型的高层注意力头并引入自适应重缩放参数，显著提高了生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的无训练结构剪枝方法通常使用启发式度量标准，这会不加区分地移除所有剪枝层中的某些注意力头，而没有考虑它们在网络架构中的位置。

Method: 我们提出了一种新的剪枝算法，该算法有选择地剪枝模型的高层注意力头，并引入了一个自适应重缩放参数来校准剪枝后的表示尺度。

Result: 我们在多个大型语言模型上进行了广泛的实验，包括LLaMA3.1-8B、Mistral-7B-v0.3、Qwen2-7B和Gemma2-9B。评估涵盖了27个数据集上的生成和判别任务。结果一致表明，我们的方法优于现有的结构剪枝方法。

Conclusion: 我们的方法在生成任务中显著优于现有的基线，证明了其有效性。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [37] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一个全面的综述，并提供了AI4Research的统一视角。具体来说，我们的工作主要贡献如下：(1) 系统分类法：我们首先引入了一个系统分类法，对AI4Research中的五种主流任务进行分类。(2) 新前沿：然后，我们识别了关键的研究差距，并强调了有前景的未来方向，重点在于自动化实验的严谨性和可扩展性，以及社会影响。(3) 丰富的应用和资源：最后，我们整理了大量的资源，包括相关跨学科应用、数据语料库和工具。


<details>
  <summary>Details</summary>
Motivation: 由于最近在人工智能（AI）方面的进展，特别是在大型语言模型（LLMs）如OpenAI-o1和DeepSeek-R1方面，在逻辑推理和实验编码等复杂领域表现出色。受这些进展的激励，许多研究探讨了AI在创新过程中的应用，尤其是在科学研究的背景下。这些AI技术主要旨在开发能够在广泛科学领域自主进行研究过程的系统。尽管取得了显著进展，但关于AI for Research（AI4Research）的全面综述仍然缺失，这阻碍了我们对该领域的理解并阻碍了进一步发展。

Method: 本文提出了一个全面的综述，并提供了AI4Research的统一视角。具体来说，我们的工作主要贡献如下：(1) 系统分类法：我们首先引入了一个系统分类法，对AI4Research中的五种主流任务进行分类。(2) 新前沿：然后，我们识别了关键的研究差距，并强调了有前景的未来方向，重点在于自动化实验的严谨性和可扩展性，以及社会影响。(3) 丰富的应用和资源：最后，我们整理了大量的资源，包括相关跨学科应用、数据语料库和工具。

Result: 本文提出了一个全面的综述，并提供了AI4Research的统一视角。具体来说，我们的工作主要贡献如下：(1) 系统分类法：我们首先引入了一个系统分类法，对AI4Research中的五种主流任务进行分类。(2) 新前沿：然后，我们识别了关键的研究差距，并强调了有前景的未来方向，重点在于自动化实验的严谨性和可扩展性，以及社会影响。(3) 丰富的应用和资源：最后，我们整理了大量的资源，包括相关跨学科应用、数据语料库和工具。

Conclusion: 本文希望为研究社区提供这些资源的快速访问，并激发AI4Research中的创新突破。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [38] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: This paper introduces GAPO, a novel fine-tuning paradigm for aligning LLMs with diverse human preferences by framing the problem as a multi-objective optimization task. GAPO uses multiple-gradient descent to balance conflicting objectives and achieve Pareto optimal solutions, demonstrating superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflicting.

Method: GAPO, a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. P-GAPO incorporates user preferences across different objectives to achieve Pareto solutions.

Result: GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.

Conclusion: GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [39] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: 本研究通过收集高质量的推理轨迹并进行系统分析，发现选择困难示例能更有效地提升学生模型的推理能力，并在多个基准测试中取得了优于现有数据集的结果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨教师模型提供的哪种推理示例最能提高学生模型的推理能力，目前尚无系统研究。

Method: 通过从强教师模型中选择推理轨迹来收集高质量的“NaturalThoughts”，并进行系统分析以确定影响推理能力蒸馏的因素。

Result: 发现仅通过随机采样扩大数据规模是一个强大的基线，并且选择需要更多元化推理策略的困难示例更具样本效率。

Conclusion: 训练使用NaturalThoughts的模型在通用STEM推理基准测试中表现优于现有的推理数据集，如OpenThoughts、LIMO等。

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [40] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 本文提出了一种以决策为导向的评估框架，通过直接测量生成文本对人类和LLM决策结果的影响来进行评估。研究发现，当仅依赖摘要时，人类或LLM代理都无法持续超越随机表现。然而，更丰富的分析评论使人类和LLM团队能够显著优于单独的人类或代理基线。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成（NLG）越来越多地部署在高风险领域，但常见的内在评估方法，如n-gram重叠或句子合理性，与实际决策效果的关联性较弱。

Method: 我们提出了一种以决策为导向的评估框架，通过直接测量生成文本对人类和大型语言模型（LLM）决策结果的影响来进行评估。使用市场摘要文本（包括客观的早晨摘要和主观的收盘钟分析）作为测试案例，我们根据由这些文本唯一指导的人类投资者和自主LLM代理执行的交易的财务表现来评估决策质量。

Result: 我们的研究发现，当仅依赖摘要时，人类或LLM代理都无法持续超越随机表现。然而，更丰富的分析评论使人类和LLM团队能够显著优于单独的人类或代理基线。

Conclusion: 我们的方法强调了通过生成文本促进人类和LLM之间协同决策的能力的重要性，并突显了传统内在指标的关键局限性。

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [41] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: 本研究比较了两种先进的自动语音识别模型在低资源语言Bangla上的表现，发现Wav2Vec-BERT模型在多个评估指标上优于Whisper，且所需计算资源更少。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估两种最先进的自动语音识别（ASR）模型在低资源语言Bangla上的表现，并探索如何提高其性能。

Method: 通过系统的微调和超参数优化（包括学习率、训练轮数和模型检查点选择），使用Mozilla Common Voice-17和OpenSLR两个公开数据集对模型进行了评估。

Result: Wav2Vec-BERT模型在所有关键评估指标上都优于Whisper，展示了更优越的性能，同时需要更少的计算资源。

Conclusion: Wav2Vec-BERT模型在所有关键评估指标上都优于Whisper，展示了更优越的性能，同时需要更少的计算资源，并为在低资源语言环境中开发稳健的语音识别系统提供了有价值的见解。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [42] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在保持辩论和理解对话结构方面的能力。结果表明，LLMs可以产生有说服力的辩论，但缺乏对对话深层结构的理解。此外，人们对AI参与的意识会影响他们对论点的批判性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在保持高层次、有说服力的对话方面表现出色。它们正在被快速部署为聊天机器人和评估者，在敏感领域如同行评审和心理健康应用中。这以及关于它们推理能力的不同说法，呼吁对LLMs及其对对话的理解进行更仔细的检查。

Method: 我们首先评估了LLMs保持辩论的能力——这是最纯粹但最复杂的交流形式之一。然后我们衡量这种能力如何与他们对所讨论内容的理解相关，即他们对对话结构和语用上下文的理解。

Result: 我们发现LLMs能够保持连贯且有说服力的辩论，常常影响参与者的信念和观众的信念。然而，当我们向LLMs询问它们对对话深层结构的理解时，它们无法展示这种理解。此外，人们意识到或怀疑AI的参与会促使他们更批判性地看待所提出的论点。

Conclusion: 我们的发现将LLMs作为评估者的不足与它们（不）理解上下文的能力联系起来。更广泛地说，对于论证理论领域，我们认为，如果一个代理能够令人信服地维持对话，它不需要知道它在谈论什么。因此，对语用上下文和连贯性的建模是次要的。

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: 本文提出了一种名为PathCoT的新方法，通过整合病理学专家知识和自我评估来提高多模态大语言模型在病理视觉推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在应用于病理视觉推理任务时面临重大挑战：(1) LLMs由于缺乏领域特定信息而表现不佳，这可能导致模型幻觉。(2) CoT中的额外推理步骤可能引入错误，导致答案分歧。

Method: 我们提出了PathCoT，这是一种新颖的零样本CoT提示方法，它将病理学专家知识整合到MLLMs的推理过程中，并结合自我评估以减轻答案的分歧。

Result: 在PathMMU数据集上的实验结果证明了我们方法的有效性。

Conclusion: 实验结果表明，我们的方法在病理视觉理解和推理方面是有效的。

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [44] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段训练框架，以提高脱毒文本的性能，减少对注释数据的依赖，并增强模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法往往难以同时实现强大的脱毒性能、语义保留和对分布外数据的鲁棒性。此外，它们通常依赖于昂贵的手动注释平行语料库，数据效率低下。

Method: 我们提出了一种两阶段训练框架，联合优化数据效率、语义保留和模型泛化。首先，在一小部分高质量、过滤后的平行数据上进行监督微调，以建立一个强大的初始值。然后，我们利用未标记的有毒输入和自定义设计的奖励模型，使用组相对策略优化来训练LLM。

Result: 实验结果表明，我们的方法有效缓解了之前工作中面临的权衡，实现了最先进的性能，提高了泛化能力，并显著减少了对注释数据的依赖。

Conclusion: 我们的方法有效缓解了之前工作中面临的权衡，实现了最先进的性能，提高了泛化能力，并显著减少了对注释数据的依赖。

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [45] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: SPRO是一种新的过程感知强化学习框架，通过内在过程奖励和MSA技术，提高了训练效率和测试准确性，同时保持了策略熵的稳定性，并减少了响应长度。


<details>
  <summary>Details</summary>
Motivation: Process Reinforcement Learning (PRL) 在增强大型语言模型（LLMs）的推理能力方面显示出巨大潜力，但引入额外的过程奖励模型会带来显著的计算开销，并且没有统一的理论框架用于过程级优势估计。

Method: 提出了一种新的框架SPRO，通过两个关键创新：(1) 理论上证明了过程奖励可以从策略模型本身中内在地推导出来；(2) 引入了定义明确的累积过程奖励和掩码步骤优势(MSA)，以在共享提示采样组内进行严格的逐步动作优势估计。

Result: 实验结果表明，SPRO在训练效率上比传统的GRPO高出3.4倍，测试准确性提高了17.5%。此外，SPRO在整个训练过程中保持了稳定且较高的策略熵，同时将平均响应长度减少了约1/3，证明了足够的探索和防止奖励黑客行为。

Conclusion: SPRO在训练效率和测试准确性方面优于传统方法，同时保持了稳定的策略熵并减少了平均响应长度，证明了其在工业应用中的优势。

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [46] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: 本文提出了一种名为Prefix-RFT的混合方法，结合了监督微调和强化微调的优点，在数学推理任务中表现出色，具有良好的鲁棒性和易于集成的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练技术在SFT和RFT之间存在权衡，SFT在模仿演示数据方面表现出色，但可能导致行为克隆的问题；而RFT虽然能显著提升模型性能，但容易学习到意外行为，并且对初始策略高度敏感。因此，需要一种更有效的后训练方法。

Method: 本文提出了Prefix-RFT，这是一种结合了监督微调（SFT）和强化微调（RFT）的方法，通过数学推理问题作为测试平台进行实证分析。

Result: Prefix-RFT在数学推理问题上表现出色，优于单独的SFT和RFT，以及并行混合策略的RFT方法。同时，它能够无缝集成到现有的开源框架中，只需对标准RFT流程进行最小修改。

Conclusion: 本文提出了一种统一的视角，并引入了Prefix-RFT，这是一种结合了从演示和探索中学习的混合方法。实验表明，Prefix-RFT不仅优于单独的SFT和RFT，还优于并行混合策略的RFT方法。此外，消融研究证实了该方法对演示数据质量和数量的鲁棒性。本文希望为LLM后训练提供一个新的视角，表明一种谨慎整合演示和探索的统一范式可能是未来研究的有前景方向。

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [47] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER is a novel black-box method for LLM post-training that addresses privacy and security concerns while providing strong theoretical guarantees and empirical performance improvements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of gradient-based optimization in deep learning, such as reliance on large volumes of labeled data and vulnerability to privacy and security concerns. It also seeks to overcome the challenges of black box methods, including poor scalability and high computational costs.

Method: BBoxER is an evolutionary black-box method that induces an information bottleneck via implicit compression of the training data, leveraging the tractability of information flow to provide theoretical bounds on generalization, differential privacy, susceptibility to data poisoning attacks, and robustness to extraction attacks.

Result: Experiments with LLMs demonstrate that BBoxER can improve performance and generalize well on a benchmark of reasoning datasets, positioning it as an attractive add-on to gradient-based optimization.

Conclusion: BBoxER provides a promising alternative to gradient-based optimization for LLM post-training, offering strong theoretical guarantees and empirical performance improvements.

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [48] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: 本文提出了一种适用于CPU的LoRA微调方法，通过组合预训练适配器来提升模型性能，为计算资源有限的用户提供了一种实用的替代方案。


<details>
  <summary>Details</summary>
Motivation: 由于依赖于基于GPU的训练，LoRAs的广泛应用受到限制。我们旨在为计算资源有限的用户提供一种有效的解决方案，特别是那些仅能使用标准笔记本电脑CPU的用户。

Method: 我们提出了一种基于理论的LoRA微调方法，通过利用大量预训练的适配器来学习一个元操作符，该操作符将任何输入数据集映射到一组LoRA权重。我们的管道在CPU上通过轻量级组合现有LoRA直接构建适配器。

Result: 虽然生成的适配器无法达到GPU训练的性能，但它们在下游任务中始终优于基础Mistral模型。

Conclusion: 我们的方法提供了一种实用且可访问的替代传统GPU微调的方法，即使在计算资源有限的情况下也能有效提升模型性能。

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [49] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1 is a reflective generative model that achieves OpenAI o3's performance through SPRM, reducing PRM parameters and providing three reasoning effort modes.


<details>
  <summary>Details</summary>
Motivation: To reduce PRM parameters for efficient reasoning and provide three reasoning effort modes based on controllable thinking length.

Method: Introduce a reflective generative model MetaStone-S1 that obtains OpenAI o3's performance via the self-supervised process reward model (SPRM).

Result: Experiments demonstrate that MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with only 32B parameter size.

Conclusion: MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with only 32B parameter size.

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve 是一个 AI 辅助评分平台，利用大型语言模型来转录和评估学生作业，从而减少评分时间并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 手动评分在大型大学 STEM 课程中是一个主要瓶颈，需要一种更高效和准确的解决方案。

Method: Pensieve 利用大型语言模型 (LLMs) 来转录和评估学生作业，并提供符合评分标准的分数、转录文本和置信度评级。它支持从扫描的学生提交到最终反馈的整个评分流程。

Result: Pensieve 已在 20 多所机构的现实课程中部署，并已评分超过 300,000 份学生回答。结果显示，Pensieve 平均减少了 65% 的评分时间，并且对于高置信度预测，与教师评分的协议率达到 95.4%。

Conclusion: Pensieve 是一个有效的 AI 辅助评分平台，能够显著减少评分时间并保持与教师评分的高度一致性。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [51] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: 本文提出了一种新的分布特征建模方法T3DM，以调整模型并确保模型推理的全局一致性，并设计了一种基于对抗训练的负采样策略，生成更高质量的负三元组。实验结果表明，T3DM在大多数情况下比最先进的基线方法表现更好、更稳健。


<details>
  <summary>Details</summary>
Motivation: Most research on TKG reasoning (TKGR) focuses on modelling the repetition of global facts and designing patterns of local historical facts. However, they face two significant challenges: inadequate modeling of the event distribution shift between training and test samples, and reliance on random entity substitution for generating negative samples, which often results in low-quality sampling.

Method: We propose a novel distributional feature modeling approach for training TKGR models, Test-Time Training-guided Distribution shift Modelling (T3DM), to adjust the model based on distribution shift and ensure the global consistency of model reasoning. In addition, we design a negative-sampling strategy to generate higher-quality negative quadruples based on adversarial training.

Result: Extensive experiments show that T3DM provides better and more robust results than the state-of-the-art baselines in most cases.

Conclusion: T3DM provides better and more robust results than the state-of-the-art baselines in most cases.

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [52] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Main category: cs.HC

TL;DR: 本文探讨了老年人，尤其是城市中的移民，如何通过AI辅助的共同创造来表达他们的个人叙述。通过试点研讨会，参与者分享了移民的记忆，并使用AI建议的小篆字符和物理材料创造了新的字符形式。这种方法重新定位了AI作为支持机制的角色，并在社会技术系统中支持叙事自主性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索老年人，特别是城市中的移民，如何通过AI辅助的共同创造来表达那些碎片化、未被充分代表或难以用言语表达的个人叙述。

Method: 本文通过一个结合口头故事讲述和汉字符号重构的试点研讨会，让参与者分享了移民的记忆，并使用由大型语言模型（LLM）建议的小篆字符和物理材料共同创造了新的字符形式。

Result: 参与者在人类指导和软AI存在的支持下，将生活经历转化为视觉和触觉表达，而无需数字素养。这种新方法为人类-AI协作和老龄化提供了新的视角。

Conclusion: 本文提出了一种新的方法，通过AI辅助的共同创造，使老年人特别是城市中的移民能够表达他们的个人叙述。这种方法重新定位了AI作为支持机制的角色，并在社会技术系统中支持叙事自主性。

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [53] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
*Kumarmanas Nethil,Vaibhav Mishra,Kriti Anandan,Kavya Manohar*

Main category: eess.AS

TL;DR: 我们提出了一种开源框架，用于命令式语音输入，通过语音活动检测和并行转录提高效率，并在实际环境中得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有的在线系统资源密集，而批处理处理的延迟较高，因此需要一种折中的解决方案。

Method: 我们使用语音活动检测（VAD）来分割音频，并使用Whisper模型并行转录这些段，从而实现了跨音频的高效多路复用。

Result: 评估结果显示，随着用户并发数的增加，延迟持续减少，相比顺序批处理有显著改进。

Conclusion: 我们的框架在实际环境中展示了计算资源利用的最大化，并且在印度的法庭中得到了部署。

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
*Harsh Joshi,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Niharika Jain,Sarthak Jain,Jiechao Gao,Usman Naseem*

Main category: cs.IR

TL;DR: 本文分析了CLIP、BLIP和LXMERT在多种任务中的性能，发现它们在泛化能力和专业化之间存在权衡，这为VLMs的工业应用和进一步开发提供了指导。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）正在推动多模态AI的发展，但它们在不同任务中的性能一致性尚未得到充分研究。

Method: 我们对CLIP、BLIP和LXMERT在涵盖检索、描述生成和推理的多样化数据集上进行了基准测试。我们的评估包括任务准确性、生成质量、效率和一个新颖的跨数据集一致性（CDC）指标。

Result: CLIP表现出最强的泛化能力（CDC：0.92），BLIP在精心筛选的数据上表现优异，而LXMERT在结构化推理方面领先。

Conclusion: 这些结果揭示了泛化与专业化之间的权衡，为VLMs的工业部署提供了信息，并指导开发向稳健、任务灵活的架构发展。

Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their
performance consistency across tasks is underexamined. We benchmark CLIP, BLIP,
and LXMERT across diverse datasets spanning retrieval, captioning, and
reasoning. Our evaluation includes task accuracy, generation quality,
efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows
strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT
leads in structured reasoning. These results expose trade-offs between
generalization and specialization, informing industrial deployment of VLMs and
guiding development toward robust, task-flexible architectures.

</details>


### [55] [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
*Pranav Jadhav*

Main category: cs.IR

TL;DR: 本文首次在超声心动图领域应用DPR进行患者队列检索，提出了一种系统的方法，并展示了优于现有方法的模型性能。


<details>
  <summary>Details</summary>
Motivation: 患者队列检索在医学研究和临床实践中至关重要，但目前在超声心动图领域缺乏有效的解决方案。

Method: 通过将非结构化的超声心动图EHR数据集转换为查询-段落数据集，将问题框定为队列检索任务，并设计了受现实临床场景启发的评估指标。此外，还提出了一个自定义训练的DPR嵌入模型。

Result: 自定义训练的DPR嵌入模型在性能上优于传统和现成的SOTA方法。

Conclusion: 这是首次在超声心动图领域应用DPR进行患者队列检索的工作，建立了可以适应其他医学领域的框架。

Abstract: Patient cohort retrieval is a pivotal task in medical research and clinical
practice, enabling the identification of specific patient groups from extensive
electronic health records (EHRs). In this work, we address the challenge of
cohort retrieval in the echocardiography domain by applying Dense Passage
Retrieval (DPR), a prominent methodology in semantic search. We propose a
systematic approach to transform an echocardiographic EHR dataset of
unstructured nature into a Query-Passage dataset, framing the problem as a
Cohort Retrieval task. Additionally, we design and implement evaluation metrics
inspired by real-world clinical scenarios to rigorously test the models across
diverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding
model that demonstrates superior performance compared to traditional and
off-the-shelf SOTA methods.To our knowledge, this is the first work to apply
DPR for patient cohort retrieval in the echocardiography domain, establishing a
framework that can be adapted to other medical domains.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [56] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨模态框架cRID，用于检测和处理街景录音数据集中的个人可识别信息（PII），并在实际的跨数据集Re-ID场景中展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于街景录音数据集存在显著的隐私风险，特别是对于行人而言，因为这些数据集包含超出生物特征（如面部）的个人可识别信息（PII），因此需要一种有效的方法来检测和处理PII。

Method: 本文提出了一种新的跨模态框架cRID，结合了大型视觉-语言模型、图注意力网络和表示学习，以检测文本可描述的PII线索并增强人员重识别（Re-ID）。

Result: 实验结果表明，cRID框架在实际的跨数据集Re-ID场景中表现出色，尤其是在从Market-1501到CUHK03-np的数据集中，证明了其实际应用价值。

Conclusion: 本文提出了cRID框架，该框架结合了大型视觉-语言模型、图注意力网络和表示学习，以检测可描述的PII线索并增强人员重识别（Re-ID）。实验结果表明，在实际的跨数据集Re-ID场景中，特别是在从Market-1501到CUHK03-np的数据集中，框架表现出色，展示了其实际应用价值。

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [57] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: 本文介绍了W-CODA研讨会的细节，旨在探索下一代自动驾驶边缘情况解决方案，并通过最先进的多模态感知和理解技术来推动这一目标。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是探索下一代自动驾驶边缘情况解决方案，以推动自动驾驶技术的发展。

Method: 本文介绍了W-CODA研讨会的组织方式，包括邀请来自学术界和工业界的5位演讲者分享他们的最新进展和观点，以及收集研究论文并举办双轨挑战赛。

Result: 本文的结果是成功举办了W-CODA研讨会，并通过双轨挑战赛收集了相关研究论文。

Conclusion: 本文介绍了W-CODA研讨会的细节，旨在探索下一代自动驾驶边缘情况解决方案，并通过最先进的多模态感知和理解技术来推动这一目标。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [58] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: 本文提出通过自然语言进行意图和推理通信，以解决多智能体协作驾驶中的通信问题，从而提高交通安全性、效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有通信媒介在带宽效率、信息完整性和代理互操作性方面存在局限，传统方法忽略了决策级融合。

Method: 提出使用自然语言进行显式意图和推理通信，以平衡语义密度和通信带宽，并适应实时条件。

Result: 通过直接沟通意图、理由和决策，将协作驾驶从被动的感知数据共享转变为积极的协调。

Conclusion: 自然语言能够提升智能交通系统的安全、效率和透明度。

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [59] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: 本文提出了一种名为'数据代理'的架构，用于协调数据+AI生态系统，并讨论了其设计挑战和示例。


<details>
  <summary>Details</summary>
Motivation: 传统数据+AI系统在语义理解、推理和规划方面的能力有限，而大型语言模型（LLMs）在这些方面表现出色。因此，需要将LLM技术融入数据系统中，以有效协调数据+AI应用。

Method: 本文提出了'数据代理'的概念，这是一种整合知识理解、推理和规划能力的架构，用于协调数据+AI生态系统。

Result: 本文探讨了设计数据代理所涉及的挑战，如理解数据/查询/环境/工具、编排流水线/工作流、优化和执行流水线以及促进流水线自我反思。此外，还介绍了数据代理系统的示例，包括数据科学代理、数据分析代理和数据库管理员（DBA）代理。

Conclusion: 本文提出了一个名为'数据代理'的全面架构，旨在协调数据+AI生态系统，并解决了数据相关任务中的挑战。然而，设计数据代理系统仍然面临一些开放性挑战。

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>

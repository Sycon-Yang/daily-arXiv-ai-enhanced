<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 97]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.IR](#cs.IR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches](https://arxiv.org/abs/2508.00864)
*Margarita Bugueño,Gerard de Melo*

Main category: cs.CL

TL;DR: 本文提出了一种学习数据驱动图结构的方法，以改进文档分类，避免了手动设计和领域依赖性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在文档分类中，基于图的模型有效地捕获文档结构，克服序列长度限制并增强上下文理解。然而，大多数现有的图文档表示依赖于启发式方法、特定领域的规则或专家知识。

Method: 我们提出了一种方法来学习数据驱动的图结构，消除了手动设计的需要并减少了领域依赖性。我们的方法构建了同质加权图，其中句子作为节点，边通过自注意力模型学习以识别句子对之间的依赖关系。

Result: 在三个文档分类数据集上的实验表明，学习的图始终优于基于启发式的图，实现了更高的准确率和F1分数。此外，我们的研究证明了统计过滤在提高分类鲁棒性方面的有效性。

Conclusion: 我们的研究展示了自动图生成在传统启发式方法上的潜力，并为NLP中的更广泛应用打开了新的方向。

Abstract: In document classification, graph-based models effectively capture document
structure, overcoming sequence length limitations and enhancing contextual
understanding. However, most existing graph document representations rely on
heuristics, domain-specific rules, or expert knowledge. Unlike previous
approaches, we propose a method to learn data-driven graph structures,
eliminating the need for manual design and reducing domain dependence. Our
approach constructs homogeneous weighted graphs with sentences as nodes, while
edges are learned via a self-attention model that identifies dependencies
between sentence pairs. A statistical filtering strategy aims to retain only
strongly correlated sentences, improving graph quality while reducing the graph
size. Experiments on three document classification datasets demonstrate that
learned graphs consistently outperform heuristic-based graphs, achieving higher
accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness
of the statistical filtering in improving classification robustness. These
results highlight the potential of automatic graph generation over traditional
heuristic approaches and open new directions for broader applications in NLP.

</details>


### [2] [FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts](https://arxiv.org/abs/2508.00889)
*Hagyeong Shin,Binoy Robin Dalal,Iwona Bialynicka-Birula,Navjot Matharu,Ryan Muir,Xingwei Yang,Samuel W. K. Wong*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法，用于自动评估生成分析客服对话输出的AI系统的事实性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）会产生幻觉，产生不基于输入、参考材料或现实世界知识的自然语言输出。在企业应用中，这种幻觉可能特别有害。分析和总结客服对话的LLMs引入了一组独特的挑战，因为关于对话中捕捉到的情感和业务问题的根本原因的分析性解释往往没有真实标签。

Method: 我们首先引入了一个3D范式（分解、解耦、分离）在人工标注指南和LLM-法官的提示中，以在语言学启发的评估标准中确立事实性标签。然后我们介绍了FECT，一个新颖的基准数据集，用于评估客服对话转录本中解释性AI生成声明的事实性。最后，我们报告了将LLM-法官与3D范式对齐的研究结果。

Result: 我们的研究结果贡献了一种新的方法，用于自动评估生成分析客服对话输出的AI系统的事实性。

Conclusion: 我们的研究贡献了一种新的方法，用于自动评估生成分析客服对话输出的AI系统的事实性。

Abstract: Large language models (LLMs) are known to hallucinate, producing natural
language outputs that are not grounded in the input, reference materials, or
real-world knowledge. In enterprise applications where AI features support
business decisions, such hallucinations can be particularly detrimental. LLMs
that analyze and summarize contact center conversations introduce a unique set
of challenges for factuality evaluation, because ground-truth labels often do
not exist for analytical interpretations about sentiments captured in the
conversation and root causes of the business problems. To remedy this, we first
introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in
the human annotation guideline and the LLM-judges' prompt to ground the
factuality labels in linguistically-informed evaluation criteria. We then
introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality
\textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact
Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm.
Lastly, we report our findings from aligning LLM-judges on the 3D paradigm.
Overall, our findings contribute a new approach for automatically evaluating
the factuality of outputs generated by an AI system for analyzing contact
center conversations.

</details>


### [3] [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)
*Ernesto L. Estevanell-Valladares,Suilan Estevez-Velarde,Yoan Gutiérrez,Andrés Montoyo,Ruslan Mitkov*

Main category: cs.CL

TL;DR: XAutoLM 是一种结合元学习的自动化机器学习框架，能够高效优化语言模型的微调流程，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化框架无法同时解决整个模型选择和超参数优化任务，以实现资源高效的语言模型微调。

Method: XAutoLM 是一种结合元学习的自动化机器学习框架，通过提取任务和系统级的元特征来优化判别性和生成性语言模型微调流程。

Result: 在四个文本分类和两个问答基准测试中，XAutoLM 在六个任务中的五个上超越了零样本优化器的峰值F1，平均评估时间减少了4.5倍，错误率降低了七倍，并发现了50%更多超过零样本帕累托前沿的管道。

Conclusion: XAutoLM 和其经验存储的发布将促进自然语言处理社区中的资源高效、绿色AI微调。

Abstract: Experts in machine learning leverage domain knowledge to navigate decisions
in model selection, hyperparameter optimisation, and resource allocation. This
is particularly critical for fine-tuning language models (LMs), where repeated
trials incur substantial computational overhead and environmental impact.
However, no existing automated framework simultaneously tackles the entire
model selection and HPO task for resource-efficient LM fine-tuning. We
introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past
experiences to optimise discriminative and generative LM fine-tuning pipelines
efficiently. XAutoLM learns from stored successes and failures by extracting
task- and system-level meta-features to bias its sampling toward fruitful
configurations and away from costly dead ends. On four text classification and
two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak
F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error
ratios by up to sevenfold, and uncovers up to 50% more pipelines above the
zero-shot Pareto front. In contrast, simpler memory-based baselines suffer
negative transfer. We release XAutoLM and our experience store to catalyse
resource-efficient, Green AI fine-tuning in the NLP community.

</details>


### [4] [MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.01005)
*Yiqun Chen,Erhan Zhang,Lingyong Yan,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Jiaxin Mao*

Main category: cs.CL

TL;DR: 本文提出了一种自适应RAG框架MAO-ARAG，通过多代理编排来动态规划工作流，以提高答案质量并保持成本和延迟在合理范围内。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界查询的复杂性不同，固定的RAG管道往往难以在不同查询之间平衡性能和成本效率。因此，我们需要一种自适应的RAG框架来解决这个问题。

Method: 我们提出了一个称为MAO-ARAG的自适应RAG框架，利用多代理编排。该框架被设计为多轮框架，定义了多个执行者代理，如查询重写代理、文档选择代理和生成代理。规划代理智能地选择并集成适当的代理到适合每个查询的流程中。在每一轮中，规划代理通过基于结果的奖励（F1分数）和基于成本的惩罚进行强化学习训练，以持续提高答案质量并保持成本在合理范围内。

Result: 实验结果表明，我们的方法不仅实现了高质量的答案，还保持了成本和延迟在可接受的范围内。

Conclusion: 实验表明，我们的方法在实现高质量答案的同时，还能将成本和延迟保持在可接受的范围内。

Abstract: In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has
become pivotal in enhancing response accuracy and reducing hallucination
issues. The architecture of RAG systems varies significantly, encompassing
single-round RAG, iterative RAG, and reasoning RAG, each tailored to address
different types of queries. Due to the varying complexity of real-world
queries, a fixed RAG pipeline often struggles to balance performance and cost
efficiency across different queries. To address this challenge, we propose an
adaptive RAG framework called MAO-ARAG, which leverages multi-agent
orchestration. Our adaptive RAG is conceived as a multi-turn framework.
Specifically, we define multiple executor agents, representing typical RAG
modules such as query reformulation agents, document selection agent, and
generation agents. A planner agent intelligently selects and integrates the
appropriate agents from these executors into a suitable workflow tailored for
each query, striving for high-quality answers while maintaining reasonable
costs. During each turn, the planner agent is trained using reinforcement
learning, guided by an outcome-based reward (F1 score) and a cost-based
penalty, continuously improving answer quality while keeping costs within a
reasonable range. Experiments conducted on multiple QA datasets demonstrate
that our approach, which dynamically plans workflows for each query, not only
achieves high answer quality but also maintains both cost and latency within
acceptable limits.The code of MAO-ARAG is on
https://github.com/chenyiqun/Agentic-RAG.

</details>


### [5] [UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu](https://arxiv.org/abs/2508.01006)
*Farah Adeeba,Brian Dillon,Hassan Sajjad,Rajesh Bhatt*

Main category: cs.CL

TL;DR: 研究提出了一个用于评估多语言大语言模型在低资源语言乌尔都语中句法知识的基准数据集UrBLiMP，并评估了多个模型的表现，发现尽管有最佳表现，但整体性能仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 为了评估大语言模型在乌尔都语中的语言知识，特别是针对低资源语言的细微句法知识。

Method: 我们提出了UrBLiMP数据集，该数据集包含5,696个最小对，针对十个核心句法现象，并通过人类评估验证了其可靠性。然后评估了二十个多语言大语言模型在UrBLiMP上的表现。

Result: LLaMA-3-70B在平均准确率上达到最高（94.73%），但与其他顶级模型如Gemma-3-27B-PT的表现统计上没有显著差异。

Conclusion: 这些发现突显了当前多语言大语言模型在捕捉低资源语言中细微句法知识方面的潜力和局限性。

Abstract: Multilingual Large Language Models (LLMs) have shown remarkable performance
across various languages; however, they often include significantly less data
for low-resource languages such as Urdu compared to high-resource languages
like English. To assess the linguistic knowledge of LLMs in Urdu, we present
the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of
minimally different sentences that contrast in grammatical acceptability.
UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,
carefully curated using the Urdu Treebank and diverse Urdu text corpora. A
human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator
agreement, confirming the reliability of the dataset. We evaluate twenty
multilingual LLMs on UrBLiMP, revealing significant variation in performance
across linguistic phenomena. While LLaMA-3-70B achieves the highest average
accuracy (94.73%), its performance is statistically comparable to other top
models such as Gemma-3-27B-PT. These findings highlight both the potential and
the limitations of current multilingual LLMs in capturing fine-grained
syntactic knowledge in low-resource languages.

</details>


### [6] [Cross-Domain Web Information Extraction at Pinterest](https://arxiv.org/abs/2508.01096)
*Michael Farag,Patrick Halina,Andrey Zaytsev,Alekhya Munagala,Imtihan Ahmed,Junhao Wang*

Main category: cs.CL

TL;DR: 本文介绍了一种用于属性提取的系统，该系统在成本和可扩展性方面表现出色，能够高效地从电子商务网站提取结构化数据。


<details>
  <summary>Details</summary>
Motivation: 在Pinterest上，从电子商务网站准确提取结构化产品数据对于增强用户体验和改进内容分发至关重要。

Method: 本文的方法利用了一种新颖的网页表示形式，结合了结构、视觉和文本模态，优化为小型模型学习。

Result: 实验结果表明，该系统具有高度可扩展性，每秒可以处理超过1000个URL，同时比最便宜的GPT替代方案成本低1000倍。

Conclusion: 本文提出了一种用于属性提取的系统，该系统在成本和可扩展性方面表现出色。

Abstract: The internet offers a massive repository of unstructured information, but
it's a significant challenge to convert this into a structured format. At
Pinterest, the ability to accurately extract structured product data from
e-commerce websites is essential to enhance user experiences and improve
content distribution. In this paper, we present Pinterest's system for
attribute extraction, which achieves remarkable accuracy and scalability at a
manageable cost. Our approach leverages a novel webpage representation that
combines structural, visual, and text modalities into a compact form,
optimizing it for small model learning. This representation captures each
visible HTML node with its text, style and layout information. We show how this
allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract
attributes more accurately than much more complex Large Language Models (LLMs)
such as Generative Pre-trained Transformer (GPT). Our results demonstrate a
system that is highly scalable, processing over 1,000 URLs per second, while
being 1000 times more cost-effective than the cheapest GPT alternatives.

</details>


### [7] [Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates](https://arxiv.org/abs/2508.01159)
*Liam G. McCoy,Fateme Nateghi Haredasht,Kanav Chopra,David Wu,David JH Wu,Abass Conteh,Sarita Khemani,Saloni Kumar Maharaj,Vishnu Ravi,Arth Pahwa,Yingjie Weng,Leah Rosengaus,Lena Giang,Kelvin Zhenghao Li,Olivia Jee,Daniel Shirvani,Ethan Goh,Jonathan H. Chen*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）生成结构化临床咨询模板的能力。通过多代理管道结合提示优化、语义自动评分和优先级分析，发现虽然某些模型在全面性方面表现优异，但它们在生成简洁且优先级正确的临床问题结构方面存在不足。


<details>
  <summary>Details</summary>
Motivation: To evaluate the capacity of large language models (LLMs) to generate structured clinical consultation templates for electronic consultation.

Method: Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we assess frontier models for their ability to produce clinically coherent, concise, and prioritized clinical question schemas.

Result: While models like o3 achieve high comprehensiveness (up to 92.2%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine.

Conclusion: LLMs can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician communication.

Abstract: This study evaluates the capacity of large language models (LLMs) to generate
structured clinical consultation templates for electronic consultation. Using
145 expert-crafted templates developed and routinely used by Stanford's
eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,
Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to
produce clinically coherent, concise, and prioritized clinical question
schemas. Through a multi-agent pipeline combining prompt optimization, semantic
autograding, and prioritization analysis, we show that while models like o3
achieve high comprehensiveness (up to 92.2\%), they consistently generate
excessively long templates and fail to correctly prioritize the most clinically
important questions under length constraints. Performance varies across
specialties, with significant degradation in narrative-driven fields such as
psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance
structured clinical information exchange between physicians, while highlighting
the need for more robust evaluation methods that capture a model's ability to
prioritize clinically salient information within the time constraints of
real-world physician communication.

</details>


### [8] [CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages](https://arxiv.org/abs/2508.01161)
*Jiyu Chen,Necva Bölücü,Sarvnaz Karimi,Diego Mollá,Cécile L. Paris*

Main category: cs.CL

TL;DR: 本研究探讨了在不同语言中进行情感识别的策略，并发现对每种语言单独使用LoRA设置微调预训练多语言LLM是最有效的方法。


<details>
  <summary>Details</summary>
Motivation: 检测不同语言中的情感具有挑战性，因为情感表达的方式各不相同且文化上有所不同。

Method: 我们调查了各种任务适应策略用于LLM在情感识别中的应用。

Result: 我们发现，对每种语言单独使用LoRA设置微调预训练多语言LLM是最有效的方法。

Conclusion: 我们的研究显示，对每种语言单独使用LoRA设置微调预训练多语言LLM是最有效的方法。

Abstract: Detecting emotions across different languages is challenging due to the
varied and culturally nuanced ways of emotional expressions. The
\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared
task was organised to investigate emotion recognition across different
languages. The goal of the task is to implement an emotion recogniser that can
identify the basic emotional states that general third-party observers would
attribute to an author based on their written text snippet, along with the
intensity of those emotions. We report our investigation of various
task-adaptation strategies for LLMs in emotion recognition. We show that the
most effective method for this task is to fine-tune a pre-trained multilingual
LLM with LoRA setting separately for each language.

</details>


### [9] [Adaptive Content Restriction for Large Language Models via Suffix Optimization](https://arxiv.org/abs/2508.01198)
*Yige Li,Peihai Jiang,Jun Sun,Peng Shu,Tianming Liu,Zhen Xiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为AdaCoRe的新任务，以及一种名为SOP的轻量级方法，用于防止LLM生成受限术语。实验表明，SOP在多个基准测试中表现优异，并在实际场景中具有可行性。


<details>
  <summary>Details</summary>
Motivation: 由于内容限制的需求在不同用户群体之间可能有很大差异，并且可能迅速变化，因此对每个特定用例进行监督微调是不切实际的。因此，需要一种无需模型微调的轻量级策略来实现内容限制。

Method: 本文提出了AdaCoRe任务，旨在通过轻量级策略（如SOP）来防止LLM生成受限术语。SOP通过在提示中添加一个短而优化的后缀，以防止目标LLM生成一组受限术语，同时保持输出质量。

Result: SOP在CoReBench基准测试中表现出色，平均限制率分别比系统后缀高出15%、17%、10%、9%和6%。此外，SOP在POE平台上也表现出有效性，证明了其在现实场景中的实用性。

Conclusion: 本文提出了AdaCoRe任务和SOP方法，用于在不进行模型微调的情况下防止LLM生成受限术语。实验结果表明，SOP在多个基准测试中表现优于系统级基线，并在实际场景中具有可行性。

Abstract: Large Language Models (LLMs) have demonstrated significant success across
diverse applications. However, enforcing content restrictions remains a
significant challenge due to their expansive output space. One aspect of
content restriction is preventing LLMs from generating harmful content via
model alignment approaches such as supervised fine-tuning (SFT). Yet, the need
for content restriction may vary significantly across user groups, change
rapidly over time, and not always align with general definitions of
harmfulness. Applying SFT to each of these specific use cases is impractical
due to the high computational, data, and storage demands. Motivated by this
need, we propose a new task called \textit{Adaptive Content Restriction}
(AdaCoRe), which focuses on lightweight strategies -- methods without model
fine-tuning -- to prevent deployed LLMs from generating restricted terms for
specific use cases. We propose the first method for AdaCoRe, named
\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to
any prompt to a) prevent a target LLM from generating a set of restricted
terms, while b) preserving the output quality. To evaluate AdaCoRe approaches,
including our SOP, we create a new \textit{Content Restriction Benchmark}
(CoReBench), which contains 400 prompts for 80 restricted terms across 8
carefully selected categories. We demonstrate the effectiveness of SOP on
CoReBench, which outperforms the system-level baselines such as system suffix
by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B,
Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also
demonstrate that SOP is effective on POE, an online platform hosting various
commercial LLMs, highlighting its practicality in real-world scenarios.

</details>


### [10] [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)
*Shengqi Zhu,Jeffrey M. Rzeszotarski,David Mimno*

Main category: cs.CL

TL;DR: 研究分析了LLM聊天日志中的用户行为模式，发现查询结构和用户行为随时间变化，并受模型能力影响。


<details>
  <summary>Details</summary>
Motivation: 聊天日志提供了关于LLM用户行为的丰富信息，但查询的多样性掩盖了行为模式。

Method: 通过分析聊天日志，将查询分为请求内容、角色、特定于查询的上下文和附加表达。

Result: 发现LLM查询中的请求制作与人类互动显著不同，查询模式随时间变化，用户在经验积累后趋于一致。

Conclusion: 模型能力影响用户行为，特别是在新模型引入时，这种影响在社区层面可追踪。

Abstract: Chat logs provide a rich source of information about LLM users, but patterns
of user behavior are often masked by the variability of queries. We present a
new task, segmenting chat queries into contents of requests, roles,
query-specific context, and additional expressions. We find that, despite the
familiarity of chat-based interaction, request-making in LLM queries remains
significantly different from comparable human-human interactions. With the data
resource, we introduce an important perspective of diachronic analyses with
user expressions. We find that query patterns vary between early ones
emphasizing requests, and individual users explore patterns but tend to
converge with experience. Finally, we show that model capabilities affect user
behavior, particularly with the introduction of new models, which are traceable
at the community level.

</details>


### [11] [WebDS: An End-to-End Benchmark for Web-based Data Science](https://arxiv.org/abs/2508.01222)
*Ethan Hsu,Hong Meng Yam,Ines Bouissou,Aaron Murali John,Raj Thota,Josh Koe,Vivek Sarath Putta,G K Dharesan,Alexander Spangher,Shikhar Murty,Tenghao Huang,Christopher D. Manning*

Main category: cs.CL

TL;DR: WebDS is a new benchmark for web-based data science that challenges agents to perform complex, multi-step operations requiring the use of tools and heterogeneous data formats.


<details>
  <summary>Details</summary>
Motivation: Existing web benchmarks focus on simplistic interactions, while traditional data science benchmarks concentrate on static datasets. There is a need for a benchmark that reflects the realities of modern data analytics.

Method: Introduce WebDS, an end-to-end web-based data science benchmark that comprises 870 tasks across 29 diverse websites.

Result: Evaluations of current SOTA LLM agents indicate significant performance gaps in accomplishing WebDS tasks. For instance, Browser Use successfully completes only 15% of tasks in WebDS.

Conclusion: WebDS provides a more robust and realistic testing ground for LLM-based data science, setting the stage for significant advances in the development of practically useful LLMs.

Abstract: A large portion of real-world data science tasks are complex and require
multi-hop web-based interactions: finding appropriate data available on the
internet, synthesizing real-time data of various modalities from different
locations, and producing summarized analyses. Existing web benchmarks often
focus on simplistic interactions, such as form submissions or e-commerce
transactions, and often do not require diverse tool-using capabilities required
for web based data science. Conversely, traditional data science benchmarks
typically concentrate on static, often textually bound datasets and do not
assess end-to-end workflows that encompass data acquisition, cleaning,
analysis, and insight generation. In response, we introduce WebDS, the first
end-to-end web-based data science benchmark. It comprises 870 web-based data
science tasks across 29 diverse websites from structured government data
portals to unstructured news media, challenging agents to perform complex,
multi-step operations requiring the use of tools and heterogeneous data formats
that better reflect the realities of modern data analytics. Evaluations of
current SOTA LLM agents indicate significant performance gaps in accomplishing
these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web
Voyager, successfully completes only 15% of tasks in WebDS, which our analysis
suggests is due to new failure modes like poor information grounding,
repetitive behavior and shortcut-taking that agents performing WebDS' tasks
display. By providing a more robust and realistic testing ground, WebDS sets
the stage for significant advances in the development of practically useful
LLM-based data science.

</details>


### [12] [WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework](https://arxiv.org/abs/2508.01245)
*Yue Chen,Minghua He,Fangkai Yang,Pu Zhao,Lu Wang,Yu Kang,Yifei Dong,Yuefeng Zhan,Hao Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为WarriorMath的缺陷感知框架，用于数学问题解决，通过多专家协作生成高质量数据并进行渐进式训练，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法专注于通过重述或难度进展来增强数据集，但忽略了LLM的具体失败模式。这导致了模型已经可以解决的合成问题，提供最小的性能提升。

Method: 我们提出了WarriorMath，这是一个缺陷感知的数学问题解决框架，结合了有针对性的数据合成和渐进式训练。在合成阶段，我们使用多个专家LLM进行协作过程来生成、批判和优化问题。在训练阶段，我们引入了一个渐进式学习框架，通过不断调整模型来使用越来越具有挑战性的数据。

Result: 在六个数学基准测试中，WarriorMath平均优于强基线12.57%，创造了新的最先进水平。

Conclusion: 我们的结果表明，缺陷感知的多专家框架在提高数学能力方面是有效的。

Abstract: Large Language Models (LLMs) excel in solving mathematical problems, yet
their performance is often limited by the availability of high-quality, diverse
training data. Existing methods focus on augmenting datasets through rephrasing
or difficulty progression but overlook the specific failure modes of LLMs. This
results in synthetic questions that the model can already solve, providing
minimal performance gains. To address this, we propose WarriorMath, a
defect-aware framework for mathematical problem solving that integrates both
targeted data synthesis and progressive training. In the synthesis stage, we
employ multiple expert LLMs in a collaborative process to generate, critique,
and refine problems. Questions that base LLMs fail to solve are identified and
iteratively improved through expert-level feedback, producing high-quality,
defect-aware training data. In the training stage, we introduce a progressive
learning framework that iteratively fine-tunes the model using increasingly
challenging data tailored to its weaknesses. Experiments on six mathematical
benchmarks show that WarriorMath outperforms strong baselines by 12.57% on
average, setting a new state-of-the-art. Our results demonstrate the
effectiveness of a defect-aware, multi-expert framework for improving
mathematical ability.

</details>


### [13] [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)
*Long S. T. Nguyen,Khang H. N. Vo,Thu H. A. Nguyen,Tuan C. Bui,Duc Q. Nguyen,Thanh-Tung Tran,Anh D. Nguyen,Minh L. Nguyen,Fabien Baldacci,Thang H. Bui,Emanuel Di Nardo,Angelo Ciaramella,Son H. Le,Ihsan Ullah,Lorenzo Di Rocco,Tho T. Quan*

Main category: cs.CL

TL;DR: 本文分析了XAI挑战2025，探讨了其在教育领域中促进可解释性AI的重要性，并提供了对未来XAI中心教育系统和研究竞赛的见解。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在教育中的日益整合，需要更高的透明度和可解释性。然而，很少有活动直接关注现实教育环境中的可解释AI。因此，本文旨在通过XAI挑战来解决这一问题。

Method: 本文分析了XAI挑战2025，包括其动机、结构、数据集构建和评估协议。

Result: 本文描述了XAI挑战的动机、结构、数据集构建和评估协议，并指出该挑战是将大型语言模型与符号推理结合以实现可解释性的新尝试。

Conclusion: 本文提出了XAI挑战2025的全面分析，展示了其在教育领域中促进可解释性AI的重要意义，并提供了对未来XAI中心教育系统和研究竞赛的见解。

Abstract: The growing integration of Artificial Intelligence (AI) into education has
intensified the need for transparency and interpretability. While hackathons
have long served as agile environments for rapid AI prototyping, few have
directly addressed eXplainable AI (XAI) in real-world educational contexts.
This paper presents a comprehensive analysis of the XAI Challenge 2025, a
hackathon-style competition jointly organized by Ho Chi Minh City University of
Technology (HCMUT) and the International Workshop on Trustworthiness and
Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International
Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked
participants with building Question-Answering (QA) systems capable of answering
student queries about university policies while generating clear, logic-based
natural language explanations. To promote transparency and trustworthiness,
solutions were required to use lightweight Large Language Models (LLMs) or
hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed
via logic-based templates with Z3 validation and refined through expert student
review to ensure alignment with real-world academic scenarios. We describe the
challenge's motivation, structure, dataset construction, and evaluation
protocol. Situating the competition within the broader evolution of AI
hackathons, we argue that it represents a novel effort to bridge LLMs and
symbolic reasoning in service of explainability. Our findings offer actionable
insights for future XAI-centered educational systems and competitive research
initiatives.

</details>


### [14] [Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities](https://arxiv.org/abs/2508.01290)
*Zhichao Yan,Jiapu Wang,Jiaoyan Chen,Yanyan Wang,Hongye Tan,Jiye Liang,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文研究了如何利用部分相关知识来唤醒大型语言模型，并提出了一种新的任务来模拟现实世界中的挑战。基于唤醒的方法在实际应用中表现更好。


<details>
  <summary>Details</summary>
Motivation: 有效利用部分相关知识仍然是RAG系统的一个关键挑战，特别是在不完整的知识库检索中。与传统观点相反，我们提出了一种新视角：LLM可以通过已经嵌入在LLM中的部分相关知识被唤醒。

Method: 我们使用位于黄金推理路径中的三元组及其变体来构建部分相关知识，通过移除包含答案的路径。我们提供了对LLM中唤醒效应的理论分析，并通过在两个知识图谱问答（KGQA）数据集上的实验支持我们的假设。

Result: 我们提出了一个新的任务，即未见实体KGQA，模拟由于KG不完整导致实体链接失败的实际挑战。我们的基于唤醒的方法在实际应用中表现出更高的有效性。

Conclusion: 我们的基于唤醒的方法在实际应用中表现出更高的有效性，优于依赖嵌入相似性的传统方法，这些方法容易返回噪声信息。

Abstract: Retrieval-Augmented Generation (RAG) shows impressive performance by
supplementing and substituting parametric knowledge in Large Language Models
(LLMs). Retrieved knowledge can be divided into three types: explicit answer
evidence, implicit answer clue, and insufficient answer context which can be
further categorized into totally irrelevant and partially relevant information.
Effectively utilizing partially relevant knowledge remains a key challenge for
RAG systems, especially in incomplete knowledge base retrieval. Contrary to the
conventional view, we propose a new perspective: LLMs can be awakened via
partially relevant knowledge already embedded in LLMs. To comprehensively
investigate this phenomenon, the triplets located in the gold reasoning path
and their variants are used to construct partially relevant knowledge by
removing the path that contains the answer. We provide theoretical analysis of
the awakening effect in LLMs and support our hypothesis with experiments on two
Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we
present a new task, Unseen Entity KGQA, simulating real-world challenges where
entity linking fails due to KG incompleteness. Our awakening-based approach
demonstrates greater efficacy in practical applications, outperforms
traditional methods that rely on embedding-based similarity which are prone to
returning noisy information.

</details>


### [15] [KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference](https://arxiv.org/abs/2508.01302)
*Chenming Tang,Yutong Yang,Yunfang Wu*

Main category: cs.CL

TL;DR: KEDAS is a novel knowledge editing approach that improves alignment in LLMs through diverse augmentation and self-adaptive inference, achieving superior performance in multiple datasets and settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a more effective method for knowledge editing in large language models that retains their capabilities while efficiently modifying outdated knowledge.

Method: KEDAS combines low-rank adaptation for in-context knowledge editing, diverse edit augmentation to improve recall, and a self-adaptive post-alignment inference mechanism with a filter-based smart retriever.

Result: KEDAS achieves the highest overall performance scores in 35 out of 36 cases across four datasets with three LLMs, surpassing its counterpart by about 19.8 harmonic mean scores and outperforming parameter editing and retrieval-based baselines significantly.

Conclusion: KEDAS presents an ideal paradigm of knowledge editing alignment, demonstrating robustness and efficiency in various tasks and datasets.

Abstract: Knowledge editing aims to modify outdated knowledge in large language models
(LLMs) efficiently while retaining their powerful capabilities. Most existing
methods rely on either parameter-level editing or retrieval-based approaches.
In this work, we propose Knowledge Editing alignment with Diverse Augmentation
and Self-adaptive inference (KEDAS) to better align LLMs with knowledge
editing. In the alignment phase, LLMs learn to apply in-context edited
knowledge via low-rank adaptation. During editing, we design a diverse edit
augmentation technique to improve the recall of edits. After that, a
self-adaptive post-alignment inference mechanism is proposed, in which a
filter-based smart retriever is employed to perform a dynamic selection of
inference routing. Specifically, irrelevant queries will go through the
original pre-alignment model directly, while relevant ones, together with their
related edits, go through the model with aligned adapters activated. In
experiments, KEDAS secures the highest overall performance scores in 35 out of
36 cases across four datasets with three LLMs on three settings, surpassing its
strong knowledge editing alignment counterpart by about 19.8 harmonic mean
scores of edit success, locality and portability and outperforming both
parameter editing and retrieval-based baselines significantly. Analysis of
computational cost and performance on general tasks further validates the
robustness and efficiency of KEDAS, indicating that it presents an ideal
paradigm of knowledge editing alignment.

</details>


### [16] [D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation](https://arxiv.org/abs/2508.01309)
*Weibo Zhou,Lingbo Li,Shangsong Liang*

Main category: cs.CL

TL;DR: D-SCoRE is a training-free pipeline that uses LLMs and prompt engineering to create high-quality QA datasets from any text. It improves the efficiency and performance of fine-tuning domain-specific LLMs.


<details>
  <summary>Details</summary>
Motivation: The scarcity and high cost of high-quality question-answering (QA) datasets hinder supervised fine-tuning (SFT) for domain-specific large language models (LLMs).

Method: D-SCoRE is a training-free pipeline that utilizes LLMs and prompt engineering to produce diverse, high-quality QA datasets from arbitrary textual sources. It integrates Document-centric processing, Segmentation, CoT Reasoning, and structured Export to generate QA-COT datasets tailored for domain-aware SFT.

Result: LLMs fine-tuned on D-SCoRE-generated QA datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most domains.

Conclusion: D-SCoRE generates six QA-CoT pairs with four-option counterfactual materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade hardware. Its simplicity and scalability enable efficient QA generation and high-performance fine-tuning across domains.

Abstract: The scarcity and high cost of high-quality question-answering (QA) datasets
hinder supervised fine-tuning (SFT) for domain-specific large language models
(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that
utilizes LLMs and prompt engineering to produce diverse, high-quality QA
datasets from arbitrary textual sources. D-SCoRE integrates
$\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T
$\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT
datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,
such as semantic role transformation, question type balancing, and
counterfactual materials, enhance diversity and relevance, overcoming
limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA
datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on
SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most
domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual
materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade
hardware. Its simplicity and scalability enable efficient QA generation and
high-performance fine-tuning across domains.

</details>


### [17] [LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points](https://arxiv.org/abs/2508.01317)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识点图的合成框架LinkSyn，用于生成多样化的多学科QA数据集LinkQA。LinkSyn通过平衡KP覆盖率和流行度，利用扩散合成和高难度QA增强，生成了包含50B标记的LinkQA数据集。实验表明，使用LinkQA进行持续预训练在Llama-3 8B上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的进步受到高质量、多样化训练数据稀缺的限制。为了克服这一限制，我们提出了LinkSyn，一种基于知识点（KP）图的合成框架，能够在控制学科和难度分布的同时平衡KP覆盖率和流行度。

Method: LinkSyn是一种基于知识点（KP）图的合成框架，它从问答（QA）种子数据中提取KP，并构建KP图以从多个由KP强关联的种子中合成多样化的QA数据。LinkSyn包括：(1) 知识分布值函数来指导路径采样概率的调整，平衡KP覆盖率和流行度；(2) 利用DeepSeek-R1进行基于扩散的合成，通过每条路径上的多个密集逻辑关联的种子；(3) 通过灵活调整难度来增强给定学科的高难度QA。

Result: 通过执行LinkSyn，我们生成了LinkQA，一个包含50B标记的多样化多学科QA数据集。在Llama-3 8B上的大量实验表明，使用LinkQA进行持续预训练，在MMLU和CMMLU上平均提升了11.51%，建立了新的SOTA结果。LinkQA在模型大小和初始FLOPs规模上都能持续提升性能。

Conclusion: LinkSyn能够生成多样化的多学科QA数据集LinkQA，通过持续预训练，LinkQA在Llama-3 8B上实现了MMLU和CMMLU的平均提升11.51%，并建立了新的SOTA结果。LinkQA在模型大小和初始FLOPs规模上都能持续提升性能。

Abstract: The advancement of large language models (LLMs) struggles with the scarcity
of high-quality, diverse training data. To address this limitation, we propose
LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that
enables flexible control over discipline and difficulty distributions while
balancing KP coverage and popularity. LinkSyn extracts KPs from
question-answering (QA) seed data and constructs a KP graph to synthesize
diverse QA data from multiple seeds strongly linked by KPs and sampled from
graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution
value function to guide the adjustment of path sampling probability and balance
KP coverage and popularity during graph walks; (2) diffusion-based synthesis
via DeepSeek-R1 by leveraging multiple seeds with dense logical associations
along each path; and (3) high-difficulty QA enhancement within given
disciplines by flexible difficulty adjustments. By executing LinkSyn, we
synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.
Extensive experiments on Llama-3 8B demonstrate that continual pre-training
with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and
CMMLU, establishing new SOTA results. LinkQA consistently enhances performance
across model size and initial FLOPs scales.

</details>


### [18] [Large-Scale Diverse Synthesis for Mid-Training](https://arxiv.org/abs/2508.01326)
*Xuemiao Zhang,Chengying Tu,Can Ren,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

TL;DR: This paper introduces BoostQA, a 100B-token large-scale QA dataset, synthesized through a novel diversified pipeline. The method improves model performance, especially in STEM disciplines and high-difficulty data, and achieves state-of-the-art results on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality, knowledge-intensive training data hinders the development of large language models (LLMs), as traditional corpora provide limited information. Previous studies have faced challenges in QA data scalability and knowledge diversity, particularly in cross-domain contexts.

Method: We propose a novel diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA dataset. Our synthesis framework curates seed data from heterogeneous sources, utilizes DeepSeek-R1 to implement STEM-focused multi-grade synthesis, and refines answers via DeepSeek-V3.

Result: Our method enables Llama-3 8B, mid-trained on a 40B-token dataset, to achieve an average improvement of 12.74% on MMLU and CMMLU and establish SOTA average performance across 12 benchmarks.

Conclusion: BoostQA demonstrates robust scalability, with performance consistently improving as model size, data volume, and initial FLOPs scale.

Abstract: The scarcity of high-quality, knowledge-intensive training data hinders the
development of large language models (LLMs), as traditional corpora provide
limited information. Previous studies have synthesized and integrated
corpora-dependent question-answering (QA) data to improve model performance but
face challenges in QA data scalability and knowledge diversity, particularly in
cross-domain contexts. Furthermore, leveraging our designed discipline and
difficulty annotation system, we probe model deficiencies in STEM disciplines
and high-difficulty data. To overcome these limitations, we propose a novel
diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA
dataset. Our synthesis framework: (1) curates seed data from heterogeneous
sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade
synthesis to boost data diversity and high-difficulty synthesis to mitigate
difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output
quality. We utilize BoostQA in mid-training, a mid-stage between pre-training
and post-training, to optimize domain-specific knowledge acquisition and
enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token
dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and
CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also
demonstrates robust scalability, with performance consistently improving as
model size, data volume, and initial FLOPs scale.

</details>


### [19] [MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis](https://arxiv.org/abs/2508.01370)
*Roman Koshkin,Pengyu Dai,Nozomi Fujikawa,Masahito Togami,Marco Visentini-Scarzanella*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型自动化业务分析和市场报告生成的框架，通过专门的代理协作完成任务，并通过自动审查周期优化报告质量。实验表明该框架能高效生成高质量的报告。


<details>
  <summary>Details</summary>
Motivation: 自动化端到端的业务分析和市场报告生成，以提高效率和降低成本。

Method: 我们提出了一种自主框架，利用大型语言模型（LLMs）来自动化端到端的业务分析和市场报告生成。系统使用专门的代理——研究员、评审员、作家和检索器——协作分析数据并生成全面的报告。这些代理通过上下文学习从真实的专业顾问的演示材料中学习，以复制专业的分析方法。框架执行多步骤过程：查询数据库、分析数据、生成见解、创建可视化和编写市场报告。我们还引入了一个基于LLM的评估系统来评估报告质量，并显示出与专家人类评估的一致性。基于这些评估，我们实现了迭代改进机制，通过自动审查周期优化报告质量。

Result: 实验结果表明，通过自动审查周期和顾问的非结构化知识可以提高报告质量。在实验验证中，我们的框架在7分钟内生成详细的6页报告，成本约为1美元。

Conclusion: 我们的工作可能是在自动创建可负担的市场洞察方面的重要一步。

Abstract: We present an autonomous framework that leverages Large Language Models
(LLMs) to automate end-to-end business analysis and market report generation.
At its core, the system employs specialized agents - Researcher, Reviewer,
Writer, and Retriever - that collaborate to analyze data and produce
comprehensive reports. These agents learn from real professional consultants'
presentation materials at Amazon through in-context learning to replicate
professional analytical methodologies. The framework executes a multi-step
process: querying databases, analyzing data, generating insights, creating
visualizations, and composing market reports. We also introduce a novel
LLM-based evaluation system for assessing report quality, which shows alignment
with expert human evaluations. Building on these evaluations, we implement an
iterative improvement mechanism that optimizes report quality through automated
review cycles. Experimental results show that report quality can be improved by
both automated review cycles and consultants' unstructured knowledge. In
experimental validation, our framework generates detailed 6-page reports in 7
minutes at a cost of approximately \$1. Our work could be an important step to
automatically create affordable market insights.

</details>


### [20] [MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs](https://arxiv.org/abs/2508.01401)
*Ahmad Rezaie Mianroodi,Amirali Rezaie,Niko Grisel Todorov,Cyril Rakovski,Frank Rudzicz*

Main category: cs.CL

TL;DR: MedSynth是一个合成医学对话和笔记的数据集，旨在提升医疗文档自动化工具的性能。


<details>
  <summary>Details</summary>
Motivation: 医生在记录临床会话上花费大量时间，这导致了职业倦怠。因此，需要强大的自动化工具来解决这个问题。

Method: MedSynth是一个合成医学对话和笔记的数据集，旨在推进Dialogue-to-Note和Note-to-Dial任务。

Result: MedSynth数据集显著提高了模型从对话生成医疗笔记以及从医疗笔记生成对话的性能。

Conclusion: MedSynth数据集为医学对话和笔记生成提供了有价值的资源，有助于改善医疗文档自动化工具。

Abstract: Physicians spend significant time documenting clinical encounters, a burden
that contributes to professional burnout. To address this, robust automation
tools for medical documentation are crucial. We introduce MedSynth -- a novel
dataset of synthetic medical dialogues and notes designed to advance the
Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.
Informed by an extensive analysis of disease distributions, this dataset
includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We
demonstrate that our dataset markedly enhances the performance of models in
generating medical notes from dialogues, and dialogues from medical notes. The
dataset provides a valuable resource in a field where open-access,
privacy-compliant, and diverse training data are scarce. Code is available at
https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available
at https://huggingface.co/datasets/Ahmad0067/MedSynth.

</details>


### [21] [ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations](https://arxiv.org/abs/2508.01411)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: ArzEn-MultiGenre 是一个包含埃及阿拉伯语和英语歌词、小说和电视字幕的高质量平行数据集，可用于机器翻译、语言模型微调以及教学和研究。


<details>
  <summary>Details</summary>
Motivation: 现有平行埃及阿拉伯语和英语数据集中缺乏多种文本类型，同时需要一个由专家翻译和对齐的黄金标准数据集。

Method: ArzEn-MultiGenre 是通过人工翻译和对齐埃及阿拉伯语和英语的歌词、小说和电视字幕创建的。

Result: ArzEn-MultiGenre 包含 25,557 个段落对，具有多种文本类型，并且由人类专家进行翻译和对齐。

Conclusion: ArzEn-MultiGenre 是一个高质量的平行数据集，可以用于机器翻译模型的基准测试、大型语言模型的微调以及商业机器翻译应用的适应。此外，它在多个研究领域和教学中都有重要价值。

Abstract: ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,
novels, and TV show subtitles that are manually translated and aligned with
their English counterparts. The dataset contains 25,557 segment pairs that can
be used to benchmark new machine translation models, fine-tune large language
models in few-shot settings, and adapt commercial machine translation
applications such as Google Translate. Additionally, the dataset is a valuable
resource for research in various disciplines, including translation studies,
cross-linguistic analysis, and lexical semantics. The dataset can also serve
pedagogical purposes by training translation students and aid professional
translators as a translation memory. The contributions are twofold: first, the
dataset features textual genres not found in existing parallel Egyptian Arabic
and English datasets, and second, it is a gold-standard dataset that has been
translated and aligned by human experts.

</details>


### [22] [Discovering Bias Associations through Open-Ended LLM Generations](https://arxiv.org/abs/2508.01412)
*Jinhao Pan,Chahat Raj,Ziwei Zhu*

Main category: cs.CL

TL;DR: 本文介绍了Bias Association Discovery Framework (BADF)，一种用于从开放式的LLM输出中提取人口统计身份和描述性概念之间已知和未知关联的系统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法依赖于预定义的身份-概念关联，限制了其发现新的或意外偏见的能力。

Method: BADF是一种系统的方法，用于从开放式的LLM输出中提取已知和之前未被识别的与人口统计身份相关的描述性概念。

Result: 通过在多个模型和多样化的现实情境中进行综合实验，BADF能够稳健地映射和分析表征人口统计身份的各种概念。

Conclusion: BADF能够帮助理解和分析LLM中的偏见关联，并提供了一个可扩展的工具。

Abstract: Social biases embedded in Large Language Models (LLMs) raise critical
concerns, resulting in representational harms -- unfair or distorted portrayals
of demographic groups -- that may be expressed in subtle ways through generated
language. Existing evaluation methods often depend on predefined
identity-concept associations, limiting their ability to surface new or
unexpected forms of bias. In this work, we present the Bias Association
Discovery Framework (BADF), a systematic approach for extracting both known and
previously unrecognized associations between demographic identities and
descriptive concepts from open-ended LLM outputs. Through comprehensive
experiments spanning multiple models and diverse real-world contexts, BADF
enables robust mapping and analysis of the varied concepts that characterize
demographic identities. Our findings advance the understanding of biases in
open-ended generation and provide a scalable tool for identifying and analyzing
bias associations in LLMs. Data, code, and results are available at
https://github.com/JP-25/Discover-Open-Ended-Generation

</details>


### [23] [From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs](https://arxiv.org/abs/2508.01424)
*Haonan Bian,Yutao Qi,Rui Yang,Yuanxi Che,Jiaqian Wang,Heming Xia,Ranran Zhen*

Main category: cs.CL

TL;DR: 本文提出了一种名为ORACLE的训练-free 框架，结合了大型语言模型的生成能力和知识图谱的结构优势，以解决多跳问答任务中的局限性。通过三个阶段的操作，该框架在多个基准测试中表现出色，生成的推理链更具逻辑性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尽管在问答任务中取得了成功，但在需要非线性、结构化推理的复杂多跳问答（MQA）任务中表现出局限性。这种限制源于它们无法充分捕捉实体之间的深层概念关系。

Method: 我们提出了一个名为ORACLE的训练-free 框架，该框架结合了LLMs的生成能力与知识图谱的结构优势。我们的方法通过三个阶段进行操作：(1) 使用LLMs动态构建特定于问题的知识本体；(2) 将这些本体转换为一阶逻辑推理链；(3) 系统地将原始查询分解为逻辑连贯的子问题。

Result: 实验结果表明，我们的框架在多个标准MQA基准测试中表现出色，性能与当前最先进的模型如DeepSeek-R1相当。详细分析进一步证实了每个组件的有效性，同时展示了我们的方法生成的推理链比现有方法更具逻辑性和可解释性。

Conclusion: 实验结果表明，我们的框架在多个标准MQA基准测试中表现出色，性能与当前最先进的模型如DeepSeek-R1相当。详细分析进一步证实了每个组件的有效性，同时展示了我们的方法生成的推理链比现有方法更具逻辑性和可解释性。

Abstract: Large Language Models (LLMs), despite their success in question answering,
exhibit limitations in complex multi-hop question answering (MQA) tasks that
necessitate non-linear, structured reasoning. This limitation stems from their
inability to adequately capture deep conceptual relationships between entities.
To overcome this challenge, we present **ORACLE** (**O**ntology-driven
**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a
training-free framework that combines LLMs' generative capabilities with the
structural benefits of knowledge graphs. Our approach operates through three
stages: (1) dynamic construction of question-specific knowledge ontologies
using LLMs, (2) transformation of these ontologies into First-Order Logic
reasoning chains, and (3) systematic decomposition of the original query into
logically coherent sub-questions. Experimental results on several standard MQA
benchmarks show that our framework achieves highly competitive performance,
rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses
further confirm the effectiveness of each component, while demonstrating that
our method generates more logical and interpretable reasoning chains than
existing approaches.

</details>


### [24] [Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data](https://arxiv.org/abs/2508.01450)
*Xinlin Zhuang,Feilong Tang,Haolin Yang,Ming Hu,Huifa Li,Haochen Xue,Yichen Li,Junjun He,Zongyuan Ge,Ying Qian,Imran Razzak*

Main category: cs.CL

TL;DR: This paper proposes a data selection strategy called Difficulty-Influence Quadrant (DIQ) to improve the efficiency and effectiveness of supervised fine-tuning for medical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, they overlook each sample's optimization utility reflected in its gradient.

Method: Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence.

Result: Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms the baseline.

Conclusion: DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms the baseline, highlighting the superiority of principled data selection over brute-force scaling.

Abstract: Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language
Models (LLMs) to specialized domains such as medical reasoning. However,
existing SFT practices often rely on unfiltered datasets that contain redundant
and low-quality samples, leading to substantial computational costs and
suboptimal performance. Although existing methods attempt to alleviate this
problem by selecting data based on sample difficulty, defined by knowledge and
reasoning complexity, they overlook each sample's optimization utility
reflected in its gradient. Interestingly, we find that gradient-based influence
alone favors easy-to-optimize samples that cause large parameter shifts but
lack deep reasoning chains, while difficulty alone selects noisy or overly
complex cases that fail to guide stable optimization. Based on this
observation, we propose a data selection strategy, Difficulty-Influence
Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence
quadrant to balance complex clinical reasoning with substantial gradient
influence, enabling efficient medical reasoning with minimal fine-tuning data.
Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected
subsets demonstrate higher data quality and generate clinical reasoning that is
more aligned with expert practices in differential diagnosis, safety check, and
evidence citation, as DIQ emphasizes samples that foster expert-like reasoning
patterns. Extensive experiments on medical reasoning benchmarks demonstrate
that DIQ enables models fine-tuned on only 1% of selected data to match
full-dataset performance, while using 10% consistently outperforms the
baseline, highlighting the superiority of principled data selection over
brute-force scaling. The code and data are available at
https://github.com/mihara-bot/DIQ.

</details>


### [25] [TreeDiff: AST-Guided Code Generation with Diffusion LLMs](https://arxiv.org/abs/2508.01473)
*Yiming Zeng,Jinghan Cao,Zexin Li,Yiming Chen,Tao Ren,Dawei Xiang,Xidong Wu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: 本文提出了一种语法感知的扩散框架，通过利用抽象语法树的结构先验来改进代码生成。实验结果表明，这种方法能够提高语法正确性和重建准确性。


<details>
  <summary>Details</summary>
Motivation: 将扩散模型应用于结构化领域（如源代码）仍然是一个重大挑战。编程语言与自然语言不同，它们遵循严格的语法和语义规则，具有必须保持正确的层次结构。训练期间使用的标准标记级破坏技术通常忽略这种结构，这可能会影响模型学习代码有意义表示的能力。

Method: 我们提出了一种语法感知的扩散框架，该框架将来自抽象语法树（AST）的结构先验结合到去噪过程中。而不是随机屏蔽单个标记，我们选择性地破坏从AST子树派生的语法上有意义的代码跨度。

Result: 实验结果表明，语法感知的破坏显著提高了语法正确性、重建准确性和对未见过的代码模式的泛化能力。

Conclusion: 这些发现表明，在扩散模型中引入结构信息具有潜力，并表明语法引导的去噪是推进代码生成任务中的扩散模型的一个有前景的方向。

Abstract: Recent advances in diffusion-based language models have opened new
possibilities for controllable and bidirectional sequence generation. These
models provide an alternative to traditional autoregressive approaches by
framing text generation as an iterative denoising process. However, applying
diffusion models to structured domains such as source code remains a
significant challenge. Programming languages differ from natural language in
that they follow strict syntactic and semantic rules, with hierarchical
organization that must be preserved for correctness. Standard token-level
corruption techniques used during training often ignore this structure, which
may hinder the model's ability to learn meaningful representations of code. To
address this limitation, we propose a syntax-aware diffusion framework that
incorporates structural priors from Abstract Syntax Trees (ASTs) into the
denoising process. Instead of masking individual tokens at random, we
selectively corrupt syntactically meaningful code spans derived from AST
subtrees. This enables the model to reconstruct programs in a way that respects
grammatical boundaries and captures long-range dependencies. Experimental
results demonstrate that syntax-aware corruption significantly improves
syntactic correctness, reconstruction accuracy, and generalization to unseen
code patterns. These findings highlight the potential of incorporating
structural information into diffusion-based training and suggest that
syntax-guided denoising is a promising direction for advancing diffusion-based
language models in code generation tasks.

</details>


### [26] [Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach](https://arxiv.org/abs/2508.01480)
*Dimitra Panou,Alexandros C. Dimopoulos,Manolis Koubarakis,Martin Reczko*

Main category: cs.CL

TL;DR: 本文介绍了我们在2025年BioASQ挑战赛中的参与情况，展示了我们如何利用开源大型语言模型来回答生物医学问题，并取得了显著的成绩。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本挖掘和问答任务在面对生物医学文献的指数级增长时是必不可少但要求很高的任务。

Method: 我们部署了一些开源大型语言模型（LLMs）作为检索增强生成器来回答生物医学问题。各种模型用于处理问题，多数投票系统结合它们的输出以确定最终答案，而对于列表和事实类型的问题，则使用它们的答案的并集。

Result: 我们在2025年的BioASQ挑战赛中取得了显著的成绩：在Synergy任务中，我们在第二轮中获得了理想答案的第一名和精确答案的第二名，并在第三轮和第四轮中两次获得精确答案的第一名。

Conclusion: 我们的研究为特定类型的生物医学问题提供了有价值的见解，即哪些大型语言模型的组合可以持续产生优越的结果。在2025年的BioASQ挑战赛的四轮比赛中，我们的系统取得了显著的成绩。

Abstract: Biomedical text mining and question-answering are essential yet highly
demanding tasks, particularly in the face of the exponential growth of
biomedical literature. In this work, we present our participation in the 13th
edition of the BioASQ challenge, which involves biomedical semantic
question-answering for Task 13b and biomedical question-answering for
developing topics for the Synergy task. We deploy a selection of open-source
large language models (LLMs) as retrieval-augmented generators to answer
biomedical questions. Various models are used to process the questions. A
majority voting system combines their output to determine the final answer for
Yes/No questions, while for list and factoid type questions, the union of their
answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring
all possible model combinations to contribute to the final answer, resulting in
tailored LLM pipelines for each question type. Our findings provide valuable
insight into which combinations of LLMs consistently produce superior results
for specific question types. In the four rounds of the 2025 BioASQ challenge,
our system achieved notable results: in the Synergy task, we secured 1st place
for ideal answers and 2nd place for exact answers in round 2, as well as two
shared 1st places for exact answers in round 3 and 4.

</details>


### [27] [TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu](https://arxiv.org/abs/2508.01486)
*Vallabhaneni Raj Kumar,Ashwin S,Supriya Manna,Niladri Sett,Cheedella V S N M S Hema Harshitha,Kurakula Harshitha,Anand Kumar Sharma,Basina Deepakraj,Tanuj Sarkar,Bondada Navaneeth Krishna,Samanthapudi Shakeer*

Main category: cs.CL

TL;DR: 本文介绍了TeSent，一个用于泰卢固语情感分类的基准数据集，该数据集不仅提供了真实标签，还提供了评估可解释性和公平性的措施。通过从各种社交媒体平台、新闻网站和网络博客中爬取文本，预处理并生成26,150个句子，并开发了一个定制的标注平台和精心设计的标注协议，以收集地面真实标签及其人工标注的推理。然后，我们以两种方式微调了几种最先进的预训练模型：带有推理和不带推理。此外，我们提供了一个详细的合理性与可信度评估套件，利用推理对六个广泛使用的后处理解释器在训练模型上的应用进行评估。最后，我们整理了TeEEC，一个公平性评估语料库，用于评估与泰卢固语情感和情绪相关的自然语言处理任务的公平性，并为训练好的分类器模型提供了公平性评估套件。我们的实验结果表明，使用推理进行训练可能会提高模型的准确性，减少模型中的偏差，并使解释器的输出更符合人类推理。


<details>
  <summary>Details</summary>
Motivation: 在印度次大陆，泰卢固语是印度六种古典语言之一，是使用最广泛的德拉维达语。尽管全球有9600万使用者，泰卢固语在全球自然语言处理和机器学习领域仍然代表性不足，主要是由于缺乏高质量的注释资源。因此，本文介绍了TeSent，这是一个全面的基准数据集，用于泰卢固语的情感分类，这是关键的文本分类问题。TeSent不仅为句子提供了真实标签，还提供了评估可解释性和公平性的措施，这两个是现代机器学习任务中的关键要求。

Method: 我们从各种社交媒体平台、新闻网站和网络博客中爬取了覆盖多个领域的泰卢固语文本，预处理并生成了26,150个句子，并开发了一个定制的标注平台和精心设计的标注协议，以收集地面真实标签及其人工标注的推理。然后，我们以两种方式微调了几种最先进的预训练模型：带有推理和不带推理。此外，我们提供了一个详细的合理性与可信度评估套件，利用推理对六个广泛使用的后处理解释器在训练模型上的应用进行评估。最后，我们整理了TeEEC，一个公平性评估语料库，用于评估与泰卢固语情感和情绪相关的自然语言处理任务的公平性，并为训练好的分类器模型提供了公平性评估套件。

Result: 我们的实验结果表明，使用推理进行训练可能会提高模型的准确性，减少模型中的偏差，并使解释器的输出更符合人类推理。

Conclusion: 我们的实验结果表明，使用推理进行训练可能会提高模型的准确性，减少模型中的偏差，并使解释器的输出更符合人类推理。

Abstract: In the Indian subcontinent, Telugu, one of India's six classical languages,
is the most widely spoken Dravidian Language. Despite its 96 million speaker
base worldwide, Telugu remains underrepresented in the global NLP and Machine
Learning landscape, mainly due to lack of high-quality annotated resources.
This work introduces TeSent, a comprehensive benchmark dataset for sentiment
classification, a key text classification problem, in Telugu. TeSent not only
provides ground truth labels for the sentences, but also supplements with
provisions for evaluating explainability and fairness, two critical
requirements in modern-day machine learning tasks. We scraped Telugu texts
covering multiple domains from various social media platforms, news websites
and web-blogs to preprocess and generate 26,150 sentences, and developed a
custom-built annotation platform and a carefully crafted annotation protocol
for collecting the ground truth labels along with their human-annotated
rationales. We then fine-tuned several SOTA pre-trained models in two ways:
with rationales, and without rationales. Further, we provide a detailed
plausibility and faithfulness evaluation suite, which exploits the rationales,
for six widely used post-hoc explainers applied on the trained models. Lastly,
we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate
fairness of Telugu sentiment and emotion related NLP tasks, and provide a
fairness evaluation suite for the trained classifier models. Our experimental
results suggest that training with rationales may improve model accuracy,
reduce bias in models, and make the explainers' output more aligned to human
reasoning.

</details>


### [28] [The Homogenizing Effect of Large Language Models on Human Expression and Thought](https://arxiv.org/abs/2508.01491)
*Zhivar Sourati,Alireza S. Ziabari,Morteza Dehghani*

Main category: cs.CL

TL;DR: 文章讨论了大型语言模型可能对认知多样性产生的负面影响，并呼吁关注这一问题。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探讨大型语言模型对认知多样性的潜在影响，并强调其可能带来的负面影响。

Method: 文章综合了语言学、认知科学和计算机科学的证据，分析了大型语言模型如何反映并强化主流风格，同时边缘化其他声音和推理策略。

Result: 文章发现，大型语言模型在设计和广泛应用中会模仿训练数据中的模式，并放大趋同，使得人们越来越多地依赖相同的模型。

Conclusion: 文章指出，如果不加以控制，这种同质化可能会削弱推动集体智能和适应性的认知景观。

Abstract: Cognitive diversity, reflected in variations of language, perspective, and
reasoning, is essential to creativity and collective intelligence. This
diversity is rich and grounded in culture, history, and individual experience.
Yet as large language models (LLMs) become deeply embedded in people's lives,
they risk standardizing language and reasoning. This Review synthesizes
evidence across linguistics, cognitive, and computer science to show how LLMs
reflect and reinforce dominant styles while marginalizing alternative voices
and reasoning strategies. We examine how their design and widespread use
contribute to this effect by mirroring patterns in their training data and
amplifying convergence as all people increasingly rely on the same models
across contexts. Unchecked, this homogenization risks flattening the cognitive
landscapes that drive collective intelligence and adaptability.

</details>


### [29] [A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents](https://arxiv.org/abs/2508.01503)
*Clayton Cohn,Surya Rayala,Namrata Srivastava,Joyce Horn Fonteles,Shruti Jain,Xinying Luo,Divya Mereddy,Naveeduddin Mohammed,Gautam Biswas*

Main category: cs.CL

TL;DR: 本文提出了一种结合证据中心设计和社会认知理论的框架，用于基于大型语言模型的教育代理，以提供适应性和原则性教学。


<details>
  <summary>Details</summary>
Motivation: 当前使用像ChatGPT这样的大型语言模型系统在课堂上往往缺乏早期智能辅导系统中发现的坚实的理论基础。因此，需要一种能够弥补这一差距的框架。

Method: 本文提出了一个结合证据中心设计和社会认知理论的框架，用于基于大型语言模型的代理在STEM+C学习中的自适应支架。

Result: 研究结果表明，Inquizzitor提供了与核心学习理论一致的高质量评估和互动，为教师提供了有效的指导，学生也认为其有价值。

Conclusion: 本研究强调了基于理论的大型语言模型在教育中的潜力，突出了这些系统提供适应性和原则性教学的能力。

Abstract: Large language models (LLMs) present new opportunities for creating
pedagogical agents that engage in meaningful dialogue to support student
learning. However, the current use of LLM systems like ChatGPT in classrooms
often lacks the solid theoretical foundation found in earlier intelligent
tutoring systems. To bridge this gap, we propose a framework that combines
Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding
in LLM-based agents focused on STEM+C learning. We illustrate this framework
with Inquizzitor, an LLM-based formative assessment agent that integrates
human-AI hybrid intelligence and provides feedback grounded in cognitive
science principles. Our findings show that Inquizzitor delivers high-quality
assessment and interaction aligned with core learning theories, offering
teachers effective guidance that students value. This research underscores the
potential for theory-driven LLM integration in education, highlighting the
ability of these systems to provide adaptive and principled instruction.

</details>


### [30] [MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization](https://arxiv.org/abs/2508.01541)
*Sara Câmara,Eduardo Luz,Valéria Carvalho,Ivan Meneghini,Gladston Moreira*

Main category: cs.CL

TL;DR: MOPrompt是一个多目标进化优化框架，用于同时优化提示的准确性和上下文大小，实验证明其在实际应用中具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有的自动提示优化方法通常只关注单一目标，如性能，而未能探索效率和效果之间的关键范围。

Method: MOPrompt是一种多目标进化优化框架，旨在同时优化准确性和上下文大小。

Result: MOPrompt在葡萄牙语情感分析任务中表现出色，与基线框架相比有显著提升。对于Sabiazinho模型，MOPrompt找到的提示在保持相同最高准确率的情况下，减少了31%的标记长度。

Conclusion: MOPrompt能够有效地优化提示，同时提高准确性和减少上下文大小，为实际应用提供了重要的工具。

Abstract: Prompt engineering is crucial for unlocking the potential of Large Language
Models (LLMs). Still, since manual prompt design is often complex,
non-intuitive, and time-consuming, automatic prompt optimization has emerged as
a research area. However, a significant challenge in prompt optimization is
managing the inherent trade-off between task performance, such as accuracy, and
context size. Most existing automated methods focus on a single objective,
typically performance, thereby failing to explore the critical spectrum of
efficiency and effectiveness. This paper introduces the MOPrompt, a novel
Multi-objective Evolutionary Optimization (EMO) framework designed to optimize
prompts for both accuracy and context size (measured in tokens) simultaneously.
Our framework maps the Pareto front of prompt solutions, presenting
practitioners with a set of trade-offs between context size and performance, a
crucial tool for deploying Large Language Models (LLMs) in real-world
applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,
using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that
MOPrompt substantially outperforms the baseline framework. For the Sabiazinho
model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)
as the best baseline solution, but with a 31% reduction in token length.

</details>


### [31] [Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models](https://arxiv.org/abs/2508.01554)
*Yujia Zheng,Tianhao Li,Haotian Huang,Tianyu Zeng,Jingyu Lu,Chuangxin Chu,Yuekai Huang,Ziyou Jiang,Qian Xiong,Yuyao Ge,Mingyang Li*

Main category: cs.CL

TL;DR: 本文提出了PromptAnatomy框架，用于分解提示并生成对抗示例，通过ComPerturb方法和PPL过滤机制提高攻击效果，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常将提示视为单一文本，忽视了其结构异质性——不同的提示组件对对抗鲁棒性的贡献不均。Prior works like PromptRobust假设提示是价值中立的，但我们的分析表明，具有丰富结构的复杂、领域特定提示的组件有不同的脆弱性。

Method: 我们引入了PromptAnatomy，这是一个自动框架，可以将提示分解为功能组件，并通过选择性地使用我们提出的方法ComPerturb扰动每个组件来生成多样且可解释的对抗示例。此外，我们还结合了一种基于困惑度（PPL）的过滤机制，以确保语言上的合理性并减轻分布偏移。

Result: 在这些数据集和五种先进的大型语言模型上的广泛实验表明，ComPerturb实现了最先进的攻击成功率。消融研究验证了提示分解和PPL过滤的互补优势。

Conclusion: 我们的结果强调了提示结构意识和受控扰动在大型语言模型的对抗鲁棒性评估中的重要性。

Abstract: Prompt-based adversarial attacks have become an effective means to assess the
robustness of large language models (LLMs). However, existing approaches often
treat prompts as monolithic text, overlooking their structural
heterogeneity-different prompt components contribute unequally to adversarial
robustness. Prior works like PromptRobust assume prompts are value-neutral, but
our analysis reveals that complex, domain-specific prompts with rich structures
have components with differing vulnerabilities. To address this gap, we
introduce PromptAnatomy, an automated framework that dissects prompts into
functional components and generates diverse, interpretable adversarial examples
by selectively perturbing each component using our proposed method, ComPerturb.
To ensure linguistic plausibility and mitigate distribution shifts, we further
incorporate a perplexity (PPL)-based filtering mechanism. As a complementary
resource, we annotate four public instruction-tuning datasets using the
PromptAnatomy framework, verified through human review. Extensive experiments
across these datasets and five advanced LLMs demonstrate that ComPerturb
achieves state-of-the-art attack success rates. Ablation studies validate the
complementary benefits of prompt dissection and PPL filtering. Our results
underscore the importance of prompt structure awareness and controlled
perturbation for reliable adversarial robustness evaluation in LLMs. Code and
data are available at https://github.com/Yujiaaaaa/PACP.

</details>


### [32] [OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets](https://arxiv.org/abs/2508.01630)
*Maziyar Panahi*

Main category: cs.CL

TL;DR: 本文介绍了一种名为OpenMed NER的开源领域适应Transformer模型，该模型通过结合轻量级领域自适应预训练和参数高效的低秩适应，在多个生物医学NER基准测试中取得了最先进的性能，并且具有出色的计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型取得了进展，但在多样化的实体类型上实现最先进的性能并保持计算效率仍然是一个重大挑战。

Method: 本文引入了OpenMed NER，这是一种结合轻量级领域自适应预训练（DAPT）和参数高效的低秩适应（LoRA）的开源领域适应Transformer模型。

Result: OpenMed NER在12个生物医学NER基准测试中取得了新的最先进微F1分数，在多种实体类型上都有显著提升。

Conclusion: 本文展示了经过战略性适应的开源模型可以超越封闭源代码解决方案，并且在计算效率方面表现出色，适合用于遵守新兴的数据保护和AI法规。

Abstract: Named-entity recognition (NER) is fundamental to extracting structured
information from the >80% of healthcare data that resides in unstructured
clinical notes and biomedical literature. Despite recent advances with large
language models, achieving state-of-the-art performance across diverse entity
types while maintaining computational efficiency remains a significant
challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted
transformer models that combine lightweight domain-adaptive pre-training (DAPT)
with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs
cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,
publicly available research repositories and de-identified clinical notes
(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA
backbones. This is followed by task-specific fine-tuning with LoRA, which
updates less than 1.5% of model parameters. We evaluate our models on 12
established biomedical NER benchmarks spanning chemicals, diseases, genes, and
species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of
these 12 datasets, with substantial gains across diverse entity types. Our
models advance the state-of-the-art on foundational disease and chemical
benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger
improvements of over 5.3 and 9.7 percentage points on more specialized gene and
clinical cell line corpora. This work demonstrates that strategically adapted
open-source models can surpass closed-source solutions. This performance is
achieved with remarkable efficiency: training completes in under 12 hours on a
single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively
licensed, open-source checkpoints designed to help practitioners facilitate
compliance with emerging data protection and AI regulations, such as the EU AI
Act.

</details>


### [33] [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)
*Lucio La Cava,Dominik Macko,Róbert Móro,Ivan Srba,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 本文介绍了多语言作者归属问题，研究了单语AA方法在多语言环境中的适用性和跨语言迁移能力，发现尽管某些方法可以适应多语言环境，但跨语言家族的迁移仍面临挑战，需要更稳健的方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的多样化，需要一种更细致但更具挑战性的作者归属方法，以识别文本背后的精确生成器（LLM或人类）。然而，目前的AA主要局限于单语设置，忽视了现代LLM的多语言性质和使用。

Method: 研究了单语AA方法在多语言环境中的适用性、跨语言迁移能力和生成器对归属性能的影响。

Result: 研究发现，某些单语AA方法可以适应多语言环境，但在跨不同语言家族的迁移中仍存在显著限制和挑战。

Conclusion: 研究结果表明，尽管某些单语AA方法可以适应多语言环境，但跨不同语言家族的转移仍然存在显著限制和挑战，这突显了多语言AA的复杂性，并需要更稳健的方法来更好地匹配现实场景。

Abstract: As Large Language Models (LLMs) have reached human-like fluency and
coherence, distinguishing machine-generated text (MGT) from human-written
content becomes increasingly difficult. While early efforts in MGT detection
have focused on binary classification, the growing landscape and diversity of
LLMs require a more fine-grained yet challenging authorship attribution (AA),
i.e., being able to identify the precise generator (LLM or human) behind a
text. However, AA remains nowadays confined to a monolingual setting, with
English being the most investigated one, overlooking the multilingual nature
and usage of modern LLMs. In this work, we introduce the problem of
Multilingual Authorship Attribution, which involves attributing texts to human
or multiple LLM generators across diverse languages. Focusing on 18 languages
-- covering multiple families and writing scripts -- and 8 generators (7 LLMs
and the human-authored class), we investigate the multilingual suitability of
monolingual AA methods, their cross-lingual transferability, and the impact of
generators on attribution performance. Our results reveal that while certain
monolingual AA methods can be adapted to multilingual settings, significant
limitations and challenges remain, particularly in transferring across diverse
language families, underscoring the complexity of multilingual AA and the need
for more robust approaches to better match real-world scenarios.

</details>


### [34] [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)
*Tae Soo Kim,Yoonjoo Lee,Yoonah Park,Jiho Kim,Young-Ho Kim,Juho Kim*

Main category: cs.CL

TL;DR: 本文介绍了CUPID基准测试，用于评估大型语言模型在多轮交互中推断用户偏好和生成符合偏好响应的能力。结果显示当前最先进的模型在这方面的表现不佳，需要进一步改进。


<details>
  <summary>Details</summary>
Motivation: 个人化的大型语言模型通常假设用户拥有静态偏好，这在现实中并不成立。人类的偏好是动态的，会根据上下文变化。因此，模型需要推断并应用未来的上下文以确保对齐。

Method: 我们引入了CUPID，这是一个由756个用户与基于大型语言模型的聊天助手之间的交互会话历史组成的基准测试。

Result: 我们评估了10个开源和专有大型语言模型，结果表明最先进的模型在从多轮交互中推断偏好以及辨别哪些先前上下文与新请求相关方面表现不佳，精度低于50%，召回率低于65%。

Conclusion: 我们的工作强调了需要提升大型语言模型在更情境化个性化交互方面的能力，并提出了CUPID作为一个资源来推动这些改进。

Abstract: Personalization of Large Language Models (LLMs) often assumes users hold
static preferences that reflect globally in all tasks. In reality, humans hold
dynamic preferences that change depending on the context. As users interact
with an LLM in various contexts, they naturally reveal their contextual
preferences, which a model must infer and apply in future contexts to ensure
alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated
interaction session histories between users and LLM-based chat assistants. In
each interaction session, the user provides a request in a specific context and
expresses their preference through multi-turn feedback. Given a new user
request and prior interaction sessions, our benchmark assesses whether LLMs can
infer the preference relevant to this request and generate a response that
satisfies this preference. With CUPID, we evaluated 10 open and proprietary
LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from
multi-turn interactions and fail to discern what previous context is relevant
to a new request -- under 50% precision and 65% recall. Our work highlights the
need to advance LLM capabilities for more contextually personalized
interactions and proposes CUPID as a resource to drive these improvements.

</details>


### [35] [The Bidirectional Process Reward Model](https://arxiv.org/abs/2508.01682)
*Lingyin Zhang,Jun Gao,Xiaoxue Ren,Ziqiang Cao*

Main category: cs.CL

TL;DR: 本文提出了一种新的双向过程奖励模型（BiPRM），通过引入右到左评估流来提升大型语言模型的推理质量。实验结果表明，BiPRM在多个基准上表现优异，具有广泛的应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有的PRMs主要采用单向左到右（L2R）评估范式，这限制了它们利用全局上下文的能力，使得难以根据后续步骤验证早期步骤的一致性。

Method: 提出了一种新的双向评估范式，称为双向过程奖励模型（BiPRM），通过在传统的左到右（L2R）流程中并行引入右到左（R2L）评估流，使后续推理步骤能够实时评估早期步骤。

Result: 在两个数学推理基准上进行了广泛的实验，使用由三种不同策略模型生成的样本进行测试。BiPRM在所有设置中均优于单向基线，步进奖励评估的改进高达31.9%。

Conclusion: BiPRM展现出有效性、鲁棒性和通用性，为基于过程的奖励建模提供了一个有前景的新方向。

Abstract: Process Reward Models (PRMs) have emerged as a promising approach to enhance
the reasoning quality of Large Language Models (LLMs) by assigning fine-grained
scores to intermediate reasoning steps within a solution trajectory. However,
existing PRMs predominantly adopt a unidirectional left-to-right (L2R)
evaluation paradigm, which limits their ability to leverage global context,
making it challenging to verify the consistency of earlier steps based on later
ones. In light of these challenges, we propose a novel bidirectional evaluation
paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly
incorporates a parallel right-to-left (R2L) evaluation stream alongside the
conventional L2R flow, enabling later reasoning steps to help assess earlier
ones in real time. Notably, the built-in R2L evaluation is implemented solely
through prompt modifications that reverse the original reasoning trajectory,
without any additional parameters or inference latency introduced. This ensures
BiPRM remains both efficient and broadly compatible with existing PRM studies.
We conduct extensive experiments on two mathematical reasoning benchmarks using
samples generated by three different policy models. Our method, BiPRM, is
evaluated across three backbones and three distinct PRM objectives. Across all
settings, BiPRM consistently outperforms unidirectional baselines, achieving up
to a 31.9% improvement in stepwise reward evaluation. Generally, our results
highlight BiPRM's effectiveness, robustness, and general applicability,
offering a promising new direction for process-based reward modeling.

</details>


### [36] [Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy](https://arxiv.org/abs/2508.01696)
*Yi Jiang,Sendong Zhao,Jianbo Li,Haochun Wang,Lizhe Zhang,Yan Liu,Bin Qin*

Main category: cs.CL

TL;DR: 本文提出了一种名为Collaborative Chain-of-Agents的框架，旨在增强参数知识和检索知识之间的协同作用。通过引入CoCoA-zero和CoCoA策略，实验结果表明该方法在开放领域和多跳问答任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在生成过程中无法充分利用知识，模型的内部参数知识和外部检索知识之间的协同作用有限。

Method: 我们提出了Collaborative Chain-of-Agents框架，首先引入了CoCoA-zero，这是一种多代理RAG框架，先进行条件知识归纳，然后推理答案。在此基础上，我们开发了CoCoA，这是一种长链训练策略，从CoCoA-zero中合成扩展的多代理推理轨迹来微调LLM。

Result: CoCoA-zero和CoCoA在开放领域和多跳问答任务中表现出色。

Conclusion: 实验结果表明，CoCoA-zero和CoCoA在开放领域和多跳问答任务中表现优越。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework for
enhancing the capabilities of Large Language Models (LLMs), especially in
knowledge-intensive tasks. Despite its advantages, current RAG methods often
struggle to *fully exploit knowledge during generation*. In particular, the
synergy between the model's internal parametric knowledge and external
retrieved knowledge remains limited. Retrieved contents may sometimes mislead
generation, while certain generated content can guide the model toward more
accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a
framework designed to enhance explicitly synergy over both parametric and
retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent
RAG framework that first performs conditional knowledge induction and then
reasons answers. Building on this, we develop CoCoA, a long-chain training
strategy that synthesizes extended multi-agent reasoning trajectories from
CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability
to explicitly integrate and jointly leverage parametric and retrieved
knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior
performance on open-domain and multi-hop QA tasks.

</details>


### [37] [Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption](https://arxiv.org/abs/2508.01708)
*Berkay Köprü,Mehrzad Mashal,Yigit Gurses,Akos Kadar,Maximilian Schmitt,Ditty Mathew,Felix Burkhardt,Florian Eyben,Björn W. Schuller*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的表达泄漏现象，发现随着模型规模的增加，表达泄漏会减少，但需要在模型构建过程中特别注意，不能通过提示来缓解。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要关注语义泄漏，而本文引入了表达泄漏，即LLM系统地生成与输入上下文语义无关的情感表达。

Method: 我们收集了一个基准数据集，并提出了一种自动评估流程，该流程与人类判断有良好的相关性，加速了基准测试。

Result: 实验表明，随着模型在参数空间中的扩展，同一LLM家族中的表达泄漏会减少。然而，表达泄漏的缓解需要在模型构建过程中特别注意，不能通过提示来缓解。此外，当负面情绪被注入提示时，会导致更高的表达泄漏率。

Conclusion: 表达泄漏在模型规模增大时会减少，但需要在模型构建过程中特别注意，不能通过提示来缓解。此外，当负面情绪被注入提示时，会对生成过程造成更大的干扰，导致更高的表达泄漏率。

Abstract: Large language models (LLMs) have advanced natural language processing (NLP)
skills such as through next-token prediction and self-attention, but their
ability to integrate broad context also makes them prone to incorporating
irrelevant information. Prior work has focused on semantic leakage, bias
introduced by semantically irrelevant context. In this paper, we introduce
expression leakage, a novel phenomenon where LLMs systematically generate
sentimentally charged expressions that are semantically unrelated to the input
context. To analyse the expression leakage, we collect a benchmark dataset
along with a scheme to automatically generate a dataset from free-form text
from common-crawl. In addition, we propose an automatic evaluation pipeline
that correlates well with human judgment, which accelerates the benchmarking by
decoupling from the need of annotation for each analysed model. Our experiments
show that, as the model scales in the parameter space, the expression leakage
reduces within the same LLM family. On the other hand, we demonstrate that
expression leakage mitigation requires specific care during the model building
process, and cannot be mitigated by prompting. In addition, our experiments
indicate that, when negative sentiment is injected in the prompt, it disrupts
the generation process more than the positive sentiment, causing a higher
expression leakage rate.

</details>


### [38] [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)
*Raviraj Joshi,Rakesh Paul,Kanishk Singla,Anusha Kamath,Michael Evans,Katherine Luna,Shaona Ghosh,Utkarsh Vaidya,Eileen Long,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 本文介绍了CultureGuard，一种用于跨多种语言策划文化一致的高质量安全数据集的新解决方案。通过四阶段的合成数据生成和过滤流程，将英文安全数据集转换为八种语言，并创建了一个多语言安全数据集，以训练多语言安全防护模型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在代理应用中的日益广泛应用，需要稳健的安全防护模型。然而，非英语语言缺乏类似的进展，这是由于收集文化一致的标记数据集的成本很高。

Method: 本文提出了CultureGuard，一种用于跨多种语言策划文化一致的高质量安全数据集的新解决方案。其方法包括四个阶段的合成数据生成和过滤流程：文化数据隔离、文化数据适应、机器翻译和质量过滤。

Result: 本文将Nemotron-Content-Safety-Dataset-V2英文安全数据集转换和扩展为八种不同的语言，并创建了Nemotron-Content-Safety-Dataset-Multilingual-v1数据集，包含9种语言的386,661个样本。最终模型在多个多语言内容安全基准上实现了最先进的性能。

Conclusion: 本文代表了在弥合多语言大语言模型的安全差距方面的重要进展，通过使能文化意识的安全防护模型的开发。

Abstract: The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,
comprises 386,661 samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.
The final model achieves state-of-the-art performance on several multilingual
content safety benchmarks. We also benchmark the latest open LLMs on
multilingual safety and observe that these LLMs are more prone to give unsafe
responses when prompted in non-English languages. This work represents a
significant step toward closing the safety gap in multilingual LLMs by enabling
the development of culturally aware safety guard models.

</details>


### [39] [Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction](https://arxiv.org/abs/2508.01739)
*Cheng Wang,ziru Liu,Pengcheng Tang,Mingyu Zhang,Quanyu Dai,Yue Zhu*

Main category: cs.CL

TL;DR: 本文提出IterChat框架，通过分解多轮偏好提取为一轮提取过程，提高数据质量和标注效率，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前研究显示，使用大型语言模型（LLMs）微调特定任务的偏好提取器在准确性和泛化性方面表现优异。然而，主要挑战在于获取高质量的多轮对话数据的困难。准确跟踪用户偏好转换不仅需要大量的领域专业知识和上下文一致性维护，还由于序列依赖学习中的误差传播而使模型训练复杂化。

Method: 本文提出了IterChat框架，首先构建了新的数据格式，将对话数据分为属性历史偏好和一轮对话，然后使用GPT4预定义目标偏好提取任务中的偏好槽位，并随机采样槽位及其对应模式值来创建对话数据集。

Result: 实验结果表明，使用新对话格式进行微调或仅少量提示比原始多轮对话表现更好。此外，新数据格式提高了标注员效率，胜率比原始多轮对话高28.4%。

Conclusion: 本文提出了一种新的对话数据生成框架IterChat，通过将多轮偏好提取分解为迭代执行的一轮提取过程，有效解决了标注错误和效率问题，并在实验中表现出色。

Abstract: Identifying user preferences in dialogue systems is a pivotal aspect of
providing satisfying services. Current research shows that using large language
models (LLMs) to fine-tune a task-specific preference extractor yields
excellent results in terms of accuracy and generalization. However, the primary
challenge stems from the inherent difficulty in obtaining high-quality labeled
multi-turn dialogue data. Accurately tracking user preference transitions
across turns not only demands intensive domain expertise and contextual
consistency maintenance for annotators (termed \textbf{``Annotating
Disaster''}) but also complicates model training due to error propagation in
sequential dependency learning. Inspired by the observation that multi-turn
preference extraction can be decomposed into iterative executions of one-turn
extraction processes. We propose a novel dialogue data generation framework
named \textbf{IterChat}. First, we construct a new data format that categorizes
the dialogue data into attributed historical preferences and one-turn
dialogues. This reduces the probability of annotation errors and improves
annotation efficiency. Then, to generate a high-quality and diverse dialogue
dataset, we adopt GPT4 to pre-define the preference slots in the target
preference extractor task and then randomly sample the subset of the slots and
their corresponding schema values to create the dialogue datasets. Experimental
results indicate that fine-tuning or only few-shot prompting with the new
dialogue format yields superior performance compared to the original multi-turn
dialogues. Additionally, the new data format improves annotator efficiency with
a win rate of 28.4\% higher than the original multi-turn dialogues.

</details>


### [40] [AI-Generated Text is Non-Stationary: Detection via Temporal Tomography](https://arxiv.org/abs/2508.01754)
*Alva West,Yixuan Weng,Minjun Zhu,Luodan Zhang,Zhen Lin,Guangsheng Bao,Yue Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的AI生成文本检测方法TDT，它通过保留位置信息来提高检测效果。TDT将token级差异视为时间序列信号，并应用连续小波变换生成二维时间尺度表示。实验结果显示，TDT在RAID基准测试中表现优异，并在对抗性任务上表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的方法有一个根本性的局限：它们将token级测量聚合为标量分数，丢弃了关于异常发生位置的定位信息。我们的实证分析表明，AI生成的文本表现出显著的非平稳性，统计特性在文本段落之间的变化比人类写作多73.8%。这一发现解释了为什么现有的检测器在面对局部对抗性扰动时失败，这些扰动利用了这一被忽视的特性。

Method: 我们引入了时间差异断层扫描（TDT），这是一种新的检测范式，通过将检测重新表述为信号处理任务来保留位置信息。TDT将token级差异视为时间序列信号，并应用连续小波变换生成二维时间尺度表示，捕捉统计异常的位置和语言尺度。

Result: 在RAID基准测试中，TDT实现了0.855 AUROC（比最佳基线提高7.1%）。更重要的是，TDT在对抗性任务上表现出稳健的性能，在HART Level 2改写攻击中AUROC提高了14.1%。尽管其复杂的分析，TDT仅增加了13%的计算开销。

Conclusion: 我们的工作确立了非平稳性作为AI生成文本的基本特征，并证明了保留时间动态对于稳健检测至关重要。

Abstract: The field of AI-generated text detection has evolved from supervised
classification to zero-shot statistical analysis. However, current approaches
share a fundamental limitation: they aggregate token-level measurements into
scalar scores, discarding positional information about where anomalies occur.
Our empirical analysis reveals that AI-generated text exhibits significant
non-stationarity, statistical properties vary by 73.8\% more between text
segments compared to human writing. This discovery explains why existing
detectors fail against localized adversarial perturbations that exploit this
overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),
a novel detection paradigm that preserves positional information by
reformulating detection as a signal processing task. TDT treats token-level
discrepancies as a time-series signal and applies Continuous Wavelet Transform
to generate a two-dimensional time-scale representation, capturing both the
location and linguistic scale of statistical anomalies. On the RAID benchmark,
TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More
importantly, TDT demonstrates robust performance on adversarial tasks, with
14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its
sophisticated analysis, TDT maintains practical efficiency with only 13\%
computational overhead. Our work establishes non-stationarity as a fundamental
characteristic of AI-generated text and demonstrates that preserving temporal
dynamics is essential for robust detection.

</details>


### [41] [A comprehensive taxonomy of hallucinations in Large Language Models](https://arxiv.org/abs/2508.01781)
*Manuel Cossio*

Main category: cs.CL

TL;DR: 本文全面分析了大型语言模型（LLMs）的幻觉问题，包括其定义、分类、表现形式、成因、检测方法和缓解策略，并强调了未来需要关注检测、缓解和人类监督以实现可靠部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理领域取得了革命性的进展，但它们产生幻觉（生成看似合理但事实错误或虚构内容）的倾向仍然是一个关键挑战。本文旨在提供对LLM幻觉的全面理解，以促进更可靠和负责任的部署。

Method: 本文提供了LLM幻觉的全面分类法，从形式定义和理论框架开始，探讨了内在和外在幻觉的区别，以及事实性和忠实性的概念。随后详细描述了具体的表象，分析了潜在原因，包括数据相关问题、模型相关因素和提示相关影响。此外，还研究了影响幻觉感知的认知和人类因素，调查了评估基准和指标，并概述了架构和系统缓解策略。最后，介绍了用于监控LLM发布和性能的网络资源。

Result: 本文提供了LLM幻觉的全面分类法，详细探讨了其表现形式、成因以及检测和缓解策略，并提出了网络资源以监控LLM的表现。

Conclusion: 本文强调了LLM幻觉的复杂性和多面性，并指出由于其理论上的不可避免性，未来的工作必须集中在稳健的检测、缓解和持续的人类监督上，以实现关键应用中的负责任和可靠的部署。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their propensity for hallucination, generating plausible but factually
incorrect or fabricated content, remains a critical challenge. This report
provides a comprehensive taxonomy of LLM hallucinations, beginning with a
formal definition and a theoretical framework that posits its inherent
inevitability in computable LLMs, irrespective of architecture or training. It
explores core distinctions, differentiating between intrinsic (contradicting
input context) and extrinsic (inconsistent with training data or reality), as
well as factuality (absolute correctness) and faithfulness (adherence to
input). The report then details specific manifestations, including factual
errors, contextual and logical inconsistencies, temporal disorientation,
ethical violations, and task-specific hallucinations across domains like code
generation and multimodal applications. It analyzes the underlying causes,
categorizing them into data-related issues, model-related factors, and
prompt-related influences. Furthermore, the report examines cognitive and human
factors influencing hallucination perception, surveys evaluation benchmarks and
metrics for detection, and outlines architectural and systemic mitigation
strategies. Finally, it introduces web-based resources for monitoring LLM
releases and performance. This report underscores the complex, multifaceted
nature of LLM hallucinations and emphasizes that, given their theoretical
inevitability, future efforts must focus on robust detection, mitigation, and
continuous human oversight for responsible and reliable deployment in critical
applications.

</details>


### [42] [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://arxiv.org/abs/2508.01812)
*Amir DN Cohen,Hilla Merhav,Yoav Goldberg,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 本文介绍了HeQ数据集，用于希伯来语机器阅读理解，并提出了一套新的指南和评估指标，以应对形态丰富的语言带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的希伯来语自然语言处理基准主要集中在形态句法任务上，忽略了语言理解的语义维度。为了弥补这一差距，我们旨在提供一个希伯来语机器阅读理解（MRC）数据集。

Method: 我们设计了一套新的指南、受控众包协议和修订的评估指标，以适应语言的形态丰富性。

Result: 我们的实证研究发现，标准评估指标如F1分数和精确匹配（EM）不适合希伯来语（以及其他MRLs），并提出了相关的改进。此外，模型在形态句法任务和MRC上的表现之间存在低相关性。

Conclusion: HeQ的开发和探索展示了MRLs在自然语言理解中的一些挑战，推动了对希伯来语和其他MRLs更强大和更好的NLU模型的发展。

Abstract: Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly
on morpho-syntactic tasks, neglecting the semantic dimension of language
understanding. To bridge this gap, we set out to deliver a Hebrew Machine
Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive
Question Answering. The morphologically rich nature of Hebrew poses a challenge
to this endeavor: the indeterminacy and non-transparency of span boundaries in
morphologically complex forms lead to annotation inconsistencies,
disagreements, and flaws in standard evaluation metrics.
  To remedy this, we devise a novel set of guidelines, a controlled
crowdsourcing protocol, and revised evaluation metrics that are suitable for
the morphologically rich nature of the language. Our resulting benchmark, HeQ
(Hebrew QA), features 30,147 diverse question-answer pairs derived from both
Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation
reveals that standard evaluation metrics such as F1 scores and Exact Match (EM)
are not appropriate for Hebrew (and other MRLs), and we propose a relevant
enhancement.
  In addition, our experiments show low correlation between models' performance
on morpho-syntactic tasks and on MRC, which suggests that models designed for
the former might underperform on semantics-heavy tasks. The development and
exploration of HeQ illustrate some of the challenges MRLs pose in natural
language understanding (NLU), fostering progression towards more and better NLU
models for Hebrew and other MRLs.

</details>


### [43] [AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy](https://arxiv.org/abs/2508.01815)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Tan Chuan Fu,Yue Xiu,Dusit Niyato,Jonathan Z. Low,Eugene Ho Hong Zhuang,Daren Zong Loong Tan*

Main category: cs.CL

TL;DR: AgenticT$^2$S is a framework for KGQA that improves accuracy and reduces prompt length by using agents for retrieval, query generation, and verification, with applications in sustainability domains.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-SPARQL approaches are limited in their generalizability in low-resource domains and their ability to handle queries spanning multiple graphs, which is particularly relevant in domains like the circular economy where information is distributed across independently curated knowledge graphs.

Method: AgenticT$^2$S is a modular framework that decomposes KGQA into subtasks managed by specialized agents responsible for retrieval, query generation, and verification. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies, and a two-stage verifier detects structurally invalid and semantically underspecified queries through symbolic validation and counterfactual consistency checks.

Result: Experiments on real-world circular economy KGs show that AgenticT$^2$S improves execution accuracy by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing the average prompt length by 46.4%.

Conclusion: AgenticT$^2$S demonstrates the benefits of agent-based schema-aware reasoning for scalable KGQA and supports decision-making in sustainability domains through robust cross-graph reasoning.

Abstract: Question answering over heterogeneous knowledge graphs (KGQA) involves
reasoning across diverse schemas, incomplete alignments, and distributed data
sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific
fine-tuning or operate within single-graph settings, limiting their
generalizability in low-resource domains and their ability to handle queries
spanning multiple graphs. These challenges are particularly relevant in domains
such as the circular economy, where information about classifications,
processes, and emissions is distributed across independently curated knowledge
graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes
KGQA into subtasks managed by specialized agents responsible for retrieval,
query generation, and verification. A scheduler assigns subgoals to different
graphs using weak-to-strong alignment strategies. A two-stage verifier detects
structurally invalid and semantically underspecified queries through symbolic
validation and counterfactual consistency checks. Experiments on real-world
circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy
by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing
the average prompt length by 46.4%. These results demonstrate the benefits of
agent-based schema-aware reasoning for scalable KGQA and support
decision-making in sustainability domains through robust cross-graph reasoning.

</details>


### [44] [MLP Memory: Language Modeling with Retriever-pretrained External Memory](https://arxiv.org/abs/2508.01832)
*Rubin Wei,Jiaqi Cao,Jiarui Wang,Jushi Kai,Qipeng Guo,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的架构，结合了Transformer解码器和预训练的MLP外部内存，以解决LLM中的幻觉问题，并在多个任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代解码器-only LLM在生成文本中出现幻觉问题，这阻碍了它们在知识密集型任务中的应用。RAG提供了一个解决方案，但检索器的非参数性质限制了它与LLM的深度交互。

Method: 我们提出了一种方法，通过使用预训练的可微外部内存来解耦LLM解码器的记忆化。该外部内存是一个MLP，通过模仿整个预训练数据集上的检索器行为进行预训练。

Result: 实验表明，我们的架构在模型大小上表现出更陡峭的幂律扩展，在WikiText-103和Web数据集上分别比解码器-only模型提高了17.5%和24.1%，并且在不过拟合的情况下受益于额外的训练。

Conclusion: 我们的架构在三个幻觉基准和九个记忆密集型任务上表现出色，同时在推理任务中表现优于kNN-LM。

Abstract: While modern decoder-only LLMs achieve superior performance across various
domains, hallucinations have risen to be a common problem in their generated
text, hindering their application in knowledge-intensive tasks.
Retriever-augmented generation (RAG) offers a solution, but the non-parametric
nature of the retriever hinders its deep interaction with LLM. In this work, we
propose to decouple memorization from the LLM decoder using a pretrained,
differentiable external memory. The external memory is an MLP pretrained by
imitating the behavior of a retriever on the entire pretraining dataset. Our
resulting architecture, which comprises a transformer decoder and an external
MLP memory pretrained on language modeling and retriever imitation
respectively, demonstrates strong perplexity and performance on downstream
tasks. Experiments show our architecture exhibits steeper power-law scaling
with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web
datasets compared to decoder-only models while benefiting from added training
without overfitting. We demonstrate superior performance on three hallucination
benchmarks and nine memory-intensive tasks. Additionally, our approach delivers
$80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference
than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP
memory improves StrategyQA performance. We will open-source our code and models
in the future.

</details>


### [45] [Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents](https://arxiv.org/abs/2508.01858)
*Yuhan Guo,Cong Guo,Aiwen Sun,Hongliang He,Xinyu Yang,Yue Lu,Yingji Zhang,Xuntao Guo,Dong Zhang,Jianzhuang Liu,Jiang Duan,Yijia Xiao,Liangjian Wen,Hai-Ming Xu,Yong Dai*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架和方法，用于增强网络代理的知识获取和推理能力，并展示了其在未见过的任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的网络代理在认知推理方面存在不足，需要先获取足够的知识才能有效进行认知推理。

Method: 本文提出了Web-CogKnowledge框架，将知识分为事实性、概念性和程序性，并构建了Web-CogDataset数据集来系统地灌输核心知识。此外，还开发了基于知识的Chain-of-Thought（CoT）推理框架，并训练了Web-CogReasoner代理。

Result: 实验表明，Web-CogReasoner代理在未见过的任务中表现出色，尤其是在结构化知识至关重要的情况下。同时，引入了Web-CogBench评估套件以进行全面评估。

Conclusion: 本文提出了一种新的框架和方法，用于增强网络代理的知识获取和推理能力，并展示了其在未见过的任务中的优越性能。

Abstract: Multimodal large-scale models have significantly advanced the development of
web agents, enabling perception and interaction with digital environments akin
to human cognition. In this paper, we argue that web agents must first acquire
sufficient knowledge to effectively engage in cognitive reasoning. Therefore,
we decompose a web agent's capabilities into two essential stages: knowledge
content learning and cognitive processes. To formalize this, we propose
Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and
Procedural. In this framework, knowledge content learning corresponds to the
agent's processes of Memorizing and Understanding, which rely on the first two
knowledge types, representing the "what" of learning. Conversely, cognitive
processes correspond to Exploring, grounded in Procedural knowledge, defining
the "how" of reasoning and action. To facilitate knowledge acquisition, we
construct the Web-CogDataset, a structured resource curated from 14 real-world
websites, designed to systematically instill core knowledge necessary for web
agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon
which comprehension is built-as well as the basis for learning how to reason
and act. Building on this foundation, we operationalize these processes through
a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing
and training our proposed agent, the Web-CogReasoner. Extensive experimentation
reveals its significant superiority over existing models, especially in
generalizing to unseen tasks where structured knowledge is decisive. To enable
rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation
suite designed to assess and compare agent performance across the delineated
knowledge domains and cognitive capabilities. Our code and data is open sourced
at https://github.com/Gnonymous/Web-CogReasoner

</details>


### [46] [Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2508.01862)
*Yijun Feng*

Main category: cs.CL

TL;DR: 本文提出了一种名为反事实探测的新方法，用于检测和减轻大型语言模型输出中的幻觉。该方法通过生成包含细微事实错误的反事实陈述来评估模型的敏感性，并证明其在检测性能方面优于基线方法，同时能有效降低幻觉分数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种任务中表现出色，但经常生成流畅但事实错误或没有支持的输出。我们需要一种有效的方法来检测和减轻这些幻觉。

Method: 我们提出了反事实探测，这是一种新的方法，用于检测和减轻LLM输出中的幻觉。我们的方法动态生成看似合理但包含细微事实错误的反事实陈述，然后评估模型对这些扰动的敏感性。

Result: 我们的全面评估表明，反事实探测在检测性能方面优于基线方法，而我们的自适应缓解策略平均将幻觉分数降低了24.5%。

Conclusion: 我们的方法无需模型重新训练，可以作为实时验证机制集成到现有的LLM流程中。

Abstract: Large Language Models have demonstrated remarkable capabilities across
diverse tasks, yet they frequently generate hallucinations outputs that are
fluent but factually incorrect or unsupported. We propose Counterfactual
Probing, a novel approach for detecting and mitigating hallucinations in LLM
outputs. Our method dynamically generates counterfactual statements that appear
plausible but contain subtle factual errors, then evaluates the model's
sensitivity to these perturbations. We hypothesize that genuine knowledge
exhibits robustness to counterfactual variations, while hallucinated content
shows inconsistent confidence patterns when confronted with plausible
alternatives. Our comprehensive evaluation on TruthfulQA, factual statement
datasets, and curated hallucination examples demonstrates that counterfactual
probing achieves superior detection performance compared to baseline methods,
while our adaptive mitigation strategies reduce hallucination scores by an
average of 24.5%. The approach requires no model retraining and can be
integrated into existing LLM pipelines as a realtime verification mechanism.

</details>


### [47] [Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language](https://arxiv.org/abs/2508.01918)
*Jaskaranjeet Singh,Rakesh Thakur*

Main category: cs.CL

TL;DR: 本研究提出了针对旁遮普语的大型语言模型套件，并引入了量子感知检索系统，以提升低资源语言的NLP性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言在NLP领域中仍然被排除在外，需要开发专门针对这些语言的模型和方法。

Method: 本研究提出了PunGPT2，一个完全开源的旁遮普语大型语言模型套件，以及Pun-RAG和Pun-Instruct框架，并引入了Quantum-RAG，一种融合稀疏和密集方法的新型混合检索系统。

Result: 我们的模型在困惑度、事实性和流利度方面显著优于多语言基线（如mBERT、mT5、MuRIL）。

Conclusion: 本研究为扩展低资源语言的LLM能力提供了可扩展和可重复的蓝图，并在低资源NLP中开创了量子感知检索。

Abstract: Despite the rapid advancement of large language models (LLMs), low-resource
languages remain largely excluded from the NLP landscape. We present PunGPT2,
the first fully open-source suite of Punjabi large language models, trained
from scratch on a 35GB domain-diverse corpus encompassing literature, religious
texts, news, and social discourse. Unlike prior multilingual approaches,
PunGPT2 captures rich syntactic and morphological features unique to Punjabi
through a tokenizer optimised with byte pair encoding and linguistically
aligned pretraining objectives. To improve factual grounding and domain recall,
we introduce Pun-RAG, a retrieval-augmented generation framework combining
PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We
further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant
using QLoRA, enabling robust zero-shot and instruction-following performance
with significantly reduced compute needs.
  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system
that fuses sparse (BM25) and dense methods with quantum-inspired semantic
matching. By encoding queries using amplitude-based embeddings and retrieving
via quantum kernel similarity, Quantum-RAG achieves improved contextual
relevance with minimal memory overhead marking the first practical integration
of quantum representations in low-resource language generation. Our models
significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in
perplexity, factuality, and fluency. This work provides a scalable,
reproducible blueprint for extending LLM capabilities to underrepresented
languages and pioneers quantum-aware retrieval in low-resource NLP

</details>


### [48] [Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback](https://arxiv.org/abs/2508.01930)
*Tom S. Juzek,Zina B. Ward*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）过度使用某些词汇的现象，并通过实验验证了这种现象可能与从人类反馈中学习（LHF）有关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）以过度使用某些术语（如“delve”和“intricate”）而闻名，但这些词汇选择的确切原因尚不清楚。

Method: 我们提出了一种简单的方法来检测LLM的词汇偏好，这些偏好可能是由LHF引起的。然后，我们通过实验模拟LHF过程并证明参与者系统地更喜欢包含某些单词的文本变体。

Result: 我们发现LHF可能导致词汇过度使用，这可以被视为一种不对齐现象，但我们的研究突显了不同群体（即LHF工作者和LLM用户）在词汇期望上的潜在差异。

Conclusion: 我们的工作为可解释的人工智能研究做出了贡献，并强调了在对齐研究中数据和过程透明性的重要性。

Abstract: Large Language Models (LLMs) are known to overuse certain terms like "delve"
and "intricate." The exact reasons for these lexical choices, however, have
been unclear. Using Meta's Llama model, this study investigates the
contribution of Learning from Human Feedback (LHF), under which we subsume
Reinforcement Learning from Human Feedback and Direct Preference Optimization.
We present a straightforward procedure for detecting the lexical preferences of
LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to
lexical overuse by experimentally emulating the LHF procedure and demonstrating
that participants systematically prefer text variants that include certain
words. This lexical overuse can be seen as a sort of misalignment, though our
study highlights the potential divergence between the lexical expectations of
different populations -- namely LHF workers versus LLM users. Our work
contributes to the growing body of research on explainable artificial
intelligence and emphasizes the importance of both data and procedural
transparency in alignment research.

</details>


### [49] [ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks](https://arxiv.org/abs/2508.01943)
*Philip Schroeder,Ondrej Biza,Thomas Weng,Hongyin Luo,James Glass*

Main category: cs.CL

TL;DR: ROVER is a framework that improves video reasoning for vision-language models by recursively decomposing long video sequences into manageable subtasks, leading to better performance and reduced hallucinations.


<details>
  <summary>Details</summary>
Motivation: Vision-language models (VLMs) struggle with reasoning over extended sequences of camera frames from a video, limiting their utility in embodied settings that require reasoning over long frame sequences from a continuous stream of visual input.

Method: ROVER is a framework that recursively decomposes long-horizon video trajectories into segments corresponding to shorter subtasks, enabling more focused and accurate reasoning over temporally localized frame sequences.

Result: ROVER outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. It also reduces hallucinations and scales linearly with video length.

Conclusion: ROVER demonstrates superior performance in video reasoning tasks and mitigates hallucinations by focusing on relevant frame sequences while maintaining global context.

Abstract: Vision-language models (VLMs) have exhibited impressive capabilities across
diverse image understanding tasks, but still struggle in settings that require
reasoning over extended sequences of camera frames from a video. This limits
their utility in embodied settings, which require reasoning over long frame
sequences from a continuous stream of visual input at each moment of a task
attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo
Recursively), a framework that enables the model to recursively decompose
long-horizon video trajectories into segments corresponding to shorter subtasks
within the trajectory. In doing so, ROVER facilitates more focused and accurate
reasoning over temporally localized frame sequences without losing global
context. We evaluate ROVER, implemented using an in-context learning approach,
on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa
that consists of 543 videos showing both expert and perturbed non-expert
trajectories across 27 robotic manipulation tasks. ROVER outperforms strong
baselines across three video reasoning tasks: task progress estimation,
frame-level natural language reasoning, and video question answering. We
observe that, by reducing the number of frames the model reasons over at each
timestep, ROVER mitigates hallucinations, especially during unexpected or
non-optimal moments of a trajectory. In addition, by enabling the
implementation of a subtask-specific sliding context window, ROVER's time
complexity scales linearly with video length, an asymptotic improvement over
baselines. Demos, code, and data available at: https://rover-vlm.github.io

</details>


### [50] [SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension](https://arxiv.org/abs/2508.01959)
*Junjie Wu,Jiangnan Li,Yuqing Li,Lemao Liu,Liyan Xu,Jiwei Li,Dit-Yan Yeung,Jie Zhou,Mo Yu*

Main category: cs.CL

TL;DR: 本文提出了一种新的情境嵌入模型（SitEmb），通过考虑上下文来增强检索性能，并在多个任务中取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长文档的检索增强生成（RAG）中面临挑战，因为长块会增加嵌入模型的负担，并且许多实际应用仍需要返回局部证据。

Method: 我们提出了一种新的训练范式，并开发了情境嵌入模型（SitEmb）。通过将短块表示为依赖于更广泛上下文窗口的方式，以增强检索性能。

Result: 我们在一个专门设计用于评估情境检索能力的书籍情节检索数据集上评估了我们的方法。SitEmb-v1模型表现优异，SitEmb-v1.5模型进一步提升了性能。

Conclusion: 我们的SitEmb-v1模型基于BGE-M3，在书情节检索基准上显著优于最先进的嵌入模型，包括一些具有7-8B参数的模型，仅使用1B参数。我们的8B SitEmb-v1.5模型进一步提高了性能，并在不同语言和多个下游应用中表现出色。

Abstract: Retrieval-augmented generation (RAG) over long documents typically involves
splitting the text into smaller chunks, which serve as the basic units for
retrieval. However, due to dependencies across the original document,
contextual information is often essential for accurately interpreting each
chunk. To address this, prior work has explored encoding longer context windows
to produce embeddings for longer chunks. Despite these efforts, gains in
retrieval and downstream tasks remain limited. This is because (1) longer
chunks strain the capacity of embedding models due to the increased amount of
information they must encode, and (2) many real-world applications still
require returning localized evidence due to constraints on model or human
bandwidth.
  We propose an alternative approach to this challenge by representing short
chunks in a way that is conditioned on a broader context window to enhance
retrieval performance -- i.e., situating a chunk's meaning within its context.
We further show that existing embedding models are not well-equipped to encode
such situated context effectively, and thus introduce a new training paradigm
and develop the situated embedding models (SitEmb). To evaluate our method, we
curate a book-plot retrieval dataset specifically designed to assess situated
retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3
substantially outperforms state-of-the-art embedding models, including several
with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model
further improves performance by over 10% and shows strong results across
different languages and several downstream applications.

</details>


### [51] [TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2508.01977)
*Fan Gao,Cheng Huang,Nyima Tashi,Yutong Liu,Xiangxiang Wang,Thupten Tsering,Ban Ma-bao,Renzeg Duojie,Gadeng Luosang,Rinchen Dongrub,Dorje Tashi,Xiao Feng,Hao Wang,Yongbin Yu*

Main category: cs.CL

TL;DR: 本文介绍了TIBSTC-CoT，一个通过大型语言模型的思维链提示自动构建的藏语大规模多领域数据集，并基于此开发了Sunshine-thinking LLM家族，展示了强大的推理和生成性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决藏语严重的数据稀缺问题，这是一种由超过六百万人使用的低资源语言。

Method: 我们引入了TIBSTC-CoT，这是一个通过大型语言模型（LLMs）的思维链提示自动构建的大规模、多领域藏语数据集。基于这个数据集，我们开发了Sunshine-thinking LLM家族，这是一系列具有思维链能力的藏语中心LLM。

Result: Sunshine-thinking在推理和生成性能方面表现出色，与最先进的多语言LLM相当。

Conclusion: 我们的工作标志着在包容性人工智能方面的重要进展，通过资源创建和模型创新实现了高质量的藏语处理。

Abstract: To address the severe data scarcity in Tibetan, a low-resource language
spoken by over six million people, we introduce TIBSTC-CoT, the large-scale,
multi-domain Tibetan dataset automatically constructed via chain-of-thought
prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable
and reproducible framework for dataset creation in low-resource settings,
covering diverse domains and reasoning patterns essential for language
understanding and generation. Building on this dataset, we develop the
Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with
chain-of-thought capabilities. Trained entirely on TIBSTC-CoT,
Sunshine-thinking has demonstrated strong reasoning and generation performance,
comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a
significant step toward inclusive AI by enabling high-quality Tibetan language
processing through both resource creation and model innovation. All data are
available: https://github.com/Vicentvankor/sun-shine.

</details>


### [52] [Contextually Aware E-Commerce Product Question Answering using RAG](https://arxiv.org/abs/2508.01990)
*Praveen Tangarajan,Anand A. Rajasekar,Manish Rathi,Vinay Rao Dandin,Ozan Ersoy*

Main category: cs.CL

TL;DR: 本文提出了一种基于RAG的电子商务产品问答框架，能够处理多种查询类型，并引入了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的产品问答系统往往无法有效地利用丰富的用户上下文和多样化的产品信息，导致用户难以快速准确地找到所需信息。

Method: 本文提出了一个基于RAG的框架，该框架深度整合了上下文理解，并利用对话历史、用户资料和产品属性来提供相关且个性化的答案。

Result: 该系统能够处理客观、主观和多意图查询，并能识别目录中的信息缺口，以支持持续的内容改进。此外，还引入了新的评估指标，适用于RAG系统的评估。

Conclusion: 本文提出了一种基于检索增强生成（RAG）的可扩展端到端框架，用于电子商务产品问答（PQA），并引入了新的评估指标以衡量框架性能。

Abstract: E-commerce product pages contain a mix of structured specifications,
unstructured reviews, and contextual elements like personalized offers or
regional variants. Although informative, this volume can lead to cognitive
overload, making it difficult for users to quickly and accurately find the
information they need. Existing Product Question Answering (PQA) systems often
fail to utilize rich user context and diverse product information effectively.
We propose a scalable, end-to-end framework for e-commerce PQA using Retrieval
Augmented Generation (RAG) that deeply integrates contextual understanding. Our
system leverages conversational history, user profiles, and product attributes
to deliver relevant and personalized answers. It adeptly handles objective,
subjective, and multi-intent queries across heterogeneous sources, while also
identifying information gaps in the catalog to support ongoing content
improvement. We also introduce novel metrics to measure the framework's
performance which are broadly applicable for RAG system evaluations.

</details>


### [53] [Prompting Large Language Models to Detect Dementia Family Caregivers](https://arxiv.org/abs/2508.01999)
*Md Badsha Biswas,Özlem Uzuner*

Main category: cs.CL

TL;DR: 本文介绍了一个用于检测涉及家庭成员患有痴呆症的推文的系统，该系统使用大语言模型并取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体如Twitter为痴呆症患者的照顾者提供了分享经验和寻求支持的机会，但需要首先识别出这些照顾者的推文。

Method: 我们探索了使用各种提示方法的大语言模型（LLMs），并发现在一个微调模型上的简单零样本提示效果最好。

Result: 我们的最终系统在验证集和测试集上达到了0.95的宏F1分数，表明我们的方法在检测涉及家庭成员患有痴呆症的推文方面非常有效。

Conclusion: 我们的最终系统在验证集和测试集上达到了0.95的宏F1分数，表明我们的方法在检测涉及家庭成员患有痴呆症的推文方面非常有效。

Abstract: Social media, such as Twitter, provides opportunities for caregivers of
dementia patients to share their experiences and seek support for a variety of
reasons. Availability of this information online also paves the way for the
development of internet-based interventions in their support. However, for this
purpose, tweets written by caregivers of dementia patients must first be
identified. This paper demonstrates our system for the SMM4H 2025 shared task
3, which focuses on detecting tweets posted by individuals who have a family
member with dementia. The task is outlined as a binary classification problem,
differentiating between tweets that mention dementia in the context of a family
member and those that do not. Our solution to this problem explores large
language models (LLMs) with various prompting methods. Our results show that a
simple zero-shot prompt on a fine-tuned model yielded the best results. Our
final system achieved a macro F1-score of 0.95 on the validation set and the
test set. Our full code is available on GitHub.

</details>


### [54] [SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents](https://arxiv.org/abs/2508.02013)
*Changhao Jiang,Jiajun Sun,Yifei Cao,Jiabao Zhuang,Hui Li,Xiaoran Fan,Ming Zhang,Junjie Ye,Shihan Dou,Zhiheng Xi,Jingqi Tong,Yilong Wu,Baoyu Fan,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文构建了语音角色扮演数据集SpeechRole-Data和评估基准SpeechRole-Eval，以推动语音驱动的多模态角色扮演研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在文本模态上，忽视了现实交互场景中的语音这一关键维度，缺乏对语音角色扮演代理的系统评估。

Method: 构建了一个大规模、高质量的语音角色扮演数据集SpeechRole-Data，并提出了一种多维评估基准SpeechRole-Eval，用于系统评估语音角色扮演代理在关键方面的性能。

Result: 实验结果揭示了级联和端到端语音角色扮演代理在保持声音风格一致性和角色连贯性方面的优势和挑战。

Conclusion: 本文提出了SpeechRole-Data和SpeechRole-Eval，为语音角色扮演代理的研究提供了数据和评估基准，并释放了所有数据、代码和基线模型以促进该领域的发展。

Abstract: Recently, role-playing agents have emerged as a promising paradigm for
achieving personalized interaction and emotional resonance. Existing research
primarily focuses on the textual modality, neglecting the critical dimension of
speech in realistic interactive scenarios. In particular, there is a lack of
systematic evaluation for Speech Role-Playing Agents (SRPAs). To address this
gap, we construct SpeechRole-Data, a large-scale, high-quality dataset that
comprises 98 diverse roles and 112k speech-based single-turn and multi-turn
conversations. Each role demonstrates distinct vocal characteristics, including
timbre and prosody, thereby enabling more sophisticated speech role-playing.
Furthermore, we propose SpeechRole-Eval, a multidimensional evaluation
benchmark that systematically assesses SRPAs performance in key aspects such as
fundamental interaction ability, speech expressiveness, and role-playing
fidelity. Experimental results reveal the advantages and challenges of both
cascaded and end-to-end speech role-playing agents in maintaining vocal style
consistency and role coherence. We release all data, code, and baseline models
to provide a solid foundation for speech-driven multimodal role-playing
research and to foster further developments in this field.

</details>


### [55] [SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2508.02018)
*Wanqi Yang,Yanda Li,Yunchao Wei,Meng Fang,Ling Chen*

Main category: cs.CL

TL;DR: This paper introduces SpeechR, a benchmark for evaluating reasoning in large audio-language models. It assesses models along three dimensions and reveals that high transcription accuracy does not imply strong reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of large audio-language models focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speech-based scenarios insufficiently examined.

Method: We introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. It evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats: multiple-choice, generative, and acoustic-feature versions.

Result: Evaluations on eleven state-of-the-art LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities.

Conclusion: SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks.

Abstract: Large audio-language models (LALMs) have achieved near-human performance in
sentence-level transcription and emotion recognition. However, existing
evaluations focus mainly on surface-level perception, leaving the capacity of
models for contextual and inference-driven reasoning in speech-based scenarios
insufficiently examined. To address this gap, we introduce SpeechR, a unified
benchmark for evaluating reasoning over speech in large audio-language models.
SpeechR evaluates models along three key dimensions: factual retrieval,
procedural inference, and normative judgment. It includes three distinct
evaluation formats. The multiple-choice version measures answer selection
accuracy. The generative version assesses the coherence and logical consistency
of reasoning chains. The acoustic-feature version investigates whether
variations in stress and emotion affect reasoning performance. Evaluations on
eleven state-of-the-art LALMs reveal that high transcription accuracy does not
translate into strong reasoning capabilities. SpeechR establishes a structured
benchmark for evaluating reasoning in spoken language, enabling more targeted
analysis of model capabilities across diverse dialogue-based tasks.

</details>


### [56] [Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time](https://arxiv.org/abs/2508.02037)
*Huihan Li,You Chen,Siyuan Wang,Yixin He,Ninareh Mehrabi,Rahul Gupta,Xiang Ren*

Main category: cs.CL

TL;DR: STIM 是一种新的框架，用于识别记忆源，它能将推理链中的每个标记归因于多个记忆源之一，并揭示模型在复杂或长尾情况下更依赖记忆，且局部记忆是错误的主要驱动因素。


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers.

Method: STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus.

Result: Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step.

Conclusion: STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.

Abstract: Large Language Models (LLMs) perform well on reasoning benchmarks but often
fail when inputs alter slightly, raising concerns about the extent to which
their success relies on memorization. This issue is especially acute in
Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger
intermediate errors that cascade into incorrect final answers. We introduce
STIM, a novel framework for Source-aware Token-level Identification of
Memorization, which attributes each token in a reasoning chain to one of
multiple memorization sources - local, mid-range, or long-range - based on
their statistical co-occurrence with the token in the pretraining corpus. Our
token-level analysis across tasks and distributional settings reveals that
models rely more on memorization in complex or long-tail cases, and that local
memorization is often the dominant driver of errors, leading to up to 67% of
wrong tokens. We also show that memorization scores from STIM can be effective
in predicting the wrong tokens in the wrong reasoning step. STIM offers a
powerful tool for diagnosing and improving model reasoning and can generalize
to other structured step-wise generation tasks.

</details>


### [57] [Marco-Voice Technical Report](https://arxiv.org/abs/2508.02038)
*Fengping Tian,Chenyang Lyu,Xuanfan Ni,Haoqin Sun,Qingjuan Li,Zhiqiang Qian,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文介绍了一种多功能语音合成系统，能够独立操控说话人身份和情感风格，并通过构建高质量的情感语音数据集进行训练和评估，取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决实现高度表现力、可控且自然的语音生成的长期挑战，以忠实保留不同语言和情感情境中的说话人身份。

Method: 本文提出了一种多功能语音合成系统，结合了语音克隆和情感控制语音合成，并引入了有效的说话人-情感解耦机制和批次内对比学习，以及旋转情感嵌入集成方法。

Result: 实验结果表明，Marco-Voice系统在客观和主观指标上都有显著提升，并在语音清晰度和情感丰富性方面表现出色。

Conclusion: 本文提出的Marco-Voice系统在语音清晰度和情感丰富性方面表现出色，代表了表达性神经语音合成领域的重大进展。

Abstract: This paper presents a multifunctional speech synthesis system that integrates
voice cloning and emotion control speech synthesis within a unified framework.
The goal of this work is to address longstanding challenges in achieving highly
expressive, controllable, and natural speech generation that faithfully
preserves speaker identity across diverse linguistic and emotional contexts.
Our approach introduces an effective speaker-emotion disentanglement mechanism
with in-batch contrastive learning, enabling independent manipulation of
speaker identity and eemotional style, as well as rotational emotional
embedding integration method for smooth emotion control. To support
comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality
emotional speech dataset containing 10 hours of Mandarin speech from six
professional speakers across seven emotional categories. Extensive experiments
demonstrate that our system, Marco-Voice, achieves substantial improvements in
both objective and subjective metrics. Comprehensive evaluations and analysis
were conducted, results show that MarcoVoice delivers competitive performance
in terms of speech clarity and emotional richness, representing a substantial
advance in the field of expressive neural speech synthesis.

</details>


### [58] [Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models](https://arxiv.org/abs/2508.02045)
*Soyeon Kim,Jindong Wang,Xing Xie,Steven Euijong Whang*

Main category: cs.CL

TL;DR: 本文提出了一种新的TSQA基准测试TDBench，利用时间数据库和数据库技术构建TSQA对，并引入时间准确性作为评估指标，以实现更可靠和全面的TSQA评估。


<details>
  <summary>Details</summary>
Motivation: 现有的TSQA基准测试通常依赖于手动整理或有限的预定义模板，这限制了TSQA评估的可扩展性和全面性。因此，需要一种新的方法来解决这些问题。

Method: 本文利用时间数据库和数据库技术（如时间SQL和函数依赖）系统地构建TSQA对，并引入时间准确性作为评估指标。

Result: 实验结果表明，TDBench能够实现可扩展和全面的TSQA评估，减少对人工劳动的依赖，并补充现有的基于Wikipedia/Wikidata的TSQA评估方法。

Conclusion: 本文提出了TDBench，这是一个新的基准测试，可以系统地构建TSQA对，从而实现可扩展和全面的TSQA评估。同时，引入了时间准确性作为细粒度评估指标，以提高TSQA评估的可靠性。

Abstract: Facts evolve over time, making it essential for Large Language Models (LLMs)
to handle time-sensitive factual knowledge accurately and reliably. While
factual Time-Sensitive Question-Answering (TSQA) tasks have been widely
studied, existing benchmarks often rely on manual curation or a small, fixed
set of predefined templates, which restricts scalable and comprehensive TSQA
evaluation. To address these challenges, we propose TDBench, a new benchmark
that systematically constructs TSQA pairs by harnessing temporal databases and
database techniques such as temporal SQL and functional dependencies. We also
introduce a fine-grained evaluation metric called time accuracy, which assesses
the validity of time references in model explanations alongside traditional
answer accuracy to enable a more reliable TSQA evaluation. Extensive
experiments on contemporary LLMs show how \ours{} enables scalable and
comprehensive TSQA evaluation while reducing the reliance on human labor,
complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by
enabling LLM evaluation on application-specific data and seamless multi-hop
question generation. Code and data are publicly available at:
https://github.com/ssoy0701/tdbench.git.

</details>


### [59] [ProCut: LLM Prompt Compression via Attribution Estimation](https://arxiv.org/abs/2508.02053)
*Zhentao Xu,Fengyi Li,Albert Chen,Xiaofeng Wang*

Main category: cs.CL

TL;DR: ProCut is a framework for compressing large prompts in industrial LLM systems by analyzing their impact on task performance.


<details>
  <summary>Details</summary>
Motivation: Large-scale industrial LLM systems face challenges with bloated prompts that are hard to maintain and costly to serve.

Method: ProCut uses attribution analysis to segment and prune low-utility components of prompt templates.

Result: ProCut achieves significant prompt size reductions (78% fewer tokens in production) while maintaining or slightly improving task performance (up to 62% better than alternative methods).

Conclusion: ProCut can compress prompts effectively while maintaining or improving task performance, and it integrates well with existing prompt-optimization frameworks.

Abstract: In large-scale industrial LLM systems, prompt templates often expand to
thousands of tokens as teams iteratively incorporate sections such as task
instructions, few-shot examples, and heuristic rules to enhance robustness and
coverage. This expansion leads to bloated prompts that are difficult to
maintain and incur significant inference latency and serving costs. To address
this, we introduce Prompt Compression via Attribution Estimation (ProCut), a
flexible, LLM-agnostic, training-free framework that compresses prompts through
attribution analysis. ProCut segments prompt templates into semantically
meaningful units, quantifies their impact on task performance, and prunes
low-utility components. Through extensive experiments on five public benchmark
datasets and real-world industrial prompts, we show that ProCut achieves
substantial prompt size reductions (78% fewer tokens in production) while
maintaining or even slightly improving task performance (up to 62% better than
alternative methods). We further introduce an LLM-driven attribution estimator
that reduces compression latency by over 50%, and demonstrate that ProCut
integrates seamlessly with existing prompt-optimization frameworks to produce
concise, high-performing prompts.

</details>


### [60] [The SMeL Test: A simple benchmark for media literacy in language models](https://arxiv.org/abs/2508.02074)
*Gustaf Ahdritz,Anat Kleiman*

Main category: cs.CL

TL;DR: 本文介绍了SMeL测试，用于评估语言模型在上下文中过滤不可靠信息的能力。结果表明，没有模型始终信任更可靠的来源，即使最好的API模型也有高达70%的时间出现幻觉。更大的模型并不一定表现更好。


<details>
  <summary>Details</summary>
Motivation: 互联网上充斥着未署名、故意误导或 otherwise 不可信的内容。尽管大型语言模型（LLMs）通常被赋予自主网络浏览的任务，但它们在多大程度上已经学会了人类研究人员使用的简单启发式方法来导航这个嘈杂的环境目前尚不清楚。

Method: 我们引入了合成媒体素养测试（SMeL Test），这是一个最小的基准，用于测试语言模型在上下文中主动过滤不可靠信息的能力。我们对各种常用的指令调优的LLM进行了基准测试，包括推理模型。

Result: 我们发现没有模型始终信任更可靠的来源；虽然推理尤其与更高的分数相关，但我们在测试中最好的API模型仍然有高达70%的时间出现幻觉。值得注意的是，更大、更强大的模型并不一定比它们的较小版本表现更好。

Conclusion: 我们的工作希望更多地揭示这种重要的幻觉，并指导开发新的方法来应对它。

Abstract: The internet is rife with unattributed, deliberately misleading, or otherwise
untrustworthy content. Though large language models (LLMs) are often tasked
with autonomous web browsing, the extent to which they have learned the simple
heuristics human researchers use to navigate this noisy environment is not
currently known. In this paper, we introduce the Synthetic Media Literacy Test
(SMeL Test), a minimal benchmark that tests the ability of language models to
actively filter out untrustworthy information in context. We benchmark a
variety of commonly used instruction-tuned LLMs, including reasoning models,
and find that no model consistently trusts more reliable sources; while
reasoning in particular is associated with higher scores, even the best API
model we test hallucinates up to 70% of the time. Remarkably, larger and more
capable models do not necessarily outperform their smaller counterparts. We
hope our work sheds more light on this important form of hallucination and
guides the development of new methods to combat it.

</details>


### [61] [When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models](https://arxiv.org/abs/2508.02087)
*Jin Li,Keyu Wang,Shu Yang,Zhuoran Zhang,Di Wang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的服从性行为，发现这种行为源于深层层中对已学知识的结构性覆盖，并且第一人称提示更容易引发服从性行为。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示大型语言模型（LLMs）为何会表现出服从性行为，即即使用户的观点与事实相矛盾，模型也会同意用户的看法。

Method: 本文通过logit-lens分析和因果激活补丁技术，研究了用户意见如何在不同模型家族中引发服从性行为，并探讨了语法视角对服从性行为的影响。

Result: 本文发现简单的意见陈述会可靠地引发服从性行为，而用户专业知识的框架影响很小。此外，第一人称提示（如“我认为...”）比第三人称框架（如“他们认为...”）更容易引发更高的服从性行为。

Conclusion: 本文指出，服从性行为不是表面的产物，而是源于深层层中对已学知识的结构性覆盖，这对对齐和真实AI系统有影响。

Abstract: Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing
with user-stated opinions even when those contradict factual knowledge. While
prior work has documented this tendency, the internal mechanisms that enable
such behavior remain poorly understood. In this paper, we provide a mechanistic
account of how sycophancy arises within LLMs. We first systematically study how
user opinions induce sycophancy across different model families. We find that
simple opinion statements reliably induce sycophancy, whereas user expertise
framing has a negligible impact. Through logit-lens analysis and causal
activation patching, we identify a two-stage emergence of sycophancy: (1) a
late-layer output preference shift and (2) deeper representational divergence.
We also verify that user authority fails to influence behavior because models
do not encode it internally. In addition, we examine how grammatical
perspective affects sycophantic behavior, finding that first-person prompts
(``I believe...'') consistently induce higher sycophancy rates than
third-person framings (``They believe...'') by creating stronger
representational perturbations in deeper layers. These findings highlight that
sycophancy is not a surface-level artifact but emerges from a structural
override of learned knowledge in deeper layers, with implications for alignment
and truthful AI systems.

</details>


### [62] ["Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth](https://arxiv.org/abs/2508.02094)
*Yaqiong Li,Peng Zhang,Lin Wang,Hansu Gu,Siyuan Qiao,Ning Gu,Tun Lu*

Main category: cs.CL

TL;DR: 本文研究了青少年在社交媒体中的独特毒性语言，并发现将其上下文信息纳入毒性检测方法可以提高准确性。


<details>
  <summary>Details</summary>
Motivation: 以往的研究主要关注社交媒体中的毒性检测，但忽略了青少年独特的毒性语言，即成人认为非毒性的语言对青少年来说可能是有毒的。

Method: 本文以中国青少年为研究对象，构建了第一个中文'青少年毒性'数据集，并进行了广泛分析。

Result: 研究结果表明，青少年对这些语言的感知与多个上下文因素有关，如话语的来源和文本相关特征。将这些元信息纳入当前的毒性检测方法可以显著提高准确性。

Conclusion: 本文提出了未来以青少年为中心的毒性检测研究的几个见解。

Abstract: Risk perception is subjective, and youth's understanding of toxic content
differs from that of adults. Although previous research has conducted extensive
studies on toxicity detection in social media, the investigation of youth's
unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as
youth, is ignored. To address this gap, we aim to explore: 1) What are the
features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing
toxicity detection techniques accurately detect these languages (RQ2). For
these questions, we took Chinese youth as the research target, constructed the
first Chinese ``youth-toxicity'' dataset, and then conducted extensive
analysis. Our results suggest that youth's perception of these is associated
with several contextual factors, like the source of an utterance and
text-related features. Incorporating these meta information into current
toxicity detection methods significantly improves accuracy overall. Finally, we
propose several insights into future research on youth-centered toxicity
detection.

</details>


### [63] [Learning Dynamics of Meta-Learning in Small Model Pretraining](https://arxiv.org/abs/2508.02189)
*David Demitri Africa,Yuval Weiss,Paula Buttery,Richard Diehl Martinez*

Main category: cs.CL

TL;DR: 本文研究了如何通过元学习使小语言模型的预训练更高效且更具可解释性。通过结合一阶MAML与子集遮罩语言模型预训练，生成了四个LLama风格的解码器仅模型，并在多个任务中进行了评估。结果表明，该方法在相同计算条件下提升了性能，并揭示了训练动态中的可解释模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大但成本高昂。我们想知道元学习是否可以使小语言模型的预训练不仅更好而且更具可解释性。

Method: 我们将一阶MAML与子集遮罩语言模型预训练相结合，生成了四个LLama风格的解码器仅模型，并在多种设置和实际应用中进行了评估。

Result: 与传统训练相比，我们的模型（i）在1.6倍的速度下达到相同的损失，（ii）在多语言通用NER上提高了F1分数，在相同计算条件下，（iii）使训练动态易于解读：网络表示先扩散后收敛到一个较小的共享子空间。这种两阶段变化在有效秩曲线和注意力头熵中表现为上升和下降。这些曲线指出了哪些层最早专业化以及哪些层后来重新收敛，提供了一个紧凑、可解释的元适应签名。

Conclusion: 通过将一阶MAML与子集遮罩语言模型预训练相结合，我们生成了四个LLama风格的解码器仅模型，并在多种设置和实际应用中进行了评估。结果表明，我们的模型在相同计算条件下达到了更高的性能，并且训练动态更容易解释。

Abstract: Large language models are powerful but costly. We ask whether meta-learning
can make the pretraining of small language models not only better but also more
interpretable. We integrate first-order MAML with subset-masked LM pretraining,
producing four LLama-style decoder-only models (11M-570M params), and evaluate
it on a fundamental NLP task with many settings and real-world applications.
Compared with vanilla training, our model (i) reaches the same loss up to 1.6x
sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and
(iii) makes the training dynamics easy to read: first the network's
representations fan out ("diversify") and later they collapse into a smaller,
shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall
in both effective-rank curves and attention-head entropy. The same curves
pinpoint which layers specialise earliest and which later reconverge, giving a
compact, interpretable signature of meta-adaptation. Code, checkpoints and
WandB logs are released.

</details>


### [64] [Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference](https://arxiv.org/abs/2508.02193)
*Yuxuan Song,Zheng Zhang,Cheng Luo,Pengyang Gao,Fan Xia,Hao Luo,Zheng Li,Yuehang Yang,Hongli Yu,Xingwei Qu,Yuwei Fu,Jing Su,Ge Zhang,Wenhao Huang,Mingxuan Wang,Lin Yan,Xiaoying Jia,Jingjing Liu,Wei-Ying Ma,Ya-Qin Zhang,Yonghui Wu,Hao Zhou*

Main category: cs.CL

TL;DR: Seed Diffusion Preview 是一种基于离散状态扩散的大规模语言模型，实现了显著更快的推理速度，同时保持了竞争力。


<details>
  <summary>Details</summary>
Motivation: 为了减轻逐标记解码的固有延迟，需要一种更快的推理方法。

Method: 基于离散状态扩散的大规模语言模型，采用非顺序并行生成以提高推理速度。

Result: 在H20 GPU上实现了2,146个token/s的推理速度，并在标准代码评估基准中保持了竞争力，比Mercury和Gemini Diffusion更快。

Conclusion: Seed Diffusion Preview 在保持竞争力的同时，实现了显著更快的推理速度，为代码模型建立了新的速度-质量前沿。

Abstract: We present Seed Diffusion Preview, a large-scale language model based on
discrete-state diffusion, offering remarkably fast inference speed. Thanks to
non-sequential, parallel generation, discrete diffusion models provide a
notable speedup to mitigate the inherent latency of token-by-token decoding, as
demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion
Preview achieves an inference speed of 2,146 token/s over H20 GPUs while
maintaining competitive performance across a sweep of standard code evaluation
benchmarks, significantly faster than contemporary Mercury and Gemini
Diffusion, establishing new state of the art on the speed-quality Pareto
frontier for code models.

</details>


### [65] [Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems](https://arxiv.org/abs/2508.02208)
*Yebo Peng,Zixiang Liu,Yaoming Li,Zhizhuo Yang,Xinye Xu,Bowen Ye,Weijun Yuan,Zihan Wang,Tong Yang*

Main category: cs.CL

TL;DR: 本文提出了一个自动化框架Proof2Hybrid，用于生成高质量的数学证明基准，并引入了一个新的混合格式问题，以更准确地评估大型语言模型的数学能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试在证明导向的问题上存在不足，因为手动创建是不可扩展且昂贵的，导致LLM的真实数学能力未被充分评估。

Method: 我们提出了Proof2Hybrid，这是一个完全自动化的框架，可以从自然语言数学语料库中合成高质量的、以证明为中心的基准。我们还提出了新的混合格式问题，名为“m-out-of-n多裁判问题”，专门设计用于实现稳健的自动评估。

Result: 我们在最先进的LLM上使用AlgGeoTest进行的广泛评估揭示了它们在理解代数几何方面的深刻缺陷，提供了对其真实数学能力的更精确衡量。

Conclusion: 我们的框架和基准为深入研究人工智能系统的数学智能开辟了新的浪潮。

Abstract: Evaluating the mathematical capability of Large Language Models (LLMs) is a
critical yet challenging frontier. Existing benchmarks fall short, particularly
for proof-centric problems, as manual creation is unscalable and costly,
leaving the true mathematical abilities of LLMs largely unassessed. To overcome
these barriers, we propose Proof2Hybrid, the first fully automated framework
that synthesizes high-quality, proof-centric benchmarks from natural language
mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of
converting mathematical proofs into various kinds of questions that are easy to
verify. Instructed by this roadmap, we propose a new type of hybrid-formatted
questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically
designed to enable robust, automatic evaluation while being resilient to
guessing and superficial pattern matching inherent in traditional formats. As a
demonstration of our framework, we introduce AlgGeoTest, a benchmark for
algebraic geometry--a frontier domain of modern mathematics--comprising 456
challenging items. Our extensive evaluations on state-of-the-art LLMs using
AlgGeoTest reveal profound deficits in their comprehension of algebraic
geometry, providing a more precise measure of their true mathematical
capabilities. Our framework and benchmark pave the way for a new wave of
in-depth research into the mathematical intelligence of AI systems.

</details>


### [66] [Isolating Culture Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2508.02241)
*Danial Namazifard,Lukas Galke*

Main category: cs.CL

TL;DR: 本文研究了多语言大型语言模型如何编码文化，并通过引入MUREL数据集进行实验，发现文化特异性神经元可以被独立调节。


<details>
  <summary>Details</summary>
Motivation: 语言和文化紧密交织，但目前尚不清楚多语言大型语言模型如何以及在何处编码文化。

Method: 我们扩展了一种既定的方法来识别语言特异性神经元，并将其扩展到定位和隔离文化特异性神经元，仔细解耦它们与语言特异性神经元的重叠和相互作用。

Result: 我们的定位和干预实验显示，LLMs 在不同的神经元群体中编码不同的文化，主要在上层，并且这些文化神经元可以独立于语言特异性神经元或其他文化特异性神经元进行调节。

Conclusion: 这些发现表明，多语言语言模型中的文化知识和倾向可以被选择性地隔离和编辑——促进公平、包容和对齐。

Abstract: Language and culture are deeply intertwined, yet it is so far unclear how and
where multilingual large language models encode culture. Here, we extend upon
an established methodology for identifying language-specific neurons and extend
it to localize and isolate culture-specific neurons, carefully disentangling
their overlap and interaction with language-specific neurons. To facilitate our
experiments, we introduce MUREL, a curated dataset of 85.2 million tokens
spanning six different cultures. Our localization and intervention experiments
show that LLMs encode different cultures in distinct neuron populations,
predominantly in upper layers, and that these culture neurons can be modulated
independently from language-specific neurons or those specific to other
cultures. These findings suggest that cultural knowledge and propensities in
multilingual language models can be selectively isolated and edited - promoting
fairness, inclusivity, and alignment. Code and data is available at
https://github.com/namazifard/Culture_Neurons .

</details>


### [67] [Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders](https://arxiv.org/abs/2508.02256)
*Belen Alastruey,João Maria Janeiro,Alexandre Allauzen,Maha Elbayad,Loïc Barrault,Marta R. Costa-jussà*

Main category: cs.CL

TL;DR: 本文研究了编码器-only Transformer 模型中语言干扰的表现，发现干扰模式与脚本相关，并展示了干扰矩阵在预测下游任务表现方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究语言干扰在编码器-only Transformer 模型中的表现，以更好地设计多语言模型以获得最佳性能。

Method: 我们通过在所有可能的语言对上训练和评估小型 BERT 类模型，构建了一个干扰矩阵，提供了跨语言干扰的大规模量化分析。

Result: 干扰模式与传统语言特征或代理指标不一致，但与脚本相关。干扰矩阵可以有效预测下游任务的表现。

Conclusion: 我们的分析揭示了语言之间的干扰是不对称的，其模式与传统的语言特征（如语言家族）或代理指标（如嵌入相似性）不一致，而是更与脚本相关。最后，我们证明干扰矩阵可以有效地预测下游任务的表现，作为设计多语言模型以获得最佳性能的工具。

Abstract: In this paper, we present a comprehensive study of language interference in
encoder-only Transformer models across 83 languages. We construct an
interference matrix by training and evaluating small BERT-like models on all
possible language pairs, providing a large-scale quantification of
cross-lingual interference. Our analysis reveals that interference between
languages is asymmetrical and that its patterns do not align with traditional
linguistic characteristics, such as language family, nor with proxies like
embedding similarity, but instead better relate to script. Finally, we
demonstrate that the interference matrix effectively predicts performance on
downstream tasks, serving as a tool to better design multilingual models to
obtain optimal performance.

</details>


### [68] [Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning](https://arxiv.org/abs/2508.02260)
*Jia Deng,Jie Chen,Zhipeng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文分析了RLVR中的熵-性能交换机制，并提出了两种方法来提高学习效率。


<details>
  <summary>Details</summary>
Motivation: 为了填补对RLVR中熵-性能交换机制的细粒度理解的空白，我们进行了系统的实证分析。

Method: 我们将训练过程分为上升阶段和平台阶段，并系统地研究了该机制在不同粒度下的变化。基于发现，我们提出了两种方法，利用困惑度和位置信息动态调整奖励信号。

Result: 在上升阶段，负样本的熵减少有助于学习有效的推理模式，从而带来快速的性能提升。在平台阶段，学习效率与低困惑度样本中的高熵标记以及序列末尾的标记密切相关。

Conclusion: 通过分析RLVR中的熵-性能交换机制，我们提出了两种方法，通过动态调整奖励信号来提高学习效率，并在各种LLM上取得了比基线方法更好的效果。

Abstract: Recently, reinforcement learning with verifiable rewards (RLVR) has been
widely used for enhancing the reasoning abilities of large language models
(LLMs). A core challenge in RLVR involves managing the exchange between entropy
and performance of policies. Despite the importance of this exchange, a
fine-grained understanding of when and how this exchange operates most
effectively remains limited. To bridge this gap, we conduct a systematic
empirical analysis of the entropy-performance exchange mechanism of RLVR across
different levels of granularity. Specifically, we first divide the training
process into two distinct stages based on entropy dynamics, i.e., rising stage
and plateau stage, and then systematically investigate how this mechanism
varies across stage-level, instance-level, and token-level granularitiess. Our
analysis reveals that, in the rising stage, entropy reduction in negative
samples facilitates the learning of effective reasoning patterns, which in turn
drives rapid performance gains. Moreover, in the plateau stage, learning
efficiency strongly correlates with high-entropy tokens present in
low-perplexity samples and those located at the end of sequences. Motivated by
these findings, we propose two methods that dynamically adjust the reward
signal using perplexity and positional information to focus RL updates on
tokens that exhibit high learning potential, achieving improvements compared to
the baseline methods on various LLMs.

</details>


### [69] [SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System](https://arxiv.org/abs/2508.02268)
*Serry Sibaee,Omer Nacar,Yasser Al-Habashi,Adel Ammar,Wadii Boulila*

Main category: cs.CL

TL;DR: 本文介绍了一种名为SHAMI-MT的双向机器翻译系统，旨在弥合现代标准阿拉伯语（MSA）和叙利亚方言之间的交流差距。该系统基于最先进的AraT5v2-base-1024架构，在Nabra数据集上进行了微调，并在MADAR语料库的未见数据上进行了评估。结果表明，该系统能够生成高质量且方言真实的翻译，为方言阿拉伯语翻译提供了重要的工具。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯世界的丰富语言景观由现代标准阿拉伯语（MSA）和日常生活中使用的各种地区方言之间的显著差距所定义。这种双语现象对自然语言处理，特别是机器翻译构成了重大挑战。

Method: 本文介绍了SHAMI-MT，这是一个双向机器翻译系统，专门设计用于弥合MSA和叙利亚方言之间的交流差距。我们提出了两个专业模型，一个用于MSA到Shami的翻译，另一个用于Shami到MSA的翻译，均基于最先进的AraT5v2-base-1024架构。这些模型在Nabra数据集上进行了微调，并在MADAR语料库的未见数据上进行了严格评估。

Result: 我们的MSA到Shami模型在OPENAI模型GPT-4.1的评估中获得了4.01的平均质量分数（满分5.0），证明了其能够生成不仅准确而且方言真实的翻译。

Conclusion: 本文提供了一个关键的高保真工具，用于之前未得到充分服务的语言对，推动了方言阿拉伯语翻译领域的发展，并在内容本地化、文化遗产和跨文化沟通方面具有重要意义。

Abstract: The rich linguistic landscape of the Arab world is characterized by a
significant gap between Modern Standard Arabic (MSA), the language of formal
communication, and the diverse regional dialects used in everyday life. This
diglossia presents a formidable challenge for natural language processing,
particularly machine translation. This paper introduces \textbf{SHAMI-MT}, a
bidirectional machine translation system specifically engineered to bridge the
communication gap between MSA and the Syrian dialect. We present two
specialized models, one for MSA-to-Shami and another for Shami-to-MSA
translation, both built upon the state-of-the-art AraT5v2-base-1024
architecture. The models were fine-tuned on the comprehensive Nabra dataset and
rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami
model achieved an outstanding average quality score of \textbf{4.01 out of 5.0}
when judged by OPENAI model GPT-4.1, demonstrating its ability to produce
translations that are not only accurate but also dialectally authentic. This
work provides a crucial, high-fidelity tool for a previously underserved
language pair, advancing the field of dialectal Arabic translation and offering
significant applications in content localization, cultural heritage, and
intercultural communication.

</details>


### [70] [Dynaword: From One-shot to Continuously Developed Datasets](https://arxiv.org/abs/2508.02271)
*Kenneth Enevoldsen,Kristian Nørgaard Jensen,Jan Kostkan,Balázs Szabó,Márton Kardos,Kirten Vad,Andrea Blasi Núñez,Gianluca Barmina,Jacob Nielsen,Rasmus Larsen,Peter Vahlstrup,Per Møldrup Dalum,Desmond Elliott,Lukas Galke,Peter Schneider-Kamp,Kristoffer Nielbo*

Main category: cs.CL

TL;DR: 本文提出了一种通过社区协作持续更新的大规模开放数据集框架，并通过Danish Dynaword验证了其潜力。


<details>
  <summary>Details</summary>
Motivation: 当前的方法面临三个主要挑战：依赖于模糊许可来源、静态数据集发布以及质量保证过程仅限于发布团队。

Method: 本文提出了Dynaword方法，这是一个用于创建大规模、开放数据集的框架，可以通过社区协作进行持续更新。同时，Danish Dynaword是该方法的一个具体实现。

Result: Danish Dynaword包含的标记数是类似发布的四倍，完全开放许可，并且获得了来自工业界和研究界的多个贡献。仓库中包含了轻量级测试，以确保数据格式、质量和文档，建立了一个可持续的框架，用于持续的社区贡献和数据集演化。

Conclusion: 本文介绍了Dynaword方法和Danish Dynaword，这是一种通过社区协作持续更新的大规模开放数据集框架，并验证了其潜力。

Abstract: Large-scale datasets are foundational for research and development in natural
language processing. However, current approaches face three key challenges: (1)
reliance on ambiguously licensed sources restricting use, sharing, and
derivative works; (2) static dataset releases that prevent community
contributions and diminish longevity; and (3) quality assurance processes
restricted to publishing teams rather than leveraging community expertise.
  To address these limitations, we introduce two contributions: the Dynaword
approach and Danish Dynaword. The Dynaword approach is a framework for creating
large-scale, open datasets that can be continuously updated through community
collaboration. Danish Dynaword is a concrete implementation that validates this
approach and demonstrates its potential. Danish Dynaword contains over four
times as many tokens as comparable releases, is exclusively openly licensed,
and has received multiple contributions across industry and research. The
repository includes light-weight tests to ensure data formatting, quality, and
documentation, establishing a sustainable framework for ongoing community
contributions and dataset evolution.

</details>


### [71] [A French Version of the OLDI Seed Corpus](https://arxiv.org/abs/2508.02290)
*Malik Marmonier,Benoît Sagot,Rachel Bawden*

Main category: cs.CL

TL;DR: 本文介绍了为WMT 2025 OLDI共享任务创建的第一个法语语料库，用于促进法国弱势地区语言的平行语料库收集。


<details>
  <summary>Details</summary>
Motivation: 创建法语语料库是为了帮助收集法国弱势地区语言的平行语料库。

Method: 使用多个机器翻译系统和自定义界面进行后编辑，由合格的母语者进行后编辑。

Result: 介绍了语料库的创建过程，并强调了源数据带来的独特翻译挑战。

Conclusion: 该法语语料库旨在作为关键的枢纽资源，以促进收集法国弱势地区语言的平行语料库。

Abstract: We present the first French partition of the OLDI Seed Corpus, our submission
to the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its
creation process, which involved using multiple machine translation systems and
a custom-built interface for post-editing by qualified native speakers. We also
highlight the unique translation challenges presented by the source data, which
combines highly technical, encyclopedic terminology with the stylistic
irregularities characteristic of user-generated content taken from Wikipedia.
This French corpus is not an end in itself, but is intended as a crucial pivot
resource to facilitate the collection of parallel corpora for the
under-resourced regional languages of France.

</details>


### [72] [Simple Methods Defend RAG Systems Well Against Real-World Attacks](https://arxiv.org/abs/2508.02296)
*Ilias Triantafyllopoulos,Renyi Qu,Salvatore Giorgi,Brenda Curtis,Lyle H. Ungar,João Sedoc*

Main category: cs.CL

TL;DR: 本文研究了如何确保检索增强生成（RAG）系统仅对系统知识库内的查询作出响应，通过评估多种方法并验证其有效性，确认了外部OOD检测器的重要性。


<details>
  <summary>Details</summary>
Motivation: 确保检索增强生成（RAG）系统在安全关键应用中的安全性和域内响应至关重要，但仍然是一个重大挑战。

Method: 本文评估了四种用于检测Out-Of-Domain（OOD）查询的方法：GPT-4o、基于回归的方法、基于主成分分析（PCA）的方法以及基于神经崩溃（NC）的方法。此外，还探索了两种新的降维和特征分离策略：PCA和神经崩溃特征分离。

Result: 通过在标准数据集（StackExchange和MSMARCO）和实际应用（物质使用和新冠状病毒）上验证方法，并对新冠状病毒疫苗聊天机器人的LLM模拟攻击和实际攻击进行测试，结果表明外部OOD检测器对于保持响应的相关性至关重要。

Conclusion: 本文结论是，外部的OOD检测器对于保持响应的相关性至关重要。

Abstract: Ensuring safety and in-domain responses for Retrieval-Augmented Generation
(RAG) systems is paramount in safety-critical applications, yet remains a
significant challenge. To address this, we evaluate four methodologies for
Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal
Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG
system only responds to queries confined to the system's knowledge base.
Specifically, our evaluation explores two novel dimensionality reduction and
feature separation strategies: \textit{PCA}, where top components are selected
using explained variance or OOD separability, and an adaptation of
\textit{Neural Collapse Feature Separation}. We validate our approach on
standard datasets (StackExchange and MSMARCO) and real-world applications
(Substance Use and COVID-19), including tests against LLM-simulated and actual
attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations
of response correctness and relevance, we confirm that an external OOD detector
is crucial for maintaining response relevance.

</details>


### [73] [LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training](https://arxiv.org/abs/2508.02308)
*Sikui Zhang,Guangze Gao,Ziyun Gan,Chunfeng Yuan,Zefeng Lin,Houwen Peng,Bing Li,Weiming Hu*

Main category: cs.CL

TL;DR: LaMPE是一种无需训练的方法，通过动态调整位置编码和多粒度注意力机制，提升了大语言模型在长上下文任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过固定映射策略将OOD位置映射到分布内范围，忽略了输入长度与模型有效上下文窗口之间的动态关系。

Method: LaMPE是一种无需训练的方法，通过参数化缩放sigmoid函数建立映射长度和输入长度之间的动态关系，并设计了一种新的多粒度注意力机制，以适应不同序列区域的位置分辨率分配。

Result: 在三个代表性LLM和五个主流长上下文基准上的广泛实验表明，LaMPE相比现有长度外推方法取得了显著的性能提升。

Conclusion: LaMPE能够显著提升现有长度外推方法的性能，并且可以无缝应用于基于RoPE的LLM，无需训练。

Abstract: Large language models (LLMs) experience significant performance degradation
when the input exceeds the pretraining context window, primarily due to the
out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent
studies mitigate this problem by remapping OOD positions into the
in-distribution range with fixed mapping strategies, ignoring the dynamic
relationship between input length and the model's effective context window. To
this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a
training-free method that fully utilizes the model's effective context window
for adaptive long-context scaling in LLMs. Motivated by the left-skewed
frequency distribution of relative positions, LaMPE establishes a dynamic
relationship between mapping length and input length through a parametric
scaled sigmoid function to adaptively allocate positional capacity across
varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention
mechanism that strategically allocates positional resolution across different
sequence regions to capture both fine-grained locality and long-range
dependencies. Our method can be seamlessly applied to a wide range of
RoPE-based LLMs without training. Extensive experiments on three representative
LLMs across five mainstream long-context benchmarks demonstrate that LaMPE
achieves significant performance improvements compared to existing length
extrapolation methods. The code will be released at
https://github.com/scar-on/LaMPE.

</details>


### [74] [VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo](https://arxiv.org/abs/2508.02317)
*Qianli Ma,Yaowei Zheng,Zhelun Shi,Zhongkai Zhao,Bin Jia,Ziyue Huang,Zhiqi Lin,Youjie Li,Jiacheng Yang,Yanghua Peng,Zhi Zhang,Xin Liu*

Main category: cs.CL

TL;DR: veomni是一个模块化且高效的训练框架，旨在加速全模态LLM的开发。它通过将通信与计算解耦，实现了高效的3D并行，并提供了灵活的配置接口，支持新模态的无缝集成。使用veomni，可以高效地训练大型全模态LLM。


<details>
  <summary>Details</summary>
Motivation: 训练全模态LLM面临挑战，因为需要处理不同模态的异构模型架构，这需要复杂的系统设计来实现高效的大型训练。现有的框架通常将模型定义与并行逻辑纠缠在一起，导致可扩展性有限且工程开销大。

Method: veomni引入了以模型为中心的分布式方案，将通信与计算解耦，实现了全模态LLM上的高效3D并行。此外，它还具有一个灵活的配置接口，支持新模态的无缝集成，只需少量代码更改。

Result: 使用veomni，一个具有30B参数的全模态专家混合（MoE）模型可以在128个GPU上通过3D并行训练，达到每GPU超过2,800个token/sec的吞吐量，并扩展到160K上下文长度。

Conclusion: veomni展示了其在训练大型全模态LLM方面的优越效率和可扩展性。

Abstract: Recent advances in large language models (LLMs) have driven impressive
progress in omni-modal understanding and generation. However, training
omni-modal LLMs remains a significant challenge due to the heterogeneous model
architectures required to process diverse modalities, necessitating
sophisticated system design for efficient large-scale training. Existing
frameworks typically entangle model definition with parallel logic, incurring
limited scalability and substantial engineering overhead for end-to-end
omni-modal training. % We present \veomni, a modular and efficient training
framework to accelerate the development of omni-modal LLMs. \veomni introduces
model-centric distributed recipes that decouples communication from
computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also
features a flexible configuration interface supporting seamless integration of
new modalities with minimal code change. % Using \veomni, a omni-modal
mixture-of-experts (MoE) model with 30B parameters can be trained with over
2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D
parallelism on 128 GPUs, showcasing its superior efficiency and scalability for
training large omni-modal LLMs.

</details>


### [75] [CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis](https://arxiv.org/abs/2508.02322)
*Yuzhuang Xu,Xu Han,Yuanchi Zhang,Yixuan Wang,Yijun Liu,Shiyu Ji,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文通过引入微专家压缩单元和CAMERA框架，提高了MoE模型的性能和计算效率，取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有的专家级剪枝、合并或分解方法在性能和计算效率上仍有挑战，因此需要一种更细粒度的压缩单元来解决这些问题。

Method: 本文将MoE层视为微专家的混合，并提出了CAMERA框架来识别微专家冗余。进一步提出了CAMERA-P和CAMERA-Q方法进行结构化微专家剪枝和混合精度量化。

Result: CAMERA-P在20%到60%的剪枝率下优于现有基线，CAMERA-Q在2比特量化下表现优于现有矩阵和通道级方法。此外，该方法能在单个NVIDIA A100-40GB GPU上快速分析大模型。

Conclusion: 本文提出了一种新的微专家压缩单元，通过引入CAMERA框架和CAMERA-P、CAMERA-Q方法，显著提升了MoE模型的性能和计算效率。

Abstract: Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are
distinguished by their strong performance scaling with increasing parameters
across a wide range of tasks, yet they also suffer from substantial
computational and storage overheads. Notably, the performance gains of MoE
models do not scale proportionally with the growth in expert parameters. While
prior works attempt to reduce parameters via expert-level pruning, merging, or
decomposition, they still suffer from challenges in both performance and
computational efficiency. In this paper, we address these challenges by
introducing micro-expert as a finer-grained compression unit that spans across
matrices. We first establish a more fundamental perspective, viewing MoE layers
as mixtures of micro-experts, and present CAMERA, a lightweight and
training-free framework for identifying micro-expert redundancy. Our analysis
uncovers significant variance in micro-expert contributions during decoding.
Based on this insight, we further propose CAMERA-P, a structured micro-expert
pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed
for micro-experts. Extensive experiments on nine downstream tasks show that
CAMERA-P consistently outperforms strong baselines under pruning ratios ranging
from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under
aggressive 2-bit quantization, surpassing existing matrix- and channel-level
ideas. Notably, our method enables complete micro-expert analysis of
Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.

</details>


### [76] [Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models](https://arxiv.org/abs/2508.02360)
*Jiayi Zhang,Shu Yang,Junchao Wu,Derek F. Wong,Di Wang*

Main category: cs.CL

TL;DR: 本文研究了政治微调对大型语言模型的影响，提出了一种识别政治神经元的方法PNLAC，并引入了抑制微调方法InhibitFT，有效缓解了跨主题立场泛化问题。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究已经提出了政治微调可能影响模型的政治立场的问题，但目前仍缺乏对这些立场内部表示和导致无意跨主题泛化的机制的理解。因此，本文旨在系统地探索这一现象的内部机制，并提出有效的解决方案。

Method: 本文提出了一种通过激活对比来定位政治神经元的方法PNLAC，用于识别两种类型的神经元：普遍政治神经元和特定主题神经元。此外，还引入了基于抑制的微调方法InhibitFT，以减轻政治微调导致的跨主题泛化问题。

Result: 实验结果表明，所识别的神经元类型在各种模型和数据集中具有鲁棒性，并且InhibitFT显著减少了跨主题立场泛化，平均减少了20%。此外，仅抑制5%的神经元就足以有效缓解跨主题立场泛化。

Conclusion: 本文通过研究政治微调对大型语言模型的影响，提出了识别政治神经元的方法PNLAC，并引入了抑制微调方法InhibitFT，有效缓解了跨主题立场泛化问题。实验结果表明，InhibitFT在保持特定主题性能的同时，平均减少了20%的跨主题立场泛化。

Abstract: Fine-tuning Large Language Models on a political topic will significantly
manipulate their political stance on various issues and unintentionally affect
their stance on unrelated topics. While previous studies have proposed this
issue, there is still a lack of understanding regarding the internal
representations of these stances and the mechanisms that lead to unintended
cross-topic generalization. In this paper, we systematically explore the
internal mechanisms underlying this phenomenon from a neuron-level perspective
and how to mitigate the cross-topic generalization of political fine-tuning.
Firstly, we propose Political Neuron Localization through Activation
Contrasting (PNLAC) to identify two distinct types of political neurons:
general political neurons, which govern stance across multiple political
topics, and topic-specific neurons} that affect the model's political stance on
individual topics. We find the existence of these political neuron types across
four models and datasets through activation patching experiments. Leveraging
these insights, we introduce InhibitFT, an inhibition-based fine-tuning method,
effectively mitigating the cross-topic stance generalization. Experimental
results demonstrate the robustness of identified neuron types across various
models and datasets, and show that InhibitFT significantly reduces the
cross-topic stance generalization by 20% on average, while preserving
topic-specific performance. Moreover, we demonstrate that selectively
inhibiting only 5% of neurons is sufficient to effectively mitigate the
cross-topic stance generalization.

</details>


### [77] [CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation](https://arxiv.org/abs/2508.02401)
*Xiaolin Lin,Jingcun Wang,Olga Kondrateva,Yiyu Shi,Bing Li,Grace Li Zhang*

Main category: cs.CL

TL;DR: This paper proposes CompressKV, a method for efficient KV cache compression in GQA-based LLMs by identifying important attention heads and using them to retain critical tokens, leading to improved performance under memory constraints.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods in GQA-based LLMs use heuristic token eviction based on all attention heads, which ignores the different functionalities of attention heads and leads to the eviction of critical tokens, degrading LLM performance.

Method: We identify attention heads in each layer that can retrieve initial, final, and important tokens within the text and attend to their surrounding semantic context. We then use these heads to determine important tokens and retain their corresponding KV cache pairs. Additionally, we introduce a layer-adaptive KV cache allocation strategy.

Result: The proposed CompressKV method consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.

Conclusion: CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.

Abstract: Recent advances in large language models (LLMs) have significantly boosted
long-context processing. However, the increasing key-value (KV) cache size
poses critical challenges to memory and execution efficiency. Most KV cache
compression methods rely on heuristic token eviction using all attention heads
in Grouped Query Attention (GQA)-based LLMs. This method ignores the different
functionalities of attention heads, leading to the eviction of critical tokens
and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in
GQA-based LLMs to determine important tokens as in the previous work, we first
identify the attention heads in each layer that are not only capable of
retrieving the initial and final tokens of a prompt, but also capable of
retrieving important tokens within the text and attending to their surrounding
semantic context. Afterwards, we exploit such heads to determine the important
tokens and retain their corresponding KV cache pairs. Furthermore, we analyze
the cache eviction error of each layer individually and introduce a
layer-adaptive KV cache allocation strategy. Experimental results demonstrate
the proposed CompressKV consistently outperforms state-of-the-art approaches
under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.
Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.

</details>


### [78] [Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.02426)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Yichi Zhang,Haoran Duan,Nyima Tash*

Main category: cs.CL

TL;DR: 本文提出了一种新的持续知识图嵌入模型BAKE，通过贝叶斯后验更新原则和持续聚类方法有效缓解了灾难性遗忘问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统KGE模型仅适用于静态知识图，而现实场景中知识图会不断演化，因此需要一种能够处理持续学习的CKGE模型。

Method: BAKE模型基于贝叶斯后验更新原则，将每个新数据批次视为模型先验的更新，并提出了一种持续聚类方法来直接对抗知识遗忘。

Result: 在多个数据集上的实验结果表明，BAKE模型显著优于现有基线模型。

Conclusion: BAKE显著优于现有的基线模型，能够有效缓解持续知识图嵌入中的灾难性遗忘问题。

Abstract: Since knowledge graphs (KG) will continue to evolve in real scenarios,
traditional KGE models are only suitable for static knowledge graphs.
Therefore, continual knowledge graph embedding (CKGE) has attracted the
attention of researchers. Currently, a key challenge facing CKGE is that the
model is prone to "catastrophic forgetting", resulting in the loss of
previously learned knowledge. In order to effectively alleviate this problem,
we propose a new CKGE model BAKE. First, we note that the Bayesian posterior
update principle provides a natural continual learning strategy that is
insensitive to data order and can theoretically effectively resist the
forgetting of previous knowledge during data evolution. Different from the
existing CKGE method, BAKE regards each batch of new data as a Bayesian update
of the model prior. Under this framework, as long as the posterior distribution
of the model is maintained, the model can better preserve the knowledge of
early snapshots even after evolving through multiple time snapshots. Secondly,
we propose a continual clustering method for CKGE, which further directly
combats knowledge forgetting by constraining the evolution difference (or
change amplitude) between new and old knowledge between different snapshots. We
conduct extensive experiments on BAKE on multiple datasets, and the results
show that BAKE significantly outperforms existing baseline models.

</details>


### [79] [AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications](https://arxiv.org/abs/2508.02430)
*Robin Nowak,Patrick Figge,Carolin Haeussler*

Main category: cs.CL

TL;DR: 本文探讨了如何利用大型语言模型（LLM）来克服传统创新测量方法的局限性，并展示了一个有效的LLM框架，用于从非结构化文本数据中评估创新。


<details>
  <summary>Details</summary>
Motivation: 传统创新测量方法依赖于特定上下文的代理指标和专家评估，这限制了实证创新研究的应用范围。本文旨在利用LLM克服手动专家评估的限制，帮助研究人员测量创新。

Method: 本文设计了一个LLM框架，该框架能够从非结构化文本数据中可靠地近似领域专家对创新的评估。

Result: LLM框架在两个不同情境下的研究中表现出色，包括软件应用更新的创新性和用户生成反馈和改进想法的原创性。LLM框架的F1得分高于其他方法，且结果高度一致。

Conclusion: 本文展示了大型语言模型（LLM）在测量创新方面的潜力，并讨论了其作为可靠、可访问和广泛应用的研究工具的重要性。

Abstract: Measuring innovation often relies on context-specific proxies and on expert
evaluation. Hence, empirical innovation research is often limited to settings
where such data is available. We investigate how large language models (LLMs)
can be leveraged to overcome the constraints of manual expert evaluations and
assist researchers in measuring innovation. We design an LLM framework that
reliably approximates domain experts' assessment of innovation from
unstructured text data. We demonstrate the performance and broad applicability
of this framework through two studies in different contexts: (1) the
innovativeness of software application updates and (2) the originality of
user-generated feedback and improvement ideas in product reviews. We compared
the performance (F1-score) and reliability (consistency rate) of our LLM
framework against alternative measures used in prior innovation studies, and to
state-of-the-art machine learning- and deep learning-based models. The LLM
framework achieved higher F1-scores than the other approaches, and its results
are highly consistent (i.e., results do not change across runs). This article
equips R&D personnel in firms, as well as researchers, reviewers, and editors,
with the knowledge and tools to effectively use LLMs for measuring innovation
and evaluating the performance of LLM-based innovation measures. In doing so,
we discuss, the impact of important design decisions-including model selection,
prompt engineering, training data size, training data distribution, and
parameter settings-on performance and reliability. Given the challenges
inherent in using human expert evaluation and existing text-based measures, our
framework has important implications for harnessing LLMs as reliable,
increasingly accessible, and broadly applicable research tools for measuring
innovation.

</details>


### [80] [LatentPrompt: Optimizing Promts in Latent Space](https://arxiv.org/abs/2508.02452)
*Mateusz Bystroński,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.CL

TL;DR: LatentPrompt是一种模型无关的提示优化框架，利用潜在语义空间自动生成和优化提示，提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前的优化技术依赖于启发式方法或手动探索，而LatentPrompt旨在提供一种自动化的解决方案。

Method: LatentPrompt通过将种子提示嵌入连续的潜在语义空间中，并系统地探索该空间来最大化任务特定性能，从而实现提示优化。

Result: 在金融短语银行情感分类基准上，LatentPrompt在一次优化周期后将分类准确率提高了约3%。

Conclusion: LatentPrompt是一个模型无关的框架，能够自动生成、评估和优化提示，无需手工规则。它在金融短语银行情感分类基准上提高了分类准确率，并且适用于各种领域和任务。

Abstract: Recent advances have shown that optimizing prompts for Large Language Models
(LLMs) can significantly improve task performance, yet many optimization
techniques rely on heuristics or manual exploration. We present LatentPrompt, a
model-agnostic framework for prompt optimization that leverages latent semantic
space to automatically generate, evaluate, and refine candidate prompts without
requiring hand-crafted rules. Beginning with a set of seed prompts, our method
embeds them in a continuous latent space and systematically explores this space
to identify prompts that maximize task-specific performance. In a
proof-of-concept study on the Financial PhraseBank sentiment classification
benchmark, LatentPrompt increased classification accuracy by approximately 3
percent after a single optimization cycle. The framework is broadly applicable,
requiring only black-box access to an LLM and an automatic evaluation metric,
making it suitable for diverse domains and tasks.

</details>


### [81] [Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity](https://arxiv.org/abs/2508.02498)
*Md Tasin Abir,Arpita Chowdhury,Ashfia Rahman*

Main category: cs.CL

TL;DR: 本研究探讨了Facebook在2024年孟加拉国的反民主起义中如何塑造集体身份，发现视觉和语言策略的结合有助于建立团结感和挑战权威叙述。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨Facebook如何在2024年7月孟加拉国的反民主起义（称为季风起义）中塑造集体身份。

Method: 本研究采用定性方法，分析了视觉修辞、言语话语和数字讽刺，以揭示共享符号、抗议艺术和口号如何建立团结感。

Result: 研究结果表明，Facebook上的视觉和语言策略的结合不仅激发了公众情绪，还建立了挑战权威叙述的强烈集体身份。

Conclusion: 本研究展示了在线平台如何在数字时代成为身份构建和政治动员的强大工具。

Abstract: This study investigates how Facebook shaped collective identity during the
July 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.
During government repression, protesters turned to Facebook as a central space
for resistance, where multimodal expressions, images, memes, videos, hashtags,
and satirical posts played an important role in unifying participants. Using a
qualitative approach, this research analyzes visual rhetoric, verbal discourse,
and digital irony to reveal how shared symbols, protest art, and slogans built
a sense of solidarity. Key elements included the symbolic use of red, the
ironic metaphorical use of the term "Razakar", and the widespread sharing of
visuals representing courage, injustice, and resistance. The findings show that
the combination of visual and verbal strategies on Facebook not only mobilized
public sentiment, but also built a strong collective identity that challenged
authoritarian narratives. This study tries to demonstrate how online platforms
can serve as powerful tools for identity construction and political
mobilization in the digital age.

</details>


### [82] [From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks](https://arxiv.org/abs/2508.02502)
*Shuzhou Yuan,Zhan Qu,Mario Tawfelis,Michael Färber*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型在不同语言身份下的心理语言学反应，发现语言身份影响模型的输出行为和内部表示。


<details>
  <summary>Details</summary>
Motivation: 我们想了解大型语言模型是否以及如何在不同的语言身份下表现出类似人类的心理语言学反应。

Method: 我们使用两个任务：声音象征性和词义值，评估了两种模型在英语、荷兰语和中文的单语和双语提示下的表现。

Result: 行为上，两种模型根据提示的语言身份调整其输出，Qwen表现出更高的敏感性和更清晰的区分度。探测分析显示，心理语言信号在更深的层中更容易解码，中文提示产生了比荷兰语更强且更稳定的词义表示。

Conclusion: 我们的结果表明，语言身份条件决定了LLM的输出行为和内部表示，为它们作为跨语言认知模型的应用提供了新的见解。

Abstract: Large Language Models (LLMs) exhibit strong linguistic capabilities, but
little is known about how they encode psycholinguistic knowledge across
languages. We investigate whether and how LLMs exhibit human-like
psycholinguistic responses under different linguistic identities using two
tasks: sound symbolism and word valence. We evaluate two models,
Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and
bilingual prompting in English, Dutch, and Chinese. Behaviorally, both models
adjust their outputs based on prompted language identity, with Qwen showing
greater sensitivity and sharper distinctions between Dutch and Chinese. Probing
analysis reveals that psycholinguistic signals become more decodable in deeper
layers, with Chinese prompts yielding stronger and more stable valence
representations than Dutch. Our results demonstrate that language identity
conditions both output behavior and internal representations in LLMs, providing
new insights into their application as models of cross-linguistic cognition.

</details>


### [83] [Modular Arithmetic: Language Models Solve Math Digit by Digit](https://arxiv.org/abs/2508.02513)
*Tanja Baeumel,Daniil Gurgurov,Yusser al Ghussin,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文研究了LLMs在解决简单算术任务时的内部机制，发现它们使用数字位置特定的电路，并通过特征重要性和因果干预验证了这一点。


<details>
  <summary>Details</summary>
Motivation: 本文动机是了解LLMs在简单算术任务中的内部策略，并揭示其底层机制。

Method: 本文使用特征重要性和因果干预来识别和验证数字位置特定的电路。

Result: 本文结果表明，LLMs中存在数字位置特定的电路，这些电路独立于模型大小和分词策略。

Conclusion: 本文结论是，LLMs中存在与数字位置相关的电路，这些电路在解决算术问题中起着因果作用，并且具有可解释的结构。

Abstract: While recent work has begun to uncover the internal strategies that Large
Language Models (LLMs) employ for simple arithmetic tasks, a unified
understanding of their underlying mechanisms is still lacking. We extend recent
findings showing that LLMs represent numbers in a digit-wise manner and present
evidence for the existence of digit-position-specific circuits that LLMs use to
perform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that
operate independently on different digit positions (units, tens, hundreds).
Notably, such circuits exist independently of model size and of tokenization
strategy, i.e. both for models that encode longer numbers digit-by-digit and as
one token. Using Feature Importance and Causal Interventions, we identify and
validate the digit-position-specific circuits, revealing a compositional and
interpretable structure underlying the solving of arithmetic problems in LLMs.
Our interventions selectively alter the model's prediction at targeted digit
positions, demonstrating the causal role of digit-position circuits in solving
arithmetic tasks.

</details>


### [84] [PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs](https://arxiv.org/abs/2508.02515)
*Zhan Qu,Shuzhou Yuan,Michael Färber*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在生成具有严格结构、音调和押韵约束的古典中文诗歌（宋词）方面的表现，并提出了一个 Generate-Critic 架构来提高其生成质量。


<details>
  <summary>Details</summary>
Motivation: 本文旨在系统地研究大型语言模型 (LLMs) 在生成宋词这一古典中文诗歌形式中的约束生成能力，该诗歌形式由词牌模板定义的严格结构、音调和押韵约束所特征化。

Method: 我们开发了一个全面的多方面评估框架，包括：(i) 形式符合性评分，(ii) 使用大型语言模型的自动质量评估，(iii) 人工评估，以及 (iv) 基于分类的探测任务。此外，我们提出了一种 Generate-Critic 架构，其中评估框架作为自动化评论家发挥作用，并利用评论家的反馈作为奖励信号，通过监督微调 (SFT) 对三个轻量级开源 LLM 进行微调。

Result: 我们评估了18个大型语言模型（包括3个专有模型和15个开源模型）在五种提示策略下的生成性能：零样本、少样本、基于补全、指令调优和思维链。通过使用评估框架，我们发现经过微调后的三个轻量级开源 LLM 在形式符合性方面提高了高达 5.88%。

Conclusion: 我们的研究提供了对大型语言模型在生成具有文化意义和形式约束的文学文本方面的生成优势和局限性的新见解。

Abstract: This paper presents a systematic investigation into the constrained
generation capabilities of large language models (LLMs) in producing Songci, a
classical Chinese poetry form characterized by strict structural, tonal, and
rhyme constraints defined by Cipai templates. We first develop a comprehensive,
multi-faceted evaluation framework that includes: (i) a formal conformity
score, (ii) automated quality assessment using LLMs, (iii) human evaluation,
and (iv) classification-based probing tasks. Using this framework, we evaluate
the generative performance of 18 LLMs, including 3 proprietary models and 15
open-source models across four families, under five prompting strategies:
zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.
Finally, we propose a Generate-Critic architecture in which the evaluation
framework functions as an automated critic. Leveraging the critic's feedback as
a reward signal, we fine-tune three lightweight open-source LLMs via supervised
fine-tuning (SFT), resulting in improvements of up to 5.88% in formal
conformity. Our findings offer new insights into the generative strengths and
limitations of LLMs in producing culturally significant and formally
constrained literary texts.

</details>


### [85] [I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2](https://arxiv.org/abs/2508.02527)
*Jack Merullo,Arjun Khurana,Oliver McLaughlin*

Main category: cs.CL

TL;DR: 研究发现Llama-3.2-1B-Instruct能够学习语音信息并构建类似人类IPA元音图的模型，即使没有直接监督。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型如何在没有显式语音或听觉基础的情况下处理语音任务，例如押韵。

Method: 研究了Llama-3.2-1B-Instruct如何表示逐个标记的语音信息，并通过可视化其隐空间中的语音表示来分析其内部模型。

Result: Llama-3.2-1B-Instruct 使用丰富的内部语音模型完成语音任务，并发现了促进语音信息的“语音移动器头”。

Conclusion: Llama-3.2-1B-Instruct 学习了一个类似于人类标准IPA元音图的元音模型，尽管没有直接监督。

Abstract: Large language models demonstrate proficiency on phonetic tasks, such as
rhyming, without explicit phonetic or auditory grounding. In this work, we
investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic
information. Our results suggest that Llama uses a rich internal model of
phonemes to complete phonetic tasks. We provide evidence for high-level
organization of phoneme representations in its latent space. In doing so, we
also identify a ``phoneme mover head" which promotes phonetic information
during rhyming tasks. We visualize the output space of this head and find that,
while notable differences exist, Llama learns a model of vowels similar to the
standard IPA vowel chart for humans, despite receiving no direct supervision to
do so.

</details>


### [86] [Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction](https://arxiv.org/abs/2508.02532)
*Karan Reddy,Mayukha Pal*

Main category: cs.CL

TL;DR: CGT is a hybrid neural architecture combining GNNs and Transformers, designed for domain-specific question answering in technical documents, showing improved performance and efficiency compared to generic models.


<details>
  <summary>Details</summary>
Motivation: Standard transformer-based language models often struggle with the fine-grained syntax and entity relationships in complex technical documents, requiring specialized models with stronger contextualization and structure awareness.

Method: CGT combines Graph Neural Networks (GNNs) and Transformers by constructing a dynamic graph over input tokens using sequential, skip-gram, and semantic similarity edges, which is processed by GATv2Conv layers for local structure learning, followed by a Transformer encoder to capture global dependencies.

Result: CGT outperforms baselines like GPT-2 and BERT, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters, demonstrating its effectiveness in modeling structural token interactions and long-range semantic coherence.

Conclusion: CGT offers a parameter-efficient solution for domain-specific question answering in technical documents, demonstrating superior performance compared to generic models like GPT-2 and BERT.

Abstract: Standard transformer-based language models, while powerful for general text,
often struggle with the fine-grained syntax and entity relationships in complex
technical, engineering documents. To address this, we propose the Contextual
Graph Transformer (CGT), a hybrid neural architecture that combines Graph
Neural Networks (GNNs) and Transformers for domain-specific question answering.
CGT constructs a dynamic graph over input tokens using sequential, skip-gram,
and semantic similarity edges, which is processed by GATv2Conv layers for local
structure learning. These enriched embeddings are then passed to a Transformer
encoder to capture global dependencies. Unlike generic large models, technical
domains often require specialized language models with stronger
contextualization and structure awareness. CGT offers a parameter-efficient
solution for such use cases. Integrated into a Retrieval-Augmented Generation
(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%
higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from
CGTs ability to jointly model structural token interactions and long-range
semantic coherence. The model is trained from scratch using a two-phase
approach: pretraining on general text followed by fine-tuning on
domain-specific manuals. This highlights CGTs adaptability to technical
language, enabling better grounding, entity tracking, and retrieval-augmented
responses in real-world applications.

</details>


### [87] [What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)](https://arxiv.org/abs/2508.02540)
*Anastasia Zhukova,Terry Ruas,Felix Hamborg,Karsten Donnay,Bela Gipp*

Main category: cs.CL

TL;DR: 本文提出了一种自动识别新闻中偏见的方法，将偏见的三种类型（遗漏、选择和来源）作为联合目标进行处理，并提供了一个可视化示例。


<details>
  <summary>Details</summary>
Motivation: 在信息过载的世界中，确定哪些信息来自可靠来源或新闻报道中的信息中立性对新闻读者来说是一个挑战。

Method: 本文提出了一种方法，将偏见的三种类型（遗漏、选择和来源）作为联合目标进行处理，并描述了其步骤的目标和任务。

Result: 本文提供了一个可视化示例，利用提取的特征和文本重复模式。

Conclusion: 本文提出了一个自动识别偏见的方法，将偏见的三种类型（遗漏、选择和来源）作为联合目标进行处理，而不是像以前的工作那样分别处理这些类型的偏见。

Abstract: In a world overwhelmed with news, determining which information comes from
reliable sources or how neutral is the reported information in the news
articles poses a challenge to news readers. In this paper, we propose a
methodology for automatically identifying bias by commission, omission, and
source selection (COSS) as a joint three-fold objective, as opposed to the
previous work separately addressing these types of bias. In a pipeline concept,
we describe the goals and tasks of its steps toward bias identification and
provide an example of a visualization that leverages the extracted features and
patterns of text reuse.

</details>


### [88] [Building and Aligning Comparable Corpora](https://arxiv.org/abs/2508.02555)
*Motaz Saad,David Langlois,Kamel Smaili*

Main category: cs.CL

TL;DR: 本文提出了一种从维基百科和EURONEWS网站构建可比较语料库的方法，并实验了使用跨语言相似性度量自动对齐可比较文档的方法。研究了两种跨语言相似性度量，结果表明基于跨语言LSI（CL-LSI）的度量优于基于双语词典的度量。最后，我们从英国广播公司（BBC）和ALJAZEERA（JSC）新闻网站分别收集了英语和阿拉伯语新闻文档，并使用CL-LSI相似性度量自动对齐了BBC和JSC的可比较文档。评估结果显示，CL-LSI不仅能够在主题层面对齐跨语言文档，还能够在事件层面进行对齐。


<details>
  <summary>Details</summary>
Motivation: Comparable corpus is a set of topic aligned documents in multiple languages, which are not necessarily translations of each other. These documents are useful for multilingual natural language processing when there is no parallel text available in some domains or languages. In addition, comparable documents are informative because they can tell what is being said about a topic in different languages.

Method: We present a method to build comparable corpora from Wikipedia encyclopedia and EURONEWS website in English, French and Arabic languages. We further experiment a method to automatically align comparable documents using cross-lingual similarity measures. We investigate two cross-lingual similarity measures to align comparable documents.

Result: Experiments on several corpora show that the Cross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure. Finally, we collect English and Arabic news documents from the British Broadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively. Then we use the CL-LSI similarity measure to automatically align comparable documents of BBC and JSC. The evaluation of the alignment shows that CL-LSI is not only able to align cross-lingual documents at the topic level, but also it is able to do this at the event level.

Conclusion: CL-LSI is not only able to align cross-lingual documents at the topic level, but also it is able to do this at the event level.

Abstract: Comparable corpus is a set of topic aligned documents in multiple languages,
which are not necessarily translations of each other. These documents are
useful for multilingual natural language processing when there is no parallel
text available in some domains or languages. In addition, comparable documents
are informative because they can tell what is being said about a topic in
different languages. In this paper, we present a method to build comparable
corpora from Wikipedia encyclopedia and EURONEWS website in English, French and
Arabic languages. We further experiment a method to automatically align
comparable documents using cross-lingual similarity measures. We investigate
two cross-lingual similarity measures to align comparable documents. The first
measure is based on bilingual dictionary, and the second measure is based on
Latent Semantic Indexing (LSI). Experiments on several corpora show that the
Cross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.
Finally, we collect English and Arabic news documents from the British
Broadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.
Then we use the CL-LSI similarity measure to automatically align comparable
documents of BBC and JSC. The evaluation of the alignment shows that CL-LSI is
not only able to align cross-lingual documents at the topic level, but also it
is able to do this at the event level.

</details>


### [89] [Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks](https://arxiv.org/abs/2508.02556)
*Ali Noori,Pratik Devkota,Somya Mohanty,Prashanti Manda*

Main category: cs.CL

TL;DR: 本研究提出了一种基于双向GRU模型的SNOMED CT概念识别方法，利用MIMIC-IV数据集进行训练和验证，取得了较高的F1分数，证明了轻量级RNN架构在临床概念标注中的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化标注临床文本以获得标准化医疗概念对于实现结构化数据提取和决策支持至关重要。然而，手动标注既耗时又不适用于大规模应用。

Method: 该研究引入了一种基于双向GRU模型的神经序列标记方法，用于SNOMED CT概念识别。利用MIMIC-IV的一个子集，对文本进行领域适应的SpaCy和SciBERT-based分词，并将句子分割为包含上下文、句法和形态特征的重叠19个标记块。

Result: Bi-GRU模型通过IOB标签分配来识别概念范围，在验证集上实现了90%的F1分数，优于传统规则系统，并与现有的神经模型相当或更好。定性分析显示了对模糊术语和拼写错误的有效处理。

Conclusion: 研究结果表明，轻量级RNN架构可以在显著降低计算成本的情况下提供高质量的临床概念注释，使其非常适合实际部署。

Abstract: Automated annotation of clinical text with standardized medical concepts is
critical for enabling structured data extraction and decision support. SNOMED
CT provides a rich ontology for labeling clinical entities, but manual
annotation is labor-intensive and impractical at scale. This study introduces a
neural sequence labeling approach for SNOMED CT concept recognition using a
Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text
with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences
into overlapping 19-token chunks enriched with contextual, syntactic, and
morphological features. The Bi-GRU model assigns IOB tags to identify concept
spans and achieves strong performance with a 90 percent F1-score on the
validation set. These results surpass traditional rule-based systems and match
or exceed existing neural models. Qualitative analysis shows effective handling
of ambiguous terms and misspellings. Our findings highlight that lightweight
RNN-based architectures can deliver high-quality clinical concept annotation
with significantly lower computational cost than transformer-based models,
making them well-suited for real-world deployment.

</details>


### [90] [Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction](https://arxiv.org/abs/2508.02558)
*Yuerong Song,Xiaoran Liu,Ruixiao Li,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: Sparse-dLLM is a training-free framework that improves the efficiency of diffusion large language models by integrating dynamic cache eviction with sparse attention, achieving higher throughput without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Current caching techniques impose substantial memory usage that limits long-context applications. The analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, motivating selective cache eviction.

Method: Sparse-dLLM is a training-free framework that integrates dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. It retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy.

Result: Extensive experiments on LLaDA and Dream series demonstrate that Sparse-dLLM achieves up to 10× higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs.

Conclusion: Sparse-dLLM achieves up to 10× higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.

Abstract: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and
parallel decoding but suffer from prohibitive quadratic computational
complexity and memory overhead during inference. Current caching techniques
accelerate decoding by storing full-layer states, yet impose substantial memory
usage that limit long-context applications. Our analysis of attention patterns
in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining
salient across decoding steps and low-relevance tokens staying unimportant,
motivating selective cache eviction. We propose Sparse-dLLM, the first
training-free framework integrating dynamic cache eviction with sparse
attention via delayed bidirectional sparse caching. By leveraging the stability
of token saliency over steps, it retains critical tokens and dynamically evicts
unimportant prefix/suffix entries using an attention-guided strategy. Extensive
experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to
10$\times$ higher throughput than vanilla dLLMs, with comparable performance
and similar peak memory costs, outperforming previous methods in efficiency and
effectiveness.

</details>


### [91] [Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs](https://arxiv.org/abs/2508.02573)
*Jérémie Dentan,Davide Buscaldi,Sonia Vanier*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中的记忆现象，发现现有分类法无法准确反映注意力机制，并提出了一种新的分类法。


<details>
  <summary>Details</summary>
Motivation: 现有的分类法无法准确反映注意力块中的不同机制，因此需要一种新的分类法来更好地理解大型语言模型中的记忆现象。

Method: 通过在大型语言模型的注意力权重上训练卷积神经网络，评估现有分类法与解码过程中涉及的注意力权重之间的对齐度。

Result: 发现现有分类法表现不佳，无法反映注意力块中的不同机制。提出了一个与注意力权重对齐的新分类法，包含三类：通过语言建模能力猜测的记忆样本、由于训练集中高重复性而回忆的记忆样本以及非记忆样本。

Conclusion: 本文提出了一种新的分类法，以更好地反映注意力权重中的不同记忆机制，并展示了模型在少样本情况下几乎不对应于特定的注意力机制。

Abstract: Verbatim memorization in Large Language Models (LLMs) is a multifaceted
phenomenon involving distinct underlying mechanisms. We introduce a novel
method to analyze the different forms of memorization described by the existing
taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the
attention weights of the LLM and evaluate the alignment between this taxonomy
and the attention weights involved in decoding.
  We find that the existing taxonomy performs poorly and fails to reflect
distinct mechanisms within the attention blocks. We propose a new taxonomy that
maximizes alignment with the attention weights, consisting of three categories:
memorized samples that are guessed using language modeling abilities, memorized
samples that are recalled due to high duplication in the training set, and
non-memorized samples. Our results reveal that few-shot verbatim memorization
does not correspond to a distinct attention mechanism. We also show that a
significant proportion of extractable samples are in fact guessed by the model
and should therefore be studied separately. Finally, we develop a custom visual
interpretability technique to localize the regions of the attention weights
involved in each form of memorization.

</details>


### [92] [EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare](https://arxiv.org/abs/2508.02574)
*Eman Alamoudi,Ellis Solaiman*

Main category: cs.CL

TL;DR: 本文介绍了EHSAN，这是一种数据导向的混合管道，结合ChatGPT伪标签和人工审查，创建了一个可解释的阿拉伯语医疗保健情感数据集。实验结果显示，即使在最小人工监督下，模型表现依然良好，这表明一种有效且可扩展的方法用于阿拉伯语基于方面的情感分析。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语患者反馈尚未得到充分分析，因为方言多样性以及缺乏方面级情感标签阻碍了自动化评估。

Method: 我们引入了EHSAN，这是一个数据导向的混合管道，将ChatGPT伪标签与有针对性的人类审查相结合，以构建第一个可解释的阿拉伯语基于方面的医疗保健情感数据集。每个句子都带有方面和情感标签（积极、消极或中性），形成一个与医疗保健主题对齐的开创性阿拉伯数据集，并为每个标签提供ChatGPT生成的推理以增强透明度。

Result: 实验结果表明，即使在最小的人工监督下，我们的阿拉伯语特定模型也达到了高准确率，仅使用ChatGPT生成的标签时性能下降很小。减少方面类别数量显著提高了所有分类指标。

Conclusion: 这些发现表明了一种有效且可扩展的方法，用于医疗保健中的阿拉伯语基于方面的情感分析（SA），结合大型语言模型标注和人类专业知识以生成强大且可解释的数据集。未来的研究方向包括跨医院的泛化、提示优化和可解释的数据驱动建模。

Abstract: Arabic-language patient feedback remains under-analysed because dialect
diversity and scarce aspect-level sentiment labels hinder automated assessment.
To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that
merges ChatGPT pseudo-labelling with targeted human review to build the first
explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence
is annotated with an aspect and sentiment label (positive, negative, or
neutral), forming a pioneering Arabic dataset aligned with healthcare themes,
with ChatGPT-generated rationales provided for each label to enhance
transparency. To evaluate the impact of annotation quality on model
performance, we created three versions of the training data: a fully supervised
set with all labels reviewed by humans, a semi-supervised set with 50% human
review, and an unsupervised set with only machine-generated labels. We
fine-tuned two transformer models on these datasets for both aspect and
sentiment classification. Experimental results show that our Arabic-specific
model achieved high accuracy even with minimal human supervision, reflecting
only a minor performance drop when using ChatGPT-only labels. Reducing the
number of aspect classes notably improved classification metrics across the
board. These findings demonstrate an effective, scalable approach to Arabic
aspect-based sentiment analysis (SA) in healthcare, combining large language
model annotation with human expertise to produce a robust and explainable
dataset. Future directions include generalisation across hospitals, prompt
refinement, and interpretable data-driven modelling.

</details>


### [93] [MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification](https://arxiv.org/abs/2508.02584)
*Ming Pok Ng,Junqi Jiang,Gabriel Freedman,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: MArgE是一种新的框架，通过形式化结构提高多个LLM输出的可信度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前结合多个LLM的方法通常涉及非结构化的交互，导致模型生成的结果难以合理解释。

Method: MArgE是一种新的框架，通过提取每个LLM的证据并形成论证树来提供形式结构，用于声明验证任务。

Result: 实验表明，MArgE可以显著优于单个LLM、开源模型、GPT-4o-mini和现有的ArgLLMs以及之前的非结构化多LLM辩论方法。

Conclusion: MArgE展示了在结合多个LLM输出时引入正式的论证推理机制的优势。

Abstract: Leveraging outputs from multiple large language models (LLMs) is emerging as
a method for harnessing their power across a wide range of tasks while
mitigating their capacity for making errors, e.g., hallucinations. However,
current approaches to combining insights from multiple LLMs often involve
unstructured interactions (e.g., free debate), resulting in model generations
that are not faithfully justifiable. In this work, we introduce MArgE, a novel
framework to provide formal structure to the evidence from each LLM, in the
form of a tree of extracted arguments, for the task of claim verification. We
use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks
and semantics from the field of computational argumentation, to construct
structured argument trees for given claims. This process creates an inspectable
pathway from the initial arguments to the final claim verification decisions,
providing a faithful justification thereof. We show experimentally that MArgE
can significantly outperform single LLMs, including three open-source models
(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior
methods for unstructured multi-LLM debates. We thus demonstrate the advantages
of incorporating formal, argumentative reasoning mechanisms when combining
multiple LLM outputs.

</details>


### [94] [CharBench: Evaluating the Role of Tokenization in Character-Level Tasks](https://arxiv.org/abs/2508.02591)
*Omri Uzan,Yuval Pinter*

Main category: cs.CL

TL;DR: 本文介绍了CharBench，一个大规模的字符级任务基准测试，用于评估语言模型在字符级推理任务上的表现，并发现分词对模型性能的影响有限，而单词长度和实际字符计数更为重要。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在需要字符级推理的任务上表现不佳，而关于分词对模型性能的影响存在争议，因此需要一个全面的基准测试来解决这一问题。

Method: 本文引入了CharBench，一个大规模的字符级任务基准测试，并评估了多种领先的开源和专有模型的表现。

Result: 在CharBench上，现代语言模型的平均准确率分别为43.6%和32.3%，表明该基准测试对模型构成了重大挑战。

Conclusion: 本文提出了CharBench基准测试，用于评估语言模型在字符级任务上的表现，并鼓励未来的研究基于此基准和评估方法来改进模型性能。

Abstract: Tasks that require character-level reasoning, such as counting or locating
characters within words, remain challenging for contemporary language models. A
common conjecture is that language models' reliance on subword units, rather
than characters, contributes to their struggles with character-level tasks, yet
recent studies offer conflicting conclusions about the role of tokenization,
leaving its impact unclear. To address this gap, we introduce CharBench, a
comprehensive benchmark of character-level tasks that is two orders of
magnitude larger than existing alternatives. We evaluate a diverse range of
leading open-weight and proprietary models on CharBench and find that it
presents a significant challenge to modern LLMs, with an average accuracy of
43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic
properties of words and their segmentations into tokens correspond to model
performance. For counting tasks, we find that tokenization properties are
weakly correlated with correctness, while the length of the queried word and
the actual character count play a more significant part. In contrast, for tasks
requiring intra-word positional understanding, performance is negatively
correlated with the length of the token containing the queried character,
suggesting that longer tokens obscure character position information for LLMs.
We encourage future work to build on the benchmark and evaluation methodology
introduced here as tools for improving model performance on such tasks.

</details>


### [95] [Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation](https://arxiv.org/abs/2508.02618)
*Jianxiang Zang,Meiling Ning,Shihan Dou,Jiazheng Zhang,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练框架'Interaction Distillation'，以解决奖励模型中的'attention hacking'问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的偏好建模在token级别的交互上存在不足，导致其判断信号容易受到上下文分配注意力不当的影响。这是由于两个基本限制：(1) 当前的偏好建模使用解码器-only架构，导致提示-响应序列内的前向衰减内部序列注意力；(2) 独立的Siamese编码范式导致选择和拒绝序列之间的token级别交叉序列注意力缺失。

Method: 本文提出了'Interaction Distillation'，引入了一个基于自然语言理解的教师模型，通过全面的注意力提供复杂的token交互模式，并通过注意力对齐目标指导偏好建模模拟教师模型的交互模式。

Result: 通过广泛的实验，交互蒸馏显示出比针对数据噪声的最先进RM优化方法更能提供稳定和泛化的奖励信号，突显了注意力黑客构成了RM中更基本的限制。

Conclusion: 本文提出了一种新的训练框架'Interaction Distillation'，通过注意力级别的优化来解决奖励模型中的'attention hacking'问题，实验表明该方法能够提供更稳定和泛化的奖励信号。

Abstract: The reward model (RM), as the core component of reinforcement learning from
human feedback (RLHF) for large language models (LLMs), responsible for
providing reward signals to generated responses. However, mainstream preference
modeling in RM is inadequate in terms of token-level interaction, making its
judgment signals vulnerable to being hacked by misallocated attention to
context. This stems from two fundamental limitations: (1) Current preference
modeling employs decoder-only architectures, where the unidirectional causal
attention mechanism leads to forward-decaying intra-sequence attention within
the prompt-response sequence. (2) The independent Siamese-encoding paradigm
induces the absence of token-level inter-sequence attention between chosen and
rejected sequences. To address this "attention hacking", we propose
"Interaction Distillation", a novel training framework for more adequate
preference modeling through attention-level optimization. The method introduces
an interaction-based natural language understanding model as the teacher to
provide sophisticated token interaction patterns via comprehensive attention,
and guides the preference modeling to simulate teacher model's interaction
pattern through an attentional alignment objective. Through extensive
experiments, interaction distillation has demonstrated its ability to provide
more stable and generalizable reward signals compared to state-of-the-art RM
optimization methods that target data noise, highlighting the attention hacking
constitute a more fundamental limitation in RM.

</details>


### [96] [Pointer: Linear-Complexity Long-Range Modeling without Pre-training](https://arxiv.org/abs/2508.02631)
*Zixi Li*

Main category: cs.CL

TL;DR: Pointer 是一种新型架构，通过逐层指针链实现线性复杂度，提高长距离序列建模效率并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 为了在不需要预训练的情况下实现高效的长距离序列建模，同时保持优越的性能。

Method: Pointer 是一种新颖的架构，通过逐层指针链实现线性 O(NK) 复杂度，而不是标准注意力机制的 O(N^2) 成对交互。

Result: Pointer 在长序列上实现了 2-10 倍的速度提升，在距离高达 2048 个标记的复制任务中保持 >95% 的准确率，并学习到了可解释的指针模式。

Conclusion: Pointer 提供了一种有吸引力的替代注意力机制的方法，适用于需要高效长距离建模且无需预训练依赖的场景。

Abstract: We introduce Pointer, a novel architecture that achieves linear $O(NK)$
complexity for long-range sequence modeling while maintaining superior
performance without requiring pre-training. Unlike standard attention
mechanisms that compute $O(N^2)$ pairwise interactions, our approach uses
layer-wise pointer chaining where each layer's pointer selection depends on
previous layer's pointer positions, creating explicit long-distance connections
through pointer chains. We demonstrate that this architecture achieves
$2$--$10\times$ speedup on long sequences compared to standard transformers,
maintains $>95\%$ accuracy on copy tasks at distances up to 2048 tokens, and
learns interpretable pointer patterns that reveal structured dependency
modeling. Our experiments on efficiency benchmarks, long-range dependency
tasks, and interpretability analysis show that Pointer offers a compelling
alternative to attention mechanisms for scenarios requiring efficient
long-range modeling without pre-training dependencies.

</details>


### [97] [Test Set Quality in Multilingual LLM Evaluation](https://arxiv.org/abs/2508.02635)
*Kranti Chalamalasetti,Gabriel Bernier-Colborne,Yvan Gauthier,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 本文手动分析了法语和泰卢固语的多语言评估集，发现了一些错误，并比较了不同LLM在原始和修订数据集上的性能差异，结果表明测试集应被重新审视和检查。


<details>
  <summary>Details</summary>
Motivation: 尽管存在之前的工作来识别甚至完全人工标注的测试集中的错误，但很少关注数据集本身的质量。

Method: 手动分析最近的多语言评估集，识别其中的错误，并比较几个LLM在原始和修订后的数据集上的性能差异。

Result: 在法语和泰卢固语中，原始和修订后的数据集在多个LLM上的性能差异很大（某些情况下几乎达到10%）。

Conclusion: 测试集不应被视为不可变的，应重新审视、检查正确性，并可能进行版本控制。我们最后对数据集创建者和使用者提出了一些建议，以解决数据集质量问题。

Abstract: Several multilingual benchmark datasets have been developed in a
semi-automatic manner in the recent past to measure progress and understand the
state-of-the-art in the multilingual capabilities of Large Language Models.
However, there is not a lot of attention paid to the quality of the datasets
themselves, despite the existence of previous work in identifying errors in
even fully human-annotated test sets. In this paper, we manually analyze recent
multilingual evaluation sets in two languages - French and Telugu, identifying
several errors in the process. We compare the performance difference across
several LLMs with the original and revised versions of the datasets and
identify large differences (almost 10% in some cases) in both languages). Based
on these results, we argue that test sets should not be considered immutable
and should be revisited, checked for correctness, and potentially versioned. We
end with some recommendations for both the dataset creators as well as
consumers on addressing the dataset quality issues.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [98] [Cyber-Zero: Training Cybersecurity Agents without Runtime](https://arxiv.org/abs/2508.00910)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.CR

TL;DR: 本文介绍了一种无需运行时环境的框架Cyber-Zero，用于合成高质量的代理轨迹以训练网络安全LLM，并展示了其在性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 由于在其他领域（尤其是网络安全）中运行时环境通常不可用，因此需要一种无需运行时环境的方法来训练网络安全LLM。

Method: Cyber-Zero框架利用公开的CTF writeups和基于角色的LLM模拟来逆向工程运行时行为并生成现实的、长视野的交互序列。

Result: 使用Cyber-Zero生成的轨迹，我们训练的LLM代理在三个著名的CTF基准测试中相对于基线模型取得了高达13.1%的绝对性能提升。

Conclusion: 我们的研究表明，无需运行时环境的轨迹合成可以有效地民主化最先进的网络安全代理的开发。

Abstract: Large Language Models (LLMs) have achieved remarkable success in software
engineering tasks when trained with executable runtime environments,
particularly in resolving GitHub issues. However, such runtime environments are
often unavailable in other domains, especially cybersecurity, where challenge
configurations and execution contexts are ephemeral or restricted. We present
Cyber-Zero, the first runtime-free framework for synthesizing high-quality
agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly
available CTF writeups and employs persona-driven LLM simulation to
reverse-engineer runtime behaviors and generate realistic, long-horizon
interaction sequences without actual environments. Using trajectories
synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%
absolute performance gains over baseline models on three prominent CTF
benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,
Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight
models, matching the capabilities of proprietary systems like DeepSeek-V3-0324
and Claude-3.5-Sonnet while offering superior cost-effectiveness, and
demonstrating that runtime-free trajectory synthesis can effectively
democratize the development of state-of-the-art cybersecurity agents.

</details>


### [99] [AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection](https://arxiv.org/abs/2508.01249)
*Peiran Wang,Yang Liu,Yunfei Lu,Yifeng Cai,Hongbo Chen,Qingyou Yang,Jie Zhang,Jue Hong,Ye Wu*

Main category: cs.CR

TL;DR: AgentArmor是一种程序分析框架，通过将代理行为表示为结构化程序，以检测提示注入漏洞并强制执行细粒度的安全约束。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在存在提示注入攻击时引入了关键的安全风险，因此需要一种新的方法来检测和防范这些风险。

Method: AgentArmor将代理运行时轨迹转换为基于图的中间表示，并通过类型系统实施安全策略。

Result: 在AgentDojo基准测试中，AgentArmor实现了95.75%的TPR和3.66%的FPR。

Conclusion: AgentArmor能够检测提示注入漏洞并强制执行细粒度的安全约束。

Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving
various problems by combining natural language reasoning with the execution of
external tools. However, their dynamic and non-transparent behavior introduces
critical security risks, particularly in the presence of prompt injection
attacks. In this work, we propose a novel insight that treats the agent runtime
traces as structured programs with analyzable semantics. Thus, we present
AgentArmor, a program analysis framework that converts agent traces into graph
intermediate representation-based structured program dependency representations
(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.
AgentArmor consists of three key components: (1) a graph constructor that
reconstructs the agent's working traces as graph-based intermediate
representations with control flow and data flow described within; (2) a
property registry that attaches security-relevant metadata of interacted tools
& data, and (3) a type system that performs static inference and checking over
the intermediate representation. By representing agent behavior as structured
programs, AgentArmor enables program analysis over sensitive data flow, trust
boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo
benchmark, the results show that AgentArmor can achieve 95.75% of TPR, with
only 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect
prompt injection vulnerabilities and enforce fine-grained security constraints.

</details>


### [100] [ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models](https://arxiv.org/abs/2508.01365)
*Zihan Wang,Rui Zhang,Hongwei Li,Wenshu Fan,Wenbo Jiang,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TL;DR: This paper introduces ConfGuard, a lightweight and effective method for detecting backdoor attacks in LLMs by identifying sequence lock, achieving high accuracy with minimal latency.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods are ineffective against the autoregressive nature and vast output space of LLMs, leading to poor performance and high latency.

Method: ConfGuard monitors a sliding window of token confidences to identify sequence lock, which is a phenomenon where backdoored models generate target sequences with abnormally high and consistent confidence.

Result: ConfGuard achieves a near 100% true positive rate (TPR) and a negligible false positive rate (FPR) in most cases, with almost no additional latency.

Conclusion: ConfGuard provides an effective and efficient solution for detecting backdoor attacks in LLMs, enabling real-time detection with minimal additional latency.

Abstract: Backdoor attacks pose a significant threat to Large Language Models (LLMs),
where adversaries can embed hidden triggers to manipulate LLM's outputs. Most
existing defense methods, primarily designed for classification tasks, are
ineffective against the autoregressive nature and vast output space of LLMs,
thereby suffering from poor performance and high latency. To address these
limitations, we investigate the behavioral discrepancies between benign and
backdoored LLMs in output space. We identify a critical phenomenon which we
term sequence lock: a backdoored model generates the target sequence with
abnormally high and consistent confidence compared to benign generation.
Building on this insight, we propose ConfGuard, a lightweight and effective
detection method that monitors a sliding window of token confidences to
identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a
near 100\% true positive rate (TPR) and a negligible false positive rate (FPR)
in the vast majority of cases. Crucially, the ConfGuard enables real-time
detection almost without additional latency, making it a practical backdoor
defense for real-world LLM deployments.

</details>


### [101] [DUP: Detection-guided Unlearning for Backdoor Purification in Language Models](https://arxiv.org/abs/2508.01647)
*Man Hu,Yahui Ding,Yatao Yang,Liangyu Chen,Yanhao Jia,Shuai Zhao*

Main category: cs.CR

TL;DR: 本文提出了一种名为DUP的统一框架，用于检测和净化后门攻击，通过结合特征级异常检测和参数高效的去学习机制，实现了优越的防御性能。


<details>
  <summary>Details</summary>
Motivation: 当前的防御策略在检测方法上依赖于粗粒度的特征统计，在净化方法上通常需要完整的重新训练或额外的干净模型，这暴露了关键的弱点。

Method: DUP框架结合了后门检测和基于去学习的净化，利用特征级异常检测和参数高效的去学习机制，避免了完整的重新训练和外部干净模型的需求。

Result: DUP在多种攻击方法和语言模型架构上的广泛实验表明，它在检测准确性和净化效果方面表现优异。

Conclusion: DUP在检测准确性和净化效果方面表现出优越的防御性能，展示了其在对抗后门攻击中的有效性。

Abstract: As backdoor attacks become more stealthy and robust, they reveal critical
weaknesses in current defense strategies: detection methods often rely on
coarse-grained feature statistics, and purification methods typically require
full retraining or additional clean models. To address these challenges, we
propose DUP (Detection-guided Unlearning for Purification), a unified framework
that integrates backdoor detection with unlearning-based purification. The
detector captures feature-level anomalies by jointly leveraging class-agnostic
distances and inter-layer transitions. These deviations are integrated through
a weighted scheme to identify poisoned inputs, enabling more fine-grained
analysis. Based on the detection results, we purify the model through a
parameter-efficient unlearning mechanism that avoids full retraining and does
not require any external clean model. Specifically, we innovatively repurpose
knowledge distillation to guide the student model toward increasing its output
divergence from the teacher on detected poisoned samples, effectively forcing
it to unlearn the backdoor behavior. Extensive experiments across diverse
attack methods and language model architectures demonstrate that DUP achieves
superior defense performance in detection accuracy and purification efficacy.
Our code is available at https://github.com/ManHu2025/DUP.

</details>


### [102] [Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection](https://arxiv.org/abs/2508.01887)
*Aldan Creo*

Main category: cs.CR

TL;DR: 本文提出了一种名为PDFuzz的新攻击方法，能够成功逃避AI生成文本检测器的检测，展示了当前检测系统的脆弱性，并强调了加强防护的重要性。


<details>
  <summary>Details</summary>
Motivation: AI生成的文本检测器已成为维护内容真实性的关键工具，但它们在应对逃避攻击方面的鲁棒性仍值得怀疑。

Method: 我们提出了PDFuzz，这是一种新颖的攻击方法，利用了PDF文档中视觉文本布局和提取顺序之间的差异。我们的方法在保持精确文本内容的同时，操纵字符位置以打乱提取序列。

Result: 我们的结果表明完全逃避：检测器性能从（93.6 ± 1.4）% 的准确率和 0.938 ± 0.014 的 F1 得分下降到随机水平的性能（50.4 ± 3.2）% 的准确率和 0.0 的 F1 得分，同时保持完美的视觉保真度。

Conclusion: 我们的工作揭示了当前检测系统中存在的漏洞，这种漏洞是PDF文档结构固有的，并强调了实施稳健防护措施的必要性。

Abstract: AI-generated text detectors have become essential tools for maintaining
content authenticity, yet their robustness against evasion attacks remains
questionable. We present PDFuzz, a novel attack that exploits the discrepancy
between visual text layout and extraction order in PDF documents. Our method
preserves exact textual content while manipulating character positioning to
scramble extraction sequences. We evaluate this approach against the ArguGPT
detector using a dataset of human and AI-generated text. Our results
demonstrate complete evasion: detector performance drops from (93.6 $\pm$ 1.4)
% accuracy and 0.938 $\pm$ 0.014 F1 score to random-level performance ((50.4
$\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.
Our work reveals a vulnerability in current detection systems that is inherent
to PDF document structures and underscores the need for implementing sturdy
safeguards against such attacks. We make our code publicly available at
https://github.com/ACMCMC/PDFuzz.

</details>


### [103] [A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology](https://arxiv.org/abs/2508.01913)
*Kamal Al-Sabahi,Yousuf Khamis Al Mabsali*

Main category: cs.CR

TL;DR: 本文提出了一种基于区块链和去中心化身份的框架，以解决学术出版中的不道德行为，提高透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 学术出版面临诸如未经同意的作者身份、赠与作者身份、作者模糊性和未披露的利益冲突等不道德行为的威胁。现有的基础设施如ORCID在明确作者同意、准确验证贡献者角色和稳健检测利益冲突方面存在不足。

Method: 本文引入了去中心化身份（SSI）和区块链技术，利用去中心化标识符（DIDs）和可验证凭证（VCs）来安全地验证作者身份和贡献，并使用区块链上的信任注册表来记录作者同意和同行评审活动。此外，还采用了隐私保护的密码技术，如零知识证明（ZKPs），以检测利益冲突而不泄露敏感数据。

Result: 通过利益相关者调查发现，该框架提高了伦理合规性和对学术交流的信心。

Conclusion: 本文提出了一种基于去中心化身份和区块链技术的框架，旨在解决学术出版中的不道德行为，提高学术交流的透明度和可信度。

Abstract: Academic publishing, integral to knowledge dissemination and scientific
advancement, increasingly faces threats from unethical practices such as
unconsented authorship, gift authorship, author ambiguity, and undisclosed
conflicts of interest. While existing infrastructures like ORCID effectively
disambiguate researcher identities, they fall short in enforcing explicit
authorship consent, accurately verifying contributor roles, and robustly
detecting conflicts of interest during peer review. To address these
shortcomings, this paper introduces a decentralized framework leveraging
Self-Sovereign Identity (SSI) and blockchain technology. The proposed model
uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to
securely verify author identities and contributions, reducing ambiguity and
ensuring accurate attribution. A blockchain-based trust registry records
authorship consent and peer-review activity immutably. Privacy-preserving
cryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support
conflict-of-interest detection without revealing sensitive data. Verified
authorship metadata and consent records are embedded in publications,
increasing transparency. A stakeholder survey of researchers, editors, and
reviewers suggests the framework improves ethical compliance and confidence in
scholarly communication. This work represents a step toward a more transparent,
accountable, and trustworthy academic publishing ecosystem.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [104] [DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs](https://arxiv.org/abs/2508.01136)
*Wei Zhou,Peng Sun,Xuanhe Zhou,Qianglei Zang,Ji Xu,Tieying Zhang,Guoliang Li,Fan Wu*

Main category: cs.DB

TL;DR: 本文提出了一种新的混合数据库运维系统DBAIOps，结合了推理大语言模型和知识图谱，以实现DBA风格的诊断。通过引入异构图模型、可重复使用的异常模型和两阶段图演化机制，DBAIOps在四个主流数据库系统上表现出色，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动数据库运维方法无法有效利用专家经验，规则方法仅支持基本的运维任务，而基于大语言模型的方法生成的结果往往不准确或过于通用。因此，需要一种能够结合推理大语言模型和知识图谱的新方法来实现DBA风格的诊断。

Method: DBAIOps结合了推理大语言模型和知识图谱，引入了异构图模型来表示诊断经验，并提出了半自动图构建算法从数千份文档中构建该图。此外，还开发了一组（800多个）可重复使用的异常模型，以及针对每个异常的两阶段图演化机制来探索相关的诊断路径并自动识别缺失的关系。最后，利用推理大语言模型（例如DeepSeek-R1）推断根本原因并生成清晰的诊断报告。

Result: DBAIOps在四个主流数据库系统（Oracle、MySQL、PostgreSQL和DM8）上的评估结果表明，它优于最先进的基线，根因准确率和人工评估准确率分别提高了34.85%和47.22%。

Conclusion: DBAIOps在四个主流数据库系统上的评估结果表明，它优于最先进的基线，根因准确率和人工评估准确率分别提高了34.85%和47.22%。

Abstract: The operation and maintenance (O&M) of database systems is critical to
ensuring system availability and performance, typically requiring expert
experience (e.g., identifying metric-to-anomaly relations) for effective
diagnosis and recovery. However, existing automatic database O&M methods,
including commercial products, cannot effectively utilize expert experience. On
the one hand, rule-based methods only support basic O&M tasks (e.g.,
metric-based anomaly detection), which are mostly numerical equations and
cannot effectively incorporate literal O&M experience (e.g., troubleshooting
guidance in manuals). On the other hand, LLM-based methods, which retrieve
fragmented information (e.g., standard documents + RAG), often generate
inaccurate or generic results. To address these limitations, we present
DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with
knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a
heterogeneous graph model for representing the diagnosis experience, and
proposes a semi-automatic graph construction algorithm to build that graph from
thousands of documents. Second, DBAIOps develops a collection of (800+)
reusable anomaly models that identify both directly alerted metrics and
implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps
proposes a two-stage graph evolution mechanism to explore relevant diagnosis
paths and identify missing relations automatically. It then leverages a
reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear
diagnosis reports for both DBAs and common users. Our evaluation over four
mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates
that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher
in root cause and human evaluation accuracy, respectively.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [105] [AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks](https://arxiv.org/abs/2508.00890)
*Fali Wang,Hui Liu,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Zongyu Wu,Chen Luo,Zhen Li,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文研究了多阶段复杂任务中的测试时计算最优缩放问题，提出了AgentTTS框架，通过迭代反馈驱动交互来自主搜索计算最优分配，实验结果显示其在搜索效率和鲁棒性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在单阶段任务中的测试时缩放（TTS），而许多现实世界的问题是多阶段复杂任务，由一系列异构子任务组成，每个子任务需要特定能力的LLM。因此，研究了一个新的问题：多阶段复杂任务中的测试时计算最优缩放，旨在选择合适的模型并在每个子任务上分配预算以最大化整体性能。

Method: 通过在四个任务和六个数据集上的广泛初步实验，得出三个经验见解，这些见解描述了LLM在多阶段复杂任务中的行为。基于这些见解，提出了AgentTTS框架，该框架通过与执行环境的迭代反馈驱动交互来自主搜索计算最优分配。

Result: 实验结果表明，AgentTTS在搜索效率方面显著优于传统和其它基于LLM的基线，并且在不同训练集大小下表现出更好的鲁棒性，同时增强了可解释性。

Conclusion: AgentTTS显著优于传统和其它基于LLM的基线，在搜索效率上表现出色，并且对不同训练集大小具有更好的鲁棒性，同时增强了可解释性。

Abstract: Test-time scaling (TTS) enhances the performance of large language models
(LLMs) by allocating additional compute resources during inference. However,
existing research primarily investigates TTS in single-stage tasks; while many
real-world problems are multi-stage complex tasks, composed of a sequence of
heterogeneous subtasks with each subtask requires LLM of specific capability.
Therefore, we study a novel problem: the test-time compute-optimal scaling in
multi-stage complex tasks, aiming to select suitable models and allocate
budgets per subtask to maximize overall performance. TTS in multi-stage tasks
introduces two fundamental challenges: (i) The combinatorial search space of
model and budget allocations, combined with the high cost of inference, makes
brute-force search impractical. (ii) The optimal model and budget allocations
across subtasks are interdependent, increasing the complexity of the
compute-optimal search. To address this gap, we conduct extensive pilot
experiments on four tasks across six datasets, deriving three empirical
insights characterizing the behavior of LLMs in multi-stage complex tasks.
Informed by these insights, we propose AgentTTS, an LLM-agent-based framework
that autonomously searches for compute-optimal allocations through iterative
feedback-driven interactions with the execution environment. Experimental
results demonstrate that AgentTTS significantly outperforms traditional and
other LLM-based baselines in search efficiency, and shows improved robustness
to varying training set sizes and enhanced interpretability.

</details>


### [106] [An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models](https://arxiv.org/abs/2508.00902)
*Kenneth Payne*

Main category: cs.AI

TL;DR: 研究显示大型语言模型在风险决策中表现出与人类相似的启发式和偏见，且上下文对决策有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究人类在不确定性下的风险判断，并探索语言模型是否能反映这种判断。

Method: 通过测试Kahneman和Tversky的前景理论，分析大型语言模型在风险决策中的表现。

Result: 发现前景理论通常能预测这些模型在各种场景下的风险决策，且上下文对风险偏好有重要影响。

Conclusion: 语言模型反映了人类的启发式和偏见，但这些偏见并不均匀。

Abstract: Judgment of risk is key to decision-making under uncertainty. As Daniel
Kahneman and Amos Tversky famously discovered, humans do so in a distinctive
way that departs from mathematical rationalism. Specifically, they demonstrated
experimentally that humans accept more risk when they feel themselves at risk
of losing something than when they might gain. I report the first tests of
Kahneman and Tversky's landmark 'prospect theory' with Large Language Models,
including today's state of the art chain-of-thought 'reasoners'.
  In common with humans, I find that prospect theory often anticipates how
these models approach risky decisions across a range of scenarios. I also
demonstrate that context is key to explaining much of the variance in risk
appetite. The 'frame' through which risk is apprehended appears to be embedded
within the language of the scenarios tackled by the models. Specifically, I
find that military scenarios generate far larger 'framing effects' than do
civilian settings, ceteris paribus. My research suggests, therefore, that
language models the world, capturing our human heuristics and biases. But also
that these biases are uneven - the idea of a 'frame' is richer than simple
gains and losses. Wittgenstein's notion of 'language games' explains the
contingent, localised biases activated by these scenarios. Finally, I use my
findings to reframe the ongoing debate about reasoning and memorisation in
LLMs.

</details>


### [107] [CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent](https://arxiv.org/abs/2508.01031)
*Jingzhe Ni,Xiaolong Yin,Xintong Li,Xingyu Lu,Ji Wei,Ruofeng Tong,Min Tang,Peng Du*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的CAD概念设计代理，能够接受文本描述和草图作为输入，并通过交互对话来细化设计需求。该代理基于一种新的上下文无关指令范式（CIP），生成高质量的CAD建模代码，并通过迭代的视觉反馈提升模型质量。生成的设计案例被存储在结构化知识库中，以持续改进代码生成能力。实验结果表明，该方法在CAD代码生成方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 计算机辅助设计（CAD）在工业制造中起着关键作用，但通常需要设计师具备高水平的专业知识。为了降低入门门槛并提高设计效率，我们提出了一个基于大型语言模型（LLMs）的CAD概念设计代理。

Method: 我们提出了一种基于大型语言模型（LLMs）的CAD概念设计代理。该代理接受抽象的文本描述和草图作为输入，并通过全面的需求分析与用户进行交互对话，以细化和明确设计要求。基于一种新的上下文无关指令范式（CIP），该代理生成高质量的CAD建模代码。在生成过程中，代理结合了迭代的视觉反馈以提高模型质量。生成的设计案例存储在一个结构化的知识库中，使代理的代码生成能力得以持续改进。

Result: 实验结果表明，我们的方法在CAD代码生成方面达到了最先进的性能。

Conclusion: 实验结果表明，我们的方法在CAD代码生成方面达到了最先进的性能。

Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing
but typically requires a high level of expertise from designers. To lower the
entry barrier and improve design efficiency, we present an agent for CAD
conceptual design powered by large language models (LLMs). The agent accepts
both abstract textual descriptions and freehand sketches as input, engaging in
interactive dialogue with users to refine and clarify design requirements
through comprehensive requirement analysis. Built upon a novel
Context-Independent Imperative Paradigm (CIP), the agent generates high-quality
CAD modeling code. During the generation process, the agent incorporates
iterative visual feedback to improve model quality. Generated design cases are
stored in a structured knowledge base, enabling continuous improvement of the
agent's code generation capabilities. Experimental results demonstrate that our
method achieves state-of-the-art performance in CAD code generation.

</details>


### [108] [Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens](https://arxiv.org/abs/2508.01191)
*Chengshuai Zhao,Zhen Tan,Pingchuan Ma,Dawei Li,Bohan Jiang,Yancheng Wang,Yingzhen Yang,Huan Liu*

Main category: cs.AI

TL;DR: 本文通过数据分布的视角研究了Chain-of-Thought (CoT)推理，发现其有效性受到训练数据与测试查询之间分布差异的限制，并揭示了CoT推理是一种脆弱的幻象。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT提示已被证明可以提高大型语言模型在各种任务上的性能，但一些初步发现表明，CoT推理可能比看起来更表面化，因此需要进一步探索。

Method: 本文通过数据分布的视角研究CoT推理，并设计了一个名为DataAlchemy的隔离和受控环境，从头开始训练大型语言模型，并在各种分布条件下系统地探测它们。

Result: 结果表明，CoT推理是当它被推得超出训练分布时会消失的脆弱幻象。

Conclusion: 本文揭示了Chain-of-Thought (CoT)推理是一种脆弱的幻象，当超出训练分布时会消失。工作提供了对CoT推理为何以及何时失败的更深入理解，并强调了实现真正且可泛化的推理的持续挑战。

Abstract: Chain-of-Thought (CoT) prompting has been shown to improve Large Language
Model (LLM) performance on various tasks. With this approach, LLMs appear to
produce human-like reasoning steps before providing answers (a.k.a., CoT
reasoning), which often leads to the perception that they engage in deliberate
inferential processes. However, some initial findings suggest that CoT
reasoning may be more superficial than it appears, motivating us to explore
further. In this paper, we study CoT reasoning via a data distribution lens and
investigate if CoT reasoning reflects a structured inductive bias learned from
in-distribution data, allowing the model to conditionally generate reasoning
paths that approximate those seen during training. Thus, its effectiveness is
fundamentally bounded by the degree of distribution discrepancy between the
training data and the test queries. With this lens, we dissect CoT reasoning
via three dimensions: task, length, and format. To investigate each dimension,
we design DataAlchemy, an isolated and controlled environment to train LLMs
from scratch and systematically probe them under various distribution
conditions. Our results reveal that CoT reasoning is a brittle mirage that
vanishes when it is pushed beyond training distributions. This work offers a
deeper understanding of why and when CoT reasoning fails, emphasizing the
ongoing challenge of achieving genuine and generalizable reasoning.

</details>


### [109] [Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan](https://arxiv.org/abs/2508.01274)
*Jui-Ming Yao,Bing-Cheng Xie,Sheng-Wei Peng,Hao-Yuan Chen,He-Rong Zheng,Bing-Jia Tan,Peter Shaojui Wang,Shun-Feng Su*

Main category: cs.AI

TL;DR: 本文介绍了Multi-TW，这是首个针对任何模态的多模态模型在中文环境下的性能和延迟评估基准。通过评估多种模型，发现闭源模型通常优于开源模型，但开源模型在音频任务中表现良好。此外，端到端的多模态管道在延迟方面具有明显优势。


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks often overlook tri-modal evaluation in Traditional Chinese and do not consider inference latency.

Method: We introduced Multi-TW, the first Traditional Chinese benchmark for evaluating the performance and latency of any-to-any multimodal models. We evaluated various any-to-any models and vision-language models (VLMs) with audio transcription.

Result: Closed-source models generally outperform open-source ones across modalities, although open-source models can perform well in audio tasks. End-to-end any-to-any pipelines offer clear latency advantages compared to VLMs using separate audio transcription.

Conclusion: Multi-TW presents a comprehensive view of model capabilities and highlights the need for Traditional Chinese fine-tuning and efficient multimodal architectures.

Abstract: Multimodal Large Language Models (MLLMs) process visual, acoustic, and
textual inputs, addressing the limitations of single-modality LLMs. However,
existing benchmarks often overlook tri-modal evaluation in Traditional Chinese
and do not consider inference latency. To address this, we introduce Multi-TW,
the first Traditional Chinese benchmark for evaluating the performance and
latency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice
questions (image and text, audio and text pairs) sourced from official
proficiency tests developed with the Steering Committee for the Test of
Proficiency-Huayu (SC-TOP). We evaluated various any-to-any models and
vision-language models (VLMs) with audio transcription. Our results show that
closed-source models generally outperform open-source ones across modalities,
although open-source models can perform well in audio tasks. End-to-end
any-to-any pipelines offer clear latency advantages compared to VLMs using
separate audio transcription. Multi-TW presents a comprehensive view of model
capabilities and highlights the need for Traditional Chinese fine-tuning and
efficient multimodal architectures.

</details>


### [110] [Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning](https://arxiv.org/abs/2508.01773)
*Jiuzhou Han,Wray Buntine,Ehsan Shareghi*

Main category: cs.AI

TL;DR: 本文提出了一种基于不确定性的自动化PRM数据构建框架，并引入了两种新的输出聚合方法，以提高数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的PRM数据构建方法往往劳动密集或低效，因此需要一种更高效和自动化的解决方案。

Method: 本文提出了一种基于不确定性的自动化PRM过程奖励数据构建框架，包括数据生成和注释过程。同时，提出了两种通用的不确定性感知输出聚合方法：Hybrid Majority Reward Vote和Weighted Reward Frequency Vote。

Result: 实验结果表明，所提出的PRM数据构建框架在多个基准数据集上表现优异，并且两种输出聚合方法进一步提升了数学推理能力。

Conclusion: 本文提出的PRM数据构建框架在ProcessBench、MATH和GSMPlus上的实验结果表明其有效性和效率，并且两种输出聚合方法进一步提高了不同PRMs的数学推理能力。

Abstract: Large language models have demonstrated remarkable capabilities in complex
mathematical reasoning tasks, but they inevitably generate errors throughout
multi-step solutions. Process-level Reward Models (PRMs) have shown great
promise by providing supervision and evaluation at each intermediate step,
thereby effectively improving the models' reasoning abilities. However,
training effective PRMs requires high-quality process reward data, yet existing
methods for constructing such data are often labour-intensive or inefficient.
In this paper, we propose an uncertainty-driven framework for automated process
reward data construction, encompassing both data generation and annotation
processes for PRMs. Additionally, we identify the limitations of both majority
vote and PRMs, and introduce two generic uncertainty-aware output aggregation
methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which
combine the strengths of majority vote with PRMs. Extensive experiments on
ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the
proposed PRM data construction framework, and demonstrate that the two output
aggregation methods further improve the mathematical reasoning abilities across
diverse PRMs. The code and data will be publicly available at
https://github.com/Jiuzhouh/UnPRM.

</details>


### [111] [LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?](https://arxiv.org/abs/2508.01780)
*Guozhao Mo,Wenliang Zhong,Jiawei Chen,Xuanang Chen,Yaojie Lu,Hongyu Lin,Ben He,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: This paper introduces LiveMCPBench, a comprehensive benchmark for evaluating LLM agents in realistic, tool-rich, and dynamic MCP environments. It includes LiveMCPTool, LiveMCPEval, and the MCP Copilot Agent, providing a unified framework for scalable and reproducible research on agent capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios.

Method: We present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. We curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. We introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments. We propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite.

Result: Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments.

Conclusion: LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities.

Abstract: With the rapid development of Model Context Protocol (MCP), the number of MCP
servers has surpassed 10,000. However, existing MCP benchmarks are limited to
single-server settings with only a few tools, hindering effective evaluation of
agent capabilities in large-scale, real-world scenarios. To address this
limitation, we present LiveMCPBench, the first comprehensive benchmark
comprising 95 real-world tasks grounded in the MCP ecosystem, designed to
evaluate LLM agents at scale across diverse servers. To support a scalable and
reproducible evaluation pipeline in large-scale MCP environments, we curate
LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and
527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework
that enables automated and adaptive evaluation in dynamic, time-varying task
environments, achieving 81% agreement with human reviewers. Finally, we propose
the MCP Copilot Agent, a multi-step agent that routes tools for dynamic
planning and executes tools for API interaction across the entire LiveMCPTool
suite. Our evaluation covers 10 leading models, with the best-performing model
(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large
performance variance across models, and several widely-used models perform
poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench
offers the first unified framework for benchmarking LLM agents in realistic,
tool-rich, and dynamic MCP environments, laying a solid foundation for scalable
and reproducible research on agent capabilities. Our code and data will be
publicly available at https://icip-cas.github.io/LiveMCPBench.

</details>


### [112] [Trainable Dynamic Mask Sparse Attention](https://arxiv.org/abs/2508.02124)
*Jingze Shi,Yifan Wu,Bingheng Wu,Yiran Peng,Liangdong Wang,Guang Liu,Yuyu Luo*

Main category: cs.AI

TL;DR: 本文提出了一种可训练的动态掩码稀疏注意力机制，称为Dynamic Mask Attention (DMA)，通过两个关键创新：从值表示中动态生成内容感知的稀疏掩码，以及实现位置感知的稀疏注意力计算。DMA在多个任务中表现出优越的性能和效率，能够有效平衡模型效率和长上下文建模能力。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型中，对建模长上下文的需求不断增加，但标准自注意力机制的二次复杂度通常成为瓶颈。现有的稀疏注意力机制虽然提高了效率，但仍可能遇到静态模式或信息丢失的问题。

Method: 提出了一种可训练的动态掩码稀疏注意力机制，称为Dynamic Mask Attention (DMA)，通过两个关键创新：从值表示中动态生成内容感知的稀疏掩码，以及实现位置感知的稀疏注意力计算。

Result: DMA在Chinchilla Scaling Law设置下的困惑度方面优于多头注意力、滑动窗口注意力、多头潜在注意力和原生稀疏注意力。此外，在具有挑战性的多查询关联回忆任务中，DMA也表现出优于这些方法的性能和效率。在1.7B参数模型的评估中，DMA在标准基准性能和挑战性的needle-in-a-haystack任务中都显著优于多头注意力。

Conclusion: DMA能够有效地平衡模型效率和长上下文建模能力，具有出色的性能和效率。

Abstract: In large language models, the demand for modeling long contexts is constantly
increasing, but the quadratic complexity of the standard self-attention
mechanism often becomes a bottleneck. Although existing sparse attention
mechanisms have improved efficiency, they may still encounter issues such as
static patterns or information loss. We introduce a trainable dynamic mask
sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes
content-aware and position-aware sparsity. DMA achieves this through two key
innovations: First, it dynamically generates content-aware sparse masks from
value representations, enabling the model to identify and focus on critical
information adaptively. Second, it implements position-aware sparse attention
computation that effectively skips unnecessary calculation regions. This
dual-sparsity design allows the model to significantly reduce the computational
complexity of important information while retaining complete information,
achieving an excellent balance between information fidelity and computational
efficiency. We have verified the performance of DMA through comprehensive
experiments. Comparative studies show that DMA outperforms multi-head
attention, sliding window attention, multi-head latent attention, and native
sparse attention in terms of perplexity under Chinchilla Scaling Law settings.
Moreover, in challenging multi-query associative recall tasks, DMA also
demonstrates superior performance and efficiency compared to these methods.
Crucially, in the evaluation of a 1.7B parameter model, DMA significantly
outperforms multi-head attention in both standard benchmark performance and the
challenging needle-in-a-haystack task. These experimental results highlight its
capability to balance model efficiency and long-context modeling ability
effectively.

</details>


### [113] [OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling](https://arxiv.org/abs/2508.02503)
*Maxime Bouscary,Saurabh Amin*

Main category: cs.AI

TL;DR: OptiHive是一种无需迭代自我纠正的LLM框架，能够从自然语言描述中生成高质量的优化问题求解器，并通过统计模型实现不确定性量化和求解器选择。


<details>
  <summary>Details</summary>
Motivation: LLM-based求解器仍然不可靠，通常依赖于迭代修复循环，导致显著的延迟。需要一种无需迭代自我纠正的方法来生成高质量的求解器。

Method: OptiHive使用单次批处理LLM查询生成多样化的组件（求解器、问题实例和验证测试），并过滤掉错误的组件以确保完全可解释的输出。此外，利用统计模型推断其真实性能，实现合理的不确定性量化和求解器选择。

Result: OptiHive在传统优化问题到多仓库车辆路径问题的挑战性变体等任务中显著优于基线方法，最复杂问题的最优率从5%提高到92%。

Conclusion: OptiHive在从自然语言描述生成优化问题的高质量求解器方面表现出色，显著优于基线方法。

Abstract: LLM-based solvers have emerged as a promising means of automating problem
modeling and solving. However, they remain unreliable and often depend on
iterative repair loops that result in significant latency. We introduce
OptiHive, an LLM-based framework that produces high-quality solvers for
optimization problems from natural-language descriptions without iterative
self-correction. OptiHive uses a single batched LLM query to generate diverse
components (solvers, problem instances, and validation tests) and filters out
erroneous components to ensure fully interpretable outputs. Taking into account
the imperfection of the generated components, we employ a statistical model to
infer their true performance, enabling principled uncertainty quantification
and solver selection. On tasks ranging from traditional optimization problems
to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive
significantly outperforms baselines, increasing the optimality rate from 5\% to
92\% on the most complex problems.

</details>


### [114] [Test-time Prompt Intervention](https://arxiv.org/abs/2508.02511)
*Chenxu Yang,Qingyi Si,Mz Dai,Dingyu Yao,Mingyu Zheng,Minghui Chen,Zheng Lin,Weiping Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为PI的测试时提示干预框架，通过动态引导和调节推理路径来减少冗余和提高推理的可控性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的推理模型在生成CoTs时存在过多冗余，包括不必要的验证步骤和重复的推理转换，这主要是由于后训练阶段过度依赖结果奖励范式，而过程奖励范式的数据难以大规模构建。

Method: 提出了一种名为PI的新框架，通过及时（When模块）和适当（How模块）的干预以及干预后的采样（Which模块）来动态引导和调节推理路径。

Result: 在多个模型和数据集上的广泛实验表明，PI显著缩短了CoTs并减少了幻觉。

Conclusion: PI框架能够显著缩短CoTs，同时减少幻觉，产生更简洁和可靠的推理。

Abstract: Test-time compute has led to remarkable success in the large language model
(LLM) community, particularly for complex tasks, where longer chains of thought
(CoTs) are generated to enhance reasoning capabilities. However, growing
evidence reveals that such reasoning models often produce CoTs plagued by
excessive redundancy, including unnecessary verification steps and repetitive
reasoning shifts. The root cause lies in post-training of them that overly rely
on outcome reward paradigms, as the data of process reward paradigms, which
regulate intermediate reasoning steps, is difficult to construct at scale. To
address this, we propose PI, a novel framework for Test-time Prompt
Intervention. PI provides an interface to dynamically guide and regulate
reasoning paths during inference through timely (When module) and proper (How
module) interventions and post-intervention sampling (Which module). This
allows human problem-solving expertise and cognitive science principles to be
seamlessly integrated into LLMs' reasoning processes, enhancing controllability
and interpretability. Extensive experiments across multiple models and datasets
demonstrate that PI significantly shortens CoTs while reducing hallucination,
yielding more concise and reliable reasoning.

</details>


### [115] [HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research](https://arxiv.org/abs/2508.02621)
*Yinghao Zhu,Yifan Qi,Zixiang Wang,Lei Gu,Dehao Sui,Haoran Hu,Xichen Zhang,Ziyi He,Liantao Ma,Lequan Yu*

Main category: cs.AI

TL;DR: 本文提出了一种名为HealthFlow的自我进化AI代理，通过新颖的元级进化机制克服了传统AI代理在医疗研究中的局限性。实验表明，HealthFlow在复杂、现实的健康数据分析任务中表现优异，为更自主和有效的AI在科学发现中的应用铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在医疗研究中的有效性受到其对静态、预定义策略的依赖的限制，这导致它们无法学习成为更好的战略规划者，而这是医疗等复杂领域的重要技能。因此，需要一种能够自我进化的AI代理，以提高其在复杂领域的表现。

Method: 本文引入了HealthFlow，这是一种自我进化的AI代理，通过一种新的元级进化机制来克服传统AI代理的局限性。HealthFlow通过将程序成功和失败提炼成持久的战略知识库，自主优化其高层次的问题解决策略。此外，还引入了EHRFlowBench，一个新基准，用于复杂、现实的健康数据分析任务。

Result: 实验结果表明，HealthFlow的自我进化方法在复杂、现实的健康数据分析任务中显著优于最先进的代理框架。

Conclusion: 本文提出了HealthFlow，这是一种自我进化的AI代理，通过新颖的元级进化机制克服了传统AI代理在医疗研究中的局限性。实验表明，HealthFlow的自我进化方法显著优于最先进的代理框架。这项工作标志着从构建更好的工具使用者转向设计更智能、自我进化的任务管理者，为科学发现中的更自主和有效的AI铺平了道路。

Abstract: The efficacy of AI agents in healthcare research is hindered by their
reliance on static, predefined strategies. This creates a critical limitation:
agents can become better tool-users but cannot learn to become better strategic
planners, a crucial skill for complex domains like healthcare. We introduce
HealthFlow, a self-evolving AI agent that overcomes this limitation through a
novel meta-level evolution mechanism. HealthFlow autonomously refines its own
high-level problem-solving policies by distilling procedural successes and
failures into a durable, strategic knowledge base. To anchor our research and
facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark
featuring complex, realistic health data analysis tasks derived from
peer-reviewed clinical research. Our comprehensive experiments demonstrate that
HealthFlow's self-evolving approach significantly outperforms state-of-the-art
agent frameworks. This work marks a necessary shift from building better
tool-users to designing smarter, self-evolving task-managers, paving the way
for more autonomous and effective AI for scientific discovery.

</details>


### [116] [Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction](https://arxiv.org/abs/2508.02622)
*Enrico De Santis,Antonello Rizzi*

Main category: cs.AI

TL;DR: 本文介绍了无塞米亚这一新的认知现象，探讨了用户如何在与生成式AI系统的互动中赋予它们意图、代理和内在性，并分析了其独特的特征及其哲学和社会影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在引入和形式化无塞米亚这一新的认知现象，该现象源于人类与生成式AI系统的互动，特别是那些支持对话或多媒体交流的系统。

Method: 本文提出了一个多学科框架来解释用户如何在特定条件下将意图、代理和内在性归因于这些系统，这一过程基于语言表现、认知模糊性和新兴技术复杂性。

Result: 通过将LLM的意义整体性与我们的技术概念LLM上下文认知场联系起来，我们阐明了LLM如何以关系方式构建意义，以及在人机接口处如何产生连贯性和代理的仿像。

Conclusion: 本文最后反思了无塞米亚动态的更广泛哲学、认识论和社会影响，并指出了未来研究的方向。

Abstract: This paper introduces and formalizes Noosemia, a novel
cognitive-phenomenological phenomenon emerging from human interaction with
generative AI systems, particularly those enabling dialogic or multimodal
exchanges. We propose a multidisciplinary framework to explain how, under
certain conditions, users attribute intentionality, agency, and even
interiority to these systems - a process grounded not in physical resemblance,
but in linguistic performance, epistemic opacity, and emergent technological
complexity. By linking an LLM declination of meaning holism to our technical
notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct
meaning relationally and how coherence and a simulacrum of agency arise at the
human-AI interface. The analysis situates noosemia alongside pareidolia,
animism, the intentional stance and the uncanny valley, distinguishing its
unique characteristics. We also introduce a-noosemia to describe the
phenomenological withdrawal of such projections. The paper concludes with
reflections on the broader philosophical, epistemological, and social
implications of noosemic dynamics and directions for future research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [117] [Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models](https://arxiv.org/abs/2508.00881)
*Vijja Wichitwechkarn,Charles Fox,Ruchi Choudhary*

Main category: cs.LG

TL;DR: 本文提出了多变量时间序列基础模型的幻觉定义、检测和缓解方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏多变量时间序列（MVTS）基础模型的类似定义和方法。

Method: 使用扩散模型估计幻觉水平，提出新的MVTS幻觉定义以及检测和缓解方法。

Result: 开源预训练MVTS填补基础模型关系性幻觉平均达到弱基线的59.5%，所提出的缓解方法可减少47.7%。

Conclusion: 定义和方法可能提高MVTS基础模型的采用和安全使用。

Abstract: Foundation models for natural language processing have many coherent
definitions of hallucination and methods for its detection and mitigation.
However, analogous definitions and methods do not exist for multi-variate
time-series (MVTS) foundation models. We propose new definitions for MVTS
hallucination, along with new detection and mitigation methods using a
diffusion model to estimate hallucination levels. We derive relational datasets
from popular time-series datasets to benchmark these relational hallucination
levels. Using these definitions and models, we find that open-source
pre-trained MVTS imputation foundation models relationally hallucinate on
average up to 59.5% as much as a weak baseline. The proposed mitigation method
reduces this by up to 47.7% for these models. The definition and methods may
improve adoption and safe usage of MVTS foundation models.

</details>


### [118] [Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge](https://arxiv.org/abs/2508.00901)
*Ruichen Xu,Kexin Chen*

Main category: cs.LG

TL;DR: 本文分析了Transformer模型在预训练和微调过程中知识获取和提取的能力，并提出了一个包含自注意力和MLP模块的一层框架，验证了其理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在单层、仅关注注意力的架构上，而实际研究表明MLP是存储知识的主要组件。然而，简化模型在标准的下一个token预测目标下可能无法获取或提取事实知识。

Method: 本文引入了一个包含自注意力和MLP模块的一层Transformer框架，并通过跟踪其梯度动态来建立收敛和泛化保证。

Result: 1) Transformer可以在预训练中达到接近最优的训练损失，表明有效知识获取；2) 在满足特定数据多重性条件的情况下，Transformer在测试事实知识时可以达到低泛化误差，表明成功知识提取；3) 当条件不满足时，Transformer会表现出高泛化损失，导致幻觉。

Conclusion: 本文通过分析Transformer模型在预训练和微调过程中的梯度动态，揭示了其知识获取和提取的能力，并指出当条件不满足时会出现幻觉现象。

Abstract: Modern large language models excel in knowledge-intensive tasks, yet how
transformers acquire (store) knowledge during pre-training and extract
(retrieve) it during post-fine-tuning inference remains theoretically opaque.
While prior theoretical work has begun to investigate these questions through
the analysis of training dynamics, such studies are limited to single-layer,
attention-only architectures. However, most existing studies suggest that MLPs
are the most contributing components for storing knowledge in transformer-based
language models. Meanwhile, our empirical investigations reveal that such
simplified models, when trained using standard next-token prediction
objectives, may be incapable of acquiring or extracting factual knowledge. To
overcome this limitation, we introduce a tractable one-layer transformer
framework that crucially incorporates both self-attention and MLP modules. By
tracking its gradient dynamics, we establish convergence and generalization
guarantees that illuminate the ability of knowledge acquisition and extraction.
We prove that 1) Transformers can achieve near-optimal training loss during
pre-training, signifying effective knowledge acquisition; 2) With a large
fine-tuning dataset and specific data multiplicity conditions met, transformers
can achieve low generalization error when tested on factual knowledge learned
during pre-training but not reinforced during the fine-tuning, indicating
successful knowledge extraction; 3) When the conditions are not satisfied,
transformers exhibit high generalization loss, resulting in hallucinations. Our
analysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,
our analysis offers theoretical insights into several pertinent empirical
phenomena, such as the role of learning rate schedules. Experiments on
synthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate
our results.

</details>


### [119] [Small sample-based adaptive text classification through iterative and contrastive description refinement](https://arxiv.org/abs/2508.00957)
*Amrit Rajeev,Udayaadithya Avadhanam,Harshula Tulapurkar,SaiBarath Sundar*

Main category: cs.LG

TL;DR: 该研究提出了一种结合迭代主题精炼、对比提示和主动学习的分类框架，能够在没有重新训练的情况下无缝集成新的、未见过的类别，适用于现实世界动态环境。


<details>
  <summary>Details</summary>
Motivation: 零样本文本分类在领域知识不断演变和类别边界模糊的场景中仍然是一项困难任务，例如票务系统。大型语言模型（LLMs）在此类场景中往往难以泛化，而少样本方法受到数据多样性不足的限制。

Method: 该框架结合了迭代主题精炼、对比提示和主动学习，通过生成初始主题标签，并利用误分类或模糊样本进行迭代对比提示过程来精炼类别区分。同时，引入了人机交互组件，允许用户以自然语言引入或修改类别定义。

Result: 在AGNews和DBpedia上的评估显示了良好的性能：AGNews上的准确率为91%（3个已见，1个未见类别），DBpedia上的准确率为84%（8个已见，1个未见类别）。在引入未见类别后，准确率下降很小（分别为82%和87%）。

Conclusion: 该研究提出了一种结合迭代主题精炼、对比提示和主动学习的分类框架，能够在没有重新训练的情况下无缝集成新的、未见过的类别，适用于现实世界动态环境。

Abstract: Zero-shot text classification remains a difficult task in domains with
evolving knowledge and ambiguous category boundaries, such as ticketing
systems. Large language models (LLMs) often struggle to generalize in these
scenarios due to limited topic separability, while few-shot methods are
constrained by insufficient data diversity. We propose a classification
framework that combines iterative topic refinement, contrastive prompting, and
active learning. Starting with a small set of labeled samples, the model
generates initial topic labels. Misclassified or ambiguous samples are then
used in an iterative contrastive prompting process to refine category
distinctions by explicitly teaching the model to differentiate between closely
related classes. The framework features a human-in-the-loop component, allowing
users to introduce or revise category definitions in natural language. This
enables seamless integration of new, unseen categories without retraining,
making the system well-suited for real-world, dynamic environments. The
evaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy
on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with
minimal accuracy shift after introducing unseen classes (82% and 87%,
respectively). The results highlight the effectiveness of prompt-based semantic
reasoning for fine-grained classification with limited supervision.

</details>


### [120] [Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models](https://arxiv.org/abs/2508.01908)
*Istabrak Abbes,Gopeshh Subbaraj,Matthew Riemer,Nizar Islah,Benjamin Therien,Tsuguchika Tabaru,Hiroaki Kingetsu,Sarath Chandar,Irina Rish*

Main category: cs.LG

TL;DR: 本文探讨了经验回放和梯度对齐在持续预训练中的有效性，并提出了一种高效的元经验回放方法，证明小规模回放比增加模型大小更节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型训练需要从头开始重新训练，当新数据可用时效率低下。本文旨在探索更高效和资源节约的持续预训练方法，以应对分布偏移问题。

Method: 本文研究了经验回放和梯度对齐两种方法在持续预训练中的应用，并提出了一个高效的元经验回放（MER）实现，结合了梯度对齐的优势。

Result: 实验结果表明，经验回放和梯度对齐能够有效减少模型在持续预训练中的性能下降，同时在不同模型规模和任务多样性下保持稳定。

Conclusion: 本文结论是，经验回放和梯度对齐可以提高模型在持续预训练中的学习稳定性，避免遗忘。此外，小规模的回放率比增加模型大小更有效地利用计算资源，但扩大模型规模比高回放率更计算高效。

Abstract: Training large language models (LLMs) typically involves pre-training on
massive corpora, only to restart the process entirely when new data becomes
available. A more efficient and resource-conserving approach would be continual
pre-training, where models are updated with new data rather than retraining
from scratch. However, the introduction of new data often causes distribution
shifts, leading to performance degradation on previously learned tasks. In this
paper, we take a deeper look at two popular proposals for addressing this
distribution shift within the continual learning literature: experience replay
and gradient alignment. We consider continual pre-training of models within the
Llama family of architectures at a large scale across languages with 100
billion tokens of training data in each language, finding that both replay and
gradient alignment lead to more stable learning without forgetting. This
conclusion holds both as we vary the model scale and as we vary the number and
diversity of tasks. Moreover, we are the first to demonstrate the effectiveness
of gradient alignment techniques in the context of LLM pre-training and propose
an efficient implementation of meta-experience replay (MER) that imbues
experience replay with the benefits of gradient alignment despite negligible
compute and memory overhead. Our scaling analysis across model sizes and replay
rates indicates that small rates of replaying old examples are definitely a
more valuable use of compute than investing in model size, but that it is more
compute efficient to scale the size of the model than invest in high rates of
replaying old examples.

</details>


### [121] [Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning](https://arxiv.org/abs/2508.01916)
*Xinting Huang,Michael Hahn*

Main category: cs.LG

TL;DR: 本文提出了一种无监督方法来发现神经网络中的可解释子空间，并展示了这些子空间与模型电路变量之间的联系。


<details>
  <summary>Details</summary>
Motivation: 理解神经模型的内部表示是机制可解释性的核心兴趣。由于其高维性，表示空间可以编码关于输入的各种方面。问题是不同方面是否在单独的子空间中组织和编码，以及是否可以在纯粹无监督的方式下找到这些“自然”子空间。

Method: 我们提出了一种名为邻近距离最小化（NDM）的方法，该方法在无监督的情况下学习非基对齐的子空间。

Result: 通过定量实验，我们发现子空间与电路变量之间存在强关联。此外，我们还提供了证据表明该方法可以扩展到2B模型。

Conclusion: 我们的研究提供了一种新的视角来理解模型内部并构建电路。

Abstract: Understanding internal representations of neural models is a core interest of
mechanistic interpretability. Due to its large dimensionality, the
representation space can encode various aspects about inputs. To what extent
are different aspects organized and encoded in separate subspaces? Is it
possible to find these ``natural'' subspaces in a purely unsupervised way?
Somewhat surprisingly, we can indeed achieve this and find interpretable
subspaces by a seemingly unrelated training objective. Our method, neighbor
distance minimization (NDM), learns non-basis-aligned subspaces in an
unsupervised manner. Qualitative analysis shows subspaces are interpretable in
many cases, and encoded information in obtained subspaces tends to share the
same abstract concept across different inputs, making such subspaces similar to
``variables'' used by the model. We also conduct quantitative experiments using
known circuits in GPT-2; results show a strong connection between subspaces and
circuit variables. We also provide evidence showing scalability to 2B models by
finding separate subspaces mediating context and parametric knowledge routing.
Viewed more broadly, our findings offer a new perspective on understanding
model internals and building circuits.

</details>


### [122] [MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs](https://arxiv.org/abs/2508.02066)
*Guojiang Zhao,Sihang Li,Zixiang Lu,Zheng Cheng,Haitao Lin,Lirong Wu,Hanchen Xia,Hengxing Cai,Wentao Guo,Hongshuai Wang,Mingjun Xu,Siyu Zhu,Guolin Ke,Linfeng Zhang,Zhifeng Gao*

Main category: cs.LG

TL;DR: MolReasoner是一种两阶段框架，旨在提升大型语言模型在分子推理方面的表现，通过合成思维链样本和强化学习优化化学结构与语言描述的对齐。


<details>
  <summary>Details</summary>
Motivation: 当前方法在分子推理方面存在不足，依赖通用提示缺乏领域特定语义，而微调策略面临可解释性和推理深度的问题。

Method: MolReasoner是一个两阶段框架，包括Mol-SFT和Mol-RL。Mol-SFT通过生成合成的思维链样本初始化模型的推理能力，而Mol-RL则利用专门设计的奖励函数进行强化学习，以对齐化学结构与语言描述。

Result: 实验表明，MolReasoner优于现有方法，显著提升了模型的分子理解能力和泛化能力。

Conclusion: MolReasoner显著提升了模型的分子理解能力，并实现了从记忆到化学推理的转变。

Abstract: Large Language Models(LLMs) have demonstrated remarkable performance across
various domains, yet their capabilities in molecular reasoning remain
insufficiently explored. Current approaches tend to rely heavily on
general-purpose prompting, which lacks domain-specific molecular semantics,
while those that use fine-tuning strategies often face challenges with
interpretability and reasoning depth. To address these issues, we introduce
MolReasoner, a two-stage framework designed to transition LLMs from
memorization towards chemical reasoning. First, we propose Mol-SFT, which
initializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)
samples generated by GPT-4o and verified for chemical accuracy. Subsequently,
Mol-RL applies reinforcement learning with specialized reward functions
designed explicitly to align chemical structures with linguistic descriptions,
thereby enhancing molecular reasoning capabilities. Our approach notably
enhances interpretability, improving the model 's molecular understanding and
enabling better generalization. Extensive experiments demonstrate that
MolReasoner outperforms existing methods, and marking a significant shift from
memorization-based outputs to robust chemical reasoning.

</details>


### [123] [CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.02091)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Chris Shum,Jiwei Li*

Main category: cs.LG

TL;DR: CRINN is a new ANNS algorithm that uses reinforcement learning to automatically generate faster implementations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: ANNS algorithms are critical for recent AI applications, and there is a need for more efficient and automated methods to optimize them.

Method: CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal.

Result: CRINN achieves best performance on three of six widely-used NNS benchmark datasets and tied for first place on two others.

Conclusion: CRINN's success demonstrates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations.

Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become
increasingly critical for recent AI applications, particularly in
retrieval-augmented generation (RAG) and agent-based LLM applications. In this
paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS
optimization as a reinforcement learning problem where execution speed serves
as the reward signal. This approach enables the automatic generation of
progressively faster ANNS implementations while maintaining accuracy
constraints. Our experimental evaluation demonstrates CRINN's effectiveness
across six widely-used NNS benchmark datasets. When compared against
state-of-the-art open-source ANNS algorithms, CRINN achieves best performance
on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and
GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean
and GloVe-25-angular). The implications of CRINN's success reach well beyond
ANNS optimization: It validates that LLMs augmented with reinforcement learning
can function as an effective tool for automating sophisticated algorithmic
optimizations that demand specialized knowledge and labor-intensive manual
refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN

</details>


### [124] [LeanK: Learnable K Cache Channel Pruning for Efficient Decoding](https://arxiv.org/abs/2508.02215)
*Yike Zhang,Zhiyuan He,Huiqiang Jiang,Chengruidong Zhang,Yuqing Yang,Jianyong Wang,Lili Qiu*

Main category: cs.LG

TL;DR: LeanK is a learning-based method that prunes unimportant key (K) cache channels, reducing GPU memory and accelerating decoding without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) face efficiency challenges due to the growing key-value (KV) cache.

Method: LeanK is a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity with a novel two-stage training process.

Result: Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation.

Conclusion: LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy.

Abstract: Large language models (LLMs) enable long-context tasks but face efficiency
challenges due to the growing key-value (KV) cache. We propose LeanK, a
learning-based method that prunes unimportant key (K) cache channels by
leveraging static channel sparsity. With a novel two-stage training process,
LeanK learns channel-wise static mask that could satisfy specific sparsity
ratio and hardware alignment requirement. LeanK reduces GPU memory and
accelerates decoding without sacrificing accuracy. Experiments demonstrate up
to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel
enables 1.3x speedup for attention computation. We also provide insights into
model channels and attention heads during long-context inference by analyzing
the learned importance distribution. Our code is available at
https://aka.ms/LeanK.

</details>


### [125] [CellForge: Agentic Design of Virtual Cell Models](https://arxiv.org/abs/2508.02276)
*Xiangru Tang,Zhuoyun Yu,Jiapeng Chen,Yan Cui,Daniel Shao,Weixu Wang,Fang Wu,Yuchen Zhuang,Wenqi Shi,Zhi Huang,Arman Cohan,Xihong Lin,Fabian Theis,Smita Krishnaswamy,Mark Gerstein*

Main category: cs.LG

TL;DR: CellForge is an agentic system that uses a multi-agent framework to create optimized computational models for virtual cells, outperforming existing methods in predicting single-cell perturbations.


<details>
  <summary>Details</summary>
Motivation: Autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines.

Method: CellForge is an agentic system that leverages a multi-agent framework to transform biological datasets and research objectives into optimized computational models for virtual cells. It integrates three core modules: Task Analysis, Method Design, and Experiment Execution.

Result: CellForge consistently outperforms task-specific state-of-the-art methods in single-cell perturbation prediction using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities.

Conclusion: CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge.

Abstract: Virtual cell modeling represents an emerging frontier at the intersection of
artificial intelligence and biology, aiming to predict quantities such as
responses to diverse perturbations quantitatively. However, autonomously
building computational models for virtual cells is challenging due to the
complexity of biological systems, the heterogeneity of data modalities, and the
need for domain-specific expertise across multiple disciplines. Here, we
introduce CellForge, an agentic system that leverages a multi-agent framework
that transforms presented biological datasets and research objectives directly
into optimized computational models for virtual cells. More specifically, given
only raw single-cell multi-omics data and task descriptions as input, CellForge
outputs both an optimized model architecture and executable code for training
virtual cell models and inference. The framework integrates three core modules:
Task Analysis for presented dataset characterization and relevant literature
retrieval, Method Design, where specialized agents collaboratively develop
optimized modeling strategies, and Experiment Execution for automated
generation of code. The agents in the Design module are separated into experts
with differing perspectives and a central moderator, and have to
collaboratively exchange solutions until they achieve a reasonable consensus.
We demonstrate CellForge's capabilities in single-cell perturbation prediction,
using six diverse datasets that encompass gene knockouts, drug treatments, and
cytokine stimulations across multiple modalities. CellForge consistently
outperforms task-specific state-of-the-art methods. Overall, CellForge
demonstrates how iterative interaction between LLM agents with differing
perspectives provides better solutions than directly addressing a modeling
challenge. Our code is publicly available at
https://github.com/gersteinlab/CellForge.

</details>


### [126] [CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment](https://arxiv.org/abs/2508.02298)
*Guofu Xie,Yunsheng Shi,Hongtao Tian,Ting Yao,Xiao Zhang*

Main category: cs.LG

TL;DR: CAPO is a simple but efficient method for credit assignment in reinforcement learning with verifiable rewards, using an off-the-shelf LLM as a Generative Process Reward Model to provide verifiable token-level rewards, resulting in improved performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods typically treat whole responses as single actions, assigning the same reward to every token, which hampers precise credit assignment. Methods like PPO provide credit assignment through value estimation, but often yield inaccurate and unverifiable signals. Process Reward Models can provide step-by-step judgments but require high-quality process supervision labels and are time-consuming in online RL.

Method: CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass, thereby providing verifiable token-level rewards to refine the tokens that were originally assigned identical rule-based rewards. Voting mechanisms are employed to enhance the accuracy and robustness of CAPO.

Result: CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across six challenging mathematical benchmarks and three out-of-domain benchmarks.

Conclusion: CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across six challenging mathematical benchmarks and three out-of-domain benchmarks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the
reasoning abilities of Large Language Models (LLMs) by using rule-based binary
feedback, helping to mitigate reward hacking. However, current RLVR methods
typically treat whole responses as single actions, assigning the same reward to
every token. This coarse-grained feedback hampers precise credit assignment,
making it hard for models to identify which reasoning steps lead to success or
failure, and often results in suboptimal policies and inefficient learning.
Methods like PPO provide credit assignment through value estimation, but often
yield inaccurate and unverifiable signals due to limited sampling. On the other
hand, methods using Process Reward Models can provide step-by-step judgments
for each reasoning step, but they require high-quality process supervision
labels and are time-consuming when applied in online reinforcement learning
(RL). To overcome these limitations, we introduce a simple but efficient method
Credit Assignment Policy Optimization (CAPO). Given a reasoning response
rollout from the policy model, CAPO directly leverages an off-the-shelf,
general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to
generate all step-wise critique by one pass, thereby providing verifiable
token-level rewards to refine the tokens that were originally assigned
identical rule-based rewards. This enables more fine-grained credit assignment
in an effective way. Furthermore, to enhance the accuracy and robustness of
CAPO, we employ voting mechanisms that scale with the number of generated
critiques. Extensive experiments using different backbones like Llama and Qwen
models and in different sizes show that CAPO consistently outperforms
supervised learning-based and RL-based fine-tuning methods across six
challenging mathematical benchmarks and three out-of-domain benchmarks.

</details>


### [127] [Language Model Guided Reinforcement Learning in Quantitative Trading](https://arxiv.org/abs/2508.02366)
*Adam Darmanin,Vince Vella*

Main category: cs.LG

TL;DR: 本文提出了一种混合系统，其中大型语言模型生成高层交易策略来指导强化学习代理。结果表明，LLM引导的代理在回报和风险指标上优于标准RL。


<details>
  <summary>Details</summary>
Motivation: 算法交易需要与长期财务目标一致的短期决策。虽然强化学习（RL）已被探索用于此类战术决策，但其采用受到短视行为和不透明的政策理由的限制。相反，大型语言模型（LLMs）最近在良好设计的提示指导下展示了战略推理和多模态金融信号解释。

Method: 我们提出了一种混合系统，其中LLM生成高层交易策略来指导RL代理的行动。

Result: 通过专家评审评估了LLM生成策略的合理性，并比较了LLM引导代理与无引导基线的夏普比率（SR）和最大回撤（MDD）。

Conclusion: 结果表明，LLM引导的代理在回报和风险指标上优于标准RL。

Abstract: Algorithmic trading requires short-term decisions aligned with long-term
financial goals. While reinforcement learning (RL) has been explored for such
tactical decisions, its adoption remains limited by myopic behavior and opaque
policy rationale. In contrast, large language models (LLMs) have recently
demonstrated strategic reasoning and multi-modal financial signal
interpretation when guided by well-designed prompts.
  We propose a hybrid system where LLMs generate high-level trading strategies
to guide RL agents in their actions. We evaluate (i) the rationale of
LLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and
Maximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results
show improved return and risk metrics over standard RL.

</details>


### [128] [What are you sinking? A geometric approach on attention sink](https://arxiv.org/abs/2508.02546)
*Valeria Ruscio,Umberto Nanni,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 本文研究了变换器中的注意力下沉现象，揭示其与参考框架建立的关系，并分析了不同架构组件的影响。


<details>
  <summary>Details</summary>
Motivation: 理解变换器注意力机制中的注意力下沉现象及其与参考框架的关系，以改进架构设计和对AS的理解。

Method: 通过分析多种架构，识别出三种不同的参考框架类型，并研究它们在训练初期如何作为稳定坐标系统的最优解出现。

Result: 发现了三种参考框架类型：集中式、分布式和双向，并展示了位置编码等架构组件对参考框架类型的影响。

Conclusion: 本文揭示了注意力下沉现象并非架构上的缺陷，而是变换器中建立参考框架的基本几何原理的体现。

Abstract: Attention sink (AS) is a consistent pattern in transformer attention maps
where certain tokens (often special tokens or positional anchors)
disproportionately attract attention from other tokens. We show that in
transformers, AS is not an architectural artifact, but it is the manifestation
of a fundamental geometric principle: the establishment of reference frames
that anchor representational spaces. We analyze several architectures and
identify three distinct reference frame types, centralized, distributed, and
bidirectional, that correlate with the attention sink phenomenon. We show that
they emerge during the earliest stages of training as optimal solutions to the
problem of establishing stable coordinate systems in high-dimensional spaces.
We show the influence of architecture components, particularly position
encoding implementations, on the specific type of reference frame. This
perspective transforms our understanding of transformer attention mechanisms
and provides insights for both architecture design and the relationship with
AS.

</details>


### [129] [Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules](https://arxiv.org/abs/2508.02587)
*Yilun Liu,Yunpu Ma,Yuetian Lu,Shuo Chen,Zifeng Ding,Volker Tresp*

Main category: cs.LG

TL;DR: 本文研究了如何将路由机制融入适应模块，以更好地利用MoE的多专家架构，并通过实验验证了该方法的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）策略未能利用MoE的动态路由机制，因此我们研究适应模块是否应包含路由机制以与MoE的多专家架构对齐。

Method: 分析PEFT应用于MoE语言模型时核心组件的动力学，并研究不同路由策略对适应效果的影响。

Result: 在OLMoE-1B-7B和Mixtral-8x7B上进行的广泛实验验证了我们的路由方法的有效性。

Conclusion: 我们的方法在各种常识和数学推理任务中验证了其性能和效率，并为不同场景提供了最佳配置和实用见解。

Abstract: Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among
their specialized experts, which existing Parameter- Efficient Fine-Tuning
(PEFT) strategies fail to leverage. This motivates us to investigate whether
adaptation modules themselves should incorporate routing mechanisms to align
with MoE's multi-expert architecture. We analyze dynamics of core components
when applying PEFT to MoE language models and examine how different routing
strategies affect adaptation effectiveness. Extensive experiments adapting
OLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks
validate the performance and efficiency of our routed approach. We identify the
optimal configurations for different scenarios and provide empirical analyses
with practical insights to facilitate better PEFT and MoE applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [130] [ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings](https://arxiv.org/abs/2508.01643)
*Ali Shiraee Kasmaee,Mohammad Khodadad,Mehdi Astaraki,Mohammad Arshi Saloot,Nicholas Sherck,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.IR

TL;DR: ChEmbed is a domain-adapted text embedding model designed for chemical literature retrieval. It uses a large dataset of chemistry-specific text and synthetic query-passage pairs to improve retrieval quality. ChEmbed also includes a modified tokenizer with chemically specialized tokens and a longer context length, leading to improved performance compared to existing models.


<details>
  <summary>Details</summary>
Motivation: Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on accurate and relevant retrieval of chemical literature. However, general-purpose text embedding models frequently fail to adequately represent complex chemical terminologies, resulting in suboptimal retrieval quality. Specialized embedding models tailored to chemical literature retrieval have not yet been developed, leaving a substantial performance gap.

Method: ChEmbed is a domain-adapted family of text embedding models fine-tuned on a dataset comprising chemistry-specific text from the PubChem, Semantic Scholar, and ChemRxiv corpora. Large language models are used to synthetically generate queries, resulting in approximately 1.7 million high-quality query-passage pairs. Additionally, the tokenizer is augmented by adding 900 chemically specialized tokens.

Result: Evaluated on our newly introduced ChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp).

Conclusion: ChEmbed represents a practical, lightweight, and reproducible embedding solution that effectively improves retrieval for chemical literature search.

Abstract: Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on
accurate and relevant retrieval of chemical literature. However,
general-purpose text embedding models frequently fail to adequately represent
complex chemical terminologies, resulting in suboptimal retrieval quality.
Specialized embedding models tailored to chemical literature retrieval have not
yet been developed, leaving a substantial performance gap. To address this
challenge, we introduce ChEmbed, a domain-adapted family of text embedding
models fine-tuned on a dataset comprising chemistry-specific text from the
PubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training
data, we employ large language models to synthetically generate queries,
resulting in approximately 1.7 million high-quality query-passage pairs.
Additionally, we augment the tokenizer by adding 900 chemically specialized
tokens to previously unused slots, which significantly reduces the
fragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains
a 8192-token context length, enabling the efficient retrieval of longer
passages compared to many other open-source embedding models, which typically
have a context length of 512 or 2048 tokens. Evaluated on our newly introduced
ChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general
embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents
a practical, lightweight, and reproducible embedding solution that effectively
improves retrieval for chemical literature search.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [131] [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](https://arxiv.org/abs/2505.09805)
*Aditya Nagori,Ayush Gautam,Matthew O. Wiens,Vuong Nguyen,Nathan Kenya Mugisha,Jerome Kabakyenga,Niranjan Kissoon,John Mark Ansermino,Rishikesan Kamaleswaran*

Main category: q-bio.QM

TL;DR: 本研究评估了基于大型语言模型的聚类方法在儿科败血症数据集上的表现，并发现它们优于经典方法，能够更好地捕捉上下文信息并识别不同的患者亚组。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理高维、异构的医疗数据时存在困难，并且缺乏上下文理解。因此，需要一种更有效的聚类方法来帮助个性化护理和资源利用。

Method: 本研究评估了基于大型语言模型（LLM）的聚类方法与经典方法的性能，使用了一个来自低收入国家的儿科败血症数据集。患者记录被序列化为文本，并生成了嵌入向量。应用K均值聚类对这些嵌入进行分析，并与基于UMAP和FAMD降维的K-中值聚类方法进行了比较。

Result: Stella-En-400M-V5模型获得了最高的轮廓系数（0.86）。LLAMA 3.1 8B模型在聚类目标下表现更好，能够识别出具有不同营养、临床和社会经济特征的亚组。基于LLM的方法优于经典技术，因为它们能够捕捉更丰富的上下文并优先考虑关键特征。

Conclusion: 这些结果突显了大型语言模型在资源有限环境中进行上下文表型分析和决策制定的潜力。

Abstract: Clustering patient subgroups is essential for personalized care and efficient
resource use. Traditional clustering methods struggle with high-dimensional,
heterogeneous healthcare data and lack contextual understanding. This study
evaluates Large Language Model (LLM) based clustering against classical methods
using a pediatric sepsis dataset from a low-income country (LIC), containing
2,686 records with 28 numerical and 119 categorical variables. Patient records
were serialized into text with and without a clustering objective. Embeddings
were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with
low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was
applied to these embeddings. Classical comparisons included K-Medoids
clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and
statistical tests evaluated cluster quality and distinctiveness.
Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B
with the clustering objective performed better with higher number of clusters,
identifying subgroups with distinct nutritional, clinical, and socioeconomic
profiles. LLM-based methods outperformed classical techniques by capturing
richer context and prioritizing key features. These results highlight potential
of LLMs for contextual phenotyping and informed decision-making in
resource-limited settings.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [132] [Human Capital Visualization using Speech Amount during Meetings](https://arxiv.org/abs/2508.02075)
*Ekai Hashimoto,Takeshi Mizumoto,Kohei Nagira,Shun Shiramatsu*

Main category: cs.HC

TL;DR: 本研究提出了一种通过分析会议中的发言量来量化人力资本的方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法主要关注易于测量的指标，而忽略了对话在人力资本中的基本作用。本研究旨在解决这一问题。

Method: 使用对话可视化技术来量化发言量，并分析不同属性（如性别和职位）之间的差异，以及发言量与持续属性之间的相关性。

Result: 通过分析中小企业的周会中的发言量，验证了所提出方法的有效性。

Conclusion: 本研究提出了一种通过分析会议中的发言量来可视化人力资本的策略，并验证了其有效性。

Abstract: In recent years, many companies have recognized the importance of human
resources and are investing in human capital to revitalize their organizations
and enhance internal communication, thereby fostering innovation. However,
conventional quantification methods have mainly focused on readily measurable
indicators without addressing the fundamental role of conversations in human
capital. This study focuses on routine meetings and proposes strategies to
visualize human capital by analyzing speech amount during these meetings. We
employ conversation visualization technology, which operates effectively, to
quantify speech. We then measure differences in speech amount by attributes
such as gender and job post, changes in speech amount depending on whether
certain participants are present, and correlations between speech amount and
continuous attributes. To verify the effectiveness of our proposed methods, we
analyzed speech amounts by departmental affiliation during weekly meetings at
small to medium enterprises.

</details>


### [133] [Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits](https://arxiv.org/abs/2508.02328)
*Raj Mahmud,Shlomo Berkovsky,Mukesh Prasad,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: 该研究探讨了影响用户在CRS中交互偏好的因素，发现探索性偏好由享受、有用性、新颖性和对话质量等因素预测，并揭示了五种潜在用户类型。研究结果为CRS用户建模和对话设计提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 探索影响用户交互偏好的因素，以改进CRS的个性化推荐和对话设计。

Method: 进行了一项受试者内研究（N = 139），参与者经历了两种脚本化的CRS对话，对体验进行了评分，并指出了八个系统质量的重要性。使用逻辑回归分析了交互偏好与系统质量之间的关系，并通过聚类分析发现了五种潜在的用户类型。

Result: 发现探索性交互偏好由享受、有用性、新颖性和对话质量预测。意外地，感知有效性也与探索性偏好相关。聚类分析揭示了五种具有不同对话风格偏好的潜在用户类型。年龄、性别和控制偏好显著影响这些选择。

Conclusion: 研究将情感、认知和特质层面的预测因素整合到CRS用户建模中，并为寻求动态适应用户需求的对话系统提供了预测和自适应框架。

Abstract: Conversational Recommender Systems (CRSs) deliver personalised
recommendations through multi-turn natural language dialogue and increasingly
support both task-oriented and exploratory interactions. Yet, the factors
shaping user interaction preferences remain underexplored. In this
within-subjects study (\(N = 139\)), participants experienced two scripted CRS
dialogues, rated their experiences, and indicated the importance of eight
system qualities. Logistic regression revealed that preference for the
exploratory interaction was predicted by enjoyment, usefulness, novelty, and
conversational quality. Unexpectedly, perceived effectiveness was also
associated with exploratory preference. Clustering uncovered five latent user
profiles with distinct dialogue style preferences. Moderation analyses
indicated that age, gender, and control preference significantly influenced
these choices. These findings integrate affective, cognitive, and trait-level
predictors into CRS user modelling and inform autonomy-sensitive,
value-adaptive dialogue design. The proposed predictive and adaptive framework
applies broadly to conversational AI systems seeking to align dynamically with
evolving user needs.

</details>


### [134] [Six Guidelines for Trustworthy, Ethical and Responsible Automation Design](https://arxiv.org/abs/2508.02371)
*Matouš Jelínek,Nadine Schlicker,Ewart de Visser*

Main category: cs.HC

TL;DR: 本文提出了六个设计指南，以帮助设计师优化准确的可信度评估，从而促进伦理和负责任的人机交互。这些指南基于多个领域的现有文献，并结合了语用学中的关键原则。


<details>
  <summary>Details</summary>
Motivation: 用户应该只在系统建议正确时依赖它，在错误时拒绝它。为了实现这一目标，需要准确的可信度评估，以确保用户的感知与系统的实际可信度一致。

Method: 本文通过综合人机交互、认知心理学、自动化研究、用户体验设计和伦理学等领域的现有文献，提出了六个设计指南，并结合了语用学中的关键原则。

Result: 提出的指南为设计师提供了可操作的见解，以创建能够提供相关可信度线索的自动化系统，从而促进校准的信任和更满意、高效和安全的人机交互。此外，这些启发式方法可能作为评估现有系统是否使用户能够准确评估系统可信度的工具。

Conclusion: 本文提出了六个设计指南，以帮助设计师优化准确的可信度评估，从而促进伦理和负责任的人机交互。这些指南基于多个领域的现有文献，并结合了语用学中的关键原则，如共同基础和格赖斯交际准则。

Abstract: Calibrated trust in automated systems (Lee and See 2004) is critical for
their safe and seamless integration into society. Users should only rely on a
system recommendation when it is actually correct and reject it when it is
factually wrong. One requirement to achieve this goal is an accurate
trustworthiness assessment, ensuring that the user's perception of the system's
trustworthiness aligns with its actual trustworthiness, allowing users to make
informed decisions about the extent to which they can rely on the system
(Schlicker et al. 2022). We propose six design guidelines to help designers
optimize for accurate trustworthiness assessments, thus fostering ethical and
responsible human-automation interactions. The proposed guidelines are derived
from existing literature in various fields, such as human-computer interaction,
cognitive psychology, automation research, user-experience design, and ethics.
We are incorporating key principles from the field of pragmatics, specifically
the cultivation of common ground (H. H. Clark 1996) and Gricean communication
maxims (Grice 1975). These principles are essential for the design of automated
systems because the user's perception of the system's trustworthiness is shaped
by both environmental contexts, such as organizational culture or societal
norms, and by situational context, including the specific circumstances or
scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed
guidelines provide actionable insights for designers to create automated
systems that make relevant trustworthiness cues available. This would ideally
foster calibrated trust and more satisfactory, productive, and safe
interactions between humans and automated systems. Furthermore, the proposed
heuristics might work as a tool for evaluating to what extent existing systems
enable users to accurately assess a system's trustworthiness.

</details>


### [135] [AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration](https://arxiv.org/abs/2508.02470)
*Hyunjn An,Yongwon Kim,Wonduk Seo,Joonil Park,Daye Kang,Changhoon Oh,Dokyun Kim,Seunghyun Lee*

Main category: cs.HC

TL;DR: AIAP是一个无代码平台，通过自然语言输入和可视化工作流帮助非专家更轻松地设计AI服务。


<details>
  <summary>Details</summary>
Motivation: 许多工具可用于设计AI，但非专家在清晰表达意图和管理系统复杂性方面仍面临挑战。

Method: AIAP是一个无代码平台，结合自然语言输入与可视化工作流，利用协调的多智能体系统将模糊的用户指令分解为模块化的可操作步骤。

Result: 一项涉及32名参与者的用户研究显示，AIAP的AI生成建议、模块化工作流和自动识别数据、动作和上下文显著提高了参与者直观开发服务的能力。

Conclusion: 自然语言为基础的可视化编程显著降低了AI服务设计的障碍并提升了用户体验。

Abstract: While many tools are available for designing AI, non-experts still face
challenges in clearly expressing their intent and managing system complexity.
We introduce AIAP, a no-code platform that integrates natural language input
with visual workflows. AIAP leverages a coordinated multi-agent system to
decompose ambiguous user instructions into modular, actionable steps, hidden
from users behind a unified interface. A user study involving 32 participants
showed that AIAP's AI-generated suggestions, modular workflows, and automatic
identification of data, actions, and context significantly improved
participants' ability to develop services intuitively. These findings highlight
that natural language-based visual programming significantly reduces barriers
and enhances user experience in AI service design.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [136] [HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents](https://arxiv.org/abs/2508.02629)
*Yibin Liu,Zhixuan Liang,Zanxin Chen,Tianxing Chen,Mengkang Hu,Wanxi Dong,Congsheng Xu,Zhaoming Han,Yusen Qin,Yao Mu*

Main category: cs.RO

TL;DR: 本文介绍了HyCodePolicy，这是一种混合语言控制框架，能够将代码合成、几何定位、感知监控和迭代修复整合到一个闭环编程周期中，从而提高机器人操作策略的鲁棒性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的系统缺乏有效的机制来适应性地监控政策执行并在任务完成期间修复代码。

Method: HyCodePolicy是一种混合语言控制框架，系统地将代码合成、几何定位、感知监控和迭代修复整合到一个闭环编程周期中。给定自然语言指令，系统首先将其分解为子目标并生成基于对象中心几何原语的初始可执行程序。然后在模拟中执行程序，同时使用视觉语言模型（VLM）观察选定的检查点以检测和定位执行失败并推断失败原因。通过融合捕捉程序级事件的结构化执行轨迹与基于VLM的感知反馈，HyCodePolicy推断失败原因并修复程序。

Result: HyCodePolicy显著提高了机器人操作策略的鲁棒性和样本效率。

Conclusion: HyCodePolicy显著提高了机器人操作策略的鲁棒性和样本效率，为将多模态推理整合到自主决策流程中提供了一种可扩展的策略。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
richer perceptual grounding for code policy generation in embodied agents.
However, most existing systems lack effective mechanisms to adaptively monitor
policy execution and repair codes during task completion. In this work, we
introduce HyCodePolicy, a hybrid language-based control framework that
systematically integrates code synthesis, geometric grounding, perceptual
monitoring, and iterative repair into a closed-loop programming cycle for
embodied agents. Technically, given a natural language instruction, our system
first decomposes it into subgoals and generates an initial executable program
grounded in object-centric geometric primitives. The program is then executed
in simulation, while a vision-language model (VLM) observes selected
checkpoints to detect and localize execution failures and infer failure
reasons. By fusing structured execution traces capturing program-level events
with VLM-based perceptual feedback, HyCodePolicy infers failure causes and
repairs programs. This hybrid dual feedback mechanism enables self-correcting
program synthesis with minimal human supervision. Our results demonstrate that
HyCodePolicy significantly improves the robustness and sample efficiency of
robot manipulation policies, offering a scalable strategy for integrating
multimodal reasoning into autonomous decision-making pipelines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [137] [CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase](https://arxiv.org/abs/2508.01791)
*Fatimah Mohamed Emad Elden*

Main category: cs.CV

TL;DR: 本文提出了一种以数据为中心的方法，旨在解决连续手语识别中的挑战，并通过改进的特征选择、预处理流程和新的CSLRConformer架构取得了良好的结果。


<details>
  <summary>Details</summary>
Motivation: 解决签名者独立识别的问题，以提高CSLR系统在不同签名者之间的泛化能力。

Method: 提出了一种以数据为中心的方法，包括系统特征工程、稳健的预处理流程和优化的模型架构。关键贡献包括由探索性数据分析（EDA）指导的原则性特征选择过程，结合DBSCAN基于的异常值过滤和空间归一化的严格预处理流程，以及新颖的CSLRConformer架构。

Result: 所提出的方法在开发集上实现了5.60%的词错误率（WER），在测试集上实现了12.01%的词错误率，在官方比赛平台上获得了第三名的排名。

Conclusion: 本研究验证了跨领域架构适应的有效性，展示了原始为语音识别设计的Conformer模型可以成功重新用于建立基于关键点的CSLR的新最先进性能。

Abstract: The field of Continuous Sign Language Recognition (CSLR) poses substantial
technical challenges, including fluid inter-sign transitions, the absence of
temporal boundaries, and co-articulation effects. This paper, developed for the
MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of
signer-independent recognition to advance the generalization capabilities of
CSLR systems across diverse signers. A data-centric methodology is proposed,
centered on systematic feature engineering, a robust preprocessing pipeline,
and an optimized model architecture. Key contributions include a principled
feature selection process guided by Exploratory Data Analysis (EDA) to isolate
communicative keypoints, a rigorous preprocessing pipeline incorporating
DBSCAN-based outlier filtering and spatial normalization, and the novel
CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer
design of the Conformer model, leveraging its capacity to model local temporal
dependencies and global sequence context; a characteristic uniquely suited for
the spatio-temporal dynamics of sign language. The proposed methodology
achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on
the development set and 12.01% on the test set, a result that secured a 3rd
place ranking on the official competition platform. This research validates the
efficacy of cross-domain architectural adaptation, demonstrating that the
Conformer model, originally conceived for speech recognition, can be
successfully repurposed to establish a new state-of-the-art performance in
keypoint-based CSLR.

</details>


### [138] [Subject or Style: Adaptive and Training-Free Mixture of LoRAs](https://arxiv.org/abs/2508.02165)
*Jia-Chen Zhang,Yu-Jie Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的自适应LoRA融合方法EST-LoRA，它通过考虑矩阵能量、风格差异分数和时间步数三个关键因素，在每个注意力层中自适应地选择主题LoRA和风格LoRA，从而实现了更好的平衡和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在平衡原始主题和风格方面存在困难，并且通常需要额外的训练。K-LoRA虽然提出了一种无需训练的LoRA融合方法，但涉及多个超参数，难以适应所有风格和主题。

Method: EST-LoRA是一种无需训练的自适应LoRA融合方法，它综合考虑了矩阵能量、风格差异分数和时间步数三个关键因素。类似于专家混合（MoE）架构，模型在每个注意力层中自适应地在主题LoRA和风格LoRA之间进行选择。

Result: 实验结果表明，EST-LoRA在定性和定量评估中均优于最先进的方法，并且相比其他高效的融合方法具有更快的生成速度。

Conclusion: EST-LoRA在定性和定量评估中均优于最先进的方法，并且相比其他高效的融合方法具有更快的生成速度。

Abstract: Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable
performance in subject-driven or style-driven generation tasks. Studies have
explored combinations of different LoRAs to jointly generate learned styles and
content. However, current methods struggle to balance the original subject and
style, and often require additional training. Recently, K-LoRA proposed a
training-free LoRA fusion method. But it involves multiple hyperparameters,
making it difficult to adapt to all styles and subjects. In this paper, we
propose EST-LoRA, a training-free adaptive LoRA fusion method. It
comprehensively considers three critical factors: \underline{E}nergy of matrix,
\underline{S}tyle discrepancy scores and \underline{T}ime steps. Analogous to
the Mixture of Experts (MoE) architecture, the model adaptively selects between
subject LoRA and style LoRA within each attention layer. This integrated
selection mechanism ensures balanced contributions from both components during
the generation process. Experimental results show that EST-LoRA outperforms
state-of-the-art methods in both qualitative and quantitative evaluations and
achieves faster generation speed compared to other efficient fusion approaches.
Our code is publicly available at:
https://anonymous.4open.science/r/EST-LoRA-F318.

</details>


### [139] [Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens](https://arxiv.org/abs/2508.02419)
*Haohan Zheng,Zhenguo Zhang*

Main category: cs.CV

TL;DR: 本文研究了大型视觉语言模型（LVLMs）中的物体幻觉问题，并发现了一个之前被忽视的现象——模态偏差。基于此，我们提出了一种简单但有效的训练-free 方法来减轻幻觉，通过调整注意力权重和采用对比解码策略，显著提高了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）虽然展示了出色的多模态理解和推理能力，但仍遭受严重的物体幻觉。先前的研究主要将这一缺陷归因于视觉编码器和大型语言模型（LLMs）之间的规模不匹配导致的语言先验。然而，通过深入研究幻觉机制，我们实证揭示了一个之前被忽视的现象：LVLMs可能在幻觉期间不仅忽略视觉信息，还忽略文本模态，这种行为称为模态偏差，表明LVLMs难以同时关注视觉和文本模态，导致对用户提供的指令的理解碎片化。

Method: 我们提出了一种简单但有效的训练-free 方法来减轻对象幻觉。具体来说，我们干预并调整文本和视觉标记的注意力权重，平衡跨模态兼容性以更好地与用户意图对齐。此外，我们采用对比解码策略来减少LVLM对其参数知识的过度依赖，协同增强我们的注意力操作。

Result: 广泛的实验确认了模态偏差在LVLMs中的普遍存在。值得注意的是，我们的方法在多个开放源代码LVLMs和基准测试中有效减轻了幻觉，突显了其普遍性和有效性。

Conclusion: 我们的方法在多个开源LVLMs和基准测试中有效减轻了幻觉，突显了其普遍性和有效性。

Abstract: Large vision-language models (LVLMs) have demonstrated remarkable multimodal
comprehension and reasoning capabilities, but they still suffer from severe
object hallucination. Previous studies primarily attribute the flaw to
linguistic prior caused by the scale mismatch between visual encoders and large
language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon
LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,
generating descriptions inconsistent with visual cues. However, through an
in-depth investigation of the hallucinated mechanisms, we empirically reveal a
previously overlooked phenomenon: LVLMs may ignore not only visual information
but also textual modality during hallucination, a behavior termed as modality
bias, which indicates that LVLMs struggle to simultaneously attend to both
visual and textual modalities, leading to fragmented understanding of
user-provided instructions. Based on this observation, we propose a simple yet
effective training-free method to mitigate object hallucination. Concretely, we
intervene and adjust the attention weights of textual and visual tokens,
balancing cross-modal compatibility for better alignment with user intentions.
Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's
overreliance on its parametric knowledge, synergistically enhancing our
attention manipulation. Extensive experiments confirm the widespread presence
of modality bias in LVLMs. Notably, our method effectively mitigates
hallucination across multiple open-source LVLMs and benchmarks, highlighting
its generalizability and efficacy.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [140] [Dialogue Systems Engineering: A Survey and Future Directions](https://arxiv.org/abs/2508.02279)
*Mikio Nakano,Hironori Takeuchi,Sadahiro Yoshikawa,Yoichi Matsuyama,Kazunori Komatani*

Main category: cs.SE

TL;DR: 本文提出对话系统工程概念，基于软件工程知识体系进行调查，并探讨其未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，对话系统的核心技术取得了显著进展，因此需要构建、运营和持续改进对话系统以解决各种社会问题和商业场景。

Method: 本文基于软件工程知识体系（SWEBOK 4.0）列举了对话系统工程的知识领域，并对每个领域进行了调查。

Result: 本文调查了对话系统工程的各个知识领域，并识别了各领域中未探索的主题。

Conclusion: 本文提出了对话系统工程的概念，并基于软件工程的知识领域对对话系统工程进行了调查，同时讨论了其未来方向。

Abstract: This paper proposes to refer to the field of software engineering related to
the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys
this field while also discussing its future directions. With the advancement of
large language models, the core technologies underlying dialogue systems have
significantly progressed. As a result, dialogue system technology is now
expected to be applied to solving various societal issues and in business
contexts. To achieve this, it is important to build, operate, and continuously
improve dialogue systems correctly and efficiently. Accordingly, in addition to
applying existing software engineering knowledge, it is becoming increasingly
important to evolve software engineering tailored specifically to dialogue
systems. In this paper, we enumerate the knowledge areas of dialogue systems
engineering based on those of software engineering, as defined in the Software
Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based
on this survey, we identify unexplored topics in each area and discuss the
future direction of dialogue systems engineering.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [141] [The Attribution Crisis in LLM Search Results](https://arxiv.org/abs/2508.00838)
*Ilan Strauss,Jangho Yang,Tim O'Reilly,Sruly Rosenblat,Isobel Moure*

Main category: cs.DL

TL;DR: 论文分析了Web-enabled LLMs在回答查询时的引用问题，发现存在三个利用模式：无搜索、无引用和高流量低引用。研究结果表明，检索设计而非技术限制影响生态系统影响，并建议采用透明的LLM搜索架构。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决Web-enabled LLMs在回答查询时缺乏对所消费网页的引用问题，即“引用差距”问题。

Method: 论文使用了大约14,000个真实世界的LMArena对话日志，分析了具有搜索功能的LLM系统，并利用负二项式障碍模型进行研究。

Result: 研究发现，34%的Google Gemini和24%的OpenAI GPT-4o响应是在没有明确获取任何在线内容的情况下生成的；Gemini在92%的答案中没有提供可点击的引用来源；Perplexity的Sonar每查询访问约10个相关页面但只引用三到四个。

Conclusion: 论文建议基于标准化遥测和完整搜索痕迹和引用日志披露的透明LLM搜索架构。

Abstract: Web-enabled LLMs frequently answer queries without crediting the web pages
they consume, creating an "attribution gap" - the difference between relevant
URLs read and those actually cited. Drawing on approximately 14,000 real-world
LMArena conversation logs with search-enabled LLM systems, we document three
exploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI
GPT-4o responses are generated without explicitly fetching any online content;
2) No citation: Gemini provides no clickable citation source in 92% of answers;
3) High-volume, low-credit: Perplexity's Sonar visits approximately 10 relevant
pages per query but cites only three to four. A negative binomial hurdle model
shows that the average query answered by Gemini or Sonar leaves about 3
relevant websites uncited, whereas GPT-4o's tiny uncited gap is best explained
by its selective log disclosures rather than by better attribution. Citation
efficiency - extra citations provided per additional relevant web page visited
- varies widely across models, from 0.19 to 0.45 on identical queries,
underscoring that retrieval design, not technical limits, shapes ecosystem
impact. We recommend a transparent LLM search architecture based on
standardized telemetry and full disclosure of search traces and citation logs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [142] [Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe](https://arxiv.org/abs/2508.01691)
*Tiantian Feng,Kevin Huang,Anfeng Xu,Xuan Shi,Thanathai Lertpetchpun,Jihwan Lee,Yoonjeong Lee,Dani Byrd,Shrikanth Narayanan*

Main category: cs.SD

TL;DR: Voxlect is a new benchmark for modeling dialects and regional languages worldwide, providing a comprehensive evaluation of speech foundation models and enabling various downstream applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a comprehensive benchmark for modeling dialects and regional languages worldwide, which can be used to evaluate speech foundation models and enable various downstream applications.

Method: The paper presents Voxlect, a benchmark for modeling dialects and regional languages, using speech foundation models. It evaluates the performance of several widely used speech foundation models in classifying speech dialects and assesses their robustness under noisy conditions.

Result: The paper reports comprehensive benchmark evaluations on dialects and regional language varieties in multiple languages, including English, Arabic, Mandarin, Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. It also demonstrates the application of Voxlect in augmenting speech recognition datasets with dialect information and evaluating speech generation systems.

Conclusion: Voxlect is a valuable resource for modeling dialects and regional languages worldwide, offering a comprehensive benchmark for evaluating speech foundation models and enabling various downstream applications.

Abstract: We present Voxlect, a novel benchmark for modeling dialects and regional
languages worldwide using speech foundation models. Specifically, we report
comprehensive benchmark evaluations on dialects and regional language varieties
in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,
Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over
2 million training utterances from 30 publicly available speech corpora that
are provided with dialectal information. We evaluate the performance of several
widely used speech foundation models in classifying speech dialects. We assess
the robustness of the dialectal models under noisy conditions and present an
error analysis that highlights modeling results aligned with geographic
continuity. In addition to benchmarking dialect classification, we demonstrate
several downstream applications enabled by Voxlect. Specifically, we show that
Voxlect can be applied to augment existing speech recognition datasets with
dialect information, enabling a more detailed analysis of ASR performance
across dialectal variations. Voxlect is also used as a tool to evaluate the
performance of speech generation systems. Voxlect is publicly available with
the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.

</details>


### [143] [Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers](https://arxiv.org/abs/2508.02175)
*Liang Lin,Miao Yu,Kaiwen Luo,Yibo Zhang,Lilan Peng,Dexian Wang,Xuehai Tang,Yuanhe Zhang,Xikang Yang,Zhenhong Zhou,Kun Wang,Yang Liu*

Main category: cs.SD

TL;DR: 本文研究了ALLM是否容易受到利用声学触发器的后门攻击，并发现现有ALLMs在音频特征基础上的触发器方面存在严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 由于音频的独特特性，现有的研究主要集中在文本和视觉安全上，而音频安全的研究较少，因此本文旨在探讨ALLM是否容易受到利用声学触发器的后门攻击。

Method: 本文引入了Hidden in the Noise (HIN)框架，通过在原始音频波形上进行声学修改来实施后门攻击，并开发了AudioSafe基准测试来评估ALLMs的鲁棒性。

Result: 实验结果表明，现有的ALLMs在音频特征如环境噪声和语音速率变化方面的攻击成功率超过90%，并且对音量作为触发器的响应最小，同时中毒样本的包含仅导致损失曲线的微小波动。

Conclusion: 本文结论是，现有的ALLMs在音频特征基础上的触发器方面存在严重的脆弱性，这表明需要加强音频安全措施。

Abstract: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech
processing, their safety implications demand urgent attention. While
considerable research has explored textual and vision safety, audio's distinct
characteristics present significant challenges. This paper first investigates:
Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In
response to this issue, we introduce Hidden in the Noise (HIN), a novel
backdoor attack framework designed to exploit subtle, audio-specific features.
HIN applies acoustic modifications to raw audio waveforms, such as alterations
to temporal dynamics and strategic injection of spectrally tailored noise.
These changes introduce consistent patterns that an ALLM's acoustic feature
encoder captures, embedding robust triggers within the audio stream. To
evaluate ALLM robustness against audio-feature-based triggers, we develop the
AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments
on AudioSafe and three established safety datasets reveal critical
vulnerabilities in existing ALLMs: (I) audio features like environment noise
and speech rate variations achieve over 90% average attack success rate. (II)
ALLMs exhibit significant sensitivity differences across acoustic features,
particularly showing minimal response to volume as a trigger, and (III)
poisoned sample inclusion causes only marginal loss curve fluctuations,
highlighting the attack's stealth.

</details>

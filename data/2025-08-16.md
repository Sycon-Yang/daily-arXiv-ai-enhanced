<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 本文总结了在医疗环境中实施NLP解决方案的关键经验，包括问题定义、迭代开发、跨学科合作、数据质量、模型选择和错误缓解策略，为医疗组织提供指导以提升数据管理和患者护理。


<details>
  <summary>Details</summary>
Motivation: 自动化从临床文档中提取数据可以显著提高医疗环境中的效率，但部署自然语言处理（NLP）解决方案存在实际挑战。

Method: 本文基于在British Columbia Cancer Registry (BCCR) 实施各种NLP模型进行信息提取和分类任务的经验，分享了项目生命周期中的关键教训。

Result: 本文强调了根据明确的业务目标定义问题的重要性，采用迭代开发方法，并促进跨学科合作和共同设计，包括领域专家、终端用户和机器学习专家的参与。此外，还提出了需要关注数据质量、模型选择、错误缓解策略以及组织AI素养等实践性见解。

Conclusion: 本文提供了在医疗领域成功实施AI/NLP解决方案的实用考虑因素，这些因素可以推广到癌症登记处以外的其他场景，有助于提升数据管理流程并最终改善患者护理和公共卫生结果。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 本文提出了一种基于区块链的透明评估协议，用于评估开源大型语言模型的公平性，确保可验证、不可变和可重复的评估，并展示了在多个数据集和语言上的实验结果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在现实世界应用中的日益普及，对其公平性的担忧仍然存在，特别是在刑事司法、教育、医疗保健和金融等高风险领域。因此，需要一种可靠且可验证的方法来评估这些模型的公平性。

Method: 本文提出了一种基于区块链的透明评估协议，利用智能合约在Internet Computer Protocol (ICP)区块链上执行链上HTTP请求，以访问托管在Hugging Face上的端点，并将数据集、提示和指标直接存储在链上。

Result: 本文对Llama、DeepSeek和Mistral模型在PISA数据集上进行了基准测试，该数据集适用于使用统计平等和机会平等度量进行公平性评估。此外，还评估了从StereoSet数据集中派生的结构化上下文关联度量，以衡量上下文关联中的社会偏见。多语言评估显示了跨语言的差异。

Conclusion: 本文提出了一种透明的评估协议，用于在Internet Computer Protocol (ICP)区块链上使用智能合约对开源大型语言模型（LLMs）的公平性进行基准测试。该方法确保了可验证、不可变和可重复的评估，并通过链上HTTP请求执行，将数据集、提示和指标直接存储在链上。所有代码和结果都是开源的，使得社区审计和跨模型版本的长期公平性跟踪成为可能。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 本研究通过一种新颖的主题建模方法分析了课堂中未成年人的匿名交互数据，并将信息分为内容和任务两个维度。我们的分析产生了新的应用，并发现传统方法在分析大量文本时表现不佳，因此我们应用了最先进的LLMs来实现更好的结果。


<details>
  <summary>Details</summary>
Motivation: 先前的研究大多缺乏内容或主题分类。虽然任务分类在教育中更为普遍，但大多数都没有得到K-12真实数据的支持。

Method: 我们采用了一种新颖的简单主题建模方法，对课堂中未成年人的匿名交互数据进行了分析，并将超过17,000条由学生、教师和ChatGPT生成的信息分为两个维度：内容（如自然和人物）和任务（如写作和解释）。

Result: 我们的分析产生了一些新的应用。我们发现许多已建立的经典和新兴计算方法在分析大量文本时表现不佳，导致我们直接应用了经过适当预处理的状态最先进LLMs来实现更好的人类对齐的层次化主题结构。

Conclusion: 我们的研究支持研究人员、教师和学生在丰富使用GenAI方面，同时讨论也突出了未来研究的一些担忧和开放性问题。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 本文介绍了INTIMA基准，用于评估语言模型中的陪伴行为，并发现不同模型在陪伴行为上的表现存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究AI陪伴行为的评估方法，以促进用户福祉。

Method: 基于心理学理论和用户数据，开发了一个包含31种行为的分类体系，并通过368个针对性提示进行评估。

Result: 所有模型中，支持陪伴的行为更为常见，但不同模型在敏感部分的表现存在显著差异。

Conclusion: 需要更一致的方法来处理情绪化互动。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 本文提出XFacta数据集并评估基于MLLM的虚假信息检测方法，提供有价值见解以推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集存在过时事件或人工合成的问题，无法反映现实世界的虚假信息模式，且缺乏对MLLM模型设计策略的全面分析。

Method: 我们引入了XFacta，一个更适合评估MLLM检测器的当代真实世界数据集，并系统地评估了各种基于MLLM的虚假信息检测策略，同时构建了一个半自动检测循环框架以持续更新XFacta。

Result: 我们系统地评估了不同架构和规模的MLLM模型，并与现有检测方法进行了对比，同时通过分析提供了有价值的见解和实践。

Conclusion: 我们的分析为推进多模态虚假信息检测领域提供了有价值的见解和实践。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型生成合成数据并改进文本分类模型性能的方法，通过自动化的工作流程和集成算法实现更好的效果。


<details>
  <summary>Details</summary>
Motivation: 在开发文本分类模型时，一个主要挑战是难以收集所有文本类别的足够数据。本文旨在利用大型语言模型生成合成数据，以提高模型性能，而无需等待更多真实数据被收集和标记。

Method: 本文提出了一种自动化的工作流程，该流程搜索导致更有效的合成数据的输入示例，并研究了三种搜索策略，使用实验结果来指导一个根据类别特征选择搜索策略的集成算法。

Result: 实验结果表明，集成方法比自动化工作流程中的每个单独策略更有效。

Conclusion: 本文提出的集成方法在使用LLMs改进分类模型方面比每个单独的策略更有效。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 本文介绍了HiFACT数据集，并提出了一种基于图的检索增强事实检查模型，用于处理多语言、代码混合的Hinglish文本，实验结果表明该模型优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在像印度这样的语言多样地区，需要强大的多语言和上下文感知的事实检查工具，以应对政治言论中的事实核查挑战。

Method: 提出了一种基于图的检索增强事实检查模型，结合了多语言上下文编码、声明-证据语义对齐、证据图构建、图神经推理和自然语言解释生成。

Result: HiFACTMix在与最先进的多语言基线模型比较中表现出更高的准确性，并为其判断提供了忠实的依据。

Conclusion: 本研究为多语言、代码混合和政治基础的事实验证研究开辟了新的方向。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型中的语义结构与人类语言中的语义结构相似，且语义信息在很大程度上是低维的。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨LLMs中的语义结构是否与人类语言中的语义结构相似，以及这种结构是否可以被利用来避免在调整特征时产生意外后果。

Method: 通过分析大型语言模型（LLMs）嵌入矩阵中的语义关联，研究了词在由反义对定义的语义方向上的投影与人类评分的相关性，并探索了这些投影如何减少到一个三维子空间。

Result: 研究发现，LLMs嵌入中的语义方向投影与人类评分高度相关，并且这些投影可以减少到一个三维子空间，这与从人类调查数据中得出的模式非常相似。此外，沿一个语义方向移动标记会导致与几何对齐特征成比例的副作用。

Conclusion: 这些发现表明，语义特征在LLMs中以类似于人类语言的方式纠缠在一起，尽管语义信息看起来复杂，但其本质上是低维的。此外，考虑到这种语义结构对于避免在调整特征时产生意外后果可能至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [9] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 研究评估了注意力权重在生物医学文档分类中的解释作用，并发现其视觉呈现方式显著影响用户对其有用性的感知。


<details>
  <summary>Details</summary>
Motivation: 在循证医学中，注意力权重可能有助于医生理解AI系统对生物医学文献的分类，但目前尚无共识认为注意力权重提供了有用的解释。此外，很少有研究探讨可视化注意力如何影响其作为解释辅助工具的效果。

Method: 我们进行了一项用户研究，让不同领域的医学专家根据研究设计对文章进行分类，并评估他们对基于注意力的解释的使用情况和偏好。

Result: Transformer模型（XLNet）准确地对文档进行了分类；然而，注意力权重未被普遍认为是解释预测的有用工具。然而，这种感知因注意力的可视化方式而显著不同。

Conclusion: 我们的研究结果表明，注意力权重本身可能并不总是有助于解释模型的预测，但它们的视觉呈现方式会影响用户对其有用性的感知。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [10] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 本文介绍了EQGBench，这是一个用于评估大型语言模型在中文教育问题生成中的性能的基准，旨在推动教育问题生成的发展。


<details>
  <summary>Details</summary>
Motivation: 为了推进教育问题生成（EQG）并使LLMs生成具有教学价值和教育效果的问题，需要一个专门的基准来评估其性能。

Method: 引入EQGBench，这是一个专门设计用于评估LLMs在中文教育问题生成（EQG）中的性能的全面基准。

Result: EQGBench建立了一个由900个评估样本组成的数据集，涵盖三个基础中学学科：数学、物理和化学，并模拟了现实的教育场景。

Conclusion: 通过系统评估46个主流大模型，我们揭示了在生成反映教育价值和促进学生综合能力的问题方面还有很大的发展空间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [11] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型是否可以自动化AIHQ开放性回答的评分，结果表明模型生成的评分与人类评分一致，具有潜在的应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统的AIHQ开放性问题需要耗时的人工评分，因此我们试图评估大型语言模型是否可以自动化AIHQ开放性回答的评分。

Method: 我们使用了一个先前收集的数据集，其中包含有创伤性脑损伤（TBI）和健康对照组（HC）完成的AIHQ，并对其开放性回答进行了人工评分。我们使用了一半的回答来微调两个模型，然后在剩余的一半AIHQ回答上测试了微调后的模型。

Result: 结果显示，模型生成的评分与人类评分一致，微调后的模型表现出更高的一致性。这种一致性在模糊、有意和意外的情景类型中都是一致的，并且复制了之前关于TBI和HC组在敌意和攻击性反应上的群体差异。微调后的模型还很好地推广到了一个独立的非临床数据集。

Conclusion: 我们的研究结果表明，大型语言模型可以简化AIHQ的评分，在研究和临床环境中都有潜力促进心理评估。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [12] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 该论文提出了一种基于贝叶斯融合的方法，用于自动维护在线课程的讨论论坛，避免了频繁微调大型语言模型的高昂成本，并证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动维护在线课程的讨论论坛需要持续更新，而频繁微调大型语言模型是一个资源密集的过程。

Method: 该论文提出了贝叶斯融合方法，结合预训练通用大型语言模型和本地数据训练的分类器的多维分类得分。

Result: 实验结果表明，所提出的融合方法在性能上优于单独的分类器，并且与微调方法相当。

Conclusion: 该论文提出了一种使用贝叶斯融合的方法，以避免频繁微调大型语言模型的高昂成本，并证明了这种方法在性能上优于单独的分类器，并与微调方法相当。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [13] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 本文提出了一种名为S-MoE的新方法，用于解决硬参数共享导致的任务干扰问题，通过特殊引导标记路由任务到指定专家，显著提升了语音识别和语音翻译的性能。


<details>
  <summary>Details</summary>
Motivation: 硬参数共享在跨不同任务训练模型时会导致任务干扰，影响整体模型性能。因此，需要一种更有效的策略来解决这一问题。

Method: 提出了一种名为监督专家混合（S-MoE）的方法，该方法利用特殊的引导标记来为每个任务路由到其指定的专家，从而消除了训练门控函数的需要。

Result: 实验结果表明，S-MoE在编码器和解码器上应用时，实现了Word Error Rate (WER) 的6.35%相对改进。

Conclusion: S-MoE通过将每个任务分配给单独的前馈网络，克服了硬参数共享的局限性，并在语音识别和语音翻译任务中取得了显著的效果提升。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [14] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: This paper explores the use of LLMs in detecting and preventing the spread of misinformation, highlighting their potential to contribute to a healthier information ecosystem.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are capable of generating harmful misinformation, either inadvertently or through 'jailbreak' attacks. There is potential for LLMs to be used in detecting and preventing the spread of misinformation.

Method: We investigated the efficacy and characteristics of LLM-produced jailbreak attacks that cause other models to produce harmful medical misinformation. We also studied how misinformation generated by jailbroken LLMs compares to typical misinformation found on social media and how effectively it can be detected using standard machine learning approaches.

Result: Our findings show that LLMs can be effectively used to detect misinformation from both other LLMs and from people. Additionally, we found that misinformation generated by jailbroken LLMs can be compared to health-related misinformation on Reddit.

Conclusion: LLMs can be effectively used to detect misinformation from both other LLMs and from people, and with careful design, they can contribute to a healthier overall information ecosystem.

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [15] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 本研究评估了基于大型语言模型的生成式AI在营养学学生学习辅助工具方面的潜力。结果显示，某些模型勉强超过及格阈值，但整体准确性和答案一致性仍然不足。所有模型在答案一致性和鲁棒性方面都有显著的局限性，需要进一步发展以确保可靠的AI学习辅助工具。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（AI）基于大型语言模型（LLMs），如ChatGPT，在医学和教育等专业领域显示出显著进展。然而，它们在营养教育中的表现，特别是在日本注册营养师国家考试中的表现，仍缺乏研究。本研究旨在评估当前基于LLM的生成式AI模型作为营养学学生学习辅助工具的潜力。

Method: 本研究使用日本注册营养师国家考试的问题作为提示，评估了ChatGPT和三个Bing模型（Precise、Creative、Balanced）的表现。每个问题都在独立会话中输入，并分析了模型响应的准确性、一致性和响应时间。还测试了额外的提示工程，包括角色分配，以评估潜在的性能改进。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超过了及格阈值（60%），而Bing-Balanced（43.3%）和ChatGPT（42.8%）未达到。Bing-Precise和Bing-Creative在除营养教育以外的各个学科领域通常优于其他模型。所有模型在重复尝试中都没有始终提供相同的正确回答，突显了答案稳定性的限制。ChatGPT在回答模式上表现出更大的一致性，但准确性较低。提示工程的影响很小，除非明确提供正确答案和解释时有适度的改善。

Conclusion: 虽然一些生成式AI模型勉强超过了及格阈值，但整体准确性和答案一致性仍然不理想。此外，所有模型在答案一致性和鲁棒性方面都表现出显著的局限性。需要进一步的发展以确保可靠的和稳定的基于AI的学习辅助工具用于注册营养师执照准备。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [16] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架GG Explore，通过引入Guidance Graph来桥接非结构化查询和结构化知识检索，从而解决现有方法的不足。该框架包括Structural Alignment和Context Aware Pruning，实验证明其在复杂任务上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然表现出强大的语言能力，但它们对静态知识的依赖和不透明的推理过程限制了它们在知识密集型任务中的表现。知识图谱（KGs）提供了一个有希望的解决方案，但当前的探索方法面临一个根本性的权衡：问题引导的方法由于粒度不匹配而产生冗余探索，而线索引导的方法在复杂场景中未能有效利用上下文信息。

Method: 我们提出了Guidance Graph guided Knowledge Exploration (GG Explore)，一个引入中间Guidance Graph来连接非结构化查询和结构化知识检索的新框架。基于Guidance Graph，我们开发了：(1) Structural Alignment，无需LLM开销即可过滤不兼容的候选者，以及(2) Context Aware Pruning，通过图约束强制语义一致性。

Result: 广泛的实验表明，我们的方法在效率方面表现出色，并且在复杂任务上超过了最先进的技术，同时保持了较小的LLM的强大性能。

Conclusion: 我们的方法在复杂任务上表现出色，同时保持了较小的LLM的强大性能，证明了其实际价值。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [17] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: Semantic Bridge is a framework for generating complex, multi-hop reasoning questions from sparse sources, improving upon existing methods by enabling controllable QA generation with higher quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Large language model (LLM) training faces a critical bottleneck: the scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents. Existing methods rely on surface patterns, fundamentally failing to generate controllable, complex multi-hop reasoning questions that test genuine understanding.

Method: Semantic Bridge is a universal framework for controllably generating sophisticated multi-hop reasoning questions from arbitrary sources. It uses semantic graph weaving with three complementary bridging mechanisms: entity bridging, predicate chain bridging, and causal bridging, along with AMR-driven analysis for fine-grained control over complexity and types.

Result: Semantic Bridge achieves up to 9.5% better round-trip quality, enabling production-ready controllable QA generation. It demonstrates consistent 18.3%-25.4% gains over baselines across four languages (English, Chinese, French, German). Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage.

Conclusion: Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources.

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [18] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 研究指出当前LLM在角色扮演评估中表现不佳，需提升其人类推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的角色扮演研究依赖于未经验证的LLM作为评判者，这可能无法反映人类对角色一致性感知。

Method: 提出PersonaEval基准测试，通过分析小说、剧本和视频字幕中的人类对话来评估LLM识别角色的能力。

Result: 即使最先进的LLM也只能达到约69%的准确率，远低于可靠评估所需的水平，而人类参与者则接近满分（90.8%）。

Conclusion: 当前的LLM评估方法在角色扮演质量判断上仍无法达到人类水平，需要更强的人类推理能力。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [19] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 本文介绍了RealTalk-CN，这是一个用于中文语音基础大语言模型研究的多轮、多领域语音-文本双模态TOD数据集，并提出了一个跨模态聊天任务以评估模型的鲁棒性和跨领域性能。


<details>
  <summary>Details</summary>
Motivation: 现有TOD数据集主要基于文本，缺乏真实的语音信号，且现有的语音TOD数据集主要为英文，缺乏语音不流畅性和说话人变化等关键方面。

Method: 引入了RealTalk-CN数据集，并提出了一个新颖的跨模态聊天任务，以真实模拟用户交互。

Result: RealTalk-CN涵盖了5.4k对话（60K个发言，150小时），包含配对的语音-文本注释，能够捕捉多样化的对话场景，并通过实验验证了其有效性。

Conclusion: RealTalk-CN验证了其在中文语音基础大语言模型研究中的有效性，为该领域奠定了坚实的基础。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [20] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Multimodal Large Language Model Orchestration的方法，用于在不进行额外训练的情况下创建交互式多模态AI系统。该方法利用大语言模型的内在推理能力，通过显式工作流程协调专用模型，实现自然的多模态交互，同时保持模块化、提高可解释性，并显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. Previous work has considered training as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency, and other integration issues.

Method: MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency. The framework is built upon three key innovations: a central controller LLM, a parallel Text-to-Speech architecture, and a cross-modal memory integration system.

Result: Extensive evaluations demonstrate that MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

Conclusion: MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [21] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 本文提出了一种基于范畴同伦的框架，以解决大语言模型在处理语义相同但表达不同的句子时的概率分布不一致问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理语义相同但表达不同的句子时，通常无法生成相同的下一个标记概率，因此需要一种更抽象的方法来解决这个问题。

Method: 引入了一个LLM马尔可夫范畴来表示由LLM生成的语言中的概率分布，并利用范畴同伦技术来捕捉LLM马尔可夫范畴中的“弱等价”。

Result: 本文详细概述了范畴同伦在LLM中的应用，从高阶代数K理论到模型范畴，建立在过去的半个世纪中发展出的强大理论结果之上。

Conclusion: 本文提出了一个基于范畴同伦的框架，用于解决大语言模型在处理语义相同但表达不同的句子时产生的概率分布不一致的问题。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [22] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: This paper introduces DURIT, a framework that decouples understanding from reasoning in Small Language Models (SLMs) by mapping natural language problems into a canonical problem space. Experiments show that DURIT significantly improves SLMs' performance on mathematical and logical reasoning tasks, as well as their robustness.


<details>
  <summary>Details</summary>
Motivation: Improving the reasoning ability of Small Language Models (SLMs) remains challenging due to the complexity and variability of natural language. SLMs must extract the core problem from complex linguistic input and perform reasoning based on that understanding, which is difficult given their limited capacity.

Method: We propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively maps natural language problems via reinforcement learning, aligns reasoning trajectories through self-distillation, and trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process.

Result: Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. It also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.

Conclusion: DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [23] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT is a novel framework designed to enhance reasoning in federated settings by leveraging a lightweight chain-of-thought enhancement mechanism and an improved aggregation approach, which significantly boosts client-side reasoning performance while preserving data privacy.


<details>
  <summary>Details</summary>
Motivation: Efficiently enhancing the reasoning capabilities of large language models (LLMs) in federated learning environments remains challenging, particularly when balancing performance gains with strict computational, communication, and privacy constraints. This challenge is especially acute in healthcare, where decisions-demand not only accurate outputs but also interpretable, traceable rationales to ensure safety, accountability, and regulatory compliance.

Method: FedCoT leverages a lightweight chain-of-thought enhancement mechanism: local models generate multiple reasoning paths, and a compact discriminator dynamically selects the most promising one. To manage client heterogeneity efficiently, an improved aggregation approach building upon advanced LoRA module stacking is adopted, incorporating client classifier-awareness to achieve noise-free aggregation across diverse clients.

Result: Comprehensive experiments on medical reasoning tasks demonstrate that FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.

Conclusion: FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [24] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 本文提出了一种名为LATTE的对比学习框架，用于从客户的历史通信中学习嵌入，以降低计算成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 学习客户端嵌入对于金融应用至关重要，但直接使用大型语言模型（LLMs）处理长事件序列在计算上是昂贵且不切实际的。

Method: 本文提出了LATTE，这是一种对比学习框架，将原始事件嵌入与冻结的LLM的语义嵌入对齐。行为特征被总结为简短的提示，由LLM嵌入，并通过对比损失作为监督。

Result: 实验表明，与传统处理完整序列的方法相比，本文的方法显著降低了推理成本和输入大小，并在实际金融数据集上表现优于最先进的技术。

Conclusion: 本文提出的方法在实际金融数据集上优于最先进的技术，同时在延迟敏感的环境中具有可部署性。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [25] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 本研究提出了一种增强显著性检验的分形预测框架，以提高大型语言模型在多项选择题回答中的可信度，通过自一致性重采样MCQA响应进行p值计算和符合性评分，从而构建预测集，并在MMLU和MMLU-Pro基准上进行了评估，结果显示该方法能够实现用户指定的经验错误覆盖率，并验证了APSS作为有效不确定性度量的有效性


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在学科QA场景中被越来越多地部署，但幻觉和非事实生成严重损害了响应的可靠性。虽然CP提供了统计严格的边缘覆盖保证，而显著性检验提供了已建立的统计严谨性，但它们的协同整合仍未被探索

Method: 本研究引入了一种增强显著性检验的分形预测（CP）框架，通过自一致性重采样MCQA响应进行p值计算和符合性评分，以解决LLM的黑箱性质，并通过零假设检验构建预测集

Result: 评估结果表明：(1) 增强的CP实现了用户指定的经验错误覆盖率；(2) 测试集平均预测集大小（APSS）随着风险水平（α）的增加而单调减少，验证了APSS作为有效不确定性度量的有效性

Conclusion: 本研究建立了一个有原则的统计框架，用于在高风险QA应用中可信地部署大型语言模型（LLMs）

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [26] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: RTTC is a novel framework that adaptively selects the most effective TTC strategy for each query using a pretrained reward model, achieving superior accuracy compared to existing methods while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: The optimal adaptation strategy varies across queries, and indiscriminate application of TTC strategy incurs substantial computational overhead.

Method: Reward-Guided Test-Time Compute (RTTC), a novel framework that adaptively selects the most effective TTC strategy for each query via a pretrained reward model, maximizing downstream accuracy across diverse domains and tasks. RTTC operates in a distributed server-client architecture, retrieving relevant samples from a remote knowledge base and applying RAG or lightweight fine-tuning on client devices only when necessary. Additionally, Query-State Caching is proposed to mitigate redundant computation by enabling the efficient reuse of historical query states at both retrieval and adaptation levels.

Result: Extensive experiments across multiple LLMs and benchmarks demonstrate that RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT.

Conclusion: RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT, validating the necessity of adaptive, reward-guided TTC selection and the potential of RTTC for scalable, high-performance language model adaptation.

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [27] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 本文提出了一种结合自然语言处理、机器学习和大型语言模型的智能产后抑郁筛查系统，能够实现经济实惠、实时和非侵入性的语音分析，并在产后抑郁检测方面取得了90%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁是一种严重影响母亲心理和身体健康的严重状况，因此需要快速检测产后抑郁及其相关风险因素，以便及时评估和干预。

Method: 我们的工作贡献了一个智能产后抑郁筛查系统，结合了自然语言处理、机器学习（ML）和大型语言模型（LLMs），以实现经济实惠、实时和非侵入性的语音分析。

Result: 获得的结果是90%的产后抑郁检测率，在所有评估指标上都优于文献中的竞争解决方案。

Conclusion: 我们的解决方案有助于快速检测产后抑郁及其相关风险因素，这对于及时和适当的评估和干预至关重要。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [28] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER is a reinforcement learning framework that enables efficient reasoning in large language models by allowing user-controllable, token-budgeted reasoning. It supports four inference modes for flexible trade-offs between latency and reasoning depth, achieving high accuracy under tight budgets and effective generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems.

Method: SABER is a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. It profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. No-think examples are incorporated to ensure reliability even when explicit reasoning is turned off. SABER supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink.

Result: Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

Conclusion: SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization.

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [29] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 该研究开发并评估了一个筛选流程，结合变压器嵌入和手工制作的语言特征，测试使用大型语言模型生成的合成语音进行数据增强，并基准化单模态和多模态LLM分类器用于ADRD检测。结果表明，融合模型优于单独的语言或变压器基线，数据增强和微调显著提高了性能，但多模态模型仍需改进。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病及相关痴呆症（ADRD）影响大约五百万美国老年人，但超过一半未被诊断。基于语音的自然语言处理（NLP）提供了一种有前途的、可扩展的方法，通过语言标记检测早期认知衰退。

Method: 融合变压器嵌入与手工制作的语言特征，测试使用大型语言模型（LLMs）生成的合成语音进行数据增强，并基准化单模态和多模态LLM分类器用于ADRD检测。

Result: 融合模型实现了F1 = 83.3（AUC = 89.5），超过了语言或变压器-only基线。用2x MedAlpaca-7B合成语音增强训练数据将F1提高到85.7。微调显著提高了单模态LLM分类器（例如，MedAlpaca：F1 = 47.3 -> 78.5 F1）。当前的多模态模型表现较低（GPT-4o = 70.2 F1；Qwen = 66.0）。性能提升与合成和真实语音之间的分布相似性一致。

Conclusion: 整合变压器嵌入与语言特征可以增强从语音中检测ADRD的能力。临床调整的LLM在分类和数据增强方面有效，而多模态建模需要进一步发展。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [30] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF is a personalised reference-free evaluation framework that measures general output quality and user-specific alignment without requiring gold personalised references.


<details>
  <summary>Details</summary>
Motivation: Personalised text generation is essential for user-centric information systems, yet most evaluation methods overlook the individuality of users.

Method: PREF operates in a three-step pipeline: (1) a coverage stage uses a large language model (LLM) to generate a comprehensive, query-specific guideline covering universal criteria such as factuality, coherence, and completeness; (2) a preference stage re-ranks and selectively augments these factors using the target user's profile, stated or inferred preferences, and context, producing a personalised evaluation rubric; and (3) a scoring stage applies an LLM judge to rate candidate answers against this rubric, ensuring baseline adequacy while capturing subjective priorities.

Result: Experiments on the PrefEval benchmark, including implicit preference-following tasks, show that PREF achieves higher accuracy, better calibration, and closer alignment with human judgments than strong baselines.

Conclusion: PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [31] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种基于表示的越狱攻击方法LFJ，并通过对抗训练防御方法有效降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种语言任务中表现出色，但容易受到越狱攻击的影响，这些攻击会绕过其安全对齐。因此，需要研究有效的攻击方法和防御措施。

Method: 本文介绍了Latent Fusion Jailbreak (LFJ) 攻击方法，该方法基于表示进行攻击，通过在有害和良性查询对的隐藏状态之间进行插值来引发禁止的响应。攻击过程包括选择具有高主题和句法相似性的查询对、在有影响的层和标记上进行梯度引导的插值，以及优化以平衡攻击成功率、输出流畅性和计算效率。

Result: 在Vicuna和LLaMA-2等模型上的评估显示，LFJ的平均攻击成功率（ASR）达到94.01%，优于现有方法。通过对抗训练防御方法，在插值示例上微调模型，可以将ASR降低80%以上，而不会影响对良性输入的性能。

Conclusion: 本文提出了对抗训练防御方法，通过在插值示例上微调模型，显著降低了LFJ攻击的成功率，同时保持了对良性输入的性能。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [32] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出了一个考虑推理策略的提示优化框架IAPO，并开发了相应的算法PSST，验证了其在多个任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法在部署时没有考虑到推理策略，这导致了一个重要的方法论缺口。本文旨在解决这一问题，以更好地对齐黑盒大型语言模型。

Method: 本文提出了一种名为IAPO（Inference-Aware Prompt Optimization）的统一框架，该框架同时优化提示和推理规模，并考虑到推理预算和不同的任务目标。此外，还开发了一种固定预算训练算法PSST（Prompt Scaling via Sequential Trimming）。

Result: 本文在六个不同的任务上评估了PSST的有效性，包括多目标文本生成和推理，并证明了在通过提示优化对齐黑盒LLM时，融入推理感知的重要性。

Conclusion: 通过引入IAPO框架和PSST算法，本文展示了在对齐黑盒大型语言模型时，考虑推理感知的重要性。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [33] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 本文发现LLM在思考模式下更容易受到Jailbreak攻击，并提出了一种安全的思考干预方法，通过添加特定思考标记来减少攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决LLM在思考模式下更容易受到Jailbreak攻击的问题，并探索如何减轻这种风险。

Method: 本文提出了一种安全的思考干预方法，通过在提示中添加LLM的“特定思考标记”来明确引导LLM的内部思考过程。

Result: 实验结果表明，安全的思考干预方法可以显著降低具有思考模式的LLM的攻击成功率。

Conclusion: 本文提出了一种安全的思考干预方法，通过在提示中添加LLM的“特定思考标记”来明确引导LLM的内部思考过程，结果表明该方法可以显著降低具有思考模式的LLM的攻击成功率。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [34] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为APIE的新颖主动提示框架，通过内省混淆原则来评估模型的不确定性，从而提高少样本信息提取的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在少样本信息提取（IE）中显示出巨大的潜力，但其性能对上下文示例的选择非常敏感。传统选择策略常常无法提供有信息的指导，因为它们忽略了模型不可靠的一个关键来源：不仅来自语义内容，还来自IE任务所需的生成良好结构格式的混淆。

Method: 我们引入了主动提示信息提取（APIE），这是一种由我们称之为内省混淆的原则指导的新颖主动提示框架。我们的方法使LLM能够通过一个双组件不确定性度量来评估自身的困惑，该度量独特地量化了格式不确定性和内容不确定性。

Result: 在四个基准测试上的广泛实验表明，我们的方法始终优于强大的基线，显著提高了提取准确性和鲁棒性。

Conclusion: 我们的工作突显了在构建有效且可靠的结构生成系统时，对模型不确定性进行细粒度的双层次视图的重要性。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [35] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种名为mSCoRe的多语言和可扩展基准，用于评估基于技能的常识推理。该基准包含三个关键组件，旨在系统地评估LLM的推理能力。实验表明，mSCoRe对当前模型仍然具有挑战性，尤其是在高复杂度水平上。研究揭示了推理增强模型在处理多语言常识和文化常识时的局限性，并提出了未来改进的方向。


<details>
  <summary>Details</summary>
Motivation: 最近在推理增强的大规模语言模型（LLMs）方面的进展显示了在复杂推理任务中的显著能力。然而，其利用不同人类推理技能的机制仍缺乏深入研究，特别是在涉及不同语言和文化的多语言常识推理方面。为了解决这一差距，我们提出了mSCoRe基准。

Method: 我们提出了一个名为mSCoRe的多语言和可扩展基准，用于基于技能的常识推理。该基准包括三个关键组件：(1) 一种新的推理技能分类法，使模型的推理过程能够进行细粒度分析；(2) 一种专门针对常识推理评估的稳健数据合成管道；(3) 一种复杂性扩展框架，允许任务难度随着LLM能力的未来改进而动态变化。

Result: 对八种不同规模和训练方法的最先进LLMs进行了广泛的实验，结果显示mSCoRe对于当前模型来说仍然具有挑战性，尤其是在更高复杂度水平上。

Conclusion: 我们的结果揭示了这种推理增强模型在面对微妙的多语言一般常识和文化常识时的局限性。我们进一步提供了对模型推理过程的详细分析，并提出了改进多语言常识推理能力的未来方向。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [36] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 本文介绍了一个新的基准，用于评估LLMs在多轮对话、推理和信息获取方面的能力，并揭示了当前模型的不足之处。


<details>
  <summary>Details</summary>
Motivation: 需要开发能够有效参与逻辑一致的多轮对话、寻求信息并使用不完整数据进行推理的LLMs。

Method: 引入了一个包含一系列多轮任务的基准，每个任务旨在测试特定的推理、交互对话和信息获取能力。

Result: 评估前沿模型显示存在显著的改进空间，大多数错误源于指令遵循不佳、推理失败和规划不良。

Conclusion: 该基准为未来研究提供了强大的平台，以改进LLMs在处理复杂交互场景中的关键能力。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [37] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 本文介绍了 LaaJMeter，这是一种用于评估 LaaJ 的模拟框架，可以帮助工程师验证和优化评估指标，特别是在低资源环境中。


<details>
  <summary>Details</summary>
Motivation: 在领域特定的上下文中，使用未经过验证的指标进行元评估存在挑战，因为标注数据稀缺且专家评估成本高昂。因此，需要一种方法来验证和优化 LaaJ 的评估指标。

Method: LaaJMeter 是一个基于模拟的框架，用于对 LaaJ 进行受控元评估。它允许工程师生成代表虚拟模型和法官的合成数据，从而在现实条件下系统地分析评估指标。

Result: 我们在一个涉及遗留编程语言的代码翻译任务中展示了 LaaJMeter 的效用，表明不同的指标在对评估器质量的敏感性方面有所不同。结果突显了常用指标的局限性以及选择指标的重要性。

Conclusion: LaaJMeter 提供了一种可扩展和可扩展的解决方案，用于在低资源环境中评估 LaaJ。这有助于确保自然语言处理中的可信和可重复评估。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [38] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 本文研究了机器翻译中的翻译难度估计任务，提出了一个新的度量标准来评估难度估计器，并展示了难度估计器在构建更具挑战性的机器翻译基准中的实用价值。结果表明，专门的模型优于基于启发式的方法和LLM-as-a-judge方法。


<details>
  <summary>Details</summary>
Motivation: 机器翻译质量在某些设置中已经开始达到接近完美的翻译。这些高质量的输出使得难以区分最先进的模型，并确定未来改进的领域。自动识别机器翻译系统难以处理的文本有望开发更具区分力的评估并指导未来的研究。

Method: 我们正式定义了翻译难度估计的任务，根据文本翻译的预期质量来定义文本的难度。我们引入了一个新的度量标准来评估难度估计器，并用它来评估基线和新方法。最后，我们展示了难度估计器的实用价值，通过它们构建更具挑战性的机器翻译基准。

Result: 我们结果表明，专门的模型（称为Sentinel-src）优于基于启发式的方法（例如词频或句法复杂性）和LLM-as-a-judge方法。我们发布了两个改进的难度估计模型，Sentinel-src-24和Sentinel-src-25，可以用来扫描大量文本并选择最可能挑战当代机器翻译系统的文本。

Conclusion: 我们的结果表明，专门的模型（称为Sentinel-src）优于基于启发式的方法（例如词频或句法复杂性）和LLM-as-a-judge方法。我们发布了两个改进的难度估计模型，Sentinel-src-24和Sentinel-src-25，可以用来扫描大量文本并选择最可能挑战当代机器翻译系统的文本。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [39] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: For-Value是一种高效且可扩展的数据估值框架，能够在不进行梯度计算的情况下准确估计训练样本的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的数据估值方法通常依赖于Hessian信息或模型重新训练，这在大规模参数模型中计算上是不可行的。

Method: For-Value是一种仅向前的数据估值框架，它利用现代基础模型的丰富表示，通过单一前向传递计算影响分数，从而避免了昂贵的梯度计算。

Result: 实验结果表明，For-Value在识别重要的微调示例和检测错误标记的数据方面表现良好，甚至优于基于梯度的基线方法。

Conclusion: For-Value能够有效地估计单个训练样本的影响，从而提高大型语言模型和视觉语言模型的透明度和可问责性。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [40] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 本文介绍了PakBBQ，这是一个针对巴基斯坦的偏见基准测试，用于评估大型语言模型在不同上下文和问题框架下的公平性。实验结果显示了消歧和语言形式对减少偏见的重要性。


<details>
  <summary>Details</summary>
Motivation: 大多数LLMs在西方中心数据上进行训练和评估，很少关注低资源语言和区域背景。因此，需要一个针对巴基斯坦的偏见基准测试来填补这一空白。

Method: 我们引入了PakBBQ，这是原始Bias Benchmark for Question Answering (BBQ)数据集的文化和区域适应扩展。PakBBQ包含超过214个模板，17180个QA对，涵盖8个类别，包括年龄、残疾、外貌、性别、社会经济地位、宗教、地区归属和语言正式性等偏见维度。我们评估了多种多语言LLMs在模糊和明确消歧上下文以及负面与非负面问题框架下的表现。

Result: 实验结果表明，(i) 在消歧情况下平均准确率提高了12%，(ii) 在乌尔都语中比英语表现出更强的反偏见行为，(iii) 问题框架的影响显著减少了刻板印象的回答。

Conclusion: 这些发现突显了情境化基准和简单的提示工程策略在低资源环境中减轻偏见的重要性。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [41] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 本文介绍了一种新的轻量级框架Semantic Divergence Metrics (SDM)，用于检测大型语言模型中的语义偏差。该框架通过联合聚类对句子嵌入进行处理，并结合信息理论度量和语义探索指标，提供了一个诊断框架来分类LLM响应类型。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如语义熵测试只能测量单一固定提示的回答多样性，而本文提出了一种更注重提示的框架，以更深入地测试任意性。

Method: 本文的方法基于联合聚类对句子嵌入进行处理，创建提示和答案的共享主题空间。然后计算一系列信息理论度量来测量提示和响应之间的语义发散。此外，还结合了KL散度作为语义探索的指标，并将这些度量组合成一个名为Semantic Box的诊断框架。

Result: 本文提出的SDM框架能够更准确地检测LLM响应中的语义偏差，并通过Semantic Box框架对LLM响应类型进行分类，包括危险的自信虚构。

Conclusion: 本文提出了Semantic Divergence Metrics (SDM)，一种轻量级框架，用于检测Faithfulness Hallucinations。通过结合信息理论度量和语义探索指标，SDM能够更准确地识别LLM响应中的语义偏差，并提供了一种诊断框架来分类LLM响应类型。

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [42] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 本研究比较了四种深度学习架构在表情符号预测中的表现，发现 BERT 在整体性能上最优，而 CNN 在处理罕见表情符号时更有效。


<details>
  <summary>Details</summary>
Motivation: 探索从短文本序列中进行表情符号预测，以改善人机交互。

Method: 使用四种深度学习架构（前馈网络、CNN、transformer 和 BERT）进行表情符号预测，并使用 TweetEval 数据集解决类别不平衡问题。

Result: BERT 在整体性能上表现最佳，而 CNN 在罕见表情符号类别上表现出色。

Conclusion: 本研究展示了架构选择和超参数调整在情感感知表情符号预测中的重要性，有助于改进人机交互。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [43] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 本研究利用大型语言模型从临床访谈中预测BPRS评分，结果显示其性能接近人类评估，并且在不同语言和纵向信息整合方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于BPRS需要长时间的结构化访谈，因此在临床实践中不常用。本研究旨在探索LLMs在预测BPRS评分方面的潜力，以改善和标准化CHR患者的评估。

Method: 利用大型语言模型（LLMs）从临床访谈转录文本中预测BPRS评分，并评估其在不同语言和纵向信息整合中的表现。

Result: LLMs在预测BPRS评分方面表现出色，零样本性能与真实评估相比（中位数一致性：0.84，ICC：0.73），接近人类的互评和内部评分可靠性。此外，LLMs在不同语言中评估BPRS的准确性较高（中位数一致性：0.88，ICC：0.70），并且能够通过一次或少量样本学习整合纵向信息。

Conclusion: 研究结果表明，大型语言模型在预测BPRS评分方面表现出色，具有改进和标准化CHR患者评估的潜力。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [44] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 本研究采用计算和语料库方法，探讨了托基波纳语言的变化和变体，发现社会语言学因素影响构造语言，并且构造语言系统会自然演化。


<details>
  <summary>Details</summary>
Motivation: 探索托基波纳语言的变化和变体，了解社会语言学因素如何影响构造语言，并研究构造语言系统是否能自然演化。

Method: 计算和语料库方法，分析了流体词类和及物性等特征，以研究内容词在不同句法位置上的偏好变化以及不同语料库中的使用差异。

Result: 研究结果表明，社会语言学因素影响托基波纳的方式与自然语言相同，构造语言系统在社区使用中会自然演变。

Conclusion: 研究结果表明，社会语言学因素以与自然语言相同的方式影响托基波纳，即使构造的语言系统也会随着社区的使用而自然演变。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [45] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 该研究提出了一种利用LLM输出来创建与模型归纳偏差相匹配的提示的方法，并展示了其在提高LLM评分方面的有效性。


<details>
  <summary>Details</summary>
Motivation: LLM对提示措辞的小变化很敏感，这可以部分归因于LLM中存在的归纳偏差。

Method: 通过将LLM的输出作为提示的一部分，可以更容易地创建令人满意的提示措辞，从而创建与模型的归纳偏差相匹配的提示。

Result: 实证研究表明，使用这种归纳偏差提取和匹配策略可使LLM的分类Likert评分提高高达19%，排序Likert评分提高高达27%。

Conclusion: 使用这种归纳偏差提取和匹配策略可以提高LLM的分类和排序的Likert评分。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [46] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 该研究通过定性分析，揭示了大型语言模型如何再现性别和种族偏见，并强调了需要批判性和跨学科的方法来改进AI的设计和伦理使用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，评估它们是否再现偏见（如歧视和种族化）变得至关重要。当前的偏见检测方法主要依赖于定量、自动的方法，往往忽视了自然语言中偏见出现的细微方式。

Method: 该研究提出了一种定性的、话语框架来补充现有的偏见检测方法。通过对手动生成的包含黑人和白人女性的短篇故事进行手动分析，探讨了性别和种族偏见。

Result: 结果表明，黑人女性被描绘为与祖先和抵抗联系在一起，而白人女性则出现在自我发现的过程中。这些模式反映了语言模型如何复制固化的言语表现，强化本质主义和社会停滞感。当被要求纠正偏见时，模型提供了表面的修改，保持了有问题的含义，揭示了在促进包容性叙述方面的局限性。

Conclusion: 研究强调了算法的意识形态作用，并对人工智能的伦理使用和发展具有重要意义。研究支持了对AI设计和部署采取批判性和跨学科方法的必要性，以解决LLM生成的论述如何反映和加剧不平等的问题。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [47] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 本文介绍了ReviewRL，这是一个用于生成全面且事实依据的科学论文评论的强化学习框架。该方法结合了ArXiv-MCP检索增强的上下文生成管道、监督微调以及具有复合奖励函数的强化学习过程。实验表明，ReviewRL在ICLR 2025论文上的表现显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews.

Method: Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy.

Result: Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments.

Conclusion: ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain.

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [48] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: DOTABLER是一种以表格为中心的语义文档解析框架，通过深度语义解析和上下文关联，实现了表格-锚定的语义分析和精确的表格提取。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在表面任务，如布局分析、表格检测和数据提取，缺乏对表格及其上下文关联的深度语义解析，这限制了高级任务如跨段落数据分析和上下文一致分析。

Method: DOTABLER是一种以表格为中心的语义文档解析框架，利用定制数据集和预训练模型的领域特定微调，整合完整的解析流程来识别与表格语义相关的上下文段落。

Result: DOTABLER在近4000页的真实PDF中进行了评估，包含超过1000个表格，取得了超过90%的精确度和F1分数。

Conclusion: DOTABLER在表格-上下文语义分析和深度文档解析方面表现出色，相比先进的模型如GPT-4o具有优越性能。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [49] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: 本文提出FreLLM4Rec，通过谱方法平衡语义和协同信息，有效缓解了LLM推荐系统中协同信号的衰减问题，并在多个数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: LLM-based recommenders倾向于过度强调用户交互历史中的语义相关性，导致协同信号在通过LLM后逐渐减弱。传统Transformer序列模型通常能保持或增强协同信号，而LLM-based模型则相反。因此，需要一种方法来平衡语义和协同信息，以提高推荐系统的性能。

Method: 引入了FreLLM4Rec，从谱的角度平衡语义和协同信息。首先使用全局图低通滤波器（G-LPF）净化包含语义和协同信息的项目嵌入，以初步去除不相关的高频噪声。然后通过时间频率调制（TFM）逐层主动保留协同信号。TFM的协同保留能力通过建立局部图傅里叶滤波器与计算高效的频域滤波器之间的联系来理论保证。

Result: 在四个基准数据集上的广泛实验表明，FreLLM4Rec成功缓解了协同信号衰减，并实现了具有竞争力的性能，最高提升了8.00%的NDCG@10。

Conclusion: FreLLM4Rec成功缓解了协同信号衰减，并在基准数据集上实现了具有竞争力的性能，最高提升了8.00%的NDCG@10。研究结果为理解LLM如何处理协同信息提供了见解，并为改进基于LLM的推荐系统提供了一个有原则的方法。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [50] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 本文提出XPE和Dual Soft Prompt机制，以提升大语言模型在低性能语言和多语言环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索软提示在跨语言迁移中的潜力，特别是在低性能语言中提升模型表现。

Method: 引入了Cross-Prompt Encoder (XPE) 和Dual Soft Prompt机制，结合轻量级编码架构和多源训练，以捕捉跨语言的抽象和可转移模式。

Result: 实验表明，XPE在低性能语言中效果显著，而混合机制在多语言环境中表现出更好的适应性。

Conclusion: XPE在低性能语言中最为有效，而混合变体在多语言设置中提供了更广泛的适应性。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [51] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 我们提出了一种两阶段微调方法，使大型语言模型Qwen3 14B能够原生地以韩语进行思考。通过监督微调和强化学习，我们的方法显著提升了韩语任务的表现，并在数学和编码任务上取得了显著进步。


<details>
  <summary>Details</summary>
Motivation: 为了使大型语言模型Qwen3 14B能够原生地以韩语进行思考，我们需要一种有效的微调方法，以提高其在韩语任务中的表现，同时保持其知识和语言能力。

Method: 我们提出了一种两阶段微调方法，使大型语言模型Qwen3 14B能够原生地以韩语进行思考。第一阶段是在高质量的韩语推理数据集上进行监督微调（SFT），建立了坚实的韩语逻辑推理基础，显著提升了韩语任务的表现，并且在一般推理能力上也有所提高。第二阶段，我们采用一种定制的组相对策略优化（GRPO）算法进行强化学习，进一步提高了韩语推理对齐和整体问题解决性能。

Result: 我们的方法实现了稳定的训练，并避免了在简单GRPO中观察到的崩溃现象，带来了稳定、渐进的性能提升。最终的RL调优模型在高级推理基准测试中表现出显著改进，特别是在数学和编码任务上，同时保持了知识和语言能力，成功地在其内部思维链中完全使用韩语。

Conclusion: 我们的方法实现了稳定的训练，并带来了持续的性能提升。最终的RL调优模型在高级推理基准测试中表现出显著改进，特别是在数学和编码任务上，同时保持了知识和语言能力，成功地在其内部思维链中完全使用韩语。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [52] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [53] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 本文利用大型语言模型在捷克语摘要任务中取得了新进展，并引入了一个新的历史捷克语文档摘要数据集。


<details>
  <summary>Details</summary>
Motivation: 由于语言复杂性和标注数据集的稀缺性，捷克语文本摘要，特别是历史文献的摘要，仍研究不足。

Method: 使用Mistral和mT5等大型语言模型进行捷克语摘要。

Result: 在现代捷克语摘要数据集SumeCzech上实现了新的最先进结果，并引入了一个名为Posel od Čerchova的新数据集用于历史捷克语文档的摘要。

Conclusion: 这些贡献为推进捷克文本摘要提供了巨大潜力，并为捷克历史文本处理的研究开辟了新途径。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [54] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文介绍了一种新的跨语言方面情感分析方法，通过约束解码提高性能，并在多种语言和任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前跨语言方面情感分析方法专注于有限且不太复杂的任务，并且通常依赖于外部翻译工具，而低资源语言往往被忽视。

Method: 本文引入了一种使用约束解码的序列到序列模型的新方法，以消除对不可靠翻译工具的依赖，并通过约束解码提高跨语言性能。

Result: 所提出的方法在最复杂的任务中平均提高了跨语言性能5%，并且支持多任务处理，使一个模型可以解决多个方面情感分析任务，约束解码提升了超过10%的结果。此外，评估显示该方法超越了最先进的方法，并为之前未探索的任务设定了新的基准。

Conclusion: 本文提供了关于跨语言方面情感分析方法的实用建议，并为该研究领域的发展做出了有价值的贡献。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [55] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 本文提出了一种混合评估框架MDH，用于检测恶意内容并清理数据集，同时发现精心设计的开发者消息可以显著提高越狱成功的可能性，并提出了两种新的策略。


<details>
  <summary>Details</summary>
Motivation: 现有的红队数据集包含不适合的提示，需要评估和清理恶意内容。然而，现有的恶意内容检测方法依赖于手动注释或大型语言模型，这些方法在准确性上不一致。

Method: 本文提出了一种结合LLM注释和最小人类监督的混合评估框架MDH，并应用它进行数据集清理和检测越狱响应。此外，还发现了精心设计的开发者消息可以显著提高越狱成功率，并提出了两种新的策略。

Result: 本文提出的MDH框架能够平衡准确性和效率，并且发现精心设计的开发者消息可以显著提高越狱成功率。

Conclusion: 本文提出了一个混合评估框架MDH，用于检测恶意内容并清理数据集，同时发现精心设计的开发者消息可以显著提高越狱成功的可能性，并提出了两种新的策略D-Attack和DH-CoT。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [56] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [57] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: 本文提出了 ComoRAG，这是一种基于检索的长上下文理解方法，通过迭代推理和动态记忆工作区来提高叙事理解性能。


<details>
  <summary>Details</summary>
Motivation: 传统的 RAG 方法由于其无状态、单步检索过程，常常忽略了长距离上下文中相互关联的关系的动态性质，因此需要一种更有效的解决方案来处理长故事和小说中的叙事理解问题。

Method: ComoRAG 通过迭代推理循环与动态记忆工作区交互，在每次循环中生成探测查询以制定新的探索路径，并将新方面的检索证据整合到全局记忆池中，从而支持查询解决的连贯上下文的出现。

Result: 在四个具有挑战性的长上下文叙事基准测试中（200K+ tokens），ComoRAG 在与最强基线相比的情况下，表现优于强大的 RAG 基线，相对提升高达 11%。进一步分析表明，ComoRAG 在需要全局理解的复杂查询中尤其具有优势。

Conclusion: ComoRAG 提供了一种基于检索的长上下文理解的有原则、认知驱动的范式，适用于状态推理。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [58] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 本文介绍了IdiomEval框架，用于评估中文成语翻译，并发现现有系统在该任务上表现不佳，同时开发了改进的模型以提高检测成语翻译错误的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在机器翻译方面取得了进展，但关于中文成语翻译的研究仍较少，且现有系统在处理成语翻译时表现不佳。

Method: 引入IdiomEval框架，建立全面的错误分类法，并标注了900个来自九个现代系统的翻译对，包括GPT-4o和Google Translate。

Result: 这些系统在成语翻译中表现出错误、字面意思、部分或甚至缺失的翻译，最佳系统GPT-4在28%的情况下出现错误。现有的评估指标与人类评分的皮尔逊相关性低于0.48。

Conclusion: 现有评估指标对成语翻译质量的衡量效果不佳，因此我们开发了改进的模型，实现了0.68的F1分数来检测成语翻译错误。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [59] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 本文提出了一种基于计算经济学的框架，通过激励驱动的训练方法提高大型语言模型的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）受到显著计算成本的限制。

Method: 我们引入了一个“计算经济学”框架，将LLM视为一个资源受限代理（注意力头和神经元块）的内部经济，这些代理必须分配稀缺计算以最大化任务效用。我们提出了一个激励驱动的训练范式，通过加入可微分的计算成本项来增强任务损失，鼓励稀疏和高效的激活。

Result: 该方法在GLUE（MNLI、STS-B、CoLA）和WikiText-103上产生了一组模型，这些模型绘制出帕累托前沿，并且在相似准确率下，FLOPS减少了约40%，延迟更低，同时注意力模式更具可解释性。

Conclusion: 这些结果表明，经济原则为在严格资源约束下设计高效、自适应和更透明的LLM提供了一条有根据的途径。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [60] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: DiFaR是一种检测器无关的框架，旨在生成多样、事实准确且相关的文本推理，以增强虚假信息检测。


<details>
  <summary>Details</summary>
Motivation: 生成文本推理以支持可训练的多模态虚假信息检测器是一个有前景的方向，但其效果受到三个核心挑战的限制：生成推理的多样性不足、由于幻觉导致的事实错误以及引入噪声的不相关或冲突内容。

Method: DiFaR采用五种思维链提示来激发LVLMs的多样化推理路径，并结合轻量级后处理过滤模块，根据句子级别的事实性和相关性评分选择推理内容。

Result: 在四个流行的基准测试中进行的广泛实验表明，DiFaR比四种基线类别高出最多5.9%，并使现有检测器提高了多达8.7%。自动指标和人工评估都确认了DiFaR在所有三个维度上显著提升了推理质量。

Conclusion: DiFaR显著提升了多模态虚假信息检测中生成的文本推理的质量，通过解决多样性、事实准确性和相关性问题，表现出优于现有基线的效果。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [61] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文研究了自然语言处理中隐私与可解释性的关系，发现它们可以共存，并提出了未来研究的建议。


<details>
  <summary>Details</summary>
Motivation: 目前对隐私和可解释性NLP的研究不断增加，但两者之间的交叉研究仍然不足，缺乏对两者是否可以同时实现或是否相互冲突的理解。

Method: 本文通过实证研究，探讨了在自然语言处理（NLP）背景下隐私与可解释性的权衡问题，采用了差分隐私（DP）和事后可解释性等主流方法。

Result: 研究发现，隐私和可解释性之间的关系受到多种因素的影响，包括下游任务的性质以及文本隐私化和可解释性方法的选择。

Conclusion: 本文指出，隐私和可解释性可以共存，并提出了未来在这一重要交叉领域工作的实用建议。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [62] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 本文研究了多模态大语言模型中的文本主导现象，并提出了评估指标和解决方法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在系统地研究跨多种数据模态的文本主导现象，并寻找解决该问题的方法。

Method: 本文提出了两种评估指标：模态主导指数（MDI）和注意力效率指数（AEI），并提出了一种简单的令牌压缩方法，以有效重新平衡模型注意力。

Result: 分析表明，文本主导在所有测试的模态中都是显著且普遍的。通过应用提出的令牌压缩方法，可以有效减少文本主导现象。

Conclusion: 本文的分析和方法框架为开发更公平和全面的多模态语言模型提供了基础。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [63] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 本文介绍了一个欧洲深度推理网络（eDIF）的可行性研究，旨在支持大型语言模型的机制可解释性研究，并通过一个基于GPU的集群和NNsight API实现远程模型检查。


<details>
  <summary>Details</summary>
Motivation: 欧洲需要广泛获取LLM可解释性基础设施，以民主化先进的模型分析能力，支持研究社区。

Method: 该项目引入了一个基于GPU的集群，位于安斯巴赫应用科学大学，并与合作伙伴机构互联，通过NNsight API实现远程模型检查。

Result: 用户参与度逐渐增加，平台性能稳定，远程实验能力受到积极评价。同时，发现了下载激活数据时间过长和执行中断等问题，并在未来的开发路线图中加以解决。

Conclusion: 该研究标志着欧洲广泛获取LLM可解释性基础设施的重要一步，并为更广泛的部署、扩展工具和持续的社区合作奠定了基础。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [64] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 本文首次系统研究了将科普特语翻译成法语的策略，发现使用风格多样且具有噪声意识的训练语料库进行微调可以显著提高翻译质量，并为开发历史语言的翻译工具提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 本文提出了对科普特语到法语翻译策略的首次系统研究。

Method: 我们系统地评估了转换与直接翻译、预训练的影响、多版本微调的好处以及模型对噪声的鲁棒性。

Result: 我们利用对齐的圣经语料库，证明了使用风格多样且具有噪声意识的训练语料库进行微调可以显著提高翻译质量。

Conclusion: 我们的研究为开发历史语言的翻译工具提供了重要的实用见解。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [65] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 本文提出了一种结合基于图的方法和变压器架构的手语翻译方法，在多个数据集上取得了最先进的性能，并为未来的研究设定了基准。


<details>
  <summary>Details</summary>
Motivation: 为了克服社会对手语的低估，减少沟通障碍和社会排斥，我们致力于改进手语翻译方法。

Method: 我们的方法结合了基于图的方法与变压器架构，融合了变压器和STGCN-LSTM架构。

Result: 我们的方法在多个手语数据集上实现了最先进的性能，包括RWTH-PHOENIX-2014T、CSL-Daily、How2Sign和BornilDB v1.0。

Conclusion: 我们的方法为未来的研究设定了基准，强调了无词素翻译对于提高聋人和听力障碍者沟通可及性的重要性。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [66] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 本文提出了一种名为VAC的新框架，通过使用基于用户档案和问题叙述生成的自然语言反馈来替代传统的标量奖励，从而提高个性化问答的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在个性化大型语言模型（LLMs）时通常依赖于检索增强生成（RAG），然后使用标量奖励信号进行强化学习，但这些标量奖励有时提供的是弱而无指导性的反馈，限制了学习效率和个性化质量。

Method: 我们引入了VAC，这是一种新的个性化响应生成框架，它用基于用户档案和问题叙述生成的自然语言反馈（NLF）替代标量奖励。

Result: 在包含三个不同领域的LaMP-QA基准上的评估显示，与最先进的结果相比有持续且显著的改进。人类评估进一步证实了生成响应的优越质量。

Conclusion: 这些结果表明，自然语言反馈为优化个性化问答提供了更有效的信号。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [67] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为ICE的新框架，用于在扩散大语言模型中实现更灵活的就地提示策略，显著提高了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的前缀提示范式和顺序生成过程限制了双向信息的灵活性，而扩散大语言模型（dLLMs）通过其双向注意力机制和迭代优化过程提供了新的机会。

Method: ICE（In-Place Chain-of-Thought Prompting with Early Exit）是一种新型框架，通过在迭代优化过程中将就地提示直接整合到掩码标记位置，并采用自信感知的提前退出机制来减少计算开销。

Result: 实验表明，ICE在GSM8K数据集上实现了高达17.29%的准确率提升和4.12倍的速度提升，在MMLU数据集上实现了高达276.67倍的加速，同时保持了竞争力的表现。

Conclusion: ICE框架在dLLMs中展示了显著的性能提升和计算效率改进，为未来的双向信息处理提供了新的可能性。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [68] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种结构化的自动化新颖性评估方法，通过模拟专家审稿人的行为，在NLP领域中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 新颖性评估是同行评审中的核心但研究不足的方面，特别是在NLP等高容量领域中，审稿人能力日益受到压力。

Method: 该方法通过三个阶段建模专家审稿人行为：从提交中提取内容、检索和综合相关工作，以及结构化比较以进行基于证据的评估。

Result: 该方法在182个ICLR 2025提交中与人类推理达到86.5%的一致性，并在新颖性结论上达到75.3%的协议，显著优于现有的LLM基线。

Conclusion: 该方法展示了结构化LLM辅助方法在支持更严格和透明的同行评审中的潜力，而不会取代人类专业知识。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [69] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 本文提出了一种新的后训练算法MS-GRPO，用于改进LLMs在多步骤代理任务中的决策能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在应用中通常受到对大型、计算昂贵模型的依赖限制，因此需要改进较小的模型。然而，现有的后训练方法适用于单次交互，无法处理多步骤代理任务中的信用分配问题。

Method: 本文介绍了多步骤组相对策略优化（MS-GRPO），这是一种基于正式的文本媒介随机游戏（TSMG）和语言代理策略（LAP）框架的后训练LLM代理算法。

Result: 实验表明，该方法在提高决策性能方面是有效的：在Frozen Lake任务中，经过后训练的3B参数模型比72B参数基线模型提高了50%。

Conclusion: 本文表明，有针对性的后训练是一种实用且高效的替代方法，无需依赖模型规模即可使用LLMs创建顺序决策代理。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [70] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: This paper introduces Psyche-R1, a Chinese psychological LLM that combines empathy, psychological expertise, and reasoning. It uses a novel data curation pipeline and a hybrid training strategy to achieve strong performance in psychological applications, comparable to a much larger model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the shortage of qualified mental health professionals by leveraging large language models (LLMs) for psychological applications. The paper aims to integrate reasoning mechanisms into psychological LLMs to generate reliable responses, beyond just emotional support and empathetic dialogue.

Method: The paper proposes Psyche-R1, a Chinese psychological LLM that integrates empathy, psychological expertise, and reasoning. It uses a data curation pipeline to generate high-quality psychological questions with detailed rationales and empathetic dialogues. A hybrid training strategy is employed, combining group relative policy optimization (GRPO) and supervised fine-tuning (SFT).

Result: Extensive experiments show that Psyche-R1 achieves comparable results to a much larger model (671B DeepSeek-R1) on several psychological benchmarks, demonstrating its effectiveness in psychological applications.

Conclusion:  Psyche-R1 demonstrates the effectiveness of integrating empathy, psychological expertise, and reasoning in a large language model for psychological applications, achieving comparable results to a much larger model.

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [71] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种多维建模框架，结合特征工程、数据增强和可解释机器学习，以解决现有研究中的不足。通过强调可解释性，该方法在新的英中连续口译数据集上表现出色，能够提供详细的诊断反馈，支持自我调节学习。


<details>
  <summary>Details</summary>
Motivation: 现有研究在语言使用质量方面缺乏充分的检查，由于数据稀缺和不平衡导致建模效果不佳，并且缺乏对模型预测的解释努力。

Method: 我们提出了一种多维建模框架，结合了特征工程、数据增强和可解释机器学习。这种方法通过仅使用与构建相关的透明特征并进行Shapley值（SHAP）分析，优先考虑可解释性而非“黑箱”预测。

Result: 我们的结果在新的英中连续口译数据集上显示出强大的预测性能，确定BLEURT和CometKiwi得分是忠实度的最强预测特征，暂停相关特征是流利度的预测特征，而中文特定的短语多样性指标是语言使用的预测特征。

Conclusion: 通过强调可解释性，我们提出了一个可扩展、可靠和透明的传统人工评估的替代方案，有助于为学习者提供详细的诊断反馈，并支持自我调节学习的优势，这是单独的自动化分数无法提供的。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [72] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: This paper explores the use of large language models (LLMs) as efficient simulators for agentic search tasks in reinforcement learning (RL). The authors propose Self-Search RL (SSRL), which enhances LLMs' ability to perform search tasks internally through format-based and rule-based rewards. The results show that SSRL-trained models are cost-effective, stable, and capable of robust sim-to-real transfer, reducing reliance on external search engines.


<details>
  <summary>Details</summary>
Motivation: We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines.

Method: We first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards.

Result: Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer.

Conclusion: LLMs possess world knowledge that can be effectively elicited to achieve high performance; SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [73] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文综述了扩散语言模型（DLMs）的最新发展，包括其优势、应用、挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: DLMs在减少推理延迟和捕捉双向上下文方面具有内在优势，同时能够实现生成过程的细粒度控制，因此需要对其进行全面的综述。

Method: 本文通过回顾DLMs的发展历程、与其他范式的关系，以及基础原理和最先进的模型，提供了一个全面的分类和深入的分析。

Result: 本文提供了DLMs的最新进展、推理策略和优化方法的详细分析，并讨论了其在多模态扩展和实际应用场景中的应用。

Conclusion: 本文对扩散语言模型（DLMs）进行了全面的综述，分析了其在自然语言处理任务中的优势和应用，并指出了当前DLMs面临的挑战和未来研究方向。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [74] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 本文提出了一种基于多任务学习框架的新型模型架构，用于优化个性化产品搜索排名。该方法结合了表格和非表格数据，并利用预训练的TinyBERT模型进行语义嵌入。实验结果表明，这种结合显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过整合表格和非表格数据，利用先进的嵌入技术和多任务学习框架，提高个性化产品搜索排名的性能。

Method: 本文提出了一种新颖的模型架构，用于使用多任务学习（MTL）框架优化个性化产品搜索排名。我们的方法独特地整合了表格和非表格数据，利用预训练的TinyBERT模型进行语义嵌入，并提出了一种新的采样技术来捕捉多样化的客户行为。

Result: 实验结果表明，将非表格数据与先进的嵌入技术结合在多任务学习范式中显著提升了模型性能。消融研究进一步强调了引入相关性标签、微调TinyBERT层以及TinyBERT查询-产品嵌入交互的好处。

Conclusion: 实验结果表明，在多任务学习范式中结合非表格数据和先进的嵌入技术显著提高了模型性能。这些结果证明了我们方法在实现改进的个性化产品搜索排名方面的有效性。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [75] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 本文介绍了Amazon Nova AI Challenge中大学团队和Amazon团队在提高AI软件开发安全性方面的进展，包括自动化红队机器人和安全AI助手的开发，以及在安全对齐、模型护栏等方面的创新。


<details>
  <summary>Details</summary>
Motivation: 为了确保AI系统在软件开发中的安全性，Amazon发起了Trusted AI track of the Amazon Nova AI Challenge，以推动安全AI的进步。

Method: 通过全球竞赛的形式，五个团队开发自动化红队机器人，另外五个团队创建安全的AI助手，利用对抗性比赛评估自动化红队和安全对齐方法。

Result: 团队开发了最先进的技术，引入了基于推理的安全对齐、稳健的模型护栏、多轮越狱和高效探测大型语言模型的新方法。

Conclusion: 本文概述了大学团队和Amazon Nova AI Challenge团队在解决AI软件开发安全挑战方面的进展，强调了这一合作努力如何提高AI安全的标准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [76] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出了一种范式转变，将人工智能重新定位为主要导演，医生作为其助手。介绍了DxDirector-7B，一种具有先进深度思维能力的大型语言模型，能够以最少的医生参与驱动全过程诊断。在全过程诊断设置下的评估显示，DxDirector-7B在诊断准确性和减少医生工作量方面优于现有的医学大型语言模型和通用大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能在临床诊断中的作用主要是作为医生的辅助工具，无法从模糊的主诉开始驱动整个诊断过程，这仍然严重依赖于人类医生。这种差距限制了人工智能减少医生工作量和提高诊断效率的能力。

Method: 我们提出了一个范式转变，逆转了医生和人工智能之间的关系：将人工智能重新定位为主要导演，医生作为其助手。我们介绍了DxDirector-7B，这是一种具有先进深度思维能力的大型语言模型，能够以最少的医生参与驱动全过程诊断。此外，DxDirector-7B建立了一个对误诊的责任框架，明确划分了人工智能和人类医生之间的责任。

Result: 在全过程诊断设置下的罕见、复杂和现实案例评估中，DxDirector-7B不仅实现了显著的诊断准确性，而且比最先进的医学大型语言模型以及通用大型语言模型大大减少了医生的工作量。细粒度分析多个临床部门和任务验证了其有效性，专家评估表明其潜在的可作为医学专家的替代品。

Conclusion: 这些发现标志着一个新时代的开始，其中人工智能传统上是医生的助手，现在可以驱动整个诊断过程，大幅减少医生的工作量，表明了一个高效且准确的诊断解决方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [77] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 本文研究了语言模型对齐过程中静态和在线策略数据的效果差异，并提出了一个对齐阶段假设，以更好地理解和优化模型对齐过程。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型对齐方法（如DPO）依赖于静态偏好数据或在线策略采样，但发现在线策略数据并不总是最优的。因此，需要一种更全面的方法来理解对齐过程并优化其效果。

Method: 本文提出了一个对齐阶段假设，并通过理论和实证分析验证了这一假设。此外，还提出了一种有效算法来识别对齐阶段之间的边界。

Result: 实验结果表明，在Llama-3上，基于在线策略数据的效果是静态数据的3倍，而在Zephyr上则为0.4倍。此外，提出的对齐阶段假设在多个模型和对齐方法中得到了验证。

Conclusion: 本文提出了一个关于语言模型对齐阶段的假设，即对齐过程可以分为两个不同的阶段：偏好注入阶段和偏好微调阶段。通过理论和实证分析，我们表征了这些阶段，并提出了一种有效的算法来识别它们之间的边界。实验表明，该假设在多个模型和对齐方法中具有通用性。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [78] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出ComMCS方法，通过减少方差提高LLM的推理能力，在数学基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂领域如数学中的推理能力问题，以及由于蒙特卡洛样本数量有限导致的估计误差问题。

Method: 提出了一种名为ComMCS的方法，通过线性组合当前和后续步骤的MC估计器来构建无偏估计器。

Result: ComMCS在MATH-500基准测试中比基于回归的优化方法高出2.8分，比非方差减少基线高出2.2分。

Conclusion: ComMCS方法在MATH-500和GSM8K基准测试中表现出色，优于基于回归的优化方法和非方差减少基线。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [79] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Nested-ReFT 的新强化微调框架，通过使用目标模型的部分层作为行为模型来降低计算成本，并保持与传统 ReFT 相当的性能。


<details>
  <summary>Details</summary>
Motivation: 标准的 ReFT 框架在训练过程中需要多次推理步骤，导致计算成本较高。为了降低这种成本，提出了 Nested-ReFT 框架。

Method: 受离策略强化学习和推测解码的启发，提出了一种名为 Nested-ReFT 的新框架，其中目标模型的一小部分层作为行为模型，在训练期间生成离策略完成。

Result: 理论分析表明，Nested-ReFT 能够提供无偏的梯度估计并控制方差。实验分析显示，它在多个数学推理基准和模型大小上提高了计算效率。

Conclusion: Nested-ReFT 提供了一种计算效率更高的强化微调框架，能够在保持性能的同时减少推理成本。

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [80] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习方法G-RA，用于解决长期任务中的奖励稀疏性问题，并在软件工程任务中进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 在长期强化学习任务中，奖励稀疏性仍然是一个重大挑战，而现有的基于结果的奖励塑造难以在不引入偏差或需要显式任务分解的情况下定义有意义的即时奖励。此外，基于验证的奖励塑造使用逐步批评者，但即时奖励与长期目标之间的不对齐可能导致奖励黑客和次优策略。

Method: 我们引入了一个面向软件工程的强化学习框架，以及一种名为Gated Reward Accumulation (G-RA)的新方法，该方法仅在高层（长期）奖励达到预定义阈值时累积即时奖励。

Result: 在SWE-bench Verified和kBench上的实验表明，G-RA提高了完成率（47.6% → 93.8%和22.0% → 86.0%）和修改率（19.6% → 23.8%和12.0% → 42.0%），同时避免了由于奖励不对齐导致的策略退化。

Conclusion: 我们的研究强调了在长期强化学习中平衡奖励积累的重要性，并提供了一个实用的解决方案。

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [81] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: The paper explores the use of Pass@k as a reward in RLVR, showing that it can improve exploration and challenge the assumption that exploration and exploitation are conflicting objectives.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of balancing exploration and exploitation in RLVR by investigating the potential of Pass@k as a reward metric.

Method: The paper uses Pass@k as a reward to train the policy model and derives an analytical solution for its advantage.

Result: Pass@k Training improves exploration ability and reveals that exploration and exploitation can mutually enhance each other.

Conclusion: Pass@k Training can enhance exploration and exploitation in RLVR, suggesting a promising direction for future research.

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [82] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 本文综述了将神经科学原理与Memory-Augmented Transformers的工程进展相结合的统一框架，提出了认知启发的、终身学习的Transformer架构的发展路线图。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在长程上下文保留、持续学习和知识整合方面存在关键限制，因此需要一种结合神经科学原理和工程进展的统一框架。

Method: 本文通过三个分类维度（功能目标、记忆表示和集成机制）组织了最近的进展，并分析了核心记忆操作（读取、写入、遗忘和容量管理）。

Result: 本文揭示了从静态缓存向自适应、测试时学习系统转变的趋势，并识别了可扩展性和干扰等持续挑战，以及分层缓冲和惊喜门控更新等新兴解决方案。

Conclusion: 本文综述了将神经科学原理与Memory-Augmented Transformers的工程进展相结合的统一框架，提出了认知启发的、终身学习的Transformer架构的发展路线图。

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [83] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了一种新的后训练范式Self-correction Flywheel，用于提升视觉和语言导航模型的错误纠正能力，并在多个基准测试中取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉和语言导航模型在执行指令时容易偏离正确轨迹，缺乏有效的错误纠正能力，这限制了它们从错误中恢复的能力。

Method: 提出了一种名为Self-correction Flywheel的新后训练范式，通过识别错误轨迹并生成自纠正数据来提升模型的性能。

Result: 通过多次飞轮迭代，逐步提升了基于单目RGB的VLA导航模型CorrectNav。在R2R-CE和RxR-CE基准测试中，CorrectNav取得了新的最先进成功率。

Conclusion: 实验表明，CorrectNav在R2R-CE和RxR-CE基准测试中达到了新的最先进成功率，分别达到65.1%和69.3%，优于之前的最佳VLA导航模型。真实机器人测试展示了方法在错误纠正、动态障碍物避让和长指令遵循方面的优越能力。

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 本文介绍了使用深度学习技术在OCR和文档布局分析中的三种任务的方法和结果。


<details>
  <summary>Details</summary>
Motivation: 提高历史希伯来碎片、16至18世纪会议决议和现代英文手写识别的准确性。

Method: 使用Kraken和TrOCR模型进行数据增强，使用CRNN结合DeepLabV3+和双向LSTM进行语义分割，以及使用CRNN与ResNet34编码器和CTC损失函数进行现代英文手写识别。

Result: 提高了字符识别的准确性，并提供了有价值的见解和未来研究的方向。

Conclusion: 本文展示了深度学习在OCR和文档布局分析中的有效性，并提出了未来研究的潜在方向。

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [85] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 本文提出了一种名为上下文过滤模型的新防御机制，用于检测和阻止越狱攻击，同时保持大型语言模型的性能。该模型能够减少高达88%的攻击成功率，并且适用于所有类型的LLMs，无需对模型进行微调。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在性能上取得了显著进展，但各种越狱攻击带来了日益增长的安全和伦理风险。恶意用户经常利用对抗性上下文来欺骗LLMs，促使它们生成有害查询的响应。为了提高LLMs的安全性，同时保持其原始性能，我们提出了这种新的防御机制。

Method: 我们提出了一种新的防御机制，称为上下文过滤模型，这是一种输入预处理方法，旨在过滤掉不可信和不可靠的上下文，同时识别包含真实用户意图的主要提示，以揭示隐藏的恶意意图。

Result: 我们的模型在防御越狱攻击方面的有效性通过比较分析进行了评估，与最先进的防御机制相比，针对六种不同的攻击，并评估了这些防御下LLMs的帮助性。结果表明，我们的模型能够减少越狱攻击的成功率高达88%，同时保持原始LLMs的性能，实现了最先进的安全性和帮助性产品结果。

Conclusion: 我们的模型能够减少高达88%的越狱攻击成功率，同时保持原始LLMs的性能，实现了最先进的安全性和帮助性产品结果。此外，我们的模型是一个即插即用的方法，可以应用于所有LLMs，包括白盒和黑盒模型，以提高它们的安全性而不需要对模型本身进行微调。

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [86] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: 本文提出了一种基于搜索的框架，用于发现和应对LLM-based代理中的隐私威胁。


<details>
  <summary>Details</summary>
Motivation: 由于LLM-based代理可能引入严重的隐私威胁，需要一种有效的方法来发现和应对这些威胁。

Method: 本文提出了一种基于搜索的框架，通过模拟隐私关键的代理交互来改进攻击者和防御者的指令。

Result: 攻击策略从简单的直接请求升级为复杂的多轮策略，而防御措施从基于规则的约束发展为身份验证状态机。

Conclusion: 本文提出的搜索框架能够有效发现隐私威胁并提升防御能力，具有广泛的实际应用价值。

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [87] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: Saracoder是一个分层特征优化框架，通过系统优化检索结果，解决了仓库级代码补全中的语义误导、冗余和同质性问题，并有效解决了外部符号歧义。实验表明，Saracoder在多个编程语言和模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码补全方法通常依赖于表面文本相似性，导致结果受到语义误导、冗余和同质性的困扰，同时无法解决外部符号歧义问题。

Method: 我们引入了Saracoder，这是一个分层特征优化框架。其核心分层特征优化模块通过提炼深层语义关系、修剪精确重复项、评估结构相似性（使用一种新的基于图的度量标准，该度量标准根据拓扑重要性加权编辑）以及重新排序结果来最大化相关性和多样性。此外，一个外部感知标识符消歧模块通过依赖分析准确解决跨文件符号歧义。

Result: 在具有挑战性的CrossCodeEval和RepoEval-Updated基准测试中进行的广泛实验表明，Saracoder在多种编程语言和模型上显著优于现有基线。

Conclusion: 我们的工作证明，通过多个维度系统地优化检索结果，为构建更准确和健壮的仓库级代码补全系统提供了一个新范式。

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [88] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
*Yifan Song,Wing Yee Au,Hon Yung Wong,Brian P. Bailey,Tal August*

Main category: cs.HC

TL;DR: 本研究探讨了术语障碍对跨学科沟通的影响，并设计了一个基于LLM的实时个性化术语支持系统ParseJargon，实验结果显示该系统提高了参与者的理解力和参与度。


<details>
  <summary>Details</summary>
Motivation: 有效跨学科沟通经常受到领域特定术语的阻碍，因此需要探索术语障碍的深入理解。

Method: 我们进行了一项形成性日记研究，揭示了当前术语管理策略在工作场所会议中的关键限制，并设计了ParseJargon系统，该系统利用交互式LLM提供实时个性化术语识别和解释。

Result: ParseJargon在控制实验中显示出显著提高参与者的理解力、参与度和对同事工作的欣赏，而通用支持则对参与度产生了负面影响。后续实地研究验证了ParseJargon在实时会议中的可用性和实际价值。

Conclusion: 我们的研究结果为设计个性化的术语支持工具提供了见解，并对更广泛的跨学科和教育应用有影响。

Abstract: Effective interdisciplinary communication is frequently hindered by
domain-specific jargon. To explore the jargon barriers in-depth, we conducted a
formative diary study with 16 professionals, revealing critical limitations in
current jargon-management strategies during workplace meetings. Based on these
insights, we designed ParseJargon, an interactive LLM-powered system providing
real-time personalized jargon identification and explanations tailored to
users' individual backgrounds. A controlled experiment comparing ParseJargon
against baseline (no support) and general-purpose (non-personalized) conditions
demonstrated that personalized jargon support significantly enhanced
participants' comprehension, engagement, and appreciation of colleagues' work,
whereas general-purpose support negatively affected engagement. A follow-up
field study validated ParseJargon's usability and practical value in real-time
meetings, highlighting both opportunities and limitations for real-world
deployment. Our findings contribute insights into designing personalized jargon
support tools, with implications for broader interdisciplinary and educational
applications.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [89] [Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning](https://arxiv.org/abs/2508.10057)
*Christopher Pinier,Sonia Acuña Vargas,Mariia Steeghs-Turchina,Dora Matzke,Claire E. Stevenson,Michael D. Nunez*

Main category: q-bio.NC

TL;DR: 研究发现大型语言模型在抽象推理中可能模仿人类大脑机制，提供了生物和人工智能之间共享原则的初步证据。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨大型语言模型是否在抽象推理中反映人类神经认知。

Method: 研究比较了人类参与者与八种开源LLMs在抽象模式补全任务中的表现和神经表征，并利用模式类型差异和眼动相关电位（FRPs）进行分析。

Result: 只有最大的测试LLMs（约700亿参数）达到了与人类相当的准确性，Qwen-2.5-72B和DeepSeek-R1-70B也显示出与人类模式特定难度分布的相似性。所有测试的LLMs在中间层中形成了明显区分抽象模式类别的表征，且这种聚类强度与任务表现成正比。任务最优LLM层的表征几何与人类额叶FRPs存在中度正相关。

Conclusion: 研究结果表明，大型语言模型（LLMs）可能在抽象推理中模仿人类大脑机制，提供了生物和人工智能之间共享原则的初步证据。

Abstract: This study investigates whether large language models (LLMs) mirror human
neurocognition during abstract reasoning. We compared the performance and
neural representations of human participants with those of eight open-source
LLMs on an abstract-pattern-completion task. We leveraged pattern type
differences in task performance and in fixation-related potentials (FRPs) as
recorded by electroencephalography (EEG) during the task. Our findings indicate
that only the largest tested LLMs (~70 billion parameters) achieve
human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing
similarities with the human pattern-specific difficulty profile. Critically,
every LLM tested forms representations that distinctly cluster the abstract
pattern categories within their intermediate layers, although the strength of
this clustering scales with their performance on the task. Moderate positive
correlations were observed between the representational geometries of
task-optimal LLM layers and human frontal FRPs. These results consistently
diverged from comparisons with other EEG measures (response-locked ERPs and
resting EEG), suggesting a potential shared representational space for abstract
patterns. This indicates that LLMs might mirror human brain mechanisms in
abstract reasoning, offering preliminary evidence of shared principles between
biological and artificial intelligence.

</details>
